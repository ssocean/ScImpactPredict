title,TNCSI,abstract,OA,authors_title
SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory,0.648752,"Accurately predicting the destination of taxi trajectories can have various
benefits for intelligent location-based services. One potential method to
accomplish this prediction is by converting the taxi trajectory into a
two-dimensional grid and using computer vision techniques. While the Swin
Transformer is an innovative computer vision architecture with demonstrated
success in vision downstream tasks, it is not commonly used to solve real-world
trajectory problems. In this paper, we propose a simplified Swin Transformer
(SST) structure that does not use the shifted window idea in the traditional
Swin Transformer, as trajectory data is consecutive in nature. Our
comprehensive experiments, based on real trajectory data, demonstrate that SST
can achieve higher accuracy compared to state-of-the-art methods.",None,-1
Unsupervised Out-of-Distribution Detection with Diffusion Inpainting,0.915165,"Unsupervised out-of-distribution detection (OOD) seeks to identify
out-of-domain data by learning only from unlabeled in-domain data. We present a
novel approach for this task - Lift, Map, Detect (LMD) - that leverages recent
advancement in diffusion models. Diffusion models are one type of generative
models. At their core, they learn an iterative denoising process that gradually
maps a noisy image closer to their training manifolds. LMD leverages this
intuition for OOD detection. Specifically, LMD lifts an image off its original
manifold by corrupting it, and maps it towards the in-domain manifold with a
diffusion model. For an out-of-domain image, the mapped image would have a
large distance away from its original manifold, and LMD would identify it as
OOD accordingly. We show through extensive experiments that LMD achieves
competitive performance across a broad variety of datasets. Code can be found
at https://github.com/zhenzhel/lift_map_detect.",None,-1
Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-commerce,0.154976,"This paper aims to establish a generic multi-modal foundation model that has
the scalable capability to massive downstream applications in E-commerce.
Recently, large-scale vision-language pretraining approaches have achieved
remarkable advances in the general domain. However, due to the significant
differences between natural and product images, directly applying these
frameworks for modeling image-level representations to E-commerce will be
inevitably sub-optimal. To this end, we propose an instance-centric multi-modal
pretraining paradigm called ECLIP in this work. In detail, we craft a decoder
architecture that introduces a set of learnable instance queries to explicitly
aggregate instance-level semantics. Moreover, to enable the model to focus on
the desired product instance without reliance on expensive manual annotations,
two specially configured pretext tasks are further proposed. Pretrained on the
100 million E-commerce-related data, ECLIP successfully extracts more generic,
semantic-rich, and robust representations. Extensive experimental results show
that, without further fine-tuning, ECLIP surpasses existing methods by a large
margin on a broad range of downstream tasks, demonstrating the strong
transferability to real-world E-commerce applications.",None,-1
How Generalizable are Deepfake Detectors? An Empirical Study,0.113636,"Deepfake videos and images are becoming increasingly credible, posing a
significant threat given their potential to facilitate fraud or bypass access
control systems. This has motivated the development of deepfake detection
methods, in which deep learning models are trained to distinguish between real
and synthesized footage. Unfortunately, existing detection models struggle to
generalize to deepfakes from datasets they were not trained on, but little work
has been done to examine why or how this limitation can be addressed. In this
paper, we present the first empirical study on the generalizability of deepfake
detectors, an essential goal for detectors to stay one step ahead of attackers.
Our study utilizes six deepfake datasets, five deepfake detection methods, and
two model augmentation approaches, confirming that detectors do not generalize
in zero-shot settings. Additionally, we find that detectors are learning
unwanted properties specific to synthesis methods and struggling to extract
discriminative features, limiting their ability to generalize. Finally, we find
that there are neurons universally contributing to detection across seen and
unseen datasets, illuminating a possible path forward to zero-shot
generalizability.",None,-1
Meta Generative Attack on Person Reidentification,0.188594,"Adversarial attacks have been recently investigated in person
re-identification. These attacks perform well under cross dataset or cross
model setting. However, the challenges present in cross-dataset cross-model
scenario does not allow these models to achieve similar accuracy. To this end,
we propose our method with the goal of achieving better transferability against
different models and across datasets. We generate a mask to obtain better
performance across models and use meta learning to boost the generalizability
in the challenging cross-dataset cross-model setting. Experiments on
Market-1501, DukeMTMC-reID and MSMT-17 demonstrate favorable results compared
to other attacks.",None,-1
"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",0.621143,"Human feedback is increasingly used to steer the behaviours of Large Language
Models (LLMs). However, it is unclear how to collect and incorporate feedback
in a way that is efficient, effective and unbiased, especially for highly
subjective human preferences and values. In this paper, we survey existing
approaches for learning from human feedback, drawing on 95 papers primarily
from the ACL and arXiv repositories.First, we summarise the past, pre-LLM
trends for integrating human feedback into language models. Second, we give an
overview of present techniques and practices, as well as the motivations for
using feedback; conceptual frameworks for defining values and preferences; and
how feedback is collected and from whom. Finally, we encourage a better future
of feedback learning in LLMs by raising five unresolved conceptual and
practical challenges.",None,-1
Redefining Digital Health Interfaces with Large Language Models,0.668982,"Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their adoption remains comparatively limited
due, in part, to challenges surrounding usability and trust. Large Language
Models (LLMs) have emerged as general-purpose models with the ability to
process complex information and produce human-quality text, presenting a wealth
of potential applications in healthcare. Directly applying LLMs in clinical
settings is not straightforward, however, with LLMs susceptible to providing
inconsistent or nonsensical answers. We demonstrate how LLM-based systems can
utilize external tools and provide a novel interface between clinicians and
digital technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLMs
in clinical settings such as hallucinations. We illustrate LLM-based interfaces
with the example of cardiovascular disease risk prediction. We develop a new
prognostic tool using automated machine learning and demonstrate how LLMs can
provide a unique interface to both our model and existing risk scores,
highlighting the benefit compared to traditional interfaces for digital tools.",None,-1
NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource Languages through Data Enrichment,0.501738,"In recent years, natural language processing has gained significant
popularity in various sectors, including the legal domain. This paper presents
NeCo Team's solutions to the Vietnamese text processing tasks provided in the
Automated Legal Question Answering Competition 2023 (ALQAC 2023), focusing on
legal domain knowledge acquisition for low-resource languages through data
enrichment. Our methods for the legal document retrieval task employ a
combination of similarity ranking and deep learning models, while for the
second task, which requires extracting an answer from a relevant legal article
in response to a question, we propose a range of adaptive techniques to handle
different question types. Our approaches achieve outstanding results on both
tasks of the competition, demonstrating the potential benefits and
effectiveness of question answering systems in the legal field, particularly
for low-resource languages.",None,-1
Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?,0.380979,"Artificial intelligence (AI) systems will increasingly be used to cause harm
as they grow more capable. In fact, AI systems are already starting to be used
to automate fraudulent activities, violate human rights, create harmful fake
images, and identify dangerous toxins. To prevent some misuses of AI, we argue
that targeted interventions on certain capabilities will be warranted. These
restrictions may include controlling who can access certain types of AI models,
what they can be used for, whether outputs are filtered or can be traced back
to their user, and the resources needed to develop them. We also contend that
some restrictions on non-AI capabilities needed to cause harm will be required.
Though capability restrictions risk reducing use more than misuse (facing an
unfavorable Misuse-Use Tradeoff), we argue that interventions on capabilities
are warranted when other interventions are insufficient, the potential harm
from misuse is high, and there are targeted ways to intervene on capabilities.
We provide a taxonomy of interventions that can reduce AI misuse, focusing on
the specific steps required for a misuse to cause harm (the Misuse Chain), and
a framework to determine if an intervention is warranted. We apply this
reasoning to three examples: predicting novel toxins, creating harmful images,
and automating spear phishing campaigns.",None,-1
Analyzing Intentional Behavior in Autonomous Agents under Uncertainty,0.0990424,"Principled accountability for autonomous decision-making in uncertain
environments requires distinguishing intentional outcomes from negligent
designs from actual accidents. We propose analyzing the behavior of autonomous
agents through a quantitative measure of the evidence of intentional behavior.
We model an uncertain environment as a Markov Decision Process (MDP). For a
given scenario, we rely on probabilistic model checking to compute the ability
of the agent to influence reaching a certain event. We call this the scope of
agency. We say that there is evidence of intentional behavior if the scope of
agency is high and the decisions of the agent are close to being optimal for
reaching the event. Our method applies counterfactual reasoning to
automatically generate relevant scenarios that can be analyzed to increase the
confidence of our assessment. In a case study, we show how our method can
distinguish between 'intentional' and 'accidental' traffic collisions.",None,-1
How are Prompts Different in Terms of Sensitivity?,0.268575,"In-context learning (ICL) has become one of the most popular learning
paradigms. While there is a growing body of literature focusing on prompt
engineering, there is a lack of systematic analysis comparing the effects of
prompts across different models and tasks. To address this gap, we present a
comprehensive prompt analysis based on the sensitivity of a function. Our
analysis reveals that sensitivity is an unsupervised proxy for model
performance, as it exhibits a strong negative correlation with accuracy. We use
gradient-based saliency scores to empirically demonstrate how different prompts
affect the relevance of input tokens to the output, resulting in different
levels of sensitivity. Furthermore, we introduce sensitivity-aware decoding
which incorporates sensitivity estimation as a penalty term in the standard
greedy decoding. We show that this approach is particularly helpful when
information in the input is scarce. Our work provides a fresh perspective on
the analysis of prompts, and contributes to a better understanding of the
mechanism of ICL.",None,-1
Why Does ChatGPT Fall Short in Providing Truthful Answers?,0.598812,"Recent advancements in large language models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in providing reliable and accurate
answers to user questions. To better understand the model's particular
weaknesses in providing truthful answers, we embark an in-depth exploration of
open-domain question answering. Specifically, we undertake a detailed
examination of ChatGPT's failures, categorized into: comprehension, factuality,
specificity, and inference. We further pinpoint factuality as the most
contributing failure and identify two critical abilities associated with
factuality: knowledge memorization and knowledge recall. Through experiments
focusing on factuality, we propose several potential enhancement strategies.
Our findings suggest that augmenting the model with granular external knowledge
and cues for knowledge recall can enhance the model's factuality in answering
questions.",None,-1
Emotionally Enhanced Talking Face Generation,0.609572,"Several works have developed end-to-end pipelines for generating lip-synced
talking faces with various real-world applications, such as teaching and
language translation in videos. However, these prior works fail to create
realistic-looking videos since they focus little on people's expressions and
emotions. Moreover, these methods' effectiveness largely depends on the faces
in the training dataset, which means they may not perform well on unseen faces.
To mitigate this, we build a talking face generation framework conditioned on a
categorical emotion to generate videos with appropriate expressions, making
them more realistic and convincing. With a broad range of six emotions, i.e.,
\emph{happiness}, \emph{sadness}, \emph{fear}, \emph{anger}, \emph{disgust},
and \emph{neutral}, we show that our model can adapt to arbitrary identities,
emotions, and languages. Our proposed framework is equipped with a
user-friendly web interface with a real-time experience for talking face
generation with emotions. We also conduct a user study for subjective
evaluation of our interface's usability, design, and functionality. Project
page: https://midas.iiitd.edu.in/emo/",None,-1
QuAVF: Quality-aware Audio-Visual Fusion for Ego4D Talking to Me Challenge,0.113046,"This technical report describes our QuAVF@NTU-NVIDIA submission to the Ego4D
Talking to Me (TTM) Challenge 2023. Based on the observation from the TTM task
and the provided dataset, we propose to use two separate models to process the
input videos and audio. By doing so, we can utilize all the labeled training
data, including those without bounding box labels. Furthermore, we leverage the
face quality score from a facial landmark prediction model for filtering noisy
face input data. The face quality score is also employed in our proposed
quality-aware fusion for integrating the results from two branches. With the
simple architecture design, our model achieves 67.4% mean average precision
(mAP) on the test set, which ranks first on the leaderboard and outperforms the
baseline method by a large margin. Code is available at:
https://github.com/hsi-che-lin/Ego4D-QuAVF-TTM-CVPR23",None,-1
Founder-GPT: Self-play to evaluate the Founder-Idea fit,0.0330701,"This research introduces an innovative evaluation method for the
""founder-idea"" fit in early-stage startups, utilizing advanced large language
model techniques to assess founders' profiles against their startup ideas to
enhance decision-making. Embeddings, self-play, tree-of-thought, and
critique-based refinement techniques show early promising results that each
idea's success patterns are unique and they should be evaluated based on the
context of the founder's background.",None,-1
Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?,0.807754,"Large Language Models (LLMs) are advancing at a rapid pace, with significant
improvements at natural language processing and coding tasks. Yet, their
ability to work with formal languages representing data, specifically within
the realm of knowledge graph engineering, remains under-investigated. To
evaluate the proficiency of various LLMs, we created a set of five tasks that
probe their ability to parse, understand, analyze, and create knowledge graphs
serialized in Turtle syntax. These tasks, each embodying distinct degrees of
complexity and being able to scale with the size of the problem, have been
integrated into our automated evaluation system, the LLM-KG-Bench. The
evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,
Claude 1.3, and Claude 2.0, as well as two freely accessible offline models,
GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth
understanding of the strengths and shortcomings of LLMs in relation to their
application within RDF knowledge graph engineering workflows utilizing Turtle
representation. While our findings show that the latest commercial models
outperform their forerunners in terms of proficiency with the Turtle language,
they also reveal an apparent weakness. These models fall short when it comes to
adhering strictly to the output formatting constraints, a crucial requirement
in this context.",None,-1
SOCS: Semantically-aware Object Coordinate Space for Category-Level 6D Object Pose Estimation under Large Shape Variations,0.572665,"Most learning-based approaches to category-level 6D pose estimation are
design around normalized object coordinate space (NOCS). While being
successful, NOCS-based methods become inaccurate and less robust when handling
objects of a category containing significant intra-category shape variations.
This is because the object coordinates induced by global and rigid alignment of
objects are semantically incoherent, making the coordinate regression hard to
learn and generalize. We propose Semantically-aware Object Coordinate Space
(SOCS) built by warping-and-aligning the objects guided by a sparse set of
keypoints with semantically meaningful correspondence. SOCS is semantically
coherent: Any point on the surface of a object can be mapped to a semantically
meaningful location in SOCS, allowing for accurate pose and size estimation
under large shape variations. To learn effective coordinate regression to SOCS,
we propose a novel multi-scale coordinate-based attention network. Evaluations
demonstrate that our method is easy to train, well-generalizing for large
intra-category shape variations and robust to inter-object occlusions.",None,-1
Integrating Semantic Information into Sketchy Reading Module of Retro-Reader for Vietnamese Machine Reading Comprehension,0.40413,"Machine Reading Comprehension has become one of the most advanced and popular
research topics in the fields of Natural Language Processing in recent years.
The classification of answerability questions is a relatively significant
sub-task in machine reading comprehension; however, there haven't been many
studies. Retro-Reader is one of the studies that has solved this problem
effectively. However, the encoders of most traditional machine reading
comprehension models in general and Retro-Reader, in particular, have not been
able to exploit the contextual semantic information of the context completely.
Inspired by SemBERT, we use semantic role labels from the SRL task to add
semantics to pre-trained language models such as mBERT, XLM-R, PhoBERT. This
experiment was conducted to compare the influence of semantics on the
classification of answerability for the Vietnamese machine reading
comprehension. Additionally, we hope this experiment will enhance the encoder
for the Retro-Reader model's Sketchy Reading Module. The improved Retro-Reader
model's encoder with semantics was first applied to the Vietnamese Machine
Reading Comprehension task and obtained positive results.",None,-1
USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense Active Learning for Super-resolution,0.397103,"Dense regression is a widely used approach in computer vision for tasks such
as image super-resolution, enhancement, depth estimation, etc. However, the
high cost of annotation and labeling makes it challenging to achieve accurate
results. We propose incorporating active learning into dense regression models
to address this problem. Active learning allows models to select the most
informative samples for labeling, reducing the overall annotation cost while
improving performance. Despite its potential, active learning has not been
widely explored in high-dimensional computer vision regression tasks like
super-resolution. We address this research gap and propose a new framework
called USIM-DAL that leverages the statistical properties of colour images to
learn informative priors using probabilistic deep neural networks that model
the heteroscedastic predictive distribution allowing uncertainty
quantification. Moreover, the aleatoric uncertainty from the network serves as
a proxy for error that is used for active learning. Our experiments on a wide
variety of datasets spanning applications in natural images (visual genome,
BSD100), medical imaging (histopathology slides), and remote sensing (satellite
images) demonstrate the efficacy of the newly proposed USIM-DAL and superiority
over several dense regression active learning methods.",None,-1
Toward Unsupervised Realistic Visual Question Answering,0.0722922,"The problem of realistic VQA (RVQA), where a model has to reject unanswerable
questions (UQs) and answer answerable ones (AQs), is studied. We first point
out 2 drawbacks in current RVQA research, where (1) datasets contain too many
unchallenging UQs and (2) a large number of annotated UQs are required for
training. To resolve the first drawback, we propose a new testing dataset,
RGQA, which combines AQs from an existing VQA dataset with around 29K
human-annotated UQs. These UQs consist of both fine-grained and coarse-grained
image-question pairs generated with 2 approaches: CLIP-based and
Perturbation-based. To address the second drawback, we introduce an
unsupervised training approach. This combines pseudo UQs obtained by randomly
pairing images and questions, with an RoI Mixup procedure to generate more
fine-grained pseudo UQs, and model ensembling to regularize model confidence.
Experiments show that using pseudo UQs significantly outperforms RVQA
baselines. RoI Mixup and model ensembling further increase the gain. Finally,
human evaluation reveals a performance gap between humans and models, showing
that more RVQA research is needed.",None,-1
Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine,0.862896,"One of the major barriers to using large language models (LLMs) in medicine
is the perception they use uninterpretable methods to make clinical decisions
that are inherently different from the cognitive processes of clinicians. In
this manuscript we develop novel diagnostic reasoning prompts to study whether
LLMs can perform clinical reasoning to accurately form a diagnosis. We find
that GPT4 can be prompted to mimic the common clinical reasoning processes of
clinicians without sacrificing diagnostic accuracy. This is significant because
an LLM that can use clinical reasoning to provide an interpretable rationale
offers physicians a means to evaluate whether LLMs can be trusted for patient
care. Novel prompting methods have the potential to expose the black box of
LLMs, bringing them one step closer to safe and effective use in medicine.",None,-1
Generating a Structured Summary of Numerous Academic Papers: Dataset and Method,0.404529,"Writing a survey paper on one research topic usually needs to cover the
salient content from numerous related papers, which can be modeled as a
multi-document summarization (MDS) task. Existing MDS datasets usually focus on
producing the structureless summary covering a few input documents. Meanwhile,
previous structured summary generation works focus on summarizing a single
document into a multi-section summary. These existing datasets and methods
cannot meet the requirements of summarizing numerous academic papers into a
structured summary. To deal with the scarcity of available data, we propose
BigSurvey, the first large-scale dataset for generating comprehensive summaries
of numerous academic papers on each topic. We collect target summaries from
more than seven thousand survey papers and utilize their 430 thousand reference
papers' abstracts as input documents. To organize the diverse content from
dozens of input documents and ensure the efficiency of processing long text
sequences, we propose a summarization method named category-based alignment and
sparse transformer (CAST). The experimental results show that our CAST method
outperforms various advanced summarization methods.",None,-1
Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,0.77426,"The language ability of Large Language Models (LLMs) is often unbalanced
towards English because of the imbalance in the distribution of the
pre-training data. This disparity is demanded in further fine-tuning and
affecting the cross-lingual abilities of LLMs. In this paper, we propose to
empower Instructiontuned LLMs (It-LLMs) in languages other than English by
building semantic alignment between them. Hence, we propose CrossAlpaca, an
It-LLM with cross-lingual instruction-following and Translation-following
demonstrations to improve semantic alignment between languages. We validate our
approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA
and adapted versions of MMLU and BBH. Our models, tested over six different
languages, outperform the It-LLMs tuned on monolingual data. The final results
show that instruction tuning on non-English data is not enough and that
semantic alignment can be further improved by Translation-following
demonstrations.",None,-1
IFAN: An Explainability-Focused Interaction Framework for Humans and NLP Models,0.106662,"Interpretability and human oversight are fundamental pillars of deploying
complex NLP models into real-world applications. However, applying
explainability and human-in-the-loop methods requires technical proficiency.
Despite existing toolkits for model understanding and analysis, options to
integrate human feedback are still limited. We propose IFAN, a framework for
real-time explanation-based interaction with NLP models. Through IFAN's
interface, users can provide feedback to selected model explanations, which is
then integrated through adapter layers to align the model with human rationale.
We show the system to be effective in debiasing a hate speech classifier with
minimal impact on performance. IFAN also offers a visual admin system and API
to manage models (and datasets) as well as control access rights. A demo is
live at https://ifan.ml.",None,-1
Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing,0.694765,"We present Impossible Distillation, a novel framework for paraphrasing and
sentence summarization, that distills a high-quality dataset and model from a
low-quality teacher that itself cannot perform these tasks. Unlike prior works
that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific
architecture, we hypothesize and verify the paraphrastic proximity intrinsic to
pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in
the LM distribution. By identifying and distilling generations from these
subspaces, Impossible Distillation produces a high-quality dataset and model
even from GPT2-scale LMs. We evaluate our method on multiple benchmarks
spanning unconstrained / syntax-controlled paraphrase generation and sentence
summarization. Our model with 770M parameters consistently outperforms strong
baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT
itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher
diversity and fidelity than up to 13 times larger datasets.",None,-1
Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond,0.855567,"Recently, multi-modality scene perception tasks, e.g., image fusion and scene
understanding, have attracted widespread attention for intelligent vision
systems. However, early efforts always consider boosting a single task
unilaterally and neglecting others, seldom investigating their underlying
connections for joint promotion. To overcome these limitations, we establish
the hierarchical dual tasks-driven deep model to bridge these tasks.
Concretely, we firstly construct an image fusion module to fuse complementary
characteristics and cascade dual task-related modules, including a
discriminator for visual effects and a semantic network for feature
measurement. We provide a bi-level perspective to formulate image fusion and
follow-up downstream tasks. To incorporate distinct task-related responses for
image fusion, we consider image fusion as a primary goal and dual modules as
learnable constraints. Furthermore, we develop an efficient first-order
approximation to compute corresponding gradients and present dynamic weighted
aggregation to balance the gradients for fusion learning. Extensive experiments
demonstrate the superiority of our method, which not only produces visually
pleasant fused results but also realizes significant promotion for detection
and segmentation than the state-of-the-art approaches.",None,-1
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,0.464182,"With an excellent balance between speed and accuracy, cutting-edge YOLO
frameworks have become one of the most efficient algorithms for object
detection. However, the performance of using YOLO networks is scarcely
investigated in brain tumor detection. We propose a novel YOLO architecture
with Reparameterized Convolution based on channel Shuffle (RCS-YOLO). We
present RCS and a One-Shot Aggregation of RCS (RCS-OSA), which link feature
cascade and computation efficiency to extract richer information and reduce
time consumption. Experimental results on the brain tumor dataset Br35H show
that the proposed model surpasses YOLOv6, YOLOv7, and YOLOv8 in speed and
accuracy. Notably, compared with YOLOv7, the precision of RCS-YOLO improves by
1%, and the inference speed by 60% at 114.8 images detected per second (FPS).
Our proposed RCS-YOLO achieves state-of-the-art performance on the brain tumor
detection task. The code is available at https://github.com/mkang315/RCS-YOLO.",None,-1
Meta-Learned Models of Cognition,0.725674,"Meta-learning is a framework for learning learning algorithms through
repeated interactions with an environment as opposed to designing them by hand.
In recent years, this framework has established itself as a promising tool for
building models of human cognition. Yet, a coherent research program around
meta-learned models of cognition is still missing. The purpose of this article
is to synthesize previous work in this field and establish such a research
program. We rely on three key pillars to accomplish this goal. We first point
out that meta-learning can be used to construct Bayes-optimal learning
algorithms. This result not only implies that any behavioral phenomenon that
can be explained by a Bayesian model can also be explained by a meta-learned
model but also allows us to draw strong connections to the rational analysis of
cognition. We then discuss several advantages of the meta-learning framework
over traditional Bayesian methods. In particular, we argue that meta-learning
can be applied to situations where Bayesian inference is impossible and that it
enables us to make rational models of cognition more realistic, either by
incorporating limited computational resources or neuroscientific knowledge.
Finally, we reexamine prior studies from psychology and neuroscience that have
applied meta-learning and put them into the context of these new insights. In
summary, our work highlights that meta-learning considerably extends the scope
of rational analysis and thereby of cognitive theories more generally.",None,-1
SSN: Stockwell Scattering Network for SAR Image Change Detection,0.554084,"Recently, synthetic aperture radar (SAR) image change detection has become an
interesting yet challenging direction due to the presence of speckle noise.
Although both traditional and modern learning-driven methods attempted to
overcome this challenge, deep convolutional neural networks (DCNNs)-based
methods are still hindered by the lack of interpretability and the requirement
of large computation power. To overcome this drawback, wavelet scattering
network (WSN) and Fourier scattering network (FSN) are proposed. Combining
respective merits of WSN and FSN, we propose Stockwell scattering network (SSN)
based on Stockwell transform which is widely applied against noisy signals and
shows advantageous characteristics in speckle reduction. The proposed SSN
provides noise-resilient feature representation and obtains state-of-art
performance in SAR image change detection as well as high computational
efficiency. Experimental results on three real SAR image datasets demonstrate
the effectiveness of the proposed method.",None,-1
Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions,0.0546637,"Recent works considering professional legal-linguistic style (PLLS) texts
have shown promising results on the charge prediction task. However,
unprofessional users also show an increasing demand on such a prediction
service. There is a clear domain discrepancy between PLLS texts and non-PLLS
texts expressed by those laypersons, which degrades the current SOTA models'
performance on non-PLLS texts. A key challenge is the scarcity of non-PLLS data
for most charge classes. This paper proposes a novel few-shot domain adaptation
(FSDA) method named Disentangled Legal Content for Charge Prediction (DLCCP).
Compared with existing FSDA works, which solely perform instance-level
alignment without considering the negative impact of text style information
existing in latent features, DLCCP (1) disentangles the content and style
representations for better domain-invariant legal content learning with
carefully designed optimization goals for content and style spaces and, (2)
employs the constitutive elements knowledge of charges to extract and align
element-level and instance-level content representations simultaneously. We
contribute the first publicly available non-PLLS dataset named NCCP for
developing layperson-friendly charge prediction models. Experiments on NCCP
show the superiority of our methods over competitive baselines.",None,-1
Multimodal Machine Unlearning,0.731273,"Machine Unlearning is the process of removing specific training data samples
and their corresponding effects from an already trained model. It has
significant practical benefits, such as purging private, inaccurate, or
outdated information from trained models without the need for complete
re-training. Unlearning within a multimodal setting presents unique challenges
due to the intrinsic dependencies between different data modalities and the
expensive cost of training on large multimodal datasets and architectures.
Current approaches to machine unlearning have not fully addressed these
challenges. To bridge this gap, we introduce MMUL, a machine unlearning
approach specifically designed for multimodal data and models. MMUL formulates
the multimodal unlearning task by focusing on three key properties: (a):
modality decoupling, which effectively decouples the association between
individual unimodal data points within multimodal inputs marked for deletion,
rendering them as unrelated data points within the model's context, (b):
unimodal knowledge retention, which retains the unimodal representation
capability of the model post-unlearning, and (c): multimodal knowledge
retention, which retains the multimodal representation capability of the model
post-unlearning. MMUL is efficient to train and is not constrained by the
requirement of using a strongly convex loss. Experiments on two multimodal
models and four multimodal benchmark datasets, including vision-language and
graph-language datasets, show that MMUL outperforms existing baselines, gaining
an average improvement of +17.6 points against the best-performing unimodal
baseline in distinguishing between deleted and remaining data. In addition,
MMUL can largely maintain pre-existing knowledge of the original model post
unlearning, with a performance gap of only 0.3 points compared to retraining a
new model from scratch.",None,-1
False perspectives on human language: why statistics needs linguistics,0.046636,"A sharp tension exists about the nature of human language between two
opposite parties: those who believe that statistical surface distributions, in
particular using measures like surprisal, provide a better understanding of
language processing, vs. those who believe that discrete hierarchical
structures implementing linguistic information such as syntactic ones are a
better tool. In this paper, we show that this dichotomy is a false one. Relying
on the fact that statistical measures can be defined on the basis of either
structural or non-structural models, we provide empirical evidence that only
models of surprisal that reflect syntactic structure are able to account for
language regularities.",None,-1
Enhancing Cross-lingual Transfer via Phonemic Transcription Integration,0.49446,"Previous cross-lingual transfer methods are restricted to orthographic
representation learning via textual scripts. This limitation hampers
cross-lingual transfer and is biased towards languages sharing similar
well-known scripts. To alleviate the gap between languages from different
writing scripts, we propose PhoneXL, a framework incorporating phonemic
transcriptions as an additional linguistic modality beyond the traditional
orthographic transcriptions for cross-lingual transfer. Particularly, we
propose unsupervised alignment objectives to capture (1) local one-to-one
alignment between the two different modalities, (2) alignment via
multi-modality contexts to leverage information from additional modalities, and
(3) alignment via multilingual contexts where additional bilingual dictionaries
are incorporated. We also release the first phonemic-orthographic alignment
dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech
Tagging) among the understudied but interconnected
Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals
phonemic transcription provides essential information beyond the orthography to
enhance cross-lingual transfer and bridge the gap among CJKV languages, leading
to consistent improvements on cross-lingual token-level tasks over
orthographic-based multilingual PLMs.",None,-1
SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration,0.280043,"The potential social harms that large language models pose, such as
generating offensive content and reinforcing biases, are steeply rising.
Existing works focus on coping with this concern while interacting with
ill-intentioned users, such as those who explicitly make hate speech or elicit
harmful responses. However, discussions on sensitive issues can become toxic
even if the users are well-intentioned. For safer models in such scenarios, we
present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a
large-scale Korean dataset of 49k sensitive questions with 42k acceptable and
46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA
in a human-in-the-loop manner based on real news headlines. Experiments show
that acceptable response generation significantly improves for HyperCLOVA and
GPT-3, demonstrating the efficacy of this dataset.",None,-1
Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,0.722883,"Retrieval augmentation can aid language models (LMs) in knowledge-intensive
tasks by supplying them with external information. Prior works on retrieval
augmentation usually jointly fine-tune the retriever and the LM, making them
closely coupled. In this paper, we explore the scheme of generic retrieval
plug-in: the retriever is to assist target LMs that may not be known beforehand
or are unable to be fine-tuned together. To retrieve useful documents for
unseen target LMs, we propose augmentation-adapted retriever (AAR), which
learns LM's preferences obtained from a known source LM. Experiments on the
MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM
is able to significantly improve the zero-shot generalization of larger target
LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates
that the preferences of different LMs overlap, enabling AAR trained with a
single source LM to serve as a generic plug-in for various target LMs. Our code
is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",None,-1
Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents,0.78911,"The optimized certainty equivalent (OCE) is a family of risk measures that
cover important examples such as entropic risk, conditional value-at-risk and
mean-variance models. In this paper, we propose a new episodic risk-sensitive
reinforcement learning formulation based on tabular Markov decision processes
with recursive OCEs. We design an efficient learning algorithm for this problem
based on value iteration and upper confidence bound. We derive an upper bound
on the regret of the proposed algorithm, and also establish a minimax lower
bound. Our bounds show that the regret rate achieved by our proposed algorithm
has optimal dependence on the number of episodes and the number of actions.",None,-1
Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT,0.883215,"In this paper, we aimed to provide a review and tutorial for researchers in
the field of medical imaging using language models to improve their tasks at
hand. We began by providing an overview of the history and concepts of language
models, with a special focus on large language models. We then reviewed the
current literature on how language models are being used to improve medical
imaging, emphasizing different applications such as image captioning, report
generation, report classification, finding extraction, visual question
answering, interpretable diagnosis, and more for various modalities and organs.
The ChatGPT was specially highlighted for researchers to explore more potential
applications. We covered the potential benefits of accurate and efficient
language models for medical imaging analysis, including improving clinical
workflow efficiency, reducing diagnostic errors, and assisting healthcare
professionals in providing timely and accurate diagnoses. Overall, our goal was
to bridge the gap between language models and medical imaging and inspire new
ideas and innovations in this exciting area of research. We hope that this
review paper will serve as a useful resource for researchers in this field and
encourage further exploration of the possibilities of language models in
medical imaging.",None,-1
Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,0.975879,"Manipulated videos often contain subtle inconsistencies between their visual
and audio signals. We propose a video forensics method, based on anomaly
detection, that can identify these inconsistencies, and that can be trained
solely using real, unlabeled data. We train an autoregressive model to generate
sequences of audio-visual features, using feature sets that capture the
temporal synchronization between video frames and sound. At test time, we then
flag videos that the model assigns low probability. Despite being trained
entirely on real videos, our model obtains strong performance on the task of
detecting manipulated speech videos. Project site:
https://cfeng16.github.io/audio-visual-forensics",None,-1
Learning Expressive Prompting With Residuals for Vision Transformers,0.303767,"Prompt learning is an efficient approach to adapt transformers by inserting
learnable set of parameters into the input and intermediate representations of
a pre-trained model. In this work, we present Expressive Prompts with Residuals
(EXPRES) which modifies the prompt learning paradigm specifically for effective
adaptation of vision transformers (ViT). Out method constructs downstream
representations via learnable ``output'' tokens, that are akin to the learned
class tokens of the ViT. Further for better steering of the downstream
representation processed by the frozen transformer, we introduce residual
learnable tokens that are added to the output of various computations. We apply
EXPRES for image classification, few shot learning, and semantic segmentation,
and show our method is capable of achieving state of the art prompt tuning on
3/3 categories of the VTAB benchmark. In addition to strong performance, we
observe that our approach is an order of magnitude more prompt efficient than
existing visual prompting baselines. We analytically show the computational
benefits of our approach over weight space adaptation techniques like
finetuning. Lastly we systematically corroborate the architectural design of
our method via a series of ablation experiments.",None,-1
Depth-Relative Self Attention for Monocular Depth Estimation,0.512263,"Monocular depth estimation is very challenging because clues to the exact
depth are incomplete in a single RGB image. To overcome the limitation, deep
neural networks rely on various visual hints such as size, shade, and texture
extracted from RGB information. However, we observe that if such hints are
overly exploited, the network can be biased on RGB information without
considering the comprehensive view. We propose a novel depth estimation model
named RElative Depth Transformer (RED-T) that uses relative depth as guidance
in self-attention. Specifically, the model assigns high attention weights to
pixels of close depth and low attention weights to pixels of distant depth. As
a result, the features of similar depth can become more likely to each other
and thus less prone to misused visual hints. We show that the proposed model
achieves competitive results in monocular depth estimation benchmarks and is
less biased to RGB information. In addition, we propose a novel monocular depth
estimation benchmark that limits the observable depth range during training in
order to evaluate the robustness of the model for unseen depths.",None,-1
Error Detection for Text-to-SQL Semantic Parsing,0.493814,"Despite remarkable progress in text-to-SQL semantic parsing in recent years,
the performance of existing parsers is still far from perfect. Specifically,
modern text-to-SQL parsers based on deep learning are often over-confident,
thus casting doubt on their trustworthiness when deployed for real use. In this
paper, we propose a parser-independent error detection model for text-to-SQL
semantic parsing. Using a language model of code as its bedrock, we enhance our
error detection model with graph neural networks that learn structural features
of both natural language questions and SQL queries. We train our model on
realistic parsing errors collected from a cross-domain setting, which leads to
stronger generalization ability. Experiments with three strong text-to-SQL
parsers featuring different decoding mechanisms show that our approach
outperforms parser-dependent uncertainty metrics. Our model could also
effectively improve the performance and usability of text-to-SQL semantic
parsers regardless of their architectures. (Our implementation is available at
https://github.com/OSU-NLP-Group/Text2SQL-Error-Detection)",None,-1
TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play,0.632699,"Multi-agent football poses an unsolved challenge in AI research. Existing
work has focused on tackling simplified scenarios of the game, or else
leveraging expert demonstrations. In this paper, we develop a multi-agent
system to play the full 11 vs. 11 game mode, without demonstrations. This game
mode contains aspects that present major challenges to modern reinforcement
learning algorithms; multi-agent coordination, long-term planning, and
non-transitivity. To address these challenges, we present TiZero; a
self-evolving, multi-agent system that learns from scratch. TiZero introduces
several innovations, including adaptive curriculum learning, a novel self-play
strategy, and an objective that optimizes the policies of multiple agents
jointly. Experimentally, it outperforms previous systems by a large margin on
the Google Research Football environment, increasing win rates by over 30%. To
demonstrate the generality of TiZero's innovations, they are assessed on
several environments beyond football; Overcooked, Multi-agent
Particle-Environment, Tic-Tac-Toe and Connect-Four.",None,-1
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing,0.387641,"Human motion sensing plays a crucial role in smart systems for
decision-making, user interaction, and personalized services. Extensive
research that has been conducted is predominantly based on cameras, whose
intrusive nature limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose milliFlow, a novel deep learning approach to estimate
scene flow as complementary motion information for mmWave point cloud, serving
as an intermediate level of features and directly benefiting downstream human
motion sensing tasks. Experimental results demonstrate the superior performance
of our method when compared with the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition and human parsing and support human body part
tracking. To foster further research in this area, we will provide our codebase
and dataset for open access.",None,-1
Coarse-to-Fine Multi-Scene Pose Regression with Transformers,0.341869,"Absolute camera pose regressors estimate the position and orientation of a
camera given the captured image alone. Typically, a convolutional backbone with
a multi-layer perceptron (MLP) head is trained using images and pose labels to
embed a single reference scene at a time. Recently, this scheme was extended to
learn multiple scenes by replacing the MLP head with a set of fully connected
layers. In this work, we propose to learn multi-scene absolute camera pose
regression with Transformers, where encoders are used to aggregate activation
maps with self-attention and decoders transform latent features and scenes
encoding into pose predictions. This allows our model to focus on general
features that are informative for localization, while embedding multiple scenes
in parallel. We extend our previous MS-Transformer approach
\cite{shavit2021learning} by introducing a mixed classification-regression
architecture that improves the localization accuracy. Our method is evaluated
on commonly benchmark indoor and outdoor datasets and has been shown to exceed
both multi-scene and state-of-the-art single-scene absolute pose regressors.",None,-1
Digital Twin Applications in Urban Logistics: An Overview,0.650761,"Urban traffic attributed to commercial and industrial transportation is
observed to largely affect living standards in cities due to external effects
pertaining to pollution and congestion. In order to counter this, smart cities
deploy technological tools to achieve sustainability. Such tools include
Digital Twins (DT)s which are virtual replicas of real-life physical systems.
Research suggests that DTs can be very beneficial in how they control a
physical system by constantly optimizing its performance. The concept has been
extensively studied in other technology-driven industries like manufacturing.
However, little work has been done with regards to their application in urban
logistics. In this paper, we seek to provide a framework by which DTs could be
easily adapted to urban logistics networks. To do this, we provide a
characterization of key factors in urban logistics for dynamic decision-making.
We also survey previous research on DT applications in urban logistics as we
found that a holistic overview is lacking. Using this knowledge in combination
with the characterization, we produce a conceptual model that describes the
ontology, learning capabilities and optimization prowess of an urban logistics
digital twin through its quantitative models. We finish off with a discussion
on potential research benefits and limitations based on previous research and
our practical experience.",None,-1
Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems,0.418351,"The paper surveys automated scientific discovery, from equation discovery and
symbolic regression to autonomous discovery systems and agents. It discusses
the individual approaches from a ""big picture"" perspective and in context, but
also discusses open issues and recent topics like the various roles of deep
neural networks in this area, aiding in the discovery of human-interpretable
knowledge. Further, we will present closed-loop scientific discovery systems,
starting with the pioneering work on the Adam system up to current efforts in
fields from material science to astronomy. Finally, we will elaborate on
autonomy from a machine learning perspective, but also in analogy to the
autonomy levels in autonomous driving. The maximal level, level five, is
defined to require no human intervention at all in the production of scientific
knowledge. Achieving this is one step towards solving the Nobel Turing Grand
Challenge to develop AI Scientists: AI systems capable of making Nobel-quality
scientific discoveries highly autonomously at a level comparable, and possibly
superior, to the best human scientists by 2050.",None,-1
Multi-Granularity Prompts for Topic Shift Detection in Dialogue,0.637737,"The goal of dialogue topic shift detection is to identify whether the current
topic in a conversation has changed or needs to change. Previous work focused
on detecting topic shifts using pre-trained models to encode the utterance,
failing to delve into the various levels of topic granularity in the dialogue
and understand dialogue contents. To address the above issues, we take a
prompt-based approach to fully extract topic information from dialogues at
multiple-granularity, i.e., label, turn, and topic. Experimental results on our
annotated Chinese Natural Topic Dialogue dataset CNTD and the publicly
available English TIAGE dataset show that the proposed model outperforms the
baselines. Further experiments show that the information extracted at different
levels of granularity effectively helps the model comprehend the conversation
topics.",None,-1
Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications,0.315703,"In this work, I discuss how Large Language Models can be applied in the legal
domain, circumventing their current drawbacks. Despite their large success and
acceptance, their lack of explainability hinders legal experts to trust in
their output, and this happens rightfully so. However, in this paper, I argue
in favor of a new view, Justifiable Artificial Intelligence, instead of
focusing on Explainable Artificial Intelligence. I discuss in this paper how
gaining evidence for and against a Large Language Model's output may make their
generated texts more trustworthy - or hold them accountable for misinformation.",None,-1
Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora,0.695636,"Grammatical error correction (GEC) is the task of correcting typos, spelling,
punctuation and grammatical issues in text. Approaching the problem as a
sequence-to-sequence task, we compare the use of a common subword unit
vocabulary and byte-level encoding. Initial synthetic training data is created
using an error-generating pipeline, and used for finetuning two subword-level
models and one byte-level model. Models are then finetuned further on
hand-corrected error corpora, including texts written by children, university
students, dyslexic and second-language writers, and evaluated over different
error types and origins. We show that a byte-level model enables higher
correction quality than a subword approach, not only for simple spelling
errors, but also for more complex semantic, stylistic and grammatical issues.
In particular, initial training on synthetic corpora followed by finetuning on
a relatively small parallel corpus of real-world errors helps the byte-level
model correct a wide range of commonly occurring errors. Our experiments are
run for the Icelandic language but should hold for other similar languages,
particularly morphologically rich ones.",None,-1
Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference!,0.16228,"Diffusion models have been successfully adapted to text generation tasks by
mapping the discrete text into the continuous space. However, there exist
nonnegligible gaps between training and inference, owing to the absence of the
forward process during inference. Thus, the model only predicts based on the
previously generated reverse noise rather than the noise computed by the
forward process. Besides, the widely-used downsampling strategy in speeding up
the inference will cause the mismatch of diffusion trajectories between
training and inference. To understand and mitigate the above two types of
training-inference discrepancies, we launch a thorough preliminary study. Based
on our observations, we propose two simple yet effective methods to bridge the
gaps mentioned above, named Distance Penalty and Adaptive Decay Sampling.
Extensive experiments on \textbf{6} generation tasks confirm the superiority of
our methods, which can achieve $100\times \rightarrow 200\times$ speedup with
better performance.",None,-1
Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization,0.548189,"Domain generalization (DG) is proposed to deal with the issue of domain
shift, which occurs when statistical differences exist between source and
target domains. However, most current methods do not account for a common
realistic scenario where the source and target domains have different classes.
To overcome this deficiency, open set domain generalization (OSDG) then emerges
as a more practical setting to recognize unseen classes in unseen domains. An
intuitive approach is to use multiple one-vs-all classifiers to define decision
boundaries for each class and reject the outliers as unknown. However, the
significant class imbalance between positive and negative samples often causes
the boundaries biased towards positive ones, resulting in misclassification for
known samples in the unseen target domain. In this paper, we propose a novel
meta-learning-based framework called dualistic MEta-learning with joint
DomaIn-Class matching (MEDIC), which considers gradient matching towards
inter-domain and inter-class splits simultaneously to find a generalizable
boundary balanced for all tasks. Experimental results demonstrate that MEDIC
not only outperforms previous methods in open set scenarios, but also maintains
competitive close set generalization ability at the same time. Our code is
available at https://github.com/zzwdx/MEDIC.",None,-1
Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance,0.893414,"Robust obstacle avoidance is one of the critical steps for successful
goal-driven indoor navigation tasks.Due to the obstacle missing in the visual
image and the possible missed detection issue, visual image-based obstacle
avoidance techniques still suffer from unsatisfactory robustness. To mitigate
it, in this paper, we propose a novel implicit obstacle map-driven indoor
navigation framework for robust obstacle avoidance, where an implicit obstacle
map is learned based on the historical trial-and-error experience rather than
the visual image. In order to further improve the navigation efficiency, a
non-local target memory aggregation module is designed to leverage a non-local
network to model the intrinsic relationship between the target semantic and the
target orientation clues during the navigation process so as to mine the most
target-correlated object clues for the navigation decision. Extensive
experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent
obstacle avoidance and navigation efficiency of our proposed method. The core
source code is available at https://github.com/xwaiyy123/object-navigation.",None,-1
ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment,0.463272,"We present a comprehensive evaluation of large language models for
multilingual readability assessment. Existing evaluation resources lack domain
and language diversity, limiting the ability for cross-domain and cross-lingual
analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset
with human annotations of 9757 sentences in Arabic, English, French, Hindi, and
Russian, collected from 112 different data sources. This benchmark will
encourage research on developing robust multilingual readability assessment
methods. Using ReadMe++, we benchmark multilingual and monolingual language
models in the supervised, unsupervised, and few-shot prompting settings. The
domain and language diversity in ReadMe++ enable us to test more effective
few-shot prompting, and identify shortcomings in state-of-the-art unsupervised
methods. Our experiments also reveal exciting results of superior domain
generalization and enhanced cross-lingual transfer capabilities by models
trained on ReadMe++. We will make our data publicly available and release a
python package tool for multilingual sentence readability prediction using our
trained models at: https://github.com/tareknaous/readme",None,-1
Cross-lingual Knowledge Transfer and Iterative Pseudo-labeling for Low-Resource Speech Recognition with Transducers,0.0938948,"Voice technology has become ubiquitous recently. However, the accuracy, and
hence experience, in different languages varies significantly, which makes the
technology not equally inclusive. The availability of data for different
languages is one of the key factors affecting accuracy, especially in training
of all-neural end-to-end automatic speech recognition systems.
  Cross-lingual knowledge transfer and iterative pseudo-labeling are two
techniques that have been shown to be successful for improving the accuracy of
ASR systems, in particular for low-resource languages, like Ukrainian.
  Our goal is to train an all-neural Transducer-based ASR system to replace a
DNN-HMM hybrid system with no manually annotated training data. We show that
the Transducer system trained using transcripts produced by the hybrid system
achieves 18% reduction in terms of word error rate. However, using a
combination of cross-lingual knowledge transfer from related languages and
iterative pseudo-labeling, we are able to achieve 35% reduction of the error
rate.",None,-1
Translating SUMO-K to Higher-Order Set Theory,0.289069,"We describe a translation from a fragment of SUMO (SUMO-K) into higher-order
set theory. The translation provides a formal semantics for portions of SUMO
which are beyond first-order and which have previously only had an informal
interpretation. It also for the first time embeds a large common-sense ontology
into a very secure interactive theorem proving system. We further extend our
previous work in finding contradictions in SUMO from first order constructs to
include a portion of SUMO's higher order constructs. Finally, using the
translation, we can create problems that can be proven using higher-order
interactive and automated theorem provers. This is tested in several systems
and can be used to form a corpus of higher-order common-sense reasoning
problems.",None,-1
Neural Authorship Attribution: Stylometric Analysis on Large Language Models,0.438494,"Large language models (LLMs) such as GPT-4, PaLM, and Llama have
significantly propelled the generation of AI-crafted text. With rising concerns
about their potential misuse, there is a pressing need for AI-generated-text
forensics. Neural authorship attribution is a forensic effort, seeking to trace
AI-generated text back to its originating LLM. The LLM landscape can be divided
into two primary categories: proprietary and open-source. In this work, we
delve into these emerging categories of LLMs, focusing on the nuances of neural
authorship attribution. To enrich our understanding, we carry out an empirical
analysis of LLM writing signatures, highlighting the contrasts between
proprietary and open-source models, and scrutinizing variations within each
group. By integrating stylometric features across lexical, syntactic, and
structural aspects of language, we explore their potential to yield
interpretable results and augment pre-trained language model-based classifiers
utilized in neural authorship attribution. Our findings, based on a range of
state-of-the-art LLMs, provide empirical insights into neural authorship
attribution, paving the way for future investigations aimed at mitigating the
threats posed by AI-generated misinformation.",None,-1
Robust Natural Language Understanding with Residual Attention Debiasing,0.216436,"Natural language understanding (NLU) models often suffer from unintended
dataset biases. Among bias mitigation methods, ensemble-based debiasing
methods, especially product-of-experts (PoE), have stood out for their
impressive empirical success. However, previous ensemble-based debiasing
methods typically apply debiasing on top-level logits without directly
addressing biased attention patterns. Attention serves as the main media of
feature interaction and aggregation in PLMs and plays a crucial role in
providing robust prediction. In this paper, we propose REsidual Attention
Debiasing (READ), an end-to-end debiasing method that mitigates unintended
biases from attention. Experiments on three NLU tasks show that READ
significantly improves the performance of BERT-based models on OOD data with
shortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on
FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the
crucial role of unbiased attention in robust NLU models and that READ
effectively mitigates biases in attention. Code is available at
https://github.com/luka-group/READ.",None,-1
Pruning Large Language Models via Accuracy Predictor,0.121618,"Large language models(LLMs) containing tens of billions of parameters (or
even more) have demonstrated impressive capabilities in various NLP tasks.
However, substantial model size poses challenges to training, inference, and
deployment so that it is necessary to compress the model. At present, most
model compression for LLMs requires manual design of pruning features, which
has problems such as complex optimization pipeline and difficulty in retaining
the capabilities of certain parts of the model.Therefore, we propose a novel
pruning approach: firstly, a training set of a certain number of
architecture-accuracy pairs is established, and then a non-neural model is
trained as an accuracy predictor. Using the accuracy predictor to further
optimize the search space and search, the optimal model can be automatically
selected. Experiments show that our proposed approach is effective and
efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB
dropped by 9.48% and 5,76% respectively, and the average accuracy of MMLU
increased by 6.28%.",None,-1
Unsupervised Skin Lesion Segmentation via Structural Entropy Minimization on Multi-Scale Superpixel Graphs,0.866018,"Skin lesion segmentation is a fundamental task in dermoscopic image analysis.
The complex features of pixels in the lesion region impede the lesion
segmentation accuracy, and existing deep learning-based methods often lack
interpretability to this problem. In this work, we propose a novel unsupervised
Skin Lesion sEgmentation framework based on structural entropy and isolation
forest outlier Detection, namely SLED. Specifically, skin lesions are segmented
by minimizing the structural entropy of a superpixel graph constructed from the
dermoscopic image. Then, we characterize the consistency of healthy skin
features and devise a novel multi-scale segmentation mechanism by outlier
detection, which enhances the segmentation accuracy by leveraging the
superpixel features from multiple scales. We conduct experiments on four skin
lesion benchmarks and compare SLED with nine representative unsupervised
segmentation methods. Experimental results demonstrate the superiority of the
proposed framework. Additionally, some case studies are analyzed to demonstrate
the effectiveness of SLED.",None,-1
Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization,0.902196,"Weakly-supervised temporal action localization aims to localize and recognize
actions in untrimmed videos with only video-level category labels during
training. Without instance-level annotations, most existing methods follow the
Segment-based Multiple Instance Learning (S-MIL) framework, where the
predictions of segments are supervised by the labels of videos. However, the
objective for acquiring segment-level scores during training is not consistent
with the target for acquiring proposal-level scores during testing, leading to
suboptimal results. To deal with this problem, we propose a novel
Proposal-based Multiple Instance Learning (P-MIL) framework that directly
classifies the candidate proposals in both the training and testing stages,
which includes three key designs: 1) a surrounding contrastive feature
extraction module to suppress the discriminative short proposals by considering
the surrounding contrastive information, 2) a proposal completeness evaluation
module to inhibit the low-quality proposals with the guidance of the
completeness pseudo labels, and 3) an instance-level rank consistency loss to
achieve robust detection by leveraging the complementarity of RGB and FLOW
modalities. Extensive experimental results on two challenging benchmarks
including THUMOS14 and ActivityNet demonstrate the superior performance of our
method.",None,-1
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs,0.754654,"Large Language Models (LLMs) have demonstrated exceptional proficiency in
language-related tasks, but their deployment poses significant challenges due
to substantial memory and storage requirements. Weight-only quantization has
emerged as a promising solution to address these challenges. Previous research
suggests that fine-tuning through up and down rounding can enhance performance.
In this study, we introduce SignRound, a method that utilizes signed gradient
descent (SignSGD) to optimize rounding values and weight clipping within just
200 steps. SignRound integrates the advantages of Quantization-Aware Training
(QAT) and Post-Training Quantization (PTQ), achieving exceptional results
across 2 to 4 bits while maintaining low tuning costs and avoiding additional
inference overhead. For example, SignRound achieves absolute average accuracy
improvements ranging from 6.91\% to 33.22\% at 2 bits. It also demonstrates
robust generalization to recent models and achieves near-lossless quantization
in most scenarios at 4 bits. The source code is publicly available at
\url{https://github.com/intel/auto-round}.",None,-1
"A Study on Accuracy, Miscalibration, and Popularity Bias in Recommendations",0.205179,"Recent research has suggested different metrics to measure the inconsistency
of recommendation performance, including the accuracy difference between user
groups, miscalibration, and popularity lift. However, a study that relates
miscalibration and popularity lift to recommendation accuracy across different
user groups is still missing. Additionally, it is unclear if particular genres
contribute to the emergence of inconsistency in recommendation performance
across user groups. In this paper, we present an analysis of these three
aspects of five well-known recommendation algorithms for user groups that
differ in their preference for popular content. Additionally, we study how
different genres affect the inconsistency of recommendation performance, and
how this is aligned with the popularity of the genres. Using data from LastFm,
MovieLens, and MyAnimeList, we present two key findings. First, we find that
users with little interest in popular content receive the worst recommendation
accuracy, and that this is aligned with miscalibration and popularity lift.
Second, our experiments show that particular genres contribute to a different
extent to the inconsistency of recommendation performance, especially in terms
of miscalibration in the case of the MyAnimeList dataset.",None,-1
The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages,0.505754,"Gender biases in language generation systems are challenging to mitigate. One
possible source for these biases is gender representation disparities in the
training and evaluation data. Despite recent progress in documenting this
problem and many attempts at mitigating it, we still lack shared methodology
and tooling to report gender representation in large datasets. Such
quantitative reporting will enable further mitigation, e.g., via data
augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware
Polyglot Pipeline), an automatic pipeline to characterize gender representation
in large-scale datasets for 55 languages. The pipeline uses a multilingual
lexicon of gendered person-nouns to quantify the gender representation in text.
We showcase it to report gender representation in WMT training data and
development data for the News task, confirming that current data is skewed
towards masculine representation. Having unbalanced datasets may indirectly
optimize our systems towards outperforming one gender over the others. We
suggest introducing our gender quantification pipeline in current datasets and,
ideally, modifying them toward a balanced representation.",None,-1
Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning,0.251296,"Instruction tuning for large language models (LLMs) has gained attention from
researchers due to its ability to unlock the potential of LLMs in following
instructions. While instruction tuning offers advantages for facilitating the
adaptation of large language models (LLMs) to downstream tasks as a fine-tuning
approach, training models with tens of millions or even billions of parameters
on large amounts of data results in unaffordable computational costs. To
address this, we focus on reducing the data used in LLM instruction tuning to
decrease training costs and improve data efficiency, dubbed as Low Training
Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper
conducts a preliminary exploration into reducing the data used in LLM training
and identifies several observations regarding task specialization for LLM
training, such as the optimization of performance for a specific task, the
number of instruction types required for instruction tuning, and the amount of
data required for task-specific models. The results suggest that task-specific
models can be trained using less than 0.5% of the original dataset, with a 2%
improvement in performance over those trained on full task-related data.",None,-1
Additive manifesto decomposition: A policy domain aware method for understanding party positioning,0.263957,"Automatic extraction of party (dis)similarities from texts such as party
election manifestos or parliamentary speeches plays an increasing role in
computational political science. However, existing approaches are fundamentally
limited to targeting only global party (dis)-similarity: they condense the
relationship between a pair of parties into a single figure, their similarity.
In aggregating over all policy domains (e.g., health or foreign policy), they
do not provide any qualitative insights into which domains parties agree or
disagree on. This paper proposes a workflow for estimating policy domain aware
party similarity that overcomes this limitation. The workflow covers (a)
definition of suitable policy domains; (b) automatic labeling of domains, if no
manual labels are available; (c) computation of domain-level similarities and
aggregation at a global level; (d) extraction of interpretable party positions
on major policy axes via multidimensional scaling. We evaluate our workflow on
manifestos from the German federal elections. We find that our method (a)
yields high correlation when predicting party similarity at a global level and
(b) provides accurate party-specific positions, even with automatically
labelled policy domains.",None,-1
Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks,0.813266,"The vision of the upcoming 6G technologies, characterized by ultra-dense
network, low latency, and fast data rate is to support Pervasive AI (PAI) using
zero-touch solutions enabling self-X (e.g., self-configuration,
self-monitoring, and self-healing) services. However, the research on 6G is
still in its infancy, and only the first steps have been taken to conceptualize
its design, investigate its implementation, and plan for use cases. Toward this
end, academia and industry communities have gradually shifted from theoretical
studies of AI distribution to real-world deployment and standardization. Still,
designing an end-to-end framework that systematizes the AI distribution by
allowing easier access to the service using a third-party application assisted
by a zero-touch service provisioning has not been well explored. In this
context, we introduce a novel platform architecture to deploy a zero-touch
PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart
system. This platform aims to standardize the pervasive AI at all levels of the
architecture and unify the interfaces in order to facilitate the service
deployment across application and infrastructure domains, relieve the users
worries about cost, security, and resource allocation, and at the same time,
respect the 6G stringent performance requirements. As a proof of concept, we
present a Federated Learning-as-a-service use case where we evaluate the
ability of our proposed system to self-optimize and self-adapt to the dynamics
of 6G networks in addition to minimizing the users' perceived costs.",None,-1
Prompt Tuning based Adapter for Vision-Language Model Adaption,0.0630394,"Large pre-trained vision-language (VL) models have shown significant promise
in adapting to various downstream tasks. However, fine-tuning the entire
network is challenging due to the massive number of model parameters. To
address this issue, efficient adaptation methods such as prompt tuning have
been proposed. We explore the idea of prompt tuning with multi-task pre-trained
initialization and find it can significantly improve model performance. Based
on our findings, we introduce a new model, termed Prompt-Adapter, that combines
pre-trained prompt tunning with an efficient adaptation network. Our approach
beat the state-of-the-art methods in few-shot image classification on the
public 11 datasets, especially in settings with limited data instances such as
1 shot, 2 shots, 4 shots, and 8 shots images. Our proposed method demonstrates
the promise of combining prompt tuning and parameter-efficient networks for
efficient vision-language model adaptation. The code is publicly available at:
https://github.com/Jingchensun/prompt_adapter.",None,-1
$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models,0.529348,"Fine-tuning a language model on a new domain is standard practice for domain
adaptation. However, it can be infeasible when it comes to modern large-scale
language models such as GPT-3, which can only be accessed through APIs, making
it difficult to access the internal parameters of the model. In this paper, we
propose $k$NN-Adapter, a method to effectively adapt these black-box large
language models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the
retrieval-augmented language model, and adaptively learns to interpolate the
output of the language model with retrieval results from a datastore consisting
of the target domain data. Our experiments on four different domains
demonstrate that $k$NN-Adapter significantly improves perplexity, and works
particularly well in settings with limited access to LLMs. Additionally, we
show that $k$NN-Adapter is more effective than fine-tuning when the amount of
training data is limited. We also release a dataset to encourage further study.",None,-1
Reimagining Retrieval Augmented Language Models for Answering Queries,0.112884,"We present a reality check on large language models and inspect the promise
of retrieval augmented language models in comparison. Such language models are
semi-parametric, where models integrate model parameters and knowledge from
external data sources to make their predictions, as opposed to the parametric
nature of vanilla large language models. We give initial experimental findings
that semi-parametric architectures can be enhanced with views, a query
analyzer/planner, and provenance to make a significantly more powerful system
for question answering in terms of accuracy and efficiency, and potentially for
other NLP tasks",None,-1
SpaDen : Sparse and Dense Keypoint Estimation for Real-World Chart Understanding,0.112715,"We introduce a novel bottom-up approach for the extraction of chart data. Our
model utilizes images of charts as inputs and learns to detect keypoints (KP),
which are used to reconstruct the components within the plot area. Our novelty
lies in detecting a fusion of continuous and discrete KP as predicted heatmaps.
A combination of sparse and dense per-pixel objectives coupled with a uni-modal
self-attention-based feature-fusion layer is applied to learn KP embeddings.
Further leveraging deep metric learning for unsupervised clustering, allows us
to segment the chart plot area into various objects. By further matching the
chart components to the legend, we are able to obtain the data series names. A
post-processing threshold is applied to the KP embeddings to refine the object
reconstructions and improve accuracy. Our extensive experiments include an
evaluation of different modules for KP estimation and the combination of deep
layer aggregation and corner pooling approaches. The results of our experiments
provide extensive evaluation for the task of real-world chart data extraction.",None,-1
ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation,0.766259,"Recent advancements in dense out-of-distribution (OOD) detection have
primarily focused on scenarios where the training and testing datasets share a
similar domain, with the assumption that no domain shift exists between them.
However, in real-world situations, domain shift often exits and significantly
affects the accuracy of existing out-of-distribution (OOD) detection models. In
this work, we propose a dual-level OOD detection framework to handle domain
shift and semantic shift jointly. The first level distinguishes whether domain
shift exists in the image by leveraging global low-level features, while the
second level identifies pixels with semantic shift by utilizing dense
high-level feature maps. In this way, we can selectively adapt the model to
unseen domains as well as enhance model's capacity in detecting novel classes.
We validate the efficacy of our proposed method on several OOD segmentation
benchmarks, including those with significant domain shifts and those without,
observing consistent performance improvements across various baseline models.
Code is available at
${\href{https://github.com/gaozhitong/ATTA}{https://github.com/gaozhitong/ATTA}}$.",None,-1
Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs,0.924968,"Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and
can solve different tasks due to their emergent ability and generalizability.
However, LLMs sometimes lack domain-specific knowledge to perform tasks, which
would also cause hallucination during inference. In some previous works,
additional modules like graph neural networks (GNNs) are trained on retrieved
knowledge from external knowledge bases, aiming to mitigate the problem of
lacking domain-specific knowledge. However, incorporating additional modules:
1) would need retraining additional modules when encountering novel domains; 2)
would become a bottleneck since LLMs' strong abilities are not fully utilized
for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver
(KSL), to teach LLMs to search for essential knowledge from external knowledge
bases by harnessing their own strong generalizability. Specifically, we design
a simple yet effective prompt to transform retrieval into a multi-hop decision
sequence, which empowers LLMs with searching knowledge ability in zero-shot
manner. Additionally, KSL is able to provide complete retrieval paths and
therefore increase explainability of LLMs' reasoning processes. We conduct
experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and
found that our approach improves LLM baseline performance by a relatively large
margin.",None,-1
Learning Efficient Representations for Image-Based Patent Retrieval,0.264303,"Patent retrieval has been attracting tremendous interest from researchers in
intellectual property and information retrieval communities in the past
decades. However, most existing approaches rely on textual and metadata
information of the patent, and content-based image-based patent retrieval is
rarely investigated. Based on traits of patent drawing images, we present a
simple and lightweight model for this task. Without bells and whistles, this
approach significantly outperforms other counterparts on a large-scale
benchmark and noticeably improves the state-of-the-art by 33.5% with the mean
average precision (mAP) score. Further experiments reveal that this model can
be elaborately scaled up to achieve a surprisingly high mAP of 93.5%. Our
method ranks first in the ECCV 2022 Patent Diagram Image Retrieval Challenge.",None,-1
Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics,0.0293992,"This paper presents a novel approach named Persona-Grouping-Intelligence
(PGI), which has been crafted to tackle the challenges posed by GPT models when
applied to real-world business issues. PGI leverages the inherent capabilities
of the GPT model to comprehend intricate language structures and generate
responses that are contextually relevant. The experiment occurred in a business
scenario where human intelligence was being underutilized due to less optimized
business processes. The primary objective of this approach is to leverage GPT
models to reduce the workload on humans in tasks that are extensive,
monotonous, and repetitive. Instead, the focus is redirected toward
decision-making activities. Remarkably, the experiment yielded an accuracy rate
of 93.81% in validating 4,000 responses generated by the model, underscoring
the effectiveness of the PGI strategies. Effectively addressing the issue of
underutilized human intelligence, this paradigm shift aligns business
environments with dynamic machine intelligence, enabling them to navigate the
intricacies of real-world challenges. This approach facilitates the practical
utilization of these models to tackle actual problems. The methodology offers
an opportunity to reshape the fundamental structure of business processes by
seamlessly integrating human decision-making with adaptable machine
intelligence. Consequently, this optimization enhances operational efficiency
and elevates strategic decision-making across diverse business contexts.",None,-1
CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,0.318747,"Chain-of-thought (CoT) prompting enables large language models (LLMs) to
solve complex reasoning tasks by generating an explanation before the final
prediction. Despite it's promising ability, a critical downside of CoT
prompting is that the performance is greatly affected by the factuality of the
generated explanation. To improve the correctness of the explanations,
fine-tuning language models with explanation data is needed. However, there
exists only a few datasets that can be used for such approaches, and no data
collection tool for building them. Thus, we introduce CoTEVer, a tool-kit for
annotating the factual correctness of generated explanations and collecting
revision data of wrong explanations. Furthermore, we suggest several use cases
where the data collected with CoTEVer can be utilized for enhancing the
faithfulness of explanations. Our toolkit is publicly available at
https://github.com/SeungoneKim/CoTEVer.",None,-1
Predicting Hateful Discussions on Reddit using Graph Transformer Networks and Communal Context,0.655788,"We propose a system to predict harmful discussions on social media platforms.
Our solution uses contextual deep language models and proposes the novel idea
of integrating state-of-the-art Graph Transformer Networks to analyze all
conversations that follow an initial post. This framework also supports
adapting to future comments as the conversation unfolds. In addition, we study
whether a community-specific analysis of hate speech leads to more effective
detection of hateful discussions. We evaluate our approach on 333,487 Reddit
discussions from various communities. We find that community-specific modeling
improves performance two-fold and that models which capture wider-discussion
context improve accuracy by 28\% (35\% for the most hateful content) compared
to limited context models.",None,-1
UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM,0.868616,"We present an uncertainty learning framework for dense neural simultaneous
localization and mapping (SLAM). Estimating pixel-wise uncertainties for the
depth input of dense SLAM methods allows re-weighing the tracking and mapping
losses towards image regions that contain more suitable information that is
more reliable for SLAM. To this end, we propose an online framework for sensor
uncertainty estimation that can be trained in a self-supervised manner from
only 2D input data. We further discuss the advantages of the uncertainty
learning for the case of multi-sensor input. Extensive analysis,
experimentation, and ablations show that our proposed modeling paradigm
improves both mapping and tracking accuracy and often performs better than
alternatives that require ground truth depth or 3D. Our experiments show that
we achieve a 38\% and 27\% lower absolute trajectory tracking error (ATE) on
the 7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset
using two types of depth sensors, we report an 11\% F1-score improvement on
RGBD SLAM compared to the recent state-of-the-art neural implicit approaches.
Source code: https://github.com/kev-in-ta/UncLe-SLAM.",None,-1
An Investigation of Noise in Morphological Inflection,0.265067,"With a growing focus on morphological inflection systems for languages where
high-quality data is scarce, training data noise is a serious but so far
largely ignored concern. We aim at closing this gap by investigating the types
of noise encountered within a pipeline for truly unsupervised morphological
paradigm completion and its impact on morphological inflection systems: First,
we propose an error taxonomy and annotation pipeline for inflection training
data. Then, we compare the effect of different types of noise on multiple
state-of-the-art inflection models. Finally, we propose a novel character-level
masked language modeling (CMLM) pretraining objective and explore its impact on
the models' resistance to noise. Our experiments show that various
architectures are impacted differently by separate types of noise, but
encoder-decoders tend to be more robust to noise than models trained with a
copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.",None,-1
Controllable Mind Visual Diffusion Model,0.827786,"Brain signal visualization has emerged as an active research area, serving as
a critical interface between the human visual system and computer vision
models. Although diffusion models have shown promise in analyzing functional
magnetic resonance imaging (fMRI) data, including reconstructing high-quality
images consistent with original visual stimuli, their accuracy in extracting
semantic and silhouette information from brain signals remains limited. In this
regard, we propose a novel approach, referred to as Controllable Mind Visual
Diffusion Model (CMVDM). CMVDM extracts semantic and silhouette information
from fMRI data using attribute alignment and assistant networks. Additionally,
a residual block is incorporated to capture information beyond semantic and
silhouette features. We then leverage a control model to fully exploit the
extracted information for image synthesis, resulting in generated images that
closely resemble the visual stimuli in terms of semantics and silhouette.
Through extensive experimentation, we demonstrate that CMVDM outperforms
existing state-of-the-art methods both qualitatively and quantitatively.",None,-1
NarrativePlay: Interactive Narrative Understanding,0.305824,"In this paper, we introduce NarrativePlay, a novel system that allows users
to role-play a fictional character and interact with other characters in
narratives such as novels in an immersive environment. We leverage Large
Language Models (LLMs) to generate human-like responses, guided by personality
traits extracted from narratives. The system incorporates auto-generated visual
display of narrative settings, character portraits, and character speech,
greatly enhancing user experience. Our approach eschews predefined sandboxes,
focusing instead on main storyline events extracted from narratives from the
perspective of a user-selected character. NarrativePlay has been evaluated on
two types of narratives, detective and adventure stories, where users can
either explore the world or improve their favorability with the narrative
characters through conversations.",None,-1
Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers,0.378468,"This paper introduces Content-aware Token Sharing (CTS), a token reduction
approach that improves the computational efficiency of semantic segmentation
networks that use Vision Transformers (ViTs). Existing works have proposed
token reduction approaches to improve the efficiency of ViT-based image
classification networks, but these methods are not directly applicable to
semantic segmentation, which we address in this work. We observe that, for
semantic segmentation, multiple image patches can share a token if they contain
the same semantic class, as they contain redundant information. Our approach
leverages this by employing an efficient, class-agnostic policy network that
predicts if image patches contain the same semantic class, and lets them share
a token if they do. With experiments, we explore the critical design choices of
CTS and show its effectiveness on the ADE20K, Pascal Context and Cityscapes
datasets, various ViT backbones, and different segmentation decoders. With
Content-aware Token Sharing, we are able to reduce the number of processed
tokens by up to 44%, without diminishing the segmentation quality.",None,-1
Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings,0.772201,"The human brain possesses the extraordinary capability to contextualize the
information it receives from our environment. The entorhinal-hippocampal plays
a critical role in this function, as it is deeply engaged in memory processing
and constructing cognitive maps using place and grid cells. Comprehending and
leveraging this ability could significantly augment the field of artificial
intelligence. The multi-scale successor representation serves as a good model
for the functionality of place and grid cells and has already shown promise in
this role. Here, we introduce a model that employs successor representations
and neural networks, along with word embedding vectors, to construct a
cognitive map of three separate concepts. The network adeptly learns two
different scaled maps and situates new information in proximity to related
pre-existing representations. The dispersion of information across the
cognitive map varies according to its scale - either being heavily
concentrated, resulting in the formation of the three concepts, or spread
evenly throughout the map. We suggest that our model could potentially improve
current AI models by providing multi-modal context information to any input,
based on a similarity metric for the input and pre-existing knowledge
representations.",None,-1
On the Challenges of Fully Incremental Neural Dependency Parsing,0.238285,"Since the popularization of BiLSTMs and Transformer-based bidirectional
encoders, state-of-the-art syntactic parsers have lacked incrementality,
requiring access to the whole sentence and deviating from human language
processing. This paper explores whether fully incremental dependency parsing
with modern architectures can be competitive. We build parsers combining
strictly left-to-right neural encoders with fully incremental sequence-labeling
and transition-based decoders. The results show that fully incremental parsing
with modern architectures considerably lags behind bidirectional parsing,
noting the challenges of psycholinguistically plausible parsing.",None,-1
Scaling Open-Vocabulary Object Detection,0.948342,"Open-vocabulary object detection has benefited greatly from pretrained
vision-language models, but is still limited by the amount of available
detection training data. While detection training data can be expanded by using
Web image-text pairs as weak supervision, this has not been done at scales
comparable to image-level pretraining. Here, we scale up detection data with
self-training, which uses an existing detector to generate pseudo-box
annotations on image-text pairs. Major challenges in scaling self-training are
the choice of label space, pseudo-annotation filtering, and training
efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which
address these challenges. OWLv2 surpasses the performance of previous
state-of-the-art open-vocabulary detectors already at comparable training
scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples,
yielding further large improvement: With an L/14 architecture, OWL-ST improves
AP on LVIS rare classes, for which the model has seen no human box annotations,
from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale
training for open-world localization, similar to what has been seen for image
classification and language modelling.",None,-1
Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains,0.511877,"As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.",None,-1
TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications,0.67874,"We introduce TabRepo, a new dataset of tabular model evaluations and
predictions. TabRepo contains the predictions and metrics of 1310 models
evaluated on 200 classification and regression datasets. We illustrate the
benefit of our dataset in multiple ways. First, we show that it allows to
perform analysis such as comparing Hyperparameter Optimization against current
AutoML systems while also considering ensembling at marginal cost by using
precomputed model predictions. Second, we show that our dataset can be readily
leveraged to perform transfer-learning. In particular, we show that applying
standard transfer-learning techniques allows to outperform current
state-of-the-art tabular systems in accuracy, runtime and latency.",None,-1
Multi-Modality Deep Network for JPEG Artifacts Reduction,0.608941,"In recent years, many convolutional neural network-based models are designed
for JPEG artifacts reduction, and have achieved notable progress. However, few
methods are suitable for extreme low-bitrate image compression artifacts
reduction. The main challenge is that the highly compressed image loses too
much information, resulting in reconstructing high-quality image difficultly.
To address this issue, we propose a multimodal fusion learning method for
text-guided JPEG artifacts reduction, in which the corresponding text
description not only provides the potential prior information of the highly
compressed image, but also serves as supplementary information to assist in
image deblocking. We fuse image features and text semantic features from the
global and local perspectives respectively, and design a contrastive loss built
upon contrastive learning to produce visually pleasing results. Extensive
experiments, including a user study, prove that our method can obtain better
deblocking results compared to the state-of-the-art methods.",None,-1
Distinguish Sense from Nonsense: Out-of-Scope Detection for Virtual Assistants,0.137382,"Out of Scope (OOS) detection in Conversational AI solutions enables a chatbot
to handle a conversation gracefully when it is unable to make sense of the
end-user query. Accurately tagging a query as out-of-domain is particularly
hard in scenarios when the chatbot is not equipped to handle a topic which has
semantic overlap with an existing topic it is trained on. We propose a simple
yet effective OOS detection method that outperforms standard OOS detection
methods in a real-world deployment of virtual assistants. We discuss the
various design and deployment considerations for a cloud platform solution to
train virtual assistants and deploy them at scale. Additionally, we propose a
collection of datasets that replicates real-world scenarios and show
comprehensive results in various settings using both offline and online
evaluation metrics.",None,-1
Segment Anything Meets Semantic Communication,0.738594,"In light of the diminishing returns of traditional methods for enhancing
transmission rates, the domain of semantic communication presents promising new
frontiers. Focusing on image transmission, this paper explores the application
of foundation models, particularly the Segment Anything Model (SAM) developed
by Meta AI Research, to improve semantic communication. SAM is a promptable
image segmentation model that has gained attention for its ability to perform
zero-shot segmentation tasks without explicit training or domain-specific
knowledge. By employing SAM's segmentation capability and lightweight neural
network architecture for semantic coding, we propose a practical approach to
semantic communication. We demonstrate that this approach retains critical
semantic features, achieving higher image reconstruction quality and reducing
communication overhead. This practical solution eliminates the
resource-intensive stage of training a segmentation model and can be applied to
any semantic coding architecture, paving the way for real-world applications.",None,-1
Collective Human Opinions in Semantic Textual Similarity,0.248216,"Despite the subjective nature of semantic textual similarity (STS) and
pervasive disagreements in STS annotation, existing benchmarks have used
averaged human ratings as the gold standard. Averaging masks the true
distribution of human opinions on examples of low agreement, and prevents
models from capturing the semantic vagueness that the individual ratings
represent. In this work, we introduce USTS, the first Uncertainty-aware STS
dataset with ~15,000 Chinese sentence pairs and 150,000 labels, to study
collective human opinions in STS. Analysis reveals that neither a scalar nor a
single Gaussian fits a set of observed judgements adequately. We further show
that current STS models cannot capture the variance caused by human
disagreement on individual instances, but rather reflect the predictive
confidence over the aggregate dataset.",None,-1
SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,0.868656,"We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent's action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.",None,-1
SortedAP: Rethinking evaluation metrics for instance segmentation,0.195995,"Designing metrics for evaluating instance segmentation revolves around
comprehensively considering object detection and segmentation accuracy.
However, other important properties, such as sensitivity, continuity, and
equality, are overlooked in the current study. In this paper, we reveal that
most existing metrics have a limited resolution of segmentation quality. They
are only conditionally sensitive to the change of masks or false predictions.
For certain metrics, the score can change drastically in a narrow range which
could provide a misleading indication of the quality gap between results.
Therefore, we propose a new metric called sortedAP, which strictly decreases
with both object- and pixel-level imperfections and has an uninterrupted
penalization scale over the entire domain. We provide the evaluation toolkit
and experiment code at https://www.github.com/looooongChen/sortedAP.",None,-1
Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths,0.370449,"The objective of Entity Alignment (EA) is to identify equivalent entity pairs
from multiple Knowledge Graphs (KGs) and create a more comprehensive and
unified KG. The majority of EA methods have primarily focused on the structural
modality of KGs, lacking exploration of multi-modal information. A few
multi-modal EA methods have made good attempts in this field. Still, they have
two shortcomings: (1) inconsistent and inefficient modality modeling that
designs complex and distinct models for each modality; (2) ineffective modality
fusion due to the heterogeneous nature of modalities in EA. To tackle these
challenges, we propose PathFusion, consisting of two main components: (1) MSP,
a unified modeling approach that simplifies the alignment process by
constructing paths connecting entities and modality nodes to represent multiple
modalities; (2) IRF, an iterative fusion method that effectively combines
information from different modalities using the path as an information carrier.
Experimental results on real-world datasets demonstrate the superiority of
PathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement
on Hits@1, and 0.194-0.245 absolute improvement on MRR.",None,-1
Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments,0.0832761,"Trustworthiness of artificially intelligent agents is vital for the
acceptance of human-machine teaming in industrial manufacturing environments.
Predictable behaviours and explainable (and understandable) rationale allow
humans collaborating with (and building) these agents to understand their
motivations and therefore validate decisions that are made. To that aim, we
make use of G\""ardenfors's cognitively inspired Conceptual Space framework to
represent the agent's knowledge using concepts as convex regions in a space
spanned by inherently comprehensible quality dimensions. A simple typicality
quantification model is built on top of it to determine fuzzy category
membership and classify instances interpretably. We apply it on a use case from
the manufacturing domain, using objects' physical properties obtained from
cobots' onboard sensors and utilisation properties from crowdsourced
commonsense knowledge available at public knowledge bases. Such flexible
knowledge representation based on property decomposition allows for
data-efficient representation learning of typically highly specialist or
specific manufacturing artefacts. In such a setting, traditional data-driven
(e.g., computer vision-based) classification approaches would struggle due to
training data scarcity. This allows for comprehensibility of an AI agent's
acquired knowledge by the human collaborator thus contributing to
trustworthiness. We situate our approach within an existing explainability
framework specifying explanation desiderata. We provide arguments for our
system's applicability and appropriateness for different roles of human agents
collaborating with the AI system throughout its design, validation, and
operation.",None,-1
Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,0.559234,"Diffusion-based generative models have recently emerged as powerful solutions
for high-quality synthesis in multiple domains. Leveraging the bidirectional
Markov chains, diffusion probabilistic models generate samples by inferring the
reversed Markov chain based on the learned distribution mapping at the forward
diffusion process. In this work, we propose Modiff, a conditional paradigm that
benefits from the denoising diffusion probabilistic model (DDPM) to tackle the
problem of realistic and diverse action-conditioned 3D skeleton-based motion
generation. We are a pioneering attempt that uses DDPM to synthesize a variable
number of motion sequences conditioned on a categorical action. We evaluate our
approach on the large-scale NTU RGB+D dataset and show improvements over
state-of-the-art motion generation methods.",None,-1
Autoregressive Diffusion Model for Graph Generation,0.993139,"Diffusion-based graph generative models have recently obtained promising
results for graph generation. However, existing diffusion-based graph
generative models are mostly one-shot generative models that apply Gaussian
diffusion in the dequantized adjacency matrix space. Such a strategy can suffer
from difficulty in model training, slow sampling speed, and incapability of
incorporating constraints. We propose an \emph{autoregressive diffusion} model
for graph generation. Unlike existing methods, we define a node-absorbing
diffusion process that operates directly in the discrete graph space. For
forward diffusion, we design a \emph{diffusion ordering network}, which learns
a data-dependent node absorbing ordering from graph topology. For reverse
generation, we design a \emph{denoising network} that uses the reverse node
ordering to efficiently reconstruct the graph by predicting the node type of
the new node and its edges with previously denoised nodes at a time. Based on
the permutation invariance of graph, we show that the two networks can be
jointly trained by optimizing a simple lower bound of data likelihood. Our
experiments on six diverse generic graph datasets and two molecule datasets
show that our model achieves better or comparable generation performance with
previous state-of-the-art, and meanwhile enjoys fast generation speed.",None,-1
Understanding In-Context Learning from Repetitions,0.0976668,"This paper explores the elusive mechanism underpinning in-context learning in
Large Language Models (LLMs). Our work provides a novel perspective by
examining in-context learning via the lens of surface repetitions. We
quantitatively investigate the role of surface features in text generation, and
empirically establish the existence of \emph{token co-occurrence
reinforcement}, a principle that strengthens the relationship between two
tokens based on their contextual co-occurrences. By investigating the dual
impacts of these features, our research illuminates the internal workings of
in-context learning and expounds on the reasons for its failures. This paper
provides an essential contribution to the understanding of in-context learning
and its potential limitations, providing a fresh perspective on this exciting
capability.",None,-1
PACO: Parts and Attributes of Common Objects,0.90905,"Object models are gradually progressing from predicting just category labels
to providing detailed descriptions of object instances. This motivates the need
for large datasets which go beyond traditional object masks and provide richer
annotations such as part masks and attributes. Hence, we introduce PACO: Parts
and Attributes of Common Objects. It spans 75 object categories, 456
object-part categories and 55 attributes across image (LVIS) and video (Ego4D)
datasets. We provide 641K part masks annotated across 260K object boxes, with
roughly half of them exhaustively annotated with attributes as well. We design
evaluation metrics and provide benchmark results for three tasks on the
dataset: part mask segmentation, object and part attribute prediction and
zero-shot instance detection. Dataset, models, and code are open-sourced at
https://github.com/facebookresearch/paco.",None,-1
Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers,0.756061,"Fine-grained classification is a challenging task that involves identifying
subtle differences between objects within the same category. This task is
particularly challenging in scenarios where data is scarce. Visual transformers
(ViT) have recently emerged as a powerful tool for image classification, due to
their ability to learn highly expressive representations of visual data using
self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine
tuned using semi-supervised learning techniques, suitable for situations where
we have lack of annotated data. This is particularly common in e-commerce,
where images are readily available but labels are noisy, nonexistent, or
expensive to obtain. Our results demonstrate that Semi-ViT outperforms
traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned
with limited annotated data. These findings indicate that Semi-ViTs hold
significant promise for applications that require precise and fine-grained
classification of visual data.",None,-1
From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for Conversational Exploratory Search,0.569733,"Exploratory search is an open-ended information retrieval process that aims
at discovering knowledge about a topic or domain rather than searching for a
specific answer or piece of information. Conversational interfaces are
particularly suitable for supporting exploratory search, allowing users to
refine queries and examine search results through interactive dialogues. In
addition to conversational search interfaces, knowledge graphs are also useful
in supporting information exploration due to their rich semantic representation
of data items. In this study, we demonstrate the synergistic effects of
combining knowledge graphs and conversational interfaces for exploratory
search, bridging the gap between structured and unstructured information
retrieval. To this end, we propose a knowledge-driven dialogue system for
exploring news articles by asking natural language questions and using the
graph structure to navigate between related topics. Based on a user study with
54 participants, we empirically evaluate the effectiveness of the graph-based
exploratory search and discuss design implications for developing such systems.",None,-1
CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts,0.520377,"The powerful ability of ChatGPT has caused widespread concern in the academic
community. Malicious users could synthesize dummy academic content through
ChatGPT, which is extremely harmful to academic rigor and originality. The need
to develop ChatGPT-written content detection algorithms call for large-scale
datasets. In this paper, we initially investigate the possible negative impact
of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract
dataset (CHEAT) to support the development of detection algorithms. In
particular, the ChatGPT-written abstract dataset contains 35,304 synthetic
abstracts, with Generation, Polish, and Mix as prominent representatives. Based
on these data, we perform a thorough analysis of the existing text synthesis
detection algorithms. We show that ChatGPT-written abstracts are detectable,
while the detection difficulty increases with human involvement.Our dataset is
available in https://github.com/botianzhe/CHEAT.",None,-1
Surface Geometry Processing: An Efficient Normal-based Detail Representation,0.109906,"With the rapid development of high-resolution 3D vision applications, the
traditional way of manipulating surface detail requires considerable memory and
computing time. To address these problems, we introduce an efficient surface
detail processing framework in 2D normal domain, which extracts new normal
feature representations as the carrier of micro geometry structures that are
illustrated both theoretically and empirically in this article. Compared with
the existing state of the arts, we verify and demonstrate that the proposed
normal-based representation has three important properties, including detail
separability, detail transferability and detail idempotence. Finally, three new
schemes are further designed for geometric surface detail processing
applications, including geometric texture synthesis, geometry detail transfer,
and 3D surface super-resolution. Theoretical analysis and experimental results
on the latest benchmark dataset verify the effectiveness and versatility of our
normal-based representation, which accepts 30 times of the input surface
vertices but at the same time only takes 6.5% memory cost and 14.0% running
time in comparison with existing competing algorithms.",None,-1
LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following,0.661069,"E-commerce authoring entails creating engaging, diverse, and targeted content
to enhance preference elicitation and retrieval experience. While Large
Language Models (LLMs) have revolutionized content generation, they often fall
short in e-commerce applications due to their limited memorization of
domain-specific features. This paper proposes LLaMA-E, the unified e-commerce
authoring models that address the contextual preferences of customers, sellers,
and platforms, the essential objects in e-commerce operation. We design the
instruction set derived from tasks of ads generation, query-enhanced product
title rewriting, product classification, purchase intent speculation, and
general e-commerce Q&A. The instruction formulation ensures the interleaved
cover of the presented and required object features, allowing the alignment of
base models to parameterise e-commerce knowledge comprehensively. The proposed
LLaMA-E models achieve state-of-the-art evaluation performance and exhibit the
advantage in zero-shot practical applications. To our knowledge, this is the
first LLM tailored to empower authoring applications with comprehensive
scenario understanding by integrating features focused on participated objects.",None,-1
Explicit Syntactic Guidance for Neural Text Generation,0.116985,"Most existing text generation models follow the sequence-to-sequence
paradigm. Generative Grammar suggests that humans generate natural language
texts by learning language grammar. We propose a syntax-guided generation
schema, which generates the sequence guided by a constituency parse tree in a
top-down direction. The decoding process can be decomposed into two parts: (1)
predicting the infilling texts for each constituent in the lexicalized syntax
context given the source sentence; (2) mapping and expanding each constituent
to construct the next-level syntax context. Accordingly, we propose a
structural beam search method to find possible syntax structures
hierarchically. Experiments on paraphrase generation and machine translation
show that the proposed method outperforms autoregressive baselines, while also
demonstrating effectiveness in terms of interpretability, controllability, and
diversity.",None,-1
ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers,0.21372,"Lack of audio-video synchronization is a common problem during television
broadcasts and video conferencing, leading to an unsatisfactory viewing
experience. A widely accepted paradigm is to create an error detection
mechanism that identifies the cases when audio is leading or lagging. We
propose ModEFormer, which independently extracts audio and video embeddings
using modality-specific transformers. Different from the other
transformer-based approaches, ModEFormer preserves the modality of the input
streams which allows us to use a larger batch size with more negative audio
samples for contrastive learning. Further, we propose a trade-off between the
number of negative samples and number of unique samples in a batch to
significantly exceed the performance of previous methods. Experimental results
show that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and
90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset
detection for test clips.",None,-1
MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset,0.72586,"Sentence Boundary Detection (SBD) is one of the foundational building blocks
of Natural Language Processing (NLP), with incorrectly split sentences heavily
influencing the output quality of downstream tasks. It is a challenging task
for algorithms, especially in the legal domain, considering the complex and
different sentence structures used. In this work, we curated a diverse
multilingual legal dataset consisting of over 130'000 annotated sentences in 6
languages. Our experimental results indicate that the performance of existing
SBD models is subpar on multilingual legal data. We trained and tested
monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers,
demonstrating state-of-the-art performance. We also show that our multilingual
models outperform all baselines in the zero-shot setting on a Portuguese test
set. To encourage further research and development by the community, we have
made our dataset, models, and code publicly available.",None,-1
Inductive reasoning in humans and large language models,0.34523,"The impressive recent performance of large language models has led many to
wonder to what extent they can serve as models of general intelligence or are
similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4
to a classic problem in human inductive reasoning known as property induction.
Over two experiments, we elicit human judgments on a range of property
induction tasks spanning multiple domains. Although GPT-3.5 struggles to
capture many aspects of human behaviour, GPT-4 is much more successful: for the
most part, its performance qualitatively matches that of humans, and the only
notable exception is its failure to capture the phenomenon of premise
non-monotonicity. Our work demonstrates that property induction allows for
interesting comparisons between human and machine intelligence and provides two
large datasets that can serve as benchmarks for future work in this vein.",None,-1
Using Large Language Models to Support Thematic Analysis in Empirical Legal Studies,0.431334,"Thematic analysis and other variants of inductive coding are widely used
qualitative analytic methods within empirical legal studies (ELS). We propose a
novel framework facilitating effective collaboration of a legal expert with a
large language model (LLM) for generating initial codes (phase 2 of thematic
analysis), searching for themes (phase 3), and classifying the data in terms of
the themes (to kick-start phase 4). We employed the framework for an analysis
of a dataset (n=785) of facts descriptions from criminal court opinions
regarding thefts. The goal of the analysis was to discover classes of typical
thefts. Our results show that the LLM, namely OpenAI's GPT-4, generated
reasonable initial codes, and it was capable of improving the quality of the
codes based on expert feedback. They also suggest that the model performed well
in zero-shot classification of facts descriptions in terms of the themes.
Finally, the themes autonomously discovered by the LLM appear to map fairly
well to the themes arrived at by legal experts. These findings can be leveraged
by legal researchers to guide their decisions in integrating LLMs into their
thematic analyses, as well as other inductive coding projects.",None,-1
Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,0.0850383,"Detecting the anomaly of human behavior is paramount to timely recognizing
endangering situations, such as street fights or elderly falls. However,
anomaly detection is complex since anomalous events are rare and because it is
an open set recognition task, i.e., what is anomalous at inference has not been
observed at training. We propose COSKAD, a novel model that encodes skeletal
human motion by a graph convolutional network and learns to COntract SKeletal
kinematic embeddings onto a latent hypersphere of minimum volume for Video
Anomaly Detection. We propose three latent spaces: the commonly-adopted
Euclidean and the novel spherical and hyperbolic. All variants outperform the
state-of-the-art on the most recent UBnormal dataset, for which we contribute a
human-related version with annotated skeletons. COSKAD sets a new
state-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK
Avenue, with performance comparable to video-based methods. Source code and
dataset will be released upon acceptance.",None,-1
Measuring Progress in Fine-grained Vision-and-Language Understanding,0.686153,"While pretraining on large-scale image-text data from the Web has facilitated
rapid progress on many vision-and-language (V&L) tasks, recent work has
demonstrated that pretrained models lack ""fine-grained"" understanding, such as
the ability to recognise relationships, verbs, and numbers in images. This has
resulted in an increased interest in the community to either develop new
benchmarks or models for such capabilities. To better understand and quantify
progress in this direction, we investigate four competitive V&L models on four
fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al.,
2022) consistently outperforms other baselines, and that modelling innovations
can impact performance more than scaling Web data, which even degrades
performance sometimes. Through a deeper investigation of X-VLM, we highlight
the importance of both novel losses and rich data sources for learning
fine-grained skills. Finally, we inspect training dynamics, and discover that
for some tasks, performance peaks early in training or significantly
fluctuates, never converging.",None,-1
Guiding AI-Generated Digital Content with Wireless Perception,0.266636,"Recent advances in artificial intelligence (AI), coupled with a surge in
training data, have led to the widespread use of AI for digital content
generation, with ChatGPT serving as a representative example. Despite the
increased efficiency and diversity, the inherent instability of AI models poses
a persistent challenge in guiding these models to produce the desired content
for users. In this paper, we introduce an integration of wireless perception
(WP) with AI-generated content (AIGC) and propose a unified WP-AIGC framework
to improve the quality of digital content production. The framework employs a
novel multi-scale perception technology to read user's posture, which is
difficult to describe accurately in words, and transmits it to the AIGC model
as skeleton images. Based on these images and user's service requirements, the
AIGC model generates corresponding digital content. Since the production
process imposes the user's posture as a constraint on the AIGC model, it makes
the generated content more aligned with the user's requirements. Additionally,
WP-AIGC can also accept user's feedback, allowing adjustment of computing
resources at edge server to improve service quality. Experiments results verify
the effectiveness of the WP-AIGC framework, highlighting its potential as a
novel approach for guiding AI models in the accurate generation of digital
content.",None,-1
Pulling Target to Source: A New Perspective on Domain Adaptive Semantic Segmentation,0.421473,"Domain adaptive semantic segmentation aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. However, existing methods
primarily focus on directly learning qualified target features, making it
challenging to guarantee their discrimination in the absence of target labels.
This work provides a new perspective. We observe that the features learned with
source data manage to keep categorically discriminative during training,
thereby enabling us to implicitly learn adequate target representations by
simply \textbf{pulling target features close to source features for each
category}. To this end, we propose T2S-DA, which we interpret as a form of
pulling Target to Source for Domain Adaptation, encouraging the model in
learning similar cross-domain features. Also, considering the pixel categories
are heavily imbalanced for segmentation datasets, we come up with a dynamic
re-weighting strategy to help the model concentrate on those underperforming
classes. Extensive experiments confirm that T2S-DA learns a more discriminative
and generalizable representation, significantly surpassing the
state-of-the-art. We further show that our method is quite qualified for the
domain generalization task, verifying its domain-invariant property.",None,-1
A GOA-Based Fault-Tolerant Trajectory Tracking Control for an Underwater Vehicle of Multi-Thruster System without Actuator Saturation,0.698725,"This paper proposes an intelligent fault-tolerant control (FTC) strategy to
tackle the trajectory tracking problem of an underwater vehicle (UV) under
thruster damage (power loss) cases and meanwhile resolve the actuator
saturation brought by the vehicle's physical constraints. In the proposed
control strategy, the trajectory tracking component is formed by a refined
backstepping algorithm that controls the velocity variation and a sliding mode
control deducts the torque/force outputs; the fault-tolerant component is
established based on a Grasshopper Optimization Algorithm (GOA), which provides
fast convergence speed as well as satisfactory accuracy of deducting optimized
reallocation of the thruster forces to compensate for the power loss in
different fault cases. Simulations with or without environmental perturbations
under different fault cases and comparisons to other traditional FTCs are
presented, thus verifying the effectiveness and robustness of the proposed
GOA-based fault-tolerant trajectory tracking design.",None,-1
An Overview on Language Models: Recent Developments and Outlook,0.452605,"Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner, while pre-trained
language models (PLMs) cover broader concepts and can be used in both causal
sequential modeling and fine-tuning for downstream applications. PLMs have
their own training paradigms (usually self-supervised) and serve as foundation
models in modern NLP systems. This overview paper provides an introduction to
both CLMs and PLMs from five aspects, i.e., linguistic units, architectures,
training methods, evaluation methods, and applications. Furthermore, we discuss
the relationship between CLMs and PLMs and shed light on the future directions
of language modeling in the pre-trained era.",None,-1
Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization,0.221994,"Existing vector quantization (VQ) based autoregressive models follow a
two-stage generation paradigm that first learns a codebook to encode images as
discrete codes, and then completes generation based on the learned codebook.
However, they encode fixed-size image regions into fixed-length codes and
ignore their naturally different information densities, which results in
insufficiency in important regions and redundancy in unimportant ones, and
finally degrades the generation quality and speed. Moreover, the fixed-length
coding leads to an unnatural raster-scan autoregressive generation. To address
the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization
VAE (DQ-VAE) which encodes image regions into variable-length codes based on
their information densities for an accurate and compact code representation.
(2) DQ-Transformer which thereby generates images autoregressively from
coarse-grained (smooth regions with fewer codes) to fine-grained (details
regions with more codes) by modeling the position and content of codes in each
granularity alternately, through a novel stacked-transformer architecture and
shared-content, non-shared position input layers designs. Comprehensive
experiments on various generation tasks validate our superiorities in both
effectiveness and efficiency. Code will be released at
https://github.com/CrossmodalGroup/DynamicVectorQuantization.",None,-1
Synthesizing a Progression of Subtasks for Block-Based Visual Programming Tasks,0.40238,"Block-based visual programming environments play an increasingly important
role in introducing computing concepts to K-12 students. In recent years, they
have also gained popularity in neuro-symbolic AI, serving as a benchmark to
evaluate general problem-solving and logical reasoning skills. The open-ended
and conceptual nature of these visual programming tasks make them challenging,
both for state-of-the-art AI agents as well as for novice programmers. A
natural approach to providing assistance for problem-solving is breaking down a
complex task into a progression of simpler subtasks; however, this is not
trivial given that the solution codes are typically nested and have non-linear
execution behavior. In this paper, we formalize the problem of synthesizing
such a progression for a given reference block-based visual programming task.
We propose a novel synthesis algorithm that generates a progression of subtasks
that are high-quality, well-spaced in terms of their complexity, and solving
this progression leads to solving the reference task. We show the utility of
our synthesis algorithm in improving the efficacy of AI agents (in this case,
neural program synthesizers) for solving tasks in the Karel programming
environment. Then, we conduct a user study to demonstrate that our synthesized
progression of subtasks can assist a novice programmer in solving tasks in the
Hour of Code: Maze Challenge by Code-dot-org.",None,-1
How to Design Translation Prompts for ChatGPT: An Empirical Study,0.886096,"The recently released ChatGPT has demonstrated surprising abilities in
natural language understanding and natural language generation. Machine
translation relies heavily on the abilities of language understanding and
generation. Thus, in this paper, we explore how to assist machine translation
with ChatGPT. We adopt several translation prompts on a wide range of
translations. Our experimental results show that ChatGPT with designed
translation prompts can achieve comparable or better performance over
commercial translation systems for high-resource language translations. We
further evaluate the translation quality using multiple references, and ChatGPT
achieves superior performance compared to commercial systems. We also conduct
experiments on domain-specific translations, the final results show that
ChatGPT is able to comprehend the provided domain keyword and adjust
accordingly to output proper translations. At last, we perform few-shot prompts
that show consistent improvement across different base prompts. Our work
provides empirical evidence that ChatGPT still has great potential in
translations.",None,-1
Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring,0.284738,"Speech fluency/disfluency can be evaluated by analyzing a range of phonetic
and prosodic features. Deep neural networks are commonly trained to map
fluency-related features into the human scores. However, the effectiveness of
deep learning-based models is constrained by the limited amount of labeled
training samples. To address this, we introduce a self-supervised learning
(SSL) approach that takes into account phonetic and prosody awareness for
fluency scoring. Specifically, we first pre-train the model using a
reconstruction loss function, by masking phones and their durations jointly on
a large amount of unlabeled speech and text prompts. We then fine-tune the
pre-trained model using human-annotated scoring data. Our experimental results,
conducted on datasets such as Speechocean762 and our non-native datasets, show
that our proposed method outperforms the baseline systems in terms of Pearson
correlation coefficients (PCC). Moreover, we also conduct an ablation study to
better understand the contribution of phonetic and prosody factors during the
pre-training stage.",None,-1
LLMSTEP: LLM proofstep suggestions in Lean,0.996014,"We present LLMSTEP, a tool for integrating a language model into the Lean
proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to
a server hosting a language model. The language model generates suggestions,
which are checked in Lean and displayed to a user in their development
environment. We provide a baseline language model, along with code for
fine-tuning and evaluation to support further development. We provide server
implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a
step towards fast, effective language model suggestions for any user.",None,-1
Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features,0.704495,"A challenge towards developing NLP systems for the world's languages is
understanding how they generalize to typological differences relevant for
real-world applications. To this end, we propose M2C, a morphologically-aware
framework for behavioral testing of NLP models. We use M2C to generate tests
that probe models' behavior in light of specific linguistic features in 12
typologically diverse languages. We evaluate state-of-the-art language models
on the generated tests. While models excel at most tests in English, we
highlight generalization failures to specific typological characteristics such
as temporal expressions in Swahili and compounding possessives in Finish. Our
findings motivate the development of models that address these blind spots.",None,-1
Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning,0.240391,"Due to their similarity-based learning objectives, pretrained sentence
encoders often internalize stereotypical assumptions that reflect the social
biases that exist within their training corpora. In this paper, we describe
several kinds of stereotypes concerning different communities that are present
in popular sentence representation models, including pretrained next sentence
prediction and contrastive sentence representation models. We compare such
models to textual entailment models that learn language logic for a variety of
downstream language understanding tasks. By comparing strong pretrained models
based on text similarity with textual entailment learning, we conclude that the
explicit logic learning with textual entailment can significantly reduce bias
and improve the recognition of social communities, without an explicit
de-biasing process",None,-1
Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification,0.168171,"Recent advances in weakly supervised text classification mostly focus on
designing sophisticated methods to turn high-level human heuristics into
quality pseudo-labels. In this paper, we revisit the seed matching-based
method, which is arguably the simplest way to generate pseudo-labels, and show
that its power was greatly underestimated. We show that the limited performance
of seed matching is largely due to the label bias injected by the simple
seed-match rule, which prevents the classifier from learning reliable
confidence for selecting high-quality pseudo-labels. Interestingly, simply
deleting the seed words present in the matched input texts can mitigate the
label bias and help learn better confidence. Subsequently, the performance
achieved by seed matching can be improved significantly, making it on par with
or even better than the state-of-the-art. Furthermore, to handle the case when
the seed words are not made known, we propose to simply delete the word tokens
in the input text randomly with a high deletion ratio. Remarkably, seed
matching equipped with this random deletion method can often achieve even
better performance than that with seed deletion.",None,-1
Super-Resolution of License Plate Images Using Attention Modules and Sub-Pixel Convolution Layers,0.487694,"Recent years have seen significant developments in the field of License Plate
Recognition (LPR) through the integration of deep learning techniques and the
increasing availability of training data. Nevertheless, reconstructing license
plates (LPs) from low-resolution (LR) surveillance footage remains challenging.
To address this issue, we introduce a Single-Image Super-Resolution (SISR)
approach that integrates attention and transformer modules to enhance the
detection of structural and textural features in LR images. Our approach
incorporates sub-pixel convolution layers (also known as PixelShuffle) and a
loss function that uses an Optical Character Recognition (OCR) model for
feature extraction. We trained the proposed architecture on synthetic images
created by applying heavy Gaussian noise to high-resolution LP images from two
public datasets, followed by bicubic downsampling. As a result, the generated
images have a Structural Similarity Index Measure (SSIM) of less than 0.10. Our
results show that our approach for reconstructing these low-resolution
synthesized images outperforms existing ones in both quantitative and
qualitative measures. Our code is publicly available at
https://github.com/valfride/lpr-rsr-ext/",None,-1
Cloud K-SVD for Image Denoising,0.0303642,"Cloud K-SVD is a dictionary learning algorithm that can train at multiple
nodes and hereby produce a mutual dictionary to represent low-dimensional
geometric structures in image data. We present a novel application of the
algorithm as we use it to recover both noiseless and noisy images from
overlapping patches. We implement a node network in Kubernetes using Docker
containers to facilitate Cloud K-SVD. Results show that Cloud K-SVD can recover
images approximately and remove quantifiable amounts of noise from benchmark
gray-scaled images without sacrificing accuracy in recovery; we achieve an SSIM
index of 0.88, 0.91 and 0.95 between clean and recovered images for noise
levels ($\mu$ = 0, $\sigma^{2}$ = 0.01, 0.005, 0.001), respectively, which is
similar to SOTA in the field. Cloud K-SVD is evidently able to learn a mutual
dictionary across multiple nodes and remove AWGN from images. The mutual
dictionary can be used to recover a specific image at any of the nodes in the
network.",None,-1
ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning,0.638877,"When personalized federated learning (FL) meets large foundation models, new
challenges arise from various limitations in resources. In addition to typical
limitations such as data, computation, and communication costs, access to the
models is also often limited. This paper endeavors to solve both the challenges
of limited resources and personalization. i.e., distribution shifts between
clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order
Optimization for Personalized Federated Learning. ZOOPFL avoids direct
interference with the foundation models and instead learns to adapt its inputs
through zeroth-order optimization. In addition, we employ simple yet effective
linear projections to remap its predictions for personalization. To reduce the
computation costs and enhance personalization, we propose input surgery to
incorporate an auto-encoder with low-dimensional and client-specific
embeddings. We provide theoretical support for ZOOPFL to analyze its
convergence. Extensive empirical experiments on computer vision and natural
language processing tasks using popular foundation models demonstrate its
effectiveness for FL on black-box foundation models.",None,-1
Understanding the Complexity and Its Impact on Testing in ML-Enabled Systems,0.0620546,"Machine learning (ML) enabled systems are emerging with recent breakthroughs
in ML. A model-centric view is widely taken by the literature to focus only on
the analysis of ML models. However, only a small body of work takes a system
view that looks at how ML components work with the system and how they affect
software engineering for MLenabled systems. In this paper, we adopt this system
view, and conduct a case study on Rasa 3.0, an industrial dialogue system that
has been widely adopted by various companies around the world. Our goal is to
characterize the complexity of such a largescale ML-enabled system and to
understand the impact of the complexity on testing. Our study reveals practical
implications for software engineering for ML-enabled systems.",None,-1
Attention-based Spatial-Temporal Graph Convolutional Recurrent Networks for Traffic Forecasting,0.313696,"Traffic forecasting is one of the most fundamental problems in transportation
science and artificial intelligence. The key challenge is to effectively model
complex spatial-temporal dependencies and correlations in modern traffic data.
Existing methods, however, cannot accurately model both long-term and
short-term temporal correlations simultaneously, limiting their expressive
power on complex spatial-temporal patterns. In this paper, we propose a novel
spatial-temporal neural network framework: Attention-based Spatial-Temporal
Graph Convolutional Recurrent Network (ASTGCRN), which consists of a graph
convolutional recurrent module (GCRN) and a global attention module. In
particular, GCRN integrates gated recurrent units and adaptive graph
convolutional networks for dynamically learning graph structures and capturing
spatial dependencies and local temporal relationships. To effectively extract
global temporal dependencies, we design a temporal attention layer and
implement it as three independent modules based on multi-head self-attention,
transformer, and informer respectively. Extensive experiments on five real
traffic datasets have demonstrated the excellent predictive performance of all
our three models with all their average MAE, RMSE and MAPE across the test
datasets lower than the baseline methods.",None,-1
Data Augmentation Alone Can Improve Adversarial Training,0.916317,"Adversarial training suffers from the issue of robust overfitting, which
seriously impairs its generalization performance. Data augmentation, which is
effective at preventing overfitting in standard training, has been observed by
many previous works to be ineffective in mitigating overfitting in adversarial
training. This work proves that, contrary to previous findings, data
augmentation alone can significantly boost accuracy and robustness in
adversarial training. We find that the hardness and the diversity of data
augmentation are important factors in combating robust overfitting. In general,
diversity can improve both accuracy and robustness, while hardness can boost
robustness at the cost of accuracy within a certain limit and degrade them both
over that limit. To mitigate robust overfitting, we first propose a new crop
transformation, Cropshift, which has improved diversity compared to the
conventional one (Padcrop). We then propose a new data augmentation scheme,
based on Cropshift, with much improved diversity and well-balanced hardness.
Empirically, our augmentation method achieves the state-of-the-art accuracy and
robustness for data augmentations in adversarial training. Furthermore, when
combined with weight averaging it matches, or even exceeds, the performance of
the best contemporary regularization methods for alleviating robust
overfitting. Code is available at:
https://github.com/TreeLLi/DA-Alone-Improves-AT.",None,-1
TUTORING: Instruction-Grounded Conversational Agent for Language Learners,0.0620104,"In this paper, we propose Tutoring bot, a generative chatbot trained on a
large scale of tutor-student conversations for English-language learning. To
mimic a human tutor's behavior in language education, the tutor bot leverages
diverse educational instructions and grounds to each instruction as additional
input context for the tutor response generation. As a single instruction
generally involves multiple dialogue turns to give the student sufficient
speaking practice, the tutor bot is required to monitor and capture when the
current instruction should be kept or switched to the next instruction. For
that, the tutor bot is learned to not only generate responses but also infer
its teaching action and progress on the current conversation simultaneously by
a multi-task learning scheme. Our Tutoring bot is deployed under a
non-commercial use license at https://tutoringai.com.",None,-1
Nemo: First Glimpse of a New Rule Engine,0.667434,"This system demonstration presents Nemo, a new logic programming engine with
a focus on reliability and performance. Nemo is built for data-centric analytic
computations, modelled in a fully declarative Datalog dialect. Its scalability
for these tasks matches or exceeds that of leading Datalog systems. We
demonstrate uses in reasoning with knowledge graphs and ontologies with 10^5 to
10^8 input facts, all on a laptop. Nemo is written in Rust and available as a
free and open source tool.",None,-1
GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab,0.406418,"The integration of robots in chemical experiments has enhanced experimental
efficiency, but lacking the human intelligence to comprehend literature, they
seldom provide assistance in experimental design. Therefore, achieving
full-process autonomy from experiment design to validation in self-driven
laboratories (SDL) remains a challenge. The introduction of Generative
Pre-trained Transformers (GPT), particularly GPT-4, into robotic
experimentation offers a solution. We introduce GPT-Lab, a paradigm that
employs GPT models to give robots human-like intelligence. With our robotic
experimentation platform, GPT-Lab mines literature for materials and methods
and validates findings through high-throughput synthesis. As a demonstration,
GPT-Lab analyzed 500 articles, identified 18 potential reagents, and
successfully produced an accurate humidity colorimetric sensor with a root mean
square error (RMSE) of 2.68%. This showcases the rapid materials discovery and
validation potential of our system.",None,-1
Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors,0.66934,"It is vital to infer signed distance functions (SDFs) from 3D point clouds.
The latest methods rely on generalizing the priors learned from large scale
supervision. However, the learned priors do not generalize well to various
geometric variations that are unseen during training, especially for extremely
sparse point clouds. To resolve this issue, we present a neural network to
directly infer SDFs from single sparse point clouds without using signed
distance supervision, learned priors or even normals. Our insight here is to
learn surface parameterization and SDFs inference in an end-to-end manner. To
make up the sparsity, we leverage parameterized surfaces as a coarse surface
sampler to provide many coarse surface estimations in training iterations,
according to which we mine supervision and our thin plate splines (TPS) based
network infers SDFs as smooth functions in a statistical way. Our method
significantly improves the generalization ability and accuracy in unseen point
clouds. Our experimental results show our advantages over the state-of-the-art
methods in surface reconstruction for sparse point clouds under synthetic
datasets and real scans.The code is available at
\url{https://github.com/chenchao15/NeuralTPS}.",None,-1
Conversational Semantic Parsing using Dynamic Context Graphs,0.353777,"In this paper we consider the task of conversational semantic parsing over
general purpose knowledge graphs (KGs) with millions of entities, and thousands
of relation-types. We focus on models which are capable of interactively
mapping user utterances into executable logical forms (e.g., Sparql) in the
context of the conversational history. Our key idea is to represent information
about an utterance and its context via a subgraph which is created dynamically,
i.e., the number of nodes varies per utterance. Rather than treating the
subgraph as a sequence, we exploit its underlying structure and encode it with
a graph neural network which further allows us to represent a large number of
(unseen) nodes. Experimental results show that dynamic context modeling is
superior to static approaches, delivering performance improvements across the
board (i.e., for simple and complex questions). Our results further confirm
that modeling the structure of context is better at processing discourse
information, (i.e., at handling ellipsis and resolving coreference) and longer
interactions.",None,-1
Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications,0.95875,"In this work, we address the NER problem by splitting it into two logical
sub-tasks: (1) Span Detection which simply extracts entity mention spans
irrespective of entity type; (2) Span Classification which classifies the spans
into their entity types. Further, we formulate both sub-tasks as
question-answering (QA) problems and produce two leaner models which can be
optimized separately for each sub-task. Experiments with four cross-domain
datasets demonstrate that this two-step approach is both effective and time
efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17
and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all
cases, it achieves a significant reduction in training time compared to its QA
baseline counterpart. The effectiveness of our system stems from fine-tuning
the BERT model twice, separately for span detection and classification. The
source code can be found at https://github.com/c3sr/split-ner.",None,-1
NeMF: Inverse Volume Rendering with Neural Microflake Field,0.750486,"Recovering the physical attributes of an object's appearance from its images
captured under an unknown illumination is challenging yet essential for
photo-realistic rendering. Recent approaches adopt the emerging implicit scene
representations and have shown impressive results.However, they unanimously
adopt a surface-based representation,and hence can not well handle scenes with
very complex geometry, translucent object and etc. In this paper, we propose to
conduct inverse volume rendering, in contrast to surface-based, by representing
a scene using microflake volume, which assumes the space is filled with
infinite small flakes and light reflects or scatters at each spatial location
according to microflake distributions. We further adopt the coordinate networks
to implicitly encode the microflake volume, and develop a differentiable
microflake volume renderer to train the network in an end-to-end way in
principle.Our NeMF enables effective recovery of appearance attributes for
highly complex geometry and scattering object, enables high-quality relighting,
material editing, and especially simulates volume rendering effects, such as
scattering, which is infeasible for surface-based approaches.",None,-1
How can objects help action recognition?,0.414735,"Current state-of-the-art video models process a video clip as a long sequence
of spatio-temporal tokens. However, they do not explicitly model objects, their
interactions across the video, and instead process all the tokens in the video.
In this paper, we investigate how we can use knowledge of objects to design
better video models, namely to process fewer tokens and to improve recognition
accuracy. This is in contrast to prior works which either drop tokens at the
cost of accuracy, or increase accuracy whilst also increasing the computation
required. First, we propose an object-guided token sampling strategy that
enables us to retain a small fraction of the input tokens with minimal impact
on accuracy. And second, we propose an object-aware attention module that
enriches our feature representation with object information and improves
overall accuracy. Our resulting framework achieves better performance when
using fewer tokens than strong baselines. In particular, we match our baseline
with 30%, 40%, and 60% of the input tokens on SomethingElse,
Something-something v2, and Epic-Kitchens, respectively. When we use our model
to process the same number of tokens as our baseline, we improve by 0.6 to 4.2
points on these datasets.",None,-1
A Parametric Similarity Method: Comparative Experiments based on Semantically Annotated Large Datasets,0.280527,"We present the parametric method SemSimp aimed at measuring semantic
similarity of digital resources. SemSimp is based on the notion of information
content, and it leverages a reference ontology and taxonomic reasoning,
encompassing different approaches for weighting the concepts of the ontology.
In particular, weights can be computed by considering either the available
digital resources or the structure of the reference ontology of a given domain.
SemSimp is assessed against six representative semantic similarity methods for
comparing sets of concepts proposed in the literature, by carrying out an
experimentation that includes both a statistical analysis and an expert
judgement evaluation. To the purpose of achieving a reliable assessment, we
used a real-world large dataset based on the Digital Library of the Association
for Computing Machinery (ACM), and a reference ontology derived from the ACM
Computing Classification System (ACM-CCS). For each method, we considered two
indicators. The first concerns the degree of confidence to identify the
similarity among the papers belonging to some special issues selected from the
ACM Transactions on Information Systems journal, the second the Pearson
correlation with human judgement. The results reveal that one of the
configurations of SemSimp outperforms the other assessed methods. An additional
experiment performed in the domain of physics shows that, in general, SemSimp
provides better results than the other similarity methods.",None,-1
RiDDLE: Reversible and Diversified De-identification with Latent Encryptor,0.958457,"This work presents RiDDLE, short for Reversible and Diversified
De-identification with Latent Encryptor, to protect the identity information of
people from being misused. Built upon a pre-learned StyleGAN2 generator, RiDDLE
manages to encrypt and decrypt the facial identity within the latent space. The
design of RiDDLE has three appealing properties. First, the encryption process
is cipher-guided and hence allows diverse anonymization using different
passwords. Second, the true identity can only be decrypted with the correct
password, otherwise the system will produce another de-identified face to
maintain the privacy. Third, both encryption and decryption share an efficient
implementation, benefiting from a carefully tailored lightweight encryptor.
Comparisons with existing alternatives confirm that our approach accomplishes
the de-identification task with better quality, higher diversity, and stronger
reversibility. We further demonstrate the effectiveness of RiDDLE in
anonymizing videos. Code and models will be made publicly available.",None,-1
Agent-based Learning of Materials Datasets from Scientific Literature,0.863402,"Advancements in machine learning and artificial intelligence are transforming
materials discovery. Yet, the availability of structured experimental data
remains a bottleneck. The vast corpus of scientific literature presents a
valuable and rich resource of such data. However, manual dataset creation from
these resources is challenging due to issues in maintaining quality and
consistency, scalability limitations, and the risk of human error and bias.
Therefore, in this work, we develop a chemist AI agent, powered by large
language models (LLMs), to overcome these challenges by autonomously creating
structured datasets from natural language text, ranging from sentences and
paragraphs to extensive scientific research articles. Our chemist AI agent,
Eunomia, can plan and execute actions by leveraging the existing knowledge from
decades of scientific research articles, scientists, the Internet and other
tools altogether. We benchmark the performance of our approach in three
different information extraction tasks with various levels of complexity,
including solid-state impurity doping, metal-organic framework (MOF) chemical
formula, and property relations. Our results demonstrate that our zero-shot
agent, with the appropriate tools, is capable of attaining performance that is
either superior or comparable to the state-of-the-art fine-tuned materials
information extraction methods. This approach simplifies compilation of machine
learning-ready datasets for various materials discovery applications, and
significantly ease the accessibility of advanced natural language processing
tools for novice users in natural language. The methodology in this work is
developed as an open-source software on https://github.com/AI4ChemS/Eunomia.",None,-1
Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,0.0628717,"Generative Networks have proved to be extremely effective in image
restoration and reconstruction in the past few years. Generating faces from
textual descriptions is one such application where the power of generative
algorithms can be used. The task of generating faces can be useful for a number
of applications such as finding missing persons, identifying criminals, etc.
This paper discusses a novel approach to generating human faces given a textual
description regarding the facial features. We use the power of state of the art
natural language processing models to convert face descriptions into learnable
latent vectors which are then fed to a generative adversarial network which
generates faces corresponding to those features. While this paper focuses on
high level descriptions of faces only, the same approach can be tailored to
generate any image based on fine grained textual features.",None,-1
Boosting Theory-of-Mind Performance in Large Language Models via Prompting,0.306653,"Large language models (LLMs) excel in many tasks in 2023, but they still face
challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require
understanding agents' beliefs, goals, and mental states, are essential for
common-sense reasoning involving humans, making it crucial to enhance LLM
performance in this area. This study measures the ToM performance of GPT-4 and
three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates
the effectiveness of in-context learning in improving their ToM comprehension.
We evaluated prompts featuring two-shot chain of thought reasoning and
step-by-step thinking instructions. We found that LLMs trained with
Reinforcement Learning from Human Feedback (RLHF) (all models excluding
Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed
best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell
short of the 87% human accuracy on the test set. However, when supplied with
prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM
accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate
prompting enhances LLM ToM reasoning, and they underscore the context-dependent
nature of LLM cognitive capacities.",None,-1
Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection,0.725526,"The expanding market for e-comics has spurred interest in the development of
automated methods to analyze comics. For further understanding of comics, an
automated approach is needed to link text in comics to characters speaking the
words. Comics speaker detection research has practical applications, such as
automatic character assignment for audiobooks, automatic translation according
to characters' personalities, and inference of character relationships and
stories.
  To deal with the problem of insufficient speaker-to-text annotations, we
created a new annotation dataset Manga109Dialog based on Manga109.
Manga109Dialog is the world's largest comics speaker annotation dataset,
containing 132,692 speaker-to-text pairs. We further divided our dataset into
different levels by prediction difficulties to evaluate speaker detection
methods more appropriately. Unlike existing methods mainly based on distances,
we propose a deep learning-based method using scene graph generation models.
Due to the unique features of comics, we enhance the performance of our
proposed model by considering the frame reading order. We conducted experiments
using Manga109Dialog and other datasets. Experimental results demonstrate that
our scene-graph-based approach outperforms existing methods, achieving a
prediction accuracy of over 75%.",None,-1
Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives,0.548665,"Cognitive diagnosis seeks to estimate the cognitive states of students by
exploring their logged practice quiz data. It plays a pivotal role in
personalized learning guidance within intelligent education systems. In this
paper, we focus on an important, practical, yet often underexplored task:
domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the
absence of student practice logs in newly launched domains. Recent cross-domain
diagnostic models have been demonstrated to be a promising strategy for DZCD.
These methods primarily focus on how to transfer student states across domains.
However, they might inadvertently incorporate non-transferable information into
student representations, thereby limiting the efficacy of knowledge transfer.
To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive
diagnosis framework via one batch of early-bird students towards three
diagnostic objectives. Our approach initiates with pre-training a diagnosis
model with dual regularizers, which decouples student states into domain-shared
and domain-specific parts. The shared cognitive signals can be transferred to
the target domain, enriching the cognitive priors for the new domain, which
ensures the cognitive state propagation objective. Subsequently, we devise a
strategy to generate simulated practice logs for cold-start students through
analyzing the behavioral patterns from early-bird students, fulfilling the
domain-adaption goal. Consequently, we refine the cognitive states of
cold-start students as diagnostic outcomes via virtual data, aligning with the
diagnosis-oriented goal. Finally, extensive experiments on six real-world
datasets highlight the efficacy of our model for DZCD and its practical
application in question recommendation. The code is publicly available at
https://github.com/bigdata-ustc/Zero-1-to-3.",None,-1
Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,0.749447,"Facial expressions convey massive information and play a crucial role in
emotional expression. Deep neural network (DNN) accompanied by deep metric
learning (DML) techniques boost the discriminative ability of the model in
facial expression recognition (FER) applications. DNN, equipped with only
classification loss functions such as Cross-Entropy cannot compact intra-class
feature variation or separate inter-class feature distance as well as when it
gets fortified by a DML supporting loss item. The triplet center loss (TCL)
function is applied on all dimensions of the sample's embedding in the
embedding space. In our work, we developed three strategies: fully-synthesized,
semi-synthesized, and prediction-based negative sample selection strategies. To
achieve better results, we introduce a selective attention module that provides
a combination of pixel-wise and element-wise attention coefficients using
high-semantic deep features of input samples. We evaluated the proposed method
on the RAF-DB, a highly imbalanced dataset. The experimental results reveal
significant improvements in comparison to the baseline for all three negative
sample selection strategies.",None,-1
What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations,0.755915,"We propose and address a new generalisation problem: can a model trained for
action recognition successfully classify actions when they are performed within
a previously unseen scenario and in a previously unseen location? To answer
this question, we introduce the Action Recognition Generalisation Over
scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from
the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We
demonstrate recognition models struggle to generalise over 10 proposed test
splits, each of an unseen scenario in an unseen location. We thus propose CIR,
a method to represent each video as a Cross-Instance Reconstruction of videos
from other domains. Reconstructions are paired with text narrations to guide
the learning of a domain generalisable representation. We provide extensive
analysis and ablations on ARGO1M that show CIR outperforms prior domain
generalisation works on all test splits. Code and data:
https://chiaraplizz.github.io/what-can-a-cook/.",None,-1
A New Class of Explanations for Classifiers with Non-Binary Features,0.423838,"Two types of explanations have been receiving increased attention in the
literature when analyzing the decisions made by classifiers. The first type
explains why a decision was made and is known as a sufficient reason for the
decision, also an abductive explanation or a PI-explanation. The second type
explains why some other decision was not made and is known as a necessary
reason for the decision, also a contrastive or counterfactual explanation.
These explanations were defined for classifiers with binary, discrete and, in
some cases, continuous features. We show that these explanations can be
significantly improved in the presence of non-binary features, leading to a new
class of explanations that relay more information about decisions and the
underlying classifiers. Necessary and sufficient reasons were also shown to be
the prime implicates and implicants of the complete reason for a decision,
which can be obtained using a quantification operator. We show that our
improved notions of necessary and sufficient reasons are also prime implicates
and implicants but for an improved notion of complete reason obtained by a new
quantification operator that we also define and study.",None,-1
Toward Joint Language Modeling for Speech Units and Text,0.352488,"Speech and text are two major forms of human language. The research community
has been focusing on mapping speech to text or vice versa for many years.
However, in the field of language modeling, very little effort has been made to
model them jointly. In light of this, we explore joint language modeling for
speech units and text. Specifically, we compare different speech tokenizers to
transform continuous speech signals into discrete units and use different
methods to construct mixed speech-text data. We introduce automatic metrics to
evaluate how well the joint LM mixes speech and text. We also fine-tune the LM
on downstream spoken language understanding (SLU) tasks with different
modalities (speech or text) and test its performance to assess the model's
learning of shared representations. Our results show that by mixing speech
units and text with our proposed mixing techniques, the joint LM improves over
a speech-only baseline on SLU tasks and shows zero-shot cross-modal
transferability.",None,-1
A store-and-forward cloud-based telemonitoring system for automatic assessing dysarthria evolution in neurological diseases from video-recording analysis,0.900509,"Background and objectives: Patients suffering from neurological diseases may
develop dysarthria, a motor speech disorder affecting the execution of speech.
Close and quantitative monitoring of dysarthria evolution is crucial for
enabling clinicians to promptly implement patient management strategies and
maximizing effectiveness and efficiency of communication functions in term of
restoring, compensating or adjusting. In the clinical assessment of orofacial
structures and functions, at rest condition or during speech and non-speech
movements, a qualitative evaluation is usually performed, throughout visual
observation. Methods: To overcome limitations posed by qualitative assessments,
this work presents a store-and-forward self-service telemonitoring system that
integrates, within its cloud architecture, a convolutional neural network (CNN)
for analyzing video recordings acquired by individuals with dysarthria. This
architecture, called facial landmark Mask RCNN, aims at locating facial
landmarks as a prior for assessing the orofacial functions related to speech
and examining dysarthria evolution in neurological diseases. Results: When
tested on the Toronto NeuroFace dataset, a publicly available annotated dataset
of video recordings from patients with amyotrophic lateral sclerosis (ALS) and
stroke, the proposed CNN achieved a normalized mean error equal to 1.79 on
localizing the facial landmarks. We also tested our system in a real-life
scenario on 11 bulbar-onset ALS subjects, obtaining promising outcomes in terms
of facial landmark position estimation. Discussion and conclusions: This
preliminary study represents a relevant step towards the use of remote tools to
support clinicians in monitoring the evolution of dysarthria.",None,-1
A Novel Self-training Approach for Low-resource Speech Recognition,0.88048,"In this paper, we propose a self-training approach for automatic speech
recognition (ASR) for low-resource settings. While self-training approaches
have been extensively developed and evaluated for high-resource languages such
as English, their applications to low-resource languages like Punjabi have been
limited, despite the language being spoken by millions globally. The scarcity
of annotated data has hindered the development of accurate ASR systems,
especially for low-resource languages (e.g., Punjabi and M\=aori languages). To
address this issue, we propose an effective self-training approach that
generates highly accurate pseudo-labels for unlabeled low-resource speech. Our
experimental analysis demonstrates that our approach significantly improves
word error rate, achieving a relative improvement of 14.94% compared to a
baseline model across four real speech datasets. Further, our proposed approach
reports the best results on the Common Voice Punjabi dataset.",None,-1
MobileVidFactory: Automatic Diffusion-Based Social Media Video Generation for Mobile Devices from Text,0.865314,"Videos for mobile devices become the most popular access to share and acquire
information recently. For the convenience of users' creation, in this paper, we
present a system, namely MobileVidFactory, to automatically generate vertical
mobile videos where users only need to give simple texts mainly. Our system
consists of two parts: basic and customized generation. In the basic
generation, we take advantage of the pretrained image diffusion model, and
adapt it to a high-quality open-domain vertical video generator for mobile
devices. As for the audio, by retrieving from our big database, our system
matches a suitable background sound for the video. Additionally to produce
customized content, our system allows users to add specified screen texts to
the video for enriching visual expression, and specify texts for automatic
reading with optional voices as they like.",None,-1
Anatomy-Driven Pathology Detection on Chest X-rays,0.924691,"Pathology detection and delineation enables the automatic interpretation of
medical scans such as chest X-rays while providing a high level of
explainability to support radiologists in making informed decisions. However,
annotating pathology bounding boxes is a time-consuming task such that large
public datasets for this purpose are scarce. Current approaches thus use weakly
supervised object detection to learn the (rough) localization of pathologies
from image-level annotations, which is however limited in performance due to
the lack of bounding box supervision. We therefore propose anatomy-driven
pathology detection (ADPD), which uses easy-to-annotate bounding boxes of
anatomical regions as proxies for pathologies. We study two training
approaches: supervised training using anatomy-level pathology labels and
multiple instance learning (MIL) with image-level pathology labels. Our results
show that our anatomy-level training approach outperforms weakly supervised
methods and fully supervised detection with limited training samples, and our
MIL approach is competitive with both baseline approaches, therefore
demonstrating the potential of our approach.",None,-1
AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,0.634853,"Despite the impressive results achieved by many existing Structure from
Motion (SfM) approaches, there is still a need to improve the robustness,
accuracy, and efficiency on large-scale scenes with many outlier matches and
sparse view graphs. In this paper, we propose AdaSfM: a coarse-to-fine adaptive
SfM approach that is scalable to large-scale and challenging datasets. Our
approach first does a coarse global SfM which improves the reliability of the
view graph by leveraging measurements from low-cost sensors such as Inertial
Measurement Units (IMUs) and wheel encoders. Subsequently, the view graph is
divided into sub-scenes that are refined in parallel by a fine local
incremental SfM regularised by the result from the coarse global SfM to improve
the camera registration accuracy and alleviate scene drifts. Finally, our
approach uses a threshold-adaptive strategy to align all local reconstructions
to the coordinate frame of global SfM. Extensive experiments on large-scale
benchmark datasets show that our approach achieves state-of-the-art accuracy
and efficiency.",None,-1
CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval,0.102759,"Passage retrieval aims to retrieve relevant passages from large collections
of the open-domain corpus. Contextual Masked Auto-Encoding has been proven
effective in representation bottleneck pre-training of a monolithic
dual-encoder for passage retrieval. Siamese or fully separated dual-encoders
are often adopted as basic retrieval architecture in the pre-training and
fine-tuning stages for encoding queries and passages into their latent
embedding spaces. However, simply sharing or separating the parameters of the
dual-encoder results in an imbalanced discrimination of the embedding spaces.
In this work, we propose to pre-train Contextual Masked Auto-Encoder with
Mixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate
textual-specific experts for individually encoding the distinct properties of
queries and passages. Meanwhile, a shared self-attention layer is still kept
for unified attention modeling. Results on large-scale passage retrieval
benchmarks show steady improvement in retrieval performances. The quantitive
analysis also shows a more balanced discrimination of the latent embedding
spaces.",None,-1
Grammar Prompting for Domain-Specific Language Generation with Large Language Models,0.484226,"Large language models (LLMs) can learn to perform a wide range of natural
language tasks from just a handful of in-context examples. However, for
generating strings from highly structured languages (e.g., semantic parsing to
complex domain-specific languages), it is challenging for the LLM to generalize
from just a few exemplars. We propose \emph{grammar prompting}, a simple
approach to enable LLMs to use external knowledge and domain-specific
constraints, expressed through a grammar in Backus--Naur Form (BNF), during
in-context learning. Grammar prompting augments each demonstration example with
a specialized grammar that is minimally sufficient for generating the
particular output example, where the specialized grammar is a subset of the
full DSL grammar. For inference, the LLM first predicts a BNF grammar given a
test input, and then generates the output according to the rules of the
grammar. Experiments demonstrate that grammar prompting can enable LLMs to
perform competitively on a diverse set of DSL generation tasks, including
semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and
SMILES-based molecule generation.",None,-1
Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration,0.318885,"We propose a novel end-to-end document understanding model called SeRum
(SElective Region Understanding Model) for extracting meaningful information
from document images, including document analysis, retrieval, and office
automation.
  Unlike state-of-the-art approaches that rely on multi-stage technical schemes
and are computationally expensive,
  SeRum converts document image understanding and recognition tasks into a
local decoding process of the visual tokens of interest, using a content-aware
token merge module.
  This mechanism enables the model to pay more attention to regions of interest
generated by the query decoder, improving the model's effectiveness and
speeding up the decoding speed of the generative scheme.
  We also designed several pre-training tasks to enhance the understanding and
local awareness of the model.
  Experimental results demonstrate that SeRum achieves state-of-the-art
performance on document understanding tasks and competitive results on text
spotting tasks.
  SeRum represents a substantial advancement towards enabling efficient and
effective end-to-end document understanding.",None,-1
"Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators",0.865335,"Large language models that exhibit instruction-following behaviour represent
one of the biggest recent upheavals in conversational interfaces, a trend in
large part fuelled by the release of OpenAI's ChatGPT, a proprietary large
language model for text generation fine-tuned through reinforcement learning
from human feedback (LLM+RLHF). We review the risks of relying on proprietary
software and survey the first crop of open-source projects of comparable
architecture and functionality. The main contribution of this paper is to show
that openness is differentiated, and to offer scientific documentation of
degrees of openness in this fast-moving field. We evaluate projects in terms of
openness of code, training data, model weights, RLHF data, licensing,
scientific documentation, and access methods. We find that while there is a
fast-growing list of projects billing themselves as 'open source', many inherit
undocumented data of dubious legality, few share the all-important
instruction-tuning (a key site where human annotation labour is involved), and
careful scientific documentation is exceedingly rare. Degrees of openness are
relevant to fairness and accountability at all points, from data collection and
curation to model architecture, and from training and fine-tuning to release
and deployment.",None,-1
Natural Language Decomposition and Interpretation of Complex Utterances,0.123732,"Designing natural language interfaces has historically required collecting
supervised data to translate user requests into carefully designed intent
representations. This requires enumerating and labeling a long tail of user
requests, which is challenging. At the same time, large language models (LLMs)
encode knowledge about goals and plans that can help conversational assistants
interpret user requests requiring numerous steps to complete. We introduce an
approach to handle complex-intent-bearing utterances from a user via a process
of hierarchical natural language decomposition and interpretation. Our approach
uses a pre-trained language model to decompose a complex utterance into a
sequence of simpler natural language steps and interprets each step using the
language-to-program model designed for the interface. To test our approach, we
collect and release DeCU -- a new NL-to-program benchmark to evaluate
Decomposition of Complex Utterances. Experiments show that the proposed
approach enables the interpretation of complex utterances with almost no
complex training data, while outperforming standard few-shot prompting
approaches.",None,-1
Neural Machine Translation Models Can Learn to be Few-shot Learners,0.301717,"The emergent ability of Large Language Models to use a small number of
examples to learn to perform in novel domains and tasks, also called in-context
learning (ICL). In this work, we show that a much smaller model can be trained
to perform ICL by fine-tuning towards a specialized training objective,
exemplified on the task of domain adaptation for neural machine translation.
With this capacity for ICL, the model can take advantage of relevant few-shot
examples to adapt its output towards the domain. We compare the quality of this
domain adaptation to traditional supervised techniques and ICL with a
40B-parameter Large Language Model. Our approach allows efficient batch
inference on a mix of domains and outperforms state-of-the-art baselines in
terms of both translation quality and immediate adaptation rate, i.e. the
ability to reproduce a specific term after being shown a single example.",None,-1
Learning with Exposure Constraints in Recommendation Systems,0.414813,"Recommendation systems are dynamic economic systems that balance the needs of
multiple stakeholders. A recent line of work studies incentives from the
content providers' point of view. Content providers, e.g., vloggers and
bloggers, contribute fresh content and rely on user engagement to create
revenue and finance their operations. In this work, we propose a contextual
multi-armed bandit setting to model the dependency of content providers on
exposure. In our model, the system receives a user context in every round and
has to select one of the arms. Every arm is a content provider who must receive
a minimum number of pulls every fixed time period (e.g., a month) to remain
viable in later rounds; otherwise, the arm departs and is no longer available.
The system aims to maximize the users' (content consumers) welfare. To that
end, it should learn which arms are vital and ensure they remain viable by
subsidizing arm pulls if needed. We develop algorithms with sub-linear regret,
as well as a lower bound that demonstrates that our algorithms are optimal up
to logarithmic factors.",None,-1
Causal Structure Learning Supervised by Large Language Model,0.883309,"Causal discovery from observational data is pivotal for deciphering complex
relationships. Causal Structure Learning (CSL), which focuses on deriving
causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast
DAG spaces and data sparsity. The integration of Large Language Models (LLMs),
recognized for their causal reasoning capabilities, offers a promising
direction to enhance CSL by infusing it with knowledge-based causal inferences.
However, existing approaches utilizing LLMs for CSL have encountered issues,
including unreliable constraints from imperfect LLM inferences and the
computational intensity of full pairwise variable analyses. In response, we
introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL
innovatively integrates LLM-based causal inference with CSL in an iterative
process, refining the causal DAG using feedback from LLMs. This method not only
utilizes LLM resources more efficiently but also generates more robust and
high-quality structural constraints compared to previous methodologies. Our
comprehensive evaluation across eight real-world datasets demonstrates
ILS-CSL's superior performance, setting a new standard in CSL efficacy and
showcasing its potential to significantly advance the field of causal
discovery. The codes are available at
\url{https://github.com/tyMadara/ILS-CSL}.",None,-1
Automatic Truss Design with Reinforcement Learning,0.318084,"Truss layout design, namely finding a lightweight truss layout satisfying all
the physical constraints, is a fundamental problem in the building industry.
Generating the optimal layout is a challenging combinatorial optimization
problem, which can be extremely expensive to solve by exhaustive search.
Directly applying end-to-end reinforcement learning (RL) methods to truss
layout design is infeasible either, since only a tiny portion of the entire
layout space is valid under the physical constraints, leading to particularly
sparse rewards for RL training. In this paper, we develop AutoTruss, a
two-stage framework to efficiently generate both lightweight and valid truss
layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse
collection of valid layouts. Then RL is applied to iteratively refine the valid
solutions. We conduct experiments and ablation studies in popular truss layout
design test cases in both 2D and 3D settings. AutoTruss outperforms the
best-reported layouts by 25.1% in the most challenging 3D test cases, resulting
in the first effective deep-RL-based approach in the truss layout design
literature.",None,-1
Geometric Ultrasound Localization Microscopy,0.718299,"Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.",None,-1
Divide and Prompt: Chain of Thought Prompting for Text-to-SQL,0.533647,"Chain-of-thought (CoT) prompting combined with large language models (LLMs)
have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a
critical semantic parsing task that converts natural language questions into
SQL statements, involving a complex reasoning process. However, there is little
work about using CoT prompting to activate LLM's reasoning capabilities on
Text-to-SQL tasks. In this work, we propose a new paradigm for prompting
Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into
subtasks, and then approach each subtask through CoT. We present 3
prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments
show that these prompts guide LLMs to generate Text-to-SQL with higher
execution accuracy.",None,-1
Design of JiuTian Intelligent Network Simulation Platform,0.517415,"This paper introduced the JiuTian Intelligent Network Simulation Platform,
which can provide wireless communication simulation data services for the Open
Innovation Platform. The platform contains a series of scalable simulator
functionalities, offering open services that enable users to use reinforcement
learning algorithms for model training and inference based on simulation
environments and data. Additionally, it allows users to address optimization
tasks in different scenarios by uploading and updating parameter
configurations. The platform and its open services were primarily introduced
from the perspectives of background, overall architecture, simulator, business
scenarios, and future directions.",None,-1
A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?,0.229152,"We administer a Turing Test to AI Chatbots. We examine how Chatbots behave in
a suite of classic behavioral games that are designed to elicit characteristics
such as trust, fairness, risk-aversion, cooperation, \textit{etc.}, as well as
how they respond to a traditional Big-5 psychological survey that measures
personality traits. ChatGPT-4 exhibits behavioral and personality traits that
are statistically indistinguishable from a random human from tens of thousands
of human subjects from more than 50 countries. Chatbots also modify their
behavior based on previous experience and contexts ``as if'' they were learning
from the interactions, and change their behavior in response to different
framings of the same strategic situation. Their behaviors are often distinct
from average and modal human behaviors, in which case they tend to behave on
the more altruistic and cooperative end of the distribution. We estimate that
they act as if they are maximizing an average of their own and partner's
payoffs.",None,-1
Substitution-based Semantic Change Detection using Contextual Embeddings,0.574856,"Measuring semantic change has thus far remained a task where methods using
contextual embeddings have struggled to improve upon simpler techniques relying
only on static word vectors. Moreover, many of the previously proposed
approaches suffer from downsides related to scalability and ease of
interpretation. We present a simplified approach to measuring semantic change
using contextual embeddings, relying only on the most probable substitutes for
masked terms. Not only is this approach directly interpretable, it is also far
more efficient in terms of storage, achieves superior average performance
across the most frequently cited datasets for this task, and allows for more
nuanced investigation of change than is possible with static word vectors.",None,-1
Representation Matters: The Game of Chess Poses a Challenge to Vision Transformers,0.0828452,"While transformers have gained the reputation as the ""Swiss army knife of
AI"", no one has challenged them to master the game of chess, one of the
classical AI benchmarks. Simply using vision transformers (ViTs) within
AlphaZero does not master the game of chess, mainly because ViTs are too slow.
Even making them more efficient using a combination of MobileNet and NextViT
does not beat what actually matters: a simple change of the input
representation and value loss, resulting in a greater boost of up to 180 Elo
points over AlphaZero.",None,-1
Applying Plain Transformers to Real-World Point Clouds,0.134066,"To apply transformer-based models to point cloud understanding, many previous
works modify the architecture of transformers by using, e.g., local attention
and down-sampling. Although they have achieved promising results, earlier works
on transformers for point clouds have two issues. First, the power of plain
transformers is still under-explored. Second, they focus on simple and small
point clouds instead of complex real-world ones. This work revisits the plain
transformers in real-world point cloud understanding. We first take a closer
look at some fundamental components of plain transformers, e.g., patchifier and
positional embedding, for both efficiency and performance. To close the
performance gap due to the lack of inductive bias and annotated data, we
investigate self-supervised pre-training with masked autoencoder (MAE).
Specifically, we propose drop patch, which prevents information leakage and
significantly improves the effectiveness of MAE. Our models achieve SOTA
results in semantic segmentation on the S3DIS dataset and object detection on
the ScanNet dataset with lower computational costs. Our work provides a new
baseline for future research on transformers for point clouds.",None,-1
TemporalMaxer: Maximize Temporal Context with only Max Pooling for Temporal Action Localization,0.889456,"Temporal Action Localization (TAL) is a challenging task in video
understanding that aims to identify and localize actions within a video
sequence. Recent studies have emphasized the importance of applying long-term
temporal context modeling (TCM) blocks to the extracted video clip features
such as employing complex self-attention mechanisms. In this paper, we present
the simplest method ever to address this task and argue that the extracted
video clip features are already informative to achieve outstanding performance
without sophisticated architectures. To this end, we introduce TemporalMaxer,
which minimizes long-term temporal context modeling while maximizing
information from the extracted video clip features with a basic,
parameter-free, and local region operating max-pooling block. Picking out only
the most critical information for adjacent and local clip embeddings, this
block results in a more efficient TAL model. We demonstrate that TemporalMaxer
outperforms other state-of-the-art methods that utilize long-term TCM such as
self-attention on various TAL datasets while requiring significantly fewer
parameters and computational resources. The code for our approach is publicly
available at https://github.com/TuanTNG/TemporalMaxer",None,-1
Spatiotemporal Deformation Perception for Fisheye Video Rectification,0.340046,"Although the distortion correction of fisheye images has been extensively
studied, the correction of fisheye videos is still an elusive challenge. For
different frames of the fisheye video, the existing image correction methods
ignore the correlation of sequences, resulting in temporal jitter in the
corrected video. To solve this problem, we propose a temporal weighting scheme
to get a plausible global optical flow, which mitigates the jitter effect by
progressively reducing the weight of frames. Subsequently, we observe that the
inter-frame optical flow of the video is facilitated to perceive the local
spatial deformation of the fisheye video. Therefore, we derive the spatial
deformation through the flows of fisheye and distorted-free videos, thereby
enhancing the local accuracy of the predicted result. However, the independent
correction for each frame disrupts the temporal correlation. Due to the
property of fisheye video, a distorted moving object may be able to find its
distorted-free pattern at another moment. To this end, a temporal deformation
aggregator is designed to reconstruct the deformation correlation between
frames and provide a reliable global feature. Our method achieves an end-to-end
correction and demonstrates superiority in correction quality and stability
compared with the SOTA correction methods.",None,-1
Toward Sufficient Spatial-Frequency Interaction for Gradient-aware Underwater Image Enhancement,0.619645,"Underwater images suffer from complex and diverse degradation, which
inevitably affects the performance of underwater visual tasks. However, most
existing learning-based Underwater image enhancement (UIE) methods mainly
restore such degradations in the spatial domain, and rarely pay attention to
the fourier frequency information. In this paper, we develop a novel UIE
framework based on spatial-frequency interaction and gradient maps, namely
SFGNet, which consists of two stages. Specifically, in the first stage, we
propose a dense spatial-frequency fusion network (DSFFNet), mainly including
our designed dense fourier fusion block and dense spatial fusion block,
achieving sufficient spatial-frequency interaction by cross connections between
these two blocks. In the second stage, we propose a gradient-aware corrector
(GAC) to further enhance perceptual details and geometric structures of images
by gradient map. Experimental results on two real-world underwater image
datasets show that our approach can successfully enhance underwater images, and
achieves competitive performance in visual quality improvement. The code is
available at https://github.com/zhihefang/SFGNet.",None,-1
An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems,0.771619,"The Segment Anything Model (SAM) has demonstrated exceptional performance and
versatility, making it a promising tool for various related tasks. In this
report, we explore the application of SAM in Weakly-Supervised Semantic
Segmentation (WSSS). Particularly, we adapt SAM as the pseudo-label generation
pipeline given only the image-level class labels. While we observed impressive
results in most cases, we also identify certain limitations. Our study includes
performance evaluations on PASCAL VOC and MS-COCO, where we achieved remarkable
improvements over the latest state-of-the-art methods on both datasets. We
anticipate that this report encourages further explorations of adopting SAM in
WSSS, as well as wider real-world applications.",None,-1
UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View,0.894304,"In the field of 3D object detection for autonomous driving, the sensor
portfolio including multi-modality and single-modality is diverse and complex.
Since the multi-modal methods have system complexity while the accuracy of
single-modal ones is relatively low, how to make a tradeoff between them is
difficult. In this work, we propose a universal cross-modality knowledge
distillation framework (UniDistill) to improve the performance of
single-modality detectors. Specifically, during training, UniDistill projects
the features of both the teacher and the student detector into Bird's-Eye-View
(BEV), which is a friendly representation for different modalities. Then, three
distillation losses are calculated to sparsely align the foreground features,
helping the student learn from the teacher without introducing additional cost
during inference. Taking advantage of the similar detection paradigm of
different detectors in BEV, UniDistill easily supports LiDAR-to-camera,
camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths.
Furthermore, the three distillation losses can filter the effect of misaligned
background information and balance between objects of different sizes,
improving the distillation effectiveness. Extensive experiments on nuScenes
demonstrate that UniDistill effectively improves the mAP and NDS of student
detectors by 2.0%~3.2%.",None,-1
AMR Parsing with Causal Hierarchical Attention and Pointers,0.236382,"Translation-based AMR parsers have recently gained popularity due to their
simplicity and effectiveness. They predict linearized graphs as free texts,
avoiding explicit structure modeling. However, this simplicity neglects
structural locality in AMR graphs and introduces unnecessary tokens to
represent coreferences. In this paper, we introduce new target forms of AMR
parsing and a novel model, CHAP, which is equipped with causal hierarchical
attention and the pointer mechanism, enabling the integration of structures
into the Transformer decoder. We empirically explore various alternative
modeling options. Experiments show that our model outperforms baseline models
on four out of five benchmarks in the setting of no additional data.",None,-1
WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models,0.853202,"The open road poses many challenges to autonomous perception, including poor
visibility from extreme weather conditions. Models trained on good-weather
datasets frequently fail at detection in these out-of-distribution settings. To
aid adversarial robustness in perception, we introduce WEDGE (WEather images by
DALL-E GEneration): a synthetic dataset generated with a vision-language
generative model via prompting. WEDGE consists of 3360 images in 16 extreme
weather conditions manually annotated with 16513 bounding boxes, supporting
research in the tasks of weather classification and 2D object detection. We
have analyzed WEDGE from research standpoints, verifying its effectiveness for
extreme-weather autonomous perception. We establish baseline performance for
classification and detection with 53.87% test accuracy and 45.41 mAP. Most
importantly, WEDGE can be used to fine-tune state-of-the-art detectors,
improving SOTA performance on real-world weather benchmarks (such as DAWN) by
4.48 AP for well-generated classes like trucks. WEDGE has been collected under
OpenAI's terms of use and is released for public use under the CC BY-NC-SA 4.0
license. The repository for this work and dataset is available at
https://infernolia.github.io/WEDGE.",None,-1
HIORE: Leveraging High-order Interactions for Unified Entity Relation Extraction,0.343354,"Entity relation extraction consists of two sub-tasks: entity recognition and
relation extraction. Existing methods either tackle these two tasks separately
or unify them with word-by-word interactions. In this paper, we propose HIORE,
a new method for unified entity relation extraction. The key insight is to
leverage the high-order interactions, i.e., the complex association among word
pairs, which contains richer information than the first-order word-by-word
interactions. For this purpose, we first devise a W-shape DNN (WNet) to capture
coarse-level high-order connections. Then, we build a heuristic high-order
graph and further calibrate the representations with a graph neural network
(GNN). Experiments on three benchmarks (ACE04, ACE05, SciERC) show that HIORE
achieves the state-of-the-art performance on relation extraction and an
improvement of 1.1~1.8 F1 points over the prior best unified model.",None,-1
Benchmarking of Cancelable Biometrics for Deep Templates,0.746949,"In this paper, we benchmark several cancelable biometrics (CB) schemes on
different biometric characteristics. We consider BioHashing, Multi-Layer
Perceptron (MLP) Hashing, Bloom Filters, and two schemes based on
Index-of-Maximum (IoM) Hashing (i.e., IoM-URP and IoM-GRP). In addition to the
mentioned CB schemes, we introduce a CB scheme (as a baseline) based on
user-specific random transformations followed by binarization. We evaluate the
unlinkability, irreversibility, and recognition performance (which are the
required criteria by the ISO/IEC 24745 standard) of these CB schemes on deep
learning based templates extracted from different physiological and behavioral
biometric characteristics including face, voice, finger vein, and iris. In
addition, we provide an open-source implementation of all the experiments
presented to facilitate the reproducibility of our results.",None,-1
Ticket-BERT: Labeling Incident Management Tickets with Language Models,0.399739,"An essential aspect of prioritizing incident tickets for resolution is
efficiently labeling tickets with fine-grained categories. However, ticket data
is often complex and poses several unique challenges for modern machine
learning methods: (1) tickets are created and updated either by machines with
pre-defined algorithms or by engineers with domain expertise that share
different protocols, (2) tickets receive frequent revisions that update ticket
status by modifying all or parts of ticket descriptions, and (3) ticket
labeling is time-sensitive and requires knowledge updates and new labels per
the rapid software and hardware improvement lifecycle. To handle these issues,
we introduce Ticket- BERT which trains a simple yet robust language model for
labeling tickets using our proposed ticket datasets. Experiments demonstrate
the superiority of Ticket-BERT over baselines and state-of-the-art text
classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT
with an active learning cycle and deploy it on the Microsoft IcM system, which
enables the model to quickly finetune on newly-collected tickets with a few
annotations.",None,-1
At Your Fingertips: Extracting Piano Fingering Instructions from Videos,0.632121,"Piano fingering -- knowing which finger to use to play each note in a musical
piece, is a hard and important skill to master when learning to play the piano.
While some sheet music is available with expert-annotated fingering
information, most pieces lack this information, and people often resort to
learning the fingering from demonstrations in online videos. We consider the AI
task of automating the extraction of fingering information from videos. This is
a non-trivial task as fingers are often occluded by other fingers, and it is
often not clear from the video which of the keys were pressed, requiring the
synchronization of hand position information and knowledge about the notes that
were played. We show how to perform this task with high-accuracy using a
combination of deep-learning modules, including a GAN-based approach for
fine-tuning on out-of-domain data. We extract the fingering information with an
f1 score of 97\%. We run the resulting system on 90 videos, resulting in
high-quality piano fingering information of 150K notes, the largest available
dataset of piano-fingering to date.",None,-1
Current and Future Challenges in Knowledge Representation and Reasoning,0.367045,"Knowledge Representation and Reasoning is a central, longstanding, and active
area of Artificial Intelligence. Over the years it has evolved significantly;
more recently it has been challenged and complemented by research in areas such
as machine learning and reasoning under uncertainty. In July 2022 a Dagstuhl
Perspectives workshop was held on Knowledge Representation and Reasoning. The
goal of the workshop was to describe the state of the art in the field,
including its relation with other areas, its shortcomings and strengths,
together with recommendations for future progress. We developed this manifesto
based on the presentations, panels, working groups, and discussions that took
place at the Dagstuhl Workshop. It is a declaration of our views on Knowledge
Representation: its origins, goals, milestones, and current foci; its relation
to other disciplines, especially to Artificial Intelligence; and on its
challenges, along with key priorities for the next decade.",None,-1
ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks,1.0,"Many NLP applications require manual data annotations for a variety of tasks,
notably to train classifiers or evaluate the performance of unsupervised
models. Depending on the size and degree of complexity, the tasks may be
conducted by crowd-workers on platforms such as MTurk as well as trained
annotators, such as research assistants. Using a sample of 2,382 tweets, we
demonstrate that ChatGPT outperforms crowd-workers for several annotation
tasks, including relevance, stance, topics, and frames detection. Specifically,
the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of
five tasks, while ChatGPT's intercoder agreement exceeds that of both
crowd-workers and trained annotators for all tasks. Moreover, the
per-annotation cost of ChatGPT is less than $0.003 -- about twenty times
cheaper than MTurk. These results show the potential of large language models
to drastically increase the efficiency of text classification.",None,-1
Diverse Conventions for Human-AI Collaboration,0.239533,"Conventions are crucial for strong performance in cooperative multi-agent
games, because they allow players to coordinate on a shared strategy without
explicit communication. Unfortunately, standard multi-agent reinforcement
learning techniques, such as self-play, converge to conventions that are
arbitrary and non-diverse, leading to poor generalization when interacting with
new partners. In this work, we present a technique for generating diverse
conventions by (1) maximizing their rewards during self-play, while (2)
minimizing their rewards when playing with previously discovered conventions
(cross-play), stimulating conventions to be semantically different. To ensure
that learned policies act in good faith despite the adversarial optimization of
cross-play, we introduce \emph{mixed-play}, where an initial state is randomly
generated by sampling self-play and cross-play transitions and the player
learns to maximize the self-play reward from this initial state. We analyze the
benefits of our technique on various multi-agent collaborative games, including
Overcooked, and find that our technique can adapt to the conventions of humans,
surpassing human-level performance when paired with real users.",None,-1
Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion,0.798131,"Diffusion models have shown superior performance in image generation and
manipulation, but the inherent stochasticity presents challenges in preserving
and manipulating image content and identity. While previous approaches like
DreamBooth and Textual Inversion have proposed model or latent representation
personalization to maintain the content, their reliance on multiple reference
images and complex training limits their practicality. In this paper, we
present a simple yet highly effective approach to personalization using highly
personalized (HiPer) text embedding by decomposing the CLIP embedding space for
personalization and content manipulation. Our method does not require model
fine-tuning or identifiers, yet still enables manipulation of background,
texture, and motion with just a single image and target text. Through
experiments on diverse target texts, we demonstrate that our approach produces
highly personalized and complex semantic image edits across a wide range of
tasks. We believe that the novel understanding of the text embedding space
presented in this work has the potential to inspire further research across
various tasks.",None,-1
Self-Attention Based Generative Adversarial Networks For Unsupervised Video Summarization,0.174248,"In this paper, we study the problem of producing a comprehensive video
summary following an unsupervised approach that relies on adversarial learning.
We build on a popular method where a Generative Adversarial Network (GAN) is
trained to create representative summaries, indistinguishable from the
originals. The introduction of the attention mechanism into the architecture
for the selection, encoding and decoding of video frames, shows the efficacy of
self-attention and transformer in modeling temporal relationships for video
summarization. We propose the SUM-GAN-AED model that uses a self-attention
mechanism for frame selection, combined with LSTMs for encoding and decoding.
We evaluate the performance of the SUM-GAN-AED model on the SumMe, TVSum and
COGNIMUSE datasets. Experimental results indicate that using a self-attention
mechanism as the frame selection mechanism outperforms the state-of-the-art on
SumMe and leads to comparable to state-of-the-art performance on TVSum and
COGNIMUSE.",None,-1
LSTM-CNN: An efficient diagnostic network for Parkinson's disease utilizing dynamic handwriting analysis,0.204372,"Background and objectives: Dynamic handwriting analysis, due to its
non-invasive and readily accessible nature, has recently emerged as a vital
adjunctive method for the early diagnosis of Parkinson's disease. In this
study, we design a compact and efficient network architecture to analyse the
distinctive handwriting patterns of patients' dynamic handwriting signals,
thereby providing an objective identification for the Parkinson's disease
diagnosis.
  Methods: The proposed network is based on a hybrid deep learning approach
that fully leverages the advantages of both long short-term memory (LSTM) and
convolutional neural networks (CNNs). Specifically, the LSTM block is adopted
to extract the time-varying features, while the CNN-based block is implemented
using one-dimensional convolution for low computational cost. Moreover, the
hybrid model architecture is continuously refined under ablation studies for
superior performance. Finally, we evaluate the proposed method with its
generalization under a five-fold cross-validation, which validates its
efficiency and robustness.
  Results: The proposed network demonstrates its versatility by achieving
impressive classification accuracies on both our new DraWritePD dataset
($96.2\%$) and the well-established PaHaW dataset ($90.7\%$). Moreover, the
network architecture also stands out for its excellent lightweight design,
occupying a mere $0.084$M of parameters, with a total of only $0.59$M
floating-point operations. It also exhibits near real-time CPU inference
performance, with inference times ranging from $0.106$ to $0.220$s.
  Conclusions: We present a series of experiments with extensive analysis,
which systematically demonstrate the effectiveness and efficiency of the
proposed hybrid neural network in extracting distinctive handwriting patterns
for precise diagnosis of Parkinson's disease.",None,-1
Description-Based Text Similarity,0.397039,"Identifying texts with a given semantics is central for many information
seeking scenarios. Similarity search over vector embeddings appear to be
central to this ability, yet the similarity reflected in current text
embeddings is corpus-driven, and is inconsistent and sub-optimal for many use
cases. What, then, is a good notion of similarity for effective retrieval of
text?
  We identify the need to search for texts based on abstract descriptions of
their content, and the corresponding notion of \emph{description based
similarity}. We demonstrate the inadequacy of current text embeddings and
propose an alternative model that significantly improves when used in standard
nearest neighbor search. The model is trained using positive and negative pairs
sourced through prompting a LLM, demonstrating how data from LLMs can be used
for creating new capabilities not immediately possible using the original
model.",None,-1
Source-Free Domain Adaptation for RGB-D Semantic Segmentation with Vision Transformers,0.558383,"With the increasing availability of depth sensors, multimodal frameworks that
combine color information with depth data are gaining interest. However, ground
truth data for semantic segmentation is burdensome to provide, thus making
domain adaptation a significant research area. Yet most domain adaptation
methods are not able to effectively handle multimodal data. Specifically, we
address the challenging source-free domain adaptation setting where the
adaptation is performed without reusing source data. We propose MISFIT:
MultImodal Source-Free Information fusion Transformer, a depth-aware framework
which injects depth data into a segmentation module based on vision
transformers at multiple stages, namely at the input, feature and output
levels. Color and depth style transfer helps early-stage domain alignment while
re-wiring self-attention between modalities creates mixed features, allowing
the extraction of better semantic content. Furthermore, a depth-based entropy
minimization strategy is also proposed to adaptively weight regions at
different distances. Our framework, which is also the first approach using
RGB-D vision transformers for source-free semantic segmentation, shows
noticeable performance improvements with respect to standard strategies.",None,-1
"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",0.105555,"Large language models (LLMs) are popular for high-quality text generation but
can produce harmful content, even when aligned with human values through
reinforcement learning. Adversarial prompts can bypass their safety measures.
We propose LLM Self Defense, a simple approach to defend against these attacks
by having an LLM screen the induced responses. Our method does not require any
fine-tuning, input preprocessing, or iterative output generation. Instead, we
incorporate the generated content into a pre-defined prompt and employ another
instance of an LLM to analyze the text and predict whether it is harmful. We
test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent
LLMs against various types of attacks, such as forcefully inducing affirmative
responses to prompts and prompt engineering attacks. Notably, LLM Self Defense
succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5
and Llama 2. The code is publicly available at
https://github.com/poloclub/llm-self-defense",None,-1
ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation,0.558938,"Compositional generalization benchmarks for semantic parsing seek to assess
whether models can accurately compute meanings for novel sentences, but
operationalize this in terms of logical form (LF) prediction. This raises the
concern that semantically irrelevant details of the chosen LFs could shape
model performance. We argue that this concern is realized for the COGS
benchmark. COGS poses generalization splits that appear impossible for
present-day models, which could be taken as an indictment of those models.
However, we show that the negative results trace to incidental features of COGS
LFs. Converting these LFs to semantically equivalent ones and factoring out
capabilities unrelated to semantic interpretation, we find that even baseline
models get traction. A recent variable-free translation of COGS LFs suggests
similar conclusions, but we observe this format is not semantically equivalent;
it is incapable of accurately representing some COGS meanings. These findings
inform our proposal for ReCOGS, a modified version of COGS that comes closer to
assessing the target semantic capabilities while remaining very challenging.
Overall, our results reaffirm the importance of compositional generalization
and careful benchmark task design.",None,-1
Learning to Optimize for Reinforcement Learning,0.0574946,"In recent years, by leveraging more data, computation, and diverse tasks,
learned optimizers have achieved remarkable success in supervised learning,
outperforming classical hand-designed optimizers. Reinforcement learning (RL)
is essentially different from supervised learning, and in practice, these
learned optimizers do not work well even in simple RL tasks. We investigate
this phenomenon and identify two issues. First, the agent-gradient distribution
is non-independent and identically distributed, leading to inefficient
meta-training. Moreover, due to highly stochastic agent-environment
interactions, the agent-gradients have high bias and variance, which increases
the difficulty of learning an optimizer for RL. We propose pipeline training
and a novel optimizer structure with a good inductive bias to address these
issues, making it possible to learn an optimizer for reinforcement learning
from scratch. We show that, although only trained in toy tasks, our learned
optimizer can generalize to unseen complex tasks in Brax.",None,-1
Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method,0.308445,"Large Language Models (LLMs) have shown great potential in Natural Language
Processing (NLP) tasks. However, recent literature reveals that LLMs generate
nonfactual responses intermittently, which impedes the LLMs' reliability for
further utilization. In this paper, we propose a novel self-detection method to
detect which questions that a LLM does not know that are prone to generate
nonfactual results. Specifically, we first diversify the textual expressions
for a given question and collect the corresponding answers. Then we examine the
divergencies between the generated answers to identify the questions that the
model may generate falsehoods. All of the above steps can be accomplished by
prompting the LLMs themselves without referring to any other external
resources. We conduct comprehensive experiments and demonstrate the
effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,
and GPT-4.",None,-1
Automated Reading Passage Generation with OpenAI's Large Language Model,0.462899,"The widespread usage of computer-based assessments and individualized
learning platforms has resulted in an increased demand for the rapid production
of high-quality items. Automated item generation (AIG), the process of using
item models to generate new items with the help of computer technology, was
proposed to reduce reliance on human subject experts at each step of the
process. AIG has been used in test development for some time. Still, the use of
machine learning algorithms has introduced the potential to improve the
efficiency and effectiveness of the process greatly. The approach presented in
this paper utilizes OpenAI's latest transformer-based language model, GPT-3, to
generate reading passages. Existing reading passages were used in carefully
engineered prompts to ensure the AI-generated text has similar content and
structure to a fourth-grade reading passage. For each prompt, we generated
multiple passages, the final passage was selected according to the Lexile score
agreement with the original passage. In the final round, the selected passage
went through a simple revision by a human editor to ensure the text was free of
any grammatical and factual errors. All AI-generated passages, along with
original passages were evaluated by human judges according to their coherence,
appropriateness to fourth graders, and readability.",None,-1
Architext: Language-Driven Generative Architecture Design,0.414525,"Architectural design is a highly complex practice that involves a wide
diversity of disciplines, technologies, proprietary design software, expertise,
and an almost infinite number of constraints, across a vast array of design
tasks. Enabling intuitive, accessible, and scalable design processes is an
important step towards performance-driven and sustainable design for all. To
that end, we introduce Architext, a novel semantic generation assistive tool.
Architext enables design generation with only natural language prompts, given
to large-scale Language Models, as input. We conduct a thorough quantitative
evaluation of Architext's downstream task performance, focusing on semantic
accuracy and diversity for a number of pre-trained language models ranging from
120 million to 6 billion parameters. Architext models are able to learn the
specific design task, generating valid residential layouts at a near 100% rate.
Accuracy shows great improvement when scaling the models, with the largest
model (GPT-J) yielding impressive accuracy ranging between 25% to over 80% for
different prompt categories. We open source the finetuned Architext models and
our synthetic dataset, hoping to inspire experimentation in this exciting area
of design research.",None,-1
Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA,0.203247,"Despite remarkable progress made in natural language processing, even the
state-of-the-art models often make incorrect predictions. Such predictions
hamper the reliability of systems and limit their widespread adoption in
real-world applications. 'Selective prediction' partly addresses the above
concern by enabling models to abstain from answering when their predictions are
likely to be incorrect. While selective prediction is advantageous, it leaves
us with a pertinent question 'what to do after abstention'. To this end, we
present an explorative study on 'Post-Abstention', a task that allows
re-attempting the abstained instances with the aim of increasing 'coverage' of
the system without significantly sacrificing its 'accuracy'. We first provide
mathematical formulation of this task and then explore several methods to solve
it. Comprehensive experiments on 11 QA datasets show that these methods lead to
considerable risk improvements -- performance metric of the Post-Abstention
task -- both in the in-domain and the out-of-domain settings. We also conduct a
thorough analysis of these results which further leads to several interesting
findings. Finally, we believe that our work will encourage and facilitate
further research in this important area of addressing the reliability of NLP
systems.",None,-1
`It is currently hodgepodge'': Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values,0.801903,"Recently, the AI/ML research community has indicated an urgent need to
establish Responsible AI (RAI) values and practices as part of the AI/ML
lifecycle. Several organizations and communities are responding to this call by
sharing RAI guidelines. However, there are gaps in awareness, deliberation, and
execution of such practices for multi-disciplinary ML practitioners. This work
contributes to the discussion by unpacking co-production challenges faced by
practitioners as they align their RAI values. We interviewed 23 individuals,
across 10 organizations, tasked to ship AI/ML based products while upholding
RAI norms and found that both top-down and bottom-up institutional structures
create burden for different roles preventing them from upholding RAI values, a
challenge that is further exacerbated when executing conflicted values. We
share multiple value levers used as strategies by the practitioners to resolve
their challenges. We end our paper with recommendations for inclusive and
equitable RAI value-practices, creating supportive organizational structures
and opportunities to further aid practitioners.",None,-1
ChatHaruhi: Reviving Anime Character in Reality via Large Language Model,0.658915,"Role-playing chatbots built on large language models have drawn interest, but
better techniques are needed to enable mimicking specific fictional characters.
We propose an algorithm that controls language models via an improved prompt
and memories of the character extracted from scripts. We construct ChatHaruhi,
a dataset covering 32 Chinese / English TV / anime characters with over 54k
simulated dialogues. Both automatic and human evaluations show our approach
improves role-playing ability over baselines. Code and data are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya .",None,-1
AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation,0.851422,"We present All-Pairs Multi-Field Transforms (AMT), a new network architecture
for video frame interpolation. It is based on two essential designs. First, we
build bidirectional correlation volumes for all pairs of pixels, and use the
predicted bilateral flows to retrieve correlations for updating both flows and
the interpolated content feature. Second, we derive multiple groups of
fine-grained flow fields from one pair of updated coarse flows for performing
backward warping on the input frames separately. Combining these two designs
enables us to generate promising task-oriented flows and reduce the
difficulties in modeling large motions and handling occluded areas during frame
interpolation. These qualities promote our model to achieve state-of-the-art
performance on various benchmarks with high efficiency. Moreover, our
convolution-based model competes favorably compared to Transformer-based models
in terms of accuracy and efficiency. Our code is available at
https://github.com/MCG-NKU/AMT.",None,-1
LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models,0.926755,"Creating graphic layouts is a fundamental step in graphic designs. In this
work, we present a novel generative model named LayoutDiffusion for automatic
layout generation. As layout is typically represented as a sequence of discrete
tokens, LayoutDiffusion models layout generation as a discrete denoising
diffusion process. It learns to reverse a mild forward process, in which
layouts become increasingly chaotic with the growth of forward steps and
layouts in the neighboring steps do not differ too much. Designing such a mild
forward process is however very challenging as layout has both categorical
attributes and ordinal attributes. To tackle the challenge, we summarize three
critical factors for achieving a mild forward process for the layout, i.e.,
legality, coordinate proximity and type disruption. Based on the factors, we
propose a block-wise transition matrix coupled with a piece-wise linear noise
schedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion
outperforms state-of-the-art approaches significantly. Moreover, it enables two
conditional layout generation tasks in a plug-and-play manner without
re-training and achieves better performance than existing methods.",None,-1
A Transformer-based Approach for Arabic Offline Handwritten Text Recognition,0.852553,"Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.",None,-1
Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory,0.417567,"Human Object Interaction (HOI) detection aims to localize and infer the
relationships between a human and an object. Arguably, training supervised
models for this task from scratch presents challenges due to the performance
drop over rare classes and the high computational cost and time required to
handle long-tailed distributions of HOIs in complex HOI scenes in realistic
settings. This observation motivates us to design an HOI detector that can be
trained even with long-tailed labeled data and can leverage existing knowledge
from pre-trained models. Inspired by the powerful generalization ability of the
large Vision-Language Models (VLM) on classification and retrieval tasks, we
propose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM).
ADA-CM has two operating modes. The first mode makes it tunable without
learning new parameters in a training-free paradigm. Its second mode
incorporates an instance-aware adapter mechanism that can further efficiently
boost performance if updating a lightweight set of parameters can be afforded.
Our proposed method achieves competitive results with state-of-the-art on the
HICO-DET and V-COCO datasets with much less training time. Code can be found at
https://github.com/ltttpku/ADA-CM.",None,-1
Masked and Adaptive Transformer for Exemplar Based Image Translation,0.44924,"We present a novel framework for exemplar based image translation. Recent
advanced methods for this task mainly focus on establishing cross-domain
semantic correspondence, which sequentially dominates image generation in the
manner of local style control. Unfortunately, cross-domain semantic matching is
challenging; and matching errors ultimately degrade the quality of generated
images. To overcome this challenge, we improve the accuracy of matching on the
one hand, and diminish the role of matching in image generation on the other
hand. To achieve the former, we propose a masked and adaptive transformer (MAT)
for learning accurate cross-domain correspondence, and executing context-aware
feature augmentation. To achieve the latter, we use source features of the
input and global style codes of the exemplar, as supplementary information, for
decoding an image. Besides, we devise a novel contrastive style learning
method, for acquire quality-discriminative style representations, which in turn
benefit high-quality image generation. Experimental results show that our
method, dubbed MATEBIT, performs considerably better than state-of-the-art
methods, in diverse image translation tasks. The codes are available at
\url{https://github.com/AiArt-HDU/MATEBIT}.",None,-1
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,0.994785,"This paper demonstrates an approach for learning highly semantic image
representations without relying on hand-crafted data-augmentations. We
introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a
non-generative approach for self-supervised learning from images. The idea
behind I-JEPA is simple: from a single context block, predict the
representations of various target blocks in the same image. A core design
choice to guide I-JEPA towards producing semantic representations is the
masking strategy; specifically, it is crucial to (a) sample target blocks with
sufficiently large scale (semantic), and to (b) use a sufficiently informative
(spatially distributed) context block. Empirically, when combined with Vision
Transformers, we find I-JEPA to be highly scalable. For instance, we train a
ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong
downstream performance across a wide range of tasks, from linear classification
to object counting and depth prediction.",None,-1
Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning,0.335099,"It has always been an important yet challenging problem to control language
models to avoid generating texts with undesirable attributes, such as toxic
language and unnatural repetition. We introduce Click for controllable text
generation, which needs no modification to the model architecture and
facilitates out-of-the-box use of trained models. It employs a contrastive loss
on sequence likelihood, which fundamentally decreases the generation
probability of negative samples (i.e., generations with undesirable
attributes). It also adopts a novel likelihood ranking-based strategy to
construct contrastive samples from model generations. On the tasks of language
detoxification, sentiment steering, and repetition reduction, we show that
Click outperforms strong baselines of controllable text generation and
demonstrate the superiority of Click's sample construction strategy.",None,-1
"""Generate"" the Future of Work through AI: Empirical Evidence from Online Labor Markets",0.203312,"Large Language Model (LLM) based generative AI, such as ChatGPT, is
considered the first generation of Artificial General Intelligence (AGI),
exhibiting zero-shot learning abilities for a wide variety of downstream tasks.
Due to its general-purpose and emergent nature, its impact on labor dynamics
becomes complex and difficult to anticipate. Leveraging an extensive dataset
from a prominent online labor market, we uncover a post-ChatGPT decline in
labor demand, supply, and transactions for submarkets pertaining to
text-related and programming-related jobs, in comparison to those not directly
exposed to ChatGPT's core functionalities. Meanwhile, these affected submarkets
exhibit a discernible increase in the complexity of the remaining jobs and a
heightened level of competition among freelancers. Intriguingly, our findings
indicate that the diminution in the labor supply pertaining to programming is
comparatively less pronounced, a phenomenon ascribed to the transition of
freelancers previously engaged in text-related tasks now bidding for
programming-related opportunities. Although the per-period job diversity
freelancers apply for tends to be more limited, those who successfully navigate
skill transitions from text to programming demonstrate greater resilience to
ChatGPT's overall market contraction impact. As AI becomes increasingly
versatile and potent, our paper offers crucial insights into AI's influence on
labor markets and individuals' reactions, underscoring the necessity for
proactive interventions to address the challenges and opportunities presented
by this transformative technology.",None,-1
Few-Shot Rotation-Invariant Aerial Image Semantic Segmentation,0.157883,"Few-shot aerial image segmentation is a challenging task that involves
precisely parsing objects in query aerial images with limited annotated
support. Conventional matching methods without consideration of varying object
orientations can fail to activate same-category objects with different
orientations. Moreover, conventional algorithms can lead to false recognition
of lower-scored rotated semantic objects. In response to these challenges, the
authors propose a novel few-shot rotation-invariant aerial semantic
segmentation network (FRINet). FRINet matches each query feature
rotation-adaptively with orientation-varying yet category-consistent support
information. The segmentation predictions from different orientations are
supervised by the same label, and the backbones are pre-trained in the base
category to boost segmentation performance. Experimental results demonstrate
that FRINet achieves state-of-the-art performance in few-shot aerial semantic
segmentation benchmark.",None,-1
Chebyshev Particles,0.515897,"Markov chain Monte Carlo (MCMC) provides a feasible method for inferring
Hidden Markov models, however, it is often computationally prohibitive,
especially constrained by the curse of dimensionality, as the Monte Carlo
sampler traverses randomly taking small steps within uncertain regions in the
parameter space. We are the first to consider the posterior distribution of the
objective as a mapping of samples in an infinite-dimensional Euclidean space
where deterministic submanifolds are embedded and propose a new criterion by
maximizing the weighted Riesz polarization quantity, to discretize rectifiable
submanifolds via pairwise interaction. We study the characteristics of
Chebyshev particles and embed them into sequential MCMC, a novel sampler with a
high acceptance ratio that proposes only a few evaluations. We have achieved
high performance from the experiments for parameter inference in a linear
Gaussian state-space model with synthetic data and a non-linear stochastic
volatility model with real-world data.",None,-1
Rsum Parsing as Hierarchical Sequence Labeling: An Empirical Study,0.210938,"Extracting information from r\'esum\'es is typically formulated as a
two-stage problem, where the document is first segmented into sections and then
each section is processed individually to extract the target entities. Instead,
we cast the whole problem as sequence labeling in two levels -- lines and
tokens -- and study model architectures for solving both tasks simultaneously.
We build high-quality r\'esum\'e parsing corpora in English, French, Chinese,
Spanish, German, Portuguese, and Swedish. Based on these corpora, we present
experimental results that demonstrate the effectiveness of the proposed models
for the information extraction task, outperforming approaches introduced in
previous work. We conduct an ablation study of the proposed architectures. We
also analyze both model performance and resource efficiency, and describe the
trade-offs for model deployment in the context of a production environment.",None,-1
Automated multilingual detection of Pro-Kremlin propaganda in newspapers and Telegram posts,0.536058,"The full-scale conflict between the Russian Federation and Ukraine generated
an unprecedented amount of news articles and social media data reflecting
opposing ideologies and narratives. These polarized campaigns have led to
mutual accusations of misinformation and fake news, shaping an atmosphere of
confusion and mistrust for readers worldwide. This study analyses how the media
affected and mirrored public opinion during the first month of the war using
news articles and Telegram news channels in Ukrainian, Russian, Romanian and
English. We propose and compare two methods of multilingual automated
pro-Kremlin propaganda identification, based on Transformers and linguistic
features. We analyse the advantages and disadvantages of both methods, their
adaptability to new genres and languages, and ethical considerations of their
usage for content moderation. With this work, we aim to lay the foundation for
further development of moderation tools tailored to the current conflict.",None,-1
Pathway toward prior knowledge-integrated machine learning in engineering,0.265766,"Despite the digitalization trend and data volume surge, first-principles
models (also known as logic-driven, physics-based, rule-based, or
knowledge-based models) and data-driven approaches have existed in parallel,
mirroring the ongoing AI debate on symbolism versus connectionism. Research for
process development to integrate both sides to transfer and utilize domain
knowledge in the data-driven process is rare. This study emphasizes efforts and
prevailing trends to integrate multidisciplinary domain professions into
machine acknowledgeable, data-driven processes in a two-fold organization:
examining information uncertainty sources in knowledge representation and
exploring knowledge decomposition with a three-tier knowledge-integrated
machine learning paradigm. This approach balances holist and reductionist
perspectives in the engineering domain.",None,-1
Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,0.667992,"Synthesizing interaction-involved human motions has been challenging due to
the high complexity of 3D environments and the diversity of possible human
behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to
synthesize natural and plausible long-term human movements in complex indoor
environments. The key motivation of LAMA is to build a unified framework to
encompass a series of everyday motions including locomotion, scene interaction,
and object manipulation. Unlike existing methods that require motion data
""paired"" with scanned 3D scenes for supervision, we formulate the problem as a
test-time optimization by using human motion capture data only for synthesis.
LAMA leverages a reinforcement learning framework coupled with a motion
matching algorithm for optimization, and further exploits a motion editing
framework via manifold learning to cover possible variations in interaction and
manipulation. Throughout extensive experiments, we demonstrate that LAMA
outperforms previous approaches in synthesizing realistic motions in various
challenging scenarios. Project page: https://jiyewise.github.io/projects/LAMA/ .",None,-1
Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation,0.797734,"One of the mainstream schemes for 2D human pose estimation (HPE) is learning
keypoints heatmaps by a neural network. Existing methods typically improve the
quality of heatmaps by customized architectures, such as high-resolution
representation and vision Transformers. In this paper, we propose
\textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypoints
heatmaps generation problem from noised heatmaps. During training, the
keypoints are diffused to random distribution by adding noises and the
diffusion model learns to recover ground-truth heatmaps from noised heatmaps
with respect to conditions constructed by image feature. During inference, the
diffusion model generates heatmaps from initialized heatmaps in a progressive
denoising way. Moreover, we further explore improving the performance of
DiffusionPose with conditions from human structural information. Extensive
experiments show the prowess of our DiffusionPose, with improvements of 1.6,
1.2, and 1.2 mAP on widely-used COCO, CrowdPose, and AI Challenge datasets,
respectively.",None,-1
Theta sequences as eligibility traces: a biological solution to credit assignment,0.196686,"Credit assignment problems, for example policy evaluation in RL, often
require bootstrapping prediction errors through preceding states \textit{or}
maintaining temporally extended memory traces; solutions which are unfavourable
or implausible for biological networks of neurons. We propose theta sequences
-- chains of neural activity during theta oscillations in the hippocampus,
thought to represent rapid playthroughs of awake behaviour -- as a solution. By
analysing and simulating a model for theta sequences we show they compress
behaviour such that existing but short $\mathsf{O}(10)$ ms neuronal memory
traces are effectively extended allowing for bootstrap-free credit assignment
without long memory traces, equivalent to the use of eligibility traces in
TD($\lambda$).",None,-1
Bipartite Graph Diffusion Model for Human Interaction Generation,0.601755,"The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.",None,-1
Context-Aware Transformer for 3D Point Cloud Automatic Annotation,0.434075,"3D automatic annotation has received increased attention since manually
annotating 3D point clouds is laborious. However, existing methods are usually
complicated, e.g., pipelined training for 3D foreground/background
segmentation, cylindrical object proposals, and point completion. Furthermore,
they often overlook the inter-object feature relation that is particularly
informative to hard samples for 3D annotation. To this end, we propose a simple
yet effective end-to-end Context-Aware Transformer (CAT) as an automated 3D-box
labeler to generate precise 3D box annotations from 2D boxes, trained with a
small number of human annotations. We adopt the general encoder-decoder
architecture, where the CAT encoder consists of an intra-object encoder (local)
and an inter-object encoder (global), performing self-attention along the
sequence and batch dimensions, respectively. The former models intra-object
interactions among points, and the latter extracts feature relations among
different objects, thus boosting scene-level understanding. Via local and
global encoders, CAT can generate high-quality 3D box annotations with a
streamlined workflow, allowing it to outperform existing state-of-the-art by up
to 1.79% 3D AP on the hard task of the KITTI test set.",None,-1
Head Rotation in Denoising Diffusion Models,0.0761638,"Denoising Diffusion Models (DDM) are emerging as the cutting-edge technology
in the realm of deep generative modeling, challenging the dominance of
Generative Adversarial Networks. However, effectively exploring the latent
space's semantics and identifying compelling trajectories for manipulating and
editing important attributes of the generated samples remains challenging,
primarily due to the high-dimensional nature of the latent space. In this
study, we specifically concentrate on face rotation, which is known to be one
of the most intricate editing operations. By leveraging a recent embedding
technique for Denoising Diffusion Implicit Models (DDIM), we achieve, in many
cases, noteworthy manipulations encompassing a wide rotation angle of $\pm
30^o$, preserving the distinct characteristics of the individual. Our
methodology exploits the computation of trajectories approximating clouds of
latent representations of dataset samples with different yaw rotations through
linear regression. Specific trajectories are obtained by restricting the
analysis to subsets of data sharing significant attributes with the source
image. One of these attributes is the light provenance: a byproduct of our
research is a labeling of CelebA, categorizing images into three major groups
based on the illumination direction: left, center, and right.",None,-1
Automatic Engineering of Long Prompts,0.202886,"Large language models (LLMs) have demonstrated remarkable capabilities in
solving complex open-domain tasks, guided by comprehensive instructions and
demonstrations provided in the form of prompts. However, these prompts can be
lengthy, often comprising hundreds of lines and thousands of tokens, and their
design often requires considerable human effort. Recent research has explored
automatic prompt engineering for short prompts, typically consisting of one or
a few sentences. However, the automatic design of long prompts remains a
challenging problem due to its immense search space. In this paper, we
investigate the performance of greedy algorithms and genetic algorithms for
automatic long prompt engineering. We demonstrate that a simple greedy approach
with beam search outperforms other methods in terms of search efficiency.
Moreover, we introduce two novel techniques that utilize search history to
enhance the effectiveness of LLM-based mutation in our search algorithm. Our
results show that the proposed automatic long prompt engineering algorithm
achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard,
highlighting the significance of automating prompt designs to fully harness the
capabilities of LLMs.",None,-1
Recommending the optimal policy by learning to act from temporal data,0.168808,"Prescriptive Process Monitoring is a prominent problem in Process Mining,
which consists in identifying a set of actions to be recommended with the goal
of optimising a target measure of interest or Key Performance Indicator (KPI).
One challenge that makes this problem difficult is the need to provide
Prescriptive Process Monitoring techniques only based on temporally annotated
(process) execution data, stored in, so-called execution logs, due to the lack
of well crafted and human validated explicit models. In this paper we aim at
proposing an AI based approach that learns, by means of Reinforcement Learning
(RL), an optimal policy (almost) only from the observation of past executions
and recommends the best activities to carry on for optimizing a KPI of
interest. This is achieved first by learning a Markov Decision Process for the
specific KPIs from data, and then by using RL training to learn the optimal
policy. The approach is validated on real and synthetic datasets and compared
with off-policy Deep RL approaches. The ability of our approach to compare
with, and often overcome, Deep RL approaches provides a contribution towards
the exploitation of white box RL techniques in scenarios where only temporal
execution data are available.",None,-1
Revisiting Supertagging for HPSG,0.864665,"We present new supertaggers trained on HPSG-based treebanks. These treebanks
feature high-quality annotation based on a well-developed linguistic theory and
include diverse and challenging test datasets, beyond the usual WSJ section 23
and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based
models. We use SVM and neural CRF- and BERT-based methods and show that both
SVM and neural supertaggers achieve considerably higher accuracy compared to
the baseline. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 1000
sentences from WSJ23 and 93.88% on the completely out-of-domain The Cathedral
and the Bazaar (cb)). We conclude that it therefore makes sense to integrate
these new supertaggers into modern HPSG parsers, and we also hope that the
diverse and difficult datasets we used here will gain more popularity in the
field. We contribute the complete dataset reformatted for token classification.",None,-1
GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation,0.249409,"We present GENTLE, a new mixed-genre English challenge corpus totaling 17K
tokens and consisting of 8 unusual text types for out-of domain evaluation:
dictionary entries, esports commentaries, legal documents, medical notes,
poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually
annotated for a variety of popular NLP tasks, including syntactic dependency
parsing, entity recognition, coreference resolution, and discourse parsing. We
evaluate state-of-the-art NLP systems on GENTLE and find severe degradation for
at least some genres in their performance on all tasks, which indicates
GENTLE's utility as an evaluation dataset for NLP systems.",None,-1
VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions,0.15088,"Video-grounded dialogue understanding is a challenging problem that requires
machine to perceive, parse and reason over situated semantics extracted from
weakly aligned video and dialogues. Most existing benchmarks treat both
modalities the same as a frame-independent visual understanding task, while
neglecting the intrinsic attributes in multimodal dialogues, such as scene and
topic transitions. In this paper, we present Video-grounded Scene&Topic AwaRe
dialogue (VSTAR) dataset, a large scale video-grounded dialogue understanding
dataset based on 395 TV series. Based on VSTAR, we propose two benchmarks for
video-grounded dialogue understanding: scene segmentation and topic
segmentation, and one benchmark for video-grounded dialogue generation.
Comprehensive experiments are performed on these benchmarks to demonstrate the
importance of multimodal information and segments in video-grounded dialogue
understanding and generation.",None,-1
Incorporating Graph Information in Transformer-based AMR Parsing,0.767792,"Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that
aims at providing a semantic graph abstraction representing a given text.
Current approaches are based on autoregressive language models such as BART or
T5, fine-tuned through Teacher Forcing to obtain a linearized version of the
AMR graph from a sentence. In this paper, we present LeakDistill, a model and
method that explores a modification to the Transformer architecture, using
structural adapters to explicitly incorporate graph information into the
learned representations and improve AMR parsing performance. Our experiments
show how, by employing word-to-node alignment to embed graph structural
information into the encoder at training time, we can obtain state-of-the-art
AMR parsing through self-knowledge distillation, even without the use of
additional data. We release the code at
\url{http://www.github.com/sapienzanlp/LeakDistill}.",None,-1
V1T: large-scale mouse V1 response prediction using a Vision Transformer,0.535673,"Accurate predictive models of the visual cortex neural response to natural
visual stimuli remain a challenge in computational neuroscience. In this work,
we introduce V1T, a novel Vision Transformer based architecture that learns a
shared visual and behavioral representation across animals. We evaluate our
model on two large datasets recorded from mouse primary visual cortex and
outperform previous convolution-based models by more than 12.7% in prediction
performance. Moreover, we show that the self-attention weights learned by the
Transformer correlate with the population receptive fields. Our model thus sets
a new benchmark for neural response prediction and can be used jointly with
behavioral and neural recordings to reveal meaningful characteristic features
of the visual cortex.",None,-1
Progressive Prompts: Continual Learning for Language Models,0.987635,"We introduce Progressive Prompts - a simple and efficient approach for
continual learning in language models. Our method allows forward transfer and
resists catastrophic forgetting, without relying on data replay or a large
number of task-specific parameters. Progressive Prompts learns a new soft
prompt for each task and sequentially concatenates it with the previously
learned prompts, while keeping the base model frozen. Experiments on standard
continual learning benchmarks show that our approach outperforms
state-of-the-art methods, with an improvement >20% in average test accuracy
over the previous best-preforming method on T5 model. We also explore a more
challenging continual learning setup with longer sequences of tasks and show
that Progressive Prompts significantly outperforms prior methods.",None,-1
Exploration of Lightweight Single Image Denoising with Transformers and Truly Fair Training,0.135949,"As multimedia content often contains noise from intrinsic defects of digital
devices, image denoising is an important step for high-level vision recognition
tasks. Although several studies have developed the denoising field employing
advanced Transformers, these networks are too momory-intensive for real-world
applications. Additionally, there is a lack of research on lightweight denosing
(LWDN) with Transformers. To handle this, this work provides seven comparative
baseline Transformers for LWDN, serving as a foundation for future research. We
also demonstrate the parts of randomly cropped patches significantly affect the
denoising performances during training. While previous studies have overlooked
this aspect, we aim to train our baseline Transformers in a truly fair manner.
Furthermore, we conduct empirical analyses of various components to determine
the key considerations for constructing LWDN Transformers. Codes are available
at https://github.com/rami0205/LWDN.",None,-1
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios,0.900563,"The emergence of generative pre-trained models has facilitated the synthesis
of high-quality text, but it has also posed challenges in identifying factual
errors in the generated text. In particular: (1) A wider range of tasks now
face an increasing risk of containing factual errors when handled by generative
models. (2) Generated texts tend to be lengthy and lack a clearly defined
granularity for individual facts. (3) There is a scarcity of explicit evidence
available during the process of fact checking. With the above challenges in
mind, in this paper, we propose FacTool, a task and domain agnostic framework
for detecting factual errors of texts generated by large language models (e.g.,
ChatGPT). Experiments on four different tasks (knowledge-based QA, code
generation, mathematical reasoning, and scientific literature review) show the
efficacy of the proposed method. We release the code of FacTool associated with
ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",None,-1
Bridging the Gap between Structural and Semantic Similarity in Diverse Planning,0.235938,"Diverse planning is the problem of finding multiple plans for a given problem
specification, which is at the core of many real-world applications. For
example, diverse planning is a critical piece for the efficiency of plan
recognition systems when dealing with noisy and missing observations. Providing
diverse solutions can also benefit situations where constraints are too
expensive or impossible to model. Current diverse planners operate by
generating multiple plans and then applying a selection procedure to extract
diverse solutions using a similarity metric. Generally, current similarity
metrics only consider the structural properties of the given plans. We argue
that this approach is a limitation that sometimes prevents such metrics from
capturing why two plans differ. In this work, we propose two new
domain-independent metrics which are able to capture relevant information on
the difference between two given plans from a domain-dependent viewpoint. We
showcase their utility in various situations where the currently used metrics
fail to capture the similarity between plans, failing to capture some
structural symmetries.",None,-1
ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes,0.411352,"Understanding the continuous states of objects is essential for task learning
and planning in the real world. However, most existing task learning benchmarks
assume discrete (e.g., binary) object goal states, which poses challenges for
the learning of complex tasks and transferring learned policy from simulated
environments to the real world. Furthermore, state discretization limits a
robot's ability to follow human instructions based on the grounding of actions
and states. To tackle these challenges, we present ARNOLD, a benchmark that
evaluates language-grounded task learning with continuous states in realistic
3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve
understanding object states and learning policies for continuous goals. To
promote language-instructed learning, we provide expert demonstrations with
template-generated language descriptions. We assess task performance by
utilizing the latest language-conditioned policy learning models. Our results
indicate that current models for language-conditioned manipulations continue to
experience significant challenges in novel goal-state generalizations, scene
generalizations, and object generalizations. These findings highlight the need
to develop new algorithms that address this gap and underscore the potential
for further research in this area. Project website:
https://arnold-benchmark.github.io.",None,-1
"Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing",0.910484,"Despite increasing interest in the automatic detection of media frames in
NLP, the problem is typically simplified as single-label classification and
adopts a topic-like view on frames, evading modelling the broader
document-level narrative. In this work, we revisit a widely used
conceptualization of framing from the communication sciences which explicitly
captures elements of narratives, including conflict and its resolution, and
integrate it with the narrative framing of key entities in the story as heroes,
victims or villains. We adapt an effective annotation paradigm that breaks a
complex annotation task into a series of simpler binary questions, and present
an annotated data set of English news articles, and a case study on the framing
of climate change in articles from news outlets across the political spectrum.
Finally, we explore automatic multi-label prediction of our frames with
supervised and semi-supervised approaches, and present a novel retrieval-based
method which is both effective and transparent in its predictions. We conclude
with a discussion of opportunities and challenges for future work on
document-level models of narrative framing.",None,-1
Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning,0.385201,"Event temporal reasoning aims at identifying the temporal relations between
two or more events from narratives. However, knowledge conflicts arise when
there is a mismatch between the actual temporal relations of events in the
context and the prior knowledge or biases learned by the model. In this paper,
we propose to detect knowledge-conflict examples in event temporal reasoning
using bias indicators, which include event relation prior bias, tense bias,
narrative bias, and dependency bias. We define conflict examples as those where
event relations are opposite to biased or prior relations. To mitigate
event-related knowledge conflicts, we introduce a Counterfactual Data
Augmentation (CDA) based method that can be applied to both Pre-trained
Language Models (PLMs) and Large Language Models (LLMs) either as additional
training data or demonstrations for In-Context Learning. Experiments suggest
both PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning,
and CDA has the potential for reducing hallucination and improving model
performance.",None,-1
"Privacy in Large Language Models: Attacks, Defenses and Future Directions",0.850771,"The advancement of large language models (LLMs) has significantly enhanced
the ability to effectively tackle various downstream NLP tasks and unify these
tasks into generative pipelines. On the one hand, powerful language models,
trained on massive textual data, have brought unparalleled accessibility and
usability for both models and users. On the other hand, unrestricted access to
these models can also introduce potential malicious and unintentional privacy
risks. Despite ongoing efforts to address the safety and privacy concerns
associated with LLMs, the problem remains unresolved. In this paper, we provide
a comprehensive analysis of the current privacy attacks targeting LLMs and
categorize them according to the adversary's assumed capabilities to shed light
on the potential vulnerabilities present in LLMs. Then, we present a detailed
overview of prominent defense strategies that have been developed to counter
these privacy attacks. Beyond existing works, we identify upcoming privacy
concerns as LLMs evolve. Lastly, we point out several potential avenues for
future exploration.",None,-1
Multilingual Sentence Transformer as A Multilingual Word Aligner,0.41291,"Multilingual pretrained language models (mPLMs) have shown their
effectiveness in multilingual word alignment induction. However, these methods
usually start from mBERT or XLM-R. In this paper, we investigate whether
multilingual sentence Transformer LaBSE is a strong multilingual word aligner.
This idea is non-trivial as LaBSE is trained to learn language-agnostic
sentence-level embeddings, while the alignment extraction task requires the
more fine-grained word-level embeddings to be language-agnostic. We demonstrate
that the vanilla LaBSE outperforms other mPLMs currently used in the alignment
task, and then propose to finetune LaBSE on parallel corpus for further
improvement. Experiment results on seven language pairs show that our best
aligner outperforms previous state-of-the-art models of all varieties. In
addition, our aligner supports different language pairs in a single model, and
even achieves new state-of-the-art on zero-shot language pairs that does not
appear in the finetuning process.",None,-1
AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing,0.710102,"The intensive care unit (ICU) is a specialized hospital space where
critically ill patients receive intensive care and monitoring. Comprehensive
monitoring is imperative in assessing patients conditions, in particular
acuity, and ultimately the quality of care. However, the extent of patient
monitoring in the ICU is limited due to time constraints and the workload on
healthcare providers. Currently, visual assessments for acuity, including fine
details such as facial expressions, posture, and mobility, are sporadically
captured, or not captured at all. These manual observations are subjective to
the individual, prone to documentation errors, and overburden care providers
with the additional workload. Artificial Intelligence (AI) enabled systems has
the potential to augment the patient visual monitoring and assessment due to
their exceptional learning capabilities. Such systems require robust annotated
data to train. To this end, we have developed pervasive sensing and data
processing system which collects data from multiple modalities depth images,
color RGB images, accelerometry, electromyography, sound pressure, and light
levels in ICU for developing intelligent monitoring systems for continuous and
granular acuity, delirium risk, pain, and mobility assessment. This paper
presents the Intelligent Intensive Care Unit (I2CU) system architecture we
developed for real-time patient monitoring and visual assessment.",None,-1
PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration,0.400094,"Document-level relation extraction (DocRE) aims to extract relations of all
entity pairs in a document. A key challenge in DocRE is the cost of annotating
such data which requires intensive human effort. Thus, we investigate the case
of DocRE in a low-resource setting, and we find that existing models trained on
low data overestimate the NA (""no relation"") label, causing limited
performance. In this work, we approach the problem from a calibration
perspective and propose PRiSM, which learns to adapt logits based on relation
semantic information. We evaluate our method on three DocRE datasets and
demonstrate that integrating existing models with PRiSM improves performance by
as much as 26.38 F1 score, while the calibration error drops as much as 36
times when trained with about 3% of data. The code is publicly available at
https://github.com/brightjade/PRiSM.",None,-1
ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning,0.594645,"Confidence-based pseudo-labeling is among the dominant approaches in
semi-supervised learning (SSL). It relies on including high-confidence
predictions made on unlabeled data as additional targets to train the model. We
propose ProtoCon, a novel SSL method aimed at the less-explored label-scarce
SSL where such methods usually underperform. ProtoCon refines the pseudo-labels
by leveraging their nearest neighbours' information. The neighbours are
identified as the training proceeds using an online clustering approach
operating in an embedding space trained via a prototypical loss to encourage
well-formed clusters. The online nature of ProtoCon allows it to utilise the
label history of the entire dataset in one training cycle to refine labels in
the following cycle without the need to store image embeddings. Hence, it can
seamlessly scale to larger datasets at a low cost. Finally, ProtoCon addresses
the poor training signal in the initial phase of training (due to fewer
confident predictions) by introducing an auxiliary self-supervised loss. It
delivers significant gains and faster convergence over state-of-the-art across
5 datasets, including CIFARs, ImageNet and DomainNet.",None,-1
Learning Topology-Preserving Data Representations,0.841901,"We propose a method for learning topology-preserving data representations
(dimensionality reduction). The method aims to provide topological similarity
between the data manifold and its latent representation via enforcing the
similarity in topological features (clusters, loops, 2D voids, etc.) and their
localization. The core of the method is the minimization of the Representation
Topology Divergence (RTD) between original high-dimensional data and
low-dimensional representation in latent space. RTD minimization provides
closeness in topological features with strong theoretical guarantees. We
develop a scheme for RTD differentiation and apply it as a loss term for the
autoencoder. The proposed method ""RTD-AE"" better preserves the global structure
and topology of the data manifold than state-of-the-art competitors as measured
by linear correlation, triplet distance ranking accuracy, and Wasserstein
distance between persistence barcodes.",None,-1
Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding,0.678847,"To tackle the high inference latency exhibited by autoregressive language
models, previous studies have proposed an early-exiting framework that
allocates adaptive computation paths for each token based on the complexity of
generating the subsequent token. However, we observed several shortcomings,
including performance degradation caused by a state copying mechanism or
numerous exit paths, and sensitivity to exit confidence thresholds.
Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework,
which incorporates a shallow-deep module and a synchronized parallel decoding.
Our framework enables faster inference by synchronizing the decoding process of
the current token with previously stacked early-exited tokens. Furthermore, as
parallel decoding allows us to observe predictions from both shallow and deep
models, we present a novel adaptive threshold estimator that exploits a Beta
mixture model to determine suitable confidence thresholds. We empirically
demonstrated the superiority of our proposed framework on extensive generation
tasks.",None,-1
Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations,0.709483,"The abundance of instructional videos and their narrations over the Internet
offers an exciting avenue for understanding procedural activities. In this
work, we propose to learn video representation that encodes both action steps
and their temporal ordering, based on a large-scale dataset of web
instructional videos and their narrations, without using human annotations. Our
method jointly learns a video representation to encode individual step
concepts, and a deep probabilistic model to capture both temporal dependencies
and immense individual variations in the step ordering. We empirically
demonstrate that learning temporal ordering not only enables new capabilities
for procedure reasoning, but also reinforces the recognition of individual
steps. Our model significantly advances the state-of-the-art results on step
classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting
(+7.4% on COIN). Moreover, our model attains promising results in zero-shot
inference for step classification and forecasting, as well as in predicting
diverse and plausible steps for incomplete procedures. Our code is available at
https://github.com/facebookresearch/ProcedureVRL.",None,-1
I-PHYRE: Interactive Physical Reasoning,0.528876,"Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.",None,-1
Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation,0.843233,"Understanding and manipulating deformable objects (e.g., ropes and fabrics)
is an essential yet challenging task with broad applications. Difficulties come
from complex states and dynamics, diverse configurations and high-dimensional
action space of deformable objects. Besides, the manipulation tasks usually
require multiple steps to accomplish, and greedy policies may easily lead to
local optimal states. Existing studies usually tackle this problem using
reinforcement learning or imitating expert demonstrations, with limitations in
modeling complex states or requiring hand-crafted expert policies. In this
paper, we study deformable object manipulation using dense visual affordance,
with generalization towards diverse states, and propose a novel kind of
foresightful dense affordance, which avoids local optima by estimating states'
values for long-term manipulation. We propose a framework for learning this
representation, with novel designs such as multi-stage stable learning and
efficient self-supervised data collection without experts. Experiments
demonstrate the superiority of our proposed foresightful dense affordance.
Project page: https://hyperplane-lab.github.io/DeformableAffordance",None,-1
To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer,0.195499,"Choosing an appropriate tokenization scheme is often a bottleneck in
low-resource cross-lingual transfer. To understand the downstream implications
of text representation choices, we perform a comparative analysis on language
models having diverse text representation modalities including 2
segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model
(\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we
propose a scoring Language Quotient (LQ) metric capable of providing a weighted
representation of both zero-shot and few-shot evaluation combined. Utilizing
this metric, we perform experiments comprising 19 source languages and 133
target languages on three tasks (POS tagging, Dependency parsing, and NER). Our
analysis reveals that image-based models excel in cross-lingual transfer when
languages are closely related and share visually similar scripts. However, for
tasks biased toward word meaning (POS, NER), segmentation-based models prove to
be superior. Furthermore, in dependency parsing tasks where word relationships
play a crucial role, models with their character-level focus, outperform
others. Finally, we propose a recommendation scheme based on our findings to
guide model selection according to task and language requirements.",None,-1
Causal Explanations for Sequential Decision-Making in Multi-Agent Systems,0.20624,"We present CEMA: Causal Explanations in Multi-Agent systems; a framework for
creating causal natural language explanations of an agent's decisions in
dynamic sequential multi-agent systems to build more trustworthy autonomous
agents. Unlike prior work that assumes a fixed causal structure, CEMA only
requires a probabilistic model for forward-simulating the state of the system.
Using such a model, CEMA simulates counterfactual worlds that identify the
salient causes behind the agent's decisions. We evaluate CEMA on the task of
motion planning for autonomous driving and test it in diverse simulated
scenarios. We show that CEMA correctly and robustly identifies the causes
behind the agent's decisions, even when a large number of other agents is
present, and show via a user study that CEMA's explanations have a positive
effect on participants' trust in autonomous vehicles and are rated as high as
high-quality baseline explanations elicited from other participants. We release
the collected explanations with annotations as the HEADD dataset.",None,-1
SSSegmenation: An Open Source Supervised Semantic Segmentation Toolbox Based on PyTorch,0.138996,"This paper presents SSSegmenation, which is an open source supervised
semantic image segmentation toolbox based on PyTorch. The design of this
toolbox is motivated by MMSegmentation while it is easier to use because of
fewer dependencies and achieves superior segmentation performance under a
comparable training and testing setup. Moreover, the toolbox also provides
plenty of trained weights for popular and contemporary semantic segmentation
methods, including Deeplab, PSPNet, OCRNet, MaskFormer, \emph{etc}. We expect
that this toolbox can contribute to the future development of semantic
segmentation. Codes and model zoos are available at
\href{https://github.com/SegmentationBLWX/sssegmentation/}{SSSegmenation}.",None,-1
Collective Privacy Recovery: Data-sharing Coordination via Decentralized Artificial Intelligence,0.66407,"Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving >27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers.",None,-1
Implicit Chain of Thought Reasoning via Knowledge Distillation,0.279924,"To augment language models with the ability to reason, researchers usually
prompt or finetune them to produce chain of thought reasoning steps before
producing the final answer. However, although people use natural language to
reason effectively, it may be that LMs could reason more effectively with some
intermediate computation that is not in natural language. In this work, we
explore an alternative reasoning approach: instead of explicitly producing the
chain of thought reasoning steps, we use the language model's internal hidden
states to perform implicit reasoning. The implicit reasoning steps are
distilled from a teacher model trained on explicit chain-of-thought reasoning,
and instead of doing reasoning ""horizontally"" by producing intermediate words
one-by-one, we distill it such that the reasoning happens ""vertically"" among
the hidden states in different layers. We conduct experiments on a multi-digit
multiplication task and a grade school math problem dataset and find that this
approach enables solving tasks previously not solvable without explicit
chain-of-thought, at a speed comparable to no chain-of-thought.",None,-1
Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation,0.374643,"We study the problem of few-shot physically-aware articulated mesh
generation. By observing an articulated object dataset containing only a few
examples, we wish to learn a model that can generate diverse meshes with high
visual fidelity and physical validity. Previous mesh generative models either
have difficulties in depicting a diverse data space from only a few examples or
fail to ensure physical validity of their samples. Regarding the above
challenges, we propose two key innovations, including 1) a hierarchical mesh
deformation-based generative model based upon the divide-and-conquer philosophy
to alleviate the few-shot challenge by borrowing transferrable deformation
patterns from large scale rigid meshes and 2) a physics-aware deformation
correction scheme to encourage physically plausible generations. We conduct
extensive experiments on 6 articulated categories to demonstrate the
superiority of our method in generating articulated meshes with better
diversity, higher visual fidelity, and better physical validity over previous
methods in the few-shot setting. Further, we validate solid contributions of
our two innovations in the ablation study. Project page with code is available
at https://meowuu7.github.io/few-arti-obj-gen.",None,-1
Extending CLIP's Image-Text Alignment to Referring Image Segmentation,0.125981,"Referring Image Segmentation (RIS) is a cross-modal task that aims to segment
an instance described by a natural language expression. Recent methods leverage
large-scale pretrained unimodal models as backbones along with fusion
techniques for joint reasoning across modalities. However, the inherent
cross-modal nature of RIS raises questions about the effectiveness of unimodal
backbones. We propose RISCLIP, a novel framework that effectively leverages the
cross-modal nature of CLIP for RIS. Observing CLIP's inherent alignment between
image and text features, we capitalize on this starting point and introduce
simple but strong modules that enhance unimodal feature extraction and leverage
rich alignment knowledge in CLIP's image-text shared-embedding space. RISCLIP
exhibits outstanding results on all three major RIS benchmarks and also
outperforms previous CLIP-based methods, demonstrating the efficacy of our
strategy in extending CLIP's image-text alignment to RIS.",None,-1
TryOnDiffusion: A Tale of Two UNets,0.859621,"Given two images depicting a person and a garment worn by another person, our
goal is to generate a visualization of how the garment might look on the input
person. A key challenge is to synthesize a photorealistic detail-preserving
visualization of the garment, while warping the garment to accommodate a
significant body pose and shape change across the subjects. Previous methods
either focus on garment detail preservation without effective pose and shape
variation, or allow try-on with the desired shape and pose but lack garment
details. In this paper, we propose a diffusion-based architecture that unifies
two UNets (referred to as Parallel-UNet), which allows us to preserve garment
details and warp the garment for significant pose and body change in a single
network. The key ideas behind Parallel-UNet include: 1) garment is warped
implicitly via a cross attention mechanism, 2) garment warp and person blend
happen as part of a unified process as opposed to a sequence of two separate
tasks. Experimental results indicate that TryOnDiffusion achieves
state-of-the-art performance both qualitatively and quantitatively.",None,-1
FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue,0.0771802,"Pre-trained language models based on general text enable huge success in the
NLP scenario. But the intrinsical difference of linguistic patterns between
general text and task-oriented dialogues makes existing pre-trained language
models less useful in practice. Current dialogue pre-training methods rely on a
contrastive framework and face the challenges of both selecting true positives
and hard negatives. In this paper, we propose a novel dialogue pre-training
model, FutureTOD, which distills future knowledge to the representation of the
previous dialogue context using a self-training framework. Our intuition is
that a good dialogue representation both learns local context information and
predicts future information. Extensive experiments on diverse downstream
dialogue tasks demonstrate the effectiveness of our model, especially the
generalization, robustness, and learning discriminative dialogue
representations capabilities.",None,-1
Conceptual Views on Tree Ensemble Classifiers,0.334503,"Random Forests and related tree-based methods are popular for supervised
learning from table based data. Apart from their ease of parallelization, their
classification performance is also superior. However, this performance,
especially parallelizability, is offset by the loss of explainability.
Statistical methods are often used to compensate for this disadvantage. Yet,
their ability for local explanations, and in particular for global
explanations, is limited. In the present work we propose an algebraic method,
rooted in lattice theory, for the (global) explanation of tree ensembles. In
detail, we introduce two novel conceptual views on tree ensemble classifiers
and demonstrate their explanatory capabilities on Random Forests that were
trained with standard parameters.",None,-1
BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection,0.971284,"While most recent autonomous driving system focuses on developing perception
methods on ego-vehicle sensors, people tend to overlook an alternative approach
to leverage intelligent roadside cameras to extend the perception ability
beyond the visual range. We discover that the state-of-the-art vision-centric
bird's eye view detection methods have inferior performances on roadside
cameras. This is because these methods mainly focus on recovering the depth
regarding the camera center, where the depth difference between the car and the
ground quickly shrinks while the distance increases. In this paper, we propose
a simple yet effective approach, dubbed BEVHeight, to address this issue. In
essence, instead of predicting the pixel-wise depth, we regress the height to
the ground to achieve a distance-agnostic formulation to ease the optimization
process of camera-only perception methods. On popular 3D detection benchmarks
of roadside cameras, our method surpasses all previous vision-centric methods
by a significant margin. The code is available at
{\url{https://github.com/ADLab-AutoDrive/BEVHeight}}.",None,-1
Evaluating Inter-Bilingual Semantic Parsing for Indian Languages,0.178496,"Despite significant progress in Natural Language Generation for Indian
languages (IndicNLP), there is a lack of datasets around complex structured
tasks such as semantic parsing. One reason for this imminent gap is the
complexity of the logical form, which makes English to multilingual translation
difficult. The process involves alignment of logical forms, intents and slots
with translated unstructured utterance. To address this, we propose an
Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct
Indian languages. We highlight the proposed task's practicality, and evaluate
existing multilingual seq2seq models across several train-test strategies. Our
experiment reveals a high correlation across performance of original
multilingual semantic parsing datasets (such as mTOP, multilingual TOP and
multiATIS++) and our proposed IE-SEMPARSE suite.",None,-1
Transformed Protoform Reconstruction,0.0573069,"Protoform reconstruction is the task of inferring what morphemes or words
appeared like in the ancestral languages of a set of daughter languages. Meloni
et al. (2021) achieved the state-of-the-art on Latin protoform reconstruction
with an RNN-based encoder-decoder with attention model. We update their model
with the state-of-the-art seq2seq model: the Transformer. Our model outperforms
their model on a suite of different metrics on two different datasets: their
Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou
2004) of 800+ cognates spanning 39 varieties. We also probe our model for
potential phylogenetic signal contained in the model. Our code is publicly
available at https://github.com/cmu-llab/acl-2023.",None,-1
Pushing the Envelope for Depth-Based Semi-Supervised 3D Hand Pose Estimation with Consistency Training,0.290024,"Despite the significant progress that depth-based 3D hand pose estimation
methods have made in recent years, they still require a large amount of labeled
training data to achieve high accuracy. However, collecting such data is both
costly and time-consuming. To tackle this issue, we propose a semi-supervised
method to significantly reduce the dependence on labeled training data. The
proposed method consists of two identical networks trained jointly: a teacher
network and a student network. The teacher network is trained using both the
available labeled and unlabeled samples. It leverages the unlabeled samples via
a loss formulation that encourages estimation equivariance under a set of
affine transformations. The student network is trained using the unlabeled
samples with their pseudo-labels provided by the teacher network. For inference
at test time, only the student network is used. Extensive experiments
demonstrate that the proposed method outperforms the state-of-the-art
semi-supervised methods by large margins.",None,-1
Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision,0.339314,"Most existing task-oriented dialog (TOD) systems track dialog states in terms
of slots and values and use them to query a database to get relevant knowledge
to generate responses. In real-life applications, user utterances are noisier,
and thus it is more difficult to accurately track dialog states and correctly
secure relevant knowledge. Recently, a progress in question answering and
document-grounded dialog systems is retrieval-augmented methods with a
knowledge retriever. Inspired by such progress, we propose a retrieval-based
method to enhance knowledge selection in TOD systems, which significantly
outperforms the traditional database query method for real-life dialogs.
Further, we develop latent variable model based semi-supervised learning, which
can work with the knowledge retriever to leverage both labeled and unlabeled
dialog data. Joint Stochastic Approximation (JSA) algorithm is employed for
semi-supervised model training, and the whole system is referred to as that
JSA-KRTOD. Experiments are conducted on a real-life dataset from China Mobile
Custom-Service, called MobileCS, and show that JSA-KRTOD achieves superior
performances in both labeled-only and semi-supervised settings.",None,-1
Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving,0.357841,"Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous
vehicles (AVs) to make informed decisions and respond proactively in critical
road scenarios. Promising results of 3D HPE have been gained in several domains
such as human-computer interaction, robotics, sports and medical analytics,
often based on data collected in well-controlled laboratory environments.
Nevertheless, the transfer of 3D HPE methods to AVs has received limited
research attention, due to the challenges posed by obtaining accurate 3D pose
annotations and the limited suitability of data from other domains.
  We present a simple yet efficient weakly supervised approach for 3D HPE in
the AV context by employing a high-level sensor fusion between camera and LiDAR
data. The weakly supervised setting enables training on the target datasets
without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor
and pseudo labels generated from LiDAR to image projections. Our approach
outperforms state-of-the-art results by up to $\sim$ 13% on the Waymo Open
Dataset in the weakly supervised setting and achieves state-of-the-art results
in the supervised setting.",None,-1
Framework for Question-Answering in Sanskrit through Automated Construction of Knowledge Graphs,0.193018,"Sanskrit (sa\d{m}sk\d{r}ta) enjoys one of the largest and most varied
literature in the whole world. Extracting the knowledge from it, however, is a
challenging task due to multiple reasons including complexity of the language
and paucity of standard natural language processing tools. In this paper, we
target the problem of building knowledge graphs for particular types of
relationships from sa\d{m}sk\d{r}ta texts. We build a natural language
question-answering system in sa\d{m}sk\d{r}ta that uses the knowledge graph to
answer factoid questions. We design a framework for the overall system and
implement two separate instances of the system on human relationships from
mah\=abh\=arata and r\=am\=aya\d{n}a, and one instance on synonymous
relationships from bh\=avaprak\=a\'sa nigha\d{n}\d{t}u, a technical text from
\=ayurveda. We show that about 50% of the factoid questions can be answered
correctly by the system. More importantly, we analyse the shortcomings of the
system in detail for each step, and discuss the possible ways forward.",None,-1
The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features,0.451123,"AI systems have been known to amplify biases in real-world data. Explanations
may help human-AI teams address these biases for fairer decision-making.
Typically, explanations focus on salient input features. If a model is biased
against some protected group, explanations may include features that
demonstrate this bias, but when biases are realized through proxy features, the
relationship between this proxy feature and the protected one may be less clear
to a human. In this work, we study the effect of the presence of protected and
proxy features on participants' perception of model fairness and their ability
to improve demographic parity over an AI alone. Further, we examine how
different treatments -- explanations, model bias disclosure and proxy
correlation disclosure -- affect fairness perception and parity. We find that
explanations help people detect direct but not indirect biases. Additionally,
regardless of bias type, explanations tend to increase agreement with model
biases. Disclosures can help mitigate this effect for indirect biases,
improving both unfairness recognition and decision-making fairness. We hope
that our findings can help guide further research into advancing explanations
in support of fair human-AI decision-making.",None,-1
An Image Processing Pipeline for Autonomous Deep-Space Optical Navigation,0.440582,"A new era of space exploration and exploitation is fast approaching. A
multitude of spacecraft will flow in the future decades under the propulsive
momentum of the new space economy. Yet, the flourishing proliferation of
deep-space assets will make it unsustainable to pilot them from ground with
standard radiometric tracking. The adoption of autonomous navigation
alternatives is crucial to overcoming these limitations. Among these, optical
navigation is an affordable and fully ground-independent approach. Probes can
triangulate their position by observing visible beacons, e.g., planets or
asteroids, by acquiring their line-of-sight in deep space. To do so, developing
efficient and robust image processing algorithms providing information to
navigation filters is a necessary action. This paper proposes an innovative
pipeline for unresolved beacon recognition and line-of-sight extraction from
images for autonomous interplanetary navigation. The developed algorithm
exploits the k-vector method for the non-stellar object identification and
statistical likelihood to detect whether any beacon projection is visible in
the image. Statistical results show that the accuracy in detecting the planet
position projection is independent of the spacecraft position uncertainty.
Whereas, the planet detection success rate is higher than 95% when the
spacecraft position is known with a 3sigma accuracy up to 10^5 km.",None,-1
Polynomial Neural Fields for Subband Decomposition and Manipulation,0.346051,"Neural fields have emerged as a new paradigm for representing signals, thanks
to their ability to do it compactly while being easy to optimize. In most
applications, however, neural fields are treated like black boxes, which
precludes many signal manipulation tasks. In this paper, we propose a new class
of neural fields called polynomial neural fields (PNFs). The key advantage of a
PNF is that it can represent a signal as a composition of a number of
manipulable and interpretable components without losing the merits of neural
fields representation. We develop a general theoretical framework to analyze
and design PNFs. We use this framework to design Fourier PNFs, which match
state-of-the-art performance in signal representation tasks that use neural
fields. In addition, we empirically demonstrate that Fourier PNFs enable signal
manipulation applications such as texture transfer and scale-space
interpolation. Code is available at https://github.com/stevenygd/PNF.",None,-1
Causality Analysis for Evaluating the Security of Large Language Models,0.0470595,"Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted
in many safety-critical applications. Their security is thus essential. Even
with considerable efforts spent on reinforcement learning from human feedback
(RLHF), recent studies have shown that LLMs are still subject to attacks such
as adversarial perturbation and Trojan attacks. Further research is thus needed
to evaluate their security and/or understand the lack of it. In this work, we
propose a framework for conducting light-weight causality-analysis of LLMs at
the token, layer, and neuron level. We applied our framework to open-source
LLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based
on a layer-level causality analysis, we show that RLHF has the effect of
overfitting a model to harmful prompts. It implies that such security can be
easily overcome by `unusual' harmful prompts. As evidence, we propose an
adversarial perturbation method that achieves 100\% attack success rate on the
red-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we
show the existence of one mysterious neuron in both Llama2 and Vicuna that has
an unreasonably high causal effect on the output. While we are uncertain on why
such a neuron exists, we show that it is possible to conduct a ``Trojan''
attack targeting that particular neuron to completely cripple the LLM, i.e., we
can generate transferable suffixes to prompts that frequently make the LLM
produce meaningless responses.",None,-1
Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation,0.410868,"Few-shot question answering (QA) aims at precisely discovering answers to a
set of questions from context passages while only a few training samples are
available. Although existing studies have made some progress and can usually
achieve proper results, they suffer from understanding deep semantics for
reasoning out the questions. In this paper, we develop Gotta, a Generative
prOmpT-based daTa Augmentation framework to mitigate the challenge above.
Inspired by the human reasoning process, we propose to integrate the cloze task
to enhance few-shot QA learning. Following the recent success of prompt-tuning,
we present the cloze task in the same format as the main QA task, allowing the
model to learn both tasks seamlessly together to fully take advantage of the
power of prompt-tuning. Extensive experiments on widely used benchmarks
demonstrate that Gotta consistently outperforms competitive baselines,
validating the effectiveness of our proposed prompt-tuning-based cloze task,
which not only fine-tunes language models but also learns to guide reasoning in
QA tasks. Further analysis shows that the prompt-based loss incorporates the
auxiliary task better than the multi-task loss, highlighting the strength of
prompt-tuning on the few-shot QA task.",None,-1
Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,0.455341,"We present an end-to-end deep learning framework for indoor panoramic image
inpainting. Although previous inpainting methods have shown impressive
performance on natural perspective images, most fail to handle panoramic
images, particularly indoor scenes, which usually contain complex structure and
texture content. To achieve better inpainting quality, we propose to exploit
both the global and local context of indoor panorama during the inpainting
process. Specifically, we take the low-level layout edges estimated from the
input panorama as a prior to guide the inpainting model for recovering the
global indoor structure. A plane-aware normalization module is employed to
embed plane-wise style features derived from the layout into the generator,
encouraging local texture restoration from adjacent room structures (i.e.,
ceiling, floor, and walls). Experimental results show that our work outperforms
the current state-of-the-art methods on a public panoramic dataset in both
qualitative and quantitative evaluations. Our code is available at
https://ericsujw.github.io/LGPN-net/",None,-1
Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction,0.99957,"Legal syllogism is a form of deductive reasoning commonly used by legal
professionals to analyze cases. In this paper, we propose legal syllogism
prompting (LoT), a simple prompting method to teach large language models
(LLMs) for legal judgment prediction. LoT teaches only that in the legal
syllogism the major premise is law, the minor premise is the fact, and the
conclusion is judgment. Then the models can produce a syllogism reasoning of
the case and give the judgment without any learning, fine-tuning, or examples.
On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment
prediction experiments with GPT-3 models. Our results show that LLMs with LoT
achieve better performance than the baseline and chain of thought prompting,
the state-of-art prompting method on diverse reasoning tasks. LoT enables the
model to concentrate on the key information relevant to the judgment and to
correctly understand the legal meaning of acts, as compared to other methods.
Our method enables LLMs to predict judgment along with law articles and
justification, which significantly enhances the explainability of models.",None,-1
Exploring XAI for the Arts: Explaining Latent Space in Generative Music,0.505282,"Explainable AI has the potential to support more interactive and fluid
co-creative AI systems which can creatively collaborate with people. To do
this, creative AI models need to be amenable to debugging by offering
eXplainable AI (XAI) features which are inspectable, understandable, and
modifiable. However, currently there is very little XAI for the arts. In this
work, we demonstrate how a latent variable model for music generation can be
made more explainable; specifically we extend MeasureVAE which generates
measures of music. We increase the explainability of the model by: i) using
latent space regularisation to force some specific dimensions of the latent
space to map to meaningful musical attributes, ii) providing a user interface
feedback loop to allow people to adjust dimensions of the latent space and
observe the results of these changes in real-time, iii) providing a
visualisation of the musical attributes in the latent space to help people
understand and predict the effect of changes to latent space dimensions. We
suggest that in doing so we bridge the gap between the latent space and the
generated musical outcomes in a meaningful way which makes the model and its
outputs more explainable and more debuggable.",None,-1
Contextual Vision Transformers for Robust Representation Learning,0.393507,"We introduce Contextual Vision Transformers (ContextViT), a method designed
to generate robust image representations for datasets experiencing shifts in
latent factors across various groups. Derived from the concept of in-context
learning, ContextViT incorporates an additional context token to encapsulate
group-specific information. This integration allows the model to adjust the
image representation in accordance with the group-specific context.
Specifically, for a given input image, ContextViT maps images with identical
group membership into this context token, which is appended to the input image
tokens. Additionally, we introduce a context inference network to predict such
tokens on-the-fly, given a batch of samples from the group. This enables
ContextViT to adapt to new testing distributions during inference time. We
demonstrate the efficacy of ContextViT across a wide range of applications. In
supervised fine-tuning, we show that augmenting pre-trained ViTs with our
proposed context conditioning mechanism results in consistent improvements in
out-of-distribution generalization on iWildCam and FMoW. We also investigate
self-supervised representation learning with ContextViT. Our experiments on the
Camelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging
benchmark demonstrate that ContextViT excels in learning stable image
featurizations amidst distribution shift, consistently outperforming its ViT
counterpart.",None,-1
Deep Learning based Multi-Label Image Classification of Protest Activities,0.323237,"With the rise of internet technology amidst increasing rates of urbanization,
sharing information has never been easier thanks to globally-adopted platforms
for digital communication. The resulting output of massive amounts of
user-generated data can be used to enhance our understanding of significant
societal issues particularly for urbanizing areas. In order to better analyze
protest behavior, we enhanced the GSR dataset and manually labeled all the
images. We used deep learning techniques to analyze social media data to detect
social unrest through image classification, which performed good in predict
multi-attributes, then also used map visualization to display protest behaviors
across the country.",None,-1
Image Super-Resolution using Efficient Striped Window Transformer,0.306799,"Transformers have achieved remarkable results in single-image
super-resolution (SR). However, the challenge of balancing model performance
and complexity has hindered their application in lightweight SR (LSR). To
tackle this challenge, we propose an efficient striped window transformer
(ESWT). We revisit the normalization layer in the transformer and design a
concise and efficient transformer structure to build the ESWT. Furthermore, we
introduce a striped window mechanism to model long-term dependencies more
efficiently. To fully exploit the potential of the ESWT, we propose a novel
flexible window training strategy that can improve the performance of the ESWT
without additional cost. Extensive experiments show that ESWT outperforms
state-of-the-art LSR transformers, and achieves a better trade-off between
model performance and complexity. The ESWT requires fewer parameters, incurs
faster inference, smaller FLOPs, and less memory consumption, making it a
promising solution for LSR.",None,-1
Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2) for KNN models,0.19746,"With the rapid growth of data availability and usage, quantifying the added
value of each training data point has become a crucial process in the field of
artificial intelligence. The Shapley values have been recognized as an
effective method for data valuation, enabling efficient training set
summarization, acquisition, and outlier removal. In this paper, we introduce
""STI-KNN"", an innovative algorithm that calculates the exact pair-interaction
Shapley values for KNN models in O(t n^2) time, which is a significant
improvement over the O(2^n)$ time complexity of baseline methods. By using
STI-KNN, we can efficiently and accurately evaluate the value of individual
data points, leading to improved training outcomes and ultimately enhancing the
effectiveness of artificial intelligence applications.",None,-1
Hallucination is the last thing you need,0.647517,"The legal profession necessitates a multidimensional approach that involves
synthesizing an in-depth comprehension of a legal issue with insightful
commentary based on personal experience, combined with a comprehensive
understanding of pertinent legislation, regulation, and case law, in order to
deliver an informed legal solution. The present offering with generative AI
presents major obstacles in replicating this, as current models struggle to
integrate and navigate such a complex interplay of understanding, experience,
and fact-checking procedures. It is noteworthy that where generative AI outputs
understanding and experience, which reflect the aggregate of various subjective
views on similar topics, this often deflects the model's attention from the
crucial legal facts, thereby resulting in hallucination. Hence, this paper
delves into the feasibility of three independent LLMs, each focused on
understanding, experience, and facts, synthesising as one single ensemble model
to effectively counteract the current challenges posed by the existing
monolithic generative AI models. We introduce an idea of mutli-length
tokenisation to protect key information assets like common law judgements, and
finally we interrogate the most advanced publicly available models for legal
hallucination, with some interesting results.",None,-1
Exploring Invariant Representation for Visible-Infrared Person Re-Identification,0.738437,"Cross-spectral person re-identification, which aims to associate identities
to pedestrians across different spectra, faces a main challenge of the modality
discrepancy. In this paper, we address the problem from both image-level and
feature-level in an end-to-end hybrid learning framework named robust feature
mining network (RFM). In particular, we observe that the reflective intensity
of the same surface in photos shot in different wavelengths could be
transformed using a linear model. Besides, we show the variable linear factor
across the different surfaces is the main culprit which initiates the modality
discrepancy. We integrate such a reflection observation into an image-level
data augmentation by proposing the linear transformation generator (LTG).
Moreover, at the feature level, we introduce a cross-center loss to explore a
more compact intra-class distribution and modality-aware spatial attention to
take advantage of textured regions more efficiently. Experiment results on two
standard cross-spectral person re-identification datasets, i.e., RegDB and
SYSU-MM01, have demonstrated state-of-the-art performance.",None,-1
Backdoor Defense via Deconfounded Representation Learning,0.763817,"Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor
attacks, where attackers embed hidden backdoors in the DNN model by injecting a
few poisoned examples into the training dataset. While extensive efforts have
been made to detect and remove backdoors from backdoored DNNs, it is still not
clear whether a backdoor-free clean model can be directly obtained from
poisoned datasets. In this paper, we first construct a causal graph to model
the generation process of poisoned data and find that the backdoor attack acts
as the confounder, which brings spurious associations between the input images
and target labels, making the model predictions less reliable. Inspired by the
causal understanding, we propose the Causality-inspired Backdoor Defense (CBD),
to learn deconfounded representations for reliable classification.
Specifically, a backdoored model is intentionally trained to capture the
confounding effects. The other clean model dedicates to capturing the desired
causal effects by minimizing the mutual information with the confounding
representations from the backdoored model and employing a sample-wise
re-weighting scheme. Extensive experiments on multiple benchmark datasets
against 6 state-of-the-art attacks verify that our proposed defense method is
effective in reducing backdoor threats while maintaining high accuracy in
predicting benign samples. Further analysis shows that CBD can also resist
potential adaptive attacks. The code is available at
\url{https://github.com/zaixizhang/CBD}.",None,-1
Instant Continual Learning of Neural Radiance Fields,0.349591,"Neural radiance fields (NeRFs) have emerged as an effective method for
novel-view synthesis and 3D scene reconstruction. However, conventional
training methods require access to all training views during scene
optimization. This assumption may be prohibitive in continual learning
scenarios, where new data is acquired in a sequential manner and a continuous
update of the NeRF is desired, as in automotive or remote sensing applications.
When naively trained in such a continual setting, traditional scene
representation frameworks suffer from catastrophic forgetting, where previously
learned knowledge is corrupted after training on new data. Prior works in
alleviating forgetting with NeRFs suffer from low reconstruction quality and
high latency, making them impractical for real-world application. We propose a
continual learning framework for training NeRFs that leverages replay-based
methods combined with a hybrid explicit--implicit scene representation. Our
method outperforms previous methods in reconstruction quality when trained in a
continual setting, while having the additional benefit of being an order of
magnitude faster.",None,-1
f-Divergence Minimization for Sequence-Level Knowledge Distillation,0.928757,"Knowledge distillation (KD) is the process of transferring knowledge from a
large model to a small one. It has gained increasing attention in the natural
language processing community, driven by the demands of compressing
ever-growing language models. In this work, we propose an f-DISTILL framework,
which formulates sequence-level knowledge distillation as minimizing a
generalized f-divergence function. We propose four distilling variants under
our framework and show that existing SeqKD and ENGINE approaches are
approximations of our f-DISTILL methods. We further derive step-wise
decomposition for our f-DISTILL, reducing intractable sequence-level divergence
to word-level losses that can be computed in a tractable manner. Experiments
across four datasets show that our methods outperform existing KD approaches,
and that our symmetric distilling losses can better force the student to learn
from the teacher distribution.",None,-1
Erasing Concepts from Diffusion Models,0.849365,"Motivated by recent advancements in text-to-image diffusion, we study erasure
of specific concepts from the model's weights. While Stable Diffusion has shown
promise in producing explicit or realistic artwork, it has raised concerns
regarding its potential for misuse. We propose a fine-tuning method that can
erase a visual concept from a pre-trained diffusion model, given only the name
of the style and using negative guidance as a teacher. We benchmark our method
against previous approaches that remove sexually explicit content and
demonstrate its effectiveness, performing on par with Safe Latent Diffusion and
censored training. To evaluate artistic style removal, we conduct experiments
erasing five modern artists from the network and conduct a user study to assess
the human perception of the removed styles. Unlike previous methods, our
approach can remove concepts from a diffusion model permanently rather than
modifying the output at the inference time, so it cannot be circumvented even
if a user has access to model weights. Our code, data, and results are
available at https://erasing.baulab.info/",None,-1
Better Diffusion Models Further Improve Adversarial Training,0.999596,"It has been recognized that the data generated by the denoising diffusion
probabilistic model (DDPM) improves adversarial training. After two years of
rapid development in diffusion models, a question naturally arises: can better
diffusion models further improve adversarial training? This paper gives an
affirmative answer by employing the most recent diffusion model which has
higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID
score) compared with DDPM. Our adversarially trained models achieve
state-of-the-art performance on RobustBench using only generated data (no
external datasets). Under the $\ell_\infty$-norm threat model with
$\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on
CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous
state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm
threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10
($+4.44\%$). These results also beat previous works that use external data. We
also provide compelling results on the SVHN and TinyImageNet datasets. Our code
is available at https://github.com/wzekai99/DM-Improves-AT.",None,-1
Multi-modal Hate Speech Detection using Machine Learning,0.409784,"With the continuous growth of internet users and media content, it is very
hard to track down hateful speech in audio and video. Converting video or audio
into text does not detect hate speech accurately as human sometimes uses
hateful words as humorous or pleasant in sense and also uses different voice
tones or show different action in the video. The state-ofthe-art hate speech
detection models were mostly developed on a single modality. In this research,
a combined approach of multimodal system has been proposed to detect hate
speech from video contents by extracting feature images, feature values
extracted from the audio, text and used machine learning and Natural language
processing.",None,-1
Boosting Cross-lingual Transferability in Multilingual Models via In-Context Learning,0.277619,"Existing cross-lingual transfer (CLT) prompting methods are only concerned
with monolingual demonstration examples in the source language. In this paper,
we propose In-CLT, a novel cross-lingual transfer prompting method that
leverages both source and target languages to construct the demonstration
examples. We conduct comprehensive evaluations on multilingual benchmarks,
focusing on question answering tasks. Experiment results show that In-CLT
prompt not only improves multilingual models' cross-lingual transferability,
but also demonstrates remarkable unseen language generalization ability. In-CLT
prompting, in particular, improves model performance by 10 to 20\% points on
average when compared to prior cross-lingual transfer approaches. We also
observe the surprising performance gain on the other multilingual benchmarks,
especially in reasoning tasks. Furthermore, we investigate the relationship
between lexical similarity and pre-training corpora in terms of the
cross-lingual transfer gap.",None,-1
"Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks",0.274104,"Deep neural networks (DNNs) are often trained on the premise that the
complete training data set is provided ahead of time. However, in real-world
scenarios, data often arrive in chunks over time. This leads to important
considerations about the optimal strategy for training DNNs, such as whether to
fine-tune them with each chunk of incoming data (warm-start) or to retrain them
from scratch with the entire corpus of data whenever a new chunk is available.
While employing the latter for training can be resource-intensive, recent work
has pointed out the lack of generalization in warm-start models. Therefore, to
strike a balance between efficiency and generalization, we introduce Learn,
Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE
interchanges between the unlearning phase, which selectively forgets the
undesirable information in the model through weight reinitialization in a
data-dependent manner, and the relearning phase, which emphasizes learning on
generalizable features. We show that our training paradigm provides consistent
performance gains across datasets in both classification and few-shot settings.
We further show that it leads to more robust and well-calibrated models.",None,-1
A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors,0.576342,"Deployment of Reinforcement Learning (RL) algorithms for robotics
applications in the real world requires ensuring the safety of the robot and
its environment. Safe Robot RL (SRRL) is a crucial step towards achieving
human-robot coexistence. In this paper, we envision a human-centered SRRL
framework consisting of three stages: safe exploration, safety value alignment,
and safe collaboration. We examine the research gaps in these areas and propose
to leverage interactive behaviors for SRRL. Interactive behaviors enable
bi-directional information transfer between humans and robots, such as
conversational robot ChatGPT. We argue that interactive behaviors need further
attention from the SRRL community. We discuss four open challenges related to
the robustness, efficiency, transparency, and adaptability of SRRL with
interactive behaviors.",None,-1
Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs,0.438003,"Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency
multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.
However, MLPs rely exclusively on the node features and fail to capture the
graph structural information. Previous methods address this issue by processing
graph edges into extra inputs for MLPs, but such graph structures may be
unavailable for various scenarios. To this end, we propose a Prototype-Guided
Knowledge Distillation~(PGKD) method, which does not require graph
edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the
graph structural information in GNN teachers, and distill such information from
GNNs to MLPs via prototypes in an edge-free setting. Experimental results on
popular graph benchmarks demonstrate the effectiveness and robustness of the
proposed PGKD.",None,-1
Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images,0.990278,"Photos serve as a way for humans to record what they experience in their
daily lives, and they are often regarded as trustworthy sources of information.
However, there is a growing concern that the advancement of artificial
intelligence (AI) technology may produce fake photos, which can create
confusion and diminish trust in photographs. This study aims to comprehensively
evaluate agents for distinguishing state-of-the-art AI-generated visual
content. Our study benchmarks both human capability and cutting-edge fake image
detection AI algorithms, using a newly collected large-scale fake image dataset
Fake2M. In our human perception evaluation, titled HPBench, we discovered that
humans struggle significantly to distinguish real photos from AI-generated
ones, with a misclassification rate of 38.7%. Along with this, we conduct the
model capability of AI-Generated images detection evaluation MPBench and the
top-performing model from MPBench achieves a 13% failure rate under the same
setting used in the human evaluation. We hope that our study can raise
awareness of the potential risks of AI-generated images and facilitate further
research to prevent the spread of false information. More information can refer
to https://github.com/Inf-imagine/Sentry.",None,-1
Language-guided Human Motion Synthesis with Atomic Actions,0.343947,"Language-guided human motion synthesis has been a challenging task due to the
inherent complexity and diversity of human behaviors. Previous methods face
limitations in generalization to novel actions, often resulting in unrealistic
or incoherent motion sequences. In this paper, we propose ATOM (ATomic mOtion
Modeling) to mitigate this problem, by decomposing actions into atomic actions,
and employing a curriculum learning strategy to learn atomic action
composition. First, we disentangle complex human motions into a set of atomic
actions during learning, and then assemble novel actions using the learned
atomic actions, which offers better adaptability to new actions. Moreover, we
introduce a curriculum learning training strategy that leverages masked motion
modeling with a gradual increase in the mask ratio, and thus facilitates atomic
action assembly. This approach mitigates the overfitting problem commonly
encountered in previous methods while enforcing the model to learn better
motion representations. We demonstrate the effectiveness of ATOM through
extensive experiments, including text-to-motion and action-to-motion synthesis
tasks. We further illustrate its superiority in synthesizing plausible and
coherent text-guided human motion sequences.",None,-1
Colo-SCRL: Self-Supervised Contrastive Representation Learning for Colonoscopic Video Retrieval,0.608015,"Colonoscopic video retrieval, which is a critical part of polyp treatment,
has great clinical significance for the prevention and treatment of colorectal
cancer. However, retrieval models trained on action recognition datasets
usually produce unsatisfactory retrieval results on colonoscopic datasets due
to the large domain gap between them. To seek a solution to this problem, we
construct a large-scale colonoscopic dataset named Colo-Pair for medical
practice. Based on this dataset, a simple yet effective training method called
Colo-SCRL is proposed for more robust representation learning. It aims to
refine general knowledge from colonoscopies through masked autoencoder-based
reconstruction and momentum contrast to improve retrieval performance. To the
best of our knowledge, this is the first attempt to employ the contrastive
learning paradigm for medical video retrieval. Empirical results show that our
method significantly outperforms current state-of-the-art methods in the
colonoscopic video retrieval task.",None,-1
A recommender for the management of chronic pain in patients undergoing spinal cord stimulation,0.426041,"Spinal cord stimulation (SCS) is a therapeutic approach used for the
management of chronic pain. It involves the delivery of electrical impulses to
the spinal cord via an implanted device, which when given suitable stimulus
parameters can mask or block pain signals. Selection of optimal stimulation
parameters usually happens in the clinic under the care of a provider whereas
at-home SCS optimization is managed by the patient. In this paper, we propose a
recommender system for the management of pain in chronic pain patients
undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB)
approach to develop a system that recommends SCS settings to patients with the
aim of improving their condition. These recommendations, sent directly to
patients though a digital health ecosystem, combined with a patient monitoring
system closes the therapeutic loop around a chronic pain patient over their
entire patient journey. We evaluated the system in a cohort of SCS-implanted
ENVISION study subjects (Clinicaltrials.gov ID: NCT03240588) using a
combination of quality of life metrics and Patient States (PS), a novel measure
of holistic outcomes. SCS recommendations provided statistically significant
improvement in clinical outcomes (pain and/or QoL) in 85\% of all subjects
(N=21). Among subjects in moderate PS (N=7) prior to receiving recommendations,
100\% showed statistically significant improvements and 5/7 had improved PS
dwell time. This analysis suggests SCS patients may benefit from SCS
recommendations, resulting in additional clinical improvement on top of
benefits already received from SCS therapy.",None,-1
HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution,0.872119,"The rise of large language models (LLMs) had a transformative impact on
search, ushering in a new era of search engines that are capable of generating
search results in natural language text, imbued with citations for supporting
sources. Building generative information-seeking models demands openly
accessible datasets, which currently remain lacking. In this paper, we
introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative
Retrieval for Information-seeking Dataset) for building end-to-end generative
information-seeking models that are capable of retrieving candidate quotes and
generating attributed explanations. Unlike recent efforts that focus on human
evaluation of black-box proprietary search engines, we built our dataset atop
the English subset of MIRACL, a publicly available information retrieval
dataset. HAGRID is constructed based on human and LLM collaboration. We first
automatically collect attributed explanations that follow an in-context
citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to
evaluate the LLM explanations based on two criteria: informativeness and
attributability. HAGRID serves as a catalyst for the development of
information-seeking models with better attribution capabilities.",None,-1
ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation,0.360195,"This paper presents a new mechanism to facilitate the training of mask
transformers for efficient panoptic segmentation, democratizing its deployment.
We observe that due to its high complexity, the training objective of panoptic
segmentation will inevitably lead to much higher false positive penalization.
Such unbalanced loss makes the training process of the end-to-end
mask-transformer based architectures difficult, especially for efficient
models. In this paper, we present ReMaX that adds relaxation to mask
predictions and class predictions during training for panoptic segmentation. We
demonstrate that via these simple relaxation techniques during training, our
model can be consistently improved by a clear margin \textbf{without} any extra
computational cost on inference. By combining our method with efficient
backbones like MobileNetV3-Small, our method achieves new state-of-the-art
results for efficient panoptic segmentation on COCO, ADE20K and Cityscapes.
Code and pre-trained checkpoints will be available at
\url{https://github.com/google-research/deeplab2}.",None,-1
Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation,0.483877,"The objective of this paper is self-supervised learning of video object
segmentation. We develop a unified framework which simultaneously models
cross-frame dense correspondence for locally discriminative feature learning
and embeds object-level context for target-mask decoding. As a result, it is
able to directly learn to perform mask-guided sequential segmentation from
unlabeled videos, in contrast to previous efforts usually relying on an oblique
solution - cheaply ""copying"" labels according to pixel-wise correlations.
Concretely, our algorithm alternates between i) clustering video pixels for
creating pseudo segmentation labels ex nihilo; and ii) utilizing the pseudo
labels to learn mask encoding and decoding for VOS. Unsupervised correspondence
learning is further incorporated into this self-taught, mask embedding scheme,
so as to ensure the generic nature of the learnt representation and avoid
cluster degeneracy. Our algorithm sets state-of-the-arts on two standard
benchmarks (i.e., DAVIS17 and YouTube-VOS), narrowing the gap between self- and
fully-supervised VOS, in terms of both performance and network architecture
design.",None,-1
Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations,0.55298,"Discourse analysis is an important task because it models intrinsic semantic
structures between sentences in a document. Discourse markers are natural
representations of discourse in our daily language. One challenge is that the
markers as well as pre-defined and human-labeled discourse relations can be
ambiguous when describing the semantics between sentences. We believe that a
better approach is to use a contextual-dependent distribution over the markers
to express discourse information. In this work, we propose to learn a
Distributed Marker Representation (DMR) by utilizing the (potentially)
unlimited discourse marker data with a latent discourse sense, thereby bridging
markers with sentence pairs. Such representations can be learned automatically
from data without supervision, and in turn provide insights into the data
itself. Experiments show the SOTA performance of our DMR on the implicit
discourse relation recognition task and strong interpretability. Our method
also offers a valuable tool to understand complex ambiguity and entanglement
among discourse markers and manually defined discourse relations.",None,-1
RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer,0.60886,"Non-autoregressive neural machine translation (NAT) models are proposed to
accelerate the inference process while maintaining relatively high performance.
However, existing NAT models are difficult to achieve the desired
efficiency-quality trade-off. For one thing, fully NAT models with efficient
inference perform inferior to their autoregressive counterparts. For another,
iterative NAT models can, though, achieve comparable performance while
diminishing the advantage of speed. In this paper, we propose RenewNAT, a
flexible framework with high efficiency and effectiveness, to incorporate the
merits of fully and iterative NAT models. RenewNAT first generates the
potential translation results and then renews them in a single pass. It can
achieve significant performance improvements at the same expense as traditional
NAT models (without introducing additional model parameters and decoding
latency). Experimental results on various translation benchmarks (e.g.,
\textbf{4} WMT) show that our framework consistently improves the performance
of strong fully NAT methods (e.g., GLAT and DSLP) without additional speed
overhead.",None,-1
FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,0.878276,"Few-shot semantic segmentation is the task of learning to locate each pixel
of the novel class in the query image with only a few annotated support images.
The current correlation-based methods construct pair-wise feature correlations
to establish the many-to-many matching because the typical prototype-based
approaches cannot learn fine-grained correspondence relations. However, the
existing methods still suffer from the noise contained in naive correlations
and the lack of context semantic information in correlations. To alleviate
these problems mentioned above, we propose a Feature-Enhanced Context-Aware
Network (FECANet). Specifically, a feature enhancement module is proposed to
suppress the matching noise caused by inter-class local similarity and enhance
the intra-class relevance in the naive correlation. In addition, we propose a
novel correlation reconstruction module that encodes extra correspondence
relations between foreground and background and multi-scale context semantic
features, significantly boosting the encoder to capture a reliable matching
pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that
our proposed FECANet leads to remarkable improvement compared to previous
state-of-the-arts, demonstrating its effectiveness.",None,-1
EVA-CLIP: Improved Training Techniques for CLIP at Scale,0.999963,"Contrastive language-image pre-training, CLIP for short, has gained
increasing attention for its potential in various scenarios. In this paper, we
propose EVA-CLIP, a series of models that significantly improve the efficiency
and effectiveness of CLIP training. Our approach incorporates new techniques
for representation learning, optimization, and augmentation, enabling EVA-CLIP
to achieve superior performance compared to previous CLIP models with the same
number of parameters but significantly smaller training costs. Notably, our
largest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples
achieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller
EVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples
achieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open
access and open research, we release the complete suite of EVA-CLIP to the
community at https://github.com/baaivision/EVA/tree/master/EVA-CLIP.",None,-1
Spanish Resource Grammar version 2023,0.123305,"We present the latest version of the Spanish Resource Grammar (SRG), a
grammar of Spanish implemented in the HPSG formalism. Such grammars encode a
complex set of hypotheses about syntax making them a resource for empirical
testing of linguistic theory. They also encode a strict notion of
grammaticality which makes them a resource for natural language processing
applications in computer-assisted language learning. This version of the SRG
uses the recent version of the Freeling morphological analyzer and is released
along with an automatically created, manually verified treebank of 2,291
sentences. We explain the treebanking process, emphasizing how it is different
from treebanking with manual annotation and how it contributes to
empirically-driven development of syntactic theory. The treebanks' high level
of consistency and detail makes them a resource for training high-quality
semantic parsers and generally systems that benefit from precise and detailed
semantics. Finally, we present the grammar's coverage and overgeneration on 100
sentences from a learner corpus, a new research line related to developing
methodologies for robust empirical evaluation of hypotheses in second language
acquisition.",None,-1
Steerable Equivariant Representation Learning,0.104248,"Pre-trained deep image representations are useful for post-training tasks
such as classification through transfer learning, image retrieval, and object
detection. Data augmentations are a crucial aspect of pre-training robust
representations in both supervised and self-supervised settings. Data
augmentations explicitly or implicitly promote invariance in the embedding
space to the input image transformations. This invariance reduces
generalization to those downstream tasks which rely on sensitivity to these
particular data augmentations. In this paper, we propose a method of learning
representations that are instead equivariant to data augmentations. We achieve
this equivariance through the use of steerable representations. Our
representations can be manipulated directly in embedding space via learned
linear maps. We demonstrate that our resulting steerable and equivariant
representations lead to better performance on transfer learning and robustness:
e.g. we improve linear probe top-1 accuracy by between 1% to 3% for transfer;
and ImageNet-C accuracy by upto 3.4%. We further show that the steerability of
our representations provides significant speedup (nearly 50x) for test-time
augmentations; by applying a large number of augmentations for
out-of-distribution detection, we significantly improve OOD AUC on the
ImageNet-C dataset over an invariant representation.",None,-1
SwinFSR: Stereo Image Super-Resolution using SwinIR and Frequency Domain Knowledge,0.542926,"Stereo Image Super-Resolution (stereoSR) has attracted significant attention
in recent years due to the extensive deployment of dual cameras in mobile
phones, autonomous vehicles and robots. In this work, we propose a new StereoSR
method, named SwinFSR, based on an extension of SwinIR, originally designed for
single image restoration, and the frequency domain knowledge obtained by the
Fast Fourier Convolution (FFC). Specifically, to effectively gather global
information, we modify the Residual Swin Transformer blocks (RSTBs) in SwinIR
by explicitly incorporating the frequency domain knowledge using the FFC and
employing the resulting residual Swin Fourier Transformer blocks (RSFTBs) for
feature extraction. Besides, for the efficient and accurate fusion of stereo
views, we propose a new cross-attention module referred to as RCAM, which
achieves highly competitive performance while requiring less computational cost
than the state-of-the-art cross-attention modules. Extensive experimental
results and ablation studies demonstrate the effectiveness and efficiency of
our proposed SwinFSR.",None,-1
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,0.80379,"Intracerebral hemorrhage (ICH) is the second most common and deadliest form
of stroke. Despite medical advances, predicting treat ment outcomes for ICH
remains a challenge. This paper proposes a novel prognostic model that utilizes
both imaging and tabular data to predict treatment outcome for ICH. Our model
is trained on observational data collected from non-randomized controlled
trials, providing reliable predictions of treatment success. Specifically, we
propose to employ a variational autoencoder model to generate a low-dimensional
prognostic score, which can effectively address the selection bias resulting
from the non-randomized controlled trials. Importantly, we develop a
variational distributions combination module that combines the information from
imaging data, non-imaging clinical data, and treatment assignment to accurately
generate the prognostic score. We conducted extensive experiments on a
real-world clinical dataset of intracerebral hemorrhage. Our proposed method
demonstrates a substantial improvement in treatment outcome prediction compared
to existing state-of-the-art approaches. Code is available at
https://github.com/med-air/TOP-GPM",None,-1
PanGu-: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,0.212715,"The scaling of large language models has greatly improved natural language
understanding, generation, and reasoning. In this work, we develop a system
that trained a trillion-parameter language model on a cluster of Ascend 910 AI
processors and MindSpore framework, and present the language model with 1.085T
parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha},
we extend the dense Transformer model to sparse one with Random Routed Experts
(RRE), and efficiently train the model over 329B tokens by using Expert
Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in
training throughput through heterogeneous computing. Our experimental findings
show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot
learning of various Chinese NLP downstream tasks. Moreover, it demonstrates
strong abilities when fine-tuned in application data of open-domain dialogue,
question answering, machine translation and code generation.",None,-1
LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination,0.732125,"Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable
proficiency in comprehending and generating natural language. On the other
hand, medical assistants hold the potential to offer substantial benefits for
individuals. However, the exploration of LLM-based personalized medical
assistant remains relatively scarce. Typically, patients converse differently
based on their background and preferences which necessitates the task of
enhancing user-oriented medical assistant. While one can fully train an LLM for
this objective, the resource consumption is unaffordable. Prior research has
explored memory-based methods to enhance the response with aware of previous
mistakes for new queries during a dialogue session. We contend that a mere
memory module is inadequate and fully training an LLM can be excessively
costly. In this study, we propose a novel computational bionic memory
mechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to
personalize medical assistants.",None,-1
Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification,0.998303,"For the visible-infrared person re-identification (VIReID) task, one of the
major challenges is the modality gaps between visible (VIS) and infrared (IR)
images. However, the training samples are usually limited, while the modality
gaps are too large, which leads that the existing methods cannot effectively
mine diverse cross-modality clues. To handle this limitation, we propose a
novel augmentation network in the embedding space, called diverse embedding
expansion network (DEEN). The proposed DEEN can effectively generate diverse
embeddings to learn the informative feature representations and reduce the
modality discrepancy between the VIS and IR images. Moreover, the VIReID model
may be seriously affected by drastic illumination changes, while all the
existing VIReID datasets are captured under sufficient illumination without
significant light changes. Thus, we provide a low-light cross-modality (LLCM)
dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9
RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasets
show the superiority of the proposed DEEN over several other state-of-the-art
methods. The code and dataset are released at: https://github.com/ZYK100/LLCM",None,-1
Towards Real-World Burst Image Super-Resolution: Benchmark and Method,0.538074,"Despite substantial advances, single-image super-resolution (SISR) is always
in a dilemma to reconstruct high-quality images with limited information from
one input image, especially in realistic scenarios. In this paper, we establish
a large-scale real-world burst super-resolution dataset, i.e., RealBSR, to
explore the faithful reconstruction of image details from multiple frames.
Furthermore, we introduce a Federated Burst Affinity network (FBAnet) to
investigate non-trivial pixel-wise displacements among images under real-world
image degradation. Specifically, rather than using pixel-wise alignment, our
FBAnet employs a simple homography alignment from a structural geometry aspect
and a Federated Affinity Fusion (FAF) strategy to aggregate the complementary
information among frames. Those fused informative representations are fed to a
Transformer-based module of burst representation decoding. Besides, we have
conducted extensive experiments on two versions of our datasets, i.e.,
RealBSR-RAW and RealBSR-RGB. Experimental results demonstrate that our FBAnet
outperforms existing state-of-the-art burst SR methods and also achieves
visually-pleasant SR image predictions with model details. Our dataset, codes,
and models are publicly available at https://github.com/yjsunnn/FBANet.",None,-1
HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention,0.797609,"The success of large-scale contrastive vision-language pretraining (CLIP) has
benefited both visual recognition and multimodal content understanding. The
concise design brings CLIP the advantage in inference efficiency against other
vision-language models with heavier cross-attention fusion layers, making it a
popular choice for a wide spectrum of downstream tasks. However, CLIP does not
explicitly capture the hierarchical nature of high-level and fine-grained
semantics conveyed in images and texts, which is arguably critical to
vision-language understanding and reasoning. To this end, we equip both the
visual and language branches in CLIP with hierarchy-aware attentions, namely
Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies
layer-by-layer from both images and texts in an unsupervised manner. As a
result, such hierarchical aggregation significantly improves the cross-modal
alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative
analysis on its unsupervised hierarchy induction during inference, as well as
extensive quantitative experiments on both visual recognition and
vision-language downstream tasks.",None,-1
Face Transformer: Towards High Fidelity and Accurate Face Swapping,0.359632,"Face swapping aims to generate swapped images that fuse the identity of
source faces and the attributes of target faces. Most existing works address
this challenging task through 3D modelling or generation using generative
adversarial networks (GANs), but 3D modelling suffers from limited
reconstruction accuracy and GANs often struggle in preserving subtle yet
important identity details of source faces (e.g., skin colors, face features)
and structural attributes of target faces (e.g., face shapes, facial
expressions). This paper presents Face Transformer, a novel face swapping
network that can accurately preserve source identities and target attributes
simultaneously in the swapped face images. We introduce a transformer network
for the face swapping task, which learns high-quality semantic-aware
correspondence between source and target faces and maps identity features of
source faces to the corresponding region in target faces. The high-quality
semantic-aware correspondence enables smooth and accurate transfer of source
identity information with minimal modification of target shapes and
expressions. In addition, our Face Transformer incorporates a multi-scale
transformation mechanism for preserving the rich fine facial details. Extensive
experiments show that our Face Transformer achieves superior face swapping
performance qualitatively and quantitatively.",None,-1
Text2Tex: Text-driven Texture Synthesis via Diffusion Models,0.953282,"We present Text2Tex, a novel method for generating high-quality textures for
3D meshes from the given text prompts. Our method incorporates inpainting into
a pre-trained depth-aware image diffusion model to progressively synthesize
high resolution partial textures from multiple viewpoints. To avoid
accumulating inconsistent and stretched artifacts across views, we dynamically
segment the rendered view into a generation mask, which represents the
generation status of each visible texel. This partitioned view representation
guides the depth-aware inpainting model to generate and update partial textures
for the corresponding regions. Furthermore, we propose an automatic view
sequence generation scheme to determine the next best view for updating the
partial texture. Extensive experiments demonstrate that our method
significantly outperforms the existing text-driven approaches and GAN-based
methods.",None,-1
Nationality Bias in Text Generation,0.384487,"Little attention is placed on analyzing nationality bias in language models,
especially when nationality is highly used as a factor in increasing the
performance of social NLP models. This paper examines how a text generation
model, GPT-2, accentuates pre-existing societal biases about country-based
demonyms. We generate stories using GPT-2 for various nationalities and use
sensitivity analysis to explore how the number of internet users and the
country's economic status impacts the sentiment of the stories. To reduce the
propagation of biases through large language models (LLM), we explore the
debiasing method of adversarial triggering. Our results show that GPT-2
demonstrates significant bias against countries with lower internet users, and
adversarial triggering effectively reduces the same.",None,-1
Enhancing Low Resource NER Using Assisting Language And Transfer Learning,0.507972,"Named Entity Recognition (NER) is a fundamental task in NLP that is used to
locate the key information in text and is primarily applied in conversational
and search systems. In commercial applications, NER or comparable slot-filling
methods have been widely deployed for popular languages. NER is used in
applications such as human resources, customer service, search engines, content
classification, and academia. In this paper, we draw focus on identifying name
entities for low-resource Indian languages that are closely related, like Hindi
and Marathi. We use various adaptations of BERT such as baseBERT, AlBERT, and
RoBERTa to train a supervised NER model. We also compare multilingual models
with monolingual models and establish a baseline. In this work, we show the
assisting capabilities of the Hindi and Marathi languages for the NER task. We
show that models trained using multiple languages perform better than a single
language. However, we also observe that blind mixing of all datasets doesn't
necessarily provide improvements and data selection methods may be required.",None,-1
RuSentNE-2023: Evaluating Entity-Oriented Sentiment Analysis on Russian News Texts,0.507972,"The paper describes the RuSentNE-2023 evaluation devoted to targeted
sentiment analysis in Russian news texts. The task is to predict sentiment
towards a named entity in a single sentence. The dataset for RuSentNE-2023
evaluation is based on the Russian news corpus RuSentNE having rich
sentiment-related annotation. The corpus is annotated with named entities and
sentiments towards these entities, along with related effects and emotional
states. The evaluation was organized using the CodaLab competition framework.
The main evaluation measure was macro-averaged measure of positive and negative
classes. The best results achieved were of 66% Macro F-measure
(Positive+Negative classes). We also tested ChatGPT on the test set from our
evaluation and found that the zero-shot answers provided by ChatGPT reached 60%
of the F-measure, which corresponds to 4th place in the evaluation. ChatGPT
also provided detailed explanations of its conclusion. This can be considered
as quite high for zero-shot application.",None,-1
Identification of Novel Classes for Improving Few-Shot Object Detection,0.504927,"Conventional training of deep neural networks requires a large number of the
annotated image which is a laborious and time-consuming task, particularly for
rare objects. Few-shot object detection (FSOD) methods offer a remedy by
realizing robust object detection using only a few training samples per class.
An unexplored challenge for FSOD is that instances from unlabeled novel classes
that do not belong to the fixed set of training classes appear in the
background. These objects behave similarly to label noise, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during training
to improve FSOD performance. Specifically, we propose a hierarchical ternary
classification region proposal network (HTRPN) to localize the potential
unlabeled novel objects and assign them new objectness labels. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
Our experimental results indicate that our method is effective and outperforms
the existing state-of-the-art (SOTA) FSOD methods.",None,-1
Painterly Image Harmonization using Diffusion Model,0.75003,"Painterly image harmonization aims to insert photographic objects into
paintings and obtain artistically coherent composite images. Previous methods
for this task mainly rely on inference optimization or generative adversarial
network, but they are either very time-consuming or struggling at fine control
of the foreground objects (e.g., texture and content details). To address these
issues, we propose a novel Painterly Harmonization stable Diffusion model
(PHDiffusion), which includes a lightweight adaptive encoder and a Dual Encoder
Fusion (DEF) module. Specifically, the adaptive encoder and the DEF module
first stylize foreground features within each encoder. Then, the stylized
foreground features from both encoders are combined to guide the harmonization
process. During training, besides the noise loss in diffusion model, we
additionally employ content loss and two style losses, i.e., AdaIN style loss
and contrastive style loss, aiming to balance the trade-off between style
migration and content preservation. Compared with the state-of-the-art models
from related fields, our PHDiffusion can stylize the foreground more
sufficiently and simultaneously retain finer content. Our code and model are
available at https://github.com/bcmi/PHDiffusion-Painterly-Image-Harmonization.",None,-1
Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula,0.291658,"Autocurricular training is an important sub-area of multi-agent reinforcement
learning~(MARL) that allows multiple agents to learn emergent skills in an
unsupervised co-evolving scheme. The robotics community has experimented
autocurricular training with physically grounded problems, such as robust
control and interactive manipulation tasks. However, the asymmetric nature of
these tasks makes the generation of sophisticated policies challenging. Indeed,
the asymmetry in the environment may implicitly or explicitly provide an
advantage to a subset of agents which could, in turn, lead to a low-quality
equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg
Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a
two-player MARL problem as a Stackelberg game with one player as the `leader'
and the other as the `follower' in a hierarchical interaction structure wherein
the leader has an advantage. We first demonstrate that the leader's advantage
from ST-MADDPG can be used to alleviate the inherent asymmetry in the
environment. By exploiting the leader's advantage, ST-MADDPG improves the
quality of a co-evolution process and results in more sophisticated and complex
strategies that work well even against an unseen strong opponent.",None,-1
Fusing Hand and Body Skeletons for Human Action Recognition in Assembly,0.256368,"As collaborative robots (cobots) continue to gain popularity in industrial
manufacturing, effective human-robot collaboration becomes crucial. Cobots
should be able to recognize human actions to assist with assembly tasks and act
autonomously. To achieve this, skeleton-based approaches are often used due to
their ability to generalize across various people and environments. Although
body skeleton approaches are widely used for action recognition, they may not
be accurate enough for assembly actions where the worker's fingers and hands
play a significant role. To address this limitation, we propose a method in
which less detailed body skeletons are combined with highly detailed hand
skeletons. We investigate CNNs and transformers, the latter of which are
particularly adept at extracting and combining important information from both
skeleton types using attention. This paper demonstrates the effectiveness of
our proposed approach in enhancing action recognition in assembly scenarios.",None,-1
Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models,0.0553656,"We investigate the effects of post-training quantization and
quantization-aware training on the generalization of Transformer language
models. We present a new method called self-distilled quantization (SDQ) that
minimizes accumulative quantization errors and outperforms baselines. We apply
SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that
both models can be reduced from 32-bit floating point weights to 8-bit integer
weights while maintaining a high level of performance on the XGLUE benchmark.
Our results also highlight the challenges of quantizing multilingual models,
which must generalize to languages they were not fine-tuned on.",None,-1
Towards LLM-based Autograding for Short Textual Answers,0.984608,"Grading of exams is an important, labor intensive, subjective, repetitive and
frequently challenging task. The feasibility of autograding textual responses
has greatly increased thanks to the availability of large language models
(LLMs) such as ChatGPT and because of the substantial influx of data brought
about by digitalization. However, entrusting AI models with decision-making
roles raises ethical considerations, mainly stemming from potential biases and
issues related to generating false information. Thus, in this manuscript we
provide an evaluation of a large language model for the purpose of autograding,
while also highlighting how LLMs can support educators in validating their
grading procedures. Our evaluation is targeted towards automatic short textual
answers grading (ASAG), spanning various languages and examinations from two
distinct courses. Our findings suggest that while ""out-of-the-box"" LLMs provide
a valuable tool to provide a complementary perspective, their readiness for
independent automated grading remains a work in progress, necessitating human
oversight.",None,-1
Holistic Network Virtualization and Pervasive Network Intelligence for 6G,0.993416,"In this tutorial paper, we look into the evolution and prospect of network
architecture and propose a novel conceptual architecture for the 6th generation
(6G) networks. The proposed architecture has two key elements, i.e., holistic
network virtualization and pervasive artificial intelligence (AI). The holistic
network virtualization consists of network slicing and digital twin, from the
aspects of service provision and service demand, respectively, to incorporate
service-centric and user-centric networking. The pervasive network intelligence
integrates AI into future networks from the perspectives of networking for AI
and AI for networking, respectively. Building on holistic network
virtualization and pervasive network intelligence, the proposed architecture
can facilitate three types of interplay, i.e., the interplay between digital
twin and network slicing paradigms, between model-driven and data-driven
methods for network management, and between virtualization and AI, to maximize
the flexibility, scalability, adaptivity, and intelligence for 6G networks. We
also identify challenges and open issues related to the proposed architecture.
By providing our vision, we aim to inspire further discussions and developments
on the potential architecture of 6G.",None,-1
Gemini Pro Defeated by GPT-4V: Evidence from Education,0.893935,"This study compared the classification performance of Gemini Pro and GPT-4V
in educational settings. Employing visual question answering (VQA) techniques,
the study examined both models' abilities to read text-based rubrics and then
automatically score student-drawn models in science education. We employed both
quantitative and qualitative analyses using a dataset derived from
student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics
for Image Feedback) prompting methods. The findings reveal that GPT-4V
significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic
Weighted Kappa. The qualitative analysis reveals that the differences may be
due to the models' ability to process fine-grained texts in images and overall
image classification performance. Even adapting the NERIF approach by further
de-sizing the input images, Gemini Pro seems not able to perform as well as
GPT-4V. The findings suggest GPT-4V's superior capability in handling complex
multimodal educational tasks. The study concludes that while both models
represent advancements in AI, GPT-4V's higher performance makes it a more
suitable tool for educational applications involving multimodal data
interpretation.",None,-1
Calibrated Explanations: with Uncertainty Information and Counterfactuals,0.195995,"While local explanations for AI models can offer insights into individual
predictions, such as feature importance, they are plagued by issues like
instability. The unreliability of feature weights, often skewed due to poorly
calibrated ML models, deepens these challenges. Moreover, the critical aspect
of feature importance uncertainty remains mostly unaddressed in Explainable AI
(XAI). The novel feature importance explanation method presented in this paper,
called Calibrated Explanations (CE), is designed to tackle these issues
head-on. Built on the foundation of Venn-Abers, CE not only calibrates the
underlying model but also delivers reliable feature importance explanations
with an exact definition of the feature weights. CE goes beyond conventional
solutions by addressing output uncertainty. It accomplishes this by providing
uncertainty quantification for both feature weights and the model's probability
estimates. Additionally, CE is model-agnostic, featuring easily comprehensible
conditional rules and the ability to generate counterfactual explanations with
embedded uncertainty quantification. Results from an evaluation with 25
benchmark datasets underscore the efficacy of CE, making it stand as a fast,
reliable, stable, and robust solution.",None,-1
GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,0.515653,"A key goal for the advancement of AI is to develop technologies that serve
the needs not just of one group but of all communities regardless of their
geographical region. In fact, a significant proportion of knowledge is locally
shared by people from certain regions but may not apply equally in other
regions because of cultural differences. If a model is unaware of regional
characteristics, it may lead to performance disparity across regions and result
in bias against underrepresented groups. We propose GIVL, a Geographically
Inclusive Vision-and-Language Pre-trained model. There are two attributes of
geo-diverse visual concepts which can help to learn geo-diverse knowledge: 1)
concepts under similar categories have unique knowledge and visual
characteristics, 2) concepts with similar visual features may fall in
completely different categories. Motivated by the attributes, we design new
pre-training objectives Image Knowledge Matching (IKM) and Image Edit Checking
(IEC) to pre-train GIVL. Compared with similar-size models pre-trained with
similar scale of data, GIVL achieves state-of-the-art (SOTA) and more balanced
performance on geo-diverse V&L tasks.",None,-1
Improved Trajectory Reconstruction for Markerless Pose Estimation,0.659436,"Markerless pose estimation allows reconstructing human movement from multiple
synchronized and calibrated views, and has the potential to make movement
analysis easy and quick, including gait analysis. This could enable much more
frequent and quantitative characterization of gait impairments, allowing better
monitoring of outcomes and responses to interventions. However, the impact of
different keypoint detectors and reconstruction algorithms on markerless pose
estimation accuracy has not been thoroughly evaluated. We tested these
algorithmic choices on data acquired from a multicamera system from a
heterogeneous sample of 25 individuals seen in a rehabilitation hospital. We
found that using a top-down keypoint detector and reconstructing trajectories
with an implicit function enabled accurate, smooth and anatomically plausible
trajectories, with a noise in the step width estimates compared to a GaitRite
walkway of only 8mm.",None,-1
Peer attention enhances student learning,0.0567606,"Human visual attention is susceptible to social influences. In education,
peer effects impact student learning, but their precise role in modulating
attention remains unclear. Our experiment (N=311) demonstrates that displaying
peer visual attention regions when students watch online course videos enhances
their focus and engagement. However, students retain adaptability in following
peer attention cues. Overall, guided peer attention improves learning
experiences and outcomes. These findings elucidate how peer visual attention
shapes students' gaze patterns, deepening understanding of peer influence on
learning. They also offer insights into designing adaptive online learning
interventions leveraging peer attention modelling to optimize student
attentiveness and success.",None,-1
Break It Down: Evidence for Structural Compositionality in Neural Networks,0.718099,"Though modern neural networks have achieved impressive performance in both
vision and language tasks, we know little about the functions that they
implement. One possibility is that neural networks implicitly break down
complex tasks into subroutines, implement modular solutions to these
subroutines, and compose them into an overall solution to a task - a property
we term structural compositionality. Another possibility is that they may
simply learn to match new inputs to learned templates, eliding task
decomposition entirely. Here, we leverage model pruning techniques to
investigate this question in both vision and language across a variety of
architectures, tasks, and pretraining regimens. Our results demonstrate that
models often implement solutions to subroutines via modular subnetworks, which
can be ablated while maintaining the functionality of other subnetworks. This
suggests that neural networks may be able to learn compositionality, obviating
the need for specialized symbolic mechanisms.",None,-1
Incorporating Unlabelled Data into Bayesian Neural Networks,0.745654,"Conventional Bayesian Neural Networks (BNNs) cannot leverage unlabelled data
to improve their predictions. To overcome this limitation, we introduce
Self-Supervised Bayesian Neural Networks, which use unlabelled data to learn
improved prior predictive distributions by maximising an evidence lower bound
during an unsupervised pre-training step. With a novel methodology developed to
better understand prior predictive distributions, we then show that
self-supervised prior predictives capture image semantics better than
conventional BNN priors. In our empirical evaluations, we see that
self-supervised BNNs offer the label efficiency of self-supervised methods and
the uncertainty estimates of Bayesian methods, particularly outperforming
conventional BNNs in low-to-medium data regimes.",None,-1
Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education,0.329497,"The advancement of biomedical research heavily relies on access to large
amounts of medical data. In the case of histopathology, Whole Slide Images
(WSI) and clinicopathological information are valuable for developing
Artificial Intelligence (AI) algorithms for Digital Pathology (DP).
Transferring medical data ""as open as possible"" enhances the usability of the
data for secondary purposes but poses a risk to patient privacy. At the same
time, existing regulations push towards keeping medical data ""as closed as
necessary"" to avoid re-identification risks. Generally, these legal regulations
require the removal of sensitive data but do not consider the possibility of
data linkage attacks due to modern image-matching algorithms. In addition, the
lack of standardization in DP makes it harder to establish a single solution
for all formats of WSIs. These challenges raise problems for bio-informatics
researchers in balancing privacy and progress while developing AI algorithms.
This paper explores the legal regulations and terminologies for medical
data-sharing. We review existing approaches and highlight challenges from the
histopathological perspective. We also present a data-sharing guideline for
histological data to foster multidisciplinary research and education.",None,-1
Late Breaking Results: Scalable and Efficient Hyperdimensional Computing for Network Intrusion Detection,0.37337,"Cybersecurity has emerged as a critical challenge for the industry. With the
large complexity of the security landscape, sophisticated and costly deep
learning models often fail to provide timely detection of cyber threats on edge
devices. Brain-inspired hyperdimensional computing (HDC) has been introduced as
a promising solution to address this issue. However, existing HDC approaches
use static encoders and require very high dimensionality and hundreds of
training iterations to achieve reasonable accuracy. This results in a serious
loss of learning efficiency and causes huge latency for detecting attacks. In
this paper, we propose CyberHD, an innovative HDC learning framework that
identifies and regenerates insignificant dimensions to capture complicated
patterns of cyber threats with remarkably lower dimensionality. Additionally,
the holographic distribution of patterns in high dimensional space provides
CyberHD with notably high robustness against hardware errors.",None,-1
Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection,0.791661,"Real-world deployment of reliable object detectors is crucial for
applications such as autonomous driving. However, general-purpose object
detectors like Faster R-CNN are prone to providing overconfident predictions
for outlier objects. Recent outlier-aware object detection approaches estimate
the density of instance-wide features with class-conditional Gaussians and
train on synthesized outlier features from their low-likelihood regions.
However, this strategy does not guarantee that the synthesized outlier features
will have a low likelihood according to the other class-conditional Gaussians.
We propose a novel outlier-aware object detection framework that distinguishes
outliers from inlier objects by learning the joint data distribution of all
inlier classes with an invertible normalizing flow. The appropriate sampling of
the flow model ensures that the synthesized outliers have a lower likelihood
than inliers of all object classes, thereby modeling a better decision boundary
between inlier and outlier objects. Our approach significantly outperforms the
state-of-the-art for outlier-aware object detection on both image and video
datasets. Code available at https://github.com/nish03/FFS",None,-1
TaBIIC: Taxonomy Building through Iterative and Interactive Clustering,0.114336,"Building taxonomies is often a significant part of building an ontology, and
many attempts have been made to automate the creation of such taxonomies from
relevant data. The idea in such approaches is either that relevant definitions
of the intension of concepts can be extracted as patterns in the data (e.g. in
formal concept analysis) or that their extension can be built from grouping
data objects based on similarity (clustering). In both cases, the process leads
to an automatically constructed structure, which can either be too coarse and
lacking in definition, or too fined-grained and detailed, therefore requiring
to be refined into the desired taxonomy. In this paper, we explore a method
that takes inspiration from both approaches in an iterative and interactive
process, so that refinement and definition of the concepts in the taxonomy
occur at the time of identifying those concepts in the data. We show that this
method is applicable on a variety of data sources and leads to taxonomies that
can be more directly integrated into ontologies.",None,-1
vMAP: Vectorised Object Mapping for Neural Field SLAM,0.868703,"We present vMAP, an object-level dense SLAM system using neural field
representations. Each object is represented by a small MLP, enabling efficient,
watertight object modelling without the need for 3D priors. As an RGB-D camera
browses a scene with no prior information, vMAP detects object instances
on-the-fly, and dynamically adds them to its map. Specifically, thanks to the
power of vectorised training, vMAP can optimise as many as 50 individual
objects in a single scene, with an extremely efficient training speed of 5Hz
map update. We experimentally demonstrate significantly improved scene-level
and object-level reconstruction quality compared to prior neural field SLAM
systems. Project page: https://kxhit.github.io/vMAP.",None,-1
Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,0.735778,"Hate speech is a severe issue that affects many online platforms. So far,
several studies have been performed to develop robust hate speech detection
systems. Large language models like ChatGPT have recently shown a great promise
in performing several tasks, including hate speech detection. However, it is
crucial to comprehend the limitations of these models to build robust hate
speech detection systems. To bridge this gap, our study aims to evaluate the
strengths and weaknesses of the ChatGPT model in detecting hate speech at a
granular level across 11 languages. Our evaluation employs a series of
functionality tests that reveals various intricate failures of the model which
the aggregate metrics like macro F1 or accuracy are not able to unfold. In
addition, we investigate the influence of complex emotions, such as the use of
emojis in hate speech, on the performance of the ChatGPT model. Our analysis
highlights the shortcomings of the generative models in detecting certain types
of hate speech and highlighting the need for further research and improvements
in the workings of these models.",None,-1
Deep Neural Networks for Encrypted Inference with TFHE,0.465609,"Fully homomorphic encryption (FHE) is an encryption method that allows to
perform computation on encrypted data, without decryption. FHE preserves the
privacy of the users of online services that handle sensitive data, such as
health data, biometrics, credit scores and other personal information. A common
way to provide a valuable service on such data is through machine learning and,
at this time, Neural Networks are the dominant machine learning model for
unstructured data. In this work we show how to construct Deep Neural Networks
(DNN) that are compatible with the constraints of TFHE, an FHE scheme that
allows arbitrary depth computation circuits. We discuss the constraints and
show the architecture of DNNs for two computer vision tasks. We benchmark the
architectures using the Concrete stack, an open-source implementation of TFHE.",None,-1
A Model for Translation of Text from Indian Languages to Bharti Braille Characters,0.0844284,"People who are visually impaired face a lot of difficulties while studying.
One of the major causes to this is lack of available text in Bharti Braille
script. In this paper, we have suggested a scheme to convert text in major
Indian languages into Bharti Braille. The system uses a hybrid approach where
at first the text in Indian language is given to a rule based system and in
case if there is any ambiguity then it is resolved by applying a LSTM based
model. The developed model has also been tested and found to have produced near
accurate results.",None,-1
Automated clinical coding using off-the-shelf large language models,0.46545,"The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment, with no
need for further task-specific training. Unsupervised pre-training alone does
not guarantee precise knowledge of the ICD ontology and specialist clinical
coding task, therefore we frame the task as information extraction, providing a
description of each coded concept and asking the model to retrieve related
mentions. For efficiency, rather than iterating over all codes, we leverage the
hierarchical nature of the ICD ontology to sparsely search for relevant codes.",None,-1
Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication,0.443566,"Individuals with complex communication needs (CCN) often rely on augmentative
and alternative communication (AAC) systems to have conversations and
communique their wants. Such systems allow message authoring by arranging
pictograms in sequence. However, the difficulty of finding the desired item to
complete a sentence can increase as the user's vocabulary increases. This paper
proposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram
prediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus
for Brazilian Portuguese to use as a training corpus. We tested different
approaches to representing a pictogram for prediction: as a word (using
pictogram captions), as a concept (using a dictionary definition), and as a set
of synonyms (using related terms). We also evaluated the usage of images for
pictogram prediction. The results demonstrate that using embeddings computed
from the pictograms' caption, synonyms, or definitions have a similar
performance. Using synonyms leads to lower perplexity, but using captions leads
to the highest accuracies. This paper provides insight into how to represent a
pictogram for prediction using a BERT-like model and the potential of using
images for pictogram prediction.",None,-1
Concept Algebra for (Score-Based) Text-Controlled Generative Models,0.247031,"This paper concerns the structure of learned representations in text-guided
generative models, focusing on score-based models. A key property of such
models is that they can compose disparate concepts in a `disentangled' manner.
This suggests these models have internal representations that encode concepts
in a `disentangled' manner. Here, we focus on the idea that concepts are
encoded as subspaces of some representation space. We formalize what this
means, show there's a natural choice for the representation, and develop a
simple method for identifying the part of the representation corresponding to a
given concept. In particular, this allows us to manipulate the concepts
expressed by the model through algebraic manipulation of the representation. We
demonstrate the idea with examples using Stable Diffusion. Code in
https://github.com/zihao12/concept-algebra-code",None,-1
KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models,0.427007,"Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches
in changing factual knowledge stored in the Language models. However, there is
a lack of research on whether present locating methods can pinpoint the exact
parameters embedding the desired knowledge. Moreover, although many researchers
have questioned the validity of locality hypothesis of factual knowledge, no
method is provided to test the a hypothesis for more in-depth discussion and
research. Therefore, we introduce KLoB, a benchmark examining three essential
properties that a reliable knowledge locating method should satisfy. KLoB can
serve as a benchmark for evaluating existing locating methods in language
models, and can contributes a method to reassessing the validity of locality
hypothesis of factual knowledge. Our is publicly available at
\url{https://github.com/juyiming/KLoB}.",None,-1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,0.859252,"Multiple instance learning is an ideal mode of analysis for histopathology
data, where vast whole slide images are typically annotated with a single
global label. In such cases, a whole slide image is modelled as a collection of
tissue patches to be aggregated and classified. Common models for performing
this classification include recurrent neural networks and transformers.
Although powerful compression algorithms, such as deep pre-trained neural
networks, are used to reduce the dimensionality of each patch, the sequences
arising from whole slide images remain excessively long, routinely containing
tens of thousands of patches. Structured state space models are an emerging
alternative for sequence modelling, specifically designed for the efficient
modelling of long sequences. These models invoke an optimal projection of an
input sequence into memory units that compress the entire sequence. In this
paper, we propose the use of state space models as a multiple instance learner
to a variety of problems in digital pathology. Across experiments in metastasis
detection, cancer subtyping, mutation classification, and multitask learning,
we demonstrate the competitiveness of this new class of models with existing
state of the art approaches. Our code is available at
https://github.com/MICS-Lab/s4_digital_pathology.",None,-1
Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models,0.443735,"After the inception of emotion recognition or affective computing, it has
increasingly become an active research topic due to its broad applications.
Over the past couple of decades, emotion recognition models have gradually
migrated from statistically shallow models to neural network-based deep models,
which can significantly boost the performance of emotion recognition models and
consistently achieve the best results on different benchmarks. Therefore, in
recent years, deep models have always been considered the first option for
emotion recognition. However, the debut of large language models (LLMs), such
as ChatGPT, has remarkably astonished the world due to their emerged
capabilities of zero/few-shot learning, in-context learning, chain-of-thought,
and others that are never shown in previous deep models. In the present paper,
we comprehensively investigate how the LLMs perform in emotion recognition in
terms of diverse aspects, including in-context learning, few-short learning,
accuracy, generalisation, and explanation. Moreover, we offer some insights and
pose other potential challenges, hoping to ignite broader discussions about
enhancing emotion recognition in the new era of advanced and generalised large
models.",None,-1
Smart Infrastructure: A Research Junction,0.058309,"Complex inner-city junctions are among the most critical traffic areas for
injury and fatal accidents. The development of highly automated driving (HAD)
systems struggles with the complex and hectic everyday life within those areas.
Sensor-equipped smart infrastructures, which can communicate and cooperate with
vehicles, are essential to enable a holistic scene understanding to resolve
occlusions drivers and vehicle perception systems for themselves can not cover.
We introduce an intelligent research infrastructure equipped with visual sensor
technology, located at a public inner-city junction in Aschaffenburg, Germany.
A multiple-view camera system monitors the traffic situation to perceive road
users' behavior. Both motorized and non-motorized traffic is considered. The
system is used for research in data generation, evaluating new HAD sensors
systems, algorithms, and Artificial Intelligence (AI) training strategies using
real-, synthetic- and augmented data. In addition, the junction features a
highly accurate digital twin. Real-world data can be taken into the digital
twin for simulation purposes and synthetic data generation.",None,-1
Rehearsal-Free Domain Continual Face Anti-Spoofing: Generalize More and Forget Less,0.962781,"Face Anti-Spoofing (FAS) is recently studied under the continual learning
setting, where the FAS models are expected to evolve after encountering the
data from new domains. However, existing methods need extra replay buffers to
store previous data for rehearsal, which becomes infeasible when previous data
is unavailable because of privacy issues. In this paper, we propose the first
rehearsal-free method for Domain Continual Learning (DCL) of FAS, which deals
with catastrophic forgetting and unseen domain generalization problems
simultaneously. For better generalization to unseen domains, we design the
Dynamic Central Difference Convolutional Adapter (DCDCA) to adapt Vision
Transformer (ViT) models during the continual learning sessions. To alleviate
the forgetting of previous domains without using previous data, we propose the
Proxy Prototype Contrastive Regularization (PPCR) to constrain the continual
learning with previous domain knowledge from the proxy prototypes. Simulate
practical DCL scenarios, we devise two new protocols which evaluate both
generalization and anti-forgetting performance. Extensive experimental results
show that our proposed method can improve the generalization performance in
unseen domains and alleviate the catastrophic forgetting of the previous
knowledge. The codes and protocols will be released soon.",None,-1
"Towards socially-competent and culturally-adaptive artificial agents Expressive order, interactional disruptions and recovery strategies",0.171146,"The development of artificial agents for social interaction pushes to enrich
robots with social skills and knowledge about (local) social norms. One
possibility is to distinguish the expressive and the functional orders during a
human-robot interaction. The overarching aim of this work is to set a framework
to make the artificial agent socially-competent beyond dyadic
interaction-interaction in varying multi-party social situations-and beyond
individual-based user personalization, thereby enlarging the current conception
of ""culturally-adaptive"". The core idea is to provide the artificial agent with
the capability to handle different kinds of interactional disruptions, and
associated recovery strategies, in microsociology. The result is obtained by
classifying functional and social disruptions, and by investigating the
requirements a robot's architecture should satisfy to exploit such knowledge.
The paper also highlights how this level of competence is achieved by focusing
on just three dimensions: (i) social capability, (ii) relational role, and
(iii) proximity, leaving aside the further complexity of full-fledged
human-human interactions. Without going into technical aspects, End-to-end
Data-driven Architectures and Modular Architectures are discussed to evaluate
the degree to which they can exploit this new set of social and cultural
knowledge. Finally, a list of general requirements for such agents is proposed.",None,-1
A Frustratingly Easy Improvement for Position Embeddings via Random Padding,0.761132,"Position embeddings, encoding the positional relationships among tokens in
text sequences, make great contributions to modeling local context features in
Transformer-based pre-trained language models. However, in Extractive Question
Answering, position embeddings trained with instances of varied context lengths
may not perform well as we expect. Since the embeddings of rear positions are
updated fewer times than the front position embeddings, the rear ones may not
be properly trained. In this paper, we propose a simple but effective strategy,
Random Padding, without any modifications to architectures of existing
pre-trained language models. We adjust the token order of input sequences when
fine-tuning, to balance the number of updating times of every position
embedding. Experiments show that Random Padding can significantly improve model
performance on the instances whose answers are located at rear positions,
especially when models are trained on short contexts but evaluated on long
contexts. Our code and data will be released for future research.",None,-1
The Current State of Summarization,0.185789,"With the explosive growth of textual information, summarization systems have
become increasingly important. This work aims to concisely indicate the current
state of the art in abstractive text summarization. As part of this, we outline
the current paradigm shifts towards pre-trained encoder-decoder models and
large autoregressive language models. Additionally, we delve further into the
challenges of evaluating summarization systems and the potential of
instruction-tuned models for zero-shot summarization. Finally, we provide a
brief overview of how summarization systems are currently being integrated into
commercial applications.",None,-1
Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines,0.613259,"Polarization and the marketplace for impressions have conspired to make
navigating information online difficult for users, and while there has been a
significant effort to detect false or misleading text, multimodal datasets have
received considerably less attention. To complement existing resources, we
present multimodal Video Misleading Headline (VMH), a dataset that consists of
videos and whether annotators believe the headline is representative of the
video's contents. After collecting and annotating this dataset, we analyze
multimodal baselines for detecting misleading headlines. Our annotation process
also focuses on why annotators view a video as misleading, allowing us to
better understand the interplay of annotators' background and the content of
the videos.",None,-1
Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using Video Prediction Networks,0.169945,"Prediction of dynamic environment is crucial to safe navigation of an
autonomous vehicle. Urban traffic scenes are particularly challenging to
forecast due to complex interactions between various dynamic agents, such as
vehicles and vulnerable road users. Previous approaches have used egocentric
occupancy grid maps to represent and predict dynamic environments. However,
these predictions suffer from blurriness, loss of scene structure at turns, and
vanishing of agents over longer prediction horizon. In this work, we propose a
novel framework to make long-term predictions by representing the traffic scene
in a fixed frame, referred as allo-centric occupancy grid. This allows for the
static scene to remain fixed and to represent motion of the ego-vehicle on the
grid like other agents'. We study the allo-centric grid prediction with
different video prediction networks and validate the approach on the real-world
Nuscenes dataset. The results demonstrate that the allo-centric grid
representation significantly improves scene prediction, in comparison to the
conventional ego-centric grid approach.",None,-1
On the Generalization of Multi-modal Contrastive Learning,0.542192,"Multi-modal contrastive learning (MMCL) has recently garnered considerable
interest due to its superior performance in visual tasks, achieved by embedding
multi-modal data, such as visual-language pairs. However, there still lack
theoretical understandings of how MMCL extracts useful visual representation
from multi-modal pairs, and particularly, how MMCL outperforms previous
approaches like self-supervised contrastive learning (SSCL). In this paper, by
drawing an intrinsic connection between MMCL and asymmetric matrix
factorization, we establish the first generalization guarantees of MMCL for
visual downstream tasks. Based on this framework, we further unify MMCL and
SSCL by showing that MMCL implicitly performs SSCL with (pseudo) positive pairs
induced by text pairs. Through this unified perspective, we characterize the
advantage of MMCL by showing that text pairs induce more semantically
consistent and diverse positive pairs, which, according to our analysis,
provably benefit downstream generalization. Inspired by this finding, we
propose CLIP-guided resampling methods to significantly improve the downstream
performance of SSCL on ImageNet by leveraging multi-modal information. Code is
available at https://github.com/PKU-ML/CLIP-Help-SimCLR.",None,-1
Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model,0.389701,"While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.",None,-1
Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis,0.715418,"Multimodal relation extraction (MRE) is the task of identifying the semantic
relationships between two entities based on the context of the sentence image
pair. Existing retrieval-augmented approaches mainly focused on modeling the
retrieved textual knowledge, but this may not be able to accurately identify
complex relations. To improve the prediction, this research proposes to
retrieve textual and visual evidence based on the object, sentence, and whole
image. We further develop a novel approach to synthesize the object-level,
image-level, and sentence-level information for better reasoning between the
same and different modalities. Extensive experiments and analyses show that the
proposed method is able to effectively select and compare evidence across
modalities and significantly outperforms state-of-the-art models.",None,-1
NeuralKG-ind: A Python Library for Inductive Knowledge Graph Representation Learning,0.224397,"Since the dynamic characteristics of knowledge graphs, many inductive
knowledge graph representation learning (KGRL) works have been proposed in
recent years, focusing on enabling prediction over new entities. NeuralKG-ind
is the first library of inductive KGRL as an important update of NeuralKG
library. It includes standardized processes, rich existing methods, decoupled
modules, and comprehensive evaluation metrics. With NeuralKG-ind, it is easy
for researchers and engineers to reproduce, redevelop, and compare inductive
KGRL methods. The library, experimental methodologies, and model
re-implementing results of NeuralKG-ind are all publicly released at
https://github.com/zjukg/NeuralKG/tree/ind .",None,-1
Tag-Based Annotation for Avatar Face Creation,0.0906461,"Currently, digital avatars can be created manually using human images as
reference. Systems such as Bitmoji are excellent producers of detailed avatar
designs, with hundreds of choices for customization. A supervised learning
model could be trained to generate avatars automatically, but the hundreds of
possible options create difficulty in securing non-noisy data to train a model.
As a solution, we train a model to produce avatars from human images using
tag-based annotations. This method provides better annotator agreement, leading
to less noisy data and higher quality model predictions. Our contribution is an
application of tag-based annotation to train a model for avatar face creation.
We design tags for 3 different facial facial features offered by Bitmoji, and
train a model using tag-based annotation to predict the nose.",None,-1
STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,0.594313,"We address the problem of extracting key steps from unlabeled procedural
videos, motivated by the potential of Augmented Reality (AR) headsets to
revolutionize job training and performance. We decompose the problem into two
steps: representation learning and key steps extraction. We propose a training
objective, Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn
discriminative representations for various steps without any labels. Different
from prior works, we develop techniques to train a light-weight temporal module
which uses off-the-shelf features for self supervision. Our approach can
seamlessly leverage information from multiple cues like optical flow, depth or
gaze to learn discriminative features for key-steps, making it amenable for AR
applications. We finally extract key steps via a tunable algorithm that
clusters the representations and samples. We show significant improvements over
prior works for the task of key step localization and phase classification.
Qualitative results demonstrate that the extracted key steps are meaningful and
succinctly represent various steps of the procedural tasks.",None,-1
A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications,0.523567,"We present a framework for the automated measurement of responsible AI (RAI)
metrics for large language models (LLMs) and associated products and services.
Our framework for automatically measuring harms from LLMs builds on existing
technical and sociotechnical expertise and leverages the capabilities of
state-of-the-art LLMs, such as GPT-4. We use this framework to run through
several case studies investigating how different LLMs may violate a range of
RAI-related principles. The framework may be employed alongside domain-specific
sociotechnical expertise to create measurements for new harm areas in the
future. By implementing this framework, we aim to enable more advanced harm
measurement efforts and further the responsible use of LLMs.",None,-1
A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings,0.423898,"Predicting where a person is looking is a complex task, requiring to
understand not only the person's gaze and scene content, but also the 3D scene
structure and the person's situation (are they manipulating? interacting or
observing others? attentive?) to detect obstructions in the line of sight or
apply attention priors that humans typically have when observing others. In
this paper, we hypothesize that identifying and leveraging such priors can be
better achieved through the exploitation of explicitly derived multimodal cues
such as depth and pose. We thus propose a modular multimodal architecture
allowing to combine these cues using an attention mechanism. The architecture
can naturally be exploited in privacy-sensitive situations such as surveillance
and health, where personally identifiable information cannot be released. We
perform extensive experiments on the GazeFollow and VideoAttentionTarget public
datasets, obtaining state-of-the-art performance and demonstrating very
competitive results in the privacy setting case.",None,-1
Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection,0.432017,"This paper presents a novel framework, named Global-Local Correspondence
Framework (GLCF), for visual anomaly detection with logical constraints. Visual
anomaly detection has become an active research area in various real-world
applications, such as industrial anomaly detection and medical disease
diagnosis. However, most existing methods focus on identifying local structural
degeneration anomalies and often fail to detect high-level functional anomalies
that involve logical constraints. To address this issue, we propose a
two-branch approach that consists of a local branch for detecting structural
anomalies and a global branch for detecting logical anomalies. To facilitate
local-global feature correspondence, we introduce a novel semantic bottleneck
enabled by the visual Transformer. Moreover, we develop feature estimation
networks for each branch separately to detect anomalies. Our proposed framework
is validated using various benchmarks, including industrial datasets, Mvtec AD,
Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show
that our method outperforms existing methods, particularly in detecting logical
anomalies.",None,-1
All Things Considered: Detecting Partisan Events from News Media with Cross-Article Comparison,0.707932,"Public opinion is shaped by the information news media provide, and that
information in turn may be shaped by the ideological preferences of media
outlets. But while much attention has been devoted to media bias via overt
ideological language or topic selection, a more unobtrusive way in which the
media shape opinion is via the strategic inclusion or omission of partisan
events that may support one side or the other. We develop a latent
variable-based framework to predict the ideology of news articles by comparing
multiple articles on the same story and identifying partisan events whose
inclusion or omission reveals ideology. Our experiments first validate the
existence of partisan event selection, and then show that article alignment and
cross-document comparison detect partisan events and article ideology better
than competitive baselines. Our results reveal the high-level form of media
bias, which is present even among mainstream media with strong norms of
objectivity and nonpartisanship. Our codebase and dataset are available at
https://github.com/launchnlp/ATC.",None,-1
sustain.AI: a Recommender System to analyze Sustainability Reports,0.28515,"We present sustainAI, an intelligent, context-aware recommender system that
assists auditors and financial investors as well as the general public to
efficiently analyze companies' sustainability reports. The tool leverages an
end-to-end trainable architecture that couples a BERT-based encoding module
with a multi-label classification head to match relevant text passages from
sustainability reports to their respective law regulations from the Global
Reporting Initiative (GRI) standards. We evaluate our model on two novel German
sustainability reporting data sets and consistently achieve a significantly
higher recommendation performance compared to multiple strong baselines.
Furthermore, sustainAI is publicly available for everyone at
https://sustain.ki.nrw/.",None,-1
Creation and evaluation of timelines for longitudinal user posts,0.146452,"There is increasing interest to work with user generated content in social
media, especially textual posts over time. Currently there is no consistent way
of segmenting user posts into timelines in a meaningful way that improves the
quality and cost of manual annotation. Here we propose a set of methods for
segmenting longitudinal user posts into timelines likely to contain interesting
moments of change in a user's behaviour, based on their online posting
activity. We also propose a novel framework for evaluating timelines and show
its applicability in the context of two different social media datasets.
Finally, we present a discussion of the linguistic content of highly ranked
timelines.",None,-1
EHRTutor: Enhancing Patient Understanding of Discharge Instructions,0.341246,"Large language models have shown success as a tutor in education in various
fields. Educating patients about their clinical visits plays a pivotal role in
patients' adherence to their treatment plans post-discharge. This paper
presents EHRTutor, an innovative multi-component framework leveraging the Large
Language Model (LLM) for patient education through conversational
question-answering. EHRTutor first formulates questions pertaining to the
electronic health record discharge instructions. It then educates the patient
through conversation by administering each question as a test. Finally, it
generates a summary at the end of the conversation. Evaluation results using
LLMs and domain experts have shown a clear preference for EHRTutor over the
baseline. Moreover, EHRTutor also offers a framework for generating synthetic
patient education dialogues that can be used for future in-house system
training.",None,-1
Securing Deep Generative Models with Universal Adversarial Signature,0.10467,"Recent advances in deep generative models have led to the development of
methods capable of synthesizing high-quality, realistic images. These models
pose threats to society due to their potential misuse. Prior research attempted
to mitigate these threats by detecting generated images, but the varying traces
left by different generative models make it challenging to create a universal
detector capable of generalizing to new, unseen generative models. In this
paper, we propose to inject a universal adversarial signature into an arbitrary
pre-trained generative model, in order to make its generated contents more
detectable and traceable. First, the imperceptible optimal signature for each
image can be found by a signature injector through adversarial training.
Subsequently, the signature can be incorporated into an arbitrary generator by
fine-tuning it with the images processed by the signature injector. In this
way, the detector corresponding to the signature can be reused for any
fine-tuned generator for tracking the generator identity. The proposed method
is validated on the FFHQ and ImageNet datasets with various state-of-the-art
generative models, consistently showing a promising detection rate. Code will
be made publicly available at \url{https://github.com/zengxianyu/genwm}.",None,-1
"Learning with Constraint Learning: New Perspective, Solution Strategy and Various Applications",0.357507,"The complexity of learning problems, such as Generative Adversarial Network
(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,
and a variety of real-world vision applications, demands a deeper understanding
of their underlying coupling mechanisms. Existing approaches often address
these problems in isolation, lacking a unified perspective that can reveal
commonalities and enable effective solutions. Therefore, in this work, we
proposed a new framework, named Learning with Constraint Learning (LwCL), that
can holistically examine challenges and provide a unified methodology to tackle
all the above-mentioned complex learning and vision problems. Specifically,
LwCL is designed as a general hierarchical optimization model that captures the
essence of these diverse learning and vision problems. Furthermore, we develop
a gradient-response based fast solution strategy to overcome optimization
challenges of the LwCL framework. Our proposed framework efficiently addresses
a wide range of applications in learning and vision, encompassing three
categories and nine different problem types. Extensive experiments on synthetic
tasks and real-world applications verify the effectiveness of our approach. The
LwCL framework offers a comprehensive solution for tackling complex machine
learning and computer vision problems, bridging the gap between theory and
practice.",None,-1
TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage,0.950213,"With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.",None,-1
Balanced Energy Regularization Loss for Out-of-distribution Detection,0.746105,"In the field of out-of-distribution (OOD) detection, a previous method that
use auxiliary data as OOD data has shown promising performance. However, the
method provides an equal loss to all auxiliary data to differentiate them from
inliers. However, based on our observation, in various tasks, there is a
general imbalance in the distribution of the auxiliary OOD data across classes.
We propose a balanced energy regularization loss that is simple but generally
effective for a variety of tasks. Our balanced energy regularization loss
utilizes class-wise different prior probabilities for auxiliary data to address
the class imbalance in OOD data. The main concept is to regularize auxiliary
samples from majority classes, more heavily than those from minority classes.
Our approach performs better for OOD detection in semantic segmentation,
long-tailed image classification, and image classification than the prior
energy regularization loss. Furthermore, our approach achieves state-of-the-art
performance in two tasks: OOD detection in semantic segmentation and
long-tailed image classification. Code is available at
https://github.com/hyunjunChhoi/Balanced_Energy.",None,-1
ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency,0.69659,"We present ShapeClipper, a novel method that reconstructs 3D object shapes
from real-world single-view RGB images. Instead of relying on laborious 3D,
multi-view or camera pose annotation, ShapeClipper learns shape reconstruction
from a set of single-view segmented images. The key idea is to facilitate shape
learning via CLIP-based shape consistency, where we encourage objects with
similar CLIP encodings to share similar shapes. We also leverage off-the-shelf
normals as an additional geometric constraint so the model can learn better
bottom-up reasoning of detailed surface geometry. These two novel consistency
constraints, when used to regularize our model, improve its ability to learn
both global shape structure and local geometric details. We evaluate our method
over three challenging real-world datasets, Pix3D, Pascal3D+, and OpenImages,
where we achieve superior performance over state-of-the-art methods.",None,-1
Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion,0.999822,"We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Our
method is designed for a novel task, which is to convert a given 3D scene to
another scene according to text instructions. Instruct 3D-to-3D applies
pretrained Image-to-Image diffusion models for 3D-to-3D conversion. This
enables the likelihood maximization of each viewpoint image and high-quality 3D
generation. In addition, our proposed method explicitly inputs the source 3D
scene as a condition, which enhances 3D consistency and controllability of how
much of the source 3D scene structure is reflected. We also propose dynamic
scaling, which allows the intensity of the geometry transformation to be
adjusted. We performed quantitative and qualitative evaluations and showed that
our proposed method achieves higher quality 3D-to-3D conversions than baseline
methods.",None,-1
Adaptive Texture Filtering for Single-Domain Generalized Segmentation,0.0500268,"Domain generalization in semantic segmentation aims to alleviate the
performance degradation on unseen domains through learning domain-invariant
features. Existing methods diversify images in the source domain by adding
complex or even abnormal textures to reduce the sensitivity to domain specific
features. However, these approaches depend heavily on the richness of the
texture bank, and training them can be time-consuming. In contrast to importing
textures arbitrarily or augmenting styles randomly, we focus on the single
source domain itself to achieve generalization. In this paper, we present a
novel adaptive texture filtering mechanism to suppress the influence of texture
without using augmentation, thus eliminating the interference of
domain-specific features. Further, we design a hierarchical guidance
generalization network equipped with structure-guided enhancement modules,
which purpose is to learn the domain-invariant generalized knowledge. Extensive
experiments together with ablation studies on widely-used datasets are
conducted to verify the effectiveness of the proposed model, and reveal its
superiority over other state-of-the-art alternatives.",None,-1
SVIT: Scaling up Visual Instruction Tuning,0.834127,"Thanks to the emerging of foundation models, the large language and vision
models are integrated to acquire the multimodal ability of visual captioning,
question answering, etc. Although existing multimodal models present impressive
performance of visual understanding and reasoning, their limits are still
largely under-explored due to the scarcity of high-quality instruction tuning
data. To push the limits of multimodal capability, we Scale up Visual
Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual
instruction tuning data including 1.6M conversation question-answer (QA) pairs,
1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed
image descriptions. Besides the volume, the proposed dataset is also featured
by the high quality and rich diversity, which is generated by prompting GPT-4
with the abundant manual annotations of images. We also propose a new data
recipe to select subset with better diversity and balance, which evokes model's
superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on
the proposed dataset, outperforms state-of-the-art Multimodal Large Language
Models on popular benchmarks. The data and code are publicly available at
https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.",None,-1
Cross-head Supervision for Crowd Counting with Noisy Annotations,0.564541,"Noisy annotations such as missing annotations and location shifts often exist
in crowd counting datasets due to multi-scale head sizes, high occlusion, etc.
These noisy annotations severely affect the model training, especially for
density map-based methods. To alleviate the negative impact of noisy
annotations, we propose a novel crowd counting model with one convolution head
and one transformer head, in which these two heads can supervise each other in
noisy areas, called Cross-Head Supervision. The resultant model, CHS-Net, can
synergize different types of inductive biases for better counting. In addition,
we develop a progressive cross-head supervision learning strategy to stabilize
the training process and provide more reliable supervision. Extensive
experimental results on ShanghaiTech and QNRF datasets demonstrate superior
performance over state-of-the-art methods. Code is available at
https://github.com/RaccoonDML/CHSNet.",None,-1
Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints,0.355432,"Learning robust local image feature matching is a fundamental low-level
vision task, which has been widely explored in the past few years. Recently,
detector-free local feature matchers based on transformers have shown promising
results, which largely outperform pure Convolutional Neural Network (CNN) based
ones. But correlations produced by transformer-based methods are spatially
limited to the center of source views' coarse patches, because of the costly
attention learning. In this work, we rethink this issue and find that such
matching formulation degrades pose estimation, especially for low-resolution
images. So we propose a transformer-based cascade matching model -- Cascade
feature Matching TRansformer (CasMTR), to efficiently learn dense feature
correlations, which allows us to choose more reliable matching pairs for the
relative pose estimation. Instead of re-training a new detector, we use a
simple yet effective Non-Maximum Suppression (NMS) post-process to filter
keypoints through the confidence map, and largely improve the matching
precision. CasMTR achieves state-of-the-art performance in indoor and outdoor
pose estimation as well as visual localization. Moreover, thorough ablations
show the efficacy of the proposed components and techniques.",None,-1
KPIs-Based Clustering and Visualization of HPC jobs: a Feature Reduction Approach,0.538556,"High-Performance Computing (HPC) systems need to be constantly monitored to
ensure their stability. The monitoring systems collect a tremendous amount of
data about different parameters or Key Performance Indicators (KPIs), such as
resource usage, IO waiting time, etc. A proper analysis of this data, usually
stored as time series, can provide insight in choosing the right management
strategies as well as the early detection of issues. In this paper, we
introduce a methodology to cluster HPC jobs according to their KPI indicators.
Our approach reduces the inherent high dimensionality of the collected data by
applying two techniques to the time series: literature-based and variance-based
feature extraction. We also define a procedure to visualize the obtained
clusters by combining the two previous approaches and the Principal Component
Analysis (PCA). Finally, we have validated our contributions on a real data set
to conclude that those KPIs related to CPU usage provide the best cohesion and
separation for clustering analysis and the good results of our visualization
methodology.",None,-1
RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,0.582375,"Underwater images typically experience mixed degradations of brightness and
structure caused by the absorption and scattering of light by suspended
particles. To address this issue, we propose a Real-time Spatial and Frequency
Domains Modulation Network (RSFDM-Net) for the efficient enhancement of colors
and details in underwater images. Specifically, our proposed conditional
network is designed with Adaptive Fourier Gating Mechanism (AFGM) and
Multiscale Convolutional Attention Module (MCAM) to generate vectors carrying
low-frequency background information and high-frequency detail features, which
effectively promote the network to model global background information and
local texture details. To more precisely correct the color cast and low
saturation of the image, we introduce a Three-branch Feature Extraction (TFE)
block in the primary net that processes images pixel by pixel to integrate the
color information extended by the same channel (R, G, or B). This block
consists of three small branches, each of which has its own weights. Extensive
experiments demonstrate that our network significantly outperforms over
state-of-the-art methods in both visual quality and quantitative metrics.",None,-1
Hallucination Improves the Performance of Unsupervised Visual Representation Learning,0.877504,"Contrastive learning models based on Siamese structure have demonstrated
remarkable performance in self-supervised learning. Such a success of
contrastive learning relies on two conditions, a sufficient number of positive
pairs and adequate variations between them. If the conditions are not met,
these frameworks will lack semantic contrast and be fragile on overfitting. To
address these two issues, we propose Hallucinator that could efficiently
generate additional positive samples for further contrast. The Hallucinator is
differentiable and creates new data in the feature space. Thus, it is optimized
directly with the pre-training task and introduces nearly negligible
computation. Moreover, we reduce the mutual information of hallucinated pairs
and smooth them through non-linear operations. This process helps avoid
over-confident contrastive learning models during the training and achieves
more transformation-invariant feature embeddings. Remarkably, we empirically
prove that the proposed Hallucinator generalizes well to various contrastive
learning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linear
classification protocol, a stable accuracy gain is achieved, ranging from 0.3%
to 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement is
also observed in transferring pre-train encoders to the downstream tasks,
including object detection and segmentation.",None,-1
Conformal Prediction for Time Series with Modern Hopfield Networks,0.751651,"To quantify uncertainty, conformal prediction methods are gaining
continuously more interest and have already been successfully applied to
various domains. However, they are difficult to apply to time series as the
autocorrelative structure of time series violates basic assumptions required by
conformal prediction. We propose HopCPT, a novel conformal prediction approach
for time series that not only copes with temporal structures but leverages
them. We show that our approach is theoretically well justified for time series
where temporal dependencies are present. In experiments, we demonstrate that
our new approach outperforms state-of-the-art conformal prediction methods on
multiple real-world time series datasets from four different domains.",None,-1
Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction,0.975747,"We present Mask-then-Fill, a flexible and effective data augmentation
framework for event extraction. Our approach allows for more flexible
manipulation of text and thus can generate more diverse data while keeping the
original event structure unchanged as much as possible. Specifically, it first
randomly masks out an adjunct sentence fragment and then infills a
variable-length text span with a fine-tuned infilling model. The main advantage
lies in that it can replace a fragment of arbitrary length in the text with
another fragment of variable length, compared to the existing methods which can
only replace a single word or a fixed-length fragment. On trigger and argument
extraction tasks, the proposed framework is more effective than baseline
methods and it demonstrates particularly strong results in the low-resource
setting. Our further analysis shows that it achieves a good balance between
diversity and distributional similarity.",None,-1
Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection,0.679431,"Natural language processing models tend to learn and encode social biases
present in the data. One popular approach for addressing such biases is to
eliminate encoded information from the model's representations. However,
current methods are restricted to removing only linearly encoded information.
In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel
method for removing non-linear encoded concepts from neural representations.
Our method consists of iteratively training neural classifiers to predict a
particular attribute we seek to eliminate, followed by a projection of the
representation on a hypersurface, such that the classifiers become oblivious to
the target attribute. We evaluate the effectiveness of our method on the task
of removing gender and race information as sensitive attributes. Our results
demonstrate that IGBP is effective in mitigating bias through intrinsic and
extrinsic evaluations, with minimal impact on downstream task accuracy.",None,-1
A Novel Convolutional Neural Network Architecture with a Continuous Symmetry,0.0846499,"This paper introduces a new Convolutional Neural Network (ConvNet)
architecture inspired by a class of partial differential equations (PDEs)
called quasi-linear hyperbolic systems. With comparable performance on the
image classification task, it allows for the modification of the weights via a
continuous group of symmetry. This is a significant shift from traditional
models where the architecture and weights are essentially fixed. We wish to
promote the (internal) symmetry as a new desirable property for a neural
network, and to draw attention to the PDE perspective in analyzing and
interpreting ConvNets in the broader Deep Learning community.",None,-1
Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios,0.0557809,"In order to reveal the rationale behind model predictions, many works have
exploited providing explanations in various forms. Recently, to further
guarantee readability, more and more works turn to generate sentence-level
human language explanations. However, current works pursuing sentence-level
explanations rely heavily on annotated training data, which limits the
development of interpretability to only a few tasks. As far as we know, this
paper is the first to explore this problem smoothly from weak-supervised
learning to unsupervised learning. Besides, we also notice the high latency of
autoregressive sentence-level explanation generation, which leads to
asynchronous interpretability after prediction. Therefore, we propose a
non-autoregressive interpretable model to facilitate parallel explanation
generation and simultaneous prediction. Through extensive experiments on
Natural Language Inference task and Spouse Prediction task, we find that users
are able to train classifiers with comparable performance $10-15\times$ faster
with parallel explanation generation using only a few or no annotated training
data.",None,-1
OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for Real-time 3D Hand Gesture Classification and Segmentation,0.561454,"Continuous mid-air hand gesture recognition based on captured hand pose
streams is fundamental for human-computer interaction, particularly in AR / VR.
However, many of the methods proposed to recognize heterogeneous hand gestures
are tested only on the classification task, and the real-time low-latency
gesture segmentation in a continuous stream is not well addressed in the
literature. For this task, we propose the On-Off deep Multi-View Multi-Task
paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related
to hand pose and movement to generate rich gesture descriptions, along with
using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the
classical MVMT paradigm, where all of the multiple tasks have to be active at
each time, by allowing specific tasks to switch on/off depending on whether
they can apply to the input. We show that OO-dMVMT defines the new SotA on
continuous/online 3D skeleton-based gesture recognition in terms of gesture
classification accuracy, segmentation accuracy, false positives, and decision
latency while maintaining real-time operation.",None,-1
Towards Agile Text Classifiers for Everyone,0.377731,"Text-based safety classifiers are widely used for content moderation and
increasingly to tune generative language model behavior - a topic of growing
concern for the safety of digital assistants and chatbots. However, different
policies require different classifiers, and safety policies themselves improve
from iteration and adaptation. This paper introduces and evaluates methods for
agile text classification, whereby classifiers are trained using small,
targeted datasets that can be quickly developed for a particular policy.
Experimenting with 7 datasets from three safety-related domains, comprising 15
annotation schemes, led to our key finding: prompt-tuning large language
models, like PaLM 62B, with a labeled dataset of as few as 80 examples can
achieve state-of-the-art performance. We argue that this enables a paradigm
shift for text classification, especially for models supporting safer online
discourse. Instead of collecting millions of examples to attempt to create
universal safety classifiers over months or years, classifiers could be tuned
using small datasets, created by individuals or small organizations, tailored
for specific use cases, and iterated on and adapted in the time-span of a day.",None,-1
Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization,0.44432,"Multilingual sentence representations are the foundation for similarity-based
bitext mining, which is crucial for scaling multilingual neural machine
translation (NMT) system to more languages. In this paper, we introduce MuSR: a
one-for-all Multilingual Sentence Representation model that supports more than
220 languages. Leveraging billions of English-centric parallel corpora, we
train a multilingual Transformer encoder, coupled with an auxiliary Transformer
decoder, by adopting a multilingual NMT framework with CrossConST, a
cross-lingual consistency regularization technique proposed in Gao et al.
(2023). Experimental results on multilingual similarity search and bitext
mining tasks show the effectiveness of our approach. Specifically, MuSR
achieves superior performance over LASER3 (Heffernan et al., 2022) which
consists of 148 independent multilingual sentence encoders.",None,-1
Does progress on ImageNet transfer to real-world datasets?,0.459414,"Does progress on ImageNet transfer to real-world datasets? We investigate
this question by evaluating ImageNet pre-trained models with varying accuracy
(57% - 83%) on six practical image classification datasets. In particular, we
study datasets collected with the goal of solving real-world tasks (e.g.,
classifying images from camera traps or satellites), as opposed to web-scraped
benchmarks collected for comparing models. On multiple datasets, models with
higher ImageNet accuracy do not consistently yield performance improvements.
For certain tasks, interventions such as data augmentation improve performance
even when architectures do not. We hope that future benchmarks will include
more diverse datasets to encourage a more comprehensive approach to improving
learning algorithms.",None,-1
Designing Behavior Trees from Goal-Oriented LTLf Formulas,0.737184,"Temporal logic can be used to formally specify autonomous agent goals, but
synthesizing planners that guarantee goal satisfaction can be computationally
prohibitive. This paper shows how to turn goals specified using a subset of
finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that
guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for
achievement goals can be derived using achievement-oriented task mission
grammars, leading to missions made up of tasks combined using LTL operators.
Constructing BTs from LTL formulas leads to a relaxed behavior synthesis
problem in which a wide range of planners can implement the action nodes in the
BT. Importantly, any successful trace induced by the planners satisfies the
corresponding LTL formula. The usefulness of the approach is demonstrated in
two ways: a) exploring the alignment between two planners and LTL goals, and b)
solving a sequential key-door problem for a Fetch robot.",None,-1
AdaOPC: A Self-Adaptive Mask Optimization Framework For Real Design Patterns,0.984528,"Optical proximity correction (OPC) is a widely-used resolution enhancement
technique (RET) for printability optimization. Recently, rigorous numerical
optimization and fast machine learning are the research focus of OPC in both
academia and industry, each of which complements the other in terms of
robustness or efficiency. We inspect the pattern distribution on a design layer
and find that different sub-regions have different pattern complexity. Besides,
we also find that many patterns repetitively appear in the design layout, and
these patterns may possibly share optimized masks. We exploit these properties
and propose a self-adaptive OPC framework to improve efficiency. Firstly we
choose different OPC solvers adaptively for patterns of different complexity
from an extensible solver pool to reach a speed/accuracy co-optimization. Apart
from that, we prove the feasibility of reusing optimized masks for repeated
patterns and hence, build a graph-based dynamic pattern library reusing stored
masks to further speed up the OPC flow. Experimental results show that our
framework achieves substantial improvement in both performance and efficiency.",None,-1
Text-to-SQL Error Correction with Language Models of Code,0.632121,"Despite recent progress in text-to-SQL parsing, current semantic parsers are
still not accurate enough for practical use. In this paper, we investigate how
to build automatic text-to-SQL error correction models. Noticing that
token-level edits are out of context and sometimes ambiguous, we propose
building clause-level edit models instead. Besides, while most language models
of code are not specifically pre-trained for SQL, they know common data
structures and their operations in programming languages such as Python. Thus,
we propose a novel representation for SQL queries and their edits that adheres
more closely to the pre-training corpora of language models of code. Our error
correction model improves the exact set match accuracy of different parsers by
2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong
baselines. Our code and data are available at
https://github.com/OSU-NLP-Group/Auto-SQL-Correction.",None,-1
Prompting for Multimodal Hateful Meme Classification,0.99798,"Hateful meme classification is a challenging multimodal task that requires
complex reasoning and contextual background knowledge. Ideally, we could
leverage an explicit external knowledge base to supplement contextual and
cultural information in hateful memes. However, there is no known explicit
external knowledge base that could provide such hate speech contextual
information. To address this gap, we propose PromptHate, a simple yet effective
prompt-based model that prompts pre-trained language models (PLMs) for hateful
meme classification. Specifically, we construct simple prompts and provide a
few in-context examples to exploit the implicit knowledge in the pre-trained
RoBERTa language model for hateful meme classification. We conduct extensive
experiments on two publicly available hateful and offensive meme datasets. Our
experimental results show that PromptHate is able to achieve a high AUC of
90.96, outperforming state-of-the-art baselines on the hateful meme
classification task. We also perform fine-grained analyses and case studies on
various prompt settings and demonstrate the effectiveness of the prompts on
hateful meme classification.",None,-1
Evaluating Online Bandit Exploration In Large-Scale Recommender System,0.442494,"Bandit learning has been an increasingly popular design choice for
recommender system. Despite the strong interest in bandit learning from the
community, there remains multiple bottlenecks that prevent many bandit learning
approaches from productionalization. One major bottleneck is how to test the
effectiveness of bandit algorithm with fairness and without data leakage.
Different from supervised learning algorithms, bandit learning algorithms
emphasize greatly on the data collection process through their explorative
nature. Such explorative behavior may induce unfair evaluation in a classic A/B
test setting. In this work, we apply upper confidence bound (UCB) to our large
scale short video recommender system and present a test framework for the
production bandit learning life-cycle with a new set of metrics. Extensive
experiment results show that our experiment design is able to fairly evaluate
the performance of bandit learning in the recommender system.",None,-1
An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models,0.236219,"Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from
massive human-written data which contains latent societal biases and toxic
contents. In this paper, we leverage the primary task of PTLMs, i.e., language
modeling, and propose a new metric to quantify manifested implicit
representational harms in PTLMs towards 13 marginalized demographics. Using
this metric, we conducted an empirical analysis of 24 widely used PTLMs. Our
analysis provides insights into the correlation between the proposed metric in
this work and other related metrics for representational harm. We observe that
our metric correlates with most of the gender-specific metrics in the
literature. Through extensive experiments, we explore the connections between
PTLMs architectures and representational harms across two dimensions: depth and
width of the networks. We found that prioritizing depth over width, mitigates
representational harms in some PTLMs. Our code and data can be found at
https://github.com/microsoft/SafeNLP.",None,-1
The Elements of Visual Art Recommendation: Learning Latent Semantic Representations of Paintings,0.356447,"Artwork recommendation is challenging because it requires understanding how
users interact with highly subjective content, the complexity of the concepts
embedded within the artwork, and the emotional and cognitive reflections they
may trigger in users. In this paper, we focus on efficiently capturing the
elements (i.e., latent semantic relationships) of visual art for personalized
recommendation. We propose and study recommender systems based on textual and
visual feature learning techniques, as well as their combinations. We then
perform a small-scale and a large-scale user-centric evaluation of the quality
of the recommendations. Our results indicate that textual features compare
favourably with visual ones, whereas a fusion of both captures the most
suitable hidden semantic relationships for artwork recommendation. Ultimately,
this paper contributes to our understanding of how to deliver content that
suitably matches the user's interests and how they are perceived.",None,-1
Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,0.500376,"Large Language Models (LLMs) have demonstrated exceptional natural language
understanding abilities and have excelled in a variety of natural language
processing (NLP)tasks in recent years. Despite the fact that most LLMs are
trained predominantly in English, multiple studies have demonstrated their
comparative performance in many other languages. However, fundamental questions
persist regarding how LLMs acquire their multi-lingual abilities and how
performance varies across different languages. These inquiries are crucial for
the study of LLMs since users and researchers often come from diverse language
backgrounds, potentially influencing their utilization and interpretation of
LLMs' results. In this work, we propose a systematic way of qualifying the
performance disparities of LLMs under multilingual settings. We investigate the
phenomenon of across-language generalizations in LLMs, wherein insufficient
multi-lingual training data leads to advanced multi-lingual capabilities. To
accomplish this, we employ a novel back-translation-based prompting method. The
results show that GPT exhibits highly translating-like behaviour in
multilingual settings.",None,-1
Learning Concise and Descriptive Attributes for Visual Recognition,0.932853,"Recent advances in foundation models present new opportunities for
interpretable visual recognition -- one can first query Large Language Models
(LLMs) to obtain a set of attributes that describe each class, then apply
vision-language models to classify images via these attributes. Pioneering work
shows that querying thousands of attributes can achieve performance competitive
with image features. However, our further investigation on 8 datasets reveals
that LLM-generated attributes in a large quantity perform almost the same as
random words. This surprising finding suggests that significant noise may be
present in these attributes. We hypothesize that there exist subsets of
attributes that can maintain the classification performance with much smaller
sizes, and propose a novel learning-to-search method to discover those concise
sets of attributes. As a result, on the CUB dataset, our method achieves
performance close to that of massive LLM-generated attributes (e.g., 10k
attributes for CUB), yet using only 32 attributes in total to distinguish 200
bird species. Furthermore, our new paradigm demonstrates several additional
benefits: higher interpretability and interactivity for humans, and the ability
to summarize knowledge for a recognition task.",None,-1
Interactive Explanations by Conflict Resolution via Argumentative Exchanges,0.252781,"As the field of explainable AI (XAI) is maturing, calls for interactive
explanations for (the outputs of) AI models are growing, but the
state-of-the-art predominantly focuses on static explanations. In this paper,
we focus instead on interactive explanations framed as conflict resolution
between agents (i.e. AI models and/or humans) by leveraging on computational
argumentation. Specifically, we define Argumentative eXchanges (AXs) for
dynamically sharing, in multi-agent systems, information harboured in
individual agents' quantitative bipolar argumentation frameworks towards
resolving conflicts amongst the agents. We then deploy AXs in the XAI setting
in which a machine and a human interact about the machine's predictions. We
identify and assess several theoretical properties characterising AXs that are
suitable for XAI. Finally, we instantiate AXs for XAI by defining various agent
behaviours, e.g. capturing counterfactual patterns of reasoning in machines and
highlighting the effects of cognitive biases in humans. We show experimentally
(in a simulated environment) the comparative advantages of these behaviours in
terms of conflict resolution, and show that the strongest argument may not
always be the most effective.",None,-1
Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining,0.346755,"Medical artificial general intelligence (MAGI) enables one foundation model
to solve different medical tasks, which is very practical in the medical
domain. It can significantly reduce the requirement of large amounts of
task-specific data by sufficiently sharing medical knowledge among different
tasks. However, due to the challenges of designing strongly generalizable
models with limited and complex medical data, most existing approaches tend to
develop task-specific models. To take a step towards MAGI, we propose a new
paradigm called Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR). In
MOTOR, we combine two kinds of basic medical knowledge, i.e., general and
specific knowledge, in a complementary manner to boost the general pretraining
process. As a result, the foundation model with comprehensive basic knowledge
can learn compact representations from pretraining radiographic data for better
cross-modal alignment. MOTOR unifies the understanding and generation, which
are two kinds of core intelligence of an AI system, into a single medical
foundation model, to flexibly handle more diverse medical tasks. To enable a
comprehensive evaluation and facilitate further research, we construct a
medical multimodal benchmark including a wide range of downstream tasks, such
as chest x-ray report generation and medical visual question answering.
Extensive experiments on our benchmark show that MOTOR obtains promising
results through simple task-oriented adaptation. The visualization shows that
the injected knowledge successfully highlights key information in the medical
data, demonstrating the excellent interpretability of MOTOR. Our MOTOR
successfully mimics the human practice of fulfilling a ""medical student"" to
accelerate the process of becoming a ""specialist"". We believe that our work
makes a significant stride in realizing MAGI.",None,-1
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,0.540743,"While natural languages differ widely in both canonical word order and word
order flexibility, their word orders still follow shared cross-linguistic
statistical patterns, often attributed to functional pressures. In the effort
to identify these pressures, prior work has compared real and counterfactual
word orders. Yet one functional pressure has been overlooked in such
investigations: the uniform information density (UID) hypothesis, which holds
that information should be spread evenly throughout an utterance. Here, we ask
whether a pressure for UID may have influenced word order patterns
cross-linguistically. To this end, we use computational models to test whether
real orders lead to greater information uniformity than counterfactual orders.
In our empirical study of 10 typologically diverse languages, we find that: (i)
among SVO languages, real word orders consistently have greater uniformity than
reverse word orders, and (ii) only linguistically implausible counterfactual
orders consistently exceed the uniformity of real orders. These findings are
compatible with a pressure for information uniformity in the development and
usage of natural languages.",None,-1
Enhanced Multimodal Representation Learning with Cross-modal KD,0.242593,"This paper explores the tasks of leveraging auxiliary modalities which are
only available at training to enhance multimodal representation learning
through cross-modal Knowledge Distillation (KD). The widely adopted mutual
information maximization-based objective leads to a short-cut solution of the
weak teacher, i.e., achieving the maximum mutual information by simply making
the teacher model as weak as the student model. To prevent such a weak
solution, we introduce an additional objective term, i.e., the mutual
information between the teacher and the auxiliary modality model. Besides, to
narrow down the information gap between the student and teacher, we further
propose to minimize the conditional entropy of the teacher given the student.
Novel training schemes based on contrastive learning and adversarial learning
are designed to optimize the mutual information and the conditional entropy,
respectively. Experimental results on three popular multimodal benchmark
datasets have shown that the proposed method outperforms a range of
state-of-the-art approaches for video recognition, video retrieval and emotion
classification.",None,-1
Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,0.870795,"Object affordance is an important concept in hand-object interaction,
providing information on action possibilities based on human motor capacity and
objects' physical property thus benefiting tasks such as action anticipation
and robot imitation learning. However, the definition of affordance in existing
datasets often: 1) mix up affordance with object functionality; 2) confuse
affordance with goal-related action; and 3) ignore human motor capacity. This
paper proposes an efficient annotation scheme to address these issues by
combining goal-irrelevant motor actions and grasp types as affordance labels
and introducing the concept of mechanical action to represent the action
possibilities between two objects. We provide new annotations by applying this
scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as
affordance recognition, hand-object interaction hotspots prediction, and
cross-domain evaluation of affordance. The results show that models trained
with our annotation can distinguish affordance from other concepts, predict
fine-grained interaction possibilities on objects, and generalize through
different domains.",None,-1
Multilingual Controllable Transformer-Based Lexical Simplification,0.144759,"Text is by far the most ubiquitous source of knowledge and information and
should be made easily accessible to as many people as possible; however, texts
often contain complex words that hinder reading comprehension and
accessibility. Therefore, suggesting simpler alternatives for complex words
without compromising meaning would help convey the information to a broader
audience. This paper proposes mTLS, a multilingual controllable
Transformer-based Lexical Simplification (LS) system fined-tuned with the T5
model. The novelty of this work lies in the use of language-specific prefixes,
control tokens, and candidates extracted from pre-trained masked language
models to learn simpler alternatives for complex words. The evaluation results
on three well-known LS datasets -- LexMTurk, BenchLS, and NNSEval -- show that
our model outperforms the previous state-of-the-art models like LSBert and
ConLS. Moreover, further evaluation of our approach on the part of the recent
TSAR-2022 multilingual LS shared-task dataset shows that our model performs
competitively when compared with the participating systems for English LS and
even outperforms the GPT-3 model on several metrics. Moreover, our model
obtains performance gains also for Spanish and Portuguese.",None,-1
Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?,0.493521,"Centralized Training with Decentralized Execution (CTDE) has recently emerged
as a popular framework for cooperative Multi-Agent Reinforcement Learning
(MARL), where agents can use additional global state information to guide
training in a centralized way and make their own decisions only based on
decentralized local policies. Despite the encouraging results achieved, CTDE
makes an independence assumption on agent policies, which limits agents to
adopt global cooperative information from each other during centralized
training. Therefore, we argue that existing CTDE methods cannot fully utilize
global information for training, leading to an inefficient joint-policy
exploration and even suboptimal results. In this paper, we introduce a novel
Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent
reinforcement learning, that not only enables an efficacious message exchange
among agents during training but also guarantees the independent policies for
execution. Firstly, CADP endows agents the explicit communication channel to
seek and take advices from different agents for more centralized training. To
further ensure the decentralized execution, we propose a smooth model pruning
mechanism to progressively constraint the agent communication into a closed one
without degradation in agent cooperation capability. Empirical evaluations on
StarCraft II micromanagement and Google Research Football benchmarks
demonstrate that the proposed framework achieves superior performance compared
with the state-of-the-art counterparts. Our code will be made publicly
available.",None,-1
Enriching language models with graph-based context information to better understand textual data,0.080906,"A considerable number of texts encountered daily are somehow connected with
each other. For example, Wikipedia articles refer to other articles via
hyperlinks, scientific papers relate to others via citations or (co)authors,
while tweets relate via users that follow each other or reshare content. Hence,
a graph-like structure can represent existing connections and be seen as
capturing the ""context"" of the texts. The question thus arises if extracting
and integrating such context information into a language model might help
facilitate a better automated understanding of the text. In this study, we
experimentally demonstrate that incorporating graph-based contextualization
into BERT model enhances its performance on an example of a classification
task. Specifically, on Pubmed dataset, we observed a reduction in error from
8.51% to 7.96%, while increasing the number of parameters just by 1.6%.
  Our source code: https://github.com/tryptofanik/gc-bert",None,-1
A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation,0.611043,"The task of semantic segmentation requires a model to assign semantic labels
to each pixel of an image. However, the performance of such models degrades
when deployed in an unseen domain with different data distributions compared to
the training domain. We present a new augmentation-driven approach to domain
generalization for semantic segmentation using a re-parameterized vision
transformer (ReVT) with weight averaging of multiple models after training. We
evaluate our approach on several benchmark datasets and achieve
state-of-the-art mIoU performance of 47.3% (prior art: 46.3%) for small models
and of 50.1% (prior art: 47.8%) for midsized models on commonly used benchmark
datasets. At the same time, our method requires fewer parameters and reaches a
higher frame rate than the best prior art. It is also easy to implement and,
unlike network ensembles, does not add any computational complexity during
inference.",None,-1
DartsReNet: Exploring new RNN cells in ReNet architectures,0.0486543,"We present new Recurrent Neural Network (RNN) cells for image classification
using a Neural Architecture Search (NAS) approach called DARTS. We are
interested in the ReNet architecture, which is a RNN based approach presented
as an alternative for convolutional and pooling steps. ReNet can be defined
using any standard RNN cells, such as LSTM and GRU. One limitation is that
standard RNN cells were designed for one dimensional sequential data and not
for two dimensions like it is the case for image classification. We overcome
this limitation by using DARTS to find new cell designs. We compare our results
with ReNet that uses GRU and LSTM cells. Our found cells outperform the
standard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicate
generalizability, as we derived the RNN cell designs from CIFAR-10 without
performing a new cell search for SVHN.",None,-1
Diverse Motion In-betweening with Dual Posture Stitching,0.192759,"In-betweening is a technique for generating transitions given initial and
target character states. The majority of existing works require multiple (often
$>$10) frames as input, which are not always accessible. Our work deals with a
focused yet challenging problem: to generate the transition when given exactly
two frames (only the first and last). To cope with this challenging scenario,
we implement our bi-directional scheme which generates forward and backward
transitions from the start and end frames with two adversarial autoregressive
networks, and stitches them in the middle of the transition where there is no
strict ground truth. The autoregressive networks based on conditional
variational autoencoders (CVAE) are optimized by searching for a pair of
optimal latent codes that minimize a novel stitching loss between their
outputs. Results show that our method achieves higher motion quality and more
diverse results than existing methods on both the LaFAN1 and Human3.6m
datasets.",None,-1
SAINE: Scientific Annotation and Inference Engine of Scientific Research,0.0375697,"We present SAINE, an Scientific Annotation and Inference ENgine based on a
set of standard open-source software, such as Label Studio and MLflow. We show
that our annotation engine can benefit the further development of a more
accurate classification. Based on our previous work on hierarchical discipline
classifications, we demonstrate its application using SAINE in understanding
the space for scholarly publications. The user study of our annotation results
shows that user input collected with the help of our system can help us better
understand the classification process. We believe that our work will help to
foster greater transparency and better understand scientific research. Our
annotation and inference engine can further support the downstream meta-science
projects. We welcome collaboration and feedback from the scientific community
on these projects. The demonstration video can be accessed from
https://youtu.be/yToO-G9YQK4. A live demo website is available at
https://app.heartex.com/user/signup/?token=e2435a2f97449fa1 upon free
registration.",None,-1
Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint,0.627672,"Clinical trials are indispensable in developing new treatments, but they face
obstacles in patient recruitment and retention, hindering the enrollment of
necessary participants. To tackle these challenges, deep learning frameworks
have been created to match patients to trials. These frameworks calculate the
similarity between patients and clinical trial eligibility criteria,
considering the discrepancy between inclusion and exclusion criteria. Recent
studies have shown that these frameworks outperform earlier approaches.
However, deep learning models may raise fairness issues in patient-trial
matching when certain sensitive groups of individuals are underrepresented in
clinical trials, leading to incomplete or inaccurate data and potential harm.
To tackle the issue of fairness, this work proposes a fair patient-trial
matching framework by generating a patient-criterion level fairness constraint.
The proposed framework considers the inconsistency between the embedding of
inclusion and exclusion criteria among patients of different sensitive groups.
The experimental results on real-world patient-trial and patient-criterion
matching tasks demonstrate that the proposed framework can successfully
alleviate the predictions that tend to be biased.",None,-1
Contrastive Multi-Task Dense Prediction,0.371477,"This paper targets the problem of multi-task dense prediction which aims to
achieve simultaneous learning and inference on a bunch of multiple dense
prediction tasks in a single framework. A core objective in design is how to
effectively model cross-task interactions to achieve a comprehensive
improvement on different tasks based on their inherent complementarity and
consistency. Existing works typically design extra expensive distillation
modules to perform explicit interaction computations among different
task-specific features in both training and inference, bringing difficulty in
adaptation for different task sets, and reducing efficiency due to clearly
increased size of multi-task models. In contrast, we introduce feature-wise
contrastive consistency into modeling the cross-task interactions for
multi-task dense prediction. We propose a novel multi-task contrastive
regularization method based on the consistency to effectively boost the
representation learning of the different sub-tasks, which can also be easily
generalized to different multi-task dense prediction frameworks, and costs no
additional computation in the inference. Extensive experiments on two
challenging datasets (i.e. NYUD-v2 and Pascal-Context) clearly demonstrate the
superiority of the proposed multi-task contrastive learning approach for dense
predictions, establishing new state-of-the-art performances.",None,-1
Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft,0.33202,"Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.",None,-1
DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition,0.989553,"Implicit Discourse Relation Recognition (IDRR) is a sophisticated and
challenging task to recognize the discourse relations between the arguments
with the absence of discourse connectives. The sense labels for each discourse
relation follow a hierarchical classification scheme in the annotation process
(Prasad et al., 2008), forming a hierarchy structure. Most existing works do
not well incorporate the hierarchy structure but focus on the syntax features
and the prior knowledge of connectives in the manner of pure text
classification. We argue that it is more effective to predict the paths inside
the hierarchical tree (e.g., ""Comparison -> Contrast -> however"") rather than
flat labels (e.g., Contrast) or connectives (e.g., however). We propose a
prompt-based path prediction method to utilize the interactive information and
intrinsic senses among the hierarchy in IDRR. This is the first work that
injects such structure information into pre-trained language models via prompt
tuning, and the performance of our solution shows significant and consistent
improvement against competitive baselines.",None,-1
SeqXGPT: Sentence-Level AI-Generated Text Detection,0.871646,"Widely applied large language models (LLMs) can generate human-like content,
raising concerns about the abuse of LLMs. Therefore, it is important to build
strong AI-generated text (AIGT) detectors. Current works only consider
document-level AIGT detection, therefore, in this paper, we first introduce a
sentence-level detection challenge by synthesizing a dataset that contains
documents that are polished with LLMs, that is, the documents contain sentences
written by humans and sentences modified by LLMs. Then we propose
\textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes
log probability lists from white-box LLMs as features for sentence-level AIGT
detection. These features are composed like \textit{waves} in speech processing
and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution
and self-attention networks. We test it in both sentence and document-level
detection challenges. Experimental results show that previous methods struggle
in solving sentence-level AIGT detection, while our method not only
significantly surpasses baseline methods in both sentence and document-level
detection challenges but also exhibits strong generalization capabilities.",None,-1
The Pipeline System of ASR and NLU with MLM-based Data Augmentation toward STOP Low-resource Challenge,0.06735,"This paper describes our system for the low-resource domain adaptation track
(Track 3) in Spoken Language Understanding Grand Challenge, which is a part of
ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a
pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain
with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on
low-resource domain data. We apply masked LM (MLM) -based data augmentation,
where some of input tokens and corresponding target labels are replaced using
MLM. We also apply a retrieval-based approach, where model input is augmented
with similar training samples. As a result, we achieved exact match (EM)
accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the
1st place at the challenge.",None,-1
Non-Contrastive Unsupervised Learning of Physiological Signals from Video,0.917498,"Subtle periodic signals such as blood volume pulse and respiration can be
extracted from RGB video, enabling remote health monitoring at low cost.
Advancements in remote pulse estimation -- or remote photoplethysmography
(rPPG) -- are currently driven by deep learning solutions. However, modern
approaches are trained and evaluated on benchmark datasets with associated
ground truth from contact-PPG sensors. We present the first non-contrastive
unsupervised learning framework for signal regression to break free from the
constraints of labelled video data. With minimal assumptions of periodicity and
finite bandwidth, our approach is capable of discovering the blood volume pulse
directly from unlabelled videos. We find that encouraging sparse power spectra
within normal physiological bandlimits and variance over batches of power
spectra is sufficient for learning visual features of periodic signals. We
perform the first experiments utilizing unlabelled video data not specifically
created for rPPG to train robust pulse rate estimators. Given the limited
inductive biases and impressive empirical results, the approach is
theoretically capable of discovering other periodic signals from video,
enabling multiple physiological measurements without the need for ground truth
signals. Codes to fully reproduce the experiments are made available along with
the paper.",None,-1
De-coupling and De-positioning Dense Self-supervised Learning,0.0858518,"Dense Self-Supervised Learning (SSL) methods address the limitations of using
image-level feature representations when handling images with multiple objects.
Although the dense features extracted by employing segmentation maps and
bounding boxes allow networks to perform SSL for each object, we show that they
suffer from coupling and positional bias, which arise from the receptive field
increasing with layer depth and zero-padding. We address this by introducing
three data augmentation strategies, and leveraging them in (i) a decoupling
module that aims to robustify the network to variations in the object's
surroundings, and (ii) a de-positioning module that encourages the network to
discard positional object information. We demonstrate the benefits of our
method on COCO and on a new challenging benchmark, OpenImage-MINI, for object
classification, semantic segmentation, and object detection. Our extensive
experiments evidence the better generalization of our method compared to the
SOTA dense SSL methods",None,-1
An Expression Tree Decoding Strategy for Mathematical Equation Generation,0.502986,"Generating mathematical equations from natural language requires an accurate
understanding of the relations among math expressions. Existing approaches can
be broadly categorized into token-level and expression-level generation. The
former treats equations as a mathematical language, sequentially generating
math tokens. Expression-level methods generate each expression one by one.
However, each expression represents a solving step, and there naturally exist
parallel or dependent relations between these steps, which are ignored by
current sequential methods. Therefore, we integrate tree structure into the
expression-level generation and advocate an expression tree decoding strategy.
To generate a tree with expression as its node, we employ a layer-wise parallel
decoding strategy: we decode multiple independent expressions (leaf nodes) in
parallel at each layer and repeat parallel decoding layer by layer to
sequentially generate these parent node expressions that depend on others.
Besides, a bipartite matching algorithm is adopted to align multiple
predictions with annotations for each layer. Experiments show our method
outperforms other baselines, especially for these equations with complex
structures.",None,-1
On the Risk of Misinformation Pollution with Large Language Models,0.343358,"In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.",None,-1
Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,0.608563,"We propose the first metric learning system for the recognition of great ape
behavioural actions. Our proposed triple stream embedding architecture works on
camera trap videos taken directly in the wild and demonstrates that the
utilisation of an explicit DensePose-C chimpanzee body part segmentation stream
effectively complements traditional RGB appearance and optical flow streams. We
evaluate system variants with different feature fusion techniques and long-tail
recognition approaches. Results and ablations show performance improvements of
~12% in top-1 accuracy over previous results achieved on the PanAf-500 dataset
containing 180,000 manually annotated frames across nine behavioural actions.
Furthermore, we provide a qualitative analysis of our findings and augment the
metric learning system with long-tail recognition techniques showing that
average per class accuracy -- critical in the domain -- can be improved by ~23%
compared to the literature on that dataset. Finally, since our embedding spaces
are constructed as metric, we provide first data-driven visualisations of the
great ape behavioural action spaces revealing emerging geometry and topology.
We hope that the work sparks further interest in this vital application area of
computer vision for the benefit of endangered great apes.",None,-1
GridMM: Grid Memory Map for Vision-and-Language Navigation,0.784122,"Vision-and-language navigation (VLN) enables the agent to navigate to a
remote location following the natural language instruction in 3D environments.
To represent the previously visited environment, most approaches for VLN
implement memory using recurrent states, topological maps, or top-down semantic
maps. In contrast to these approaches, we build the top-down egocentric and
dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited
environment. From a global perspective, historical observations are projected
into a unified grid map in a top-down view, which can better represent the
spatial relations of the environment. From a local perspective, we further
propose an instruction relevance aggregation method to capture fine-grained
visual clues in each grid region. Extensive experiments are conducted on both
the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE
dataset in the continuous environments, showing the superiority of our proposed
method.",None,-1
Multi-Head Feature Pyramid Networks for Breast Mass Detection,0.316048,"Analysis of X-ray images is one of the main tools to diagnose breast cancer.
The ability to quickly and accurately detect the location of masses from the
huge amount of image data is the key to reducing the morbidity and mortality of
breast cancer. Currently, the main factor limiting the accuracy of breast mass
detection is the unequal focus on the mass boxes, leading the network to focus
too much on larger masses at the expense of smaller ones. In the paper, we
propose the multi-head feature pyramid module (MHFPN) to solve the problem of
unbalanced focus of target boxes during feature map fusion and design a
multi-head breast mass detection network (MBMDnet). Experimental studies show
that, comparing to the SOTA detection baselines, our method improves by 6.58%
(in AP@50) and 5.4% (in TPR@50) on the commonly used INbreast dataset, while
about 6-8% improvements (in AP@20) are also observed on the public MIAS and
BCS-DBT datasets.",None,-1
Multi-View Keypoints for Reliable 6D Object Pose Estimation,0.572846,"6D Object pose estimation is a fundamental component in robotics enabling
efficient interaction with the environment. It is particularly challenging in
bin-picking applications, where many objects are low-feature and reflective,
and self-occlusion between objects of the same type is common. We propose a
novel multi-view approach leveraging known camera transformations from an
eye-in-hand setup to combine heatmap and keypoint estimates into a probability
density map over 3D space. The result is a robust approach that is scalable in
the number of views. It relies on a confidence score composed of keypoint
probabilities and point-cloud alignment error, which allows reliable rejection
of false positives. We demonstrate an average pose estimation error of
approximately 0.5mm and 2 degrees across a variety of difficult low-feature and
reflective objects in the ROBI dataset, while also surpassing the state-of-art
correct detection rate, measured using the 10% object diameter threshold on ADD
error.",None,-1
Document-Level Language Models for Machine Translation,0.610523,"Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.",None,-1
FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation,0.339386,"We present FACADE, a novel probabilistic and geometric framework designed for
unsupervised mechanistic anomaly detection in deep neural networks. Its primary
goal is advancing the understanding and mitigation of adversarial attacks.
FACADE aims to generate probabilistic distributions over circuits, which
provide critical insights to their contribution to changes in the manifold
properties of pseudo-classes, or high-dimensional modes in activation space,
yielding a powerful tool for uncovering and combating adversarial attacks. Our
approach seeks to improve model robustness, enhance scalable model oversight,
and demonstrates promising applications in real-world deployment settings.",None,-1
Infinite Photorealistic Worlds using Procedural Generation,0.343849,"We introduce Infinigen, a procedural generator of photorealistic 3D scenes of
the natural world. Infinigen is entirely procedural: every asset, from shape to
texture, is generated from scratch via randomized mathematical rules, using no
external source and allowing infinite variation and composition. Infinigen
offers broad coverage of objects and scenes in the natural world including
plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and
snow. Infinigen can be used to generate unlimited, diverse training data for a
wide range of computer vision tasks including object detection, semantic
segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a
useful resource for computer vision research and beyond. Please visit
https://infinigen.org for videos, code and pre-generated data.",None,-1
Improving Language Models via Plug-and-Play Retrieval Feedback,0.725941,"Large language models (LLMs) exhibit remarkable performance across various
NLP tasks. However, they often generate incorrect or hallucinated information,
which hinders their practical applicability in real-world scenarios. Human
feedback has been shown to effectively enhance the factuality and quality of
generated content, addressing some of these limitations. However, this approach
is resource-intensive, involving manual input and supervision, which can be
time-consuming and expensive. Moreover, it cannot be provided during inference,
further limiting its practical utility in dynamic and interactive applications.
In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs
by providing automatic retrieval feedback in a plug-and-play framework without
the need for expensive fine-tuning. ReFeed first generates initial outputs,
then utilizes a retrieval model to acquire relevant information from large
document collections, and finally incorporates the retrieved information into
the in-context demonstration for output refinement, thereby addressing the
limitations of LLMs in a more efficient and cost-effective manner. Experiments
on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed
could improve over +6.0% under zero-shot setting and +2.5% under few-shot
setting, compared to baselines without using retrieval feedback.",None,-1
Erasure of Unaligned Attributes from Neural Representations,0.63754,"We present the Assignment-Maximization Spectral Attribute removaL (AMSAL)
algorithm, which erases information from neural representations when the
information to be erased is implicit rather than directly being aligned to each
input example. Our algorithm works by alternating between two steps. In one, it
finds an assignment of the input representations to the information to be
erased, and in the other, it creates projections of both the input
representations and the information to be erased into a joint latent space. We
test our algorithm on an extensive array of datasets, including a Twitter
dataset with multiple guarded attributes, the BiasBios dataset and the
BiasBench benchmark. The last benchmark includes four datasets with various
types of protected attributes. Our results demonstrate that bias can often be
removed in our setup. We also discuss the limitations of our approach when
there is a strong entanglement between the main task and the information to be
erased.",None,-1
When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset,0.272108,"Annotators are not fungible. Their demographics, life experiences, and
backgrounds all contribute to how they label data. However, NLP has only
recently considered how annotator identity might influence their decisions.
Here, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering,
Offensiveness, text Rewriting, and politeness rating with demographic Nuance).
POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a
representative sample regarding sex, age, and race as the US population.
Through a series of analyses, we show that annotators' background plays a
significant role in their judgments. Further, our work shows that backgrounds
not previously considered in NLP (e.g., education), are meaningful and should
be considered. Our study suggests that understanding the background of
annotators and collecting labels from a demographically balanced pool of crowd
workers is important to reduce the bias of datasets. The dataset, annotator
background, and annotation interface are available at
https://github.com/Jiaxin-Pei/potato-prolific-dataset .",None,-1
Explore the difficulty of words and its influential attributes based on the Wordle game,0.681788,"We adopt the distribution and expectation of guessing times in game Wordle as
metrics to predict the difficulty of words and explore their influence factors.
In order to predictthe difficulty distribution, we use Monte Carlo to simulate
the guessing process of players and then narrow the gap between raw and actual
distribution of guessing times for each word with Markov which generates the
associativity of words. Afterwards, we take advantage of lasso regression to
predict the deviation of guessing times expectation and quadratic programming
to obtain the correction of the original distribution.To predict the difficulty
levels, we first use hierarchical clustering to classify the difficulty levels
based on the expectation of guessing times. Afterwards we downscale the
variables of lexical attributes based on factor analysis. Significant factors
include the number of neighboring words, letter similarity, sub-string
similarity, and word frequency. Finally, we build the relationship between
lexical attributes and difficulty levels through ordered logistic regression.",None,-1
CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model,0.750882,"With the development of deep generative models, recent years have seen great
success of Chinese landscape painting generation. However, few works focus on
controllable Chinese landscape painting generation due to the lack of data and
limited modeling capabilities. In this work, we propose a controllable Chinese
landscape painting generation method named CCLAP, which can generate painting
with specific content and style based on Latent Diffusion Model. Specifically,
it consists of two cascaded modules, i.e., content generator and style
aggregator. The content generator module guarantees the content of generated
paintings specific to the input text. While the style aggregator module is to
generate paintings of a style corresponding to a reference image. Moreover, a
new dataset of Chinese landscape paintings named CLAP is collected for
comprehensive evaluation. Both the qualitative and quantitative results
demonstrate that our method achieves state-of-the-art performance, especially
in artfully-composed and artistic conception. Codes are available at
https://github.com/Robin-WZQ/CCLAP.",None,-1
Learning to Forecast Aleatoric and Epistemic Uncertainties over Long Horizon Trajectories,0.266892,"Giving autonomous agents the ability to forecast their own outcomes and
uncertainty will allow them to communicate their competencies and be used more
safely. We accomplish this by using a learned world model of the agent system
to forecast full agent trajectories over long time horizons. Real world systems
involve significant sources of both aleatoric and epistemic uncertainty that
compound and interact over time in the trajectory forecasts. We develop a deep
generative world model that quantifies aleatoric uncertainty while
incorporating the effects of epistemic uncertainty during the learning process.
We show on two reinforcement learning problems that our uncertainty model
produces calibrated outcome uncertainty estimates over the full trajectory
horizon.",None,-1
A Unified Generative Approach to Product Attribute-Value Identification,0.956841,"Product attribute-value identification (PAVI) has been studied to link
products on e-commerce sites with their attribute values (e.g., <Material,
Cotton>) using product text as clues. Technical demands from real-world
e-commerce platforms require PAVI methods to handle unseen values,
multi-attribute values, and canonicalized values, which are only partly
addressed in existing extraction- and classification-based approaches.
Motivated by this, we explore a generative approach to the PAVI task. We
finetune a pre-trained generative model, T5, to decode a set of attribute-value
pairs as a target sequence from the given product text. Since the attribute
value pairs are unordered set elements, how to linearize them will matter; we,
thus, explore methods of composing an attribute-value pair and ordering the
pairs for the task. Experimental results confirm that our generation-based
approach outperforms the existing extraction and classification-based methods
on large-scale real-world datasets meant for those methods.",None,-1
SpotEM: Efficient Video Search for Episodic Memory,0.796142,"The goal in episodic memory (EM) is to search a long egocentric video to
answer a natural language query (e.g., ""where did I leave my purse?""). Existing
EM methods exhaustively extract expensive fixed-length clip features to look
everywhere in the video for the answer, which is infeasible for long
wearable-camera videos that span hours or even days. We propose SpotEM, an
approach to achieve efficiency for a given EM method while maintaining good
accuracy. SpotEM consists of three key ideas: 1) a novel clip selector that
learns to identify promising video regions to search conditioned on the
language query; 2) a set of low-cost semantic indexing features that capture
the context of rooms, objects, and interactions that suggest where to look; and
3) distillation losses that address the optimization issues arising from
end-to-end joint training of the clip selector and EM model. Our experiments on
200+ hours of video from the Ego4D EM Natural Language Queries benchmark and
three different EM models demonstrate the effectiveness of our approach:
computing only 10% - 25% of the clip features, we preserve 84% - 97% of the
original EM model's accuracy. Project page:
https://vision.cs.utexas.edu/projects/spotem",None,-1
WYWEB: A NLP Evaluation Benchmark For Classical Chinese,0.649107,"To fully evaluate the overall performance of different NLP models in a given
domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and
CLUE. The fi eld of natural language understanding has traditionally focused on
benchmarks for various tasks in languages such as Chinese, English, and
multilingua, however, there has been a lack of attention given to the area of
classical Chinese, also known as ""wen yan wen"", which has a rich history
spanning thousands of years and holds signifi cant cultural and academic value.
For the prosperity of the NLP community, in this paper, we introduce the WYWEB
evaluation benchmark, which consists of nine NLP tasks in classical Chinese,
implementing sentence classifi cation, sequence labeling, reading
comprehension, and machine translation. We evaluate the existing pre-trained
language models, which are all struggling with this benchmark. We also
introduce a number of supplementary datasets and additional tools to help
facilitate further progress on classical Chinese NLU. The github repository is
https://github.com/baudzhou/WYWEB.",None,-1
Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation,0.588244,"Zero-shot instance segmentation aims to detect and precisely segment objects
of unseen categories without any training samples. Since the model is trained
on seen categories, there is a strong bias that the model tends to classify all
the objects into seen categories. Besides, there is a natural confusion between
background and novel objects that have never shown up in training. These two
challenges make novel objects hard to be raised in the final instance
segmentation results. It is desired to rescue novel objects from background and
dominated seen categories. To this end, we propose D$^2$Zero with
Semantic-Promoted Debiasing and Background Disambiguation to enhance the
performance of Zero-shot instance segmentation. Semantic-promoted debiasing
utilizes inter-class semantic relationships to involve unseen categories in
visual feature training and learns an input-conditional classifier to conduct
dynamical classification based on the input image. Background disambiguation
produces image-adaptive background representation to avoid mistaking novel
objects for background. Extensive experiments show that we significantly
outperform previous state-of-the-art methods by a large margin, e.g., 16.86%
improvement on COCO. Project page: https://henghuiding.github.io/D2Zero/",None,-1
MTS-Mixers: Multivariate Time Series Forecasting via Factorized Temporal and Channel Mixing,0.8702,"Multivariate time series forecasting has been widely used in various
practical scenarios. Recently, Transformer-based models have shown significant
potential in forecasting tasks due to the capture of long-range dependencies.
However, recent studies in the vision and NLP fields show that the role of
attention modules is not clear, which can be replaced by other token
aggregation operations. This paper investigates the contributions and
deficiencies of attention mechanisms on the performance of time series
forecasting. Specifically, we find that (1) attention is not necessary for
capturing temporal dependencies, (2) the entanglement and redundancy in the
capture of temporal and channel interaction affect the forecasting performance,
and (3) it is important to model the mapping between the input and the
prediction sequence. To this end, we propose MTS-Mixers, which use two
factorized modules to capture temporal and channel dependencies. Experimental
results on several real-world datasets show that MTS-Mixers outperform existing
Transformer-based models with higher efficiency.",None,-1
High-dimensional Clustering onto Hamiltonian Cycle,0.932976,"Clustering aims to group unlabelled samples based on their similarities. It
has become a significant tool for the analysis of high-dimensional data.
However, most of the clustering methods merely generate pseudo labels and thus
are unable to simultaneously present the similarities between different
clusters and outliers. This paper proposes a new framework called
High-dimensional Clustering onto Hamiltonian Cycle (HCHC) to solve the above
problems. First, HCHC combines global structure with local structure in one
objective function for deep clustering, improving the labels as relative
probabilities, to mine the similarities between different clusters while
keeping the local structure in each cluster. Then, the anchors of different
clusters are sorted on the optimal Hamiltonian cycle generated by the cluster
similarities and mapped on the circumference of a circle. Finally, a sample
with a higher probability of a cluster will be mapped closer to the
corresponding anchor. In this way, our framework allows us to appreciate three
aspects visually and simultaneously - clusters (formed by samples with high
probabilities), cluster similarities (represented as circular distances), and
outliers (recognized as dots far away from all clusters). The experiments
illustrate the superiority of HCHC.",None,-1
Constraint and Union for Partially-Supervised Temporal Sentence Grounding,0.596228,"Temporal sentence grounding aims to detect the event timestamps described by
the natural language query from given untrimmed videos. The existing
fully-supervised setting achieves great performance but requires expensive
annotation costs; while the weakly-supervised setting adopts cheap labels but
performs poorly. To pursue high performance with less annotation cost, this
paper introduces an intermediate partially-supervised setting, i.e., only
short-clip or even single-frame labels are available during training. To take
full advantage of partial labels, we propose a novel quadruple constraint
pipeline to comprehensively shape event-query aligned representations, covering
intra- and inter-samples, uni- and multi-modalities. The former raises
intra-cluster compactness and inter-cluster separability; while the latter
enables event-background separation and event-query gather. To achieve more
powerful performance with explicit grounding optimization, we further introduce
a partial-full union framework, i.e., bridging with an additional
fully-supervised branch, to enjoy its impressive grounding bonus, and be robust
to partial annotations. Extensive experiments and ablations on Charades-STA and
ActivityNet Captions demonstrate the significance of partial supervision and
our superior performance.",None,-1
Artificial Intelligence for Drug Discovery: Are We There Yet?,0.908088,"Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages.",None,-1
Elucidating STEM Concepts through Generative AI: A Multi-modal Exploration of Analogical Reasoning,0.236289,"This study explores the integration of generative artificial intelligence
(AI), specifically large language models, with multi-modal analogical reasoning
as an innovative approach to enhance science, technology, engineering, and
mathematics (STEM) education. We have developed a novel system that utilizes
the capacities of generative AI to transform intricate principles in
mathematics, physics, and programming into comprehensible metaphors. To further
augment the educational experience, these metaphors are subsequently converted
into visual form. Our study aims to enhance the learners' understanding of STEM
concepts and their learning engagement by using the visual metaphors. We
examine the efficacy of our system via a randomized A/B/C test, assessing
learning gains and motivation shifts among the learners. Our study demonstrates
the potential of applying large language models to educational practice on STEM
subjects. The results will shed light on the design of educational system in
terms of harnessing AI's potential to empower educational stakeholders.",None,-1
TopicGPT: A Prompt-based Topic Modeling Framework,0.817646,"Topic modeling is a well-established technique for exploring text corpora.
Conventional topic models (e.g., LDA) represent topics as bags of words that
often require ""reading the tea leaves"" to interpret; additionally, they offer
users minimal control over the formatting and specificity of resulting topics.
To tackle these issues, we introduce TopicGPT, a prompt-based framework that
uses large language models (LLMs) to uncover latent topics in a text
collection. TopicGPT produces topics that align better with human
categorizations compared to competing methods: it achieves a harmonic mean
purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for
the strongest baseline. Its topics are also interpretable, dispensing with
ambiguous bags of words in favor of topics with natural language labels and
associated free-form descriptions. Moreover, the framework is highly adaptable,
allowing users to specify constraints and modify topics without the need for
model retraining. By streamlining access to high-quality and interpretable
topics, TopicGPT represents a compelling, human-centered approach to topic
modeling.",None,-1
Dynamic Multi-View Fusion Mechanism For Chinese Relation Extraction,0.346696,"Recently, many studies incorporate external knowledge into character-level
feature based models to improve the performance of Chinese relation extraction.
However, these methods tend to ignore the internal information of the Chinese
character and cannot filter out the noisy information of external knowledge. To
address these issues, we propose a mixture-of-view-experts framework (MoVE) to
dynamically learn multi-view features for Chinese relation extraction. With
both the internal and external knowledge of Chinese characters, our framework
can better capture the semantic information of Chinese characters. To
demonstrate the effectiveness of the proposed framework, we conduct extensive
experiments on three real-world datasets in distinct domains. Experimental
results show consistent and significant superiority and robustness of our
proposed framework. Our code and dataset will be released at:
https://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction",None,-1
CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation,0.502305,"Dialogue structure discovery is essential in dialogue generation.
Well-structured topic flow can leverage background information and predict
future topics to help generate controllable and explainable responses. However,
most previous work focused on dialogue structure learning in task-oriented
dialogue other than open-domain dialogue which is more complicated and
challenging. In this paper, we present a new framework CTRLStruct for dialogue
structure learning to effectively explore topic-level dialogue clusters as well
as their transitions with unlabelled information. Precisely, dialogue
utterances encoded by bi-directional Transformer are further trained through a
special designed contrastive learning task to improve representation. Then we
perform clustering to utterance-level representations and form topic-level
clusters that can be considered as vertices in dialogue structure graph. The
edges in the graph indicating transition probability between vertices are
calculated by mimicking expert behavior in datasets. Finally, dialogue
structure graph is integrated into dialogue model to perform controlled
response generation. Experiments on two popular open-domain dialogue datasets
show our model can generate more coherent responses compared to some excellent
dialogue models, as well as outperform some typical sentence embedding methods
in dialogue utterance representation. Code is available in GitHub.",None,-1
Jambu: A historical linguistic database for South Asian languages,0.409659,"We introduce Jambu, a cognate database of South Asian languages which unifies
dozens of previous sources in a structured and accessible format. The database
includes 287k lemmata from 602 lects, grouped together in 23k sets of cognates.
We outline the data wrangling necessary to compile the dataset and train neural
models for reflex prediction on the Indo-Aryan subset of the data. We hope that
Jambu is an invaluable resource for all historical linguists and Indologists,
and look towards further improvement and expansion of the database.",None,-1
DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields,0.248984,"Advances in neural fields are enabling high-fidelity capture of the shape and
appearance of dynamic 3D scenes. However, their capabilities lag behind those
offered by conventional representations such as 2D videos because of
algorithmic challenges and the lack of large-scale multi-view real-world
datasets. We address the dataset limitation with DiVa-360, a real-world 360
dynamic visual dataset that contains synchronized high-resolution and
long-duration multi-view video sequences of table-scale scenes captured using a
customized low-cost system with 53 cameras. It contains 21 object-centric
sequences categorized by different motion types, 25 intricate hand-object
interaction sequences, and 8 long-duration sequences for a total of 17.4 M
image frames. In addition, we provide foreground-background segmentation masks,
synchronized audio, and text descriptions. We benchmark the state-of-the-art
dynamic neural field methods on DiVa-360 and provide insights about existing
methods and future challenges on long-duration neural field capture.",None,-1
Fairlearn: Assessing and Improving Fairness of AI Systems,0.987112,"Fairlearn is an open source project to help practitioners assess and improve
fairness of artificial intelligence (AI) systems. The associated Python
library, also named fairlearn, supports evaluation of a model's output across
affected populations and includes several algorithms for mitigating fairness
issues. Grounded in the understanding that fairness is a sociotechnical
challenge, the project integrates learning resources that aid practitioners in
considering a system's broader societal context.",None,-1
Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling,0.038327,"Dropout is a widely used regularization trick to resolve the overfitting
issue in large feedforward neural networks trained on a small dataset, which
performs poorly on the held-out test subset. Although the effectiveness of this
regularization trick has been extensively studied for convolutional neural
networks, there is a lack of analysis of it for unsupervised models and in
particular, VAE-based neural topic models. In this paper, we have analyzed the
consequences of dropout in the encoder as well as in the decoder of the VAE
architecture in three widely used neural topic models, namely, contextualized
topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly
available datasets. We characterize the dropout effect on these models in terms
of the quality and predictive performance of the generated topics.",None,-1
PMatch: Paired Masked Image Modeling for Dense Geometric Matching,0.772557,"Dense geometric matching determines the dense pixel-wise correspondence
between a source and support image corresponding to the same 3D structure.
Prior works employ an encoder of transformer blocks to correlate the two-frame
features. However, existing monocular pretraining tasks, e.g., image
classification, and masked image modeling (MIM), can not pretrain the
cross-frame module, yielding less optimal performance. To resolve this, we
reformulate the MIM from reconstructing a single masked image to reconstructing
a pair of masked images, enabling the pretraining of transformer module.
Additionally, we incorporate a decoder into pretraining for improved upsampling
results. Further, to be robust to the textureless area, we propose a novel
cross-frame global matching module (CFGM). Since the most textureless area is
planar surfaces, we propose a homography loss to further regularize its
learning. Combined together, we achieve the State-of-The-Art (SoTA) performance
on geometric matching. Codes and models are available at
https://github.com/ShngJZ/PMatch.",None,-1
Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling,0.988195,"The paper proposes a framework that combines behavioral and computational
experiments employing fictional prompts as a novel tool for investigating
cultural artifacts and social biases in storytelling both by humans and
generative AI. The study analyzes 250 stories authored by crowdworkers in June
2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging
methods from narratology and inferential statistics. Both crowdworkers and
large language models responded to identical prompts about creating and falling
in love with an artificial human. The proposed experimental paradigm allows a
direct comparison between human and LLM-generated storytelling. Responses to
the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth
in the collective imaginary of both humans and large language models. All
solicited narratives present a scientific or technological pursuit. The
analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more
more progressive in terms of gender roles and sexuality than those written by
humans. While AI narratives can occasionally provide innovative plot twists,
they offer less imaginative scenarios and rhetoric than human-authored texts.
The proposed framework argues that fiction can be used as a window into human
and AI-based collective imaginary and social dimensions.",None,-1
NerVE: Neural Volumetric Edges for Parametric Curve Extraction from Point Cloud,0.922434,"Extracting parametric edge curves from point clouds is a fundamental problem
in 3D vision and geometry processing. Existing approaches mainly rely on
keypoint detection, a challenging procedure that tends to generate noisy
output, making the subsequent edge extraction error-prone. To address this
issue, we propose to directly detect structured edges to circumvent the
limitations of the previous point-wise methods. We achieve this goal by
presenting NerVE, a novel neural volumetric edge representation that can be
easily learned through a volumetric learning framework. NerVE can be seamlessly
converted to a versatile piece-wise linear (PWL) curve representation, enabling
a unified strategy for learning all types of free-form curves. Furthermore, as
NerVE encodes rich structural information, we show that edge extraction based
on NerVE can be reduced to a simple graph search problem. After converting
NerVE to the PWL representation, parametric curves can be obtained via
off-the-shelf spline fitting algorithms. We evaluate our method on the
challenging ABC dataset. We show that a simple network based on NerVE can
already outperform the previous state-of-the-art methods by a great margin.
Project page: https://dongdu3.github.io/projects/2023/NerVE/.",None,-1
Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues,0.435928,"Discourse processing suffers from data sparsity, especially for dialogues. As
a result, we explore approaches to build discourse structures for dialogues,
based on attention matrices from Pre-trained Language Models (PLMs). We
investigate multiple tasks for fine-tuning and show that the dialogue-tailored
Sentence Ordering task performs best. To locate and exploit discourse
information in PLMs, we propose an unsupervised and a semi-supervised method.
Our proposals achieve encouraging results on the STAC corpus, with F1 scores of
57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When
restricted to projective trees, our scores improved to 63.3 and 68.1.",None,-1
Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis,0.924298,"Generative latent diffusion models have been established as state-of-the-art
in data generation. One promising application is generation of realistic
synthetic medical imaging data for open data sharing without compromising
patient privacy. Despite the promise, the capacity of such models to memorize
sensitive patient training data and synthesize samples showing high resemblance
to training data samples is relatively unexplored. Here, we assess the
memorization capacity of 3D latent diffusion models on photon-counting coronary
computed tomography angiography and knee magnetic resonance imaging datasets.
To detect potential memorization of training samples, we utilize
self-supervised models based on contrastive learning. Our results suggest that
such latent diffusion models indeed memorize training data, and there is a dire
need for devising strategies to mitigate memorization.",None,-1
MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension,0.347289,"The large language models have achieved superior performance on various
natural language tasks. One major drawback of such approaches is they are
resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a
resource-efficient solution to fine-tune the pre-trained language models (PLMs)
while keeping their weight frozen. Existing soft prompt methods mainly focus on
designing the input-independent prompts that steer the model to fit the domain
of the new dataset. Those methods often ignore the fine-grained information
about the task and context of the text. In this paper, we propose a multi-level
prompt tuning (MPrompt) method for machine reading comprehension. It utilizes
prompts at task-specific, domain-specific, and context-specific levels to
enhance the comprehension of input semantics at different granularities. We
also propose an independence constraint to steer each domain-specific prompt to
focus on information within its domain to avoid redundancy. Moreover, we
present a prompt generator that incorporates context-related knowledge in the
prompt generation to enhance contextual relevancy. We conducted extensive
experiments on 12 benchmarks of various QA formats and achieved an average
improvement of 1.94\% over the state-of-the-art methods.",None,-1
Scaling Laws for Associative Memories,0.968791,"Learning arguably involves the discovery and memorization of abstract rules.
The aim of this paper is to study associative memory mechanisms. Our model is
based on high-dimensional matrices consisting of outer products of embeddings,
which relates to the inner layers of transformer language models. We derive
precise scaling laws with respect to sample size and parameter size, and
discuss the statistical efficiency of different estimators, including
optimization-based algorithms. We provide extensive numerical experiments to
validate and interpret theoretical results, including fine-grained
visualizations of the stored memory associations.",None,-1
Topics in Contextualised Attention Embeddings,0.137263,"Contextualised word vectors obtained via pre-trained language models encode a
variety of knowledge that has already been exploited in applications.
Complementary to these language models are probabilistic topic models that
learn thematic patterns from the text. Recent work has demonstrated that
conducting clustering on the word-level contextual representations from a
language model emulates word clusters that are discovered in latent topics of
words from Latent Dirichlet Allocation. The important question is how such
topical word clusters are automatically formed, through clustering, in the
language model when it has not been explicitly designed to model latent topics.
To address this question, we design different probe experiments. Using BERT and
DistilBERT, we find that the attention framework plays a key role in modelling
such word topic clusters. We strongly believe that our work paves way for
further research into the relationships between probabilistic topic models and
pre-trained language models.",None,-1
Can Contextual Biasing Remain Effective with Whisper and GPT-2?,0.848173,"End-to-end automatic speech recognition (ASR) and large language models, such
as Whisper and GPT-2, have recently been scaled to use vast amounts of training
data. Despite the large amount of training data, infrequent content words that
occur in a particular task may still exhibit poor ASR performance, with
contextual biasing a possible remedy. This paper investigates the effectiveness
of neural contextual biasing for Whisper combined with GPT-2. Specifically,
this paper proposes integrating an adapted tree-constrained pointer generator
(TCPGen) component for Whisper and a dedicated training scheme to dynamically
adjust the final output without modifying any Whisper model parameters.
Experiments across three datasets show a considerable reduction in errors on
biasing words with a biasing list of 1000 words. Contextual biasing was more
effective when applied to domain-specific data and can boost the performance of
Whisper and GPT-2 without losing their generality.",None,-1
Unsupervised Domain Adaptation for Training Event-Based Networks Using Contrastive Learning and Uncorrelated Conditioning,0.626462,"Event-based cameras offer reliable measurements for preforming computer
vision tasks in high-dynamic range environments and during fast motion
maneuvers. However, adopting deep learning in event-based vision faces the
challenge of annotated data scarcity due to recency of event cameras.
Transferring the knowledge that can be obtained from conventional camera
annotated data offers a practical solution to this challenge. We develop an
unsupervised domain adaptation algorithm for training a deep network for
event-based data image classification using contrastive learning and
uncorrelated conditioning of data. Our solution outperforms the existing
algorithms for this purpose.",None,-1
Test-Time Adaptation with Perturbation Consistency Learning,0.0754629,"Currently, pre-trained language models (PLMs) do not cope well with the
distribution shift problem, resulting in models trained on the training set
failing in real test scenarios. To address this problem, the test-time
adaptation (TTA) shows great potential, which updates model parameters to suit
the test data at the testing time. Existing TTA methods rely on well-designed
auxiliary tasks or self-training strategies based on pseudo-label. However,
these methods do not achieve good trade-offs regarding performance gains and
computational costs. To obtain some insights into such a dilemma, we take two
representative TTA methods, i.e., Tent and OIL, for exploration and find that
stable prediction is the key to achieving a good balance. Accordingly, in this
paper, we propose perturbation consistency learning (PCL), a simple test-time
adaptation method to promote the model to make stable predictions for samples
with distribution shifts. Extensive experiments on adversarial robustness and
cross-lingual transferring demonstrate that our method can achieve higher or
comparable performance with less inference time over strong PLM backbones and
previous state-of-the-art TTA methods.",None,-1
Learning Universal Policies via Text-Guided Video Generation,0.975186,"A goal of artificial intelligence is to construct an agent that can solve a
wide variety of tasks. Recent progress in text-guided image synthesis has
yielded models with an impressive ability to generate complex novel images,
exhibiting combinatorial generalization across domains. Motivated by this
success, we investigate whether such tools can be used to construct more
general-purpose agents. Specifically, we cast the sequential decision making
problem as a text-conditioned video generation problem, where, given a
text-encoded specification of a desired goal, a planner synthesizes a set of
future frames depicting its planned actions in the future, after which control
actions are extracted from the generated video. By leveraging text as the
underlying goal specification, we are able to naturally and combinatorially
generalize to novel goals. The proposed policy-as-video formulation can further
represent environments with different state and action spaces in a unified
space of images, which, for example, enables learning and generalization across
a variety of robot manipulation tasks. Finally, by leveraging pretrained
language embeddings and widely available videos from the internet, the approach
enables knowledge transfer through predicting highly realistic video plans for
real robots.",None,-1
RLLTE: Long-Term Evolution Project of Reinforcement Learning,0.0407192,"We present RLLTE: a long-term evolution, extremely modular, and open-source
framework for reinforcement learning (RL) research and application. Beyond
delivering top-notch algorithm implementations, RLLTE also serves as a toolkit
for developing algorithms. More specifically, RLLTE decouples the RL algorithms
completely from the exploitation-exploration perspective, providing a large
number of components to accelerate algorithm development and evolution. In
particular, RLLTE is the first RL framework to build a complete and luxuriant
ecosystem, which includes model training, evaluation, deployment, benchmark
hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set
standards for RL engineering practice and be highly stimulative for industry
and academia.",None,-1
Characterization and Learning of Causal Graphs with Small Conditioning Sets,0.0437546,"Constraint-based causal discovery algorithms learn part of the causal graph
structure by systematically testing conditional independences observed in the
data. These algorithms, such as the PC algorithm and its variants, rely on
graphical characterizations of the so-called equivalence class of causal graphs
proposed by Pearl. However, constraint-based causal discovery algorithms
struggle when data is limited since conditional independence tests quickly lose
their statistical power, especially when the conditioning set is large. To
address this, we propose using conditional independence tests where the size of
the conditioning set is upper bounded by some integer $k$ for robust causal
discovery. The existing graphical characterizations of the equivalence classes
of causal graphs are not applicable when we cannot leverage all the conditional
independence statements. We first define the notion of $k$-Markov equivalence:
Two causal graphs are $k$-Markov equivalent if they entail the same conditional
independence constraints where the conditioning set size is upper bounded by
$k$. We propose a novel representation that allows us to graphically
characterize $k$-Markov equivalence between two causal graphs. We propose a
sound constraint-based algorithm called the $k$-PC algorithm for learning this
equivalence class. Finally, we conduct synthetic, and semi-synthetic
experiments to demonstrate that the $k$-PC algorithm enables more robust causal
discovery in the small sample regime compared to the baseline algorithms.",None,-1
A Study on Bias and Fairness In Deep Speaker Recognition,0.0868842,"With the ubiquity of smart devices that use speaker recognition (SR) systems
as a means of authenticating individuals and personalizing their services,
fairness of SR systems has becomes an important point of focus. In this paper
we study the notion of fairness in recent SR systems based on 3 popular and
relevant definitions, namely Statistical Parity, Equalized Odds, and Equal
Opportunity. We examine 5 popular neural architectures and 5 commonly used loss
functions in training SR systems, while evaluating their fairness against
gender and nationality groups. Our detailed experiments shed light on this
concept and demonstrate that more sophisticated encoder architectures better
align with the definitions of fairness. Additionally, we find that the choice
of loss functions can significantly impact the bias of SR models.",None,-1
Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines,0.295843,"In applied mathematics and related disciplines, the
modeling-simulation-optimization workflow is a prominent scheme, with
mathematical models and numerical algorithms playing a crucial role. For these
types of mathematical research data, the Mathematical Research Data Initiative
has developed, merged and implemented ontologies and knowledge graphs. This
contributes to making mathematical research data FAIR by introducing semantic
technology and documenting the mathematical foundations accordingly. Using the
concrete example of microfracture analysis of porous media, it is shown how the
knowledge of the underlying mathematical model and the corresponding numerical
algorithms for its solution can be represented by the ontologies.",None,-1
Cryptocurrency Price Prediction using Twitter Sentiment Analysis,0.525252,"The cryptocurrency ecosystem has been the centre of discussion on many social
media platforms, following its noted volatility and varied opinions. Twitter is
rapidly being utilised as a news source and a medium for bitcoin discussion.
Our algorithm seeks to use historical prices and sentiment of tweets to
forecast the price of Bitcoin. In this study, we develop an end-to-end model
that can forecast the sentiment of a set of tweets (using a Bidirectional
Encoder Representations from Transformers - based Neural Network Model) and
forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted
sentiment and other metrics like historical cryptocurrency price data, tweet
volume, a user's following, and whether or not a user is verified. The
sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average
of real-time data, and test data. The mean absolute percent error for the price
prediction was 3.6%.",None,-1
Adaptive Superpixel for Active Learning in Semantic Segmentation,0.226328,"Learning semantic segmentation requires pixel-wise annotations, which can be
time-consuming and expensive. To reduce the annotation cost, we propose a
superpixel-based active learning (AL) framework, which collects a dominant
label per superpixel instead. To be specific, it consists of adaptive
superpixel and sieving mechanisms, fully dedicated to AL. At each round of AL,
we adaptively merge neighboring pixels of similar learned features into
superpixels. We then query a selected subset of these superpixels using an
acquisition function assuming no uniform superpixel size. This approach is more
efficient than existing methods, which rely only on innate features such as RGB
color and assume uniform superpixel sizes. Obtaining a dominant label per
superpixel drastically reduces annotators' burden as it requires fewer clicks.
However, it inevitably introduces noisy annotations due to mismatches between
superpixel and ground truth segmentation. To address this issue, we further
devise a sieving mechanism that identifies and excludes potentially noisy
annotations from learning. Our experiments on both Cityscapes and PASCAL VOC
datasets demonstrate the efficacy of adaptive superpixel and sieving
mechanisms.",None,-1
EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models,0.347567,"We introduce EmphAssess, a prosodic benchmark designed to evaluate the
capability of speech-to-speech models to encode and reproduce prosodic
emphasis. We apply this to two tasks: speech resynthesis and speech-to-speech
translation. In both cases, the benchmark evaluates the ability of the model to
encode emphasis in the speech input and accurately reproduce it in the output,
potentially across a change of speaker and language. As part of the evaluation
pipeline, we introduce EmphaClass, a new model that classifies emphasis at the
frame or word level.",None,-1
"The Ontology for Agents, Systems and Integration of Services: OASIS version 2",0.560412,"Semantic representation is a key enabler for several application domains, and
the multi-agent systems realm makes no exception. Among the methods for
semantically representing agents, one has been essentially achieved by taking a
behaviouristic vision, through which one can describe how they operate and
engage with their peers. The approach essentially aims at defining the
operational capabilities of agents through the mental states related with the
achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and
Integration of Services, presented in 2019 -- pursues the behaviouristic
approach to deliver a semantic representation system and a communication
protocol for agents and their commitments. This paper reports on the main
modeling choices concerning the representation of agents in OASIS 2, the latest
major upgrade of OASIS, and the achievement reached by the ontology since it
was first introduced, in particular in the context of ontologies for
blockchains.",None,-1
Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification,0.451894,"Due to the expensive costs of collecting labels in multi-label classification
datasets, partially annotated multi-label classification has become an emerging
field in computer vision. One baseline approach to this task is to assume
unobserved labels as negative labels, but this assumption induces label noise
as a form of false negative. To understand the negative impact caused by false
negative labels, we study how these labels affect the model's explanation. We
observe that the explanation of two models, trained with full and partial
labels each, highlights similar regions but with different scaling, where the
latter tends to have lower attribution scores. Based on these findings, we
propose to boost the attribution scores of the model trained with partial
labels to make its explanation resemble that of the model trained with full
labels. Even with the conceptually simple approach, the multi-label
classification performance improves by a large margin in three different
datasets on a single positive label setting and one on a large-scale partial
label setting. Code is available at
https://github.com/youngwk/BridgeGapExplanationPAMC.",None,-1
Contrastive Feature Masking Open-Vocabulary Vision Transformer,0.908116,"We present Contrastive Feature Masking Vision Transformer (CFM-ViT) - an
image-text pretraining methodology that achieves simultaneous learning of
image- and region-level representation for open-vocabulary object detection
(OVD). Our approach combines the masked autoencoder (MAE) objective into the
contrastive learning objective to improve the representation for localization
tasks. Unlike standard MAE, we perform reconstruction in the joint image-text
embedding space, rather than the pixel space as is customary with the classical
MAE method, which causes the model to better learn region-level semantics.
Moreover, we introduce Positional Embedding Dropout (PED) to address scale
variation between image-text pretraining and detection finetuning by randomly
dropping out the positional embeddings during pretraining. PED improves
detection performance and enables the use of a frozen ViT backbone as a region
classifier, preventing the forgetting of open-vocabulary knowledge during
detection finetuning. On LVIS open-vocabulary detection benchmark, CFM-ViT
achieves a state-of-the-art 33.9 AP$r$, surpassing the best approach by 7.6
points and achieves better zero-shot detection transfer. Finally, CFM-ViT
acquires strong image-level representation, outperforming the state of the art
on 8 out of 12 metrics on zero-shot image-text retrieval benchmarks.",None,-1
Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation,0.0640881,"The advent of ChatGPT has introduced innovative methods for information
gathering and analysis. However, the information provided by ChatGPT is limited
to text, and the visualization of this information remains constrained.
Previous research has explored zero-shot text-to-video (TTV) approaches to
transform text into videos. However, these methods lacked control over the
identity of the generated audio, i.e., not identity-agnostic, hindering their
effectiveness. To address this limitation, we propose a novel two-stage
framework for person-agnostic video cloning, specifically focusing on TTV
generation. In the first stage, we leverage pretrained zero-shot models to
achieve text-to-speech (TTS) conversion. In the second stage, an audio-driven
talking head generation method is employed to produce compelling videos
privided the audio generated in the first stage. This paper presents a
comparative analysis of different TTS and audio-driven talking head generation
methods, identifying the most promising approach for future research and
development. Some audio and videos samples can be found in the following link:
https://github.com/ZhichaoWang970201/Text-to-Video/tree/main.",None,-1
"(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions",0.671933,"The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents.",None,-1
A Benchmark for Chinese-English Scene Text Image Super-resolution,0.250377,"Scene Text Image Super-resolution (STISR) aims to recover high-resolution
(HR) scene text images with visually pleasant and readable text content from
the given low-resolution (LR) input. Most existing works focus on recovering
English texts, which have relatively simple character structures, while little
work has been done on the more challenging Chinese texts with diverse and
complex character structures. In this paper, we propose a real-world
Chinese-English benchmark dataset, namely Real-CE, for the task of STISR with
the emphasis on restoring structurally complex Chinese characters. The
benchmark provides 1,935/783 real-world LR-HR text image pairs~(contains 33,789
text lines in total) for training/testing in 2$\times$ and 4$\times$ zooming
modes, complemented by detailed annotations, including detection boxes and text
transcripts. Moreover, we design an edge-aware learning method, which provides
structural supervision in image and feature domains, to effectively reconstruct
the dense structures of Chinese characters. We conduct experiments on the
proposed Real-CE benchmark and evaluate the existing STISR models with and
without our edge-aware loss. The benchmark, including data and source code, is
available at https://github.com/mjq11302010044/Real-CE.",None,-1
Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning,0.764459,"A general framework of unsupervised learning for combinatorial optimization
(CO) is to train a neural network (NN) whose output gives a problem solution by
directly optimizing the CO objective. Albeit with some advantages over
traditional solvers, the current framework optimizes an averaged performance
over the distribution of historical problem instances, which misaligns with the
actual goal of CO that looks for a good solution to every future encountered
instance. With this observation, we propose a new objective of unsupervised
learning for CO where the goal of learning is to search for good initialization
for future problem instances rather than give direct solutions. We propose a
meta-learning-based training pipeline for this new objective. Our method
achieves good empirical performance. We observe that even just the initial
solution given by our model before fine-tuning can significantly outperform the
baselines under various evaluation settings including evaluation across
multiple datasets, and the case with big shifts in the problem scale. The
reason we conjecture is that meta-learning-based training lets the model be
loosely tied to each local optima for a training instance while being more
adaptive to the changes of optimization landscapes across instances.",None,-1
Preference Transformer: Modeling Human Preferences using Transformers for RL,0.790777,"Preference-based reinforcement learning (RL) provides a framework to train
agents using human preferences between two behaviors. However, preference-based
RL has been challenging to scale since it requires a large amount of human
feedback to learn a reward function aligned with human intent. In this paper,
we present Preference Transformer, a neural architecture that models human
preferences using transformers. Unlike prior approaches assuming human judgment
is based on the Markovian rewards which contribute to the decision equally, we
introduce a new preference model based on the weighted sum of non-Markovian
rewards. We then design the proposed preference model using a transformer
architecture that stacks causal and bidirectional self-attention layers. We
demonstrate that Preference Transformer can solve a variety of control tasks
using real human preferences, while prior approaches fail to work. We also show
that Preference Transformer can induce a well-specified reward and attend to
critical events in the trajectory by automatically capturing the temporal
dependencies in human decision-making. Code is available on the project
website: https://sites.google.com/view/preference-transformer.",None,-1
Predicting Spine Geometry and Scoliosis from DXA Scans,0.551582,"Our objective in this paper is to estimate spine curvature in DXA scans. To
this end we first train a neural network to predict the middle spine curve in
the scan, and then use an integral-based method to determine the curvature
along the spine curve. We use the curvature to compare to the standard angle
scoliosis measure obtained using the DXA Scoliosis Method (DSM). The
performance improves over the prior work of Jamaludin et al. 2018. We show that
the maximum curvature can be used as a scoring function for ordering the
severity of spinal deformation.",None,-1
Neural Style Transfer for Vector Graphics,0.145368,"Neural style transfer draws researchers' attention, but the interest focuses
on bitmap images. Various models have been developed for bitmap image
generation both online and offline with arbitrary and pre-trained styles.
However, the style transfer between vector images has not almost been
considered. Our research shows that applying standard content and style losses
insignificantly changes the vector image drawing style because the structure of
vector primitives differs a lot from pixels. To handle this problem, we
introduce new loss functions. We also develop a new method based on
differentiable rasterization that uses these loss functions and can change the
color and shape parameters of the content image corresponding to the drawing of
the style image. Qualitative experiments demonstrate the effectiveness of the
proposed VectorNST method compared with the state-of-the-art neural style
transfer approaches for bitmap images and the only existing approach for
stylizing vector images, DiffVG. Although the proposed model does not achieve
the quality and smoothness of style transfer between bitmap images, we consider
our work an important early step in this area. VectorNST code and demo service
are available at https://github.com/IzhanVarsky/VectorNST.",None,-1
Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf,1.0,"Communication games, which we refer to as incomplete information games that
heavily depend on natural language communication, hold significant research
value in fields such as economics, social science, and artificial intelligence.
In this work, we explore the problem of how to engage large language models
(LLMs) in communication games, and in response, propose a tuning-free
framework. Our approach keeps LLMs frozen, and relies on the retrieval and
reflection on past communications and experiences for improvement. An empirical
study on the representative and widely-studied communication game,
``Werewolf'', demonstrates that our framework can effectively play Werewolf
game without tuning the parameters of the LLMs. More importantly, strategic
behaviors begin to emerge in our experiments, suggesting that it will be a
fruitful journey to engage LLMs in communication games and associated domains.",None,-1
ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis,0.0919453,"Multimodal Sentiment Analysis leverages multimodal signals to detect the
sentiment of a speaker. Previous approaches concentrate on performing
multimodal fusion and representation learning based on general knowledge
obtained from pretrained models, which neglects the effect of domain-specific
knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI)
for multimodal sentiment analysis, where specific-knowledge representations for
each modality can be learned together with general knowledge representations
via knowledge injection based on an adapter architecture. In addition, ConKI
uses a hierarchical contrastive learning procedure performed between knowledge
types within every single modality, across modalities within each sample, and
across samples to facilitate the effective learning of the proposed
representations, hence improving multimodal sentiment predictions. The
experiments on three popular multimodal sentiment analysis benchmarks show that
ConKI outperforms all prior methods on a variety of performance metrics.",None,-1
VQ3D: Learning a 3D-Aware Generative Model on ImageNet,0.370057,"Recent work has shown the possibility of training generative models of 3D
content from 2D image collections on small datasets corresponding to a single
object class, such as human faces, animal faces, or cars. However, these models
struggle on larger, more complex datasets. To model diverse and unconstrained
image collections such as ImageNet, we present VQ3D, which introduces a
NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1
allows for the reconstruction of an input image and the ability to change the
camera position around the image, and our Stage 2 allows for the generation of
new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images
from the 1000-class ImageNet dataset of 1.2 million training images. We achieve
an ImageNet generation FID score of 16.8, compared to 69.8 for the next best
baseline method.",None,-1
Efficient Generator of Mathematical Expressions for Symbolic Regression,0.123915,"We propose an approach to symbolic regression based on a novel variational
autoencoder for generating hierarchical structures, HVAE. It combines simple
atomic units with shared weights to recursively encode and decode the
individual nodes in the hierarchy. Encoding is performed bottom-up and decoding
top-down. We empirically show that HVAE can be trained efficiently with small
corpora of mathematical expressions and can accurately encode expressions into
a smooth low-dimensional latent space. The latter can be efficiently explored
with various optimization methods to address the task of symbolic regression.
Indeed, random search through the latent space of HVAE performs better than
random search through expressions generated by manually crafted probabilistic
grammars for mathematical expressions. Finally, EDHiE system for symbolic
regression, which applies an evolutionary algorithm to the latent space of
HVAE, reconstructs equations from a standard symbolic regression benchmark
better than a state-of-the-art system based on a similar combination of deep
learning and evolutionary algorithms.\v{z}",None,-1
Transformer-based World Models Are Happy With 100k Interactions,0.685862,"Deep neural networks have been successful in many reinforcement learning
settings. However, compared to human learners they are overly data hungry. To
build a sample-efficient world model, we apply a transformer to real-world
episodes in an autoregressive manner: not only the compact latent states and
the taken actions but also the experienced or predicted rewards are fed into
the transformer, so that it can attend flexibly to all three modalities at
different time steps. The transformer allows our world model to access previous
states directly, instead of viewing them through a compressed recurrent state.
By utilizing the Transformer-XL architecture, it is able to learn long-term
dependencies while staying computationally efficient. Our transformer-based
world model (TWM) generates meaningful, new experience, which is used to train
a policy that outperforms previous model-free and model-based reinforcement
learning algorithms on the Atari 100k benchmark.",None,-1
Knowledge Enhanced Model for Live Video Comment Generation,0.422859,"Live video commenting is popular on video media platforms, as it can create a
chatting atmosphere and provide supplementary information for users while
watching videos. Automatically generating live video comments can improve user
experience and enable human-like generation for bot chatting. Existing works
mostly focus on short video datasets while ignoring other important video types
such as long videos like movies. In this work, we collect a new Movie Live
Comments (MovieLC) dataset to support research on live video comment generation
for long videos. We also propose a knowledge enhanced generation model inspired
by the divergent and informative nature of live video comments. Our model
adopts a pre-training encoder-decoder framework and incorporates external
knowledge. Extensive experiments show that both objective metrics and human
evaluation demonstrate the effectiveness of our proposed model. The MovieLC
dataset and our code will be released.",None,-1
PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View,0.496942,"3D plane recovery from a single image can usually be divided into several
subtasks of plane detection, segmentation, parameter estimation and possibly
depth estimation. Previous works tend to solve this task by either extending
the RCNN-based segmentation network or the dense pixel embedding-based
clustering framework. However, none of them tried to integrate above related
subtasks into a unified framework but treat them separately and sequentially,
which we suspect is potentially a main source of performance limitation for
existing approaches. Motivated by this finding and the success of query-based
learning in enriching reasoning among semantic entities, in this paper, we
propose PlaneRecTR, a Transformer-based architecture, which for the first time
unifies all subtasks related to single-view plane recovery with a single
compact model. Extensive quantitative and qualitative experiments demonstrate
that our proposed unified learning achieves mutual benefits across subtasks,
obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane
datasets. Codes are available at https://github.com/SJingjia/PlaneRecTR.",None,-1
Pseudo-label Alignment for Semi-supervised Instance Segmentation,0.867588,"Pseudo-labeling is significant for semi-supervised instance segmentation,
which generates instance masks and classes from unannotated images for
subsequent training. However, in existing pipelines, pseudo-labels that contain
valuable information may be directly filtered out due to mismatches in class
and mask quality. To address this issue, we propose a novel framework, called
pseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we
devise a dynamic aligning loss (DALoss) that adjusts the weights of
semi-supervised loss terms with varying class and mask score pairs. Through
extensive experiments conducted on the COCO and Cityscapes datasets, we
demonstrate that PAIS is a promising framework for semi-supervised instance
segmentation, particularly in cases where labeled data is severely limited.
Notably, with just 1\% labeled data, PAIS achieves 21.2 mAP (based on
Mask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the
current state-of-the-art model, \ie, NoisyBoundary with 7.7 mAP, by a margin of
over 12 points. Code is available at: \url{https://github.com/hujiecpp/PAIS}.",None,-1
Word sense extension,0.756963,"Humans often make creative use of words to express novel senses. A
long-standing effort in natural language processing has been focusing on word
sense disambiguation (WSD), but little has been explored about how the sense
inventory of a word may be extended toward novel meanings. We present a
paradigm of word sense extension (WSE) that enables words to spawn new senses
toward novel context. We develop a framework that simulates novel word sense
extension by first partitioning a polysemous word type into two pseudo-tokens
that mark its different senses, and then inferring whether the meaning of a
pseudo-token can be extended to convey the sense denoted by the token
partitioned from the same word type. Our framework combines cognitive models of
chaining with a learning scheme that transforms a language model embedding
space to support various types of word sense extension. We evaluate our
framework against several competitive baselines and show that it is superior in
predicting plausible novel senses for over 7,500 English words. Furthermore, we
show that our WSE framework improves performance over a range of
transformer-based WSD models in predicting rare word senses with few or zero
mentions in the training data.",None,-1
A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement,0.148272,"Generating proper embedding of sentences through an unsupervised way is
beneficial to semantic matching and retrieval problems in real-world scenarios.
This paper presents Representation ALchemy (RepAL), an extremely simple
post-processing method that enhances sentence representations. The basic idea
in RepAL is to de-emphasize redundant information of sentence embedding
generated by pre-trained models. Through comprehensive experiments, we show
that RepAL is free of training and is a plug-and-play method that can be
combined with most existing unsupervised sentence learning models. We also
conducted in-depth analysis to understand RepAL.",None,-1
Thistle: A Vector Database in Rust,0.222669,"We present Thistle, a fully functional vector database. Thistle is an entry
into the domain of latent knowledge use in answering search queries, an ongoing
research topic at both start-ups and search engine companies. We implement
Thistle with several well-known algorithms, and benchmark results on the MS
MARCO dataset. Results help clarify the latent knowledge domain as well as the
growing Rust ML ecosystem.",None,-1
MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos,0.604357,"User-generated content (UGC) live videos are often bothered by various
distortions during capture procedures and thus exhibit diverse visual
qualities. Such source videos are further compressed and transcoded by media
server providers before being distributed to end-users. Because of the
flourishing of UGC live videos, effective video quality assessment (VQA) tools
are needed to monitor and perceptually optimize live streaming videos in the
distributing process. In this paper, we address \textbf{UGC Live VQA} problems
by constructing a first-of-a-kind subjective UGC Live VQA database and
developing an effective evaluation tool. Concretely, 418 source UGC videos are
collected in real live streaming scenarios and 3,762 compressed ones at
different bit rates are generated for the subsequent subjective VQA
experiments. Based on the built database, we develop a
\underline{M}ulti-\underline{D}imensional \underline{VQA} (\textbf{MD-VQA})
evaluator to measure the visual quality of UGC live videos from semantic,
distortion, and motion aspects respectively. Extensive experimental results
show that MD-VQA achieves state-of-the-art performance on both our UGC Live VQA
database and existing compressed UGC VQA databases.",None,-1
Single Image LDR to HDR Conversion using Conditional Diffusion,0.109199,"Digital imaging aims to replicate realistic scenes, but Low Dynamic Range
(LDR) cameras cannot represent the wide dynamic range of real scenes, resulting
in under-/overexposed images. This paper presents a deep learning-based
approach for recovering intricate details from shadows and highlights while
reconstructing High Dynamic Range (HDR) images. We formulate the problem as an
image-to-image (I2I) translation task and propose a conditional Denoising
Diffusion Probabilistic Model (DDPM) based framework using classifier-free
guidance. We incorporate a deep CNN-based autoencoder in our proposed framework
to enhance the quality of the latent representation of the input LDR image used
for conditioning. Moreover, we introduce a new loss function for LDR-HDR
translation tasks, termed Exposure Loss. This loss helps direct gradients in
the opposite direction of the saturation, further improving the results'
quality. By conducting comprehensive quantitative and qualitative experiments,
we have effectively demonstrated the proficiency of our proposed method. The
results indicate that a simple conditional diffusion-based method can replace
the complex camera pipeline-based architectures.",None,-1
Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,0.793979,"Large language models (LLMs) have advanced in large strides due to the
effectiveness of the self-attention mechanism that processes and compares all
tokens at once. However, this mechanism comes with a fundamental issue -- the
predetermined context window is bound to be limited. Despite attempts to extend
the context window through methods like extrapolating the positional embedding,
using recurrence, or selectively retrieving essential parts of the long
sequence, long-text understanding continues to be a challenge. We propose an
alternative approach which instead treats the LLM as an interactive agent,
allowing it to decide how to read the text via iterative prompting. We
introduce MemWalker, a method that first processes the long context into a tree
of summary nodes. Upon receiving a query, the model navigates this tree in
search of relevant information, and responds once it gathers sufficient
information. On long-text question answering tasks our method outperforms
baseline approaches that use long context windows, recurrence, and retrieval.
We show that, beyond effective reading, MemWalker enhances explainability by
highlighting the reasoning steps as it interactively reads the text;
pinpointing the relevant text segments related to the query.",None,-1
Pre-Trained Large Language Models for Industrial Control,0.381217,"For industrial control, developing high-performance controllers with few
samples and low technical debt is appealing. Foundation models, possessing rich
prior knowledge obtained from pre-training with Internet-scale corpus, have the
potential to be a good controller with proper prompts. In this paper, we take
HVAC (Heating, Ventilation, and Air Conditioning) building control as an
example to examine the ability of GPT-4 (one of the first-tier foundation
models) as the controller. To control HVAC, we wrap the task as a language game
by providing text including a short description for the task, several selected
demonstrations, and the current observation to GPT-4 on each step and execute
the actions responded by GPT-4. We conduct series of experiments to answer the
following questions: 1)~How well can GPT-4 control HVAC? 2)~How well can GPT-4
generalize to different scenarios for HVAC control? 3) How different parts of
the text context affect the performance? In general, we found GPT-4 achieves
the performance comparable to RL methods with few samples and low technical
debt, indicating the potential of directly applying foundation models to
industrial control tasks.",None,-1
Supervised Deep Learning for Content-Aware Image Retargeting with Fourier Convolutions,0.162898,"Image retargeting aims to alter the size of the image with attention to the
contents. One of the main obstacles to training deep learning models for image
retargeting is the need for a vast labeled dataset. Labeled datasets are
unavailable for training deep learning models in the image retargeting tasks.
As a result, we present a new supervised approach for training deep learning
models. We use the original images as ground truth and create inputs for the
model by resizing and cropping the original images. A second challenge is
generating different image sizes in inference time. However, regular
convolutional neural networks cannot generate images of different sizes than
the input image. To address this issue, we introduced a new method for
supervised learning. In our approach, a mask is generated to show the desired
size and location of the object. Then the mask and the input image are fed to
the network. Comparing image retargeting methods and our proposed method
demonstrates the model's ability to produce high-quality retargeted images.
Afterward, we compute the image quality assessment score for each output image
based on different techniques and illustrate the effectiveness of our approach.",None,-1
From Visual Prompt Learning to Zero-Shot Transfer: Mapping Is All You Need,0.0850769,"Visual prompt learning, as a newly emerged technique, leverages the knowledge
learned by a large-scale pre-trained model and adapts it to downstream tasks
through the usage of prompts. While previous research has focused on designing
effective prompts, in this work, we argue that compared to prompt design, a
good mapping strategy matters more. In this sense, we propose SeMap, a more
effective mapping using the semantic alignment between the pre-trained model's
knowledge and the downstream task. Our experimental results show that SeMap can
largely boost the performance of visual prompt learning. Moreover, our
experiments show that SeMap is capable of achieving competitive zero-shot
transfer, indicating that it can perform the downstream task without any
fine-tuning on the corresponding dataset. This demonstrates the potential of
our proposed method to be used in a broader range of applications where the
zero-shot transfer is desired. Results suggest that our proposed SeMap could
lead to significant advancements in both visual prompt learning and zero-shot
transfer. We hope with SeMap, we can help the community move forward to more
efficient and lightweight utilization of large vision models.",None,-1
R2H: Building Multimodal Navigation Helpers that Respond to Help Requests,0.392616,"Intelligent navigation-helper agents are critical as they can navigate users
in unknown areas through environmental awareness and conversational ability,
serving as potential accessibility tools for individuals with disabilities. In
this work, we first introduce a novel benchmark, Respond to Help Requests
(R2H), to promote the development of multi-modal navigation helpers capable of
responding to requests for help, utilizing existing dialog-based embodied
datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH),
which assesses the helper agent's ability to generate informative responses
based on a given dialog history, and (2) Respond during Interaction (RdI),
which evaluates the effectiveness and efficiency of the response during
consistent cooperation with a task performer. Furthermore, we explore two
approaches to construct the navigation-helper agent, including fine-tuning a
novel task-oriented multi-modal response generation model that can see and
respond, named SeeRee, and employing a multi-modal large language model in a
zero-shot manner. Analysis of the task and method was conducted based on both
automatic benchmarking and human evaluations. Project website:
https://sites.google.com/view/response2helprequests/home.",None,-1
Learned Thresholds Token Merging and Pruning for Vision Transformers,0.229751,"Vision transformers have demonstrated remarkable success in a wide range of
computer vision tasks over the last years. However, their high computational
costs remain a significant barrier to their practical deployment. In
particular, the complexity of transformer models is quadratic with respect to
the number of input tokens. Therefore techniques that reduce the number of
input tokens that need to be processed have been proposed. This paper
introduces Learned Thresholds token Merging and Pruning (LTMP), a novel
approach that leverages the strengths of both token merging and token pruning.
LTMP uses learned threshold masking modules that dynamically determine which
tokens to merge and which to prune. We demonstrate our approach with extensive
experiments on vision transformers on the ImageNet classification task. Our
results demonstrate that LTMP achieves state-of-the-art accuracy across
reduction rates while requiring only a single fine-tuning epoch, which is an
order of magnitude faster than previous methods. Code is available at
https://github.com/Mxbonn/ltmp .",None,-1
No Offense Taken: Eliciting Offensiveness from Language Models,0.0700523,"This work was completed in May 2022.
  For safe and reliable deployment of language models in the real world,
testing needs to be robust. This robustness can be characterized by the
difficulty and diversity of the test cases we evaluate these models on.
Limitations in human-in-the-loop test case generation has prompted an advent of
automated test case generation approaches. In particular, we focus on Red
Teaming Language Models with Language Models by Perez et al.(2022). Our
contributions include developing a pipeline for automated test case generation
via red teaming that leverages publicly available smaller language models
(LMs), experimenting with different target LMs and red classifiers, and
generating a corpus of test cases that can help in eliciting offensive
responses from widely deployed LMs and identifying their failure modes.",None,-1
Randomized Adversarial Style Perturbations for Domain Generalization,0.0796691,"We propose a novel domain generalization technique, referred to as Randomized
Adversarial Style Perturbation (RASP), which is motivated by the observation
that the characteristics of each domain are captured by the feature statistics
corresponding to style. The proposed algorithm perturbs the style of a feature
in an adversarial direction towards a randomly selected class, and makes the
model learn against being misled by the unexpected styles observed in unseen
target domains. While RASP is effective to handle domain shifts, its naive
integration into the training procedure might degrade the capability of
learning knowledge from source domains because it has no restriction on the
perturbations of representations. This challenge is alleviated by Normalized
Feature Mixup (NFM), which facilitates the learning of the original features
while achieving robustness to perturbed representations via their mixup during
training. We evaluate the proposed algorithm via extensive experiments on
various benchmarks and show that our approach improves domain generalization
performance, especially in large-scale benchmarks.",None,-1
EPINN-NSE: Enhanced Physics-Informed Neural Networks for Solving Navier-Stokes Equations,0.323393,"Fluid mechanics is a fundamental field in engineering and science. Solving
the Navier-Stokes equation (NSE) is critical for understanding the behavior of
fluids. However, the NSE is a complex partial differential equation that is
difficult to solve, and classical numerical methods can be computationally
expensive. In this paper, we present an innovative approach for solving the NSE
using Physics Informed Neural Networks (PINN) and several novel techniques that
improve their performance. The first model is based on an assumption that
involves approximating the velocity component by employing the derivative of a
stream function. This assumption serves to simplify the system and guarantees
that the velocity adheres to the divergence-free equation. We also developed a
second more flexible model that approximates the solution without any
assumptions. The proposed models can effectively solve two-dimensional NSE.
Moreover, we successfully applied the second model to solve the
three-dimensional NSE. The results show that the models can efficiently and
accurately solve the NSE in three dimensions. These approaches offer several
advantages, including high trainability, flexibility, and efficiency.",None,-1
Attributing Image Generative Models using Latent Fingerprints,0.276375,"Generative models have enabled the creation of contents that are
indistinguishable from those taken from nature. Open-source development of such
models raised concerns about the risks of their misuse for malicious purposes.
One potential risk mitigation strategy is to attribute generative models via
fingerprinting. Current fingerprinting methods exhibit a significant tradeoff
between robust attribution accuracy and generation quality while lacking design
principles to improve this tradeoff. This paper investigates the use of latent
semantic dimensions as fingerprints, from where we can analyze the effects of
design variables, including the choice of fingerprinting dimensions, strength,
and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA,
our method requires minimum computation and is more applicable to large-scale
models. We use StyleGAN2 and the latent diffusion model to demonstrate the
efficacy of our method.",None,-1
World Models for Math Story Problems,0.310181,"Solving math story problems is a complex task for students and NLP models
alike, requiring them to understand the world as described in the story and
reason over it to compute an answer. Recent years have seen impressive
performance on automatically solving these problems with large pre-trained
language models and innovative techniques to prompt them. However, it remains
unclear if these models possess accurate representations of mathematical
concepts. This leads to lack of interpretability and trustworthiness which
impedes their usefulness in various applications. In this paper, we consolidate
previous work on categorizing and representing math story problems and develop
MathWorld, which is a graph-based semantic formalism specific for the domain of
math story problems. With MathWorld, we can assign world models to math story
problems which represent the situations and actions introduced in the text and
their mathematical relationships. We combine math story problems from several
existing datasets and annotate a corpus of 1,019 problems and 3,204 logical
forms with MathWorld. Using this data, we demonstrate the following use cases
of MathWorld: (1) prompting language models with synthetically generated
question-answer pairs to probe their reasoning and world modeling abilities,
and (2) generating new problems by using the world models as a design space.",None,-1
Automatic summarisation of Instagram social network posts Combining semantic and statistical approaches,0.0849517,"The proliferation of data and text documents such as articles, web pages,
books, social network posts, etc. on the Internet has created a fundamental
challenge in various fields of text processing under the title of ""automatic
text summarisation"". Manual processing and summarisation of large volumes of
textual data is a very difficult, expensive, time-consuming and impossible
process for human users. Text summarisation systems are divided into extractive
and abstract categories. In the extractive summarisation method, the final
summary of a text document is extracted from the important sentences of the
same document without any modification. In this method, it is possible to
repeat a series of sentences and to interfere with pronouns. However, in the
abstract summarisation method, the final summary of a textual document is
extracted from the meaning and significance of the sentences and words of the
same document or other documents. Many of the works carried out have used
extraction methods or abstracts to summarise the collection of web documents,
each of which has advantages and disadvantages in the results obtained in terms
of similarity or size. In this work, a crawler has been developed to extract
popular text posts from the Instagram social network with appropriate
preprocessing, and a set of extraction and abstraction algorithms have been
combined to show how each of the abstraction algorithms can be used.
Observations made on 820 popular text posts on the social network Instagram
show the accuracy (80%) of the proposed system.",None,-1
RGB-T Multi-Modal Crowd Counting Based on Transformer,0.568092,"Crowd counting aims to estimate the number of persons in a scene. Most
state-of-the-art crowd counting methods based on color images can't work well
in poor illumination conditions due to invisible objects. With the widespread
use of infrared cameras, crowd counting based on color and thermal images is
studied. Existing methods only achieve multi-modal fusion without count
objective constraint. To better excavate multi-modal information, we use
count-guided multi-modal fusion and modal-guided count enhancement to achieve
the impressive performance. The proposed count-guided multi-modal fusion module
utilizes a multi-scale token transformer to interact two-modal information
under the guidance of count information and perceive different scales from the
token perspective. The proposed modal-guided count enhancement module employs
multi-scale deformable transformer decoder structure to enhance one modality
feature and count information by the other modality. Experiment in public
RGBT-CC dataset shows that our method refreshes the state-of-the-art results.
https://github.com/liuzywen/RGBTCC",None,-1
Towards Detecting Harmful Agendas in News Articles,0.0634929,"Manipulated news online is a growing problem which necessitates the use of
automated systems to curtail its spread. We argue that while misinformation and
disinformation detection have been studied, there has been a lack of investment
in the important open challenge of detecting harmful agendas in news articles;
identifying harmful agendas is critical to flag news campaigns with the
greatest potential for real world harm. Moreover, due to real concerns around
censorship, harmful agenda detectors must be interpretable to be effective. In
this work, we propose this new task and release a dataset, NewsAgendas, of
annotated news articles for agenda identification. We show how interpretable
systems can be effective on this task and demonstrate that they can perform
comparably to black-box models.",None,-1
"Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models",0.840644,"While adversarial training has been extensively studied for ResNet
architectures and low resolution datasets like CIFAR, much less is known for
ImageNet. Given the recent debate about whether transformers are more robust
than convnets, we revisit adversarial training on ImageNet comparing ViTs and
ConvNeXts. Extensive experiments show that minor changes in architecture, most
notably replacing PatchStem with ConvStem, and training scheme have a
significant impact on the achieved robustness. These changes not only increase
robustness in the seen $\ell_\infty$-threat model, but even more so improve
generalization to unseen $\ell_1/\ell_2$-attacks. Our modified ConvNeXt,
ConvNeXt + ConvStem, yields the most robust $\ell_\infty$-models across
different ranges of model parameters and FLOPs, while our ViT + ConvStem yields
the best generalization to unseen threat models.",None,-1
Decentralized Monte Carlo Tree Search for Partially Observable Multi-agent Pathfinding,0.811397,"The Multi-Agent Pathfinding (MAPF) problem involves finding a set of
conflict-free paths for a group of agents confined to a graph. In typical MAPF
scenarios, the graph and the agents' starting and ending vertices are known
beforehand, allowing the use of centralized planning algorithms. However, in
this study, we focus on the decentralized MAPF setting, where the agents may
observe the other agents only locally and are restricted in communications with
each other. Specifically, we investigate the lifelong variant of MAPF, where
new goals are continually assigned to the agents upon completion of previous
ones. Drawing inspiration from the successful AlphaZero approach, we propose a
decentralized multi-agent Monte Carlo Tree Search (MCTS) method for MAPF tasks.
Our approach utilizes the agent's observations to recreate the intrinsic Markov
decision process, which is then used for planning with a tailored for
multi-agent tasks version of neural MCTS. The experimental results show that
our approach outperforms state-of-the-art learnable MAPF solvers. The source
code is available at https://github.com/AIRI-Institute/mats-lp.",None,-1
Is There Any Social Principle for LLM-Based Agents?,0.0272146,"Focus on Large Language Model based agents should involve more than
""human-centered"" alignment or application. We argue that more attention should
be paid to the agent itself and discuss the potential of establishing tailored
social sciences for agents.",None,-1
Dynamic fairness-aware recommendation through multi-agent social choice,0.276668,"Algorithmic fairness in the context of personalized recommendation presents
significantly different challenges to those commonly encountered in
classification tasks. Researchers studying classification have generally
considered fairness to be a matter of achieving equality of outcomes between a
protected and unprotected group, and built algorithmic interventions on this
basis. We argue that fairness in real-world application settings in general,
and especially in the context of personalized recommendation, is much more
complex and multi-faceted, requiring a more general approach. We propose a
model to formalize multistakeholder fairness in recommender systems as a two
stage social choice problem. In particular, we express recommendation fairness
as a novel combination of an allocation and an aggregation problem, which
integrate both fairness concerns and personalized recommendation provisions,
and derive new recommendation techniques based on this formulation. Simulations
demonstrate the ability of the framework to integrate multiple fairness
concerns in a dynamic way.",None,-1
Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages,0.534582,"Connectionist Temporal Classification (CTC) models are popular for their
balance between speed and performance for Automatic Speech Recognition (ASR).
However, these CTC models still struggle in other areas, such as
personalization towards custom words. A recent approach explores Contextual
Adapters, wherein an attention-based biasing model for CTC is used to improve
the recognition of custom entities. While this approach works well with enough
data, we showcase that it isn't an effective strategy for low-resource
languages. In this work, we propose a supervision loss for smoother training of
the Contextual Adapters. Further, we explore a multilingual strategy to improve
performance with limited training data. Our method achieves 48% F1 improvement
in retrieving unseen custom entities for a low-resource language.
Interestingly, as a by-product of training the Contextual Adapters, we see a
5-11% Word Error Rate (WER) reduction in the performance of the base CTC model
as well.",None,-1
How To Build Competitive Multi-gender Speech Translation Models For Controlling Speaker Gender Translation,0.225314,"When translating from notional gender languages (e.g., English) into
grammatical gender languages (e.g., Italian), the generated translation
requires explicit gender assignments for various words, including those
referring to the speaker. When the source sentence does not convey the
speaker's gender, speech translation (ST) models either rely on the
possibly-misleading vocal traits of the speaker or default to the masculine
gender, the most frequent in existing training corpora. To avoid such biased
and not inclusive behaviors, the gender assignment of speaker-related
expressions should be guided by externally-provided metadata about the
speaker's gender. While previous work has shown that the most effective
solution is represented by separate, dedicated gender-specific models, the goal
of this paper is to achieve the same results by integrating the speaker's
gender metadata into a single ""multi-gender"" neural ST model, easier to
maintain. Our experiments demonstrate that a single multi-gender model
outperforms gender-specialized ones when trained from scratch (with gender
accuracy gains up to 12.9 for feminine forms), while fine-tuning from existing
ST models does not lead to competitive results.",None,-1
Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents,0.795438,"Robustly cooperating with unseen agents and human partners presents
significant challenges due to the diverse cooperative conventions these
partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this
challenge by training an agent with a population of diverse teammate policies
obtained through maximizing specific diversity metrics. However, prior
heuristic-based diversity metrics do not always maximize the agent's robustness
in all cooperative problems. In this work, we first propose that maximizing an
AHT agent's robustness requires it to emulate policies in the minimum coverage
set (MCS), the set of best-response policies to any partner policies in the
environment. We then introduce the L-BRDiv algorithm that generates a set of
teammate policies that, when used for AHT training, encourage agents to emulate
policies from the MCS. L-BRDiv works by solving a constrained optimization
problem to jointly train teammate policies for AHT training and approximating
AHT agent policies that are members of the MCS. We empirically demonstrate that
L-BRDiv produces more robust AHT agents than state-of-the-art methods in a
broader range of two-player cooperative problems without the need for extensive
hyperparameter tuning for its objectives. Our study shows that L-BRDiv
outperforms the baseline methods by prioritizing discovering distinct members
of the MCS instead of repeatedly finding redundant policies.",None,-1
GLEN: General-Purpose Event Detection for Thousands of Types,0.772151,"The progress of event extraction research has been hindered by the absence of
wide-coverage, large-scale datasets. To make event extraction systems more
accessible, we build a general-purpose event detection dataset GLEN, which
covers 205K event mentions with 3,465 different types, making it more than 20x
larger in ontology than today's largest event dataset. GLEN is created by
utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and
PropBank rolesets. This enables us to use the abundant existing annotation for
PropBank as distant supervision. In addition, we also propose a new multi-stage
event detection model CEDAR specifically designed to handle the large ontology
size in GLEN. We show that our model exhibits superior performance compared to
a range of baselines including InstructGPT. Finally, we perform error analysis
and show that label noise is still the largest challenge for improving
performance for this new dataset. Our dataset, code, and models are released at
\url{https://github.com/ZQS1943/GLEN}.}",None,-1
Crosslingual Retrieval Augmented In-context Learning for Bangla,0.625468,"The promise of Large Language Models (LLMs) in Natural Language Processing
has often been overshadowed by their limited performance in low-resource
languages such as Bangla. To address this, our paper presents a pioneering
approach that utilizes cross-lingual retrieval augmented in-context learning.
By strategically sourcing semantically similar prompts from high-resource
language, we enable multilingual pretrained language models (MPLMs), especially
the generative model BLOOMZ, to successfully boost performance on Bangla tasks.
Our extensive evaluation highlights that the cross-lingual retrieval augmented
prompts bring steady improvements to MPLMs over the zero-shot performance.",None,-1
Future Lens: Anticipating Subsequent Tokens from a Single Hidden State,0.901025,"We conjecture that hidden state vectors corresponding to individual input
tokens encode information sufficient to accurately predict several tokens
ahead. More concretely, in this paper we ask: Given a hidden (internal)
representation of a single token at position $t$ in an input, can we reliably
anticipate the tokens that will appear at positions $\geq t + 2$? To test this,
we measure linear approximation and causal intervention methods in GPT-J-6B to
evaluate the degree to which individual hidden states in the network contain
signal rich enough to predict future hidden states and, ultimately, token
outputs. We find that, at some layers, we can approximate a model's output with
more than 48% accuracy with respect to its prediction of subsequent tokens
through a single hidden state. Finally we present a ""Future Lens"" visualization
that uses these methods to create a new view of transformer states.",None,-1
Lost in Translation: Large Language Models in Non-English Content Analysis,0.436908,"In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,
Google's PaLM) have become the dominant approach for building AI systems to
analyze and generate language online. However, the automated systems that
increasingly mediate our interactions online -- such as chatbots, content
moderation systems, and search engines -- are primarily designed for and work
far more effectively in English than in the world's other 7,000 languages.
Recently, researchers and technology companies have attempted to extend the
capabilities of large language models into languages other than English by
building what are called multilingual language models.
  In this paper, we explain how these multilingual language models work and
explore their capabilities and limits. Part I provides a simple technical
explanation of how large language models work, why there is a gap in available
data between English and other languages, and how multilingual language models
attempt to bridge that gap. Part II accounts for the challenges of doing
content analysis with large language models in general and multilingual
language models in particular. Part III offers recommendations for companies,
researchers, and policymakers to keep in mind when considering researching,
developing and deploying large and multilingual language models.",None,-1
International Governance of Civilian AI: A Jurisdictional Certification Approach,0.939734,"This report describes trade-offs in the design of international governance
arrangements for civilian artificial intelligence (AI) and presents one
approach in detail. This approach represents the extension of a standards,
licensing, and liability regime to the global level. We propose that states
establish an International AI Organization (IAIO) to certify state
jurisdictions (not firms or AI projects) for compliance with international
oversight standards. States can give force to these international standards by
adopting regulations prohibiting the import of goods whose supply chains embody
AI from non-IAIO-certified jurisdictions. This borrows attributes from models
of existing international organizations, such as the International Civilian
Aviation Organization (ICAO), the International Maritime Organization (IMO),
and the Financial Action Task Force (FATF). States can also adopt multilateral
controls on the export of AI product inputs, such as specialized hardware, to
non-certified jurisdictions. Indeed, both the import and export standards could
be required for certification. As international actors reach consensus on risks
of and minimum standards for advanced AI, a jurisdictional certification regime
could mitigate a broad range of potential harms, including threats to public
safety.",None,-1
PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature,0.373712,"Scientific information extraction (SciIE), which aims to automatically
extract information from scientific literature, is becoming more important than
ever. However, there are no existing SciIE datasets for polymer materials,
which is an important class of materials used ubiquitously in our daily lives.
To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer
materials. POLYIE is curated from 146 full-length polymer scholarly articles,
which are annotated with different named entities (i.e., materials, properties,
values, conditions) as well as their N-ary relations by domain experts. POLYIE
presents several unique challenges due to diverse lexical formats of entities,
ambiguity between entities, and variable-length relations. We evaluate
state-of-the-art named entity extraction and relation extraction models on
POLYIE, analyze their strengths and weaknesses, and highlight some difficult
cases for these models. To the best of our knowledge, POLYIE is the first SciIE
benchmark for polymer materials, and we hope it will lead to more research
efforts from the community on this challenging task. Our code and data are
available on: https://github.com/jerry3027/PolyIE.",None,-1
Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements,0.704882,"Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.",None,-1
A Data Fusion Framework for Multi-Domain Morality Learning,0.774198,"Language models can be trained to recognize the moral sentiment of text,
creating new opportunities to study the role of morality in human life. As
interest in language and morality has grown, several ground truth datasets with
moral annotations have been released. However, these datasets vary in the
method of data collection, domain, topics, instructions for annotators, etc.
Simply aggregating such heterogeneous datasets during training can yield models
that fail to generalize well. We describe a data fusion framework for training
on multiple heterogeneous datasets that improve performance and
generalizability. The model uses domain adversarial training to align the
datasets in feature space and a weighted loss function to deal with label
shift. We show that the proposed framework achieves state-of-the-art
performance in different datasets compared to prior works in morality
inference.",None,-1
Single-view Neural Radiance Fields with Depth Teacher,0.0697325,"Neural Radiance Fields (NeRF) have been proposed for photorealistic novel
view rendering. However, it requires many different views of one scene for
training. Moreover, it has poor generalizations to new scenes and requires
retraining or fine-tuning on each scene. In this paper, we develop a new NeRF
model for novel view synthesis using only a single image as input. We propose
to combine the (coarse) planar rendering and the (fine) volume rendering to
achieve higher rendering quality and better generalizations. We also design a
depth teacher net that predicts dense pseudo depth maps to supervise the joint
rendering mechanism and boost the learning of consistent 3D geometry. We
evaluate our method on three challenging datasets. It outperforms
state-of-the-art single-view NeRFs by achieving 5$\sim$20\% improvements in
PSNR and reducing 20$\sim$50\% of the errors in the depth rendering. It also
shows excellent generalization abilities to unseen data without the need to
fine-tune on each new scene.",None,-1
Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models,0.73487,"This work introduces approaches to assessing phrase breaks in ESL learners'
speech using pre-trained language models (PLMs) and large language models
(LLMs). There are two tasks: overall assessment of phrase break for a speech
clip and fine-grained assessment of every possible phrase break position. To
leverage NLP models, speech input is first force-aligned with texts, and then
pre-processed into a token sequence, including words and phrase break
information. To utilize PLMs, we propose a pre-training and fine-tuning
pipeline with the processed tokens. This process includes pre-training with a
replaced break token detection module and fine-tuning with text classification
and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The
experiments show that with the PLMs, the dependence on labeled training data
has been greatly reduced, and the performance has improved. Meanwhile, we
verify that ChatGPT, a renowned LLM, has potential for further advancement in
this area.",None,-1
Graph-based Asynchronous Event Processing for Rapid Object Recognition,0.976846,"Different from traditional video cameras, event cameras capture asynchronous
events stream in which each event encodes pixel location, trigger time, and the
polarity of the brightness changes. In this paper, we introduce a novel
graph-based framework for event cameras, namely SlideGCN. Unlike some recent
graph-based methods that use groups of events as input, our approach can
efficiently process data event-by-event, unlock the low latency nature of
events data while still maintaining the graph's structure internally. For fast
graph construction, we develop a radius search algorithm, which better exploits
the partial regular structure of event cloud against k-d tree based generic
methods. Experiments show that our method reduces the computational complexity
up to 100 times with respect to current graph-based methods while keeping
state-of-the-art performance on object recognition. Moreover, we verify the
superiority of event-wise processing with our method. When the state becomes
stable, we can give a prediction with high confidence, thus making an early
recognition. Project page: \url{https://zju3dv.github.io/slide_gcn/}.",None,-1
Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation,0.833826,"The left ventricular of ejection fraction is one of the most important metric
of cardiac function. It is used by cardiologist to identify patients who are
eligible for lifeprolonging therapies. However, the assessment of ejection
fraction suffers from inter-observer variability. To overcome this challenge,
we propose a deep learning approach, based on hierarchical vision Transformers,
to estimate the ejection fraction from echocardiogram videos. The proposed
method can estimate ejection fraction without the need for left ventrice
segmentation first, make it more efficient than other methods. We evaluated our
method on EchoNet-Dynamic dataset resulting 5.59, 7.59 and 0.59 for MAE, RMSE
and R2 respectivelly. This results are better compared to the state-of-the-art
method, Ultrasound Video Transformer (UVT). The source code is available on
https://github.com/lhfazry/UltraSwin.",None,-1
Zero-shot Referring Image Segmentation with Global-Local Context Features,0.695557,"Referring image segmentation (RIS) aims to find a segmentation mask given a
referring expression grounded to a region of the input image. Collecting
labelled datasets for this task, however, is notoriously costly and
labor-intensive. To overcome this issue, we propose a simple yet effective
zero-shot referring image segmentation method by leveraging the pre-trained
cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded
to the input text, we propose a mask-guided visual encoder that captures global
and local contextual information of an input image. By utilizing instance masks
obtained from off-the-shelf mask proposal techniques, our method is able to
segment fine-detailed Istance-level groundings. We also introduce a
global-local text encoder where the global feature captures complex
sentence-level semantics of the entire input expression while the local feature
focuses on the target noun phrase extracted by a dependency parser. In our
experiments, the proposed method outperforms several zero-shot baselines of the
task and even the weakly supervised referring expression segmentation method
with substantial margins. Our code is available at
https://github.com/Seonghoon-Yu/Zero-shot-RIS.",None,-1
Drafting Event Schemas using Language Models,0.819449,"Past work has studied event prediction and event language modeling, sometimes
mediated through structured representations of knowledge in the form of event
schemas. Such schemas can lead to explainable predictions and forecasting of
unseen events given incomplete information. In this work, we look at the
process of creating such schemas to describe complex events. We use large
language models (LLMs) to draft schemas directly in natural language, which can
be further refined by human curators as necessary. Our focus is on whether we
can achieve sufficient diversity and recall of key events and whether we can
produce the schemas in a sufficiently descriptive style. We show that large
language models are able to achieve moderate recall against schemas taken from
two different datasets, with even better results when multiple prompts and
multiple samples are combined. Moreover, we show that textual entailment
methods can be used for both matching schemas to instances of events as well as
evaluating overlap between gold and predicted schemas. Our method paves the way
for easier distillation of event knowledge from large language model into
schemas.",None,-1
DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers,0.444631,"We propose a novel talking head synthesis pipeline called ""DiT-Head"", which
is based on diffusion transformers and uses audio as a condition to drive the
denoising process of a diffusion model. Our method is scalable and can
generalise to multiple identities while producing high-quality results. We
train and evaluate our proposed approach and compare it against existing
methods of talking head synthesis. We show that our model can compete with
these methods in terms of visual quality and lip-sync accuracy. Our results
highlight the potential of our proposed approach to be used for a wide range of
applications, including virtual assistants, entertainment, and education. For a
video demonstration of the results and our user study, please refer to our
supplementary material.",None,-1
Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning,0.545031,"In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly
autonomously in solving a task, but can request help from an external expert
when needed. However, knowing when to request such assistance is critical: too
few requests can lead to the robot making mistakes, but too many requests can
overload the expert. In this paper, we present a Reinforcement Learning based
approach to this problem, where a semi-autonomous agent asks for external
assistance when it has low confidence in the eventual success of the task. The
confidence level is computed by estimating the variance of the return from the
current state. We show that this estimate can be iteratively improved during
training using a Bellman-like recursion. On discrete navigation problems with
both fully- and partially-observable state information, we show that our method
makes effective use of a limited budget of expert calls at run-time, despite
having no access to the expert at training time.",None,-1
GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis,0.0819596,"In this paper, we propose a novel approach (called GPT4MIA) that utilizes
Generative Pre-trained Transformer (GPT) as a plug-and-play transductive
inference tool for medical image analysis (MIA). We provide theoretical
analysis on why a large pre-trained language model such as GPT-3 can be used as
a plug-and-play transductive inference model for MIA. At the methodological
level, we develop several technical treatments to improve the efficiency and
effectiveness of GPT4MIA, including better prompt structure design, sample
selection, and prompt ordering of representative samples/features. We present
two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction
errors and (2) improving prediction accuracy, working in conjecture with
well-established vision-based models for image classification (e.g., ResNet).
Experiments validate that our proposed method is effective for these two tasks.
We further discuss the opportunities and challenges in utilizing
Transformer-based large language models for broader MIA applications.",None,-1
SemHint-MD: Learning from Noisy Semantic Labels for Self-Supervised Monocular Depth Estimation,0.186522,"Without ground truth supervision, self-supervised depth estimation can be
trapped in a local minimum due to the gradient-locality issue of the
photometric loss. In this paper, we present a framework to enhance depth by
leveraging semantic segmentation to guide the network to jump out of the local
minimum. Prior works have proposed to share encoders between these two tasks or
explicitly align them based on priors like the consistency between edges in the
depth and segmentation maps. Yet, these methods usually require ground truth or
high-quality pseudo labels, which may not be easily accessible in real-world
applications. In contrast, we investigate self-supervised depth estimation
along with a segmentation branch that is supervised with noisy labels provided
by models pre-trained with limited data. We extend parameter sharing from the
encoder to the decoder and study the influence of different numbers of shared
decoder parameters on model performance. Also, we propose to use cross-task
information to refine current depth and segmentation predictions to generate
pseudo-depth and semantic labels for training. The advantages of the proposed
method are demonstrated through extensive experiments on the KITTI benchmark
and a downstream task for endoscopic tissue deformation tracking.",None,-1
Aligning Bag of Regions for Open-Vocabulary Object Detection,0.856991,"Pre-trained vision-language models (VLMs) learn to align vision and language
representations on large-scale datasets, where each image-text pair usually
contains a bag of semantic concepts. However, existing open-vocabulary object
detectors only align region embeddings individually with the corresponding
features extracted from the VLMs. Such a design leaves the compositional
structure of semantic concepts in a scene under-exploited, although the
structure may be implicitly learned by the VLMs. In this work, we propose to
align the embedding of bag of regions beyond individual regions. The proposed
method groups contextually interrelated regions as a bag. The embeddings of
regions in a bag are treated as embeddings of words in a sentence, and they are
sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which
is learned to be aligned to the corresponding features extracted by a frozen
VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the
previous best results by 4.6 box AP50 and 2.8 mask AP on novel categories of
open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are
available at https://github.com/wusize/ovdet.",None,-1
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,0.914755,"Reconstructing soft tissues from stereo endoscope videos is an essential
prerequisite for many medical applications. Previous methods struggle to
produce high-quality geometry and appearance due to their inadequate
representations of 3D scenes. To address this issue, we propose a novel
neural-field-based method, called EndoSurf, which effectively learns to
represent a deforming surface from an RGBD sequence. In EndoSurf, we model
surface dynamics, shape, and texture with three neural fields. First, 3D points
are transformed from the observed space to the canonical space using the
deformation field. The signed distance function (SDF) field and radiance field
then predict their SDFs and colors, respectively, with which RGBD images can be
synthesized via differentiable volume rendering. We constrain the learned shape
by tailoring multiple regularization strategies and disentangling geometry and
appearance. Experiments on public endoscope datasets demonstrate that EndoSurf
significantly outperforms existing solutions, particularly in reconstructing
high-fidelity shapes. Code is available at
https://github.com/Ruyi-Zha/endosurf.git.",None,-1
SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings,0.221102,"Adding interpretability to word embeddings represents an area of active
research in text representation. Recent work has explored thepotential of
embedding words via so-called polar dimensions (e.g. good vs. bad, correct vs.
wrong). Examples of such recent approaches include SemAxis, POLAR, FrameAxis,
and BiImp. Although these approaches provide interpretable dimensions for
words, they have not been designed to deal with polysemy, i.e. they can not
easily distinguish between different senses of words. To address this
limitation, we present SensePOLAR, an extension of the original POLAR framework
that enables word-sense aware interpretability for pre-trained contextual word
embeddings. The resulting interpretable word embeddings achieve a level of
performance that is comparable to original contextual word embeddings across a
variety of natural language processing tasks including the GLUE and SQuAD
benchmarks. Our work removes a fundamental limitation of existing approaches by
offering users sense aware interpretations for contextual word embeddings.",None,-1
Structured Voronoi Sampling,0.0208439,"Gradient-based sampling algorithms have demonstrated their effectiveness in
text generation, especially in the context of controlled text generation.
However, there exists a lack of theoretically grounded and principled
approaches for this task. In this paper, we take an important step toward
building a principled approach for sampling from language models with
gradient-based methods. We use discrete distributions given by language models
to define densities and develop an algorithm based on Hamiltonian Monte Carlo
to sample from them. We name our gradient-based technique Structured Voronoi
Sampling (SVS). In an experimental setup where the reference distribution is
known, we show that the empirical distribution of SVS samples is closer to the
reference distribution compared to alternative sampling schemes. Furthermore,
in a controlled generation task, SVS is able to generate fluent and diverse
samples while following the control targets significantly better than other
methods.",None,-1
Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction,0.546302,"In this study, we investigated the potential of GPT-3 for the anti-cancer
drug sensitivity prediction task using structured pharmacogenomics data across
five tissue types and evaluated its performance with zero-shot prompting and
fine-tuning paradigms. The drug's smile representation and cell line's genomic
mutation features were predictive of the drug response. The results from this
study have the potential to pave the way for designing more efficient treatment
protocols in precision oncology.",None,-1
Enhancing Large Language Model with Self-Controlled Memory Framework,0.0772392,"Large Language Models (LLMs) are constrained by their inability to process
lengthy inputs, resulting in the loss of critical historical information. To
address this limitation, in this paper, we propose the Self-Controlled Memory
(SCM) framework to enhance the ability of LLMs to maintain long-term memory and
recall relevant information. Our SCM framework comprises three key components:
an LLM-based agent serving as the backbone of the framework, a memory stream
storing agent memories, and a memory controller updating memories and
determining when and how to utilize memories from memory stream. Additionally,
the proposed SCM is able to process ultra-long texts without any modification
or fine-tuning, which can integrate with any instruction following LLMs in a
plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the
effectiveness of SCM for handling lengthy inputs. The annotated dataset covers
three tasks: long-term dialogues, book summarization, and meeting
summarization. Experimental results demonstrate that our method achieves better
retrieval recall and generates more informative responses compared to
competitive baselines in long-term dialogues.
(https://github.com/wbbeyourself/SCM4LLMs)",None,-1
GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding,0.697726,"Humans subconsciously engage in geospatial reasoning when reading articles.
We recognize place names and their spatial relations in text and mentally
associate them with their physical locations on Earth. Although pretrained
language models can mimic this cognitive process using linguistic context, they
do not utilize valuable geospatial information in large, widely available
geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a
geospatially grounded language model that enhances the understanding of
geo-entities in natural language. GeoLM leverages geo-entity mentions as
anchors to connect linguistic information in text corpora with geospatial
information extracted from geographical databases. GeoLM connects the two types
of context through contrastive learning and masked language modeling. It also
incorporates a spatial coordinate embedding mechanism to encode distance and
direction relations to capture geospatial context. In the experiment, we
demonstrate that GeoLM exhibits promising capabilities in supporting toponym
recognition, toponym linking, relation extraction, and geo-entity typing, which
bridge the gap between natural language processing and geospatial sciences. The
code is publicly available at https://github.com/knowledge-computing/geolm.",None,-1
SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image,0.947175,"We propose SMPLitex, a method for estimating and manipulating the complete 3D
appearance of humans captured from a single image. SMPLitex builds upon the
recently proposed generative models for 2D images, and extends their use to the
3D domain through pixel-to-surface correspondences computed on the input image.
To this end, we first train a generative model for complete 3D human
appearance, and then fit it into the input image by conditioning the generative
model to the visible parts of the subject. Furthermore, we propose a new
dataset of high-quality human textures built by sampling SMPLitex conditioned
on subject descriptions and images. We quantitatively and qualitatively
evaluate our method in 3 publicly available datasets, demonstrating that
SMPLitex significantly outperforms existing methods for human texture
estimation while allowing for a wider variety of tasks such as editing,
synthesis, and manipulation",None,-1
IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases,0.350466,"Named Entity Recognition (NER) is a core natural language processing task in
which pre-trained language models have shown remarkable performance. However,
standard benchmarks like CoNLL 2003 do not address many of the challenges that
deployed NER systems face, such as having to classify emerging or complex
entities in a fine-grained way. In this paper we present a novel NER cascade
approach comprising three steps: first, identifying candidate entities in the
input sentence; second, linking the each candidate to an existing knowledge
base; third, predicting the fine-grained category for each entity candidate. We
empirically demonstrate the significance of external knowledge bases in
accurately classifying fine-grained and emerging entities. Our system exhibits
robust performance in the MultiCoNER2 shared task, even in the low-resource
language setting where we leverage knowledge bases of high-resource languages.",None,-1
GDDS: Pulmonary Bronchioles Segmentation with Group Deep Dense Supervision,0.734749,"Airway segmentation, especially bronchioles segmentation, is an important but
challenging task because distal bronchus are sparsely distributed and of a fine
scale. Existing neural networks usually exploit sparse topology to learn the
connectivity of bronchioles and inefficient shallow features to capture such
high-frequency information, leading to the breakage or missed detection of
individual thin branches. To address these problems, we contribute a new
bronchial segmentation method based on Group Deep Dense Supervision (GDDS) that
emphasizes fine-scale bronchioles segmentation in a simple-but-effective
manner. First, Deep Dense Supervision (DDS) is proposed by constructing local
dense topology skillfully and implementing dense topological learning on a
specific shallow feature layer. GDDS further empowers the shallow features with
better perception ability to detect bronchioles, even the ones that are not
easily discernible to the naked eye. Extensive experiments on the BAS benchmark
dataset have shown that our method promotes the network to have a high
sensitivity in capturing fine-scale branches and outperforms state-of-the-art
methods by a large margin (+12.8 % in BD and +8.8 % in TD) while only
introducing a small number of extra parameters.",None,-1
Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions,0.737271,"We propose a method for synthesizing edited photo-realistic digital avatars
with text instructions. Given a short monocular RGB video and text
instructions, our method uses an image-conditioned diffusion model to edit one
head image and uses the video stylization method to accomplish the editing of
other head images. Through iterative training and update (three times or more),
our method synthesizes edited photo-realistic animatable 3D neural head avatars
with a deformable neural radiance field head synthesis method. In quantitative
and qualitative studies on various subjects, our method outperforms
state-of-the-art methods.",None,-1
NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support,0.590679,"We present a method for generating high-quality watertight manifold meshes
from multi-view input images. Existing volumetric rendering methods are robust
in optimization but tend to generate noisy meshes with poor topology.
Differentiable rasterization-based methods can generate high-quality meshes but
are sensitive to initialization. Our method combines the benefits of both
worlds; we take the geometry initialization obtained from neural volumetric
fields, and further optimize the geometry as well as a compact neural texture
representation with differentiable rasterizers. Through extensive experiments,
we demonstrate that our method can generate accurate mesh reconstructions with
faithful appearance that are comparable to previous volume rendering methods
while being an order of magnitude faster in rendering. We also show that our
generated mesh and neural texture reconstruction is compatible with existing
graphics pipelines and enables downstream 3D applications such as simulation.
Project page: https://sarahweiii.github.io/neumanifold/",None,-1
Can Programming Languages Boost Each Other via Instruction Tuning?,0.337616,"When human programmers have mastered a programming language, it would be
easier when they learn a new programming language. In this report, we focus on
exploring whether programming languages can boost each other during the
instruction fine-tuning phase of code large language models. We conduct
extensive experiments of 8 popular programming languages (Python, JavaScript,
TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that
programming languages can significantly improve each other. For example,
CodeM-Python 15B trained on Python is able to increase Java by an absolute
17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B
trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our
training data is released at https://github.com/NL2Code/CodeM.",None,-1
Knowledge-grounded Natural Language Recommendation Explanation,0.238709,"Explanations accompanied by a recommendation can assist users in
understanding the decision made by recommendation systems, which in turn
increases a user's confidence and trust in the system. Recently, research has
focused on generating natural language explanations in a human-readable format.
Thus far, the proposed approaches leverage item reviews written by users, which
are often subjective, sparse in language, and unable to account for new items
that have not been purchased or reviewed before. Instead, we aim to generate
fact-grounded recommendation explanations that are objectively described with
item features while implicitly considering a user's preferences, based on the
user's purchase history. To achieve this, we propose a knowledge graph (KG)
approach to natural language explainable recommendation. Our approach draws on
user-item features through a novel collaborative filtering-based KG
representation to produce fact-grounded, personalized explanations, while
jointly learning user-item representations for recommendation scoring.
Experimental results show that our approach consistently outperforms previous
state-of-the-art models on natural language explainable recommendation.",None,-1
ChatGPT: A Meta-Analysis after 2.5 Months,0.951498,"ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and
media attention since its release in November 2022. However, little hard
evidence is available regarding its perception in various sources. In this
paper, we analyze over 300,000 tweets and more than 150 scientific papers to
investigate how ChatGPT is perceived and discussed. Our findings show that
ChatGPT is generally viewed as of high quality, with positive sentiment and
emotions of joy dominating in social media. Its perception has slightly
decreased since its debut, however, with joy decreasing and (negative) surprise
on the rise, and it is perceived more negatively in languages other than
English. In recent scientific papers, ChatGPT is characterized as a great
opportunity across various fields including the medical domain, but also as a
threat concerning ethics and receives mixed assessments for education. Our
comprehensive meta-analysis of ChatGPT's current perception after 2.5 months
since its release can contribute to shaping the public debate and informing its
future development. We make our data available.",None,-1
Privacy Measurement in Tabular Synthetic Data: State of the Art and Future Research Directions,0.391416,"Synthetic data (SD) have garnered attention as a privacy enhancing
technology. Unfortunately, there is no standard for quantifying their degree of
privacy protection. In this paper, we discuss proposed quantification
approaches. This contributes to the development of SD privacy standards;
stimulates multi-disciplinary discussion; and helps SD researchers make
informed modeling and evaluation decisions.",None,-1
Generative Knowledge Selection for Knowledge-Grounded Dialogues,0.679619,"Knowledge selection is the key in knowledge-grounded dialogues (KGD), which
aims to select an appropriate knowledge snippet to be used in the utterance
based on dialogue history. Previous studies mainly employ the classification
approach to classify each candidate snippet as ""relevant"" or ""irrelevant""
independently. However, such approaches neglect the interactions between
snippets, leading to difficulties in inferring the meaning of snippets.
Moreover, they lack modeling of the discourse structure of dialogue-knowledge
interactions. We propose a simple yet effective generative approach for
knowledge selection, called GenKS. GenKS learns to select snippets by
generating their identifiers with a sequence-to-sequence model. GenKS therefore
captures intra-knowledge interaction inherently through attention mechanisms.
Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge
interactions explicitly. We conduct experiments on three benchmark datasets,
and verify GenKS achieves the best results on both knowledge selection and
response generation.",None,-1
PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining,0.42551,"A rich representation is key to general robotic manipulation, but existing
approaches to representation learning require large amounts of multimodal
demonstrations. In this work we propose PLEX, a transformer-based architecture
that learns from a small amount of task-agnostic visuomotor trajectories and a
much larger amount of task-conditioned object manipulation videos -- a type of
data available in quantity. PLEX uses visuomotor trajectories to induce a
latent feature space and to learn task-agnostic manipulation routines, while
diverse video-only demonstrations teach PLEX how to plan in the induced latent
feature space for a wide variety of tasks. Experiments showcase PLEX's
generalization on Meta-World and SOTA performance in challenging Robosuite
environments. In particular, using relative positional encoding in PLEX's
transformers greatly helps in low-data regimes of learning from human-collected
demonstrations. The paper's accompanying code and data are available at
https://microsoft.github.io/PLEX.",None,-1
"ChatGPT may Pass the Bar Exam soon, but has a Long Way to Go for the LexGLUE benchmark",0.502465,"Following the hype around OpenAI's ChatGPT conversational agent, the last
straw in the recent development of Large Language Models (LLMs) that
demonstrate emergent unprecedented zero-shot capabilities, we audit the latest
OpenAI's GPT-3.5 model, `gpt-3.5-turbo', the first available ChatGPT model, in
the LexGLUE benchmark in a zero-shot fashion providing examples in a templated
instruction-following format. The results indicate that ChatGPT achieves an
average micro-F1 score of 47.6% across LexGLUE tasks, surpassing the baseline
guessing rates. Notably, the model performs exceptionally well in some
datasets, achieving micro-F1 scores of 62.8% and 70.2% in the ECtHR B and
LEDGAR datasets, respectively. The code base and model predictions are
available for review on https://github.com/coastalcph/zeroshot_lexglue.",None,-1
TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition,0.770044,"Emotion Support Conversation (ESC) is an emerging and challenging task with
the goal of reducing the emotional distress of people. Previous attempts fail
to maintain smooth transitions between utterances in ESC because they ignore to
grasp the fine-grained transition information at each dialogue turn. To solve
this problem, we propose to take into account turn-level state
\textbf{Trans}itions of \textbf{ESC} (\textbf{TransESC}) from three
perspectives, including semantics transition, strategy transition and emotion
transition, to drive the conversation in a smooth and natural way.
Specifically, we construct the state transition graph with a two-step way,
named transit-then-interact, to grasp such three types of turn-level transition
information. Finally, they are injected into the transition-aware decoder to
generate more engaging responses. Both automatic and human evaluations on the
benchmark dataset demonstrate the superiority of TransESC to generate more
smooth and effective supportive responses. Our source code is available at
\url{https://github.com/circle-hit/TransESC}.",None,-1
DAVA: Disentangling Adversarial Variational Autoencoder,0.428561,"The use of well-disentangled representations offers many advantages for
downstream tasks, e.g. an increased sample efficiency, or better
interpretability. However, the quality of disentangled interpretations is often
highly dependent on the choice of dataset-specific hyperparameters, in
particular the regularization strength. To address this issue, we introduce
DAVA, a novel training procedure for variational auto-encoders. DAVA completely
alleviates the problem of hyperparameter selection. We compare DAVA to models
with optimal hyperparameters. Without any hyperparameter tuning, DAVA is
competitive on a diverse range of commonly used datasets. Underlying DAVA, we
discover a necessary condition for unsupervised disentanglement, which we call
PIPE. We demonstrate the ability of PIPE to positively predict the performance
of downstream models in abstract reasoning. We also thoroughly investigate
correlations with existing supervised and unsupervised metrics. The code is
available at https://github.com/besterma/dava.",None,-1
MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System,0.865593,"Multi-modal sarcasm detection has attracted much recent attention.
Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder
the development of reliable multi-modal sarcasm detection system: (1) There are
some spurious cues in MMSD, leading to the model bias learning; (2) The
negative samples in MMSD are not always reasonable. To solve the aforementioned
issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings
of MMSD, by removing the spurious cues and re-annotating the unreasonable
samples. Meanwhile, we present a novel framework called multi-view CLIP that is
capable of leveraging multi-grained cues from multiple perspectives (i.e.,
text, image, and text-image interaction view) for multi-modal sarcasm
detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for
building reliable multi-modal sarcasm detection systems and multi-view CLIP can
significantly outperform the previous best baselines.",None,-1
GPT is becoming a Turing machine: Here are some ways to program it,0.3861,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",None,-1
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,0.66683,"The Transformer structures have been widely used in computer vision and have
recently made an impact in the area of medical image registration. However, the
use of Transformer in most registration networks is straightforward. These
networks often merely use the attention mechanism to boost the feature learning
as the segmentation networks do, but do not sufficiently design to be adapted
for the registration task. In this paper, we propose a novel motion
decomposition Transformer (ModeT) to explicitly model multiple motion
modalities by fully exploiting the intrinsic capability of the Transformer
structure for deformation estimation. The proposed ModeT naturally transforms
the multi-head neighborhood attention relationship into the multi-coordinate
relationship to model multiple motion modes. Then the competitive weighting
module (CWM) fuses multiple deformation sub-fields to generate the resulting
deformation field. Extensive experiments on two public brain magnetic resonance
imaging (MRI) datasets show that our method outperforms current
state-of-the-art registration networks and Transformers, demonstrating the
potential of our ModeT for the challenging non-rigid deformation estimation
problem. The benchmarks and our code are publicly available at
https://github.com/ZAX130/SmileCode.",None,-1
InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling,0.898925,"Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.",None,-1
Learning a Depth Covariance Function,0.248531,"We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.",None,-1
Toward A Logical Theory Of Fairness and Bias,0.248584,"Fairness in machine learning is of considerable interest in recent years
owing to the propensity of algorithms trained on historical data to amplify and
perpetuate historical biases. In this paper, we argue for a formal
reconstruction of fairness definitions, not so much to replace existing
definitions but to ground their application in an epistemic setting and allow
for rich environmental modelling. Consequently we look into three notions:
fairness through unawareness, demographic parity and counterfactual fairness,
and formalise these in the epistemic situation calculus.",None,-1
Inst-Inpaint: Instructing to Remove Objects with Diffusion Models,0.917994,"Image inpainting task refers to erasing unwanted pixels from images and
filling them in a semantically consistent and realistic way. Traditionally, the
pixels that are wished to be erased are defined with binary masks. From the
application point of view, a user needs to generate the masks for the objects
they would like to remove which can be time-consuming and prone to errors. In
this work, we are interested in an image inpainting algorithm that estimates
which object to be removed based on natural language input and removes it,
simultaneously. For this purpose, first, we construct a dataset named
GQA-Inpaint for this task. Second, we present a novel inpainting framework,
Inst-Inpaint, that can remove objects from images based on the instructions
given as text prompts. We set various GAN and diffusion-based baselines and run
experiments on synthetic and real image datasets. We compare methods with
different evaluation metrics that measure the quality and accuracy of the
models and show significant quantitative and qualitative improvements.",None,-1
GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions,0.812818,"There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings",None,-1
HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation,0.628845,"In this paper, we study Text-to-3D content generation leveraging 2D diffusion
priors to enhance the quality and detail of the generated 3D models. Recent
progress (Magic3D) in text-to-3D has shown that employing high-resolution
(e.g., 512 x 512) renderings can lead to the production of high-quality 3D
models using latent diffusion priors. To enable rendering at even higher
resolutions, which has the potential to further augment the quality and detail
of the models, we propose a novel approach that combines multiple noise
estimation processes with a pretrained 2D diffusion prior. Distinct from the
Bar-Tal et al.s' study which binds multiple denoised results to generate images
from texts, our approach integrates the computation of scoring distillation
losses such as SDS loss and VSD loss which are essential techniques for the 3D
content generation with 2D diffusion priors. We experimentally evaluated the
proposed approach. The results show that the proposed approach can generate
high-quality details compared to the baselines.",None,-1
Turning large language models into cognitive models,0.841132,"Large language models are powerful systems that excel at many tasks, ranging
from translation to mathematical reasoning. Yet, at the same time, these models
often show unhuman-like characteristics. In the present paper, we address this
gap and ask whether large language models can be turned into cognitive models.
We find that -- after finetuning them on data from psychological experiments --
these models offer accurate representations of human behavior, even
outperforming traditional cognitive models in two decision-making domains. In
addition, we show that their representations contain the information necessary
to model behavior on the level of individual subjects. Finally, we demonstrate
that finetuning on multiple tasks enables large language models to predict
human behavior in a previously unseen task. Taken together, these results
suggest that large, pre-trained models can be adapted to become generalist
cognitive models, thereby opening up new research directions that could
transform cognitive psychology and the behavioral sciences as a whole.",None,-1
The Importance of Time in Causal Algorithmic Recourse,0.332348,"The application of Algorithmic Recourse in decision-making is a promising
field that offers practical solutions to reverse unfavorable decisions.
However, the inability of these methods to consider potential dependencies
among variables poses a significant challenge due to the assumption of feature
independence. Recent advancements have incorporated knowledge of causal
dependencies, thereby enhancing the quality of the recommended recourse
actions. Despite these improvements, the inability to incorporate the temporal
dimension remains a significant limitation of these approaches. This is
particularly problematic as identifying and addressing the root causes of
undesired outcomes requires understanding time-dependent relationships between
variables. In this work, we motivate the need to integrate the temporal
dimension into causal algorithmic recourse methods to enhance recommendations'
plausibility and reliability. The experimental evaluation highlights the
significance of the role of time in this field.",None,-1
Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs,0.464739,"Intent-based management will play a critical role in achieving customers'
expectations in the next-generation mobile networks. Traditional methods cannot
perform efficient resource management since they tend to handle each
expectation independently. Existing approaches, e.g., based on multi-agent
reinforcement learning (MARL) allocate resources in an efficient fashion when
there are conflicting expectations on the network slice. However, in reality,
systems are often far more complex to be addressed by a standalone MARL
formulation. Often there exists a hierarchical structure of intent fulfilment
where multiple pre-trained, self-interested agents may need to be further
orchestrated by a supervisor or controller agent. Such agents may arrive in the
system adhoc, which then needs to be orchestrated along with other available
agents. Retraining the whole system every time is often infeasible given the
associated time and cost. Given the challenges, such adhoc coordination of
pre-trained systems could be achieved through an intelligent supervisor agent
which incentivizes pre-trained RL/MARL agents through sets of dynamic contracts
(goals or bonuses) and encourages them to act as a cohesive unit towards
fulfilling a global expectation. Some approaches use a rule-based supervisor
agent and deploy the hierarchical constituent agents sequentially, based on
human-coded rules.
  In the current work, we propose a framework whereby pre-trained agents can be
orchestrated in parallel leveraging an AI-based supervisor agent. For this, we
propose to use Adhoc-Teaming approaches which assign optimal goals to the MARL
agents and incentivize them to exhibit certain desired behaviours. Results on
the network emulator show that the proposed approach results in faster and
improved fulfilment of expectations when compared to rule-based approaches and
even generalizes to changes in environments.",None,-1
Prototype Knowledge Distillation for Medical Segmentation with Missing Modality,0.646528,"Multi-modality medical imaging is crucial in clinical treatment as it can
provide complementary information for medical image segmentation. However,
collecting multi-modal data in clinical is difficult due to the limitation of
the scan time and other clinical situations. As such, it is clinically
meaningful to develop an image segmentation paradigm to handle this missing
modality problem. In this paper, we propose a prototype knowledge distillation
(ProtoKD) method to tackle the challenging problem, especially for the toughest
scenario when only single modal data can be accessed. Specifically, our ProtoKD
can not only distillate the pixel-wise knowledge of multi-modality data to
single-modality data but also transfer intra-class and inter-class feature
variations, such that the student model could learn more robust feature
representation from the teacher model and inference with only one single
modality data. Our method achieves state-of-the-art performance on BraTS
benchmark. The code is available at
\url{https://github.com/SakurajimaMaiii/ProtoKD}.",None,-1
Learning Sparse and Low-Rank Priors for Image Recovery via Iterative Reweighted Least Squares Minimization,0.327946,"We introduce a novel optimization algorithm for image recovery under learned
sparse and low-rank constraints, which we parameterize as weighted extensions
of the $\ell_p^p$-vector and $\mathcal S_p^p$ Schatten-matrix quasi-norms for
$0\!<p\!\le1$, respectively. Our proposed algorithm generalizes the Iteratively
Reweighted Least Squares (IRLS) method, used for signal recovery under $\ell_1$
and nuclear-norm constrained minimization. Further, we interpret our overall
minimization approach as a recurrent network that we then employ to deal with
inverse low-level computer vision problems. Thanks to the convergence
guarantees that our IRLS strategy offers, we are able to train the derived
reconstruction networks using a memory-efficient implicit back-propagation
scheme, which does not pose any restrictions on their effective depth. To
assess our networks' performance, we compare them against other existing
reconstruction methods on several inverse problems, namely image deblurring,
super-resolution, demosaicking and sparse recovery. Our reconstruction results
are shown to be very competitive and in many cases outperform those of existing
unrolled networks, whose number of parameters is orders of magnitude higher
than that of our learned models.",None,-1
The Best of Both Worlds: Combining Human and Machine Translations for Multilingual Semantic Parsing with Active Learning,0.4696,"Multilingual semantic parsing aims to leverage the knowledge from the
high-resource languages to improve low-resource semantic parsing, yet commonly
suffers from the data imbalance problem. Prior works propose to utilize the
translations by either humans or machines to alleviate such issues. However,
human translations are expensive, while machine translations are cheap but
prone to error and bias. In this work, we propose an active learning approach
that exploits the strengths of both human and machine translations by
iteratively adding small batches of human translations into the
machine-translated training set. Besides, we propose novel aggregated
acquisition criteria that help our active learning method select utterances to
be manually translated. Our experiments demonstrate that an ideal utterance
selection can significantly reduce the error and bias in the translated data,
resulting in higher parser accuracies than the parsers merely trained on the
machine-translated data.",None,-1
Transfer Learning and Class Decomposition for Detecting the Cognitive Decline of Alzheimer Disease,0.127708,"Early diagnosis of Alzheimer's disease (AD) is essential in preventing the
disease's progression. Therefore, detecting AD from neuroimaging data such as
structural magnetic resonance imaging (sMRI) has been a topic of intense
investigation in recent years. Deep learning has gained considerable attention
in Alzheimer's detection. However, training a convolutional neural network from
scratch is challenging since it demands more computational time and a
significant amount of annotated data. By transferring knowledge learned from
other image recognition tasks to medical image classification, transfer
learning can provide a promising and effective solution. Irregularities in the
dataset distribution present another difficulty. Class decomposition can tackle
this issue by simplifying learning a dataset's class boundaries. Motivated by
these approaches, this paper proposes a transfer learning method using class
decomposition to detect Alzheimer's disease from sMRI images. We use two
ImageNet-trained architectures: VGG19 and ResNet50, and an entropy-based
technique to determine the most informative images. The proposed model achieved
state-of-the-art performance in the Alzheimer's disease (AD) vs mild cognitive
impairment (MCI) vs cognitively normal (CN) classification task with a 3\%
increase in accuracy from what is reported in the literature.",None,-1
Federated Neural Radiance Fields,0.2223,"The ability of neural radiance fields or NeRFs to conduct accurate 3D
modelling has motivated application of the technique to scene representation.
Previous approaches have mainly followed a centralised learning paradigm, which
assumes that all training images are available on one compute node for
training. In this paper, we consider training NeRFs in a federated manner,
whereby multiple compute nodes, each having acquired a distinct set of
observations of the overall scene, learn a common NeRF in parallel. This
supports the scenario of cooperatively modelling a scene using multiple agents.
Our contribution is the first federated learning algorithm for NeRF, which
splits the training effort across multiple compute nodes and obviates the need
to pool the images at a central node. A technique based on low-rank
decomposition of NeRF layers is introduced to reduce bandwidth consumption to
transmit the model parameters for aggregation. Transferring compressed models
instead of the raw data also contributes to the privacy of the data collecting
agents.",None,-1
Probing neural representations of scene perception in a hippocampally dependent task using artificial neural networks,0.220326,"Deep artificial neural networks (DNNs) trained through backpropagation
provide effective models of the mammalian visual system, accurately capturing
the hierarchy of neural responses through primary visual cortex to inferior
temporal cortex (IT). However, the ability of these networks to explain
representations in higher cortical areas is relatively lacking and considerably
less well researched. For example, DNNs have been less successful as a model of
the egocentric to allocentric transformation embodied by circuits in
retrosplenial and posterior parietal cortex. We describe a novel scene
perception benchmark inspired by a hippocampal dependent task, designed to
probe the ability of DNNs to transform scenes viewed from different egocentric
perspectives. Using a network architecture inspired by the connectivity between
temporal lobe structures and the hippocampus, we demonstrate that DNNs trained
using a triplet loss can learn this task. Moreover, by enforcing a factorized
latent space, we can split information propagation into ""what"" and ""where""
pathways, which we use to reconstruct the input. This allows us to beat the
state-of-the-art for unsupervised object segmentation on the CATER and
MOVi-A,B,C benchmarks.",None,-1
3D View Prediction Models of the Dorsal Visual Stream,0.676852,"Deep neural network representations align well with brain activity in the
ventral visual stream. However, the primate visual system has a distinct dorsal
processing stream with different functional properties. To test if a model
trained to perceive 3D scene geometry aligns better with neural responses in
dorsal visual areas, we trained a self-supervised geometry-aware recurrent
neural network (GRNN) to predict novel camera views using a 3D feature memory.
We compared GRNN to self-supervised baseline models that have been shown to
align well with ventral regions using the large-scale fMRI Natural Scenes
Dataset (NSD). We found that while the baseline models accounted better for
ventral brain regions, GRNN accounted for a greater proportion of variance in
dorsal brain regions. Our findings demonstrate the potential for using
task-relevant models to probe representational differences across visual
streams.",None,-1
DualGenerator: Information Interaction-based Generative Network for Point Cloud Completion,0.0849805,"Point cloud completion estimates complete shapes from incomplete point clouds
to obtain higher-quality point cloud data. Most existing methods only consider
global object features, ignoring spatial and semantic information of adjacent
points. They cannot distinguish structural information well between different
object parts, and the robustness of models is poor. To tackle these challenges,
we propose an information interaction-based generative network for point cloud
completion ($\mathbf{DualGenerator}$). It contains an adversarial generation
path and a variational generation path, which interact with each other and
share weights. DualGenerator introduces a local refinement module in generation
paths, which captures general structures from partial inputs, and then refines
shape details of the point cloud. It promotes completion in the unknown region
and makes a distinction between different parts more obvious. Moreover, we
design DGStyleGAN to improve the generation quality further. It promotes the
robustness of this network combined with fusion analysis of dual-path
completion results. Qualitative and quantitative evaluations demonstrate that
our method is superior on MVP and Completion3D datasets. The performance will
not degrade significantly after adding noise interference or sparse sampling.",None,-1
Semi-supervised Relation Extraction via Data Augmentation and Consistency-training,0.513124,"Due to the semantic complexity of the Relation extraction (RE) task,
obtaining high-quality human labelled data is an expensive and noisy process.
To improve the sample efficiency of the models, semi-supervised learning (SSL)
methods aim to leverage unlabelled data in addition to learning from limited
labelled data points. Recently, strong data augmentation combined with
consistency-based semi-supervised learning methods have advanced the state of
the art in several SSL tasks. However, adapting these methods to the RE task
has been challenging due to the difficulty of data augmentation for RE. In this
work, we leverage the recent advances in controlled text generation to perform
high quality data augmentation for the RE task. We further introduce small but
significant changes to model architecture that allows for generation of more
training data by interpolating different data points in their latent space.
These data augmentations along with consistency training result in very
competitive results for semi-supervised relation extraction on four benchmark
datasets.",None,-1
LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning,0.872162,"Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)
competition is detailed in this paper. Unlike conventional video captioning
tasks, GEBC demands that the captioning model possess an understanding of
immediate changes in status around the designated video boundary, making it a
difficult task. This paper proposes an effective model LLMVA-GEBC (Large
Language Model with Video Adapter for Generic Event Boundary Captioning): (1)
We utilize a pretrained LLM for generating human-like captions with high
quality. (2) To adapt the model to the GEBC task, we take the video Q-former as
an adapter and train it with the frozen visual feature extractors and LLM. Our
proposed method achieved a 76.14 score on the test set and won the first place
in the challenge. Our code is available at
https://github.com/zjr2000/LLMVA-GEBC .",None,-1
Is forgetting less a good inductive bias for forward transfer?,0.491911,"One of the main motivations of studying continual learning is that the
problem setting allows a model to accrue knowledge from past tasks to learn new
tasks more efficiently. However, recent studies suggest that the key metric
that continual learning algorithms optimize, reduction in catastrophic
forgetting, does not correlate well with the forward transfer of knowledge. We
believe that the conclusion previous works reached is due to the way they
measure forward transfer. We argue that the measure of forward transfer to a
task should not be affected by the restrictions placed on the continual learner
in order to preserve knowledge of previous tasks. Instead, forward transfer
should be measured by how easy it is to learn a new task given a set of
representations produced by continual learning on previous tasks. Under this
notion of forward transfer, we evaluate different continual learning algorithms
on a variety of image classification benchmarks. Our results indicate that less
forgetful representations lead to a better forward transfer suggesting a strong
correlation between retaining past information and learning efficiency on new
tasks. Further, we found less forgetful representations to be more diverse and
discriminative compared to their forgetful counterparts.",None,-1
PanelNet: Understanding 360 Indoor Environment via Panel Representation,0.756741,"Indoor 360 panoramas have two essential properties. (1) The panoramas are
continuous and seamless in the horizontal direction. (2) Gravity plays an
important role in indoor environment design. By leveraging these properties, we
present PanelNet, a framework that understands indoor environments using a
novel panel representation of 360 images. We represent an equirectangular
projection (ERP) as consecutive vertical panels with corresponding 3D panel
geometry. To reduce the negative impact of panoramic distortion, we incorporate
a panel geometry embedding network that encodes both the local and global
geometric features of a panel. To capture the geometric context in room design,
we introduce Local2Global Transformer, which aggregates local information
within a panel and panel-wise global context. It greatly improves the model
performance with low training overhead. Our method outperforms existing methods
on indoor 360 depth estimation and shows competitive results against
state-of-the-art approaches on the task of indoor layout estimation and
semantic segmentation.",None,-1
Instance Segmentation of Dislocations in TEM Images,0.243798,"Quantitative Transmission Electron Microscopy (TEM) during in-situ straining
experiment is able to reveal the motion of dislocations -- linear defects in
the crystal lattice of metals. In the domain of materials science, the
knowledge about the location and movement of dislocations is important for
creating novel materials with superior properties. A long-standing problem,
however, is to identify the position and extract the shape of dislocations,
which would ultimately help to create a digital twin of such materials. In this
work, we quantitatively compare state-of-the-art instance segmentation methods,
including Mask R-CNN and YOLOv8. The dislocation masks as the results of the
instance segmentation are converted to mathematical lines, enabling
quantitative analysis of dislocation length and geometry -- important
information for the domain scientist, which we then propose to include as a
novel length-aware quality metric for estimating the network performance. Our
segmentation pipeline shows a high accuracy suitable for all domain-specific,
further post-processing. Additionally, our physics-based metric turns out to
perform much more consistently than typically used pixel-wise metrics.",None,-1
StyLess: Boosting the Transferability of Adversarial Examples,0.448485,"Adversarial attacks can mislead deep neural networks (DNNs) by adding
imperceptible perturbations to benign examples. The attack transferability
enables adversarial examples to attack black-box DNNs with unknown
architectures or parameters, which poses threats to many real-world
applications. We find that existing transferable attacks do not distinguish
between style and content features during optimization, limiting their attack
transferability. To improve attack transferability, we propose a novel attack
method called style-less perturbation (StyLess). Specifically, instead of using
a vanilla network as the surrogate model, we advocate using stylized networks,
which encode different style features by perturbing an adaptive instance
normalization. Our method can prevent adversarial examples from using
non-robust style features and help generate transferable perturbations.
Comprehensive experiments show that our method can significantly improve the
transferability of adversarial examples. Furthermore, our approach is generic
and can outperform state-of-the-art transferable attacks when combined with
other attack techniques.",None,-1
SpinDOE: A ball spin estimation method for table tennis robot,0.701458,"Spin plays a considerable role in table tennis, making a shot's trajectory
harder to read and predict. However, the spin is challenging to measure because
of the ball's high velocity and the magnitude of the spin values. Existing
methods either require extremely high framerate cameras or are unreliable
because they use the ball's logo, which may not always be visible. Because of
this, many table tennis-playing robots ignore the spin, which severely limits
their capabilities. This paper proposes an easily implementable and reliable
spin estimation method. We developed a dotted-ball orientation estimation (DOE)
method, that can then be used to estimate the spin. The dots are first
localized on the image using a CNN and then identified using geometric hashing.
The spin is finally regressed from the estimated orientations. Using our
algorithm, the ball's orientation can be estimated with a mean error of
2.4{\deg} and the spin estimation has an relative error lower than 1%. Spins up
to 175 rps are measurable with a camera of 350 fps in real time. Using our
method, we generated a dataset of table tennis ball trajectories with position
and spin, available on our project page.",None,-1
GCNet: Probing Self-Similarity Learning for Generalized Counting Network,0.250829,"The class-agnostic counting (CAC) problem has caught increasing attention
recently due to its wide societal applications and arduous challenges. To count
objects of different categories, existing approaches rely on user-provided
exemplars, which is hard-to-obtain and limits their generality. In this paper,
we aim to empower the framework to recognize adaptive exemplars within the
whole images. A zero-shot Generalized Counting Network (GCNet) is developed,
which uses a pseudo-Siamese structure to automatically and effectively learn
pseudo exemplar clues from inherent repetition patterns. In addition, a
weakly-supervised scheme is presented to reduce the burden of laborious density
maps required by all contemporary CAC models, allowing GCNet to be trained
using count-level supervisory signals in an end-to-end manner. Without
providing any spatial location hints, GCNet is capable of adaptively capturing
them through a carefully-designed self-similarity learning strategy. Extensive
experiments and ablation studies on the prevailing benchmark FSC147 for
zero-shot CAC demonstrate the superiority of our GCNet. It performs on par with
existing exemplar-dependent methods and shows stunning cross-dataset generality
on crowd-specific datasets, e.g., ShanghaiTech Part A, Part B and UCF_QNRF.",None,-1
Improving Seq2Seq Grammatical Error Correction via Decoding Interventions,0.745858,"The sequence-to-sequence (Seq2Seq) approach has recently been widely used in
grammatical error correction (GEC) and shows promising performance. However,
the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC
model can only be trained on parallel data, which, in GEC task, is often noisy
and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an
explicit awareness of the correctness of the token being generated. In this
paper, we propose a unified decoding intervention framework that employs an
external critic to assess the appropriateness of the token to be generated
incrementally, and then dynamically influence the choice of the next token. We
discover and investigate two types of critics: a pre-trained left-to-right
language model critic and an incremental target-side grammatical error detector
critic. Through extensive experiments on English and Chinese datasets, our
framework consistently outperforms strong baselines and achieves results
competitive with state-of-the-art methods.",None,-1
Presenting Multiagent Challenges in Team Sports Analytics,0.753084,"This paper draws correlations between several challenges and opportunities
within the area of team sports analytics and key research areas within
multiagent systems (MAS). We specifically consider invasion games, defined as
sports where players invade the opposing team's territory and can interact
anywhere on a playing surface such as ice hockey, soccer, and basketball. We
argue that MAS is well-equipped to study invasion games and will benefit both
MAS and sports analytics fields. Our discussion highlights areas for MAS
implementation and further development along two axes: short-term in-game
strategy (coaching) and long-term team planning (management).",None,-1
Wavelet-based Unsupervised Label-to-Image Translation,0.312598,"Semantic Image Synthesis (SIS) is a subclass of image-to-image translation
where a semantic layout is used to generate a photorealistic image.
State-of-the-art conditional Generative Adversarial Networks (GANs) need a huge
amount of paired data to accomplish this task while generic unpaired
image-to-image translation frameworks underperform in comparison, because they
color-code semantic layouts and learn correspondences in appearance instead of
semantic content. Starting from the assumption that a high quality generated
image should be segmented back to its semantic layout, we propose a new
Unsupervised paradigm for SIS (USIS) that makes use of a self-supervised
segmentation loss and whole image wavelet based discrimination. Furthermore, in
order to match the high-frequency distribution of real images, a novel
generator architecture in the wavelet domain is proposed. We test our
methodology on 3 challenging datasets and demonstrate its ability to bridge the
performance gap between paired and unpaired models.",None,-1
Make a Choice! Knowledge Base Question Answering with In-Context Learning,0.619981,"Question answering over knowledge bases (KBQA) aims to answer factoid
questions with a given knowledge base (KB). Due to the large scale of KB,
annotated data is impossible to cover all fact schemas in KB, which poses a
challenge to the generalization ability of methods that require a sufficient
amount of annotated data. Recently, LLMs have shown strong few-shot performance
in many NLP tasks. We expect LLM can help existing methods improve their
generalization ability, especially in low-resource situations. In this paper,
we present McL-KBQA, a framework that incorporates the few-shot ability of LLM
into the KBQA method via ICL-based multiple choice and then improves the
effectiveness of the QA tasks. Experimental results on two KBQA datasets
demonstrate the competitive performance of McL-KBQA with strong improvements in
generalization. We expect to explore a new way to QA tasks from KBQA in
conjunction with LLM, how to generate answers normatively and correctly with
strong generalization.",None,-1
Continual Multimodal Knowledge Graph Construction,0.877479,"Current Multimodal Knowledge Graph Construction (MKGC) models struggle with
the real-world dynamism of continuously emerging entities and relations, often
succumbing to catastrophic forgetting-loss of previously acquired knowledge.
This study introduces benchmarks aimed at fostering the development of the
continual MKGC domain. We further introduce MSPT framework, designed to
surmount the shortcomings of existing MKGC approaches during multimedia data
processing. MSPT harmonizes the retention of learned knowledge (stability) and
the integration of new data (plasticity), outperforming current continual
learning and multimodal methods. Our results confirm MSPT's superior
performance in evolving knowledge environments, showcasing its capacity to
navigate balance between stability and plasticity.",None,-1
RLIF: Interactive Imitation Learning as Reinforcement Learning,0.230822,"Although reinforcement learning methods offer a powerful framework for
automatic skill acquisition, for practical learning-based control problems in
domains such as robotics, imitation learning often provides a more convenient
and accessible alternative. In particular, an interactive imitation learning
method such as DAgger, which queries a near-optimal expert to intervene online
to collect correction data for addressing the distributional shift challenges
that afflict na\""ive behavioral cloning, can enjoy good performance both in
theory and practice without requiring manually specified reward functions and
other components of full reinforcement learning methods. In this paper, we
explore how off-policy reinforcement learning can enable improved performance
under assumptions that are similar but potentially even more practical than
those of interactive imitation learning. Our proposed method uses reinforcement
learning with user intervention signals themselves as rewards. This relaxes the
assumption that intervening experts in interactive imitation learning should be
near-optimal and enables the algorithm to learn behaviors that improve over the
potential suboptimal human expert. We also provide a unified framework to
analyze our RL method and DAgger; for which we present the asymptotic analysis
of the suboptimal gap for both methods as well as the non-asymptotic sample
complexity bound of our method. We then evaluate our method on challenging
high-dimensional continuous control simulation benchmarks as well as real-world
robotic vision-based manipulation tasks. The results show that it strongly
outperforms DAgger-like approaches across the different tasks, especially when
the intervening experts are suboptimal. Code and videos can be found on the
project website: https://rlif-page.github.io",None,-1
Human Pose as Compositional Tokens,0.892381,"Human pose is typically represented by a coordinate vector of body joints or
their heatmap embeddings. While easy for data processing, unrealistic pose
estimates are admitted due to the lack of dependency modeling between the body
joints. In this paper, we present a structured representation, named Pose as
Compositional Tokens (PCT), to explore the joint dependency. It represents a
pose by M discrete tokens with each characterizing a sub-structure with several
interdependent joints. The compositional design enables it to achieve a small
reconstruction error at a low cost. Then we cast pose estimation as a
classification task. In particular, we learn a classifier to predict the
categories of the M tokens from an image. A pre-learned decoder network is used
to recover the pose from the tokens without further post-processing. We show
that it achieves better or comparable pose estimation results as the existing
methods in general scenarios, yet continues to work well when occlusion occurs,
which is ubiquitous in practice. The code and models are publicly available at
https://github.com/Gengzigang/PCT.",None,-1
Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network,0.785543,"Building extraction aims to segment building pixels from remote sensing
images and plays an essential role in many applications, such as city planning
and urban dynamic monitoring. Over the past few years, deep learning methods
with encoder-decoder architectures have achieved remarkable performance due to
their powerful feature representation capability. Nevertheless, due to the
varying scales and styles of buildings, conventional deep learning models
always suffer from uncertain predictions and cannot accurately distinguish the
complete footprints of the building from the complex distribution of ground
objects, leading to a large degree of omission and commission. In this paper,
we realize the importance of uncertain prediction and propose a novel and
straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To
verify the performance of our proposed UANet, we conduct extensive experiments
on three public building datasets, including the WHU building dataset, the
Massachusetts building dataset, and the Inria aerial image dataset. Results
demonstrate that the proposed UANet outperforms other state-of-the-art
algorithms by a large margin.",None,-1
Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search,0.733832,"Text-based Person Search (TPS), is targeted on retrieving pedestrians to
match text descriptions instead of query images. Recent Vision-Language
Pre-training (VLP) models can bring transferable knowledge to downstream TPS
tasks, resulting in more efficient performance gains. However, existing TPS
methods improved by VLP only utilize pre-trained visual encoders, neglecting
the corresponding textual representation and breaking the significant modality
alignment learned from large-scale pre-training. In this paper, we explore the
full utilization of textual potential from VLP in TPS tasks. We build on the
proposed VLP-TPS baseline model, which is the first TPS model with both
pre-trained modalities. We propose the Multi-Integrity Description Constraints
(MIDC) to enhance the robustness of the textual modality by incorporating
different components of fine-grained corpus during training. Inspired by the
prompt approach for zero-shot classification with VLP models, we propose the
Dynamic Attribute Prompt (DAP) to provide a unified corpus of fine-grained
attributes as language hints for the image modality. Extensive experiments show
that our proposed TPS framework achieves state-of-the-art performance,
exceeding the previous best method by a margin.",None,-1
Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation,0.857866,"Language models have achieved impressive performances on dialogue generation
tasks. However, when generating responses for a conversation that requires
factual knowledge, they are far from perfect, due to an absence of mechanisms
to retrieve, encode, and reflect the knowledge in the generated responses. Some
knowledge-grounded dialogue generation methods tackle this problem by
leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee
that the model utilizes a relevant piece of knowledge from the KG. To overcome
this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a
framework for generating context-relevant and knowledge-grounded dialogues with
the KG. Specifically, our SURGE framework first retrieves the relevant subgraph
from the KG, and then enforces consistency across facts by perturbing their
word embeddings conditioned by the retrieved subgraph. Then, we utilize
contrastive learning to ensure that the generated texts have high similarity to
the retrieved subgraphs. We validate our SURGE framework on OpendialKG and
KOMODIS datasets, showing that it generates high-quality dialogues that
faithfully reflect the knowledge from KG.",None,-1
Learning Occupancy for Monocular 3D Object Detection,0.708842,"Monocular 3D detection is a challenging task due to the lack of accurate 3D
information. Existing approaches typically rely on geometry constraints and
dense depth estimates to facilitate the learning, but often fail to fully
exploit the benefits of three-dimensional feature extraction in frustum and 3D
space. In this paper, we propose \textbf{OccupancyM3D}, a method of learning
occupancy for monocular 3D detection. It directly learns occupancy in frustum
and 3D space, leading to more discriminative and informative 3D features and
representations. Specifically, by using synchronized raw sparse LiDAR point
clouds, we define the space status and generate voxel-based occupancy labels.
We formulate occupancy prediction as a simple classification problem and design
associated occupancy losses. Resulting occupancy estimates are employed to
enhance original frustum/3D features. As a result, experiments on KITTI and
Waymo open datasets demonstrate that the proposed method achieves a new state
of the art and surpasses other methods by a significant margin. Codes and
pre-trained models will be available at
\url{https://github.com/SPengLiang/OccupancyM3D}.",None,-1
Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-Spectral Image Fusion,0.166634,"The success of deep neural networks for pan-sharpening is commonly in a form
of black box, lacking transparency and interpretability. To alleviate this
issue, we propose a novel model-driven deep unfolding framework with image
reasoning prior tailored for the pan-sharpening task. Different from existing
unfolding solutions that deliver the proximal operator networks as the
uncertain and vague priors, our framework is motivated by the content reasoning
ability of masked autoencoders (MAE) with insightful designs. Specifically, the
pre-trained MAE with spatial masking strategy, acting as intrinsic reasoning
prior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAE
with spatial-spectral masking strategy is treated as the regularization term
within loss function to constrain the spatial-spectral consistency. Such
designs penetrate the image reasoning prior into deep unfolding networks while
improving its interpretability and representation capability. The uniqueness of
our framework is that the holistic learning process is explicitly integrated
with the inherent physical mechanism underlying the pan-sharpening task.
Extensive experiments on multiple satellite datasets demonstrate the
superiority of our method over the existing state-of-the-art approaches. Code
will be released at \url{https://manman1995.github.io/}.",None,-1
Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop,0.525956,"This paper presents our project proposal for extracting biomedical
information from German clinical narratives with limited amounts of
annotations. We first describe the applied strategies in transfer learning and
active learning for solving our problem. After that, we discuss the design of
the user interface for both supplying model inspection and obtaining user
annotations in the interactive environment.",None,-1
Skip-Attention: Improving Vision Transformers by Paying Less Attention,0.21812,"This work aims to improve the efficiency of vision transformers (ViT). While
ViTs use computationally expensive self-attention operations in every layer, we
identify that these operations are highly correlated across layers -- a key
redundancy that causes unnecessary computations. Based on this observation, we
propose SkipAt, a method to reuse self-attention computation from preceding
layers to approximate attention at one or more subsequent layers. To ensure
that reusing self-attention blocks across layers does not degrade the
performance, we introduce a simple parametric function, which outperforms the
baseline transformer's performance while running computationally faster. We
show the effectiveness of our method in image classification and
self-supervised learning on ImageNet-1K, semantic segmentation on ADE20K, image
denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput
at the same-or-higher accuracy levels in all these tasks.",None,-1
AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial,0.0966447,"Art curatorial practice is characterized by the presentation of an art
collection in a knowledgeable way. Machine processes are characterized by their
capacity to manage and analyze large amounts of data. This paper envisages AI
curation and audience interaction to explore the implications of contemporary
machine learning models for the curatorial world. This project was developed
for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May
Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city
of Helsinki through the lens of machine perception. We use visual-textual
models to place indoor artworks in public spaces, assigning fictional
coordinates based on similarity scores. We transform the space that each
artwork inhabits in the city by generating synthetic 360 art panoramas. We
guide the generation estimating depth values from 360 panoramas at each artwork
location, and machine-generated prompts of the artworks. The result of this
project is an AI curation that places the artworks in their imagined physical
space, blurring the lines of artwork, context, and machine perception. The work
is virtually presented as a web-based installation on this link
http://newlyformedcity.net/, where users can navigate an alternative version of
the city while exploring and interacting with its cultural heritage at scale.",None,-1
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,0.768886,"Recently, deep learning models have shown the potential to predict breast
cancer risk and enable targeted screening strategies, but current models do not
consider the change in the breast over time. In this paper, we present a new
method, PRIME+, for breast cancer risk prediction that leverages prior
mammograms using a transformer decoder, outperforming a state-of-the-art risk
prediction method that only uses mammograms from a single time point. We
validate our approach on a dataset with 16,113 exams and further demonstrate
that it effectively captures patterns of changes from prior mammograms, such as
changes in breast density, resulting in improved short-term and long-term
breast cancer risk prediction. Experimental results show that our model
achieves a statistically significant improvement in performance over the
state-of-the-art based model, with a C-index increase from 0.68 to 0.73 (p <
0.05) on held-out test sets.",None,-1
Weakly Supervised Reasoning by Neuro-Symbolic Approaches,0.623636,"Deep learning has largely improved the performance of various natural
language processing (NLP) tasks. However, most deep learning models are
black-box machinery, and lack explicit interpretation. In this chapter, we will
introduce our recent progress on neuro-symbolic approaches to NLP, which
combines different schools of AI, namely, symbolism and connectionism.
Generally, we will design a neural system with symbolic latent structures for
an NLP task, and apply reinforcement learning or its relaxation to perform
weakly supervised reasoning in the downstream task. Our framework has been
successfully applied to various tasks, including table query reasoning,
syntactic structure reasoning, information extraction reasoning, and rule
reasoning. For each application, we will introduce the background, our
approach, and experimental results.",None,-1
Evaluation of Induced Expert Knowledge in Causal Structure Learning by NOTEARS,0.438177,"Causal modeling provides us with powerful counterfactual reasoning and
interventional mechanism to generate predictions and reason under various
what-if scenarios. However, causal discovery using observation data remains a
nontrivial task due to unobserved confounding factors, finite sampling, and
changes in the data distribution. These can lead to spurious cause-effect
relationships. To mitigate these challenges in practice, researchers augment
causal learning with known causal relations. The goal of the paper is to study
the impact of expert knowledge on causal relations in the form of additional
constraints used in the formulation of the nonparametric NOTEARS. We provide a
comprehensive set of comparative analyses of biasing the model using different
types of knowledge. We found that (i) knowledge that corrects the mistakes of
the NOTEARS model can lead to statistically significant improvements, (ii)
constraints on active edges have a larger positive impact on causal discovery
than inactive edges, and surprisingly, (iii) the induced knowledge does not
correct on average more incorrect active and/or inactive edges than expected.
We also demonstrate the behavior of the model and the effectiveness of domain
knowledge on a real-world dataset.",None,-1
Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication,0.311715,"Over the past decade explainable artificial intelligence has evolved from a
predominantly technical discipline into a field that is deeply intertwined with
social sciences. Insights such as human preference for contrastive -- more
precisely, counterfactual -- explanations have played a major role in this
transition, inspiring and guiding the research in computer science. Other
observations, while equally important, have received much less attention. The
desire of human explainees to communicate with artificial intelligence
explainers through a dialogue-like interaction has been mostly neglected by the
community. This poses many challenges for the effectiveness and widespread
adoption of such technologies as delivering a single explanation optimised
according to some predefined objectives may fail to engender understanding in
its recipients and satisfy their unique needs given the diversity of human
knowledge and intention. Using insights elaborated by Niklas Luhmann and, more
recently, Elena Esposito we apply social systems theory to highlight challenges
in explainable artificial intelligence and offer a path forward, striving to
reinvigorate the technical research in this direction. This paper aims to
demonstrate the potential of systems theoretical approaches to communication in
understanding problems and limitations of explainable artificial intelligence.",None,-1
Self-supervised dense representation learning for live-cell microscopy with time arrow prediction,0.794942,"State-of-the-art object detection and segmentation methods for microscopy
images rely on supervised machine learning, which requires laborious manual
annotation of training data. Here we present a self-supervised method based on
time arrow prediction pre-training that learns dense image representations from
raw, unlabeled live-cell microscopy videos. Our method builds upon the task of
predicting the correct order of time-flipped image regions via a single-image
feature extractor followed by a time arrow prediction head that operates on the
fused features. We show that the resulting dense representations capture
inherently time-asymmetric biological processes such as cell divisions on a
pixel-level. We furthermore demonstrate the utility of these representations on
several live-cell microscopy datasets for detection and segmentation of
dividing cells, as well as for cell state classification. Our method
outperforms supervised methods, particularly when only limited ground truth
annotations are available as is commonly the case in practice. We provide code
at https://github.com/weigertlab/tarrow.",None,-1
A Template Is All You Meme,0.654088,"Memes are a modern form of communication and meme templates possess a base
semantics that is customizable by whomever posts it on social media. Machine
learning systems struggle with memes, which is likely due to such systems
having insufficient context to understand memes, as there is more to memes than
the obvious image and text. Here, to aid understanding of memes, we release a
knowledge base of memes and information found on www.knowyourmeme.com, which we
call the Know Your Meme Knowledge Base (KYMKB), composed of more than 54,000
images. The KYMKB includes popular meme templates, examples of each template,
and detailed information about the template. We hypothesize that meme templates
can be used to inject models with the context missing from previous approaches.
To test our hypothesis, we create a non-parametric majority-based classifier,
which we call Template-Label Counter (TLC). We find TLC more effective than or
competitive with fine-tuned baselines. To demonstrate the power of meme
templates and the value of both our knowledge base and method, we conduct
thorough classification experiments and exploratory data analysis in the
context of five meme analysis tasks.",None,-1
Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models,0.218649,"Emergency management urgently requires comprehensive knowledge while having a
high possibility to go beyond individuals' cognitive scope. Therefore,
artificial intelligence(AI) supported decision-making under that circumstance
is of vital importance. Recent emerging large language models (LLM) provide a
new direction for enhancing targeted machine intelligence. However, the
utilization of LLM directly would inevitably introduce unreliable output for
its inherent issue of hallucination and poor reasoning skills. In this work, we
develop a system called Enhancing Emergency decision-making with Knowledge
Graph and LLM (E-KELL), which provides evidence-based decision-making in
various emergency stages. The study constructs a structured emergency knowledge
graph and guides LLMs to reason over it via a prompt chain. In real-world
evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in
comprehensibility, accuracy, conciseness, and instructiveness from a group of
emergency commanders and firefighters, demonstrating a significant improvement
across various situations compared to baseline models. This work introduces a
novel approach to providing reliable emergency decision support.",None,-1
Med-EASi: Finely Annotated Dataset and Models for Controllable Simplification of Medical Texts,0.617886,"Automatic medical text simplification can assist providers with
patient-friendly communication and make medical texts more accessible, thereby
improving health literacy. But curating a quality corpus for this task requires
the supervision of medical experts. In this work, we present
$\textbf{Med-EASi}$ ($\underline{\textbf{Med}}$ical dataset for
$\underline{\textbf{E}}$laborative and $\underline{\textbf{A}}$bstractive
$\underline{\textbf{Si}}$mplification), a uniquely crowdsourced and finely
annotated dataset for supervised simplification of short medical texts. Its
$\textit{expert-layman-AI collaborative}$ annotations facilitate
$\textit{controllability}$ over text simplification by marking four kinds of
textual transformations: elaboration, replacement, deletion, and insertion. To
learn medical text simplification, we fine-tune T5-large with four different
styles of input-output combinations, leading to two control-free and two
controllable versions of the model. We add two types of
$\textit{controllability}$ into text simplification, by using a multi-angle
training approach: $\textit{position-aware}$, which uses in-place annotated
inputs and outputs, and $\textit{position-agnostic}$, where the model only
knows the contents to be edited, but not their positions. Our results show that
our fine-grained annotations improve learning compared to the unannotated
baseline. Furthermore, $\textit{position-aware}$ control generates better
simplification than the $\textit{position-agnostic}$ one. The data and code are
available at https://github.com/Chandrayee/CTRL-SIMP.",None,-1
Image Completion via Dual-path Cooperative Filtering,0.298582,"Given the recent advances with image-generating algorithms, deep image
completion methods have made significant progress. However, state-of-art
methods typically provide poor cross-scene generalization, and generated masked
areas often contain blurry artifacts. Predictive filtering is a method for
restoring images, which predicts the most effective kernels based on the input
scene. Motivated by this approach, we address image completion as a filtering
problem. Deep feature-level semantic filtering is introduced to fill in missing
information, while preserving local structure and generating visually realistic
content. In particular, a Dual-path Cooperative Filtering (DCF) model is
proposed, where one path predicts dynamic kernels, and the other path extracts
multi-level features by using Fast Fourier Convolution to yield semantically
coherent reconstructions. Experiments on three challenging image completion
datasets show that our proposed DCF outperforms state-of-art methods.",None,-1
Object Preserving Siamese Network for Single Object Tracking on Point Clouds,0.0946944,"Obviously, the object is the key factor of the 3D single object tracking
(SOT) task. However, previous Siamese-based trackers overlook the negative
effects brought by randomly dropped object points during backbone sampling,
which hinder trackers to predict accurate bounding boxes (BBoxes). Exploring an
approach that seeks to maximize the preservation of object points and their
object-aware features is of particular significance. Motivated by this, we
propose an Object Preserving Siamese Network
  (OPSNet), which can significantly maintain object integrity and boost
tracking performance. Firstly, the object highlighting module enhances the
object-aware features and extracts discriminative features from template and
search area. Then, the object-preserved sampling selects object candidates to
obtain object-preserved search area seeds and drop the background points that
contribute less to tracking. Finally, the object localization network precisely
locates 3D BBoxes based on the object-preserved search area seeds. Extensive
experiments demonstrate our method outperforms the state-of-the-art performance
(9.4% and 2.5% success gain on KITTI and Waymo Open Dataset respectively).",None,-1
Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,0.0582988,"Precise localization is critical for autonomous vehicles. We present a
self-supervised learning method that employs Transformers for the first time
for the task of outdoor localization using LiDAR data. We propose a pre-text
task that reorganizes the slices of a $360^\circ$ LiDAR scan to leverage its
axial properties. Our model, called Slice Transformer, employs multi-head
attention while systematically processing the slices. To the best of our
knowledge, this is the first instance of leveraging multi-head attention for
outdoor point clouds. We additionally introduce the Perth-WA dataset, which
provides a large-scale LiDAR map of Perth city in Western Australia, covering
$\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The
proposed localization method is thoroughly evaluated on Perth-WA and
Appollo-SouthBay datasets. We also establish the efficacy of our
self-supervised learning approach for the common downstream task of object
classification using ModelNet40 and ScanNN datasets. The code and Perth-WA data
will be publicly released.",None,-1
MultiMediate'23: Engagement Estimation and Bodily Behaviour Recognition in Social Interactions,0.901612,"Automatic analysis of human behaviour is a fundamental prerequisite for the
creation of machines that can effectively interact with- and support humans in
social interactions. In MultiMediate'23, we address two key human social
behaviour analysis tasks for the first time in a controlled challenge:
engagement estimation and bodily behaviour recognition in social interactions.
This paper describes the MultiMediate'23 challenge and presents novel sets of
annotations for both tasks. For engagement estimation we collected novel
annotations on the NOvice eXpert Interaction (NOXI) database. For bodily
behaviour recognition, we annotated test recordings of the MPIIGroupInteraction
corpus with the BBSI annotation scheme. In addition, we present baseline
results for both challenge tasks.",None,-1
Exploring Paracrawl for Document-level Neural Machine Translation,0.349132,"Document-level neural machine translation (NMT) has outperformed
sentence-level NMT on a number of datasets. However, document-level NMT is
still not widely adopted in real-world translation systems mainly due to the
lack of large-scale general-domain training data for document-level NMT. We
examine the effectiveness of using Paracrawl for learning document-level
translation. Paracrawl is a large-scale parallel corpus crawled from the
Internet and contains data from various domains. The official Paracrawl corpus
was released as parallel sentences (extracted from parallel webpages) and
therefore previous works only used Paracrawl for learning sentence-level
translation. In this work, we extract parallel paragraphs from Paracrawl
parallel webpages using automatic sentence alignments and we use the extracted
parallel paragraphs as parallel documents for training document-level
translation models. We show that document-level NMT models trained with only
parallel paragraphs from Paracrawl can be used to translate real documents from
TED, News and Europarl, outperforming sentence-level NMT models. We also
perform a targeted pronoun evaluation and show that document-level models
trained with Paracrawl data can help context-aware pronoun translation.",None,-1
Low-Rank Adaptation for Multilingual Summarization: An Empirical Study,0.0680047,"Although the advancements of pre-trained Large Language Models have
significantly accelerated recent progress in NLP, their ever-increasing size
poses significant challenges for conventional fine-tuning, especially in
memory-intensive tasks. We investigate the potential of Parameter-Efficient
Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of
multilingual summarization, a task that is both challenging (due to typically
long inputs), and relatively unexplored. We conduct an extensive study across
different data availability scenarios, including high- and low-data settings,
and cross-lingual transfer, leveraging models of different sizes. Our findings
reveal that LoRA is competitive with full fine-tuning when trained with high
quantities of data, and excels in low-data scenarios and cross-lingual
transfer. We also study different strategies for few-shot cross-lingual
transfer, finding that continued LoRA tuning outperforms full fine-tuning and
the dynamic composition of language-specific LoRA modules.",None,-1
PAMS: Platform for Artificial Market Simulations,0.208747,"This paper presents a new artificial market simulation platform, PAMS:
Platform for Artificial Market Simulations. PAMS is developed as a Python-based
simulator that is easily integrated with deep learning and enabling various
simulation that requires easy users' modification. In this paper, we
demonstrate PAMS effectiveness through a study using agents predicting future
prices by deep learning.",None,-1
Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints,0.522158,"In safety-critical robotic tasks, potential failures must be reduced, and
multiple constraints must be met, such as avoiding collisions, limiting energy
consumption, and maintaining balance. Thus, applying safe reinforcement
learning (RL) in such robotic tasks requires to handle multiple constraints and
use risk-averse constraints rather than risk-neutral constraints. To this end,
we propose a trust region-based safe RL algorithm for multiple constraints
called a safe distributional actor-critic (SDAC). Our main contributions are as
follows: 1) introducing a gradient integration method to manage infeasibility
issues in multi-constrained problems, ensuring theoretical convergence, and 2)
developing a TD($\lambda$) target distribution to estimate risk-averse
constraints with low biases. We evaluate SDAC through extensive experiments
involving multi- and single-constrained robotic tasks. While maintaining high
scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in
multi-constrained tasks and 1.78 times fewer constraint violations in
single-constrained tasks compared to safe RL baselines. Code is available at:
https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.",None,-1
The Secret of Metaphor on Expressing Stronger Emotion,0.864665,"Metaphors are proven to have stronger emotional impact than literal
expressions. Although this conclusion is shown to be promising in benefiting
various NLP applications, the reasons behind this phenomenon are not well
studied. This paper conducts the first study in exploring how metaphors convey
stronger emotion than their literal counterparts. We find that metaphors are
generally more specific than literal expressions. The more specific property of
metaphor can be one of the reasons for metaphors' superiority in emotion
expression. When we compare metaphors with literal expressions with the same
specificity level, the gap of emotion expressing ability between both reduces
significantly. In addition, we observe specificity is crucial in literal
language as well, as literal language can express stronger emotion by making it
more specific.",None,-1
Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities,0.427374,"State-of-the-art Text-To-Speech (TTS) models are capable of producing
high-quality speech. The generated speech, however, is usually neutral in
emotional expression, whereas very often one would want fine-grained emotional
control of words or phonemes. Although still challenging, the first TTS models
have been recently proposed that are able to control voice by manually
assigning emotion intensity. Unfortunately, due to the neglect of intra-class
distance, the intensity differences are often unrecognizable. In this paper, we
propose a fine-grained controllable emotional TTS, that considers both inter-
and intra-class distances and be able to synthesize speech with recognizable
intensity difference. Our subjective and objective experiments demonstrate that
our model exceeds two state-of-the-art controllable TTS models for
controllability, emotion expressiveness and naturalness.",None,-1
TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective,0.649053,"Vision Transformers (ViTs) have demonstrated powerful representation ability
in various visual tasks thanks to their intrinsic data-hungry nature. However,
we unexpectedly find that ViTs perform vulnerably when applied to face
recognition (FR) scenarios with extremely large datasets. We investigate the
reasons for this phenomenon and discover that the existing data augmentation
approach and hard sample mining strategy are incompatible with ViTs-based FR
backbone due to the lack of tailored consideration on preserving face
structural information and leveraging each local token information. To remedy
these problems, this paper proposes a superior FR model called TransFace, which
employs a patch-level data augmentation strategy named DPAP and a hard sample
mining strategy named EHSM. Specially, DPAP randomly perturbs the amplitude
information of dominant patches to expand sample diversity, which effectively
alleviates the overfitting problem in ViTs. EHSM utilizes the information
entropy in the local tokens to dynamically adjust the importance weight of easy
and hard samples during training, leading to a more stable prediction.
Experiments on several benchmarks demonstrate the superiority of our TransFace.
Code and models are available at https://github.com/DanJun6737/TransFace.",None,-1
The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps,0.467818,"Scanned historical maps in libraries and archives are valuable repositories
of geographic data that often do not exist elsewhere. Despite the potential of
machine learning tools like the Google Vision APIs for automatically
transcribing text from these maps into machine-readable formats, they do not
work well with large-sized images (e.g., high-resolution scanned documents),
cannot infer the relation between the recognized text and other datasets, and
are challenging to integrate with post-processing tools. This paper introduces
the mapKurator system, an end-to-end system integrating machine learning models
with a comprehensive data processing pipeline. mapKurator empowers automated
extraction, post-processing, and linkage of text labels from large numbers of
large-dimension historical map scans. The output data, comprising bounding
polygons and recognized text, is in the standard GeoJSON format, making it
easily modifiable within Geographic Information Systems (GIS). The proposed
system allows users to quickly generate valuable data from large numbers of
historical maps for in-depth analysis of the map content and, in turn,
encourages map findability, accessibility, interoperability, and reusability
(FAIR principles). We deployed the mapKurator system and enabled the processing
of over 60,000 maps and over 100 million text/place names in the David Rumsey
Historical Map collection. We also demonstrated a seamless integration of
mapKurator with a collaborative web platform to enable accessing automated
approaches for extracting and linking text labels from historical map scans and
collective work to improve the results.",None,-1
Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification,0.565402,"This article aims to classify diabetic retinopathy (DR) disease into five
different classes using an ensemble approach based on two popular pre-trained
convolutional neural networks: VGG16 and Inception V3. The proposed model aims
to leverage the strengths of the two individual nets to enhance the
classification performance for diabetic retinopathy. The ensemble model
architecture involves freezing a portion of the layers in each pre-trained
model to utilize their learned representations effectively. Global average
pooling layers are added to transform the output feature maps into fixed-length
vectors. These vectors are then concatenated to form a consolidated
representation of the input image. The ensemble model is trained using a
dataset of diabetic retinopathy images (APTOS), divided into training and
validation sets. During the training process, the model learns to classify the
retinal images into the corresponding diabetic retinopathy classes.
Experimental results on the test set demonstrate the efficacy of the proposed
ensemble model for DR classification achieving an accuracy of 96.4%.",None,-1
"The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising ""Alignment"" in Large Language Models",0.0217728,"In this paper, we address the concept of ""alignment"" in large language models
(LLMs) through the lens of post-structuralist socio-political theory,
specifically examining its parallels to empty signifiers. To establish a shared
vocabulary around how abstract concepts of alignment are operationalised in
empirical datasets, we propose a framework that demarcates: 1) which dimensions
of model behaviour are considered important, then 2) how meanings and
definitions are ascribed to these dimensions, and by whom. We situate existing
empirical literature and provide guidance on deciding which paradigm to follow.
Through this framework, we aim to foster a culture of transparency and critical
evaluation, aiding the community in navigating the complexities of aligning
LLMs with human populations.",None,-1
Duplex Diffusion Models Improve Speech-to-Speech Translation,0.589174,"Speech-to-speech translation is a typical sequence-to-sequence learning task
that naturally has two directions. How to effectively leverage bidirectional
supervision signals to produce high-fidelity audio for both directions?
Existing approaches either train two separate models or a multitask-learned
model with low efficiency and inferior performance. In this paper, we propose a
duplex diffusion model that applies diffusion probabilistic models to both
sides of a reversible duplex Conformer, so that either end can simultaneously
input and output a distinct language's speech. Our model enables reversible
speech translation by simply flipping the input and output ends. Experiments
show that our model achieves the first success of reversible speech translation
with significant improvements of ASR-BLEU scores compared with a list of
state-of-the-art baselines.",None,-1
Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI,0.121113,"Large Language Models (LLMs) are capable of reasoning over diverse input data
modalities through pre-trained encoders. However, the growing diversity of
input data modalities prevents incorporating all modalities into LLMs,
especially when LLMs are deployed on resource-constrained edge devices for
embodied AI applications. Instead, a better option is to adaptively involve
only the useful modalities at runtime, depending on the current environmental
contexts and task requirements. For such modality adaptation, existing work
adopts fixed connections between encoders and the LLM's input layer, leading to
high training cost at runtime and ineffective cross-modal interaction. In this
paper, we address these limitations by presenting mPnP-LLM, a new technique
that allows fully elastic, automated and prompt runtime modality adaptation, by
connecting unimodal encoders to a flexible set of last LLM blocks and making
such latent connections fully trainable at runtime. Experiments over the
nuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction
and 30% GPU memory usage reduction, while retaining on-par accuracy with the
existing schemes. Under the same compute budget, mPnP-LLM improves the task
accuracy by up to 4% compared to the best existing scheme.",None,-1
Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean,0.0370753,"Accurate and up-to-date information on building characteristics is essential
for vulnerability assessment; however, the high costs and long timeframes
associated with conducting traditional field surveys can be an obstacle to
obtaining critical exposure datasets needed for disaster risk management. In
this work, we leverage deep learning techniques for the automated
classification of roof characteristics from very high-resolution orthophotos
and airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.
We demonstrate that the fusion of multimodal earth observation data performs
better than using any single data source alone. Using our proposed methods, we
achieve F1 scores of 0.93 and 0.92 for roof type and roof material
classification, respectively. This work is intended to help governments produce
more timely building information to improve resilience and disaster response in
the Caribbean.",None,-1
Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness,0.573949,"Learning from raw high dimensional data via interaction with a given
environment has been effectively achieved through the utilization of deep
neural networks. Yet the observed degradation in policy performance caused by
imperceptible worst-case policy dependent translations along high sensitivity
directions (i.e. adversarial perturbations) raises concerns on the robustness
of deep reinforcement learning policies. In our paper, we show that these high
sensitivity directions do not lie only along particular worst-case directions,
but rather are more abundant in the deep neural policy landscape and can be
found via more natural means in a black-box setting. Furthermore, we show that
vanilla training techniques intriguingly result in learning more robust
policies compared to the policies learnt via the state-of-the-art adversarial
training techniques. We believe our work lays out intriguing properties of the
deep reinforcement learning policy manifold and our results can help to build
robust and generalizable deep reinforcement learning policies.",None,-1
Instruction Mining: When Data Mining Meets Large Language Model Finetuning,0.222347,"Large language models (LLMs) are initially pretrained for broad capabilities
and then finetuned with instruction-following datasets to improve their
performance in interacting with humans. Despite advances in finetuning, a
standardized guideline for selecting high-quality datasets to optimize this
process remains elusive. In this paper, we first propose InstructMining, an
innovative method designed for automatically selecting premium
instruction-following data for finetuning LLMs. Specifically, InstructMining
utilizes natural language indicators as a measure of data quality, applying
them to evaluate unseen datasets. During experimentation, we discover that
double descent phenomenon exists in large language model finetuning. Based on
this observation, we further leverage BlendSearch to help find the best subset
among the entire dataset (i.e., 2,532 out of 100,000). Experiment results show
that InstructMining-7B achieves state-of-the-art performance on two of the most
popular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.",None,-1
NeRD: Neural field-based Demosaicking,0.359782,"We introduce NeRD, a new demosaicking method for generating full-color images
from Bayer patterns. Our approach leverages advancements in neural fields to
perform demosaicking by representing an image as a coordinate-based neural
network with sine activation functions. The inputs to the network are spatial
coordinates and a low-resolution Bayer pattern, while the outputs are the
corresponding RGB values. An encoder network, which is a blend of ResNet and
U-net, enhances the implicit neural representation of the image to improve its
quality and ensure spatial consistency through prior learning. Our experimental
results demonstrate that NeRD outperforms traditional and state-of-the-art
CNN-based methods and significantly closes the gap to transformer-based
methods.",None,-1
SPRING: Studying the Paper and Reasoning to Play Games,0.922154,"Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent's current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM's answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
""reasoning"" induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.",None,-1
Meta-ZSDETR: Zero-shot DETR with Meta-learning,0.171536,"Zero-shot object detection aims to localize and recognize objects of unseen
classes. Most of existing works face two problems: the low recall of RPN in
unseen classes and the confusion of unseen classes with background. In this
paper, we present the first method that combines DETR and meta-learning to
perform zero-shot object detection, named Meta-ZSDETR, where model training is
formalized as an individual episode based meta-learning task. Different from
Faster R-CNN based methods that firstly generate class-agnostic proposals, and
then classify them with visual-semantic alignment module, Meta-ZSDETR directly
predict class-specific boxes with class-specific queries and further filter
them with the predicted accuracy from classification head. The model is
optimized with meta-contrastive learning, which contains a regression head to
generate the coordinates of class-specific boxes, a classification head to
predict the accuracy of generated boxes, and a contrastive head that utilizes
the proposed contrastive-reconstruction loss to further separate different
classes in visual space. We conduct extensive experiments on two benchmark
datasets MS COCO and PASCAL VOC. Experimental results show that our method
outperforms the existing ZSD methods by a large margin.",None,-1
The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector,0.811518,"Deep learning-based edge detectors heavily rely on pixel-wise labels which
are often provided by multiple annotators. Existing methods fuse multiple
annotations using a simple voting process, ignoring the inherent ambiguity of
edges and labeling bias of annotators. In this paper, we propose a novel
uncertainty-aware edge detector (UAED), which employs uncertainty to
investigate the subjectivity and ambiguity of diverse annotations.
Specifically, we first convert the deterministic label space into a learnable
Gaussian distribution, whose variance measures the degree of ambiguity among
different annotations. Then we regard the learned variance as the estimated
uncertainty of the predicted edge maps, and pixels with higher uncertainty are
likely to be hard samples for edge detection. Therefore we design an adaptive
weighting loss to emphasize the learning from those pixels with high
uncertainty, which helps the network to gradually concentrate on the important
pixels. UAED can be combined with various encoder-decoder backbones, and the
extensive experiments demonstrate that UAED achieves superior performance
consistently across multiple edge detection benchmarks. The source code is
available at \url{https://github.com/ZhouCX117/UAED}",None,-1
TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks,0.195881,"Activation functions are essential components of neural networks. In this
paper, we introduce a new activation function called the Truncated and Signed
Square Root (TSSR) function. This function is distinctive because it is odd,
nonlinear, monotone and differentiable. Its gradient is continuous and always
positive. Thanks to these properties, it has the potential to improve the
numerical stability of neural networks. Several experiments confirm that the
proposed TSSR has better performance than other stat-of-the-art activation
functions. The proposed function has significant implications for the
development of neural network models and can be applied to a wide range of
applications in fields such as computer vision, natural language processing,
and speech recognition.",None,-1
DyLiN: Making Light Field Networks Dynamic,0.924527,"Light Field Networks, the re-formulations of radiance fields to oriented
rays, are magnitudes faster than their coordinate network counterparts, and
provide higher fidelity with respect to representing 3D structures from 2D
observations. They would be well suited for generic scene representation and
manipulation, but suffer from one problem: they are limited to holistic and
static scenes. In this paper, we propose the Dynamic Light Field Network
(DyLiN) method that can handle non-rigid deformations, including topological
changes. We learn a deformation field from input rays to canonical rays, and
lift them into a higher dimensional space to handle discontinuities. We further
introduce CoDyLiN, which augments DyLiN with controllable attribute inputs. We
train both models via knowledge distillation from pretrained dynamic radiance
fields. We evaluated DyLiN using both synthetic and real world datasets that
include various non-rigid deformations. DyLiN qualitatively outperformed and
quantitatively matched state-of-the-art methods in terms of visual fidelity,
while being 25 - 71x computationally faster. We also tested CoDyLiN on
attribute annotated data and it surpassed its teacher model. Project page:
https://dylin2023.github.io .",None,-1
Analyzing Font Style Usage and Contextual Factors in Real Images,0.699819,"There are various font styles in the world. Different styles give different
impressions and readability. This paper analyzes the relationship between font
styles and contextual factors that might affect font style selection with
large-scale datasets. For example, we will analyze the relationship between
font style and its surrounding object (such as ``bus'') by using about 800,000
words in the Open Images dataset. We also use a book cover dataset to analyze
the relationship between font styles with book genres. Moreover, the meaning of
the word is assumed as another contextual factor. For these numeric analyses,
we utilize our own font-style feature extraction model and word2vec. As a
result of co-occurrence-based relationship analysis, we found several instances
of specific font styles being used for specific contextual factors.",None,-1
Leveraging Cross-Utterance Context For ASR Decoding,0.121407,"While external language models (LMs) are often incorporated into the decoding
stage of automated speech recognition systems, these models usually operate
with limited context. Cross utterance information has been shown to be
beneficial during second pass re-scoring, however this limits the hypothesis
space based on the local information available to the first pass LM. In this
work, we investigate the incorporation of long-context transformer LMs for
cross-utterance decoding of acoustic models via beam search, and compare
against results from n-best rescoring. Results demonstrate that beam search
allows for an improved use of cross-utterance context. When evaluating on the
long-format dataset AMI, results show a 0.7\% and 0.3\% absolute reduction on
dev and test sets compared to the single-utterance setting, with improvements
when including up to 500 tokens of prior context. Evaluations are also provided
for Tedlium-1 with less significant improvements of around 0.1\% absolute.",None,-1
Compliance Costs of AI Technology Commercialization: A Field Deployment Perspective,0.14201,"While Artificial Intelligence (AI) technologies are progressing fast,
compliance costs have become a huge financial burden for AI startups, which are
already constrained on research & development budgets. This situation creates a
compliance trap, as many AI startups are not financially prepared to cope with
a broad spectrum of regulatory requirements. Particularly, the complex and
varying regulatory processes across the globe subtly give advantages to
well-established and resourceful technology firms over resource-constrained AI
startups [1]. The continuation of this trend may phase out the majority of AI
startups and lead to giant technology firms' monopolies of AI technologies. To
demonstrate the reality of the compliance trap, from a field deployment
perspective, we delve into the details of compliance costs of AI commercial
operations.",None,-1
Generative AI in Mafia-like Game Simulation,0.0433638,"In this research, we explore the efficacy and potential of Generative AI
models, specifically focusing on their application in role-playing simulations
exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's
advanced capabilities, the study aimed to showcase the model's potential in
understanding, decision-making, and interaction during game scenarios.
Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo,
demonstrated GPT-4's enhanced adaptability to the game environment, with
significant improvements in posing relevant questions and forming human-like
responses. However, challenges such as the model;s limitations in bluffing and
predicting opponent moves emerged. Reflections on game development, financial
constraints, and non-verbal limitations of the study were also discussed. The
findings suggest that while GPT-4 exhibits promising advancements over earlier
models, there remains potential for further development, especially in
instilling more human-like attributes in AI.",None,-1
"Milestones in Autonomous Driving and Intelligent Vehicles Part I: Control, Computing System Design, Communication, HD Map, Testing, and Human Behaviors",0.867843,"Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing
at a rapid pace due to the convenience, safety, and economic benefits. Although
a number of surveys have reviewed research achievements in this field, they are
still limited in specific tasks and lack systematic summaries and research
directions in the future. Our work is divided into 3 independent articles and
the first part is a Survey of Surveys (SoS) for total technologies of AD and
IVs that involves the history, summarizes the milestones, and provides the
perspectives, ethics, and future research directions. This is the second part
(Part I for this technical survey) to review the development of control,
computing system design, communication, High Definition map (HD map), testing,
and human behaviors in IVs. In addition, the third part (Part II for this
technical survey) is to review the perception and planning sections. The
objective of this paper is to involve all the sections of AD, summarize the
latest technical milestones, and guide abecedarians to quickly understand the
development of AD and IVs. Combining the SoS and Part II, we anticipate that
this work will bring novel and diverse insights to researchers and
abecedarians, and serve as a bridge between past and future.",None,-1
ChiroDiff: Modelling chirographic data with Diffusion Models,0.343674,"Generative modelling over continuous-time geometric constructs, a.k.a such as
handwriting, sketches, drawings etc., have been accomplished through
autoregressive distributions. Such strictly-ordered discrete factorization
however falls short of capturing key properties of chirographic data -- it
fails to build holistic understanding of the temporal concept due to one-way
visibility (causality). Consequently, temporal data has been modelled as
discrete token sequences of fixed sampling rate instead of capturing the true
underlying concept. In this paper, we introduce a powerful model-class namely
""Denoising Diffusion Probabilistic Models"" or DDPMs for chirographic data that
specifically addresses these flaws. Our model named ""ChiroDiff"", being
non-autoregressive, learns to capture holistic concepts and therefore remains
resilient to higher temporal sampling rate up to a good extent. Moreover, we
show that many important downstream utilities (e.g. conditional sampling,
creative mixing) can be flexibly implemented using ChiroDiff. We further show
some unique use-cases like stochastic vectorization, de-noising/healing,
abstraction are also possible with this model-class. We perform quantitative
and qualitative evaluation of our framework on relevant datasets and found it
to be better or on par with competing approaches.",None,-1
Language Model Analysis for Ontology Subsumption Inference,0.987982,"Investigating whether pre-trained language models (LMs) can function as
knowledge bases (KBs) has raised wide research interests recently. However,
existing works focus on simple, triple-based, relational KBs, but omit more
sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To
investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of
inference-based probing tasks and datasets from ontology subsumption axioms
involving both atomic and complex concepts. We conduct extensive experiments on
ontologies of different domains and scales, and our results demonstrate that
LMs encode relatively less background knowledge of Subsumption Inference (SI)
than traditional Natural Language Inference (NLI) but can improve on SI
significantly when a small number of samples are given. We will open-source our
code and datasets.",None,-1
Modeling Continuous Motion for 3D Point Cloud Object Tracking,0.243768,"The task of 3D single object tracking (SOT) with LiDAR point clouds is
crucial for various applications, such as autonomous driving and robotics.
However, existing approaches have primarily relied on appearance matching or
motion modeling within only two successive frames, thereby overlooking the
long-range continuous motion property of objects in 3D space. To address this
issue, this paper presents a novel approach that views each tracklet as a
continuous stream: at each timestamp, only the current frame is fed into the
network to interact with multi-frame historical features stored in a memory
bank, enabling efficient exploitation of sequential information. To achieve
effective cross-frame message passing, a hybrid attention mechanism is designed
to account for both long-range relation modeling and local geometric feature
extraction. Furthermore, to enhance the utilization of multi-frame features for
robust tracking, a contrastive sequence enhancement strategy is proposed, which
uses ground truth tracklets to augment training sequences and promote
discrimination against false positives in a contrastive manner. Extensive
experiments demonstrate that the proposed method outperforms the
state-of-the-art method by significant margins on multiple benchmarks.",None,-1
Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models,0.998772,"We present the first framework to solve linear inverse problems leveraging
pre-trained latent diffusion models. Previously proposed algorithms (such as
DPS and DDRM) only apply to pixel-space diffusion models. We theoretically
analyze our algorithm showing provable sample recovery in a linear model
setting. The algorithmic insight obtained from our analysis extends to more
general settings often considered in practice. Experimentally, we outperform
previously proposed posterior sampling algorithms in a wide variety of problems
including random inpainting, block inpainting, denoising, deblurring,
destriping, and super-resolution.",None,-1
Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,0.707349,"Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies as well as BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to perform long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.",None,-1
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement,0.731583,"Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN's potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.",None,-1
Egocentric Auditory Attention Localization in Conversations,0.801655,"In a noisy conversation environment such as a dinner party, people often
exhibit selective auditory attention, or the ability to focus on a particular
speaker while tuning out others. Recognizing who somebody is listening to in a
conversation is essential for developing technologies that can understand
social behavior and devices that can augment human hearing by amplifying
particular sound sources. The computer vision and audio research communities
have made great strides towards recognizing sound sources and speakers in
scenes. In this work, we take a step further by focusing on the problem of
localizing auditory attention targets in egocentric video, or detecting who in
a camera wearer's field of view they are listening to. To tackle the new and
challenging Selective Auditory Attention Localization problem, we propose an
end-to-end deep learning approach that uses egocentric video and multichannel
audio to predict the heatmap of the camera wearer's auditory attention. Our
approach leverages spatiotemporal audiovisual features and holistic reasoning
about the scene to make predictions, and outperforms a set of baselines on a
challenging multi-speaker conversation dataset. Project page:
https://fkryan.github.io/saal",None,-1
Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization,0.452244,"Speaker diarization(SD) is a classic task in speech processing and is crucial
in multi-party scenarios such as meetings and conversations. Current mainstream
speaker diarization approaches consider acoustic information only, which result
in performance degradation when encountering adverse acoustic conditions. In
this paper, we propose methods to extract speaker-related information from
semantic content in multi-party meetings, which, as we will show, can further
benefit speaker diarization. We introduce two sub-tasks, Dialogue Detection and
Speaker-Turn Detection, in which we effectively extract speaker information
from conversational semantics. We also propose a simple yet effective algorithm
to jointly model acoustic and semantic information and obtain
speaker-identified texts. Experiments on both AISHELL-4 and AliMeeting datasets
show that our method achieves consistent improvements over acoustic-only
speaker diarization systems.",None,-1
Behavioral Cloning via Search in Embedded Demonstration Dataset,0.117969,"Behavioural cloning uses a dataset of demonstrations to learn a behavioural
policy. To overcome various learning and policy adaptation problems, we propose
to use latent space to index a demonstration dataset, instantly access similar
relevant experiences, and copy behavior from these situations. Actions from a
selected similar situation can be performed by the agent until representations
of the agent's current situation and the selected experience diverge in the
latent space. Thus, we formulate our control problem as a search problem over a
dataset of experts' demonstrations. We test our approach on BASALT
MineRL-dataset in the latent representation of a Video PreTraining model. We
compare our model to state-of-the-art Minecraft agents. Our approach can
effectively recover meaningful demonstrations and show human-like behavior of
an agent in the Minecraft environment in a wide variety of scenarios.
Experimental results reveal that performance of our search-based approach is
comparable to trained models, while allowing zero-shot task adaptation by
changing the demonstration examples.",None,-1
ZIGNeRF: Zero-shot 3D Scene Representation with Invertible Generative Neural Radiance Fields,0.0486567,"Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable
proficiency in synthesizing multi-view images by learning the distribution of a
set of unposed images. Despite the aptitude of existing generative NeRFs in
generating 3D-consistent high-quality random samples within data distribution,
the creation of a 3D representation of a singular input image remains a
formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative
model that executes zero-shot Generative Adversarial Network (GAN) inversion
for the generation of multi-view images from a single out-of-domain image. The
model is underpinned by a novel inverter that maps out-of-domain images into
the latent code of the generator manifold. Notably, ZIGNeRF is capable of
disentangling the object from the background and executing 3D operations such
as 360-degree rotation or depth and horizontal translation. The efficacy of our
model is validated using multiple real-image datasets: Cats, AFHQ, CelebA,
CelebA-HQ, and CompCars.",None,-1
Collage Diffusion,0.224005,"We seek to give users precise control over diffusion-based image generation
by modeling complex scenes as sequences of layers, which define the desired
spatial arrangement and visual attributes of objects in the scene. Collage
Diffusion harmonizes the input layers to make objects fit together -- the key
challenge involves minimizing changes in the positions and key visual
attributes of the input layers while allowing other attributes to change in the
harmonization process. We ensure that objects are generated in the correct
locations by modifying text-image cross-attention with the layers' alpha masks.
We preserve key visual attributes of input layers by learning specialized text
representations per layer and by extending ControlNet to operate on layers.
Layer input allows users to control the extent of image harmonization on a
per-object basis, and users can even iteratively edit individual objects in
generated images while keeping other objects fixed. By leveraging the rich
information present in layer input, Collage Diffusion generates globally
harmonized images that maintain desired object characteristics better than
prior approaches.",None,-1
Service Reservation and Pricing for Green Metaverses: A Stackelberg Game Approach,0.213652,"Metaverse enables users to communicate, collaborate and socialize with each
other through their digital avatars. Due to the spatio-temporal
characteristics, co-located users are served well by performing their software
components in a collaborative manner such that a Metaverse service provider
(MSP) eliminates redundant data transmission and processing, ultimately
reducing the total energy consumption. The energyefficient service provision is
crucial for enabling the green and sustainable Metaverse. In this article, we
take an augmented reality (AR) application as an example to achieve this goal.
Moreover, we study an economic issue on how the users reserve offloading
services from the MSP and how the MSP determines an optimal charging price
since each user is rational to decide whether to accept the offloading service
by taking into account the monetary cost. A single-leader multi-follower
Stackelberg game is formulated between the MSP and users while each user
optimizes an offloading probability to minimize the weighted sum of time,
energy consumption and monetary cost. Numerical results show that our scheme
achieves energy savings and satisfies individual rationality simultaneously
compared with the conventional schemes. Finally, we identify and discuss open
directions on how several emerging technologies are combined with the
sustainable green Metaverse.",None,-1
Table-GPT: Table-tuned GPT for Diverse Table Tasks,0.560268,"Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable
abilities to follow diverse human instructions and perform a wide range of
tasks. However, when probing language models using a range of basic
table-understanding tasks, we observe that today's language models are still
sub-optimal in many table-related tasks, likely because they are pre-trained
predominantly on \emph{one-dimensional} natural-language texts, whereas
relational tables are \emph{two-dimensional} objects.
  In this work, we propose a new ""\emph{table-tuning}"" paradigm, where we
continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using
diverse table-tasks synthesized from real tables as training data, with the
goal of enhancing language models' ability to understand tables and perform
table tasks. We show that our resulting Table-GPT models demonstrate (1) better
\emph{table-understanding} capabilities, by consistently outperforming the
vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout
unseen tasks, and (2) strong \emph{generalizability}, in its ability to respond
to diverse human instructions to perform new table-tasks, in a manner similar
to GPT-3.5 and ChatGPT.",None,-1
IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction,0.845477,"Reliable multi-agent trajectory prediction is crucial for the safe planning
and control of autonomous systems. Compared with single-agent cases, the major
challenge in simultaneously processing multiple agents lies in modeling complex
social interactions caused by various driving intentions and road conditions.
Previous methods typically leverage graph-based message propagation or
attention mechanism to encapsulate such interactions in the format of marginal
probabilistic distributions. However, it is inherently sub-optimal. In this
paper, we propose IPCC-TP, a novel relevance-aware module based on Incremental
Pearson Correlation Coefficient to improve multi-agent interaction modeling.
IPCC-TP learns pairwise joint Gaussian Distributions through the
tightly-coupled estimation of the means and covariances according to
interactive incremental movements. Our module can be conveniently embedded into
existing multi-agent prediction methods to extend original motion distribution
decoders. Extensive experiments on nuScenes and Argoverse 2 datasets
demonstrate that IPCC-TP improves the performance of baselines by a large
margin.",None,-1
SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues,0.816174,"In second language vocabulary learning, existing works have primarily focused
on either the learning interface or scheduling personalized retrieval practices
to maximize memory retention. However, the learning content, i.e., the
information presented on flashcards, has mostly remained constant. Keyword
mnemonic is a notable learning strategy that relates new vocabulary to existing
knowledge by building an acoustic and imagery link using a keyword that sounds
alike. Beyond that, producing verbal and visual cues associated with the
keyword to facilitate building these links requires a manual process and is not
scalable. In this paper, we explore an opportunity to use large language models
to automatically generate verbal and visual cues for keyword mnemonics. Our
approach, an end-to-end pipeline for auto-generating verbal and visual cues,
can automatically generate highly memorable cues. We investigate the
effectiveness of our approach via a human participant experiment by comparing
it with manually generated cues.",None,-1
Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment,0.47402,"Text-video retrieval is a challenging cross-modal task, which aims to align
visual entities with natural language descriptions. Current methods either fail
to leverage the local details or are computationally expensive. What's worse,
they fail to leverage the heterogeneous concepts in data. In this paper, we
propose the Disentangled Conceptualization and Set-to-set Alignment (DiCoSA) to
simulate the conceptualizing and reasoning process of human beings. For
disentangled conceptualization, we divide the coarse feature into multiple
latent factors related to semantic concepts. For set-to-set alignment, where a
set of visual concepts correspond to a set of textual concepts, we propose an
adaptive pooling method to aggregate semantic concepts to address the partial
matching. In particular, since we encode concepts independently in only a few
dimensions, DiCoSA is superior at efficiency and granularity, ensuring
fine-grained interactions using a similar computational complexity as
coarse-grained alignment. Extensive experiments on five datasets, including
MSR-VTT, LSMDC, MSVD, ActivityNet, and DiDeMo, demonstrate that our method
outperforms the existing state-of-the-art methods.",None,-1
Improving the Out-Of-Distribution Generalization Capability of Language Models: Counterfactually-Augmented Data is not Enough,0.0176146,"Counterfactually-Augmented Data (CAD) has the potential to improve language
models' Out-Of-Distribution (OOD) generalization capability, as CAD induces
language models to exploit causal features and exclude spurious correlations.
However, the empirical results of OOD generalization on CAD are not as
efficient as expected. In this paper, we attribute the inefficiency to Myopia
Phenomenon caused by CAD: language models only focus on causal features that
are edited in the augmentation and exclude other non-edited causal features. As
a result, the potential of CAD is not fully exploited. Based on the structural
properties of CAD, we design two additional constraints to help language models
extract more complete causal features contained in CAD, thus improving the OOD
generalization capability. We evaluate our method on two tasks: Sentiment
Analysis and Natural Language Inference, and the experimental results
demonstrate that our method could unlock CAD's potential and improve language
models' OOD generalization capability.",None,-1
Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents,0.333257,"Temporal credit assignment is crucial for learning and skill development in
natural and artificial intelligence. While computational methods like the TD
approach in reinforcement learning have been proposed, it's unclear if they
accurately represent how humans handle feedback delays. Cognitive models intend
to represent the mental steps by which humans solve problems and perform a
number of tasks, but limited research in cognitive science has addressed the
credit assignment problem in humans and cognitive models. Our research uses a
cognitive model based on a theory of decisions from experience, Instance-Based
Learning Theory (IBLT), to test different credit assignment mechanisms in a
goal-seeking navigation task with varying levels of decision complexity.
Instance-Based Learning (IBL) models simulate the process of making sequential
choices with different credit assignment mechanisms, including a new IBL-TD
model that combines the IBL decision mechanism with the TD approach. We found
that (1) An IBL model that gives equal credit assignment to all decisions is
able to match human performance better than other models, including IBL-TD and
Q-learning; (2) IBL-TD and Q-learning models underperform compared to humans
initially, but eventually, they outperform humans; (3) humans are influenced by
decision complexity, while models are not. Our study provides insights into the
challenges of capturing human behavior and the potential opportunities to use
these models in future AI systems to support human activities.",None,-1
Adversarial Training For Low-Resource Disfluency Correction,0.443974,"Disfluencies commonly occur in conversational speech. Speech with
disfluencies can result in noisy Automatic Speech Recognition (ASR)
transcripts, which affects downstream tasks like machine translation. In this
paper, we propose an adversarially-trained sequence-tagging model for
Disfluency Correction (DC) that utilizes a small amount of labeled real
disfluent data in conjunction with a large amount of unlabeled data. We show
the benefit of our proposed technique, which crucially depends on synthetically
generated disfluent data, by evaluating it for DC in three Indian languages-
Bengali, Hindi, and Marathi (all from the Indo-Aryan family). Our technique
also performs well in removing stuttering disfluencies in ASR transcripts
introduced by speech impairments. We achieve an average 6.15 points improvement
in F1-score over competitive baselines across all three languages mentioned. To
the best of our knowledge, we are the first to utilize adversarial training for
DC and use it to correct stuttering disfluencies in English, establishing a new
benchmark for this task.",None,-1
SUnAA: Sparse Unmixing using Archetypal Analysis,0.283329,"This paper introduces a new sparse unmixing technique using archetypal
analysis (SUnAA). First, we design a new model based on archetypal analysis. We
assume that the endmembers of interest are a convex combination of endmembers
provided by a spectral library and that the number of endmembers of interest is
known. Then, we propose a minimization problem. Unlike most conventional sparse
unmixing methods, here the minimization problem is non-convex. We minimize the
optimization objective iteratively using an active set algorithm. Our method is
robust to the initialization and only requires the number of endmembers of
interest. SUnAA is evaluated using two simulated datasets for which results
confirm its better performance over other conventional and advanced techniques
in terms of signal-to-reconstruction error. SUnAA is also applied to Cuprite
dataset and the results are compared visually with the available geological map
provided for this dataset. The qualitative assessment demonstrates the
successful estimation of the minerals abundances and significantly improves the
detection of dominant minerals compared to the conventional regression-based
sparse unmixing methods. The Python implementation of SUnAA can be found at:
https://github.com/BehnoodRasti/SUnAA.",None,-1
Image-free Classifier Injection for Zero-Shot Classification,0.0592625,"Zero-shot learning models achieve remarkable results on image classification
for samples from classes that were not seen during training. However, such
models must be trained from scratch with specialised methods: therefore, access
to a training dataset is required when the need for zero-shot classification
arises. In this paper, we aim to equip pre-trained models with zero-shot
classification capabilities without the use of image data. We achieve this with
our proposed Image-free Classifier Injection with Semantics (ICIS) that injects
classifiers for new, unseen classes into pre-trained classification models in a
post-hoc fashion without relying on image data. Instead, the existing
classifier weights and simple class-wise descriptors, such as class names or
attributes, are used. ICIS has two encoder-decoder networks that learn to
reconstruct classifier weights from descriptors (and vice versa), exploiting
(cross-)reconstruction and cosine losses to regularise the decoding process.
Notably, ICIS can be cheaply trained and applied directly on top of pre-trained
classification models. Experiments on benchmark ZSL datasets show that ICIS
produces unseen classifier weights that achieve strong (generalised) zero-shot
classification performance. Code is available at
https://github.com/ExplainableML/ImageFreeZSL .",None,-1
AVSegFormer: Audio-Visual Segmentation with Transformer,0.772234,"The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.",None,-1
Beyond Grids: Exploring Elastic Input Sampling for Vision Transformers,0.069257,"Vision transformers have excelled in various computer vision tasks but mostly
rely on rigid input sampling using a fixed-size grid of patches. This limits
their applicability in real-world problems, such as in the field of robotics
and UAVs, where one can utilize higher input elasticity to boost model
performance and efficiency. Our paper addresses this limitation by formalizing
the concept of input elasticity for vision transformers and introducing an
evaluation protocol, including dedicated metrics for measuring input
elasticity. Moreover, we propose modifications to the transformer architecture
and training regime, which increase its elasticity. Through extensive
experimentation, we spotlight opportunities and challenges associated with
input sampling strategies.",None,-1
Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models,0.273212,"Large language models can perform various reasoning tasks by using
chain-of-thought prompting, which guides them to find answers through
step-by-step demonstrations. However, the quality of the prompts depends on the
demonstrations given to the models, and creating many of them by hand is
costly. We introduce Synthetic prompting, a method that leverages a few
handcrafted examples to prompt the model to generate more examples by itself,
and selects effective demonstrations to elicit better reasoning. Our method
alternates between a backward and forward process to generate new examples. The
backward process generates a question that match a sampled reasoning chain, so
that the question is solvable and clear. The forward process produces a more
detailed reasoning chain for the question, improving the quality of the
example. We evaluate our method on numerical, symbolic, and algorithmic
reasoning tasks, and show that it outperforms existing prompting techniques.",None,-1
Literal-Aware Knowledge Graph Embedding for Welding Quality Monitoring: A Bosch Case,0.317137,"Recently there has been a series of studies in knowledge graph embedding
(KGE), which attempts to learn the embeddings of the entities and relations as
numerical vectors and mathematical mappings via machine learning (ML). However,
there has been limited research that applies KGE for industrial problems in
manufacturing. This paper investigates whether and to what extent KGE can be
used for an important problem: quality monitoring for welding in manufacturing
industry, which is an impactful process accounting for production of millions
of cars annually. The work is in line with Bosch research of data-driven
solutions that intends to replace the traditional way of destroying cars, which
is extremely costly and produces waste. The paper tackles two very challenging
questions simultaneously: how large the welding spot diameter is; and to which
car body the welded spot belongs to. The problem setting is difficult for
traditional ML because there exist a high number of car bodies that should be
assigned as class labels. We formulate the problem as link prediction, and
experimented popular KGE methods on real industry data, with consideration of
literals. Our results reveal both limitations and promising aspects of adapted
KGE methods.",None,-1
Behavioral Analysis of Vision-and-Language Navigation Agents,0.471945,"To be successful, Vision-and-Language Navigation (VLN) agents must be able to
ground instructions to actions based on their surroundings. In this work, we
develop a methodology to study agent behavior on a skill-specific basis --
examining how well existing agents ground instructions about stopping, turning,
and moving towards specified objects or rooms. Our approach is based on
generating skill-specific interventions and measuring changes in agent
predictions. We present a detailed case study analyzing the behavior of a
recent agent and then compare multiple agents in terms of skill-specific
competency scores. This analysis suggests that biases from training have
lasting effects on agent behavior and that existing models are able to ground
simple referring expressions. Our comparisons between models show that
skill-specific scores correlate with improvements in overall VLN task
performance.",None,-1
Mathematical Structure of Syntactic Merge,0.613637,"The syntactic Merge operation of the Minimalist Program in linguistics can be
described mathematically in terms of Hopf algebras, with a formalism similar to
the one arising in the physics of renormalization. This mathematical
formulation of Merge has good descriptive power, as phenomena empirically
observed in linguistics can be justified from simple mathematical arguments. It
also provides a possible mathematical model for externalization and for the
role of syntactic parameters.",None,-1
Five A$^{+}$ Network: You Only Need 9K Parameters for Underwater Image Enhancement,0.989463,"A lightweight underwater image enhancement network is of great significance
for resource-constrained platforms, but balancing model size, computational
efficiency, and enhancement performance has proven difficult for previous
approaches. In this work, we propose the Five A$^{+}$ Network (FA$^{+}$Net), a
highly efficient and lightweight real-time underwater image enhancement network
with only $\sim$ 9k parameters and $\sim$ 0.01s processing time. The
FA$^{+}$Net employs a two-stage enhancement structure. The strong prior stage
aims to decompose challenging underwater degradations into sub-problems, while
the fine-grained stage incorporates multi-branch color enhancement module and
pixel attention module to amplify the network's perception of details. To the
best of our knowledge, FA$^{+}$Net is the only network with the capability of
real-time enhancement of 1080P images. Thorough extensive experiments and
comprehensive visual comparison, we show that FA$^{+}$Net outperforms previous
approaches by obtaining state-of-the-art performance on multiple datasets while
significantly reducing both parameter count and computational complexity. The
code is open source at https://github.com/Owen718/FiveAPlus-Network.",None,-1
Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports,0.907538,"With the increasing number of clinical trial reports generated every day, it
is becoming hard to keep up with novel discoveries that inform evidence-based
healthcare recommendations. To help automate this process and assist medical
experts, NLP solutions are being developed. This motivated the SemEval-2023
Task 7, where the goal was to develop an NLP system for two tasks: evidence
retrieval and natural language inference from clinical trial data. In this
paper, we describe our two developed systems. The first one is a pipeline
system that models the two tasks separately, while the second one is a joint
system that learns the two tasks simultaneously with a shared representation
and a multi-task learning approach. The final system combines their outputs in
an ensemble system. We formalize the models, present their characteristics and
challenges, and provide an analysis of achieved results. Our system ranked 3rd
out of 40 participants with a final submission.",None,-1
Improving Zero-Shot Action Recognition using Human Instruction with Text Description,0.110111,"Zero-shot action recognition, which recognizes actions in videos without
having received any training examples, is gaining wide attention considering it
can save labor costs and training time. Nevertheless, the performance of
zero-shot learning is still unsatisfactory, which limits its practical
application. To solve this problem, this study proposes a framework to improve
zero-shot action recognition using human instructions with text descriptions.
The proposed framework manually describes video contents, which incurs some
labor costs; in many situations, the labor costs are worth it. We manually
annotate text features for each action, which can be a word, phrase, or
sentence. Then by computing the matching degrees between the video and all text
features, we can predict the class of the video. Furthermore, the proposed
model can also be combined with other models to improve its accuracy. In
addition, our model can be continuously optimized to improve the accuracy by
repeating human instructions. The results with UCF101 and HMDB51 showed that
our model achieved the best accuracy and improved the accuracies of other
models.",None,-1
Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,0.251534,"Recent work in Natural Language Processing and Computer Vision has been using
textual information -- e.g., entity names and descriptions -- available in
knowledge graphs to ground neural models to high-quality structured data.
However, when it comes to non-English languages, the quantity and quality of
textual information are comparatively scarce. To address this issue, we
introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and
perform a thorough investigation on bridging the gap in both the quantity and
quality of textual information between English and non-English languages. More
specifically, we: i) bring to light the problem of increasing multilingual
coverage and precision of entity names and descriptions in Wikidata; ii)
demonstrate that state-of-the-art methods, namely, Machine Translation (MT),
Web Search (WS), and Large Language Models (LLMs), struggle with this task;
iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and
LLMs to generate high-quality textual information; and, iv) study the impact of
increasing multilingual coverage and precision of non-English textual
information in Entity Linking, Knowledge Graph Completion, and Question
Answering. As part of our effort towards better multilingual knowledge graphs,
we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE
approaches in 10 languages across 7 language families.",None,-1
Continual Learning with Dirichlet Generative-based Rehearsal,0.502339,"Recent advancements in data-driven task-oriented dialogue systems (ToDs)
struggle with incremental learning due to computational constraints and
time-consuming issues. Continual Learning (CL) attempts to solve this by
avoiding intensive pre-training, but it faces the problem of catastrophic
forgetting (CF). While generative-based rehearsal CL methods have made
significant strides, generating pseudo samples that accurately reflect the
underlying task-specific distribution is still a challenge. In this paper, we
present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal
strategy for CL. Unlike the traditionally used Gaussian latent variable in the
Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and
versatility of the Dirichlet distribution to model the latent prior variable.
This enables it to efficiently capture sentence-level features of previous
tasks and effectively guide the generation of pseudo samples. In addition, we
introduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based
knowledge distillation method that enhances knowledge transfer during pseudo
sample generation. Our experiments confirm the efficacy of our approach in both
intent detection and slot-filling tasks, outperforming state-of-the-art
methods.",None,-1
AutoDroid: LLM-powered Task Automation in Android,0.87099,"Mobile task automation is an attractive technique that aims to enable
voice-based hands-free user interaction with smartphones. However, existing
approaches suffer from poor scalability due to the limited language
understanding ability and the non-trivial manual efforts required from
developers or end-users. The recent advance of large language models (LLMs) in
language understanding and reasoning inspires us to rethink the problem from a
model-centric perspective, where task preparation, comprehension, and execution
are handled by a unified language model. In this work, we introduce AutoDroid,
a mobile task automation system capable of handling arbitrary tasks on any
Android application without manual efforts. The key insight is to combine the
commonsense knowledge of LLMs and domain-specific knowledge of apps through
automated dynamic analysis. The main components include a functionality-aware
UI representation method that bridges the UI with the LLM, exploration-based
memory injection techniques that augment the app-specific domain knowledge of
LLM, and a multi-granularity query optimization module that reduces the cost of
model inference. We integrate AutoDroid with off-the-shelf LLMs including
online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a
new benchmark for memory-augmented Android task automation with 158 common
tasks. The results demonstrated that AutoDroid is able to precisely generate
actions with an accuracy of 90.9%, and complete tasks with a success rate of
71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,
benchmark suites, and source code of AutoDroid will be released at
url{https://autodroid-sys.github.io/}.",None,-1
ArcGPT: A Large Language Model Tailored for Real-world Archival Applications,0.378363,"Archives play a crucial role in preserving information and knowledge, and the
exponential growth of such data necessitates efficient and automated tools for
managing and utilizing archive information resources. Archival applications
involve managing massive data that are challenging to process and analyze.
Although LLMs have made remarkable progress in diverse domains, there are no
publicly available archives tailored LLM. Addressing this gap, we introduce
ArcGPT, to our knowledge, the first general-purpose LLM tailored to the
archival field. To enhance model performance on real-world archival tasks,
ArcGPT has been pre-trained on massive and extensive archival domain data.
Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world
archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing
state-of-the-art models, marking a substantial step forward in effective
archival data management. Ultimately, ArcGPT aims to better serve the archival
community, aiding archivists in their crucial role of preserving and harnessing
our collective information and knowledge.",None,-1
Certified Robust Neural Networks: Generalization and Corruption Resistance,0.345281,"Recent work have demonstrated that robustness (to ""corruption"") can be at
odds with generalization. Adversarial training, for instance, aims to reduce
the problematic susceptibility of modern neural networks to small data
perturbations. Surprisingly, overfitting is a major concern in adversarial
training despite being mostly absent in standard training. We provide here
theoretical evidence for this peculiar ""robust overfitting"" phenomenon.
Subsequently, we advance a novel distributionally robust loss function bridging
robustness and generalization. We demonstrate both theoretically as well as
empirically the loss to enjoy a certified level of robustness against two
common types of corruption--data evasion and poisoning attacks--while ensuring
guaranteed generalization. We show through careful numerical experiments that
our resulting holistic robust (HR) training procedure yields SOTA performance.
Finally, we indicate that HR training can be interpreted as a direct extension
of adversarial training and comes with a negligible additional computational
burden. A ready-to-use python library implementing our algorithm is available
at https://github.com/RyanLucas3/HR_Neural_Networks.",None,-1
Masked Contrastive Graph Representation Learning for Age Estimation,0.883153,"Age estimation of face images is a crucial task with various practical
applications in areas such as video surveillance and Internet access control.
While deep learning-based age estimation frameworks, e.g., convolutional neural
network (CNN), multi-layer perceptrons (MLP), and transformers have shown
remarkable performance, they have limitations when modelling complex or
irregular objects in an image that contains a large amount of redundant
information. To address this issue, this paper utilizes the robustness property
of graph representation learning in dealing with image redundancy information
and proposes a novel Masked Contrastive Graph Representation Learning (MCGRL)
method for age estimation. Specifically, our approach first leverages CNN to
extract semantic features of the image, which are then partitioned into patches
that serve as nodes in the graph. Then, we use a masked graph convolutional
network (GCN) to derive image-based node representations that capture rich
structural information. Finally, we incorporate multiple losses to explore the
complementary relationship between structural information and semantic
features, which improves the feature representation capability of GCN.
Experimental results on real-world face image datasets demonstrate the
superiority of our proposed method over other state-of-the-art age estimation
approaches.",None,-1
ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,0.278232,"We address a weakly-supervised low-shot instance segmentation, an
annotation-efficient training method to deal with novel classes effectively.
Since it is an under-explored problem, we first investigate the difficulty of
the problem and identify the performance bottleneck by conducting systematic
analyses of model components and individual sub-tasks with a simple baseline
model. Based on the analyses, we propose ENInst with sub-task enhancement
methods: instance-wise mask refinement for enhancing pixel localization quality
and novel classifier composition for improving classification accuracy. Our
proposed method lifts the overall performance by enhancing the performance of
each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in
achieving comparable performance to the existing fully-supervised few-shot
models and even outperforms them at times.",None,-1
ECQED: Emotion-Cause Quadruple Extraction in Dialogs,0.573469,"The existing emotion-cause pair extraction (ECPE) task, unfortunately,
ignores extracting the emotion type and cause type, while these fine-grained
meta-information can be practically useful in real-world applications, i.e.,
chat robots and empathic dialog generation. Also the current ECPE is limited to
the scenario of single text piece, while neglecting the studies at dialog level
that should have more realistic values. In this paper, we extend the ECPE task
with a broader definition and scenario, presenting a new task, Emotion-Cause
Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause
utterance pairs and emotion and cause types. We present an ECQED model based on
a structural and semantic heterogeneous graph as well as a parallel grid
tagging scheme, which advances in effectively incorporating the dialog context
structure, meanwhile solving the challenging overlapped quadruple issue. Via
experiments we show that introducing the fine-grained emotion and cause
features evidently helps better dialog generation. Also our proposed ECQED
system shows exceptional superiority over baselines on both the emotion-cause
quadruple or pair extraction tasks, meanwhile being highly efficient.",None,-1
STNet: Spatial and Temporal feature fusion network for change detection in remote sensing images,0.360282,"As an important task in remote sensing image analysis, remote sensing change
detection (RSCD) aims to identify changes of interest in a region from
spatially co-registered multi-temporal remote sensing images, so as to monitor
the local development. Existing RSCD methods usually formulate RSCD as a binary
classification task, representing changes of interest by merely feature
concatenation or feature subtraction and recovering the spatial details via
densely connected change representations, whose performances need further
improvement. In this paper, we propose STNet, a RSCD network based on spatial
and temporal feature fusions. Specifically, we design a temporal feature fusion
(TFF) module to combine bi-temporal features using a cross-temporal gating
mechanism for emphasizing changes of interest; a spatial feature fusion module
is deployed to capture fine-grained information using a cross-scale attention
mechanism for recovering the spatial details of change representations.
Experimental results on three benchmark datasets for RSCD demonstrate that the
proposed method achieves the state-of-the-art performance. Code is available at
https://github.com/xwmaxwma/rschange.",None,-1
Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism,0.239248,"Communication can impressively improve cooperation in multi-agent
reinforcement learning (MARL), especially for partially-observed tasks.
However, existing works either broadcast the messages leading to information
redundancy, or learn targeted communication by modeling all the other agents as
targets, which is not scalable when the number of agents varies. In this work,
to tackle the scalability problem of MARL communication for partially-observed
tasks, we propose a novel framework Transformer-based Email Mechanism (TEM).
The agents adopt local communication to send messages only to the ones that can
be observed without modeling all the agents. Inspired by human cooperation with
email forwarding, we design message chains to forward information to cooperate
with the agents outside the observation range. We introduce Transformer to
encode and decode the message chain to choose the next receiver selectively.
Empirically, TEM outperforms the baselines on multiple cooperative MARL
benchmarks. When the number of agents varies, TEM maintains superior
performance without further training.",None,-1
Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything,0.907193,"The research on extrinsic calibration between Light Detection and
Ranging(LiDAR) and camera are being promoted to a more accurate, automatic and
generic manner. Since deep learning has been employed in calibration, the
restrictions on the scene are greatly reduced. However, data driven method has
the drawback of low transfer-ability. It cannot adapt to dataset variations
unless additional training is taken. With the advent of foundation model, this
problem can be significantly mitigated. By using the Segment Anything
Model(SAM), we propose a novel LiDAR-camera calibration method, which requires
zero extra training and adapts to common scenes. With an initial guess, we
opimize the extrinsic parameter by maximizing the consistency of points that
are projected inside each image mask. The consistency includes three properties
of the point cloud: the intensity, normal vector and categories derived from
some segmentation methods. The experiments on different dataset have
demonstrated the generality and comparable accuracy of our method. The code is
available at https://github.com/OpenCalib/CalibAnything.",None,-1
Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games,0.663078,"We introduce a class of networked Markov potential games in which agents are
associated with nodes in a network. Each agent has its own local potential
function, and the reward of each agent depends only on the states and actions
of the agents within a neighborhood. In this context, we propose a localized
actor-critic algorithm. The algorithm is scalable since each agent uses only
local information and does not need access to the global state. Further, the
algorithm overcomes the curse of dimensionality through the use of function
approximation. Our main results provide finite-sample guarantees up to a
localization error and a function approximation error. Specifically, we achieve
an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by
the averaged Nash regret. This is the first finite-sample bound for multi-agent
competitive games that does not depend on the number of agents.",None,-1
Rolling Horizon based Temporal Decomposition for the Offline Pickup and Delivery Problem with Time Windows,0.236943,"The offline pickup and delivery problem with time windows (PDPTW) is a
classical combinatorial optimization problem in the transportation community,
which has proven to be very challenging computationally. Due to the complexity
of the problem, practical problem instances can be solved only via heuristics,
which trade-off solution quality for computational tractability. Among the
various heuristics, a common strategy is problem decomposition, that is, the
reduction of a large-scale problem into a collection of smaller sub-problems,
with spatial and temporal decompositions being two natural approaches. While
spatial decomposition has been successful in certain settings, effective
temporal decomposition has been challenging due to the difficulty of stitching
together the sub-problem solutions across the decomposition boundaries. In this
work, we introduce a novel temporal decomposition scheme for solving a class of
PDPTWs that have narrow time windows, for which it is able to provide both fast
and high-quality solutions. We utilize techniques that have been popularized
recently in the context of online dial-a-ride problems along with the general
idea of rolling horizon optimization. To the best of our knowledge, this is the
first attempt to solve offline PDPTWs using such an approach. To show the
performance and scalability of our framework, we use the optimization of
paratransit services as a motivating example. We compare our results with an
offline heuristic algorithm using Google OR-Tools. In smaller problem
instances, the baseline approach is as competitive as our framework. However,
in larger problem instances, our framework is more scalable and can provide
good solutions to problem instances of varying degrees of difficulty, while the
baseline algorithm often fails to find a feasible solution within comparable
compute times.",None,-1
Anthropomorphization of AI: Opportunities and Risks,0.899339,"Anthropomorphization is the tendency to attribute human-like traits to
non-human entities. It is prevalent in many social contexts -- children
anthropomorphize toys, adults do so with brands, and it is a literary device.
It is also a versatile tool in science, with behavioral psychology and
evolutionary biology meticulously documenting its consequences. With widespread
adoption of AI systems, and the push from stakeholders to make it human-like
through alignment techniques, human voice, and pictorial avatars, the tendency
for users to anthropomorphize it increases significantly. We take a dyadic
approach to understanding this phenomenon with large language models (LLMs) by
studying (1) the objective legal implications, as analyzed through the lens of
the recent blueprint of AI bill of rights and the (2) subtle psychological
aspects customization and anthropomorphization. We find that anthropomorphized
LLMs customized for different user bases violate multiple provisions in the
legislative blueprint. In addition, we point out that anthropomorphization of
LLMs affects the influence they can have on their users, thus having the
potential to fundamentally change the nature of human-AI interaction, with
potential for manipulation and negative influence. With LLMs being
hyper-personalized for vulnerable groups like children and patients among
others, our work is a timely and important contribution. We propose a
conservative strategy for the cautious use of anthropomorphization to improve
trustworthiness of AI systems.",None,-1
SummIt: Iterative Text Summarization via ChatGPT,0.954572,"Text summarization systems have made significant progress in recent years,
but typically generate summaries in one single step. However, the one-shot
summarization setting is sometimes inadequate, as the generated summary may
contain hallucinations or overlook essential details related to the reader's
interests. This paper addresses this limitation by proposing SummIt, an
iterative text summarization framework based on large language models like
ChatGPT. Our framework enables the model to refine the generated summary
iteratively through self-evaluation and feedback, resembling humans' iterative
process when drafting and revising summaries. Furthermore, we explore the
potential benefits of integrating knowledge and topic extractors into the
framework to enhance summary faithfulness and controllability. We automatically
evaluate the performance of our framework on three benchmark summarization
datasets. We also conduct a human evaluation to validate the effectiveness of
the iterative refinements and identify a potential issue of over-correction.",None,-1
Mimic-IV-ICD: A new benchmark for eXtreme MultiLabel Classification,0.893704,"Clinical notes are assigned ICD codes - sets of codes for diagnoses and
procedures. In the recent years, predictive machine learning models have been
built for automatic ICD coding. However, there is a lack of widely accepted
benchmarks for automated ICD coding models based on large-scale public EHR
data.
  This paper proposes a public benchmark suite for ICD-10 coding using a large
EHR dataset derived from MIMIC-IV, the most recent public EHR dataset. We
implement and compare several popular methods for ICD coding prediction tasks
to standardize data preprocessing and establish a comprehensive ICD coding
benchmark dataset. This approach fosters reproducibility and model comparison,
accelerating progress toward employing automated ICD coding in future studies.
Furthermore, we create a new ICD-9 benchmark using MIMIC-IV data, providing
more data points and a higher number of ICD codes than MIMIC-III. Our
open-source code offers easy access to data processing steps, benchmark
creation, and experiment replication for those with MIMIC-IV access, providing
insights, guidance, and protocols to efficiently develop ICD coding models.",None,-1
PASS: Peer-Agreement based Sample Selection for training with Noisy Labels,0.146227,"The prevalence of noisy-label samples poses a significant challenge in deep
learning, inducing overfitting effects. This has, therefore, motivated the
emergence of learning with noisy-label (LNL) techniques that focus on
separating noisy- and clean-label samples to apply different learning
strategies to each group of samples. Current methodologies often rely on the
small-loss hypothesis or feature-based selection to separate noisy- and
clean-label samples, yet our empirical observations reveal their limitations,
especially for labels with instance dependent noise (IDN). An important
characteristic of IDN is the difficulty to distinguish the clean-label samples
that lie near the decision boundary (i.e., the hard samples) from the
noisy-label samples. We, therefore, propose a new noisy-label detection method,
termed Peer-Agreement based Sample Selection (PASS), to address this problem.
Utilising a trio of classifiers, PASS employs consensus-driven peer-based
agreement of two models to select the samples to train the remaining model.
PASS is easily integrated into existing LNL models, enabling the improvement of
the detection accuracy of noisy- and clean-label samples, which increases the
classification accuracy across various LNL benchmarks.",None,-1
Universal Morphology Control via Contextual Modulation,0.181193,"Learning a universal policy across different robot morphologies can
significantly improve learning efficiency and generalization in continuous
control. However, it poses a challenging multi-task reinforcement learning
problem, as the optimal policy may be quite different across robots and
critically depend on the morphology. Existing methods utilize graph neural
networks or transformers to handle heterogeneous state and action spaces across
different morphologies, but pay little attention to the dependency of a robot's
control policy on its morphology context. In this paper, we propose a
hierarchical architecture to better model this dependency via contextual
modulation, which includes two key submodules: (1) Instead of enforcing hard
parameter sharing across robots, we use hypernetworks to generate
morphology-dependent control parameters; (2) We propose a fixed attention
mechanism that solely depends on the morphology to modulate the interactions
between different limbs in a robot. Experimental results show that our method
not only improves learning performance on a diverse set of training robots, but
also generalizes better to unseen morphologies in a zero-shot fashion.",None,-1
Value Engineering for Autonomous Agents,0.182522,"Machine Ethics (ME) is concerned with the design of Artificial Moral Agents
(AMAs), i.e. autonomous agents capable of reasoning and behaving according to
moral values. Previous approaches have treated values as labels associated with
some actions or states of the world, rather than as integral components of
agent reasoning. It is also common to disregard that a value-guided agent
operates alongside other value-guided agents in an environment governed by
norms, thus omitting the social dimension of AMAs. In this blue sky paper, we
propose a new AMA paradigm grounded in moral and social psychology, where
values are instilled into agents as context-dependent goals. These goals
intricately connect values at individual levels to norms at a collective level
by evaluating the outcomes most incentivized by the norms in place. We argue
that this type of normative reasoning, where agents are endowed with an
understanding of norms' moral implications, leads to value-awareness in
autonomous agents. Additionally, this capability paves the way for agents to
align the norms enforced in their societies with respect to the human values
instilled in them, by complementing the value-based reasoning on norms with
agreement mechanisms to help agents collectively agree on the best set of norms
that suit their human values. Overall, our agent model goes beyond the
treatment of values as inert labels by connecting them to normative reasoning
and to the social functionalities needed to integrate value-aware agents into
our modern hybrid human-computer societies.",None,-1
Conversation Style Transfer using Few-Shot Learning,0.379243,"Conventional text style transfer approaches focus on sentence-level style
transfer without considering contextual information, and the style is described
with attributes (e.g., formality). When applying style transfer in
conversations such as task-oriented dialogues, existing approaches suffer from
these limitations as context can play an important role and the style
attributes are often difficult to define in conversations. In this paper, we
introduce conversation style transfer as a few-shot learning problem, where the
model learns to perform style transfer by observing only a few example
dialogues in the target style. We propose a novel in-context learning approach
to solve the task with style-free dialogues as a pivot. Human evaluation shows
that by incorporating multi-turn context, the model is able to match the target
style while having better appropriateness and semantic correctness compared to
utterance/sentence-level style transfer. Additionally, we show that
conversation style transfer can also benefit downstream tasks. For example, in
multi-domain intent classification tasks, the F1 scores improve after
transferring the style of training data to match the style of the test data.",None,-1
CryoChains: Heterogeneous Reconstruction of Molecular Assembly of Semi-flexible Chains from Cryo-EM Images,0.441843,"Cryogenic electron microscopy (cryo-EM) has transformed structural biology by
allowing to reconstruct 3D biomolecular structures up to near-atomic
resolution. However, the 3D reconstruction process remains challenging, as the
3D structures may exhibit substantial shape variations, while the 2D image
acquisition suffers from a low signal-to-noise ratio, requiring to acquire very
large datasets that are time-consuming to process. Current reconstruction
methods are precise but computationally expensive, or faster but lack a
physically-plausible model of large molecular shape variations. To fill this
gap, we propose CryoChains that encodes large deformations of biomolecules via
rigid body transformation of their chains, while representing their finer shape
variations with the normal mode analysis framework of biophysics. Our synthetic
data experiments on the human GABA\textsubscript{B} and heat shock protein show
that CryoChains gives a biophysically-grounded quantification of the
heterogeneous conformations of biomolecules, while reconstructing their 3D
molecular structures at an improved resolution compared to the current fastest,
interpretable deep learning method.",None,-1
ParaFormer: Parallel Attention Transformer for Efficient Feature Matching,0.683391,"Heavy computation is a bottleneck limiting deep-learningbased feature
matching algorithms to be applied in many realtime applications. However,
existing lightweight networks optimized for Euclidean data cannot address
classical feature matching tasks, since sparse keypoint based descriptors are
expected to be matched. This paper tackles this problem and proposes two
concepts: 1) a novel parallel attention model entitled ParaFormer and 2) a
graph based U-Net architecture with attentional pooling. First, ParaFormer
fuses features and keypoint positions through the concept of amplitude and
phase, and integrates self- and cross-attention in a parallel manner which
achieves a win-win performance in terms of accuracy and efficiency. Second,
with U-Net architecture and proposed attentional pooling, the ParaFormer-U
variant significantly reduces computational complexity, and minimize
performance loss caused by downsampling. Sufficient experiments on various
applications, including homography estimation, pose estimation, and image
matching, demonstrate that ParaFormer achieves state-of-the-art performance
while maintaining high efficiency. The efficient ParaFormer-U variant achieves
comparable performance with less than 50% FLOPs of the existing attention-based
models.",None,-1
ADD: An Automatic Desensitization Fisheye Dataset for Autonomous Driving,0.218728,"Autonomous driving systems require many images for analyzing the surrounding
environment. However, there is fewer data protection for private information
among these captured images, such as pedestrian faces or vehicle license
plates, which has become a significant issue. In this paper, in response to the
call for data security laws and regulations and based on the advantages of
large Field of View(FoV) of the fisheye camera, we build the first Autopilot
Desensitization Dataset, called ADD, and formulate the first
deep-learning-based image desensitization framework, to promote the study of
image desensitization in autonomous driving scenarios. The compiled dataset
consists of 650K images, including different face and vehicle license plate
information captured by the surround-view fisheye camera. It covers various
autonomous driving scenarios, including diverse facial characteristics and
license plate colors. Then, we propose an efficient multitask desensitization
network called DesCenterNet as a benchmark on the ADD dataset, which can
perform face and vehicle license plate detection and desensitization tasks.
Based on ADD, we further provide an evaluation criterion for desensitization
performance, and extensive comparison experiments have verified the
effectiveness and superiority of our method on image desensitization.",None,-1
Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation,0.684019,"Emotion regulation is a crucial element in dealing with emotional events and
has positive effects on mental health. This paper aims to provide a more
comprehensive understanding of emotional events by introducing a new French
corpus of emotional narratives collected using a questionnaire for emotion
regulation. We follow the theoretical framework of the Component Process Model
which considers emotions as dynamic processes composed of four interrelated
components (behavior, feeling, thinking and territory). Each narrative is
related to a discrete emotion and is structured based on all emotion components
by the writers. We study the interaction of components and their impact on
emotion classification with machine learning methods and pre-trained language
models. Our results show that each component improves prediction performance,
and that the best results are achieved by jointly considering all components.
Our results also show the effectiveness of pre-trained language models in
predicting discrete emotion from certain components, which reveal differences
in how emotion components are expressed.",None,-1
SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,0.989537,"Prompt tuning is a technology that tunes a small set of parameters to steer a
pre-trained language model (LM) to directly generate the output for downstream
tasks. Recently, prompt tuning has demonstrated its storage and computation
efficiency in both natural language processing (NLP) and speech processing
fields. These advantages have also revealed prompt tuning as a candidate
approach to serving pre-trained LM for multiple tasks in a unified manner. For
speech processing, SpeechPrompt shows its high parameter efficiency and
competitive performance on a few speech classification tasks. However, whether
SpeechPrompt is capable of serving a large number of tasks is unanswered. In
this work, we propose SpeechPrompt v2, a prompt tuning framework capable of
performing a wide variety of speech classification tasks, covering multiple
languages and prosody-related tasks. The experiment result shows that
SpeechPrompt v2 achieves performance on par with prior works with less than
0.15M trainable parameters in a unified framework.",None,-1
Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers,0.0817354,"This paper explores the effectiveness of model-generated signals in improving
zero-shot generalization of text-to-text Transformers such as T5. We study
various designs to pretrain T5 using an auxiliary model to construct more
challenging token replacements for the main model to denoise. Key aspects under
study include the decoding target, the location of the RTD head, and the
masking pattern. Based on these studies, we develop a new model, METRO-T0,
which is pretrained using the redesigned ELECTRA-Style pretraining strategies
and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all
similar-sized baselines on prompted NLP benchmarks, such as T0 Eval and MMLU,
and rivals the state-of-the-art T0-11B model with only 8% of its parameters.
Our analysis on model's neural activation and parameter sensitivity reveals
that the effectiveness of METRO-T0 stems from more balanced contribution of
parameters and better utilization of their capacity. The code and model
checkpoints are available at https://github.com/gonglinyuan/metro_t0.",None,-1
Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual Translation of Dravidian Languages,0.0243025,"Current research in zero-shot translation is plagued by several issues such
as high compute requirements, increased training time and off target
translations. Proposed remedies often come at the cost of additional data or
compute requirements. Pivot based neural machine translation is preferred over
a single-encoder model for most settings despite the increased training and
evaluation time. In this work, we overcome the shortcomings of zero-shot
translation by taking advantage of transliteration and linguistic similarity.
We build a single encoder-decoder neural machine translation system for
Dravidian-Dravidian multilingual translation and perform zero-shot translation.
We compare the data vs zero-shot accuracy tradeoff and evaluate the performance
of our vanilla method against the current state of the art pivot based method.
We also test the theory that morphologically rich languages require large
vocabularies by restricting the vocabulary using an optimal transport based
technique. Our model manages to achieves scores within 3 BLEU of large-scale
pivot-based models when it is trained on 50\% of the language directions.",None,-1
MiniRBT: A Two-stage Distilled Small Chinese Pre-trained Model,0.121598,"In natural language processing, pre-trained language models have become
essential infrastructures. However, these models often suffer from issues such
as large size, long inference time, and challenging deployment. Moreover, most
mainstream pre-trained models focus on English, and there are insufficient
studies on small Chinese pre-trained models. In this paper, we introduce
MiniRBT, a small Chinese pre-trained model that aims to advance research in
Chinese natural language processing. MiniRBT employs a narrow and deep student
model and incorporates whole word masking and two-stage distillation during
pre-training to make it well-suited for most downstream tasks. Our experiments
on machine reading comprehension and text classification tasks reveal that
MiniRBT achieves 94% performance relative to RoBERTa, while providing a 6.8x
speedup, demonstrating its effectiveness and efficiency.",None,-1
CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning,0.251534,"Nowadays, the research on Large Vision-Language Models (LVLMs) has been
significantly promoted thanks to the success of Large Language Models (LLM).
Nevertheless, these Vision-Language Models (VLMs) are suffering from the
drawback of hallucination -- due to insufficient understanding of vision and
language modalities, VLMs may generate incorrect perception information when
doing downstream applications, for example, captioning a non-existent entity.
To address the hallucination phenomenon, on the one hand, we introduce a
Contrastive Instruction Evaluation Method (CIEM), which is an automatic
pipeline that leverages an annotated image-text dataset coupled with an LLM to
generate factual/contrastive question-answer pairs for the evaluation of the
hallucination of VLMs. On the other hand, based on CIEM, we further propose a
new instruction tuning method called CIT (the abbreviation of Contrastive
Instruction Tuning) to alleviate the hallucination of VLMs by automatically
producing high-quality factual/contrastive question-answer pairs and
corresponding justifications for model tuning. Through extensive experiments on
CIEM and CIT, we pinpoint the hallucination issues commonly present in existing
VLMs, the disability of the current instruction-tuning dataset to handle the
hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM
and public datasets.",None,-1
ModelScope Text-to-Video Technical Report,0.996967,"This paper introduces ModelScopeT2V, a text-to-video synthesis model that
evolves from a text-to-image synthesis model (i.e., Stable Diffusion).
ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame
generation and smooth movement transitions. The model could adapt to varying
frame numbers during training and inference, rendering it suitable for both
image-text and video-text datasets. ModelScopeT2V brings together three
components (i.e., VQGAN, a text encoder, and a denoising UNet), totally
comprising 1.7 billion parameters, in which 0.5 billion parameters are
dedicated to temporal capabilities. The model demonstrates superior performance
over state-of-the-art methods across three evaluation metrics. The code and an
online demo are available at
\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.",None,-1
An End-to-End Framework For Universal Lesion Detection With Missing Annotations,0.299054,"Fully annotated large-scale medical image datasets are highly valuable.
However, because labeling medical images is tedious and requires specialized
knowledge, the large-scale datasets available often have missing annotation
issues. For instance, DeepLesion, a large-scale CT image dataset with labels
for various kinds of lesions, is reported to have a missing annotation rate of
50\%. Directly training a lesion detector on it would suffer from false
negative supervision caused by unannotated lesions. To address this issue,
previous works have used sophisticated multi-stage strategies to switch between
lesion mining and detector training. In this work, we present a novel
end-to-end framework for mining unlabeled lesions while simultaneously training
the detector. Our framework follows the teacher-student paradigm. In each
iteration, the teacher model infers the input data and creates a set of
predictions. High-confidence predictions are combined with partially-labeled
ground truth for training the student model. On the DeepLesion dataset, using
the original partially labeled training set, our model can outperform all other
more complicated methods and surpass the previous best method by 2.3\% on
average sensitivity and 2.7\% on average precision, achieving state-of-the-art
universal lesion detection results.",None,-1
Toward Open-ended Embodied Tasks Solving,0.640492,"Empowering embodied agents, such as robots, with Artificial Intelligence (AI)
has become increasingly important in recent years. A major challenge is task
open-endedness. In practice, robots often need to perform tasks with novel
goals that are multifaceted, dynamic, lack a definitive ""end-state"", and were
not encountered during training. To tackle this problem, this paper introduces
\textit{Diffusion for Open-ended Goals} (DOG), a novel framework designed to
enable embodied AI to plan and act flexibly and dynamically for open-ended task
goals. DOG synergizes the generative prowess of diffusion models with
state-of-the-art, training-free guidance techniques to adaptively perform
online planning and control. Our evaluations demonstrate that DOG can handle
various kinds of novel task goals not seen during training, in both maze
navigation and robot control problems. Our work sheds light on enhancing
embodied AI's adaptability and competency in tackling open-ended goals.",None,-1
Unsupervised 3D out-of-distribution detection with latent diffusion models,0.700243,"Methods for out-of-distribution (OOD) detection that scale to 3D data are
crucial components of any real-world clinical deep learning system. Classic
denoising diffusion probabilistic models (DDPMs) have been recently proposed as
a robust way to perform reconstruction-based OOD detection on 2D datasets, but
do not trivially scale to 3D data. In this work, we propose to use Latent
Diffusion Models (LDMs), which enable the scaling of DDPMs to high-resolution
3D medical data. We validate the proposed approach on near- and far-OOD
datasets and compare it to a recently proposed, 3D-enabled approach using
Latent Transformer Models (LTMs). Not only does the proposed LDM-based approach
achieve statistically significant better performance, it also shows less
sensitivity to the underlying latent representation, more favourable memory
scaling, and produces better spatial anomaly maps. Code is available at
https://github.com/marksgraham/ddpm-ood",None,-1
Continual Road-Scene Semantic Segmentation via Feature-Aligned Symmetric Multi-Modal Network,0.566107,"State-of-the-art multimodal semantic segmentation strategies combining LiDAR
and color data are usually designed on top of asymmetric information-sharing
schemes and assume that both modalities are always available. This strong
assumption may not hold in real-world scenarios, where sensors are prone to
failure or can face adverse conditions that make the acquired information
unreliable. This problem is exacerbated when continual learning scenarios are
considered since they have stringent data reliability constraints. In this
work, we re-frame the task of multimodal semantic segmentation by enforcing a
tightly coupled feature representation and a symmetric information-sharing
scheme, which allows our approach to work even when one of the input modalities
is missing. We also introduce an ad-hoc class-incremental continual learning
scheme, proving our approach's effectiveness and reliability even in
safety-critical settings, such as autonomous driving. We evaluate our approach
on the SemanticKITTI dataset, achieving impressive performances.",None,-1
NoCoLA: The Norwegian Corpus of Linguistic Acceptability,0.480904,"While there has been a surge of large language models for Norwegian in recent
years, we lack any tool to evaluate their understanding of grammaticality. We
present two new Norwegian datasets for this task. NoCoLA_class is a supervised
binary classification task where the goal is to discriminate between acceptable
and non-acceptable sentences. On the other hand, NoCoLA_zero is a purely
diagnostic task for evaluating the grammatical judgement of a language model in
a completely zero-shot manner, i.e. without any further training. In this
paper, we describe both datasets in detail, show how to use them for different
flavors of language models, and conduct a comparative study of the existing
Norwegian language models.",None,-1
Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes,0.524935,"The waterdrops on windshields during driving can cause severe visual
obstructions, which may lead to car accidents. Meanwhile, the waterdrops can
also degrade the performance of a computer vision system in autonomous driving.
To address these issues, we propose an attention-based framework that fuses the
spatio-temporal representations from multiple frames to restore visual
information occluded by waterdrops. Due to the lack of training data for video
waterdrop removal, we propose a large-scale synthetic dataset with simulated
waterdrops in complex driving scenes on rainy days. To improve the generality
of our proposed method, we adopt a cross-modality training strategy that
combines synthetic videos and real-world images. Extensive experiments show
that our proposed method can generalize well and achieve the best waterdrop
removal performance in complex real-world driving scenes.",None,-1
Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning,0.089403,"The Bidirectional Encoder Representations from Transformers (BERT) were
proposed in the natural language process (NLP) and shows promising results.
Recently researchers applied the BERT to source-code representation learning
and reported some good news on several downstream tasks. However, in this
paper, we illustrated that current methods cannot effectively understand the
logic of source codes. The representation of source code heavily relies on the
programmer-defined variable and function names. We design and implement a set
of experiments to demonstrate our conjecture and provide some insights for
future works.",None,-1
An Analysis of Abstractive Text Summarization Using Pre-trained Models,0.352864,"People nowadays use search engines like Google, Yahoo, and Bing to find
information on the Internet. Due to explosion in data, it is helpful for users
if they are provided relevant summaries of the search results rather than just
links to webpages. Text summarization has become a vital approach to help
consumers swiftly grasp vast amounts of information.In this paper, different
pre-trained models for text summarization are evaluated on different datasets.
Specifically, we have used three different pre-trained models, namely,
google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have
considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum
to get the output from the above three models. The pre-trained models are
compared over these different datasets, each of 2000 examples, through ROUGH
and BLEU metrics.",None,-1
Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI,0.867297,"Explainable AI (XAI) systems are sociotechnical in nature; thus, they are
subject to the sociotechnical gap--divide between the technical affordances and
the social needs. However, charting this gap is challenging. In the context of
XAI, we argue that charting the gap improves our problem understanding, which
can reflexively provide actionable insights to improve explainability.
Utilizing two case studies in distinct domains, we empirically derive a
framework that facilitates systematic charting of the sociotechnical gap by
connecting AI guidelines in the context of XAI and elucidating how to use them
to address the gap. We apply the framework to a third case in a new domain,
showcasing its affordances. Finally, we discuss conceptual implications of the
framework, share practical considerations in its operationalization, and offer
guidance on transferring it to new contexts. By making conceptual and practical
contributions to understanding the sociotechnical gap in XAI, the framework
expands the XAI design space.",None,-1
HoloDiffusion: Training a 3D Diffusion Model using 2D Images,0.821449,"Diffusion models have emerged as the best approach for generative modeling of
2D images. Part of their success is due to the possibility of training them on
millions if not billions of images with a stable learning objective. However,
extending these models to 3D remains difficult for two reasons. First, finding
a large quantity of 3D training data is much more complex than for 2D images.
Second, while it is conceptually trivial to extend the models to operate on 3D
rather than 2D grids, the associated cubic growth in memory and compute
complexity makes this infeasible. We address the first challenge by introducing
a new diffusion setup that can be trained, end-to-end, with only posed 2D
images for supervision; and the second challenge by proposing an image
formation model that decouples model memory from spatial memory. We evaluate
our method on real-world data, using the CO3D dataset which has not been used
to train 3D generative models before. We show that our diffusion models are
scalable, train robustly, and are competitive in terms of sample quality and
fidelity to existing approaches for 3D generative modeling.",None,-1
Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention,0.12904,"Deep learning has shown excellent performance in analysing medical images.
However, datasets are difficult to obtain due privacy issues, standardization
problems, and lack of annotations. We address these problems by producing
realistic synthetic images using a combination of 3D technologies and
generative adversarial networks. We propose CUT-seg, a joint training where a
segmentation model and a generative model are jointly trained to produce
realistic images while learning to segment polyps. We take advantage of recent
one-sided translation models because they use significantly less memory,
allowing us to add a segmentation model in the training loop. CUT-seg performs
better, is computationally less expensive, and requires less real images than
other memory-intensive image translation approaches that require two stage
training. Promising results are achieved on five real polyp segmentation
datasets using only one real image and zero real annotations. As a part of this
study we release Synth-Colon, an entirely synthetic dataset that includes 20000
realistic colon images and additional details about depth and 3D geometry:
https://enric1994.github.io/synth-colon",None,-1
Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,0.885462,"Performant Convolutional Neural Network (CNN) architectures must be tailored
to specific tasks in order to consider the length, resolution, and
dimensionality of the input data. In this work, we tackle the need for
problem-specific CNN architectures. We present the Continuous Convolutional
Neural Network (CCNN): a single CNN able to process data of arbitrary
resolution, dimensionality and length without any structural changes. Its key
component are its continuous convolutional kernels which model long-range
dependencies at every layer, and thus remove the need of current CNN
architectures for task-dependent downsampling and depths. We showcase the
generality of our method by using the same architecture for tasks on sequential
($1{\rm D}$), visual ($2{\rm D}$) and point-cloud ($3{\rm D}$) data. Our CCNN
matches and often outperforms the current state-of-the-art across all tasks
considered.",None,-1
Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations,0.825474,"Large language models (LLMs) can generate fluent natural language texts when
given relevant documents as background context. This ability has attracted
considerable interest in developing industry applications of LLMs. However,
LLMs are prone to generate hallucinations that are not supported by the
provided sources. In this paper, we propose a hierarchical framework to detect
and mitigate such ungrounded hallucination. Our framework uses Chain of Natural
Language Inference (CoNLI) for hallucination detection and hallucination
reduction via post-editing. Our approach achieves state-of-the-art performance
on hallucination detection and enhances text quality through rewrite, using
LLMs without any fine-tuning or domain-specific prompt engineering. We show
that this simple plug-and-play framework can serve as an effective choice for
hallucination detection and reduction, achieving competitive performance across
various contexts.",None,-1
Efficient Transformer-based 3D Object Detection with Dynamic Token Halting,0.220714,"Balancing efficiency and accuracy is a long-standing problem for deploying
deep learning models. The trade-off is even more important for real-time
safety-critical systems like autonomous vehicles. In this paper, we propose an
effective approach for accelerating transformer-based 3D object detectors by
dynamically halting tokens at different layers depending on their contribution
to the detection task. Although halting a token is a non-differentiable
operation, our method allows for differentiable end-to-end learning by
leveraging an equivalent differentiable forward-pass. Furthermore, our
framework allows halted tokens to be reused to inform the model's predictions
through a straightforward token recycling mechanism. Our method significantly
improves the Pareto frontier of efficiency versus accuracy when compared with
the existing approaches. By halting tokens and increasing model capacity, we
are able to improve the baseline model's performance without increasing the
model's latency on the Waymo Open Dataset.",None,-1
InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction,0.0770435,"Learning template based information extraction from documents is a crucial
yet difficult task. Prior template-based IE approaches assume foreknowledge of
the domain templates; however, real-world IE do not have pre-defined schemas
and it is a figure-out-as you go phenomena. To quickly bootstrap templates in a
real-world setting, we need to induce template slots from documents with zero
or minimal supervision. Since the purpose of question answering intersect with
the goal of information extraction, we use automatic question generation to
induce template slots from the documents and investigate how a tiny amount of a
proxy human-supervision on-the-fly (termed as InteractiveIE) can further boost
the performance. Extensive experiments on biomedical and legal documents, where
obtaining training data is expensive, reveal encouraging trends of performance
improvement using InteractiveIE over AI-only baseline.",None,-1
ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing,0.893339,"English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is available only for research purposes.",None,-1
PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers,0.310626,"Existing methods of multi-person video 3D human Pose and Shape Estimation
(PSE) typically adopt a two-stage strategy, which first detects human instances
in each frame and then performs single-person PSE with temporal model. However,
the global spatio-temporal context among spatial instances can not be captured.
In this paper, we propose a new end-to-end multi-person 3D Pose and Shape
estimation framework with progressive Video Transformer, termed PSVT. In PSVT,
a spatio-temporal encoder (STE) captures the global feature dependencies among
spatial objects. Then, spatio-temporal pose decoder (STPD) and shape decoder
(STSD) capture the global dependencies between pose queries and feature tokens,
shape queries and feature tokens, respectively. To handle the variances of
objects as time proceeds, a novel scheme of progressive decoding is used to
update pose and shape queries at each frame. Besides, we propose a novel
pose-guided attention (PGA) for shape decoder to better predict shape
parameters. The two components strengthen the decoder of PSVT to improve
performance. Extensive experiments on the four datasets show that PSVT achieves
stage-of-the-art results.",None,-1
"Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking",0.451024,"Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle
a variety of task-oriented dialogue domains without the cost of collecting
in-domain data. Existing works mainly study common data- or model-level
augmentation methods to enhance the generalization but fail to effectively
decouple the semantics of samples, limiting the zero-shot performance of DST.
In this paper, we present a simple and effective ""divide, conquer and combine""
solution, which explicitly disentangles the semantics of seen data, and
leverages the performance and robustness with the mixture-of-experts mechanism.
Specifically, we divide the seen data into semantically independent subsets and
train corresponding experts, the newly unseen samples are mapped and inferred
with mixture-of-experts with our designed ensemble inference. Extensive
experiments on MultiWOZ2.1 upon the T5-Adapter show our schema significantly
and consistently improves the zero-shot performance, achieving the SOTA on
settings without external knowledge, with only 10M trainable parameters1.",None,-1
Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models,0.527267,"We analyze sentiment analysis and toxicity detection models to detect the
presence of explicit bias against people with disability (PWD). We employ the
bias identification framework of Perturbation Sensitivity Analysis to examine
conversations related to PWD on social media platforms, specifically Twitter
and Reddit, in order to gain insight into how disability bias is disseminated
in real-world social settings. We then create the \textit{Bias Identification
Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any
sentiment analysis and toxicity detection models. Our study utilizes BITS to
uncover significant biases in four open AIaaS (AI as a Service) sentiment
analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API,
DistilBERT and two toxicity detection models, namely two versions of
Toxic-BERT. Our findings indicate that all of these models exhibit
statistically significant explicit bias against PWD.",None,-1
ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT,0.879127,"In this paper, we investigate the use of data obtained from prompting a large
generative language model, ChatGPT, to generate synthetic training data with
the aim of augmenting data in low resource scenarios. We show that with
appropriate task-specific ChatGPT prompts, we outperform the most popular
existing approaches for such data augmentation. Furthermore, we investigate
methodologies for evaluating the similarity of the augmented data generated
from ChatGPT with the aim of validating and assessing the quality of the data
generated.",None,-1
Beyond Toxic: Toxicity Detection Datasets are Not Enough for Brand Safety,0.0455853,"The rapid growth in user generated content on social media has resulted in a
significant rise in demand for automated content moderation. Various methods
and frameworks have been proposed for the tasks of hate speech detection and
toxic comment classification. In this work, we combine common datasets to
extend these tasks to brand safety. Brand safety aims to protect commercial
branding by identifying contexts where advertisements should not appear and
covers not only toxicity, but also other potentially harmful content. As these
datasets contain different label sets, we approach the overall problem as a
binary classification task. We demonstrate the need for building brand safety
specific datasets via the application of common toxicity detection datasets to
a subset of brand safety and empirically analyze the effects of weighted
sampling strategies in text classification.",None,-1
Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection,0.672244,"Prompt tuning with large-scale pretrained vision-language models empowers
open-vocabulary predictions trained on limited base categories, e.g., object
classification and detection. In this paper, we propose compositional prompt
tuning with motion cues: an extended prompt tuning paradigm for compositional
predictions of video data. In particular, we present Relation Prompt (RePro)
for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where
conventional prompt tuning is easily biased to certain subject-object
combinations and motion patterns. To this end, RePro addresses the two
technical challenges of Open-VidVRD: 1) the prompt tokens should respect the
two different semantic roles of subject and object, and 2) the tuning should
account for the diverse spatio-temporal motion patterns of the subject-object
compositions. Without bells and whistles, our RePro achieves a new
state-of-the-art performance on two VidVRD benchmarks of not only the base
training object and predicate categories, but also the unseen ones. Extensive
ablations also demonstrate the effectiveness of the proposed compositional and
multi-mode design of prompts. Code is available at
https://github.com/Dawn-LX/OpenVoc-VidVRD.",None,-1
Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems,0.946008,"This report describes a test of the large language model GPT-4 with the
Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in
science and math, at the high school and college levels, carried out in
June-August 2023. Our tests suggest that the plug-ins significantly enhance
GPT's ability to solve these problems. Having said that, there are still often
""interface"" failures; that is, GPT often has trouble formulating problems in a
way that elicits useful answers from the plug-ins. Fixing these interface
failures seems like a central challenge in making GPT a reliable tool for
college-level calculation problems.",None,-1
Empirical study of the modulus as activation function in computer vision applications,0.509669,"In this work we propose a new non-monotonic activation function: the modulus.
The majority of the reported research on nonlinearities is focused on monotonic
functions. We empirically demonstrate how by using the modulus activation
function on computer vision tasks the models generalize better than with other
nonlinearities - up to a 15% accuracy increase in CIFAR100 and 4% in CIFAR10,
relative to the best of the benchmark activations tested. With the proposed
activation function the vanishing gradient and dying neurons problems
disappear, because the derivative of the activation function is always 1 or -1.
The simplicity of the proposed function and its derivative make this solution
specially suitable for TinyML and hardware applications.",None,-1
Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification,0.119139,"Intent classification (IC) plays an important role in task-oriented dialogue
systems. However, IC models often generalize poorly when training without
sufficient annotated examples for each user intent. We propose a novel
pre-training method for text encoders that uses contrastive learning with
intent psuedo-labels to produce embeddings that are well-suited for IC tasks,
reducing the need for manual annotations. By applying this pre-training
strategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is
designed to align encodings of utterances with their intent names.
Specifically, we first train a tagger to identify key phrases within utterances
that are crucial for interpreting intents. We then use these extracted phrases
to create examples for pre-training a text encoder in a contrastive manner. As
a result, our PIE model achieves up to 5.4% and 4.0% higher accuracy than the
previous state-of-the-art text encoder for the N-way zero- and one-shot
settings on four IC datasets.",None,-1
A Few-Shot Attention Recurrent Residual U-Net for Crack Segmentation,0.2181,"Recent studies indicate that deep learning plays a crucial role in the
automated visual inspection of road infrastructures. However, current learning
schemes are static, implying no dynamic adaptation to users' feedback. To
address this drawback, we present a few-shot learning paradigm for the
automated segmentation of road cracks, which is based on a U-Net architecture
with recurrent residual and attention modules (R2AU-Net). The retraining
strategy dynamically fine-tunes the weights of the U-Net as a few new rectified
samples are being fed into the classifier. Extensive experiments show that the
proposed few-shot R2AU-Net framework outperforms other state-of-the-art
networks in terms of Dice and IoU metrics, on a new dataset, named CrackMap,
which is made publicly available at https://github.com/ikatsamenis/CrackMap.",None,-1
Proving Conjectures Acquired by Composing Multiple Biases,0.552112,"We present the proofs of the conjectures mentioned in the paper published in
the proceedings of the 2024 AAAI conference [1], and discovered by the
decomposition methods presented in the same paper.",None,-1
The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis,0.289069,"Recently, ChatGPT has attracted great attention from both industry and
academia due to its surprising abilities in natural language understanding and
generation. We are particularly curious about whether it can achieve promising
performance on one of the most complex tasks in aspect-based sentiment
analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from
texts. To this end, in this paper we develop a specialized prompt template that
enables ChatGPT to effectively tackle this complex quadruple extraction task.
Further, we propose a selection method on few-shot examples to fully exploit
the in-context learning ability of ChatGPT and uplift its effectiveness on this
complex task. Finally, we provide a comparative evaluation on ChatGPT against
existing state-of-the-art quadruple extraction models based on four public
datasets and highlight some important findings regarding the capability
boundaries of ChatGPT in the quadruple extraction.",None,-1
"Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals",0.428229,"Counterfactual explanations are an increasingly popular form of post hoc
explanation due to their (i) applicability across problem domains, (ii)
proposed legal compliance (e.g., with GDPR), and (iii) reliance on the
contrastive nature of human explanation. Although counterfactual explanations
are normally used to explain individual predictive-instances, we explore a
novel use case in which groups of similar instances are explained in a
collective fashion using ``group counterfactuals'' (e.g., to highlight a
repeating pattern of illness in a group of patients). These group
counterfactuals meet a human preference for coherent, broad explanations
covering multiple events/instances. A novel, group-counterfactual algorithm is
proposed to generate high-coverage explanations that are faithful to the
to-be-explained model. This explanation strategy is also evaluated in a large,
controlled user study (N=207), using objective (i.e., accuracy) and subjective
(i.e., confidence, explanation satisfaction, and trust) psychological measures.
The results show that group counterfactuals elicit modest but definite
improvements in people's understanding of an AI system. The implications of
these findings for counterfactual methods and for XAI are discussed.",None,-1
A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces,0.170465,"We study semantic construal in grammatical constructions using large language
models. First, we project contextual word embeddings into three interpretable
semantic spaces, each defined by a different set of psycholinguistic feature
norms. We validate these interpretable spaces and then use them to
automatically derive semantic characterizations of lexical items in two
grammatical constructions: nouns in subject or object position within the same
sentence, and the AANN construction (e.g., `a beautiful three days'). We show
that a word in subject position is interpreted as more agentive than the very
same word in object position, and that the nouns in the AANN construction are
interpreted as more measurement-like than when in the canonical alternation.
Our method can probe the distributional meaning of syntactic constructions at a
templatic level, abstracted away from specific lexemes.",None,-1
Knowledge Graph Embedding: An Overview,0.272004,"Many mathematical models have been leveraged to design embeddings for
representing Knowledge Graph (KG) entities and relations for link prediction
and many downstream tasks. These mathematically-inspired models are not only
highly scalable for inference in large KGs, but also have many explainable
advantages in modeling different relation patterns that can be validated
through both formal proofs and empirical results. In this paper, we make a
comprehensive overview of the current state of research in KG completion. In
particular, we focus on two main branches of KG embedding (KGE) design: 1)
distance-based methods and 2) semantic matching-based methods. We discover the
connections between recently proposed models and present an underlying trend
that might help researchers invent novel and more effective models. Next, we
delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D
affine operations, respectively. They encompass a broad spectrum of techniques
including distance-based and semantic-based methods. We will also discuss an
emerging approach for KG completion which leverages pre-trained language models
(PLMs) and textual descriptions of entities and relations and offer insights
into the integration of KGE embedding methods with PLMs for KG completion.",None,-1
Automated Action Model Acquisition from Narrative Texts,0.596158,"Action models, which take the form of precondition/effect axioms, facilitate
causal and motivational connections between actions for AI agents. Action model
acquisition has been identified as a bottleneck in the application of planning
technology, especially within narrative planning. Acquiring action models from
narrative texts in an automated way is essential, but challenging because of
the inherent complexities of such texts. We present NaRuto, a system that
extracts structured events from narrative text and subsequently generates
planning-language-style action models based on predictions of commonsense event
relations, as well as textual contradictions and similarities, in an
unsupervised manner. Experimental results in classical narrative planning
domains show that NaRuto can generate action models of significantly better
quality than existing fully automated methods, and even on par with those of
semi-automated methods.",None,-1
Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition,0.896655,"The integration of Language Models (LMs) has proven to be an effective way to
address domain shifts in speech recognition. However, these approaches usually
require a significant amount of target domain text data for the training of
LMs. Different from these methods, in this work, with only a domain-specific
text prompt, we propose two zero-shot ASR domain adaptation methods using
LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two
ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR
system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an
encoder-decoder based ASR system. Experiments show that, with only one domain
prompt, both methods can effectively reduce word error rates (WER) on
out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep
LLM-fusion has the advantage of better recall of entity and out-of-vocabulary
words.",None,-1
InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,0.651931,"Inferring traffic object such as lane information is of foremost importance
for deployment of autonomous driving. Previous approaches focus on offline
construction of HD map inferred with GPS localization, which is insufficient
for globally scalable autonomous driving. To alleviate these issues, we propose
online HD map learning framework that detects HD map elements from onboard
sensor observations. We represent the map elements as a graph; we propose
InstaGraM, instance-level graph modeling of HD map that brings accurate and
fast end-to-end vectorized HD map learning. Along with the graph modeling
strategy, we propose end-to-end neural network composed of three stages: a
unified BEV feature extraction, map graph component detection, and association
via graph neural networks. Comprehensive experiments on public open dataset
show that our proposed network outperforms previous models by up to 13.7 mAP
with up to 33.8X faster computation time.",None,-1
Salient Span Masking for Temporal Understanding,0.589817,"Salient Span Masking (SSM) has shown itself to be an effective strategy to
improve closed-book question answering performance. SSM extends general masked
language model pretraining by creating additional unsupervised training
sentences that mask a single entity or date span, thus oversampling factual
information. Despite the success of this paradigm, the span types and sampling
strategies are relatively arbitrary and not widely studied for other tasks.
Thus, we investigate SSM from the perspective of temporal tasks, where learning
a good representation of various temporal expressions is important. To that
end, we introduce Temporal Span Masking (TSM) intermediate training. First, we
find that SSM alone improves the downstream performance on three temporal tasks
by an avg. +5.8 points. Further, we are able to achieve additional improvements
(avg. +0.29 points) by adding the TSM task. These comprise the new best
reported results on the targeted tasks. Our analysis suggests that the
effectiveness of SSM stems from the sentences chosen in the training data
rather than the mask choice: sentences with entities frequently also contain
temporal expressions. Nonetheless, the additional targeted spans of TSM can
still improve performance, especially in a zero-shot context.",None,-1
Real-Time Simultaneous Localization and Mapping with LiDAR intensity,0.582636,"We propose a novel real-time LiDAR intensity image-based simultaneous
localization and mapping method , which addresses the geometry degeneracy
problem in unstructured environments. Traditional LiDAR-based front-end
odometry mostly relies on geometric features such as points, lines and planes.
A lack of these features in the environment can lead to the failure of the
entire odometry system. To avoid this problem, we extract feature points from
the LiDAR-generated point cloud that match features identified in LiDAR
intensity images. We then use the extracted feature points to perform scan
registration and estimate the robot ego-movement. For the back-end, we jointly
optimize the distance between the corresponding feature points, and the point
to plane distance for planes identified in the map. In addition, we use the
features extracted from intensity images to detect loop closure candidates from
previous scans and perform pose graph optimization. Our experiments show that
our method can run in real time with high accuracy and works well with
illumination changes, low-texture, and unstructured environments.",None,-1
Super-Resolution Neural Operator,0.921305,"We propose Super-resolution Neural Operator (SRNO), a deep operator learning
framework that can resolve high-resolution (HR) images at arbitrary scales from
the low-resolution (LR) counterparts. Treating the LR-HR image pairs as
continuous functions approximated with different grid sizes, SRNO learns the
mapping between the corresponding function spaces. From the perspective of
approximation theory, SRNO first embeds the LR input into a higher-dimensional
latent representation space, trying to capture sufficient basis functions, and
then iteratively approximates the implicit image function with a kernel
integral mechanism, followed by a final dimensionality reduction step to
generate the RGB representation at the target coordinates. The key
characteristics distinguishing SRNO from prior continuous SR works are: 1) the
kernel integral in each layer is efficiently implemented via the Galerkin-type
attention, which possesses non-local properties in the spatial domain and
therefore benefits the grid-free continuum; and 2) the multilayer attention
architecture allows for the dynamic latent basis update, which is crucial for
SR problems to ""hallucinate"" high-frequency information from the LR image.
Experiments show that SRNO outperforms existing continuous SR methods in terms
of both accuracy and running time. Our code is at
https://github.com/2y7c3/Super-Resolution-Neural-Operator",None,-1
MoSFPAD: An end-to-end Ensemble of MobileNet and Support Vector Classifier for Fingerprint Presentation Attack Detection,0.756462,"Automatic fingerprint recognition systems are the most extensively used
systems for person authentication although they are vulnerable to Presentation
attacks. Artificial artifacts created with the help of various materials are
used to deceive these systems causing a threat to the security of
fingerprint-based applications. This paper proposes a novel end-to-end model to
detect fingerprint Presentation attacks. The proposed model incorporates
MobileNet as a feature extractor and a Support Vector Classifier as a
classifier to detect presentation attacks in cross-material and cross-sensor
paradigms. The feature extractor's parameters are learned with the loss
generated by the support vector classifier. The proposed model eliminates the
need for intermediary data preparation procedures, unlike other static hybrid
architectures. The performance of the proposed model has been validated on
benchmark LivDet 2011, 2013, 2015, 2017, and 2019 databases, and overall
accuracy of 98.64%, 99.50%, 97.23%, 95.06%, and 95.20% is achieved on these
databases, respectively. The performance of the proposed model is compared with
state-of-the-art methods and the proposed method outperforms in cross-material
and cross-sensor paradigms in terms of average classification error.",None,-1
Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation,0.333623,"The images and sounds that we perceive undergo subtle but geometrically
consistent changes as we rotate our heads. In this paper, we use these cues to
solve a problem we call Sound Localization from Motion (SLfM): jointly
estimating camera rotation and localizing sound sources. We learn to solve
these tasks solely through self-supervision. A visual model predicts camera
rotation from a pair of images, while an audio model predicts the direction of
sound sources from binaural sounds. We train these models to generate
predictions that agree with one another. At test time, the models can be
deployed independently. To obtain a feature representation that is well-suited
to solving this challenging problem, we also propose a method for learning an
audio-visual representation through cross-view binauralization: estimating
binaural sound from one view, given images and sound from another. Our model
can successfully estimate accurate rotations on both real and synthetic scenes,
and localize sound sources with accuracy competitive with state-of-the-art
self-supervised approaches. Project site: https://ificl.github.io/SLfM/",None,-1
T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation,0.90464,"Despite the stunning ability to generate high-quality images by recent
text-to-image models, current approaches often struggle to effectively compose
objects with different attributes and relationships into a complex and coherent
scene. We propose T2I-CompBench, a comprehensive benchmark for open-world
compositional text-to-image generation, consisting of 6,000 compositional text
prompts from 3 categories (attribute binding, object relationships, and complex
compositions) and 6 sub-categories (color binding, shape binding, texture
binding, spatial relationships, non-spatial relationships, and complex
compositions). We further propose several evaluation metrics specifically
designed to evaluate compositional text-to-image generation and explore the
potential and limitations of multimodal LLMs for evaluation. We introduce a new
approach, Generative mOdel fine-tuning with Reward-driven Sample selection
(GORS), to boost the compositional text-to-image generation abilities of
pretrained text-to-image models. Extensive experiments and evaluations are
conducted to benchmark previous methods on T2I-CompBench, and to validate the
effectiveness of our proposed evaluation metrics and GORS approach. Project
page is available at https://karine-h.github.io/T2I-CompBench/.",None,-1
RoboBEV: Towards Robust Bird's Eye View Perception under Corruptions,0.922749,"The recent advances in camera-based bird's eye view (BEV) representation
exhibit great potential for in-vehicle 3D perception. Despite the substantial
progress achieved on standard benchmarks, the robustness of BEV algorithms has
not been thoroughly examined, which is critical for safe operations. To bridge
this gap, we introduce RoboBEV, a comprehensive benchmark suite that
encompasses eight distinct corruptions, including Bright, Dark, Fog, Snow,
Motion Blur, Color Quant, Camera Crash, and Frame Lost. Based on it, we
undertake extensive evaluations across a wide range of BEV-based models to
understand their resilience and reliability. Our findings indicate a strong
correlation between absolute performance on in-distribution and
out-of-distribution datasets. Nonetheless, there are considerable variations in
relative performance across different approaches. Our experiments further
demonstrate that pre-training and depth-free BEV transformation has the
potential to enhance out-of-distribution robustness. Additionally, utilizing
long and rich temporal information largely helps with robustness. Our findings
provide valuable insights for designing future BEV models that can achieve both
accuracy and robustness in real-world deployments.",None,-1
PandaGPT: One Model To Instruction-Follow Them All,0.999999,"We present PandaGPT, an approach to emPower large lANguage moDels with visual
and Auditory instruction-following capabilities. Our pilot experiments show
that PandaGPT can perform complex tasks such as detailed image description
generation, writing stories inspired by videos, and answering questions about
audios. More interestingly, PandaGPT can take multimodal inputs simultaneously
and compose their semantics naturally. For example, PandaGPT can connect how
objects look in an image/video and how they sound in an audio. To do so,
PandaGPT combines the multimodal encoders from ImageBind and the large language
models from Vicuna. Notably, only aligned image-text pairs are required for the
training of PandaGPT. Thanks to the strong capability of ImageBind in embedding
data from different modalities into the same space, PandaGPT displays emergent,
i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g.,
video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an
initial step toward building AGI that can perceive and understand inputs in
different modalities holistically, as we humans do. Our project page is at
https://panda-gpt.github.io/.",None,-1
BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,0.709024,"In this paper, we introduce the BeaverTails dataset, aimed at fostering
research on safety alignment in large language models (LLMs). This dataset
uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have gathered safety meta-labels for 333,963
question-answer (QA) pairs and 361,903 pairs of expert comparison data for both
the helpfulness and harmlessness metrics. We further showcase applications of
BeaverTails in content moderation and reinforcement learning with human
feedback (RLHF), emphasizing its potential for practical safety measures in
LLMs. We believe this dataset provides vital resources for the community,
contributing towards the safe development and deployment of LLMs. Our project
page is available at the following URL:
https://sites.google.com/view/pku-beavertails.",None,-1
Autocorrelations Decay in Texts and Applicability Limits of Language Models,0.0198413,"We show that the laws of autocorrelations decay in texts are closely related
to applicability limits of language models. Using distributional semantics we
empirically demonstrate that autocorrelations of words in texts decay according
to a power law. We show that distributional semantics provides coherent
autocorrelations decay exponents for texts translated to multiple languages.
The autocorrelations decay in generated texts is quantitatively and often
qualitatively different from the literary texts. We conclude that language
models exhibiting Markov behavior, including large autoregressive language
models, may have limitations when applied to long texts, whether analysis or
generation.",None,-1
Efficient Computation of Counterfactual Bounds,0.27069,"We assume to be given structural equations over discrete variables inducing a
directed acyclic graph, namely, a structural causal model, together with data
about its internal nodes. The question we want to answer is how we can compute
bounds for partially identifiable counterfactual queries from such an input. We
start by giving a map from structural casual models to credal networks. This
allows us to compute exact counterfactual bounds via algorithms for credal nets
on a subclass of structural causal models. Exact computation is going to be
inefficient in general given that, as we show, causal inference is NP-hard even
on polytrees. We target then approximate bounds via a causal EM scheme. We
evaluate their accuracy by providing credible intervals on the quality of the
approximation; we show through a synthetic benchmark that the EM scheme
delivers accurate results in a fair number of runs. In the course of the
discussion, we also point out what seems to be a neglected limitation to the
trending idea that counterfactual bounds can be computed without knowledge of
the structural equations. We also present a real case study on palliative care
to show how our algorithms can readily be used for practical purposes.",None,-1
SkinSAM: Empowering Skin Cancer Segmentation with Segment Anything Model,0.963174,"Skin cancer is a prevalent and potentially fatal disease that requires
accurate and efficient diagnosis and treatment. Although manual tracing is the
current standard in clinics, automated tools are desired to reduce human labor
and improve accuracy. However, developing such tools is challenging due to the
highly variable appearance of skin cancers and complex objects in the
background. In this paper, we present SkinSAM, a fine-tuned model based on the
Segment Anything Model that showed outstanding segmentation performance. The
models are validated on HAM10000 dataset which includes 10015 dermatoscopic
images. While larger models (ViT_L, ViT_H) performed better than the smaller
one (ViT_b), the finetuned model (ViT_b_finetuned) exhibited the greatest
improvement, with a Mean pixel accuracy of 0.945, Mean dice score of 0.8879,
and Mean IoU score of 0.7843. Among the lesion types, vascular lesions showed
the best segmentation results. Our research demonstrates the great potential of
adapting SAM to medical image segmentation tasks.",None,-1
RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue,0.687599,"Evaluating open-domain dialogue systems is challenging for reasons such as
the one-to-many problem, i.e., many appropriate responses other than just the
golden response. As of now, automatic evaluation methods need better
consistency with humans, while reliable human evaluation can be time- and
cost-intensive. To this end, we propose the Reference-Assisted Dialogue
Evaluation (RADE) approach under the multi-task learning framework, which
leverages the pre-created utterance as reference other than the gold response
to relief the one-to-many problem. Specifically, RADE explicitly compares
reference and the candidate response to predict their overall scores. Moreover,
an auxiliary response generation task enhances prediction via a shared encoder.
To support RADE, we extend three datasets with additional rated responses other
than just a golden response by human annotation. Experiments on our three
datasets and two existing benchmarks demonstrate the effectiveness of our
method, where Pearson, Spearman, and Kendall correlations with human evaluation
outperform state-of-the-art baselines.",None,-1
"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",0.994502,"Large Language Models (LLMs) have made remarkable strides in various tasks.
Whether LLMs are competitive few-shot solvers for information extraction (IE)
tasks, however, remains an open problem. In this work, we aim to provide a
thorough answer to this question. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate that current advanced LLMs
consistently exhibit inferior performance, higher latency, and increased budget
requirements compared to fine-tuned SLMs under most settings. Therefore, we
conclude that LLMs are not effective few-shot information extractors in
general. Nonetheless, we illustrate that with appropriate prompting strategies,
LLMs can effectively complement SLMs and tackle challenging samples that SLMs
struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm
to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as
filters and LLMs serve as rerankers. By prompting LLMs to rerank a small
portion of difficult samples identified by SLMs, our preliminary system
consistently achieves promising improvements (2.4% F1-gain on average) on
various IE tasks, with an acceptable time and cost investment.",None,-1
Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models,0.503423,"Text-to-image (T2I) personalization allows users to guide the creative image
generation process by combining their own visual concepts in natural language
prompts. Recently, encoder-based techniques have emerged as a new effective
approach for T2I personalization, reducing the need for multiple images and
long training times. However, most existing encoders are limited to a
single-class domain, which hinders their ability to handle diverse concepts. In
this work, we propose a domain-agnostic method that does not require any
specialized dataset or prior information about the personalized concepts. We
introduce a novel contrastive-based regularization technique to maintain high
fidelity to the target concept characteristics while keeping the predicted
embeddings close to editable regions of the latent space, by pushing the
predicted tokens toward their nearest existing CLIP tokens. Our experimental
results demonstrate the effectiveness of our approach and show how the learned
tokens are more semantic than tokens predicted by unregularized models. This
leads to a better representation that achieves state-of-the-art performance
while being more flexible than previous methods.",None,-1
Learning Customized Visual Models with Retrieval-Augmented Knowledge,0.660302,"Image-text contrastive learning models such as CLIP have demonstrated strong
task transfer ability. The high generality and usability of these visual models
is achieved via a web-scale data collection process to ensure broad concept
coverage, followed by expensive pre-training to feed all the knowledge into
model weights. Alternatively, we propose REACT, REtrieval-Augmented
CusTomization, a framework to acquire the relevant web knowledge to build
customized visual models for target domains. We retrieve the most relevant
image-text pairs (~3% of CLIP pre-training data) from the web-scale database as
external knowledge, and propose to customize the model by only training new
modualized blocks while freezing all the original weights. The effectiveness of
REACT is demonstrated via extensive experiments on classification, retrieval,
detection and segmentation tasks, including zero, few, and full-shot settings.
Particularly, on the zero-shot classification task, compared with CLIP, it
achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark
(20 datasets).",None,-1
Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,0.340702,"Identifying disease phenotypes from electronic health records (EHRs) is
critical for numerous secondary uses. Manually encoding physician knowledge
into rules is particularly challenging for rare diseases due to inadequate EHR
coding, necessitating review of clinical notes. Large language models (LLMs)
offer promise in text understanding but may not efficiently handle real-world
clinical documentation. We propose a zero-shot LLM-based method enriched by
retrieval-augmented generation and MapReduce, which pre-identifies
disease-related text snippets to be used in parallel as queries for the LLM to
establish diagnosis. We show that this method as applied to pulmonary
hypertension (PH), a rare disease characterized by elevated arterial pressures
in the lungs, significantly outperforms physician logic rules ($F_1$ score of
0.62 vs. 0.75). This method has the potential to enhance rare disease cohort
identification, expanding the scope of robust clinical research and care gap
identification.",None,-1
Learnable Ophthalmology SAM,0.607272,"Segmentation is vital for ophthalmology image analysis. But its various modal
images hinder most of the existing segmentation algorithms applications, as
they rely on training based on a large number of labels or hold weak
generalization ability. Based on Segment Anything (SAM), we propose a simple
but effective learnable prompt layer suitable for multiple target segmentation
in ophthalmology multi-modal images, named Learnable Ophthalmology Segment
Anything (SAM). The learnable prompt layer learns medical prior knowledge from
each transformer layer. During training, we only train the prompt layer and
task head based on a one-shot mechanism. We demonstrate the effectiveness of
our thought based on four medical segmentation tasks based on nine publicly
available datasets. Moreover, we only provide a new improvement thought for
applying the existing fundamental CV models in the medical field. Our codes are
available at \href{https://github.com/Qsingle/LearnablePromptSAM}{website}.",None,-1
C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction,0.284628,"There is an emerging effort to combine the two popular 3D frameworks using
Multi-View Stereo (MVS) and Neural Implicit Surfaces (NIS) with a specific
focus on the few-shot / sparse view setting. In this paper, we introduce a
novel integration scheme that combines the multi-view stereo with neural signed
distance function representations, which potentially overcomes the limitations
of both methods. MVS uses per-view depth estimation and cross-view fusion to
generate accurate surfaces, while NIS relies on a common coordinate volume.
Based on this strategy, we propose to construct per-view cost frustum for finer
geometry estimation, and then fuse cross-view frustums and estimate the
implicit signed distance functions to tackle artifacts that are due to noise
and holes in the produced surface reconstruction. We further apply a cascade
frustum fusion strategy to effectively captures global-local information and
structural consistency. Finally, we apply cascade sampling and a
pseudo-geometric loss to foster stronger integration between the two
architectures. Extensive experiments demonstrate that our method reconstructs
robust surfaces and outperforms existing state-of-the-art methods.",None,-1
Explain-then-Translate: An Analysis on Improving Program Translation with Self-generated Explanations,0.646464,"This work explores the use of self-generated natural language explanations as
an intermediate step for code-to-code translation with language models. Across
three types of explanations and 19 programming languages constructed from the
MultiPL-E dataset, we find the explanations to be particularly effective in the
zero-shot case, improving performance by 12% on average. Improvements with
natural language explanations are particularly pronounced on difficult
programs. We release our dataset, code, and canonical solutions in all 19
languages.",None,-1
TextMesh: Generation of Realistic 3D Meshes From Text Prompts,0.986285,"The ability to generate highly realistic 2D images from mere text prompts has
recently made huge progress in terms of speed and quality, thanks to the advent
of image diffusion models. Naturally, the question arises if this can be also
achieved in the generation of 3D content from such text prompts. To this end, a
new line of methods recently emerged trying to harness diffusion models,
trained on 2D images, for supervision of 3D model generation using view
dependent prompts. While achieving impressive results, these methods, however,
have two major drawbacks. First, rather than commonly used 3D meshes, they
instead generate neural radiance fields (NeRFs), making them impractical for
most real applications. Second, these approaches tend to produce over-saturated
models, giving the output a cartoonish looking effect. Therefore, in this work
we propose a novel method for generation of highly realistic-looking 3D meshes.
To this end, we extend NeRF to employ an SDF backbone, leading to improved 3D
mesh extraction. In addition, we propose a novel way to finetune the mesh
texture, removing the effect of high saturation and improving the details of
the output 3D mesh.",None,-1
Towards Better Entity Linking with Multi-View Enhanced Distillation,0.224034,"Dense retrieval is widely used for entity linking to retrieve entities from
large-scale knowledge bases. Mainstream techniques are based on a dual-encoder
framework, which encodes mentions and entities independently and calculates
their relevances via rough interaction metrics, resulting in difficulty in
explicitly modeling multiple mention-relevant parts within entities to match
divergent mentions. Aiming at learning entity representations that can match
divergent mentions, this paper proposes a Multi-View Enhanced Distillation
(MVD) framework, which can effectively transfer knowledge of multiple
fine-grained and mention-relevant parts within entities from cross-encoders to
dual-encoders. Each entity is split into multiple views to avoid irrelevant
information being over-squashed into the mention-relevant view. We further
design cross-alignment and self-alignment mechanisms for this framework to
facilitate fine-grained knowledge distillation from the teacher model to the
student model. Meanwhile, we reserve a global-view that embeds the entity as a
whole to prevent dispersal of uniform information. Experiments show our method
achieves state-of-the-art performance on several entity linking benchmarks.",None,-1
Exploiting Abstract Meaning Representation for Open-Domain Question Answering,0.0994162,"The Open-Domain Question Answering (ODQA) task involves retrieving and
subsequently generating answers from fine-grained relevant passages within a
database. Current systems leverage Pretrained Language Models (PLMs) to model
the relationship between questions and passages. However, the diversity in
surface form expressions can hinder the model's ability to capture accurate
correlations, especially within complex contexts. Therefore, we utilize
Abstract Meaning Representation (AMR) graphs to assist the model in
understanding complex semantic information. We introduce a method known as
Graph-as-Token (GST) to incorporate AMRs into PLMs. Results from Natural
Questions (NQ) and TriviaQA (TQ) demonstrate that our GST method can
significantly improve performance, resulting in up to 2.44/3.17 Exact Match
score improvements on NQ/TQ respectively. Furthermore, our method enhances
robustness and outperforms alternative Graph Neural Network (GNN) methods for
integrating AMRs. To the best of our knowledge, we are the first to employ
semantic graphs in ODQA.",None,-1
Who's in Charge? Roles and Responsibilities of Decision-Making Components in Conversational Robots,0.376197,"Software architectures for conversational robots typically consist of
multiple modules, each designed for a particular processing task or
functionality. Some of these modules are developed for the purpose of making
decisions about the next action that the robot ought to perform in the current
context. Those actions may relate to physical movements, such as driving
forward or grasping an object, but may also correspond to communicative acts,
such as asking a question to the human user. In this position paper, we reflect
on the organization of those decision modules in human-robot interaction
platforms. We discuss the relative benefits and limitations of modular vs.
end-to-end architectures, and argue that, despite the increasing popularity of
end-to-end approaches, modular architectures remain preferable when developing
conversational robots designed to execute complex tasks in collaboration with
human users. We also show that most practical HRI architectures tend to be
either robot-centric or dialogue-centric, depending on where developers wish to
place the ``command center'' of their system. While those design choices may be
justified in some application domains, they also limit the robot's ability to
flexibly interleave physical movements and conversational behaviours. We
contend that architectures placing ``action managers'' and ``interaction
managers'' on an equal footing may provide the best path forward for future
human-robot interaction systems.",None,-1
Learning to Localize in Unseen Scenes with Relative Pose Regressors,0.116586,"Relative pose regressors (RPRs) localize a camera by estimating its relative
translation and rotation to a pose-labelled reference. Unlike scene coordinate
regression and absolute pose regression methods, which learn absolute scene
parameters, RPRs can (theoretically) localize in unseen environments, since
they only learn the residual pose between camera pairs. In practice, however,
the performance of RPRs is significantly degraded in unseen scenes. In this
work, we propose to aggregate paired feature maps into latent codes, instead of
operating on global image descriptors, in order to improve the generalization
of RPRs. We implement aggregation with concatenation, projection, and attention
operations (Transformer Encoders) and learn to regress the relative pose
parameters from the resulting latent codes. We further make use of a recently
proposed continuous representation of rotation matrices, which alleviates the
limitations of the commonly used quaternions. Compared to state-of-the-art
RPRs, our model is shown to localize significantly better in unseen
environments, across both indoor and outdoor benchmarks, while maintaining
competitive performance in seen scenes. We validate our findings and
architecture design through multiple ablations. Our code and pretrained models
is publicly available.",None,-1
Why Can Large Language Models Generate Correct Chain-of-Thoughts?,0.381856,"This paper delves into the capabilities of large language models (LLMs),
specifically focusing on advancing the theoretical comprehension of
chain-of-thought prompting. We investigate how LLMs can be effectively induced
to generate a coherent chain of thoughts. To achieve this, we introduce a
two-level hierarchical graphical model tailored for natural language
generation. Within this framework, we establish a compelling geometrical
convergence rate that gauges the likelihood of an LLM-generated chain of
thoughts compared to those originating from the true language. Our findings
provide a theoretical justification for the ability of LLMs to produce the
correct sequence of thoughts (potentially) explaining performance gains in
tasks demanding reasoning skills.",None,-1
CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval,0.194958,"Growing techniques have been emerging to improve the performance of passage
retrieval. As an effective representation bottleneck pretraining technique, the
contextual masked auto-encoder utilizes contextual embedding to assist in the
reconstruction of passages. However, it only uses a single auto-encoding
pre-task for dense representation pre-training. This study brings multi-view
modeling to the contextual masked auto-encoder. Firstly, multi-view
representation utilizes both dense and sparse vectors as multi-view
representations, aiming to capture sentence semantics from different aspects.
Moreover, multiview decoding paradigm utilizes both autoencoding and
auto-regressive decoders in representation bottleneck pre-training, aiming to
provide both reconstructive and generative signals for better contextual
representation pretraining. We refer to this multi-view pretraining method as
CoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective
and robust on large-scale passage retrieval benchmarks and out-of-domain
zero-shot benchmarks.",None,-1
Boundary-weighted logit consistency improves calibration of segmentation networks,0.0743001,"Neural network prediction probabilities and accuracy are often only
weakly-correlated. Inherent label ambiguity in training data for image
segmentation aggravates such miscalibration. We show that logit consistency
across stochastic transformations acts as a spatially varying regularizer that
prevents overconfident predictions at pixels with ambiguous labels. Our
boundary-weighted extension of this regularizer provides state-of-the-art
calibration for prostate and heart MRI segmentation.",None,-1
"Industrial Segment Anything -- a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul",0.538005,"Deploying deep learning-based applications in specialized domains like the
aircraft production industry typically suffers from the training data
availability problem. Only a few datasets represent non-everyday objects,
situations, and tasks. Recent advantages in research around Vision Foundation
Models (VFM) opened a new area of tasks and models with high generalization
capabilities in non-semantic and semantic predictions. As recently demonstrated
by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a
promising direction in tackling the boundaries spanned by data, context, and
sensor variety. Although, investigating its application within specific domains
is subject to ongoing research. This paper contributes here by surveying
applications of the SAM in aircraft production-specific use cases. We include
manufacturing, intralogistics, as well as maintenance, repair, and overhaul
processes, also representing a variety of other neighboring industrial domains.
Besides presenting the various use cases, we further discuss the injection of
domain knowledge.",None,-1
Neuromorphic Event-based Facial Expression Recognition,0.949129,"Recently, event cameras have shown large applicability in several computer
vision fields especially concerning tasks that require high temporal
resolution. In this work, we investigate the usage of such kind of data for
emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based
Facial Expression Recognition. NEFER is composed of paired RGB and event videos
representing human faces labeled with the respective emotions and also
annotated with face bounding boxes and facial landmarks. We detail the data
acquisition process as well as providing a baseline method for RGB and event
data. The collected data captures subtle micro-expressions, which are hard to
spot with RGB data, yet emerge in the event domain. We report a double
recognition accuracy for the event-based approach, proving the effectiveness of
a neuromorphic approach for analyzing fast and hardly detectable expressions
and the emotions they conceal.",None,-1
Spatiotemporal Self-supervised Learning for Point Clouds in the Wild,0.910958,"Self-supervised learning (SSL) has the potential to benefit many
applications, particularly those where manually annotating data is cumbersome.
One such situation is the semantic segmentation of point clouds. In this
context, existing methods employ contrastive learning strategies and define
positive pairs by performing various augmentation of point clusters in a single
frame. As such, these methods do not exploit the temporal nature of LiDAR data.
In this paper, we introduce an SSL strategy that leverages positive pairs in
both the spatial and temporal domain. To this end, we design (i) a
point-to-cluster learning strategy that aggregates spatial information to
distinguish objects; and (ii) a cluster-to-cluster learning strategy based on
unsupervised object tracking that exploits temporal correspondences. We
demonstrate the benefits of our approach via extensive experiments performed by
self-supervised training on two large-scale LiDAR datasets and transferring the
resulting models to other point cloud segmentation benchmarks. Our results
evidence that our method outperforms the state-of-the-art point cloud SSL
methods.",None,-1
UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation,0.550671,"We describe the systems of the University of Alberta team for the
SemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel
algorithm that leverages glosses retrieved from BabelNet, in combination with
text and image encoders. Furthermore, we compare language-specific encoders
against the application of English encoders to translated texts. As the
contexts given in the task datasets are extremely short, we also experiment
with augmenting these contexts with descriptions generated by a language model.
This yields substantial improvements in accuracy. We describe and evaluate
additional V-WSD methods which use image generation and text-conditioned image
segmentation. Overall, the results of our official submission rank us 18 out of
56 teams. Some of our unofficial results are even better than the official
ones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.",None,-1
On the Neural Tangent Kernel of Equilibrium Models,0.889757,"This work studies the neural tangent kernel (NTK) of the deep equilibrium
(DEQ) model, a practical ``infinite-depth'' architecture which directly
computes the infinite-depth limit of a weight-tied network via root-finding.
Even though the NTK of a fully-connected neural network can be stochastic if
its width and depth both tend to infinity simultaneously, we show that
contrarily a DEQ model still enjoys a deterministic NTK despite its width and
depth going to infinity at the same time under mild conditions. Moreover, this
deterministic NTK can be found efficiently via root-finding.",None,-1
ControversialQA: Exploring Controversy in Question Answering,0.0215258,"Controversy is widespread online. Previous studies mainly define controversy
based on vague assumptions of its relation to sentiment such as hate speech and
offensive words. This paper introduces the first question-answering dataset
that defines content controversy by user perception, i.e., votes from plenty of
users. It contains nearly 10K questions, and each question has a best answer
and a most controversial answer. Experimental results reveal that controversy
detection in question answering is essential and challenging, and there is no
strong correlation between controversy and sentiment tasks.",None,-1
AnyDoor: Zero-shot Object-level Image Customization,0.999994,"This work presents AnyDoor, a diffusion-based image generator with the power
to teleport target objects to new scenes at user-specified locations in a
harmonious way. Instead of tuning parameters for each object, our model is
trained only once and effortlessly generalizes to diverse object-scene
combinations at the inference stage. Such a challenging zero-shot setting
requires an adequate characterization of a certain object. To this end, we
complement the commonly used identity feature with detail features, which are
carefully designed to maintain texture details yet allow versatile local
variations (e.g., lighting, orientation, posture, etc.), supporting the object
in favorably blending with different surroundings. We further propose to borrow
knowledge from video datasets, where we can observe various forms (i.e., along
the time axis) of a single object, leading to stronger model generalizability
and robustness. Extensive experiments demonstrate the superiority of our
approach over existing alternatives as well as its great potential in
real-world applications, such as virtual try-on and object moving. Project page
is https://damo-vilab.github.io/AnyDoor-Page/.",None,-1
GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds,0.757016,"We study the problem of 3D semantic segmentation from raw point clouds.
Unlike existing methods which primarily rely on a large amount of human
annotations for training neural networks, we propose the first purely
unsupervised method, called GrowSP, to successfully identify complex semantic
classes for every point in 3D scenes, without needing any type of human labels
or pretrained models. The key to our approach is to discover 3D semantic
elements via progressive growing of superpoints. Our method consists of three
major components, 1) the feature extractor to learn per-point features from
input point clouds, 2) the superpoint constructor to progressively grow the
sizes of superpoints, and 3) the semantic primitive clustering module to group
superpoints into semantic elements for the final semantic segmentation. We
extensively evaluate our method on multiple datasets, demonstrating superior
performance over all unsupervised baselines and approaching the classic
fully-supervised PointNet. We hope our work could inspire more advanced methods
for unsupervised 3D semantic learning.",None,-1
Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling,0.24785,"We study speech intent classification and slot filling (SICSF) by proposing
to use an encoder pretrained on speech recognition (ASR) to initialize an
end-to-end (E2E) Conformer-Transformer model, which achieves the new
state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and
82.27% SLURP-F1. We compare our model with encoders pretrained on
self-supervised learning (SSL), and show that ASR pretraining is much more
effective than SSL for SICSF. To explore parameter efficiency, we freeze the
encoder and add Adapter modules, and show that parameter efficiency is only
achievable with an ASR-pretrained encoder, while the SSL encoder needs full
finetuning to achieve comparable results. In addition, we provide an in-depth
comparison on end-to-end models versus cascading models (ASR+NLU), and show
that E2E models are better than cascaded models unless an oracle ASR model is
provided. Last but not least, our model is the first E2E model that achieves
the same performance as cascading models with oracle ASR. Code, checkpoints and
configs are available.",None,-1
DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection,0.770375,"The increasingly pervasive facial recognition (FR) systems raise serious
concerns about personal privacy, especially for billions of users who have
publicly shared their photos on social media. Several attempts have been made
to protect individuals from being identified by unauthorized FR systems
utilizing adversarial attacks to generate encrypted face images. However,
existing methods suffer from poor visual quality or low attack success rates,
which limit their utility. Recently, diffusion models have achieved tremendous
success in image generation. In this work, we ask: can diffusion models be used
to generate adversarial examples to improve both visual quality and attack
performance? We propose DiffProtect, which utilizes a diffusion autoencoder to
generate semantically meaningful perturbations on FR systems. Extensive
experiments demonstrate that DiffProtect produces more natural-looking
encrypted images than state-of-the-art methods while achieving significantly
higher attack success rates, e.g., 24.5% and 25.1% absolute improvements on the
CelebA-HQ and FFHQ datasets.",None,-1
On the Calibration and Uncertainty with Plya-Gamma Augmentation for Dialog Retrieval Models,0.0826843,"Deep neural retrieval models have amply demonstrated their power but
estimating the reliability of their predictions remains challenging. Most
dialog response retrieval models output a single score for a response on how
relevant it is to a given question. However, the bad calibration of deep neural
network results in various uncertainty for the single score such that the
unreliable predictions always misinform user decisions. To investigate these
issues, we present an efficient calibration and uncertainty estimation
framework PG-DRR for dialog response retrieval models which adds a Gaussian
Process layer to a deterministic deep neural network and recovers conjugacy for
tractable posterior inference by P\'{o}lya-Gamma augmentation. Finally, PG-DRR
achieves the lowest empirical calibration error (ECE) in the in-domain datasets
and the distributional shift task while keeping $R_{10}@1$ and MAP performance.",None,-1
Calibration of Quantum Decision Theory: Aversion to Large Losses and Predictability of Probabilistic Choices,0.173112,"We present the first calibration of quantum decision theory (QDT) to a
dataset of binary risky choice. We quantitatively account for the fraction of
choice reversals between two repetitions of the experiment, using a
probabilistic choice formulation in the simplest form without model assumption
or adjustable parameters. The prediction of choice reversal is then refined by
introducing heterogeneity between decision makers through their differentiation
into two groups: ``majoritarian'' and ``contrarian'' (in proportion 3:1). This
supports the first fundamental tenet of QDT, which models choice as an inherent
probabilistic process, where the probability of a prospect can be expressed as
the sum of its utility and attraction factors. We propose to parameterise the
utility factor with a stochastic version of cumulative prospect theory
(logit-CPT), and the attraction factor with a constant absolute risk aversion
(CARA) function. For this dataset, and penalising the larger number of QDT
parameters via the Wilks test of nested hypotheses, the QDT model is found to
perform significantly better than logit-CPT at both the aggregate and
individual levels, and for all considered fit criteria for the first experiment
iteration and for predictions (second ``out-of-sample'' iteration). The
distinctive QDT effect captured by the attraction factor is mostly appreciable
(i.e., most relevant and strongest in amplitude) for prospects with big losses.
Our quantitative analysis of the experimental results supports the existence of
an intrinsic limit of predictability, which is associated with the inherent
probabilistic nature of choice. The results of the paper can find applications
both in the prediction of choice of human decision makers as well as for
organizing the operation of artificial intelligence.",None,-1
Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy,0.131613,"This paper introduces a multifaceted methodology for fine-tuning and
evaluating large language models (LLMs) for specialized monetization tasks. The
goal is to balance general language proficiency with domain-specific skills.
The methodology has three main components: 1) Carefully blending in-domain and
general-purpose data during fine-tuning to achieve an optimal balance between
general and specialized capabilities; 2) Designing a comprehensive evaluation
framework with 45 questions tailored to assess performance on functionally
relevant dimensions like reliability, consistency, and business impact; 3)
Analyzing how model size and continual training influence metrics to guide
efficient resource allocation during fine-tuning. The paper details the design,
data collection, analytical techniques, and results validating the proposed
frameworks. It aims to provide businesses and researchers with actionable
insights on effectively adapting LLMs for specialized contexts. We also intend
to make public the comprehensive evaluation framework, which includes the 45
tailored questions and their respective scoring guidelines, to foster
transparency and collaboration in adapting LLMs for specialized tasks.",None,-1
Exploring Challenges and Opportunities to Support Designers in Learning to Co-create with AI-based Manufacturing Design Tools,0.9895,"AI-based design tools are proliferating in professional software to assist
engineering and industrial designers in complex manufacturing and design tasks.
These tools take on more agentic roles than traditional computer-aided design
tools and are often portrayed as ""co-creators."" Yet, working effectively with
such systems requires different skills than working with complex CAD tools
alone. To date, we know little about how engineering designers learn to work
with AI-based design tools. In this study, we observed trained designers as
they learned to work with two AI-based tools on a realistic design task. We
find that designers face many challenges in learning to effectively co-create
with current systems, including challenges in understanding and adjusting AI
outputs and in communicating their design goals. Based on our findings, we
highlight several design opportunities to better support designer-AI
co-creation.",None,-1
Exploring Large Language Models for Ontology Alignment,0.341839,"This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.",None,-1
PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations,0.632673,"PokerKit is an open-source Python library designed to overcome the
restrictions of existing poker game simulation and hand evaluation tools, which
typically support only a handful of poker variants and lack flexibility in game
state control. In contrast, PokerKit significantly expands this scope by
supporting an extensive array of poker variants and it provides a flexible
architecture for users to define their custom games. This paper details the
design and implementation of PokerKit, including its intuitive programmatic
API, multi-variant game support, and a unified hand evaluation suite across
different hand types. The flexibility of PokerKit allows for applications in
diverse areas, such as poker AI development, tool creation, and online poker
casino implementation. PokerKit's reliability has been established through
static type checking, extensive doctests, and unit tests, achieving 99% code
coverage. The introduction of PokerKit represents a significant contribution to
the field of computer poker, fostering future research and advanced AI
development for a wide variety of poker games. The source code is available at
https://github.com/uoftcprg/pokerkit",None,-1
Artificial Intelligence Impact On The Labour Force -- Searching For The Analytical Skills Of The Future Software Engineers,0.236324,"This systematic literature review aims to investigate the impact of
artificial intelligence (AI) on the labour force in software engineering, with
a particular focus on the skills needed for future software engineers, the
impact of AI on the demand for software engineering skills, and the future of
work for software engineers. The review identified 42 relevant publications
through a comprehensive search strategy and analysed their findings. The
results indicate that future software engineers will need to be competent in
programming and have soft skills such as problem-solving and interpersonal
communication. AI will have a significant impact on the software engineering
workforce, with the potential to automate many jobs currently done by software
engineers. The role of a software engineer is changing and will continue to
change in the future, with AI-assisted software development posing challenges
for the software engineering profession. The review suggests that the software
engineering profession must adapt to the changing landscape to remain relevant
and effective in the future.",None,-1
Tailoring Requirements Engineering for Responsible AI,0.203406,"Requirements Engineering (RE) is the discipline for identifying, analyzing,
as well as ensuring the implementation and delivery of user, technical, and
societal requirements. Recently reported issues concerning the acceptance of
Artificial Intelligence (AI) solutions after deployment, e.g. in the medical,
automotive, or scientific domains, stress the importance of RE for designing
and delivering Responsible AI systems. In this paper, we argue that RE should
not only be carefully conducted but also tailored for Responsible AI. We
outline related challenges for research and practice.",None,-1
Fully Bayesian VIB-DeepSSM,0.310853,"Statistical shape modeling (SSM) enables population-based quantitative
analysis of anatomical shapes, informing clinical diagnosis. Deep learning
approaches predict correspondence-based SSM directly from unsegmented 3D images
but require calibrated uncertainty quantification, motivating Bayesian
formulations. Variational information bottleneck DeepSSM (VIB-DeepSSM) is an
effective, principled framework for predicting probabilistic shapes of anatomy
from images with aleatoric uncertainty quantification. However, VIB is only
half-Bayesian and lacks epistemic uncertainty inference. We derive a fully
Bayesian VIB formulation and demonstrate the efficacy of two scalable
implementation approaches: concrete dropout and batch ensemble. Additionally,
we introduce a novel combination of the two that further enhances uncertainty
calibration via multimodal marginalization. Experiments on synthetic shapes and
left atrium data demonstrate that the fully Bayesian VIB network predicts SSM
from images with improved uncertainty reasoning without sacrificing accuracy.",None,-1
Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment,0.913031,"The multi-modal entity alignment (MMEA) aims to find all equivalent entity
pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and
neighboring entities are valuable for the alignment task, but existing works
ignore contextual gap problems that the aligned entities have different numbers
of attributes on specific modality when learning entity representations. In
this paper, we propose a novel attribute-consistent knowledge graph
representation learning framework for MMEA (ACK-MMEA) to compensate the
contextual gaps through incorporating consistent alignment knowledge.
Attribute-consistent KGs (ACKGs) are first constructed via multi-modal
attribute uniformization with merge and generate operators so that each entity
has one and only one uniform feature in each modality. The ACKGs are then fed
into a relation-aware graph neural network with random dropouts, to obtain
aggregated relation representations and robust entity representations. In order
to evaluate the ACK-MMEA facilitated for entity alignment, we specially design
a joint alignment loss for both entity and attribute evaluation. Extensive
experiments conducted on two benchmark datasets show that our approach achieves
excellent performance compared to its competitors.",None,-1
Event Blob Tracking: An Asynchronous Real-Time Algorithm,0.439,"Event-based cameras have become increasingly popular for tracking fast-moving
objects due to their high temporal resolution, low latency, and high dynamic
range. In this paper, we propose a novel algorithm for tracking event blobs
using raw events asynchronously in real time. We introduce the concept of an
event blob as a spatio-temporal likelihood of event occurrence where the
conditional spatial likelihood is blob-like. Many real-world objects generate
event blob data, for example, flickering LEDs such as car headlights or any
small foreground object moving against a static or slowly varying background.
The proposed algorithm uses a nearest neighbour classifier with a dynamic
threshold criteria for data association coupled with a Kalman filter to track
the event blob state. Our algorithm achieves highly accurate tracking and event
blob shape estimation even under challenging lighting conditions and high-speed
motions. The microsecond time resolution achieved means that the filter output
can be used to derive secondary information such as time-to-contact or range
estimation, that will enable applications to real-world problems such as
collision avoidance in autonomous driving.",None,-1
Evaluating Shutdown Avoidance of Language Models in Textual Scenarios,0.0283411,"Recently, there has been an increase in interest in evaluating large language
models for emergent and dangerous capabilities. Importantly, agents could
reason that in some scenarios their goal is better achieved if they are not
turned off, which can lead to undesirable behaviors. In this paper, we
investigate the potential of using toy textual scenarios to evaluate
instrumental reasoning and shutdown avoidance in language models such as GPT-4
and Claude. Furthermore, we explore whether shutdown avoidance is merely a
result of simple pattern matching between the dataset and the prompt or if it
is a consistent behaviour across different environments and variations.
  We evaluated behaviours manually and also experimented with using language
models for automatic evaluations, and these evaluations demonstrate that simple
pattern matching is likely not the sole contributing factor for shutdown
avoidance. This study provides insights into the behaviour of language models
in shutdown avoidance scenarios and inspires further research on the use of
textual scenarios for evaluations.",None,-1
Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain,0.826762,"Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
Parameter-Efficient Fine-Tuning (PEFT) techniques for fine-tuning language
models significantly reduce computational requirements by selectively
fine-tuning small subsets of parameters. In this study, we propose a two-step
PEFT framework and evaluate it in the clinical domain. Our approach combines a
specialised PEFT adapter layer designed for clinical domain adaptation with
another adapter specialised for downstream tasks. We evaluate the framework on
multiple clinical outcome prediction datasets, comparing it to clinically
trained language models. Our framework achieves a better AUROC score averaged
across all clinical downstream tasks compared to clinical language models. In
particular, we observe large improvements of 4-5% AUROC in large-scale
multilabel classification tasks, such as diagnoses and procedures
classification. To our knowledge, this study is the first to provide an
extensive empirical analysis of the interplay between PEFT techniques and
domain adaptation in an important real-world domain of clinical applications.",None,-1
ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data,0.571867,"Compared with traditional RGB-only visual tracking, few datasets have been
constructed for RGB-D tracking. In this paper, we propose ARKitTrack, a new
RGB-D tracking dataset for both static and dynamic scenes captured by
consumer-grade LiDAR scanners equipped on Apple's iPhone and iPad. ARKitTrack
contains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total.
Along with the bounding box annotations and frame-level attributes, we also
annotate this dataset with 123.9K pixel-level target masks. Besides, the camera
intrinsic and camera pose of each frame are provided for future developments.
To demonstrate the potential usefulness of this dataset, we further present a
unified baseline for both box-level and pixel-level tracking, which integrates
RGB features with bird's-eye-view representations to better explore
cross-modality 3D geometry. In-depth empirical analysis has verified that the
ARKitTrack dataset can significantly facilitate RGB-D tracking and that the
proposed baseline method compares favorably against the state of the arts. The
code and dataset is available at https://arkittrack.github.io.",None,-1
Benchmarking Deepart Detection,0.158657,"Deepfake technologies have been blurring the boundaries between the real and
unreal, likely resulting in malicious events. By leveraging newly emerged
deepfake technologies, deepfake researchers have been making a great upending
to create deepfake artworks (deeparts), which are further closing the gap
between reality and fantasy. To address potentially appeared ethics questions,
this paper establishes a deepart detection database (DDDB) that consists of a
set of high-quality conventional art images (conarts) and five sets of deepart
images generated by five state-of-the-art deepfake models. This database
enables us to explore once-for-all deepart detection and continual deepart
detection. For the two new problems, we suggest four benchmark evaluations and
four families of solutions on the constructed DDDB. The comprehensive study
demonstrates the effectiveness of the proposed solutions on the established
benchmark dataset, which is capable of paving a way to more interesting
directions of deepart detection. The constructed benchmark dataset and the
source code will be made publicly available.",None,-1
Incremental Generalized Category Discovery,0.787969,"We explore the problem of Incremental Generalized Category Discovery (IGCD).
This is a challenging category incremental learning setting where the goal is
to develop models that can correctly categorize images from previously seen
categories, in addition to discovering novel ones. Learning is performed over a
series of time steps where the model obtains new labeled and unlabeled data,
and discards old data, at each iteration. The difficulty of the problem is
compounded in our generalized setting as the unlabeled data can contain images
from categories that may or may not have been observed before. We present a new
method for IGCD which combines non-parametric categorization with efficient
image sampling to mitigate catastrophic forgetting. To quantify performance, we
propose a new benchmark dataset named iNatIGCD that is motivated by a
real-world fine-grained visual categorization task. In our experiments we
outperform existing related methods",None,-1
A Novel Multi-scale Attention Feature Extraction Block for Aerial Remote Sensing Image Classification,0.313259,"Classification of very high-resolution (VHR) aerial remote sensing (RS)
images is a well-established research area in the remote sensing community as
it provides valuable spatial information for decision-making. Existing works on
VHR aerial RS image classification produce an excellent classification
performance; nevertheless, they have a limited capability to well-represent VHR
RS images having complex and small objects, thereby leading to performance
instability. As such, we propose a novel plug-and-play multi-scale attention
feature extraction block (MSAFEB) based on multi-scale convolution at two
levels with skip connection, producing discriminative/salient information at a
deeper/finer level. The experimental study on two benchmark VHR aerial RS image
datasets (AID and NWPU) demonstrates that our proposal achieves a
stable/consistent performance (minimum standard deviation of $0.002$) and
competent overall classification performance (AID: 95.85\% and NWPU: 94.09\%).",None,-1
Evaluating Temporal Observation-Based Causal Discovery Techniques Applied to Road Driver Behaviour,0.535372,"Autonomous robots are required to reason about the behaviour of dynamic
agents in their environment. The creation of models to describe these
relationships is typically accomplished through the application of causal
discovery techniques. However, as it stands observational causal discovery
techniques struggle to adequately cope with conditions such as causal sparsity
and non-stationarity typically seen during online usage in autonomous agent
domains. Meanwhile, interventional techniques are not always feasible due to
domain restrictions. In order to better explore the issues facing observational
techniques and promote further discussion of these topics we carry out a
benchmark across 10 contemporary observational temporal causal discovery
methods in the domain of autonomous driving. By evaluating these methods upon
causal scenes drawn from real world datasets in addition to those generated
synthetically we highlight where improvements need to be made in order to
facilitate the application of causal discovery techniques to the aforementioned
use-cases. Finally, we discuss potential directions for future work that could
help better tackle the difficulties currently experienced by state of the art
techniques.",None,-1
A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model,0.301094,"Recently, the instruction-tuning of large language models is a crucial area
of research in the field of natural language processing. Due to resource and
cost limitations, several researchers have employed parameter-efficient tuning
techniques, such as LoRA, for instruction tuning, and have obtained encouraging
results In comparison to full-parameter fine-tuning, LoRA-based tuning
demonstrates salient benefits in terms of training costs. In this study, we
undertook experimental comparisons between full-parameter fine-tuning and
LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental
results show that the selection of the foundational model, training dataset
scale, learnable parameter quantity, and model training cost are all important
factors. We hope that the experimental conclusions of this paper can provide
inspiration for training large language models, especially in the field of
Chinese, and help researchers find a better trade-off strategy between training
cost and model performance. To facilitate the reproduction of the paper's
results, the dataset, model and code will be released.",None,-1
Learning Human-Compatible Representations for Case-Based Decision Support,0.514484,"Algorithmic case-based decision support provides examples to help human make
sense of predicted labels and aid human in decision-making tasks. Despite the
promising performance of supervised learning, representations learned by
supervised models may not align well with human intuitions: what models
consider as similar examples can be perceived as distinct by humans. As a
result, they have limited effectiveness in case-based decision support. In this
work, we incorporate ideas from metric learning with supervised learning to
examine the importance of alignment for effective decision support. In addition
to instance-level labels, we use human-provided triplet judgments to learn
human-compatible decision-focused representations. Using both synthetic data
and human subject experiments in multiple classification tasks, we demonstrate
that such representation is better aligned with human perception than
representation solely optimized for classification. Human-compatible
representations identify nearest neighbors that are perceived as more similar
by humans and allow humans to make more accurate predictions, leading to
substantial improvements in human decision accuracies (17.8% in butterfly vs.
moth classification and 13.2% in pneumonia classification).",None,-1
Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving,0.999872,"Robotic perception requires the modeling of both 3D geometry and semantics.
Existing methods typically focus on estimating 3D bounding boxes, neglecting
finer geometric details and struggling to handle general, out-of-vocabulary
objects. 3D occupancy prediction, which estimates the detailed occupancy states
and semantics of a scene, is an emerging task to overcome these limitations. To
support 3D occupancy prediction, we develop a label generation pipeline that
produces dense, visibility-aware labels for any given scene. This pipeline
comprises three stages: voxel densification, occlusion reasoning, and
image-guided voxel refinement. We establish two benchmarks, derived from the
Waymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and
Occ3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of the
proposed dataset with various baseline models. Lastly, we propose a new model,
dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior
performance on the Occ3D benchmarks. The code, data, and benchmarks are
released at https://tsinghua-mars-lab.github.io/Occ3D/.",None,-1
DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars,0.904779,"We present DINAR, an approach for creating realistic rigged fullbody avatars
from single RGB images. Similarly to previous works, our method uses neural
textures combined with the SMPL-X body model to achieve photo-realistic quality
of avatars while keeping them easy to animate and fast to infer. To restore the
texture, we use a latent diffusion model and show how such model can be trained
in the neural texture space. The use of the diffusion model allows us to
realistically reconstruct large unseen regions such as the back of a person
given the frontal view. The models in our pipeline are trained using 2D images
and videos only. In the experiments, our approach achieves state-of-the-art
rendering quality and good generalization to new poses and viewpoints. In
particular, the approach improves state-of-the-art on the SnapshotPeople public
benchmark.",None,-1
MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,0.97303,"Neural radiance fields enable state-of-the-art photorealistic view synthesis.
However, existing radiance field representations are either too
compute-intensive for real-time rendering or require too much memory to scale
to large scenes. We present a Memory-Efficient Radiance Field (MERF)
representation that achieves real-time rendering of large-scale scenes in a
browser. MERF reduces the memory consumption of prior sparse volumetric
radiance fields using a combination of a sparse feature grid and
high-resolution 2D feature planes. To support large-scale unbounded scenes, we
introduce a novel contraction function that maps scene coordinates into a
bounded volume while still allowing for efficient ray-box intersection. We
design a lossless procedure for baking the parameterization used during
training into a model that achieves real-time rendering while still preserving
the photorealistic view synthesis quality of a volumetric radiance field.",None,-1
ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text,0.848816,"We present an overview of the ArAIEval shared task, organized as part of the
first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two
tasks over Arabic text: (i) persuasion technique detection, focusing on
identifying persuasion techniques in tweets and news articles, and (ii)
disinformation detection in binary and multiclass setups over tweets. A total
of 20 teams participated in the final evaluation phase, with 14 and 16 teams
participating in Tasks 1 and 2, respectively. Across both tasks, we observed
that fine-tuning transformer models such as AraBERT was at the core of the
majority of the participating systems. We provide a description of the task
setup, including a description of the dataset construction and the evaluation
setup. We further give a brief overview of the participating systems. All
datasets and evaluation scripts from the shared task are released to the
research community. (https://araieval.gitlab.io/) We hope this will enable
further research on these important tasks in Arabic.",None,-1
Cost-Sensitive Best Subset Selection for Logistic Regression: A Mixed-Integer Conic Optimization Perspective,0.19611,"A key challenge in machine learning is to design interpretable models that
can reduce their inputs to the best subset for making transparent predictions,
especially in the clinical domain. In this work, we propose a certifiably
optimal feature selection procedure for logistic regression from a
mixed-integer conic optimization perspective that can take an auxiliary cost to
obtain features into account. Based on an extensive review of the literature,
we carefully create a synthetic dataset generator for clinical prognostic model
research. This allows us to systematically evaluate different heuristic and
optimal cardinality- and budget-constrained feature selection procedures. The
analysis shows key limitations of the methods for the low-data regime and when
confronted with label noise. Our paper not only provides empirical
recommendations for suitable methods and dataset designs, but also paves the
way for future research in the area of meta-learning.",None,-1
Multi-Agent Consensus Seeking via Large Language Models,0.831454,"Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: westlakeintelligentrobotics.github.io/ConsensusLLM/.",None,-1
Tree Prompting: Efficient Task Adaptation without Fine-Tuning,0.125345,"Prompting language models (LMs) is the main interface for applying them to
new tasks. However, for smaller LMs, prompting provides low accuracy compared
to gradient-based finetuning. Tree Prompting is an approach to prompting which
builds a decision tree of prompts, linking multiple LM calls together to solve
a task. At inference time, each call to the LM is determined by efficiently
routing the outcome of the previous call using the tree. Experiments on
classification datasets show that Tree Prompting improves accuracy over
competing methods and is competitive with fine-tuning. We also show that
variants of Tree Prompting allow inspection of a model's decision-making
process.",None,-1
DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation,0.968707,"Monocular depth estimation is a challenging task that predicts the pixel-wise
depth from a single 2D image. Current methods typically model this problem as a
regression or classification task. We propose DiffusionDepth, a new approach
that reformulates monocular depth estimation as a denoising diffusion process.
It learns an iterative denoising process to `denoise' random depth distribution
into a depth map with the guidance of monocular visual conditions. The process
is performed in the latent space encoded by a dedicated depth encoder and
decoder. Instead of diffusing ground truth (GT) depth, the model learns to
reverse the process of diffusing the refined depth of itself into random depth
distribution. This self-diffusion formulation overcomes the difficulty of
applying generative models to sparse GT depth scenarios. The proposed approach
benefits this task by refining depth estimation step by step, which is superior
for generating accurate and highly detailed depth maps. Experimental results on
KITTI and NYU-Depth-V2 datasets suggest that a simple yet efficient diffusion
approach could reach state-of-the-art performance in both indoor and outdoor
scenarios with acceptable inference time.",None,-1
Explainable Multi-Agent Reinforcement Learning for Temporal Queries,0.577713,"As multi-agent reinforcement learning (MARL) systems are increasingly
deployed throughout society, it is imperative yet challenging for users to
understand the emergent behaviors of MARL agents in complex environments. This
work presents an approach for generating policy-level contrastive explanations
for MARL to answer a temporal user query, which specifies a sequence of tasks
completed by agents with possible cooperation. The proposed approach encodes
the temporal query as a PCTL logic formula and checks if the query is feasible
under a given MARL policy via probabilistic model checking. Such explanations
can help reconcile discrepancies between the actual and anticipated multi-agent
behaviors. The proposed approach also generates correct and complete
explanations to pinpoint reasons that make a user query infeasible. We have
successfully applied the proposed approach to four benchmark MARL domains (up
to 9 agents in one domain). Moreover, the results of a user study show that the
generated explanations significantly improve user performance and satisfaction.",None,-1
"ProsAudit, a prosodic benchmark for self-supervised speech models",0.100177,"We present ProsAudit, a benchmark in English to assess structural prosodic
knowledge in self-supervised learning (SSL) speech models. It consists of two
subtasks, their corresponding metrics, and an evaluation dataset. In the
protosyntax task, the model must correctly identify strong versus weak prosodic
boundaries. In the lexical task, the model needs to correctly distinguish
between pauses inserted between words and within words. We also provide human
evaluation scores on this benchmark. We evaluated a series of SSL models and
found that they were all able to perform above chance on both tasks, even when
evaluated on an unseen language. However, non-native models performed
significantly worse than native ones on the lexical task, highlighting the
importance of lexical knowledge in this task. We also found a clear effect of
size with models trained on more data performing better in the two subtasks.",None,-1
Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?,0.339179,"Pretraining a neural network on a large dataset is becoming a cornerstone in
machine learning that is within the reach of only a few communities with
large-resources. We aim at an ambitious goal of democratizing pretraining.
Towards that goal, we train and release a single neural network that can
predict high quality ImageNet parameters of other neural networks. By using
predicted parameters for initialization we are able to boost training of
diverse ImageNet models available in PyTorch. When transferred to other
datasets, models initialized with predicted parameters also converge faster and
reach competitive final performance.",None,-1
Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning,0.197927,"Energy market rules should incentivize market participants to behave in a
market and grid conform way. However, they can also provide incentives for
undesired and unexpected strategies if the market design is flawed. Multi-agent
Reinforcement learning (MARL) is a promising new approach to predicting the
expected profit-maximizing behavior of energy market participants in
simulation. However, reinforcement learning requires many interactions with the
system to converge, and the power system environment often consists of
extensive computations, e.g., optimal power flow (OPF) calculation for market
clearing. To tackle this complexity, we provide a model of the energy market to
a basic MARL algorithm in the form of a learned OPF approximation and explicit
market rules. The learned OPF surrogate model makes an explicit solving of the
OPF completely unnecessary. Our experiments demonstrate that the model
additionally reduces training time by about one order of magnitude but at the
cost of a slightly worse performance. Potential applications of our method are
market design, more realistic modeling of market participants, and analysis of
manipulative behavior.",None,-1
Inferring Capabilities from Task Performance with Bayesian Triangulation,0.905794,"As machine learning models become more general, we need to characterise them
in richer, more meaningful ways. We describe a method to infer the cognitive
profile of a system from diverse experimental data. To do so, we introduce
measurement layouts that model how task-instance features interact with system
capabilities to affect performance. These features must be triangulated in
complex ways to be able to infer capabilities from non-populational data -- a
challenge for traditional psychometric and inferential tools. Using the
Bayesian probabilistic programming library PyMC, we infer different cognitive
profiles for agents in two scenarios: 68 actual contestants in the AnimalAI
Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery.
We showcase the potential for capability-oriented evaluation.",None,-1
Outlier detection using flexible categorisation and interrogative agendas,0.59811,"Categorization is one of the basic tasks in machine learning and data
analysis. Building on formal concept analysis (FCA), the starting point of the
present work is that different ways to categorize a given set of objects exist,
which depend on the choice of the sets of features used to classify them, and
different such sets of features may yield better or worse categorizations,
relative to the task at hand. In their turn, the (a priori) choice of a
particular set of features over another might be subjective and express a
certain epistemic stance (e.g. interests, relevance, preferences) of an agent
or a group of agents, namely, their interrogative agenda. In the present paper,
we represent interrogative agendas as sets of features, and explore and compare
different ways to categorize objects w.r.t. different sets of features
(agendas). We first develop a simple unsupervised FCA-based algorithm for
outlier detection which uses categorizations arising from different agendas. We
then present a supervised meta-learning algorithm to learn suitable (fuzzy)
agendas for categorization as sets of features with different weights or
masses. We combine this meta-learning algorithm with the unsupervised outlier
detection algorithm to obtain a supervised outlier detection algorithm. We show
that these algorithms perform at par with commonly used algorithms for outlier
detection on commonly used datasets in outlier detection. These algorithms
provide both local and global explanations of their results.",None,-1
"SAM Struggles in Concealed Scenes -- Empirical Study on ""Segment Anything""",0.883141,"Segmenting anything is a ground-breaking step toward artificial general
intelligence, and the Segment Anything Model (SAM) greatly fosters the
foundation models for computer vision. We could not be more excited to probe
the performance traits of SAM. In particular, exploring situations in which SAM
does not perform well is interesting. In this report, we choose three concealed
scenes, i.e., camouflaged animals, industrial defects, and medical lesions, to
evaluate SAM under unprompted settings. Our main observation is that SAM looks
unskilled in concealed scenes.",None,-1
Local Implicit Ray Function for Generalizable Radiance Field Representation,0.585239,"We propose LIRF (Local Implicit Ray Function), a generalizable neural
rendering approach for novel view rendering. Current generalizable neural
radiance fields (NeRF) methods sample a scene with a single ray per pixel and
may therefore render blurred or aliased views when the input views and rendered
views capture scene content with different resolutions. To solve this problem,
we propose LIRF to aggregate the information from conical frustums to construct
a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates
and the features of conical frustums as inputs and predicts a local volumetric
radiance field. Since the coordinates are continuous, LIRF renders high-quality
novel views at a continuously-valued scale via volume rendering. Besides, we
predict the visible weights for each input view via transformer-based feature
matching to improve the performance in occluded areas. Experimental results on
real-world scenes validate that our method outperforms state-of-the-art methods
on novel view rendering of unseen scenes at arbitrary scales.",None,-1
Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration,0.757599,"Kendall's tau is frequently used to meta-evaluate how well machine
translation (MT) evaluation metrics score individual translations. Its focus on
pairwise score comparisons is intuitive but raises the question of how ties
should be handled, a gray area that has motivated different variants in the
literature. We demonstrate that, in settings like modern MT meta-evaluation,
existing variants have weaknesses arising from their handling of ties, and in
some situations can even be gamed. We propose instead to meta-evaluate metrics
with a version of pairwise accuracy that gives metrics credit for correctly
predicting ties, in combination with a tie calibration procedure that
automatically introduces ties into metric scores, enabling fair comparison
between metrics that do and do not predict ties. We argue and provide
experimental evidence that these modifications lead to fairer ranking-based
assessments of metric performance.",None,-1
Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators,0.692428,"The recent success of Large Language Models (LLMs) signifies an impressive
stride towards artificial general intelligence. They have shown a promising
prospect in automatically completing tasks upon user instructions, functioning
as brain-like coordinators. The associated risks will be revealed as we
delegate an increasing number of tasks to machines for automated completion. A
big question emerges: how can we make machines behave responsibly when helping
humans automate tasks as personal copilots? In this paper, we explore this
question in depth from the perspectives of feasibility, completeness and
security. In specific, we present Responsible Task Automation (ResponsibleTA)
as a fundamental framework to facilitate responsible collaboration between
LLM-based coordinators and executors for task automation with three empowered
capabilities: 1) predicting the feasibility of the commands for executors; 2)
verifying the completeness of executors; 3) enhancing the security (e.g., the
protection of users' privacy). We further propose and compare two paradigms for
implementing the first two capabilities. One is to leverage the generic
knowledge of LLMs themselves via prompt engineering while the other is to adopt
domain-specific learnable models. Moreover, we introduce a local memory
mechanism for achieving the third capability. We evaluate our proposed
ResponsibleTA on UI task automation and hope it could bring more attentions to
ensuring LLMs more responsible in diverse scenarios.",None,-1
XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words,0.416717,"Due to the absence of explicit word boundaries in the speech stream, the task
of segmenting spoken sentences into word units without text supervision is
particularly challenging. In this work, we leverage the most recent
self-supervised speech models that have proved to quickly adapt to new tasks
through fine-tuning, even in low resource conditions. Taking inspiration from
semi-supervised learning, we fine-tune an XLS-R model to predict word
boundaries themselves produced by top-tier speech segmentation systems: DPDP,
VG-HuBERT, GradSeg and DP-Parse. Once XLS-R is fine-tuned, it is used to infer
new word boundary labels that are used in turn for another fine-tuning step.
Our method consistently improves the performance of each system and sets a new
state-of-the-art that is, on average 130% higher than the previous one as
measured by the F1 score on correctly discovered word tokens on five corpora
featuring different languages. Finally, our system can segment speech from
languages unseen during fine-tuning in a zero-shot fashion.",None,-1
Large Language Models are biased to overestimate profoundness,0.0711078,"Recent advancements in natural language processing by large language models
(LLMs), such as GPT-4, have been suggested to approach Artificial General
Intelligence. And yet, it is still under dispute whether LLMs possess similar
reasoning abilities to humans. This study evaluates GPT-4 and various other
LLMs in judging the profoundness of mundane, motivational, and pseudo-profound
statements. We found a significant statement-to-statement correlation between
the LLMs and humans, irrespective of the type of statements and the prompting
technique used. However, LLMs systematically overestimate the profoundness of
nonsensical statements, with the exception of Tk-instruct, which uniquely
underestimates the profoundness of statements. Only few-shot learning prompts,
as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.
Furthermore, this work provides insights into the potential biases induced by
Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the
bias to overestimate the profoundness of statements.",None,-1
Level Generation Through Large Language Models,0.779704,"Large Language Models (LLMs) are powerful tools, capable of leveraging their
training on natural language to write stories, generate code, and answer
questions. But can they generate functional video game levels? Game levels,
with their complex functional constraints and spatial relationships in more
than one dimension, are very different from the kinds of data an LLM typically
sees during training. Datasets of game levels are also hard to come by,
potentially taxing the abilities of these data-hungry models. We investigate
the use of LLMs to generate levels for the game Sokoban, finding that LLMs are
indeed capable of doing so, and that their performance scales dramatically with
dataset size. We also perform preliminary experiments on controlling LLM level
generators and discuss promising areas for future work.",None,-1
Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions,0.538044,"Large-scale pre-trained language models have been shown to be helpful in
improving the naturalness of text-to-speech (TTS) models by enabling them to
produce more naturalistic prosodic patterns. However, these models are usually
word-level or sup-phoneme-level and jointly trained with phonemes, making them
inefficient for the downstream TTS task where only phonemes are needed. In this
work, we propose a phoneme-level BERT (PL-BERT) with a pretext task of
predicting the corresponding graphemes along with the regular masked phoneme
predictions. Subjective evaluations show that our phoneme-level BERT encoder
has significantly improved the mean opinion scores (MOS) of rated naturalness
of synthesized speech compared with the state-of-the-art (SOTA) StyleTTS
baseline on out-of-distribution (OOD) texts.",None,-1
Challenges in Context-Aware Neural Machine Translation,0.390509,"Context-aware neural machine translation involves leveraging information
beyond sentence-level context to resolve inter-sentential discourse
dependencies and improve document-level translation quality, and has given rise
to a number of recent techniques. However, despite well-reasoned intuitions,
most context-aware translation models show only modest improvements over
sentence-level systems. In this work, we investigate several challenges that
impede progress within this field, relating to discourse phenomena, context
usage, model architectures, and document-level evaluation. To address these
problems, we propose a more realistic setting for document-level translation,
called paragraph-to-paragraph (para2para) translation, and collect a new
dataset of Chinese-English novels to promote future research.",None,-1
When SAM Meets Sonar Images,0.405191,"Segment Anything Model (SAM) has revolutionized the way of segmentation.
However, SAM's performance may decline when applied to tasks involving domains
that differ from natural images. Nonetheless, by employing fine-tuning
techniques, SAM exhibits promising capabilities in specific domains, such as
medicine and planetary science. Notably, there is a lack of research on the
application of SAM to sonar imaging. In this paper, we aim to address this gap
by conducting a comprehensive investigation of SAM's performance on sonar
images. Specifically, we evaluate SAM using various settings on sonar images.
Additionally, we fine-tune SAM using effective methods both with prompts and
for semantic segmentation, thereby expanding its applicability to tasks
requiring automated segmentation. Experimental results demonstrate a
significant improvement in the performance of the fine-tuned SAM.",None,-1
Time Series as Images: Vision Transformer for Irregularly Sampled Time Series,0.738281,"Irregularly sampled time series are increasingly prevalent, particularly in
medical domains. While various specialized methods have been developed to
handle these irregularities, effectively modeling their complex dynamics and
pronounced sparsity remains a challenge. This paper introduces a novel
perspective by converting irregularly sampled time series into line graph
images, then utilizing powerful pre-trained vision transformers for time series
classification in the same way as image classification. This method not only
largely simplifies specialized algorithm designs but also presents the
potential to serve as a universal framework for time series modeling.
Remarkably, despite its simplicity, our approach outperforms state-of-the-art
specialized algorithms on several popular healthcare and human activity
datasets. Especially in the rigorous leave-sensors-out setting where a portion
of variables is omitted during testing, our method exhibits strong robustness
against varying degrees of missing observations, achieving an impressive
improvement of 42.8% in absolute F1 score points over leading specialized
baselines even with half the variables masked. Code and data are available at
https://github.com/Leezekun/ViTST",None,-1
Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks,0.237234,"Recently large language models (LLMs) like ChatGPT have shown impressive
performance on many natural language processing tasks with zero-shot. In this
paper, we investigate the effectiveness of zero-shot LLMs in the financial
domain. We compare the performance of ChatGPT along with some open-source
generative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We
address three inter-related research questions on data annotation, performance
gaps, and the feasibility of employing generative models in the finance domain.
Our findings demonstrate that ChatGPT performs well even without labeled data
but fine-tuned models generally outperform it. Our research also highlights how
annotating with generative models can be time-intensive. Our codebase is
publicly available on GitHub under CC BY-NC 4.0 license.",None,-1
Learning Coordination Policies over Heterogeneous Graphs for Human-Robot Teams via Recurrent Neural Schedule Propagation,0.32682,"As human-robot collaboration increases in the workforce, it becomes essential
for human-robot teams to coordinate efficiently and intuitively. Traditional
approaches for human-robot scheduling either utilize exact methods that are
intractable for large-scale problems and struggle to account for stochastic,
time varying human task performance, or application-specific heuristics that
require expert domain knowledge to develop. We propose a deep learning-based
framework, called HybridNet, combining a heterogeneous graph-based encoder with
a recurrent schedule propagator for scheduling stochastic human-robot teams
under upper- and lower-bound temporal constraints. The HybridNet's encoder
leverages Heterogeneous Graph Attention Networks to model the initial
environment and team dynamics while accounting for the constraints. By
formulating task scheduling as a sequential decision-making process, the
HybridNet's recurrent neural schedule propagator leverages Long Short-Term
Memory (LSTM) models to propagate forward consequences of actions to carry out
fast schedule generation, removing the need to interact with the environment
between every task-agent pair selection. The resulting scheduling policy
network provides a computationally lightweight yet highly expressive model that
is end-to-end trainable via Reinforcement Learning algorithms. We develop a
virtual task scheduling environment for mixed human-robot teams in a
multi-round setting, capable of modeling the stochastic learning behaviors of
human workers. Experimental results showed that HybridNet outperformed other
human-robot scheduling solutions across problem sizes for both deterministic
and stochastic human performance, with faster runtime compared to
pure-GNN-based schedulers.",None,-1
Joint Skeletal and Semantic Embedding Loss for Micro-gesture Classification,0.649246,"In this paper, we briefly introduce the solution of our team HFUT-VUT for the
Micros-gesture Classification in the MiGA challenge at IJCAI 2023. The
micro-gesture classification task aims at recognizing the action category of a
given video based on the skeleton data. For this task, we propose a
3D-CNNs-based micro-gesture recognition network, which incorporates a skeletal
and semantic embedding loss to improve action classification performance.
Finally, we rank 1st in the Micro-gesture Classification Challenge, surpassing
the second-place team in terms of Top-1 accuracy by 1.10%.",None,-1
Irreducible Curriculum for Language Model Pretraining,0.0722687,"Automatic data selection and curriculum design for training large language
models is challenging, with only a few existing methods showing improvements
over standard training. Furthermore, current schemes focus on domain-level
selection, overlooking the more fine-grained contributions of each individual
training point. It is difficult to apply traditional datapoint selection
methods on large language models: most online batch selection methods perform
two-times forward or backward passes, which introduces considerable extra costs
with large-scale models. To mitigate these obstacles, we propose irreducible
curriculum as a curriculum learning algorithm for language model pretraining,
which prioritizes samples with higher learnability. Specifically, to avoid
prohibitive extra computation overhead, we simulate the sample loss along the
main model's training trajectory using a small-scale proxy model. Our
experiments on the RedPajama-1B dataset demonstrate a consistent improvement on
validation perplexity across all 7 domains compared to random uniform baseline
and the anti-curriculum strategy. Our method also reduces the sharpness of the
network and illustrates a better 5-shot accuracy on MMLU benchmarks.",None,-1
Towards a Praxis for Intercultural Ethics in Explainable AI,0.0728369,"Explainable AI (XAI) is often promoted with the idea of helping users
understand how machine learning models function and produce predictions. Still,
most of these benefits are reserved for those with specialized domain
knowledge, such as machine learning developers. Recent research has argued that
making AI explainable can be a viable way of making AI more useful in
real-world contexts, especially within low-resource domains in the Global
South. While AI has transcended borders, a limited amount of work focuses on
democratizing the concept of explainable AI to the ""majority world"", leaving
much room to explore and develop new approaches within this space that cater to
the distinct needs of users within culturally and socially-diverse regions.
This article introduces the concept of an intercultural ethics approach to AI
explainability. It examines how cultural nuances impact the adoption and use of
technology, the factors that impede how technical concepts such as AI are
explained, and how integrating an intercultural ethics approach in the
development of XAI can improve user understanding and facilitate efficient
usage of these methods.",None,-1
Axiomatic Preference Modeling for Longform Question Answering,0.117877,"The remarkable abilities of large language models (LLMs) like GPT-4 partially
stem from post-training processes like Reinforcement Learning from Human
Feedback (RLHF) involving human preferences encoded in a reward model. However,
these reward models (RMs) often lack direct knowledge of why, or under what
principles, the preferences annotations were made. In this study, we identify
principles that guide RMs to better align with human preferences, and then
develop an axiomatic framework to generate a rich variety of preference signals
to uphold them. We use these axiomatic signals to train a model for scoring
answers to longform questions. Our approach yields a Preference Model with only
about 220M parameters that agrees with gold human-annotated preference labels
more often than GPT-4. The contributions of this work include: training a
standalone preference model that can score human- and LLM-generated answers on
the same scale; developing an axiomatic framework for generating training data
pairs tailored to certain principles; and showing that a small amount of
axiomatic signals can help small models outperform GPT-4 in preference scoring.
We release our model on huggingface:
https://huggingface.co/corbyrosset/axiomatic_preference_model",None,-1
HOKEM: Human and Object Keypoint-based Extension Module for Human-Object Interaction Detection,0.289733,"Human-object interaction (HOI) detection for capturing relationships between
humans and objects is an important task in the semantic understanding of
images. When processing human and object keypoints extracted from an image
using a graph convolutional network (GCN) to detect HOI, it is crucial to
extract appropriate object keypoints regardless of the object type and to
design a GCN that accurately captures the spatial relationships between
keypoints. This paper presents the human and object keypoint-based extension
module (HOKEM) as an easy-to-use extension module to improve the accuracy of
the conventional detection models. The proposed object keypoint extraction
method is simple yet accurately represents the shapes of various objects.
Moreover, the proposed human-object adaptive GCN (HO-AGCN), which introduces
adaptive graph optimization and attention mechanism, accurately captures the
spatial relationships between keypoints. Experiments using the HOI dataset,
V-COCO, showed that HOKEM boosted the accuracy of an appearance-based model by
a large margin.",None,-1
A Blackbox Approach to Best of Both Worlds in Bandits and Beyond,0.631179,"Best-of-both-worlds algorithms for online learning which achieve near-optimal
regret in both the adversarial and the stochastic regimes have received growing
attention recently. Existing techniques often require careful adaptation to
every new problem setup, including specialised potentials and careful tuning of
algorithm parameters. Yet, in domains such as linear bandits, it is still
unknown if there exists an algorithm that can simultaneously obtain
$O(\log(T))$ regret in the stochastic regime and $\tilde{O}(\sqrt{T})$ regret
in the adversarial regime. In this work, we resolve this question positively
and present a general reduction from best of both worlds to a wide family of
follow-the-regularized-leader (FTRL) and online-mirror-descent (OMD)
algorithms. We showcase the capability of this reduction by transforming
existing algorithms that are only known to achieve worst-case guarantees into
new algorithms with best-of-both-worlds guarantees in contextual bandits, graph
bandits and tabular Markov decision processes.",None,-1
Examining Autoexposure for Challenging Scenes,0.864665,"Autoexposure (AE) is a critical step applied by camera systems to ensure
properly exposed images. While current AE algorithms are effective in well-lit
environments with constant illumination, these algorithms still struggle in
environments with bright light sources or scenes with abrupt changes in
lighting. A significant hurdle in developing new AE algorithms for challenging
environments, especially those with time-varying lighting, is the lack of
suitable image datasets. To address this issue, we have captured a new 4D
exposure dataset that provides a large solution space (i.e., shutter speed
range from (1/500 to 15 seconds) over a temporal sequence with moving objects,
bright lights, and varying lighting. In addition, we have designed a software
platform to allow AE algorithms to be used in a plug-and-play manner with the
dataset. Our dataset and associate platform enable repeatable evaluation of
different AE algorithms and provide a much-needed starting point to develop
better AE methods. We examine several existing AE strategies using our dataset
and show that most users prefer a simple saliency method for challenging
lighting conditions.",None,-1
GameEval: Evaluating LLMs on Conversational Games,0.701052,"The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.",None,-1
ALDi: Quantifying the Arabic Level of Dialectness of Text,0.81652,"Transcribed speech and user-generated text in Arabic typically contain a
mixture of Modern Standard Arabic (MSA), the standardized language taught in
schools, and Dialectal Arabic (DA), used in daily communications. To handle
this variation, previous work in Arabic NLP has focused on Dialect
Identification (DI) on the sentence or the token level. However, DI treats the
task as binary, whereas we argue that Arabic speakers perceive a spectrum of
dialectness, which we operationalize at the sentence level as the Arabic Level
of Dialectness (ALDi), a continuous linguistic variable. We introduce the
AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences
(17% from news articles and 83% from user comments on those articles) which are
manually labeled with their level of dialectness. We provide a detailed
analysis of AOC-ALDi and show that a model trained on it can effectively
identify levels of dialectness on a range of other corpora (including dialects
and genres not included in AOC-ALDi), providing a more nuanced picture than
traditional DI systems. Through case studies, we illustrate how ALDi can reveal
Arabic speakers' stylistic choices in different situations, a useful property
for sociolinguistic analyses.",None,-1
Grounded Text-to-Image Synthesis with Attention Refocusing,0.905369,"Driven by the scalable diffusion models trained on large-scale datasets,
text-to-image synthesis methods have shown compelling results. However, these
models still fail to precisely follow the text prompt involving multiple
objects, attributes, or spatial compositions. In this paper, we reveal the
potential causes in the diffusion model's cross-attention and self-attention
layers. We propose two novel losses to refocus attention maps according to a
given spatial layout during sampling. Creating the layouts manually requires
additional effort and can be tedious. Therefore, we explore using large
language models (LLM) to produce these layouts for our method. We conduct
extensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate
our proposed method. We show that our proposed attention refocusing effectively
improves the controllability of existing approaches.",None,-1
MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems,0.736872,"MeetEval is an open-source toolkit to evaluate all kinds of meeting
transcription systems. It provides a unified interface for the computation of
commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER
along other WER definitions. We extend the cpWER computation by a temporal
constraint to ensure that only words are identified as correct when the
temporal alignment is plausible. This leads to a better quality of the matching
of the hypothesis string to the reference string that more closely resembles
the actual transcription quality, and a system is penalized if it provides poor
time annotations. Since word-level timing information is often not available,
we present a way to approximate exact word-level timings from segment-level
timings (e.g., a sentence) and show that the approximation leads to a similar
WER as a matching with exact word-level annotations. At the same time, the time
constraint leads to a speedup of the matching algorithm, which outweighs the
additional overhead caused by processing the time stamps.",None,-1
Compositional preference models for aligning LMs,0.227535,"As language models (LMs) become more capable, it is increasingly important to
align them with human preferences. However, the dominant paradigm for training
Preference Models (PMs) for that purpose suffers from fundamental limitations,
such as lack of transparency and scalability, along with susceptibility to
overfitting the preference dataset. We propose Compositional Preference Models
(CPMs), a novel PM framework that decomposes one global preference assessment
into several interpretable features, obtains scalar scores for these features
from a prompted LM, and aggregates these scores using a logistic regression
classifier. Through these simple steps, CPMs allow to control which properties
of the preference data are used to train the preference model and to build it
based on features that are believed to underlie the human preference judgment.
Our experiments show that CPMs not only improve generalization and are more
robust to overoptimization than standard PMs, but also that best-of-n samples
obtained using CPMs tend to be preferred over samples obtained using
conventional PMs. Overall, our approach demonstrates the benefits of endowing
PMs with priors about which features determine human preferences while relying
on LM capabilities to extract those features in a scalable and robust way.",None,-1
V2N Service Scaling with Deep Reinforcement Learning,0.0391415,"The fifth generation (5G) of wireless networks is set out to meet the
stringent requirements of vehicular use cases. Edge computing resources can aid
in this direction by moving processing closer to end-users, reducing latency.
However, given the stochastic nature of traffic loads and availability of
physical resources, appropriate auto-scaling mechanisms need to be employed to
support cost-efficient and performant services. To this end, we employ Deep
Reinforcement Learning (DRL) for vertical scaling in Edge computing to support
vehicular-to-network communications. We address the problem using Deep
Deterministic Policy Gradient (DDPG). As DDPG is a model-free off-policy
algorithm for learning continuous actions, we introduce a discretization
approach to support discrete scaling actions. Thus we address scalability
problems inherent to high-dimensional discrete action spaces. Employing a
real-world vehicular trace data set, we show that DDPG outperforms existing
solutions, reducing (at minimum) the average number of active CPUs by 23% while
increasing the long-term reward by 24%.",None,-1
Multi-Modal Evaluation Approach for Medical Image Segmentation,0.0768659,"Manual segmentation of medical images (e.g., segmenting tumors in CT scans)
is a high-effort task that can be accelerated with machine learning techniques.
However, selecting the right segmentation approach depends on the evaluation
function, particularly in medical image segmentation where we must deal with
dependency between voxels. For instance, in contrast to classical systems where
the predictions are either correct or incorrect, predictions in medical image
segmentation may be partially correct and incorrect simultaneously. In this
paper, we explore this expressiveness to extract the useful properties of these
systems and formally define a novel multi-modal evaluation (MME) approach to
measure the effectiveness of different segmentation methods. This approach
improves the segmentation evaluation by introducing new relevant and
interpretable characteristics, including detection property, boundary
alignment, uniformity, total volume, and relative volume. Our proposed approach
is open-source and publicly available for use. We have conducted several
reproducible experiments, including the segmentation of pancreas, liver tumors,
and multi-organs datasets, to show the applicability of the proposed approach.",None,-1
Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News,0.550671,"This paper explains the participation of team Hitachi to SemEval-2023 Task 3
""Detecting the genre, the framing, and the persuasion techniques in online news
in a multi-lingual setup.'' Based on the multilingual, multi-task nature of the
task and the low-resource setting, we investigated different cross-lingual and
multi-task strategies for training the pretrained language models. Through
extensive experiments, we found that (a) cross-lingual/multi-task training, and
(b) collecting an external balanced dataset, can benefit the genre and framing
detection. We constructed ensemble models from the results and achieved the
highest macro-averaged F1 scores in Italian and Russian genre categorization
subtasks.",None,-1
Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions,0.999704,"The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves
participants finding text strings or ``flags'' by exploiting system
vulnerabilities. Large Language Models (LLMs) are natural-language models
trained on vast amounts of words to understand and generate text; they can
perform well on many CTF challenges. Such LLMs are freely available to
students. In the context of CTF exercises in the classroom, this raises
concerns about academic integrity. Educators must understand LLMs' capabilities
to modify their teaching to accommodate generative AI assistance. This research
investigates the effectiveness of LLMs, particularly in the realm of CTF
challenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,
Google Bard, and Microsoft Bing. First, we assess the LLMs' question-answering
performance on five Cisco certifications with varying difficulty levels. Next,
we qualitatively study the LLMs' abilities in solving CTF challenges to
understand their limitations. We report on the experience of using the LLMs for
seven test cases in all five types of CTF challenges. In addition, we
demonstrate how jailbreak prompts can bypass and break LLMs' ethical
safeguards. The paper concludes by discussing LLM's impact on CTF exercises and
its implications.",None,-1
GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions,0.532998,"Image restoration in adverse weather conditions is a difficult task in
computer vision. In this paper, we propose a novel transformer-based framework
called GridFormer which serves as a backbone for image restoration under
adverse weather conditions. GridFormer is designed in a grid structure using a
residual dense transformer block, and it introduces two core designs. First, it
uses an enhanced attention mechanism in the transformer layer. The mechanism
includes stages of the sampler and compact self-attention to improve
efficiency, and a local enhancement stage to strengthen local information.
Second, we introduce a residual dense transformer block (RDTB) as the final
GridFormer layer. This design further improves the network's ability to learn
effective features from both preceding and current local features. The
GridFormer framework achieves state-of-the-art results on five diverse image
restoration tasks in adverse weather conditions, including image deraining,
dehazing, deraining \& dehazing, desnowing, and multi-weather restoration. The
source code and pre-trained models are available at
https://github.com/TaoWangzj/GridFormer.",None,-1
INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification,0.586227,"This paper describes our approach to the multi-modal fact verification
(FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of
social media, fake news can spread rapidly and negatively impact social
security. Automatic claim verification becomes more and more crucial to combat
fake news. In fact verification involving multiple modal data, there should be
a structural coherence between claim and document. Therefore, we proposed a
structure coherence-based multi-modal fact verification scheme to classify fake
news. Our structure coherence includes the following four aspects: sentence
length, vocabulary similarity, semantic similarity, and image similarity.
Specifically, CLIP and Sentence BERT are combined to extract text features, and
ResNet50 is used to extract image features. In addition, we also extract the
length of the text as well as the lexical similarity. Then the features were
concatenated and passed through the random forest classifier. Finally, our
weighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2.",None,-1
Multilevel Objective-Function-Free Optimization with an Application to Neural Networks Training,0.128639,"A class of multi-level algorithms for unconstrained nonlinear optimization is
presented which does not require the evaluation of the objective function. The
class contains the momentum-less AdaGrad method as a particular (single-level)
instance. The choice of avoiding the evaluation of the objective function is
intended to make the algorithms of the class less sensitive to noise, while the
multi-level feature aims at reducing their computational cost. The evaluation
complexity of these algorithms is analyzed and their behaviour in the presence
of noise is then illustrated in the context of training deep neural networks
for supervised learning applications.",None,-1
Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning,0.418988,"We have developed a methodology for the systematic generation of a large
image dataset of macerated wood references, which we used to generate image
data for nine hardwood genera. This is the basis for a substantial approach to
automate, for the first time, the identification of hardwood species in
microscopic images of fibrous materials by deep learning. Our methodology
includes a flexible pipeline for easy annotation of vessel elements. We compare
the performance of different neural network architectures and hyperparameters.
Our proposed method performs similarly well to human experts. In the future,
this will improve controls on global wood fiber product flows to protect
forests.",None,-1
Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development,0.154113,"Despite large progress in Explainable and Safe AI, practitioners suffer from
a lack of regulation and standards for AI safety. In this work we merge recent
regulation efforts by the European Union and first proposals for AI guidelines
with recent trends in research: data and model cards. We propose the use of
standardized cards to document AI applications throughout the development
process. Our main contribution is the introduction of use-case and operation
cards, along with updates for data and model cards to cope with regulatory
requirements. We reference both recent research as well as the source of the
regulation in our cards and provide references to additional support material
and toolboxes whenever possible. The goal is to design cards that help
practitioners develop safe AI systems throughout the development process, while
enabling efficient third-party auditing of AI applications, being easy to
understand, and building trust in the system. Our work incorporates insights
from interviews with certification experts as well as developers and
individuals working with the developed AI applications.",None,-1
The Obscure Limitation of Modular Multilingual Language Models,0.44278,"We expose the limitation of modular multilingual language models (MLMs) in
multilingual inference scenarios with unknown languages. Existing evaluations
of modular MLMs exclude the involvement of language identification (LID)
modules, which obscures the performance of real-case multilingual scenarios of
modular MLMs. In this work, we showcase the effect of adding LID on the
multilingual evaluation of modular MLMs and provide discussions for closing the
performance gap of caused by the pipelined approach of LID and modular MLMs.",None,-1
Deep Learning Systems for Advanced Driving Assistance,0.239191,"Next generation cars embed intelligent assessment of car driving safety
through innovative solutions often based on usage of artificial intelligence.
The safety driving monitoring can be carried out using several methodologies
widely treated in scientific literature. In this context, the author proposes
an innovative approach that uses ad-hoc bio-sensing system suitable to
reconstruct the physio-based attentional status of the car driver. To
reconstruct the car driver physiological status, the author proposed the use of
a bio-sensing probe consisting of a coupled LEDs at Near infrared (NiR)
spectrum with a photodetector. This probe placed over the monitored subject
allows to detect a physiological signal called PhotoPlethysmoGraphy (PPG). The
PPG signal formation is regulated by the change in oxygenated and
non-oxygenated hemoglobin concentration in the monitored subject bloodstream
which will be directly connected to cardiac activity in turn regulated by the
Autonomic Nervous System (ANS) that characterizes the subject's attention
level. This so designed car driver drowsiness monitoring will be combined with
further driving safety assessment based on correlated intelligent driving
scenario understanding.",None,-1
Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3),0.501471,"Addressing pose ambiguity in 6D object pose estimation from single RGB images
presents a significant challenge, particularly due to object symmetries or
occlusions. In response, we introduce a novel score-based diffusion method
applied to the $SE(3)$ group, marking the first application of diffusion models
to $SE(3)$ within the image domain, specifically tailored for pose estimation
tasks. Extensive evaluations demonstrate the method's efficacy in handling pose
ambiguity, mitigating perspective-induced ambiguity, and showcasing the
robustness of our surrogate Stein score formulation on $SE(3)$. This
formulation not only improves the convergence of denoising process but also
enhances computational efficiency. Thus, we pioneer a promising strategy for 6D
object pose estimation.",None,-1
Controllable Guide-Space for Generalizable Face Forgery Detection,0.813225,"Recent studies on face forgery detection have shown satisfactory performance
for methods involved in training datasets, but are not ideal enough for unknown
domains. This motivates many works to improve the generalization, but
forgery-irrelevant information, such as image background and identity, still
exists in different domain features and causes unexpected clustering, limiting
the generalization. In this paper, we propose a controllable guide-space (GS)
method to enhance the discrimination of different forgery domains, so as to
increase the forgery relevance of features and thereby improve the
generalization. The well-designed guide-space can simultaneously achieve both
the proper separation of forgery domains and the large distance between
real-forgery domains in an explicit and controllable manner. Moreover, for
better discrimination, we use a decoupling module to weaken the interference of
forgery-irrelevant correlations between domains. Furthermore, we make
adjustments to the decision boundary manifold according to the clustering
degree of the same domain features within the neighborhood. Extensive
experiments in multiple in-domain and cross-domain settings confirm that our
method can achieve state-of-the-art generalization.",None,-1
Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion,0.0687808,"Time Series Classification (TSC) is an important and challenging task for
many visual computing applications. Despite the extensive range of methods
developed for TSC, relatively few utilized Deep Neural Networks (DNNs). In this
paper, we propose two novel attention blocks (Global Temporal Attention and
Temporal Pseudo-Gaussian augmented Self-Attention) that can enhance deep
learning-based TSC approaches, even when such approaches are designed and
optimized for a specific dataset or task. We validate this claim by evaluating
multiple state-of-the-art deep learning-based TSC models on the University of
East Anglia (UEA) benchmark, a standardized collection of 30 Multivariate Time
Series Classification (MTSC) datasets. We show that adding the proposed
attention blocks improves base models' average accuracy by up to 3.6%.
Additionally, the proposed TPS block uses a new injection module to include the
relative positional information in transformers. As a standalone unit with less
computational complexity, it enables TPS to perform better than most of the
state-of-the-art DNN-based TSC methods. The source codes for our experimental
setups and proposed attention blocks are made publicly available.",None,-1
AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning,0.558006,"Safety is a critical hurdle that limits the application of deep reinforcement
learning (RL) to real-world control tasks. To this end, constrained
reinforcement learning leverages cost functions to improve safety in
constrained Markov decision processes. However, such constrained RL methods
fail to achieve zero violation even when the cost limit is zero. This paper
analyzes the reason for such failure, which suggests that a proper cost
function plays an important role in constrained RL. Inspired by the analysis,
we propose AutoCost, a simple yet effective framework that automatically
searches for cost functions that help constrained RL to achieve zero-violation
performance. We validate the proposed method and the searched cost function on
the safe RL benchmark Safety Gym. We compare the performance of augmented
agents that use our cost function to provide additive intrinsic costs with
baseline agents that use the same policy learners but with only extrinsic
costs. Results show that the converged policies with intrinsic costs in all
environments achieve zero constraint violation and comparable performance with
baselines.",None,-1
Revisiting Token Dropping Strategy in Efficient BERT Pretraining,0.453476,"Token dropping is a recently-proposed strategy to speed up the pretraining of
masked language models, such as BERT, by skipping the computation of a subset
of the input tokens at several middle layers. It can effectively reduce the
training time without degrading much performance on downstream tasks. However,
we empirically find that token dropping is prone to a semantic loss problem and
falls short in handling semantic-intense tasks. Motivated by this, we propose a
simple yet effective semantic-consistent learning method (ScTD) to improve the
token dropping. ScTD aims to encourage the model to learn how to preserve the
semantic information in the representation space. Extensive experiments on 12
tasks show that, with the help of our ScTD, token dropping can achieve
consistent and significant performance gains across all task types and model
sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings
up to +1.56% average improvement over the vanilla token dropping.",None,-1
Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities,0.649518,"One of the challenges of studying common neurological disorders is disease
heterogeneity including differences in causes, neuroimaging characteristics,
comorbidities, or genetic variation. Normative modelling has become a popular
method for studying such cohorts where the 'normal' behaviour of a
physiological system is modelled and can be used at subject level to detect
deviations relating to disease pathology. For many heterogeneous diseases, we
expect to observe abnormalities across a range of neuroimaging and biological
variables. However, thus far, normative models have largely been developed for
studying a single imaging modality. We aim to develop a multi-modal normative
modelling framework where abnormality is aggregated across variables of
multiple modalities and is better able to detect deviations than uni-modal
baselines. We propose two multi-modal VAE normative models to detect subject
level deviations across T1 and DTI data. Our proposed models were better able
to detect diseased individuals, capture disease severity, and correlate with
patient cognition than baseline approaches. We also propose a multivariate
latent deviation metric, measuring deviations from the joint latent space,
which outperformed feature-based metrics.",None,-1
AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator,0.884432,"Designing robust machine learning systems remains an open problem, and there
is a need for benchmark problems that cover both environmental changes and
evaluation on a downstream task. In this work, we introduce AVOIDDS, a
realistic object detection benchmark for the vision-based aircraft
detect-and-avoid problem. We provide a labeled dataset consisting of 72,000
photorealistic images of intruder aircraft with various lighting conditions,
weather conditions, relative geometries, and geographic locations. We also
provide an interface that evaluates trained models on slices of this dataset to
identify changes in performance with respect to changing environmental
conditions. Finally, we implement a fully-integrated, closed-loop simulator of
the vision-based detect-and-avoid problem to evaluate trained models with
respect to the downstream collision avoidance task. This benchmark will enable
further research in the design of robust machine learning systems for use in
safety-critical applications. The AVOIDDS dataset and code are publicly
available at https://purl.stanford.edu/hj293cv5980 and
https://github.com/sisl/VisionBasedAircraftDAA respectively.",None,-1
Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting,0.233985,"Federated learning has exhibited vulnerabilities to Byzantine attacks, where
the Byzantine attackers can send arbitrary gradients to a central server to
destroy the convergence and performance of the global model. A wealth of robust
AGgregation Rules (AGRs) have been proposed to defend against Byzantine
attacks. However, Byzantine clients can still circumvent robust AGRs when data
is non-Identically and Independently Distributed (non-IID). In this paper, we
first reveal the root causes of performance degradation of current robust AGRs
in non-IID settings: the curse of dimensionality and gradient heterogeneity. In
order to address this issue, we propose GAS, a \shorten approach that can
successfully adapt existing robust AGRs to non-IID settings. We also provide a
detailed convergence analysis when the existing robust AGRs are combined with
GAS. Experiments on various real-world datasets verify the efficacy of our
proposed GAS. The implementation code is provided in
https://github.com/YuchenLiu-a/byzantine-gas.",None,-1
Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems,0.387991,"Clean-label (CL) attack is a form of data poisoning attack where an adversary
modifies only the textual input of the training data, without requiring access
to the labeling function. CL attacks are relatively unexplored in NLP, as
compared to label flipping (LF) attacks, where the latter additionally requires
access to the labeling function as well. While CL attacks are more resilient to
data sanitization and manual relabeling methods than LF attacks, they often
demand as high as ten times the poisoning budget than LF attacks. In this work,
we first introduce an Adversarial Clean Label attack which can adversarially
perturb in-class training examples for poisoning the training set. We then show
that an adversary can significantly bring down the data requirements for a CL
attack, using the aforementioned approach, to as low as 20% of the data
otherwise required. We then systematically benchmark and analyze a number of
defense methods, for both LF and CL attacks, some previously employed solely
for LF attacks in the textual domain and others adapted from computer vision.
We find that text-specific defenses greatly vary in their effectiveness
depending on their properties.",None,-1
Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty,0.819467,"We present our submission to the BabyLM challenge, whose goal was to improve
the sample efficiency of language models. We trained an ensemble consisting of
a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word
BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,
which exceeds in performance both of its teachers as well as a similar model
trained without distillation. This suggests that distillation can not only
retain the full performance of the teacher model when the latter is trained on
a sufficiently small dataset; it can exceed it, and lead to significantly
better performance than direct training.",None,-1
Language-Guided Audio-Visual Source Separation via Trimodal Consistency,0.946964,"We propose a self-supervised approach for learning to perform audio source
separation in videos based on natural language queries, using only unlabeled
video and audio pairs as training data. A key challenge in this task is
learning to associate the linguistic description of a sound-emitting object to
its visual features and the corresponding components of the audio waveform, all
without access to annotations during training. To overcome this challenge, we
adapt off-the-shelf vision-language foundation models to provide pseudo-target
supervision via two novel loss functions and encourage a stronger alignment
between the audio, visual and natural language modalities. During inference,
our approach can separate sounds given text, video and audio input, or given
text and audio input alone. We demonstrate the effectiveness of our
self-supervised approach on three audio-visual separation datasets, including
MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly
supervised approaches despite not using object detectors or text labels during
training.",None,-1
A Multi-Modal Transformer Network for Action Detection,0.740992,"This paper proposes a novel multi-modal transformer network for detecting
actions in untrimmed videos. To enrich the action features, our transformer
network utilizes a new multi-modal attention mechanism that computes the
correlations between different spatial and motion modalities combinations.
Exploring such correlations for actions has not been attempted previously. To
use the motion and spatial modality more effectively, we suggest an algorithm
that corrects the motion distortion caused by camera movement. Such motion
distortion, common in untrimmed videos, severely reduces the expressive power
of motion features such as optical flow fields. Our proposed algorithm
outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and
ActivityNet. We also conducted comparative experiments on our new instructional
activity dataset, including a large set of challenging classroom videos
captured from elementary schools.",None,-1
HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation,0.841917,"Panoptic Scene Graph generation (PSG) is a recently proposed task in image
scene understanding that aims to segment the image and extract triplets of
subjects, objects and their relations to build a scene graph. This task is
particularly challenging for two reasons. First, it suffers from a long-tail
problem in its relation categories, making naive biased methods more inclined
to high-frequency relations. Existing unbiased methods tackle the long-tail
problem by data/loss rebalancing to favor low-frequency relations. Second, a
subject-object pair can have two or more semantically overlapping relations.
While existing methods favor one over the other, our proposed HiLo framework
lets different network branches specialize on low and high frequency relations,
enforce their consistency and fuse the results. To the best of our knowledge we
are the first to propose an explicitly unbiased PSG method. In extensive
experiments we show that our HiLo framework achieves state-of-the-art results
on the PSG task. We also apply our method to the Scene Graph Generation task
that predicts boxes instead of masks and see improvements over all baseline
methods. Code is available at https://github.com/franciszzj/HiLo.",None,-1
Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation,0.755187,"We address the problem of referring image segmentation that aims to generate
a mask for the object specified by a natural language expression. Many recent
works utilize Transformer to extract features for the target object by
aggregating the attended visual regions. However, the generic attention
mechanism in Transformer only uses the language input for attention weight
calculation, which does not explicitly fuse language features in its output.
Thus, its output feature is dominated by vision information, which limits the
model to comprehensively understand the multi-modal information, and brings
uncertainty for the subsequent mask decoder to extract the output mask. To
address this issue, we propose Multi-Modal Mutual Attention ($\mathrm{M^3Att}$)
and Multi-Modal Mutual Decoder ($\mathrm{M^3Dec}$) that better fuse information
from the two input modalities. Based on {$\mathrm{M^3Dec}$}, we further propose
Iterative Multi-modal Interaction ($\mathrm{IMI}$) to allow continuous and
in-depth interactions between language and vision features. Furthermore, we
introduce Language Feature Reconstruction ($\mathrm{LFR}$) to prevent the
language information from being lost or distorted in the extracted feature.
Extensive experiments show that our proposed approach significantly improves
the baseline and outperforms state-of-the-art referring image segmentation
methods on RefCOCO series datasets consistently.",None,-1
Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model,0.0131142,"We use both Bayesian and neural models to dissect a data set of Chinese
learners' pre- and post-interventional responses to two tests measuring their
understanding of English prepositions. The results mostly replicate previous
findings from frequentist analyses and newly reveal crucial interactions
between student ability, task type, and stimulus sentence. Given the sparsity
of the data as well as high diversity among learners, the Bayesian method
proves most useful; but we also see potential in using language model
probabilities as predictors of grammaticality and learnability.",None,-1
"LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages",0.140267,"Knowing the language of an input text/audio is a necessary first step for
using almost every NLP tool such as taggers, parsers, or translation systems.
Language identification is a well-studied problem, sometimes even considered
solved; in reality, due to lack of data and computational challenges, current
systems cannot accurately identify most of the world's 7000 languages. To
tackle this bottleneck, we first compile a corpus, MCS-350, of 50K multilingual
and parallel children's stories in 350+ languages. MCS-350 can serve as a
benchmark for language identification of short texts and for 1400+ new
translation directions in low-resource Indian and African languages. Second, we
propose a novel misprediction-resolution hierarchical model, LIMIt, for
language identification that reduces error by 55% (from 0.71 to 0.32) on our
compiled children's stories dataset and by 40% (from 0.23 to 0.14) on the
FLORES-200 benchmark. Our method can expand language identification coverage
into low-resource languages by relying solely on systemic misprediction
patterns, bypassing the need to retrain large models from scratch.",None,-1
Examining Temporal Bias in Abusive Language Detection,0.196738,"The use of abusive language online has become an increasingly pervasive
problem that damages both individuals and society, with effects ranging from
psychological harm right through to escalation to real-life violence and even
death. Machine learning models have been developed to automatically detect
abusive language, but these models can suffer from temporal bias, the
phenomenon in which topics, language use or social norms change over time. This
study aims to investigate the nature and impact of temporal bias in abusive
language detection across various languages and explore mitigation methods. We
evaluate the performance of models on abusive data sets from different time
periods. Our results demonstrate that temporal bias is a significant challenge
for abusive language detection, with models trained on historical data showing
a significant drop in performance over time. We also present an extensive
linguistic analysis of these abusive data sets from a diachronic perspective,
aiming to explore the reasons for language evolution and performance decline.
This study sheds light on the pervasive issue of temporal bias in abusive
language detection across languages, offering crucial insights into language
evolution and temporal bias mitigation.",None,-1
SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT,0.101349,"Data-driven unit discovery in self-supervised learning (SSL) of speech has
embarked on a new era of spoken language processing. Yet, the discovered units
often remain in phonetic space and the units beyond phonemes are largely
underexplored. Here, we demonstrate that a syllabic organization emerges in
learning sentence-level representation of speech. In particular, we adopt
""self-distillation"" objective to fine-tune the pretrained HuBERT with an
aggregator token that summarizes the entire sentence. Without any supervision,
the resulting model draws definite boundaries in speech, and the
representations across frames exhibit salient syllabic structures. We
demonstrate that this emergent structure largely corresponds to the ground
truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech
ABX, for evaluating sentence-level representation of speech. When compared to
previous models, our model outperforms in both unsupervised syllable discovery
and learning sentence-level representation. Together, we demonstrate that the
self-distillation of HuBERT gives rise to syllabic organization without relying
on external labels or modalities, and potentially provides novel data-driven
units for spoken language modeling.",None,-1
Cross-modal Place Recognition in Image Databases using Event-based Sensors,0.208087,"Visual place recognition is an important problem towards global localization
in many robotics tasks. One of the biggest challenges is that it may suffer
from illumination or appearance changes in surrounding environments. Event
cameras are interesting alternatives to frame-based sensors as their high
dynamic range enables robust perception in difficult illumination conditions.
However, current event-based place recognition methods only rely on event
information, which restricts downstream applications of VPR. In this paper, we
present the first cross-modal visual place recognition framework that is
capable of retrieving regular images from a database given an event query. Our
method demonstrates promising results with respect to the state-of-the-art
frame-based and event-based methods on the Brisbane-Event-VPR dataset under
different scenarios. We also verify the effectiveness of the combination of
retrieval and classification, which can boost performance by a large margin.",None,-1
Weakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training,0.952331,"Scientific document classification is a critical task for a wide range of
applications, but the cost of obtaining massive amounts of human-labeled data
can be prohibitive. To address this challenge, we propose a weakly-supervised
approach for scientific document classification using label names only. In
scientific domains, label names often include domain-specific concepts that may
not appear in the document corpus, making it difficult to match labels and
documents precisely. To tackle this issue, we propose WANDER, which leverages
dense retrieval to perform matching in the embedding space to capture the
semantics of label names. We further design the label name expansion module to
enrich the label name representations. Lastly, a self-training step is used to
refine the predictions. The experiments on three datasets show that WANDER
outperforms the best baseline by 11.9% on average. Our code will be published
at https://github.com/ritaranx/wander.",None,-1
Deep Anomaly Detection under Labeling Budget Constraints,0.498994,"Selecting informative data points for expert feedback can significantly
improve the performance of anomaly detection (AD) in various contexts, such as
medical diagnostics or fraud detection. In this paper, we determine a set of
theoretical conditions under which anomaly scores generalize from labeled
queries to unlabeled data. Motivated by these results, we propose a data
labeling strategy with optimal data coverage under labeling budget constraints.
In addition, we propose a new learning framework for semi-supervised AD.
Extensive experiments on image, tabular, and video data sets show that our
approach results in state-of-the-art semi-supervised AD performance under
labeling budget constraints.",None,-1
Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs,0.517899,"In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.",None,-1
"Adverbs, Surprisingly",0.346696,"This paper begins with the premise that adverbs are neglected in
computational linguistics. This view derives from two analyses: a literature
review and a novel adverb dataset to probe a state-of-the-art language model,
thereby uncovering systematic gaps in accounts for adverb meaning. We suggest
that using Frame Semantics for characterizing word meaning, as in FrameNet,
provides a promising approach to adverb analysis, given its ability to describe
ambiguity, semantic roles, and null instantiation.",None,-1
Search for universal minimum drag resistance underwater vehicle hull using CFD,0.711708,"In Autonomous Underwater Vehicles (AUVs) design, hull resistance is an
important factor in determining the power requirements and range of vehicle and
consequently affect battery size, weight, and volume requirement of the design.
In this paper, we leverage on AI-based optimization algorithm along with
Computational Fluid Dynamics (CFD) simulation to study the optimal hull design
that minimizing the resistance. By running the CFD-based optimization at
different operating velocities and turbulence intensity, we want to
study/search the possibility of a universal design that will provide least
resistance/near-optimal design across all operating conditions (operating
velocity) and environmental conditions (turbulence intensity). Early result
demonstrated that the optimal design found at low velocity and low turbulence
condition performs very poor at high velocity and high turbulence conditions.
However, a design that is optimal at high velocity and high turbulence
conditions performs near-optimal across many considered velocity and turbulence
conditions.",None,-1
CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection,0.612356,"Task driven object detection aims to detect object instances suitable for
affording a task in an image. Its challenge lies in object categories available
for the task being too diverse to be limited to a closed set of object
vocabulary for traditional object detection. Simply mapping categories and
visual features of common objects to the task cannot address the challenge. In
this paper, we propose to explore fundamental affordances rather than object
categories, i.e., common attributes that enable different objects to accomplish
the same task. Moreover, we propose a novel multi-level chain-of-thought
prompting (MLCoT) to extract the affordance knowledge from large language
models, which contains multi-level reasoning steps from task to object examples
to essential visual attributes with rationales. Furthermore, to fully exploit
knowledge to benefit object recognition and localization, we propose a
knowledge-conditional detection framework, namely CoTDet. It conditions the
detector from the knowledge to generate object queries and regress boxes.
Experimental results demonstrate that our CoTDet outperforms state-of-the-art
methods consistently and significantly (+15.6 box AP and +14.8 mask AP) and can
generate rationales for why objects are detected to afford the task.",None,-1
Concept Alignment as a Prerequisite for Value Alignment,0.201259,"Value alignment is essential for building AI systems that can safely and
reliably interact with people. However, what a person values -- and is even
capable of valuing -- depends on the concepts that they are currently using to
understand and evaluate what happens in the world. The dependence of values on
concepts means that concept alignment is a prerequisite for value alignment --
agents need to align their representation of a situation with that of humans in
order to successfully align their values. Here, we formally analyze the concept
alignment problem in the inverse reinforcement learning setting, show how
neglecting concept alignment can lead to systematic value mis-alignment, and
describe an approach that helps minimize such failure modes by jointly
reasoning about a person's concepts and values. Additionally, we report
experimental results with human participants showing that humans reason about
the concepts used by an agent when acting intentionally, in line with our joint
reasoning model.",None,-1
Biomedical Entity Linking with Triple-aware Pre-Training,0.432897,"Linking biomedical entities is an essential aspect in biomedical natural
language processing tasks, such as text mining and question answering. However,
a difficulty of linking the biomedical entities using current large language
models (LLM) trained on a general corpus is that biomedical entities are
scarcely distributed in texts and therefore have been rarely seen during
training by the LLM. At the same time, those LLMs are not aware of high level
semantic connection between different biomedical entities, which are useful in
identifying similar concepts in different textual contexts. To cope with
aforementioned problems, some recent works focused on injecting knowledge graph
information into LLMs. However, former methods either ignore the relational
knowledge of the entities or lead to catastrophic forgetting. Therefore, we
propose a novel framework to pre-train the powerful generative LLM by a corpus
synthesized from a KG. In the evaluations we are unable to confirm the benefit
of including synonym, description or relational information.",None,-1
Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis,0.287833,"In this paper, we lay out a vision for analysing semantic trajectory traces
and generating synthetic semantic trajectory data (SSTs) using generative
language model. Leveraging the advancements in deep learning, as evident by
progress in the field of natural language processing (NLP), computer vision,
etc. we intend to create intelligent models that can study the semantic
trajectories in various contexts, predicting future trends, increasing machine
understanding of the movement of animals, humans, goods, etc. enhancing
human-computer interactions, and contributing to an array of applications
ranging from urban-planning to personalized recommendation engines and business
strategy.",None,-1
To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support,0.757846,"Optimizing the phrasing of argumentative text is crucial in higher education
and professional development. However, assessing whether and how the different
claims in a text should be revised is a hard task, especially for novice
writers. In this work, we explore the main challenges to identifying
argumentative claims in need of specific revisions. By learning from
collaborative editing behaviors in online debates, we seek to capture implicit
revision patterns in order to develop approaches aimed at guiding writers in
how to further improve their arguments. We systematically compare the ability
of common word embedding models to capture the differences between different
versions of the same text, and we analyze their impact on various types of
writing issues. To deal with the noisy nature of revision-based corpora, we
propose a new sampling strategy based on revision distance. Opposed to
approaches from prior work, such sampling can be done without employing
additional annotations and judgments. Moreover, we provide evidence that using
contextual information and domain knowledge can further improve prediction
results. How useful a certain type of context is, depends on the issue the
claim is suffering from, though.",None,-1
Fair Multi-Exit Framework for Facial Attribute Classification,0.345673,"Fairness has become increasingly pivotal in facial recognition. Without bias
mitigation, deploying unfair AI would harm the interest of the underprivileged
population. In this paper, we observe that though the higher accuracy that
features from the deeper layer of a neural networks generally offer, fairness
conditions deteriorate as we extract features from deeper layers. This
phenomenon motivates us to extend the concept of multi-exit framework. Unlike
existing works mainly focusing on accuracy, our multi-exit framework is
fairness-oriented, where the internal classifiers are trained to be more
accurate and fairer. During inference, any instance with high confidence from
an internal classifier is allowed to exit early. Moreover, our framework can be
applied to most existing fairness-aware frameworks. Experiment results show
that the proposed framework can largely improve the fairness condition over the
state-of-the-art in CelebA and UTK Face datasets.",None,-1
LM vs LM: Detecting Factual Errors via Cross Examination,0.834093,"A prominent weakness of modern language models (LMs) is their tendency to
generate factually incorrect text, which hinders their usability. A natural
question is whether such factual errors can be detected automatically. Inspired
by truth-seeking mechanisms in law, we propose a factuality evaluation
framework for LMs that is based on cross-examination. Our key idea is that an
incorrect claim is likely to result in inconsistency with other claims that the
model generates. To discover such inconsistencies, we facilitate a multi-turn
interaction between the LM that generated the claim and another LM (acting as
an examiner) which introduces questions to discover inconsistencies. We
empirically evaluate our method on factual claims made by multiple recent LMs
on four benchmarks, finding that it outperforms existing methods and baselines,
often by a large gap. Our results demonstrate the potential of using
interacting LMs for capturing factual errors.",None,-1
Partial Network Cloning,0.83801,"In this paper, we study a novel task that enables partial knowledge transfer
from pre-trained models, which we term as Partial Network Cloning (PNC). Unlike
prior methods that update all or at least part of the parameters in the target
network throughout the knowledge transfer process, PNC conducts partial
parametric ""cloning"" from a source network and then injects the cloned module
to the target, without modifying its parameters. Thanks to the transferred
module, the target network is expected to gain additional functionality, such
as inference on new classes; whenever needed, the cloned module can be readily
removed from the target, with its original parameters and competence kept
intact. Specifically, we introduce an innovative learning scheme that allows us
to identify simultaneously the component to be cloned from the source and the
position to be inserted within the target network, so as to ensure the optimal
performance. Experimental results on several datasets demonstrate that, our
method yields a significant improvement of 5% in accuracy and 50% in locality
when compared with parameter-tuning based methods. Our code is available at
https://github.com/JngwenYe/PNCloning.",None,-1
NeRF-Supervised Deep Stereo,0.572812,"We introduce a novel framework for training deep stereo networks effortlessly
and without any ground-truth. By leveraging state-of-the-art neural rendering
solutions, we generate stereo training data from image sequences collected with
a single handheld camera. On top of them, a NeRF-supervised training procedure
is carried out, from which we exploit rendered stereo triplets to compensate
for occlusions and depth maps as proxy labels. This results in stereo networks
capable of predicting sharp and detailed disparity maps. Experimental results
show that models trained under this regime yield a 30-40% improvement over
existing self-supervised methods on the challenging Middlebury dataset, filling
the gap to supervised models and, most times, outperforming them at zero-shot
generalization.",None,-1
Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance,0.586783,"Fine-grained multi-label classification models have broad applications in
e-commerce, such as visual based label predictions ranging from fashion
attribute detection to brand recognition. One challenge to achieve satisfactory
performance for those classification tasks in real world is the wild visual
background signal that contains irrelevant pixels which confuses model to focus
onto the region of interest and make prediction upon the specific region. In
this paper, we introduce a generic semantic-embedding deep neural network to
apply the spatial awareness semantic feature incorporating a channel-wise
attention based model to leverage the localization guidance to boost model
performance for multi-label prediction. We observed an Avg.relative improvement
of 15.27% in terms of AUC score across all labels compared to the baseline
approach. Core experiment and ablation studies involve multi-label fashion
attribute classification performed on Instagram fashion apparels' image. We
compared the model performances among our approach, baseline approach, and 3
alternative approaches to leverage semantic features. Results show favorable
performance for our approach.",None,-1
PREADD: Prefix-Adaptive Decoding for Controlled Text Generation,0.166082,"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for
controlled text generation. Unlike existing methods that use auxiliary expert
models to control for attributes, PREADD does not require an external model,
instead relying on linearly combining output logits from multiple prompts.
Specifically, PREADD contrasts the output logits generated using a raw prompt
against those generated using a prefix-prepended prompt, enabling both positive
and negative control with respect to any attribute encapsulated by the prefix.
We evaluate PREADD on three tasks -- toxic output mitigation, gender bias
reduction, and sentiment control -- and find that PREADD outperforms not only
prompting baselines, but also an auxiliary-expert control method, by 12% or
more in relative gain on our main metrics for each task.",None,-1
Improving Language Plasticity via Pretraining with Active Forgetting,0.237397,"Pretrained language models (PLMs) are today the primary model for natural
language processing. Despite their impressive downstream performance, it can be
difficult to apply PLMs to new languages, a barrier to making their
capabilities universally accessible. While prior work has shown it possible to
address this issue by learning a new embedding layer for the new language,
doing so is both data and compute inefficient. We propose to use an active
forgetting mechanism during pretraining, as a simple way of creating PLMs that
can quickly adapt to new languages. Concretely, by resetting the embedding
layer every K updates during pretraining, we encourage the PLM to improve its
ability of learning new embeddings within a limited number of updates, similar
to a meta-learning effect. Experiments with RoBERTa show that models pretrained
with our forgetting mechanism not only demonstrate faster convergence during
language adaptation but also outperform standard ones in a low-data regime,
particularly for languages that are distant from English.",None,-1
Comparative study of Transformer and LSTM Network with attention mechanism on Image Captioning,0.272431,"In a globalized world at the present epoch of generative intelligence, most
of the manual labour tasks are automated with increased efficiency. This can
support businesses to save time and money. A crucial component of generative
intelligence is the integration of vision and language. Consequently, image
captioning become an intriguing area of research. There have been multiple
attempts by the researchers to solve this problem with different deep learning
architectures, although the accuracy has increased, but the results are still
not up to standard. This study buckles down to the comparison of Transformer
and LSTM with attention block model on MS-COCO dataset, which is a standard
dataset for image captioning. For both the models we have used pretrained
Inception-V3 CNN encoder for feature extraction of the images. The Bilingual
Evaluation Understudy score (BLEU) is used to checked the accuracy of caption
generated by both models. Along with the transformer and LSTM with attention
block models,CLIP-diffusion model, M2-Transformer model and the X-Linear
Attention model have been discussed with state of the art accuracy.",None,-1
ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing,0.998579,"This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing.",None,-1
Semantic Compression With Large Language Models,0.485915,"The rise of large language models (LLMs) is revolutionizing information
retrieval, question answering, summarization, and code generation tasks.
However, in addition to confidently presenting factually inaccurate information
at times (known as ""hallucinations""), LLMs are also inherently limited by the
number of input and output tokens that can be processed at once, making them
potentially less effective on tasks that require processing a large set or
continuous stream of information. A common approach to reducing the size of
data is through lossless or lossy compression. Yet, in some cases it may not be
strictly necessary to perfectly recover every detail from the original data, as
long as a requisite level of semantic precision or intent is conveyed.
  This paper presents three contributions to research on LLMs. First, we
present the results from experiments exploring the viability of approximate
compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT
interfaces. Second, we investigate and quantify the capability of LLMs to
compress text and code, as well as to recall and manipulate compressed
representations of prompts. Third, we present two novel metrics -- Exact
Reconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness
(SRE) -- that quantify the level of preserved intent between text compressed
and decompressed by the LLMs we studied. Our initial results indicate that
GPT-4 can effectively compress and reconstruct text while preserving the
semantic essence of the original text, providing a path to leverage
$\sim$5$\times$ more tokens than present limits allow.",None,-1
A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training,0.487528,"Mixture-of-Experts (MoE) is a neural network architecture that adds sparsely
activated expert blocks to a base model, increasing the number of parameters
without impacting computational costs. However, current distributed deep
learning frameworks are limited in their ability to train high-quality MoE
models with large base models. In this work, we present DeepSpeed-TED, a novel,
three-dimensional, hybrid parallel algorithm that combines data, tensor, and
expert parallelism to enable the training of MoE models with 4 to 8x larger
base models than the current state-of-the-art. We also describe memory
optimizations in the optimizer step, and communication optimizations that
eliminate unnecessary data movement. We implement our approach in DeepSpeed and
achieve speedups of 26% over a baseline (i.e. without our communication
optimizations) when training a 40 billion parameter MoE model (6.7 billion base
model with 16 experts) on 128 V100 GPUs.",None,-1
Learning Rational Subgoals from Demonstrations and Instructions,0.222193,"We present a framework for learning useful subgoals that support efficient
long-term planning to achieve novel goals. At the core of our framework is a
collection of rational subgoals (RSGs), which are essentially binary
classifiers over the environmental states. RSGs can be learned from
weakly-annotated data, in the form of unsegmented demonstration trajectories,
paired with abstract task descriptions, which are composed of terms initially
unknown to the agent (e.g., collect-wood then craft-boat then go-across-river).
Our framework also discovers dependencies between RSGs, e.g., the task
collect-wood is a helpful subgoal for the task craft-boat. Given a goal
description, the learned subgoals and the derived dependencies facilitate
off-the-shelf planning algorithms, such as A* and RRT, by setting helpful
subgoals as waypoints to the planner, which significantly improves
performance-time efficiency.",None,-1
HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification,0.197664,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification as the labels form a complex hierarchical structure.
Existing dual-encoder methods in HTC achieve weak performance gains with huge
memory overheads and their structure encoders heavily rely on domain knowledge.
Under such observation, we tend to investigate the feasibility of a
memory-friendly model with strong generalization capability that could boost
the performance of HTC without prior statistics or label semantics. In this
paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance
the text representations with only syntactic information of the label
hierarchy. Specifically, we convert the label hierarchy into an unweighted tree
structure, termed coding tree, with the guidance of structural entropy. Then we
design a structure encoder to incorporate hierarchy-aware information in the
coding tree into text representations. Besides the text encoder, HiTIN only
contains a few multi-layer perceptions and linear transformations, which
greatly saves memory. We conduct experiments on three commonly used datasets
and the results demonstrate that HiTIN could achieve better test performance
and less memory consumption than state-of-the-art (SOTA) methods.",None,-1
Transformer-based model for monocular visual odometry: a video understanding approach,0.717169,"Estimating the camera's pose given images of a single camera is a traditional
task in mobile robots and autonomous vehicles. This problem is called monocular
visual odometry and it often relies on geometric approaches that require
considerable engineering effort for a specific scenario. Deep learning methods
have shown to be generalizable after proper training and a large amount of
available data. Transformer-based architectures have dominated the
state-of-the-art in natural language processing and computer vision tasks, such
as image and video understanding. In this work, we deal with the monocular
visual odometry as a video understanding task to estimate the 6-DoF camera's
pose. We contribute by presenting the TSformer-VO model based on
spatio-temporal self-attention mechanisms to extract features from clips and
estimate the motions in an end-to-end manner. Our approach achieved competitive
state-of-the-art performance compared with geometry-based and deep
learning-based methods on the KITTI visual odometry dataset, outperforming the
DeepVO implementation highly accepted in the visual odometry community.",None,-1
Robust estimation of exposure ratios in multi-exposure image stacks,0.396165,"Merging multi-exposure image stacks into a high dynamic range (HDR) image
requires knowledge of accurate exposure times. When exposure times are
inaccurate, for example, when they are extracted from a camera's EXIF metadata,
the reconstructed HDR images reveal banding artifacts at smooth gradients. To
remedy this, we propose to estimate exposure ratios directly from the input
images. We derive the exposure time estimation as an optimization problem, in
which pixels are selected from pairs of exposures to minimize estimation error
caused by camera noise. When pixel values are represented in the logarithmic
domain, the problem can be solved efficiently using a linear solver. We
demonstrate that the estimation can be easily made robust to pixel misalignment
caused by camera or object motion by collecting pixels from multiple spatial
tiles. The proposed automatic exposure estimation and alignment eliminates
banding artifacts in popular datasets and is essential for applications that
require physically accurate reconstructions, such as measuring the modulation
transfer function of a display. The code for the method is available.",None,-1
Ancient Chinese Word Segmentation and Part-of-Speech Tagging Using Distant Supervision,0.871603,"Ancient Chinese word segmentation (WSG) and part-of-speech tagging (POS) are
important to study ancient Chinese, but the amount of ancient Chinese WSG and
POS tagging data is still rare. In this paper, we propose a novel augmentation
method of ancient Chinese WSG and POS tagging data using distant supervision
over parallel corpus. However, there are still mislabeled and unlabeled ancient
Chinese words inevitably in distant supervision. To address this problem, we
take advantage of the memorization effects of deep neural networks and a small
amount of annotated data to get a model with much knowledge and a little noise,
and then we use this model to relabel the ancient Chinese sentences in parallel
corpus. Experiments show that the model trained over the relabeled data
outperforms the model trained over the data generated from distant supervision
and the annotated data. Our code is available at
https://github.com/farlit/ACDS.",None,-1
Dense Text-to-Image Generation with Attention Modulation,0.843161,"Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.",None,-1
DaliID: Distortion-Adaptive Learned Invariance for Identification Models,0.221516,"In unconstrained scenarios, face recognition and person re-identification are
subject to distortions such as motion blur, atmospheric turbulence, or
upsampling artifacts. To improve robustness in these scenarios, we propose a
methodology called Distortion-Adaptive Learned Invariance for Identification
(DaliID) models. We contend that distortion augmentations, which degrade image
quality, can be successfully leveraged to a greater degree than has been shown
in the literature. Aided by an adaptive weighting schedule, a novel distortion
augmentation is applied at severe levels during training. This training
strategy increases feature-level invariance to distortions and decreases domain
shift to unconstrained scenarios. At inference, we use a magnitude-weighted
fusion of features from parallel models to retain robustness across the range
of images. DaliID models achieve state-of-the-art (SOTA) for both face
recognition and person re-identification on seven benchmark datasets, including
IJB-S, TinyFace, DeepChange, and MSMT17. Additionally, we provide recaptured
evaluation data at a distance of 750+ meters and further validate on real
long-distance face imagery.",None,-1
PaintSeg: Training-free Segmentation via Painting,0.292813,"The paper introduces PaintSeg, a new unsupervised method for segmenting
objects without any training. We propose an adversarial masked contrastive
painting (AMCP) process, which creates a contrast between the original image
and a painted image in which a masked area is painted using off-the-shelf
generative models. During the painting process, inpainting and outpainting are
alternated, with the former masking the foreground and filling in the
background, and the latter masking the background while recovering the missing
part of the foreground object. Inpainting and outpainting, also referred to as
I-step and O-step, allow our method to gradually advance the target
segmentation mask toward the ground truth without supervision or training.
PaintSeg can be configured to work with a variety of prompts, e.g. coarse
masks, boxes, scribbles, and points. Our experimental results demonstrate that
PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and
point-prompt segmentation tasks, providing a training-free solution suitable
for unsupervised segmentation.",None,-1
DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles,0.434178,"We propose a new object-centric video prediction algorithm based on the deep
latent particle (DLP) representation. In comparison to existing slot- or
patch-based representations, DLPs model the scene using a set of keypoints with
learned parameters for properties such as position and size, and are both
efficient and interpretable. Our method, deep dynamic latent particles (DDLP),
yields state-of-the-art object-centric video prediction results on several
challenging datasets. The interpretable nature of DDLP allows us to perform
``what-if'' generation -- predict the consequence of changing properties of
objects in the initial frames, and DLP's compact structure enables efficient
diffusion-based unconditional video generation. Videos, code and pre-trained
models are available: https://taldatech.github.io/ddlp-web",None,-1
Automated Query Generation for Evidence Collection from Web Search Engines,0.111231,"It is widely accepted that so-called facts can be checked by searching for
information on the Internet. This process requires a fact-checker to formulate
a search query based on the fact and to present it to a search engine. Then,
relevant and believable passages need to be identified in the search results
before a decision is made. This process is carried out by sub-editors at many
news and media organisations on a daily basis. Here, we ask the question as to
whether it is possible to automate the first step, that of query generation.
Can we automatically formulate search queries based on factual statements which
are similar to those formulated by human experts? Here, we consider similarity
both in terms of textual similarity and with respect to relevant documents
being returned by a search engine. First, we introduce a moderate-sized
evidence collection dataset which includes 390 factual statements together with
associated human-generated search queries and search results. Then, we
investigate generating queries using a number of rule-based and automatic text
generation methods based on pre-trained large language models (LLMs). We show
that these methods have different merits and propose a hybrid approach which
has superior performance in practice.",None,-1
Visualizing Skiers' Trajectories in Monocular Videos,0.0864937,"Trajectories are fundamental to winning in alpine skiing. Tools enabling the
analysis of such curves can enhance the training activity and enrich
broadcasting content. In this paper, we propose SkiTraVis, an algorithm to
visualize the sequence of points traversed by a skier during its performance.
SkiTraVis works on monocular videos and constitutes a pipeline of a visual
tracker to model the skier's motion and of a frame correspondence module to
estimate the camera's motion. The separation of the two motions enables the
visualization of the trajectory according to the moving camera's perspective.
We performed experiments on videos of real-world professional competitions to
quantify the visualization error, the computational efficiency, as well as the
applicability. Overall, the results achieved demonstrate the potential of our
solution for broadcasting media enhancement and coach assistance.",None,-1
A Universal Question-Answering Platform for Knowledge Graphs,0.546348,"Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
  In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.",None,-1
Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum,0.487255,"Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to extending the capability of LLMs. Although some works
employ open-source LLMs for the tool learning task, most of them are trained in
a controlled environment in which LLMs only learn to execute the human-provided
tools. However, selecting proper tools from the large toolset is also a crucial
ability for the tool learning model to be applied in real-world applications.
Existing methods usually directly employ self-instruction methods to train the
model, which ignores differences in tool complexity. In this paper, we propose
the Confucius, a novel tool learning framework to train LLM to use complicated
tools in real-world scenarios, which contains two main phases: (1) We first
propose a multi-stage learning method to teach the LLM to use various tools
from an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative
Self-instruct from Introspective Feedback (ISIF) to dynamically construct the
dataset to improve the ability to use the complicated tool. Extensive
experiments conducted on both controlled and real-world settings demonstrate
the superiority of our tool learning framework in the real-world application
scenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based
baselines (e.g. GPT4Tools).",None,-1
WISE: full-Waveform variational Inference via Subsurface Extensions,0.997912,"We introduce a probabilistic technique for full-waveform inversion, employing
variational inference and conditional normalizing flows to quantify uncertainty
in migration-velocity models and its impact on imaging. Our approach integrates
generative artificial intelligence with physics-informed common-image gathers,
reducing reliance on accurate initial velocity models. Considered case studies
demonstrate its efficacy producing realizations of migration-velocity models
conditioned by the data. These models are used to quantify amplitude and
positioning effects during subsequent imaging.",None,-1
Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning,0.25845,"Large Language models (LLMs) possess the capability to engage In-context
Learning (ICL) by leveraging a few demonstrations pertaining to a new
downstream task as conditions. However, this particular learning paradigm
suffers from high instability stemming from substantial variances induced by
factors such as the input distribution of selected examples, their ordering,
and prompt formats. In this work, we demonstrate that even when all these
factors are held constant, the random selection of examples still results in
high variance. Consequently, we aim to explore the informative ability of data
examples by quantifying the Information Gain (IG) obtained in prediction after
observing a given example candidate. Then we propose to sample those with
maximum IG. Additionally, we identify the presence of template bias, which can
lead to unfair evaluations of IG during the sampling process. To mitigate this
bias, we introduce Calibration Before Sampling strategy. The experimental
results illustrate that our proposed method can yield an average relative
improvement of 14.3% across six classification tasks using three LLMs.",None,-1
PUPS: Point Cloud Unified Panoptic Segmentation,0.805893,"Point cloud panoptic segmentation is a challenging task that seeks a holistic
solution for both semantic and instance segmentation to predict groupings of
coherent points. Previous approaches treat semantic and instance segmentation
as surrogate tasks, and they either use clustering methods or bounding boxes to
gather instance groupings with costly computation and hand-crafted designs in
the instance segmentation task. In this paper, we propose a simple but
effective point cloud unified panoptic segmentation (PUPS) framework, which use
a set of point-level classifiers to directly predict semantic and instance
groupings in an end-to-end manner. To realize PUPS, we introduce bipartite
matching to our training pipeline so that our classifiers are able to
exclusively predict groupings of instances, getting rid of hand-crafted
designs, e.g. anchors and Non-Maximum Suppression (NMS). In order to achieve
better grouping results, we utilize a transformer decoder to iteratively refine
the point classifiers and develop a context-aware CutMix augmentation to
overcome the class imbalance problem. As a result, PUPS achieves 1st place on
the leader board of SemanticKITTI panoptic segmentation task and
state-of-the-art results on nuScenes.",None,-1
Explaining CLIP through Co-Creative Drawings and Interaction,0.436432,"This paper analyses a visual archive of drawings produced by an interactive
robotic art installation where audience members narrated their dreams into a
system powered by CLIPdraw deep learning (DL) model that interpreted and
transformed their dreams into images. The resulting archive of prompt-image
pairs were examined and clustered based on concept representation accuracy. As
a result of the analysis, the paper proposes four groupings for describing and
explaining CLIP-generated results: clear concept, text-to-text as image,
indeterminacy and confusion, and lost in translation. This article offers a
glimpse into a collection of dreams interpreted, mediated and given form by
Artificial Intelligence (AI), showcasing oftentimes unexpected, visually
compelling or, indeed, the dream-like output of the system, with the emphasis
on processes and results of translations between languages, sign-systems and
various modules of the installation. In the end, the paper argues that proposed
clusters support better understanding of the neural model.",None,-1
A Conditional Generative Chatbot using Transformer Model,0.033864,"A Chatbot serves as a communication tool between a human user and a machine
to achieve an appropriate answer based on the human input. In more recent
approaches, a combination of Natural Language Processing and sequential models
are used to build a generative Chatbot. The main challenge of these models is
their sequential nature, which leads to less accurate results. To tackle this
challenge, in this paper, a novel architecture is proposed using conditional
Wasserstein Generative Adversarial Networks and a transformer model for answer
generation in Chatbots. While the generator of the proposed model consists of a
full transformer model to generate an answer, the discriminator includes only
the encoder part of a transformer model followed by a classifier. To the best
of our knowledge, this is the first time that a generative Chatbot is proposed
using the embedded transformer in both generator and discriminator models.
Relying on the parallel computing of the transformer model, the results of the
proposed model on the Cornell Movie-Dialog corpus and the Chit-Chat datasets
confirm the superiority of the proposed model compared to state-of-the-art
alternatives using different evaluation metrics.",None,-1
AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web,0.81599,"Existing datasets for automated fact-checking have substantial limitations,
such as relying on artificial claims, lacking annotations for evidence and
intermediate reasoning, or including evidence published after the claim. In
this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims
covering fact-checks by 50 different organizations. Each claim is annotated
with question-answer pairs supported by evidence available online, as well as
textual justifications explaining how the evidence combines to produce a
verdict. Through a multi-round annotation process, we avoid common pitfalls
including context dependence, evidence insufficiency, and temporal leakage, and
reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We
develop a baseline as well as an evaluation scheme for verifying claims through
several question-answering steps against the open web.",None,-1
Predicting Sentence-Level Factuality of News and Bias of Media Outlets,0.219317,"Automated news credibility and fact-checking at scale require accurately
predicting news factuality and media bias. This paper introduces a large
sentence-level dataset, titled ""FactNews"", composed of 6,191 sentences expertly
annotated according to factuality and media bias definitions proposed by
AllSides. We use FactNews to assess the overall reliability of news sources, by
formulating two text classification problems for predicting sentence-level
factuality of news reporting and bias of media outlets. Our experiments
demonstrate that biased sentences present a higher number of words compared to
factual sentences, besides having a predominance of emotions. Hence, the
fine-grained analysis of subjectivity and impartiality of news articles
provided promising results for predicting the reliability of media outlets.
Finally, due to the severity of fake news and political polarization in Brazil,
and the lack of research for Portuguese, both dataset and baseline were
proposed for Brazilian Portuguese.",None,-1
"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",0.999753,"With the promotion of chatgpt to the public, Large language models indeed
showcase remarkable common sense, reasoning, and planning skills, frequently
providing insightful guidance. These capabilities hold significant promise for
their application in urban traffic management and control. However, LLMs
struggle with addressing traffic issues, especially processing numerical data
and interacting with simulations, limiting their potential in solving
traffic-related challenges. In parallel, specialized traffic foundation models
exist but are typically designed for specific tasks with limited input-output
interactions. Combining these models with LLMs presents an opportunity to
enhance their capacity for tackling complex traffic-related problems and
providing insightful suggestions. To bridge this gap, we present TrafficGPT, a
fusion of ChatGPT and traffic foundation models. This integration yields the
following key enhancements: 1) empowering ChatGPT with the capacity to view,
analyze, process traffic data, and provide insightful decision support for
urban transportation system management; 2) facilitating the intelligent
deconstruction of broad and complex tasks and sequential utilization of traffic
foundation models for their gradual completion; 3) aiding human decision-making
in traffic control through natural language dialogues; and 4) enabling
interactive feedback and solicitation of revised outcomes. By seamlessly
intertwining large language model and traffic expertise, TrafficGPT not only
advances traffic management but also offers a novel approach to leveraging AI
capabilities in this domain. The TrafficGPT demo can be found in
https://github.com/lijlansg/TrafficGPT.git.",None,-1
A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset,0.801013,"Text Classification is the process of categorizing text into the relevant
categories and its algorithms are at the core of many Natural Language
Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP
are the most highly used information retrieval methods in text classification.
We have investigated and analyzed the feature weighting method for text
classification on unstructured data. The proposed model considered two features
N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset
for sentiment analysis. Then we have used the state-of-the-art classifier to
validate the method i.e., Support Vector Machine (SVM), Logistic Regression,
Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and
k-nearest neighbors (KNN). From those two feature extractions, a significant
increase in feature extraction with TF-IDF features rather than based on
N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall
(93.81%), and F1-score (91.99%) value in Random Forest classifier.",None,-1
Large Language Models Perform Diagnostic Reasoning,0.396936,"We explore the extension of chain-of-thought (CoT) prompting to medical
reasoning for the task of automatic diagnosis. Motivated by doctors' underlying
reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical
results demonstrate that by simply prompting large language models trained only
on general text corpus with two DR-CoT exemplars, the diagnostic accuracy
improves by 15% comparing to standard prompting. Moreover, the gap reaches a
pronounced 18% in out-domain settings. Our findings suggest expert-knowledge
reasoning in large language models can be elicited through proper promptings.",None,-1
Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning,0.697368,"Large language models (LLMs) have shown impressive performance in following
natural language instructions to solve unseen tasks. However, it remains
unclear whether models truly understand task definitions and whether the
human-written definitions are optimal. In this paper, we systematically study
the role of task definitions in instruction learning. We first conduct an
ablation analysis informed by human annotations to understand which parts of a
task definition are most important, and find that model performance only drops
substantially when removing contents describing the task output, in particular
label information. Next, we propose an automatic algorithm to compress task
definitions to a minimal supporting set of tokens, and find that 60\% of tokens
can be removed while maintaining or even improving model performance. Based on
these results, we propose two strategies to help models better leverage task
instructions: (1) providing only key information for tasks in a common
structured format, and (2) adding a meta-tuning stage to help the model better
understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L
improvement over 119 unseen test tasks.",None,-1
Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,0.926625,"The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection",None,-1
Context Matters: A Strategy to Pre-train Language Model for Science Education,0.933014,"This study aims at improving the performance of scoring student responses in
science education automatically. BERT-based language models have shown
significant superiority over traditional NLP models in various language-related
tasks. However, science writing of students, including argumentation and
explanation, is domain-specific. In addition, the language used by students is
different from the language in journals and Wikipedia, which are training
sources of BERT and its existing variants. All these suggest that a
domain-specific model pre-trained using science education data may improve
model performance. However, the ideal type of data to contextualize pre-trained
language model and improve the performance in automatically scoring student
written responses remains unclear. Therefore, we employ different data in this
study to contextualize both BERT and SciBERT models and compare their
performance on automatic scoring of assessment tasks for scientific
argumentation. We use three datasets to pre-train the model: 1) journal
articles in science education, 2) a large dataset of students' written
responses (sample size over 50,000), and 3) a small dataset of students'
written responses of scientific argumentation tasks. Our experimental results
show that in-domain training corpora constructed from science questions and
responses improve language model performance on a wide variety of downstream
tasks. Our study confirms the effectiveness of continual pre-training on
domain-specific data in the education domain and demonstrates a generalizable
strategy for automating science education tasks with high accuracy. We plan to
release our data and SciEdBERT models for public use and community engagement.",None,-1
"TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",0.723464,"Tables are prevalent in real-world databases, requiring significant time and
effort for humans to analyze and manipulate. The advancements in large language
models (LLMs) have made it possible to interact with tables using natural
language input, bringing this capability closer to reality. In this paper, we
present TableGPT, a unified fine-tuned framework that enables LLMs to
understand and operate on tables using external functional commands. It
introduces the capability to seamlessly interact with tables, enabling a wide
range of functionalities such as question answering, data manipulation (e.g.,
insert, delete, query, and modify operations), data visualization, analysis
report generation, and automated prediction. TableGPT aims to provide
convenience and accessibility to users by empowering them to effortlessly
leverage tabular data. At the core of TableGPT lies the novel concept of global
tabular representations, which empowers LLMs to gain a comprehensive
understanding of the entire table beyond meta-information. By jointly training
LLMs on both table and text modalities, TableGPT achieves a deep understanding
of tabular data and the ability to perform complex operations on tables through
chain-of-command instructions. Importantly, TableGPT offers the advantage of
being a self-contained system rather than relying on external API interfaces.
Moreover, it supports efficient data process flow, query rejection (when
appropriate) and private deployment, enabling faster domain data fine-tuning
and ensuring data privacy, which enhances the framework's adaptability to
specific use cases.",None,-1
SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases,0.0468658,"Recent studies reveal that various biases exist in different NLP tasks, and
over-reliance on biases results in models' poor generalization ability and low
adversarial robustness. To mitigate datasets biases, previous works propose
lots of debiasing techniques to tackle specific biases, which perform well on
respective adversarial sets but fail to mitigate other biases. In this paper,
we propose a new debiasing method Sparse Mixture-of-Adapters (SMoA), which can
mitigate multiple dataset biases effectively and efficiently. Experiments on
Natural Language Inference and Paraphrase Identification tasks demonstrate that
SMoA outperforms full-finetuning, adapter tuning baselines, and prior strong
debiasing methods. Further analysis indicates the interpretability of SMoA that
sub-adapter can capture specific pattern from the training data and specialize
to handle specific bias.",None,-1
FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis,0.666602,"In this paper, we propose FinVis-GPT, a novel multimodal large language model
(LLM) specifically designed for financial chart analysis. By leveraging the
power of LLMs and incorporating instruction tuning and multimodal capabilities,
FinVis-GPT is capable of interpreting financial charts and providing valuable
analysis. To train FinVis-GPT, a financial task oriented dataset was generated
for pre-training alignment and instruction tuning, comprising various types of
financial charts and their corresponding descriptions. We evaluate the model
performance via several case studies due to the time limit, and the promising
results demonstrated that FinVis-GPT is superior in various financial chart
related tasks, including generating descriptions, answering questions and
predicting future market trends, surpassing existing state-of-the-art
multimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in
utilizing multimodal LLMs in the finance domain and our generated dataset will
be release for public use in the near future to speedup related research.",None,-1
Measuring Normative and Descriptive Biases in Language Models Using Census Data,0.0131493,"We investigate in this paper how distributions of occupations with respect to
gender is reflected in pre-trained language models. Such distributions are not
always aligned to normative ideals, nor do they necessarily reflect a
descriptive assessment of reality. In this paper, we introduce an approach for
measuring to what degree pre-trained language models are aligned to normative
and descriptive occupational distributions. To this end, we use official
demographic information about gender--occupation distributions provided by the
national statistics agencies of France, Norway, United Kingdom, and the United
States. We manually generate template-based sentences combining gendered
pronouns and nouns with occupations, and subsequently probe a selection of ten
language models covering the English, French, and Norwegian languages. The
scoring system we introduce in this work is language independent, and can be
used on any combination of template-based sentences, occupations, and
languages. The approach could also be extended to other dimensions of national
census data and other demographic variables.",None,-1
In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models,0.422095,"The phenomena of in-context learning has typically been thought of as
""learning from examples"". In this work which focuses on Machine Translation, we
present a perspective of in-context learning as the desired generation task
maintaining coherency with its context, i.e., the prompt examples. We first
investigate randomly sampled prompts across 4 domains, and find that
translation performance improves when shown in-domain prompts. Next, we
investigate coherency for the in-domain setting, which uses prompt examples
from a moving window. We study this with respect to other factors that have
previously been identified in the literature such as length, surface similarity
and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B,
Bloom3B, XGLM2.9B), and three translation directions
(\texttt{en}$\rightarrow$\{\texttt{pt, de, fr}\}) suggest that the long-term
coherency of the prompts and the test sentence is a good indicator of
downstream translation performance. In doing so, we demonstrate the efficacy of
In-context Machine Translation for on-the-fly adaptation.",None,-1
UNICORN: A Unified Backdoor Trigger Inversion Framework,0.765972,"The backdoor attack, where the adversary uses inputs stamped with triggers
(e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat
to Deep Neural Network (DNN) models. Trigger inversion is an effective way of
identifying backdoor models and understanding embedded adversarial behaviors. A
challenge of trigger inversion is that there are many ways of constructing the
trigger. Existing methods cannot generalize to various types of triggers by
making certain assumptions or attack-specific constraints. The fundamental
reason is that existing work does not consider the trigger's design space in
their formulation of the inversion problem. This work formally defines and
analyzes the triggers injected in different spaces and the inversion problem.
Then, it proposes a unified framework to invert backdoor triggers based on the
formalization of triggers and the identified inner behaviors of backdoor models
from our analysis. Our prototype UNICORN is general and effective in inverting
backdoor triggers in DNNs. The code can be found at
https://github.com/RU-System-Software-and-Security/UNICORN.",None,-1
Choice Fusion as Knowledge for Zero-Shot Dialogue State Tracking,0.0729697,"With the demanding need for deploying dialogue systems in new domains with
less cost, zero-shot dialogue state tracking (DST), which tracks user's
requirements in task-oriented dialogues without training on desired domains,
draws attention increasingly. Although prior works have leveraged
question-answering (QA) data to reduce the need for in-domain training in DST,
they fail to explicitly model knowledge transfer and fusion for tracking
dialogue states. To address this issue, we propose CoFunDST, which is trained
on domain-agnostic QA datasets and directly uses candidate choices of
slot-values as knowledge for zero-shot dialogue-state generation, based on a T5
pre-trained language model. Specifically, CoFunDST selects highly-relevant
choices to the reference context and fuses them to initialize the decoder to
constrain the model outputs. Our experimental results show that our proposed
model achieves outperformed joint goal accuracy compared to existing zero-shot
DST approaches in most domains on the MultiWOZ 2.1. Extensive analyses
demonstrate the effectiveness of our proposed approach for improving zero-shot
DST learning from QA.",None,-1
Advancing Referring Expression Segmentation Beyond Single Image,0.794147,"Referring Expression Segmentation (RES) is a widely explored multi-modal
task, which endeavors to segment the pre-existing object within a single image
with a given linguistic expression. However, in broader real-world scenarios,
it is not always possible to determine if the described object exists in a
specific image. Typically, we have a collection of images, some of which may
contain the described objects. The current RES setting curbs its practicality
in such situations. To overcome this limitation, we propose a more realistic
and general setting, named Group-wise Referring Expression Segmentation (GRES),
which expands RES to a collection of related images, allowing the described
objects to be present in a subset of input images. To support this new setting,
we introduce an elaborately compiled dataset named Grouped Referring Dataset
(GRD), containing complete group-wise annotations of target objects described
by given expressions. We also present a baseline method named Grouped Referring
Segmenter (GRSer), which explicitly captures the language-vision and
intra-group vision-vision interactions to achieve state-of-the-art results on
the proposed GRES and related tasks, such as Co-Salient Object Detection and
RES. Our dataset and codes will be publicly released in
https://github.com/yixuan730/group-res.",None,-1
Efficient Monotonic Multihead Attention,0.213788,"We introduce the Efficient Monotonic Multihead Attention (EMMA), a
state-of-the-art simultaneous translation model with numerically-stable and
unbiased monotonic alignment estimation. In addition, we present improved
training and inference strategies, including simultaneous fine-tuning from an
offline translation model and reduction of monotonic alignment variance. The
experimental results demonstrate that the proposed model attains
state-of-the-art performance in simultaneous speech-to-text translation on the
Spanish and English translation task.",None,-1
Reliability Scores from Saliency Map Clusters for Improved Image-based Harvest-Readiness Prediction in Cauliflower,0.748747,"Cauliflower is a hand-harvested crop that must fulfill high-quality standards
in sales making the timing of harvest important. However, accurately
determining harvest-readiness can be challenging due to the cauliflower head
being covered by its canopy. While deep learning enables automated
harvest-readiness estimation, errors can occur due to field-variability and
limited training data. In this paper, we analyze the reliability of a
harvest-readiness classifier with interpretable machine learning. By
identifying clusters of saliency maps, we derive reliability scores for each
classification result using knowledge about the domain and the image
properties. For unseen data, the reliability can be used to (i) inform farmers
to improve their decision-making and (ii) increase the model prediction
accuracy. Using RGB images of single cauliflower plants at different
developmental stages from the GrowliFlower dataset, we investigate various
saliency mapping approaches and find that they result in different quality of
reliability scores. With the most suitable interpretation tool, we adjust the
classification result and achieve a 15.72% improvement of the overall accuracy
to 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for
the GrowliFlower dataset.",None,-1
Dynamic Transformers Provide a False Sense of Efficiency,0.146577,"Despite much success in natural language processing (NLP), pre-trained
language models typically lead to a high computational cost during inference.
Multi-exit is a mainstream approach to address this issue by making a trade-off
between efficiency and accuracy, where the saving of computation comes from an
early exit. However, whether such saving from early-exiting is robust remains
unknown. Motivated by this, we first show that directly adapting existing
adversarial attack approaches targeting model accuracy cannot significantly
reduce inference efficiency. To this end, we propose a simple yet effective
attacking framework, SAME, a novel slowdown attack framework on multi-exit
models, which is specially tailored to reduce the efficiency of the multi-exit
models. By leveraging the multi-exit models' design characteristics, we utilize
all internal predictions to guide the adversarial sample generation instead of
merely considering the final prediction. Experiments on the GLUE benchmark show
that SAME can effectively diminish the efficiency gain of various multi-exit
models by 80% on average, convincingly validating its effectiveness and
generalization ability.",None,-1
Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias,0.0422091,"Recent studies show that instruction tuning (IT) and reinforcement learning
from human feedback (RLHF) improve the abilities of large language models (LMs)
dramatically. While these tuning methods can help align models with human
objectives and generate high-quality text, not much is known about their
potential adverse effects. In this work, we investigate the effect of IT and
RLHF on decision making and reasoning in LMs, focusing on three cognitive
biases - the decoy effect, the certainty effect, and the belief bias - all of
which are known to influence human decision-making and reasoning. Our findings
highlight the presence of these biases in various models from the GPT-3,
Mistral, and T5 families. Notably, we find a stronger presence of biases in
models that have undergone instruction tuning, such as Flan-T5,
Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward
comprehending cognitive biases in instruction-tuned LMs, which is crucial for
the development of more reliable and unbiased language models.",None,-1
Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making,0.958815,"In AI-assisted decision-making, it is critical for human decision-makers to
know when to trust AI and when to trust themselves. However, prior studies
calibrated human trust only based on AI confidence indicating AI's correctness
likelihood (CL) but ignored humans' CL, hindering optimal team decision-making.
To mitigate this gap, we proposed to promote humans' appropriate trust based on
the CL of both sides at a task-instance level. We first modeled humans' CL by
approximating their decision-making models and computing their potential
performance in similar instances. We demonstrated the feasibility and
effectiveness of our model via two preliminary studies. Then, we proposed three
CL exploitation strategies to calibrate users' trust explicitly/implicitly in
the AI-assisted decision-making process. Results from a between-subjects
experiment (N=293) showed that our CL exploitation strategies promoted more
appropriate human trust in AI, compared with only using AI confidence. We
further provided practical implications for more human-compatible AI-assisted
decision-making.",None,-1
GaitMPL: Gait Recognition with Memory-Augmented Progressive Learning,0.569314,"Gait recognition aims at identifying the pedestrians at a long distance by
their biometric gait patterns. It is inherently challenging due to the various
covariates and the properties of silhouettes (textureless and colorless), which
result in two kinds of pair-wise hard samples: the same pedestrian could have
distinct silhouettes (intra-class diversity) and different pedestrians could
have similar silhouettes (inter-class similarity). In this work, we propose to
solve the hard sample issue with a Memory-augmented Progressive Learning
network (GaitMPL), including Dynamic Reweighting Progressive Learning module
(DRPL) and Global Structure-Aligned Memory bank (GSAM). Specifically, DRPL
reduces the learning difficulty of hard samples by easy-to-hard progressive
learning. GSAM further augments DRPL with a structure-aligned memory mechanism,
which maintains and models the feature distribution of each ID. Experiments on
two commonly used datasets, CASIA-B and OU-MVLP, demonstrate the effectiveness
of GaitMPL. On CASIA-B, we achieve the state-of-the-art performance, i.e.,
88.0% on the most challenging condition (Clothing) and 93.3% on the average
condition, which outperforms the other methods by at least 3.8% and 1.4%,
respectively.",None,-1
Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation,0.510616,"New Natural Langauge Process~(NLP) benchmarks are urgently needed to align
with the rapid development of large language models (LLMs). We present Xiezhi,
the most comprehensive evaluation suite designed to assess holistic domain
knowledge. Xiezhi comprises multiple-choice questions across 516 diverse
disciplines ranging from 13 different subjects with 249,587 questions and
accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k
questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results
indicate that LLMs exceed average performance of humans in science,
engineering, agronomy, medicine, and art, but fall short in economics,
jurisprudence, pedagogy, literature, history, and management. We anticipate
Xiezhi will help analyze important strengths and shortcomings of LLMs, and the
benchmark is released in~\url{https://github.com/MikeGu721/XiezhiBenchmark}.",None,-1
DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep Inconsistency Prior,0.904193,"RGB-NIR fusion is a promising method for low-light imaging. However,
high-intensity noise in low-light images amplifies the effect of structure
inconsistency between RGB-NIR images, which fails existing algorithms. To
handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net
(DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior
(DIP). The Deep Structure extracts clear structure details in deep multiscale
feature space rather than raw input space, which is more robust to noisy
inputs. Based on the deep structures from both RGB and NIR domains, we
introduce the DIP to leverage the structure inconsistency to guide the fusion
of RGB-NIR. Benefiting from this, the proposed DVN obtains high-quality
lowlight images without the visual artifacts. We also propose a new dataset
called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as
the first public RGBNIR fusion benchmark. Quantitative and qualitative results
on the proposed benchmark show that DVN significantly outperforms other
comparison algorithms in PSNR and SSIM, especially in extremely low light
conditions.",None,-1
Multi3DRefer: Grounding Text Description to Multiple 3D Objects,0.728358,"We introduce the task of localizing a flexible number of objects in
real-world 3D scenes using natural language descriptions. Existing 3D visual
grounding tasks focus on localizing a unique object given a text description.
However, such a strict setting is unnatural as localizing potentially multiple
objects is a common need in real-world scenarios and robotic tasks (e.g.,
visual navigation and object rearrangement). To address this setting we propose
Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains
61926 descriptions of 11609 objects, where zero, single or multiple target
objects are referenced by each description. We also introduce a new evaluation
metric and benchmark methods from prior work to enable further investigation of
multi-modal 3D scene understanding. Furthermore, we develop a better baseline
leveraging 2D features from CLIP by rendering object proposals online with
contrastive learning, which outperforms the state of the art on the ScanRefer
benchmark.",None,-1
ThoughtSource: A central hub for large language model reasoning data,0.452033,"Large language models (LLMs) such as GPT-4 have recently demonstrated
impressive results across a wide range of tasks. LLMs are still limited,
however, in that they frequently fail at complex reasoning, their reasoning
processes are opaque, they are prone to 'hallucinate' facts, and there are
concerns about their underlying biases. Letting models verbalize reasoning
steps as natural language, a technique known as chain-of-thought prompting, has
recently been proposed as a way to address some of these issues. Here we
present ThoughtSource, a meta-dataset and software library for chain-of-thought
(CoT) reasoning. The goal of ThoughtSource is to improve future artificial
intelligence systems by facilitating qualitative understanding of CoTs,
enabling empirical evaluations, and providing training data. This first release
of ThoughtSource integrates seven scientific/medical, three general-domain and
five math word question answering datasets.",None,-1
Unsupervised Meta-Learning via Few-shot Pseudo-supervised Contrastive Learning,0.527867,"Unsupervised meta-learning aims to learn generalizable knowledge across a
distribution of tasks constructed from unlabeled data. Here, the main challenge
is how to construct diverse tasks for meta-learning without label information;
recent works have proposed to create, e.g., pseudo-labeling via pretrained
representations or creating synthetic samples via generative models. However,
such a task construction strategy is fundamentally limited due to heavy
reliance on the immutable pseudo-labels during meta-learning and the quality of
the representations or the generated samples. To overcome the limitations, we
propose a simple yet effective unsupervised meta-learning framework, coined
Pseudo-supervised Contrast (PsCo), for few-shot classification. We are inspired
by the recent self-supervised learning literature; PsCo utilizes a momentum
network and a queue of previous batches to improve pseudo-labeling and
construct diverse tasks in a progressive manner. Our extensive experiments
demonstrate that PsCo outperforms existing unsupervised meta-learning methods
under various in-domain and cross-domain few-shot classification benchmarks. We
also validate that PsCo is easily scalable to a large-scale benchmark, while
recent prior-art meta-schemes are not.",None,-1
External Language Model Integration for Factorized Neural Transducers,0.186699,"We propose an adaptation method for factorized neural transducers (FNT) with
external language models. We demonstrate that both neural and n-gram external
LMs add significantly more value when linearly interpolated with predictor
output compared to shallow fusion, thus confirming that FNT forces the
predictor to act like regular language models. Further, we propose a method to
integrate class-based n-gram language models into FNT framework resulting in
accuracy gains similar to a hybrid setup. We show average gains of 18% WERR
with lexical adaptation across various scenarios and additive gains of up to
60% WERR in one entity-rich scenario through a combination of class-based
n-gram and neural LMs.",None,-1
Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity,0.736049,"A widespread view is that Artificial Intelligence cannot be creative. We
tested this assumption by comparing human-generated ideas with those generated
by six Generative Artificial Intelligence (GAI) chatbots: $alpa.\!ai$,
$Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and YouChat. Humans and
a specifically trained AI independently assessed the quality and quantity of
ideas. We found no qualitative difference between AI and human-generated
creativity, although there are differences in how ideas are generated.
Interestingly, 9.4 percent of humans were more creative than the most creative
GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the
creative process. Continued research and development of GAI in creative tasks
is crucial to fully understand this technology's potential benefits and
drawbacks in shaping the future of creativity. Finally, we discuss the question
of whether GAIs are capable of being truly creative.",None,-1
Data Augmentation for Human Behavior Analysis in Multi-Person Conversations,0.159402,"In this paper, we present the solution of our team HFUT-VUT for the
MultiMediate Grand Challenge 2023 at ACM Multimedia 2023. The solution covers
three sub-challenges: bodily behavior recognition, eye contact detection, and
next speaker prediction. We select Swin Transformer as the baseline and exploit
data augmentation strategies to address the above three tasks. Specifically, we
crop the raw video to remove the noise from other parts. At the same time, we
utilize data augmentation to improve the generalization of the model. As a
result, our solution achieves the best results of 0.6262 for bodily behavior
recognition in terms of mean average precision and the accuracy of 0.7771 for
eye contact detection on the corresponding test set. In addition, our approach
also achieves comparable results of 0.5281 for the next speaker prediction in
terms of unweighted average recall.",None,-1
A Study on the Appropriate size of the Mongolian general corpus,0.225112,"This study aims to determine the appropriate size of the Mongolian general
corpus. This study used the Heaps function and Type Token Ratio to determine
the appropriate size of the Mongolian general corpus. The sample corpus of
906,064 tokens comprised texts from 10 domains of newspaper politics, economy,
society, culture, sports, world articles and laws, middle and high school
literature textbooks, interview articles, and podcast transcripts. First, we
estimated the Heaps function with this sample corpus. Next, we observed changes
in the number of types and TTR values while increasing the number of tokens by
one million using the estimated Heaps function. As a result of observation, we
found that the TTR value hardly changed when the number of tokens exceeded from
39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian
general corpus is from 39 to 42 million tokens.",None,-1
SIO: Synthetic In-Distribution Data Benefits Out-of-Distribution Detection,0.147791,"Building up reliable Out-of-Distribution (OOD) detectors is challenging,
often requiring the use of OOD data during training. In this work, we develop a
data-driven approach which is distinct and complementary to existing works:
Instead of using external OOD data, we fully exploit the internal
in-distribution (ID) training set by utilizing generative models to produce
additional synthetic ID images. The classifier is then trained using a novel
objective that computes weighted loss on real and synthetic ID samples
together. Our training framework, which is termed SIO, serves as a
""plug-and-play"" technique that is designed to be compatible with existing and
future OOD detection algorithms, including the ones that leverage available OOD
training data. Our experiments on CIFAR-10, CIFAR-100, and ImageNet variants
demonstrate that SIO consistently improves the performance of nearly all
state-of-the-art (SOTA) OOD detection algorithms. For instance, on the
challenging CIFAR-10 v.s. CIFAR-100 detection problem, SIO improves the average
OOD detection AUROC of 18 existing methods from 86.25\% to 89.04\% and achieves
a new SOTA of 92.94\% according to the OpenOOD benchmark. Code is available at
https://github.com/zjysteven/SIO.",None,-1
Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples,0.719559,"The objective of this work is to explore the learning of visually grounded
speech models (VGS) from multilingual perspective. Bilingual VGS models are
generally trained with an equal number of spoken captions from both languages.
However, in reality, there can be an imbalance among the languages for the
available spoken captions. Our key contribution in this work is to leverage the
power of a high-resource language in a bilingual visually grounded speech model
to improve the performance of a low-resource language. We introduce two methods
to distill the knowledge of high-resource language into low-resource languages:
(1) incorporating a strong pre-trained high-resource language encoder and (2)
using semantically similar spoken captions. Our experiments show that combining
these two approaches effectively enables the low-resource language to surpass
the performances of monolingual and bilingual counterparts for cross-modal
retrieval tasks.",None,-1
GECCO: Geometrically-Conditioned Point Diffusion Models,0.554375,"Diffusion models generating images conditionally on text, such as Dall-E 2
and Stable Diffusion, have recently made a splash far beyond the computer
vision community. Here, we tackle the related problem of generating point
clouds, both unconditionally, and conditionally with images. For the latter, we
introduce a novel geometrically-motivated conditioning scheme based on
projecting sparse image features into the point cloud and attaching them to
each individual point, at every step in the denoising process. This approach
improves geometric consistency and yields greater fidelity than current methods
relying on unstructured, global latent codes. Additionally, we show how to
apply recent continuous-time diffusion schemes. Our method performs on par or
above the state of art on conditional and unconditional experiments on
synthetic data, while being faster, lighter, and delivering tractable
likelihoods. We show it can also scale to diverse indoors scenes.",None,-1
Spatio-Temporal AU Relational Graph Representation Learning For Facial Action Units Detection,0.933374,"This paper presents our Facial Action Units (AUs) detection submission to the
fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our approach
consists of three main modules: (i) a pre-trained facial representation encoder
which produce a strong facial representation from each input face image in the
input sequence; (ii) an AU-specific feature generator that specifically learns
a set of AU features from each facial representation; and (iii) a
spatio-temporal graph learning module that constructs a spatio-temporal graph
representation. This graph representation describes AUs contained in all frames
and predicts the occurrence of each AU based on both the modeled spatial
information within the corresponding face and the learned temporal dynamics
among frames. The experimental results show that our approach outperformed the
baseline and the spatio-temporal graph representation learning allows our model
to generate the best results among all ablated systems. Our model ranks at the
4th place in the AU recognition track at the 5th ABAW Competition. Our code is
publicly available at https://github.com/wzh125/ABAW-5.",None,-1
A Reparameterized Discrete Diffusion Model for Text Generation,0.363538,"This work studies discrete diffusion probabilistic models with applications
to natural language generation. We derive an alternative yet equivalent
formulation of the sampling from discrete diffusion processes and leverage this
insight to develop a family of reparameterized discrete diffusion models. The
derived generic framework is highly flexible, offers a fresh perspective of the
generation process in discrete diffusion models, and features more effective
training and decoding techniques. We conduct extensive experiments to evaluate
the text generation capability of our model, demonstrating significant
improvements over existing diffusion models.",None,-1
Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,0.794954,"Building agents with large language models (LLMs) for computer control is a
burgeoning research area, where the agent receives computer states and performs
actions to complete complex tasks. Previous computer agents have demonstrated
the benefits of in-context learning (ICL); however, their performance is
hindered by several issues. First, the limited context length of LLMs and
complex computer states restrict the number of exemplars, as a single webpage
can consume the entire context. Second, the exemplars in current methods, such
as high-level plans and multi-choice questions, cannot represent complete
trajectories, leading to suboptimal performance in long-horizon tasks. Third,
existing computer agents rely on task-specific exemplars and overlook the
similarity among tasks, resulting in poor generalization to novel tasks. To
address these challenges, we introduce Synapse, a computer agent featuring
three key components: i) state abstraction, which filters out task-irrelevant
information from raw states, allowing more exemplars within the limited
context, ii) trajectory-as-exemplar prompting, which prompts the LLM with
complete trajectories of the abstracted states and actions to improve
multi-step decision-making, and iii) exemplar memory, which stores the
embeddings of exemplars and retrieves them via similarity search for
generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard
task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse
achieves a 99.2% average success rate (a 10% relative improvement) across 64
tasks using demonstrations from only 48 tasks. Notably, Synapse is the first
ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a
56% relative improvement in average step success rate over the previous
state-of-the-art prompting scheme in Mind2Web.",None,-1
EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models,0.2499,"Events serve as fundamental units of occurrence within various contexts. The
processing of event semantics in textual information forms the basis of
numerous natural language processing (NLP) applications. Recent studies have
begun leveraging large language models (LLMs) to address event semantic
processing. However, the extent that LLMs can effectively tackle these
challenges remains uncertain. Furthermore, the lack of a comprehensive
evaluation framework for event semantic processing poses a significant
challenge in evaluating these capabilities. In this paper, we propose an
overarching framework for event semantic processing, encompassing
understanding, reasoning, and prediction, along with their fine-grained
aspects. To comprehensively evaluate the event semantic processing abilities of
models, we introduce a novel benchmark called EVEVAL. We collect 8 datasets
that cover all aspects of event semantic processing. Extensive experiments are
conducted on EVEVAL, leading to several noteworthy findings based on the
obtained results.",None,-1
Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition,0.610264,"The development of robust, generalized models in human activity recognition
(HAR) has been hindered by the scarcity of large-scale, labeled data sets.
Recent work has shown that virtual IMU data extracted from videos using
computer vision techniques can lead to substantial performance improvements
when training HAR models combined with small portions of real IMU data.
Inspired by recent advances in motion synthesis from textual descriptions and
connecting Large Language Models (LLMs) to various AI models, we introduce an
automated pipeline that first uses ChatGPT to generate diverse textual
descriptions of activities. These textual descriptions are then used to
generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and
later converted to streams of virtual IMU data. We benchmarked our approach on
three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the
use of virtual IMU training data generated using our new approach leads to
significantly improved HAR model performance compared to only using real IMU
data. Our approach contributes to the growing field of cross-modality transfer
methods and illustrate how HAR models can be improved through the generation of
virtual training data that do not require any manual effort.",None,-1
rynSpeech: A multi-purpose Yorb Speech Corpus,0.831193,"We introduce \`{I}r\`{o}y\`{i}nSpeech, a new corpus influenced by the desire
to increase the amount of high quality, contemporary Yor\`{u}b\'{a} speech
data, which can be used for both Text-to-Speech (TTS) and Automatic Speech
Recognition (ASR) tasks. We curated about 23000 text sentences from news and
creative writing domains with the open license CC-BY-4.0. To encourage a
participatory approach to data creation, we provide 5000 curated sentences to
the Mozilla Common Voice platform to crowd-source the recording and validation
of Yor\`{u}b\'{a} speech data. In total, we created about 42 hours of speech
data recorded by 80 volunteers in-house, and 6 hours of validated recordings on
Mozilla Common Voice platform. Our TTS evaluation suggests that a
high-fidelity, general domain, single-speaker Yor\`{u}b\'{a} voice is possible
with as little as 5 hours of speech. Similarly, for ASR we obtained a baseline
word error rate (WER) of 23.8.",None,-1
Low-Light Image Enhancement by Learning Contrastive Representations in Spatial and Frequency Domains,0.606564,"Images taken under low-light conditions tend to suffer from poor visibility,
which can decrease image quality and even reduce the performance of the
downstream tasks. It is hard for a CNN-based method to learn generalized
features that can recover normal images from the ones under various unknow
low-light conditions. In this paper, we propose to incorporate the contrastive
learning into an illumination correction network to learn abstract
representations to distinguish various low-light conditions in the
representation space, with the purpose of enhancing the generalizability of the
network. Considering that light conditions can change the frequency components
of the images, the representations are learned and compared in both spatial and
frequency domains to make full advantage of the contrastive learning. The
proposed method is evaluated on LOL and LOL-V2 datasets, the results show that
the proposed method achieves better qualitative and quantitative results
compared with other state-of-the-arts.",None,-1
Attention Disturbance and Dual-Path Constraint Network for Occluded Person Re-identification,0.265037,"Occluded person re-identification (Re-ID) aims to address the potential
occlusion problem when matching occluded or holistic pedestrians from different
camera views. Many methods use the background as artificial occlusion and rely
on attention networks to exclude noisy interference. However, the significant
discrepancy between simple background occlusion and realistic occlusion can
negatively impact the generalization of the network. To address this issue, we
propose a novel transformer-based Attention Disturbance and Dual-Path
Constraint Network (ADP) to enhance the generalization of attention networks.
Firstly, to imitate real-world obstacles, we introduce an Attention Disturbance
Mask (ADM) module that generates an offensive noise, which can distract
attention like a realistic occluder, as a more complex form of occlusion.
Secondly, to fully exploit these complex occluded images, we develop a
Dual-Path Constraint Module (DPC) that can obtain preferable supervision
information from holistic images through dual-path interaction. With our
proposed method, the network can effectively circumvent a wide variety of
occlusions using the basic ViT baseline. Comprehensive experimental evaluations
conducted on person re-ID benchmarks demonstrate the superiority of ADP over
state-of-the-art methods.",None,-1
Diffusion Models for Imperceptible and Transferable Adversarial Attack,0.831842,"Many existing adversarial attacks generate $L_p$-norm perturbations on image
RGB space. Despite some achievements in transferability and attack success
rate, the crafted adversarial examples are easily perceived by human eyes.
Towards visual imperceptibility, some recent works explore unrestricted attacks
without $L_p$-norm constraints, yet lacking transferability of attacking
black-box models. In this work, we propose a novel imperceptible and
transferable attack by leveraging both the generative and discriminative power
of diffusion models. Specifically, instead of direct manipulation in pixel
space, we craft perturbations in the latent space of diffusion models. Combined
with well-designed content-preserving structures, we can generate
human-insensitive perturbations embedded with semantic clues. For better
transferability, we further ""deceive"" the diffusion model which can be viewed
as an implicit recognition surrogate, by distracting its attention away from
the target regions. To our knowledge, our proposed method, DiffAttack, is the
first that introduces diffusion models into the adversarial attack field.
Extensive experiments on various model structures, datasets, and defense
methods have demonstrated the superiority of our attack over the existing
attack methods.",None,-1
EMP-EVAL: A Framework for Measuring Empathy in Open Domain Dialogues,0.0901236,"Measuring empathy in conversation can be challenging, as empathy is a complex
and multifaceted psychological construct that involves both cognitive and
emotional components. Human evaluations can be subjective, leading to
inconsistent results. Therefore, there is a need for an automatic method for
measuring empathy that reduces the need for human evaluations. In this paper,
we proposed a novel approach EMP-EVAL, a simple yet effective automatic empathy
evaluation method. The proposed technique takes the influence of Emotion,
Cognitive and Emotional empathy. To the best knowledge, our work is the first
to systematically measure empathy without the human-annotated provided scores.
Experimental results demonstrate that our metrics can correlate with human
preference, achieving comparable results with human judgments.",None,-1
Enhanced Masked Image Modeling for Analysis of Dental Panoramic Radiographs,0.457081,"The computer-assisted radiologic informative report has received increasing
research attention to facilitate diagnosis and treatment planning for dental
care providers. However, manual interpretation of dental images is limited,
expensive, and time-consuming. Another barrier in dental imaging is the limited
number of available images for training, which is a challenge in the era of
deep learning. This study proposes a novel self-distillation (SD) enhanced
self-supervised learning on top of the masked image modeling (SimMIM)
Transformer, called SD-SimMIM, to improve the outcome with a limited number of
dental radiographs. In addition to the prediction loss on masked patches,
SD-SimMIM computes the self-distillation loss on the visible patches. We apply
SD-SimMIM on dental panoramic X-rays for teeth numbering, detection of dental
restorations and orthodontic appliances, and instance segmentation tasks. Our
results show that SD-SimMIM outperforms other self-supervised learning methods.
Furthermore, we augment and improve the annotation of an existing dataset of
panoramic X-rays.",None,-1
Ben-ge: Extending BigEarthNet with Geographical and Environmental Data,0.355462,"Deep learning methods have proven to be a powerful tool in the analysis of
large amounts of complex Earth observation data. However, while Earth
observation data are multi-modal in most cases, only single or few modalities
are typically considered. In this work, we present the ben-ge dataset, which
supplements the BigEarthNet-MM dataset by compiling freely and globally
available geographical and environmental data. Based on this dataset, we
showcase the value of combining different data modalities for the downstream
tasks of patch-based land-use/land-cover classification and land-use/land-cover
segmentation. ben-ge is freely available and expected to serve as a test bed
for fully supervised and self-supervised Earth observation applications.",None,-1
Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning,0.541887,"Adapting to regularities of the environment is critical for biological
organisms to anticipate events and plan. A prominent example is the circadian
rhythm corresponding to the internalization by organisms of the $24$-hour
period of the Earth's rotation. In this work, we study the emergence of
circadian-like rhythms in deep reinforcement learning agents. In particular, we
deployed agents in an environment with a reliable periodic variation while
solving a foraging task. We systematically characterize the agent's behavior
during learning and demonstrate the emergence of a rhythm that is endogenous
and entrainable. Interestingly, the internal rhythm adapts to shifts in the
phase of the environmental signal without any re-training. Furthermore, we show
via bifurcation and phase response curve analyses how artificial neurons
develop dynamics to support the internalization of the environmental rhythm.
From a dynamical systems view, we demonstrate that the adaptation proceeds by
the emergence of a stable periodic orbit in the neuron dynamics with a phase
response that allows an optimal phase synchronisation between the agent's
dynamics and the environmental rhythm.",None,-1
Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction,0.74759,"Recently, aspect sentiment quad prediction has received widespread attention
in the field of aspect-based sentiment analysis. Existing studies extract
quadruplets via pre-trained generative language models to paraphrase the
original sentence into a templated target sequence. However, previous works
only focus on what to generate but ignore what not to generate. We argue that
considering the negative samples also leads to potential benefits. In this
work, we propose a template-agnostic method to control the token-level
generation, which boosts original learning and reduces mistakes simultaneously.
Specifically, we introduce Monte Carlo dropout to understand the built-in
uncertainty of pre-trained language models, acquiring the noises and errors. We
further propose marginalized unlikelihood learning to suppress the
uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to
balance the effects of marginalized unlikelihood learning. Extensive
experiments on four public datasets demonstrate the effectiveness of our
approach on various generation templates.",None,-1
"What's ""up"" with vision-language models? Investigating their struggle with spatial reasoning",0.527831,"Recent vision-language (VL) models are powerful, but can they reliably
distinguish ""right"" from ""left""? We curate three new corpora to quantify model
comprehension of such basic spatial relations. These tests isolate spatial
reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp
benchmark contains sets of photographs varying only the spatial relations of
objects, keeping their identity fixed (see Figure 1: models must comprehend not
only the usual case of a dog under a table, but also, the same dog on top of
the same table). We evaluate 18 VL models, finding that all perform poorly,
e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56%
accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of
this surprising behavior, finding: 1) that popular vision-language pretraining
corpora like LAION-2B contain little reliable data for learning spatial
relationships; and 2) that basic modeling interventions like up-weighting
preposition-containing instances or fine-tuning on our corpora are not
sufficient to address the challenges our benchmarks pose. We are hopeful that
these corpora will facilitate further research, and we release our data and
code at https://github.com/amitakamath/whatsup_vlms.",None,-1
Structured Pruning for Multi-Task Deep Neural Networks,0.166413,"Although multi-task deep neural network (DNN) models have computation and
storage benefits over individual single-task DNN models, they can be further
optimized via model compression. Numerous structured pruning methods are
already developed that can readily achieve speedups in single-task models, but
the pruning of multi-task networks has not yet been extensively studied. In
this work, we investigate the effectiveness of structured pruning on multi-task
models. We use an existing single-task filter pruning criterion and also
introduce an MTL-based filter pruning criterion for estimating the filter
importance scores. We prune the model using an iterative pruning strategy with
both pruning methods. We show that, with careful hyper-parameter tuning,
architectures obtained from different pruning methods do not have significant
differences in their performances across tasks when the number of parameters is
similar. We also show that iterative structure pruning may not be the best way
to achieve a well-performing pruned model because, at extreme pruning levels,
there is a high drop in performance across all tasks. But when the same models
are randomly initialized and re-trained, they show better results.",None,-1
General Method for Solving Four Types of SAT Problems,0.641623,"Existing methods provide varying algorithms for different types of Boolean
satisfiability problems (SAT), lacking a general solution framework.
Accordingly, this study proposes a unified framework DCSAT based on integer
programming and reinforcement learning (RL) algorithm to solve different types
of SAT problems such as MaxSAT, Weighted MaxSAT, PMS, WPMS. Specifically, we
first construct a consolidated integer programming representation for four
types of SAT problems by adjusting objective function coefficients. Secondly,
we construct an appropriate reinforcement learning models based on the 0-1
integer programming for SAT problems. Based on the binary tree search
structure, we apply the Monte Carlo tree search (MCTS) method on SAT problems.
Finally, we prove that this method can find all optimal Boolean assignments
based on Wiener-khinchin law of large Numbers. We experimentally verify that
this paradigm can prune the unnecessary search space to find the optimal
Boolean assignments for the problem. Furthermore, the proposed method can
provide diverse labels for supervised learning methods for SAT problems.",None,-1
HybridPoint: Point Cloud Registration Based on Hybrid Point Sampling and Matching,0.0877374,"Patch-to-point matching has become a robust way of point cloud registration.
However, previous patch-matching methods employ superpoints with poor
localization precision as nodes, which may lead to ambiguous patch partitions.
In this paper, we propose a HybridPoint-based network to find more robust and
accurate correspondences. Firstly, we propose to use salient points with
prominent local features as nodes to increase patch repeatability, and
introduce some uniformly distributed points to complete the point cloud, thus
constituting hybrid points. Hybrid points not only have better localization
precision but also give a complete picture of the whole point cloud.
Furthermore, based on the characteristic of hybrid points, we propose a
dual-classes patch matching module, which leverages the matching results of
salient points and filters the matching noise of non-salient points.
Experiments show that our model achieves state-of-the-art performance on
3DMatch, 3DLoMatch, and KITTI odometry, especially with 93.0% Registration
Recall on the 3DMatch dataset. Our code and models are available at
https://github.com/liyih/HybridPoint.",None,-1
A Global Model Approach to Robust Few-Shot SAR Automatic Target Recognition,0.875866,"In real-world scenarios, it may not always be possible to collect hundreds of
labeled samples per class for training deep learning-based SAR Automatic Target
Recognition (ATR) models. This work specifically tackles the few-shot SAR ATR
problem, where only a handful of labeled samples may be available to support
the task of interest. Our approach is composed of two stages. In the first, a
global representation model is trained via self-supervised learning on a large
pool of diverse and unlabeled SAR data. In the second stage, the global model
is used as a fixed feature extractor and a classifier is trained to partition
the feature space given the few-shot support samples, while simultaneously
being calibrated to detect anomalous inputs. Unlike competing approaches which
require a pristine labeled dataset for pretraining via meta-learning, our
approach learns highly transferable features from unlabeled data that have
little-to-no relation to the downstream task. We evaluate our method in
standard and extended MSTAR operating conditions and find it to achieve high
accuracy and robust out-of-distribution detection in many different few-shot
settings. Our results are particularly significant because they show the merit
of a global model approach to SAR ATR, which makes minimal assumptions, and
provides many axes for extendability.",None,-1
OPE-SR: Orthogonal Position Encoding for Designing a Parameter-free Upsampling Module in Arbitrary-scale Image Super-Resolution,0.624222,"Implicit neural representation (INR) is a popular approach for
arbitrary-scale image super-resolution (SR), as a key component of INR,
position encoding improves its representation ability. Motivated by position
encoding, we propose orthogonal position encoding (OPE) - an extension of
position encoding - and an OPE-Upscale module to replace the INR-based
upsampling module for arbitrary-scale image super-resolution. Same as INR, our
OPE-Upscale Module takes 2D coordinates and latent code as inputs; however it
does not require training parameters. This parameter-free feature allows the
OPE-Upscale Module to directly perform linear combination operations to
reconstruct an image in a continuous manner, achieving an arbitrary-scale image
reconstruction. As a concise SR framework, our method has high computing
efficiency and consumes less memory comparing to the state-of-the-art (SOTA),
which has been confirmed by extensive experiments and evaluations. In addition,
our method has comparable results with SOTA in arbitrary scale image
super-resolution. Last but not the least, we show that OPE corresponds to a set
of orthogonal basis, justifying our design principle.",None,-1
A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing,0.949606,"Air-writing refers to virtually writing linguistic characters through hand
gestures in three-dimensional space with six degrees of freedom. This paper
proposes a generic video camera-aided convolutional neural network (CNN) based
air-writing framework. Gestures are performed using a marker of fixed color in
front of a generic video camera, followed by color-based segmentation to
identify the marker and track the trajectory of the marker tip. A pre-trained
CNN is then used to classify the gesture. The recognition accuracy is further
improved using transfer learning with the newly acquired data. The performance
of the system varies significantly on the illumination condition due to
color-based segmentation. In a less fluctuating illumination condition, the
system is able to recognize isolated unistroke numerals of multiple languages.
The proposed framework has achieved 97.7%, 95.4% and 93.7% recognition rates in
person independent evaluations on English, Bengali and Devanagari numerals,
respectively.",None,-1
Low-Light Image Enhancement via Structure Modeling and Guidance,0.999538,"This paper proposes a new framework for low-light image enhancement by
simultaneously conducting the appearance as well as structure modeling. It
employs the structural feature to guide the appearance enhancement, leading to
sharp and realistic results. The structure modeling in our framework is
implemented as the edge detection in low-light images. It is achieved with a
modified generative model via designing a structure-aware feature extractor and
generator. The detected edge maps can accurately emphasize the essential
structural information, and the edge prediction is robust towards the noises in
dark areas. Moreover, to improve the appearance modeling, which is implemented
with a simple U-Net, a novel structure-guided enhancement module is proposed
with structure-guided feature synthesis layers. The appearance modeling, edge
detector, and enhancement module can be trained end-to-end. The experiments are
conducted on representative datasets (sRGB and RAW domains), showing that our
model consistently achieves SOTA performance on all datasets with the same
architecture.",None,-1
Beyond the Prior Forgery Knowledge: Mining Critical Clues for General Face Forgery Detection,0.757616,"Face forgery detection is essential in combating malicious digital face
attacks. Previous methods mainly rely on prior expert knowledge to capture
specific forgery clues, such as noise patterns, blending boundaries, and
frequency artifacts. However, these methods tend to get trapped in local
optima, resulting in limited robustness and generalization capability. To
address these issues, we propose a novel Critical Forgery Mining (CFM)
framework, which can be flexibly assembled with various backbones to boost
their generalization and robustness performance. Specifically, we first build a
fine-grained triplet and suppress specific forgery traces through prior
knowledge-agnostic data augmentation. Subsequently, we propose a fine-grained
relation learning prototype to mine critical information in forgeries through
instance and local similarity-aware losses. Moreover, we design a novel
progressive learning controller to guide the model to focus on principal
feature components, enabling it to learn critical forgery features in a
coarse-to-fine manner. The proposed method achieves state-of-the-art forgery
detection performance under various challenging evaluation settings.",None,-1
Cross-domain Compositing with Pretrained Diffusion Models,0.314547,"Diffusion models have enabled high-quality, conditional image editing
capabilities. We propose to expand their arsenal, and demonstrate that
off-the-shelf diffusion models can be used for a wide range of cross-domain
compositing tasks. Among numerous others, these include image blending, object
immersion, texture-replacement and even CG2Real translation or stylization. We
employ a localized, iterative refinement scheme which infuses the injected
objects with contextual information derived from the background scene, and
enables control over the degree and types of changes the object may undergo. We
conduct a range of qualitative and quantitative comparisons to prior work, and
exhibit that our method produces higher quality and realistic results without
requiring any annotations or training. Finally, we demonstrate how our method
may be used for data augmentation of downstream tasks.",None,-1
Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models,0.936578,"Large-scale text-to-image diffusion models achieve unprecedented success in
image generation and editing. However, how to extend such success to video
editing is unclear. Recent initial attempts at video editing require
significant text-to-video data and computation resources for training, which is
often not accessible. In this work, we propose vid2vid-zero, a simple yet
effective method for zero-shot video editing. Our vid2vid-zero leverages
off-the-shelf image diffusion models, and doesn't require training on any
video. At the core of our method is a null-text inversion module for
text-to-video alignment, a cross-frame modeling module for temporal
consistency, and a spatial regularization module for fidelity to the original
video. Without any training, we leverage the dynamic nature of the attention
mechanism to enable bi-directional temporal modeling at test time. Experiments
and analyses show promising results in editing attributes, subjects, places,
etc., in real-world videos. Code is made available at
\url{https://github.com/baaivision/vid2vid-zero}.",None,-1
"Better ""CMOS"" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution",0.350254,"Most of the existing blind image Super-Resolution (SR) methods assume that
the blur kernels are space-invariant. However, the blur involved in real
applications are usually space-variant due to object motion, out-of-focus,
etc., resulting in severe performance drop of the advanced SR methods. To
address this problem, we firstly introduce two new datasets with out-of-focus
blur, i.e., NYUv2-BSR and Cityscapes-BSR, to support further researches of
blind SR with space-variant blur. Based on the datasets, we design a novel
Cross-MOdal fuSion network (CMOS) that estimate both blur and semantics
simultaneously, which leads to improved SR results. It involves a feature
Grouping Interactive Attention (GIA) module to make the two modalities interact
more effectively and avoid inconsistency. GIA can also be used for the
interaction of other features because of the universality of its structure.
Qualitative and quantitative experiments compared with state-of-the-art methods
on above datasets and real-world images demonstrate the superiority of our
method, e.g., obtaining PSNR/SSIM by +1.91/+0.0048 on NYUv2-BSR than MANet.",None,-1
Benchmarking LLM-based Machine Translation on Cultural Awareness,0.770707,"Translating cultural-specific content is crucial for effective cross-cultural
communication. However, many MT systems still struggle to translate sentences
containing cultural-specific entities accurately and understandably. Recent
advancements in in-context learning utilize lightweight prompts to guide large
language models (LLMs) in machine translation tasks. Nevertheless, the
effectiveness of this approach in enhancing machine translation with cultural
awareness remains uncertain. To address this gap, we introduce a new data
curation pipeline to construct a culturally relevant parallel corpus, enriched
with annotations of cultural-specific items. Furthermore, we devise a novel
evaluation metric to assess the understandability of translations in a
reference-free manner by GPT-4. We evaluate a variety of neural machine
translation (NMT) and LLM-based MT systems using our dataset. Additionally, we
propose several prompting strategies for LLMs to incorporate external and
internal cultural knowledge into the translation process. Our results
demonstrate that eliciting explanations can significantly enhance the
understandability of cultural-specific entities, especially those without
well-known translations.",None,-1
A Parameter-efficient Multi-subject Model for Predicting fMRI Activity,0.209793,"This is the Algonauts 2023 submission report for team ""BlobGPT"". Our model
consists of a multi-subject linear encoding head attached to a pretrained trunk
model. The multi-subject head consists of three components: (1) a shared
multi-layer feature projection, (2) shared plus subject-specific low-dimension
linear transformations, and (3) a shared PCA fMRI embedding. In this report, we
explain these components in more detail and present some experimental results.
Our code is available at https://github.com/cmi-dair/algonauts23.",None,-1
Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer,0.0521554,"Adapting a large language model for multiple-attribute text style transfer
via fine-tuning can be challenging due to the significant amount of
computational resources and labeled data required for the specific task. In
this paper, we address this challenge by introducing AdapterTST, a framework
that freezes the pre-trained model's original parameters and enables the
development of a multiple-attribute text style transfer model. Using BART as
the backbone model, Adapter-TST utilizes different neural adapters to capture
different attribute information, like a plug-in connected to BART. Our method
allows control over multiple attributes, like sentiment, tense, voice, etc.,
and configures the adapters' architecture to generate multiple outputs
respected to attributes or compositional editing on the same sentence. We
evaluate the proposed model on both traditional sentiment transfer and
multiple-attribute transfer tasks. The experiment results demonstrate that
Adapter-TST outperforms all the state-of-the-art baselines with significantly
lesser computational resources. We have also empirically shown that each
adapter is able to capture specific stylistic attributes effectively and can be
configured to perform compositional editing.",None,-1
A Frustratingly Simple Decoding Method for Neural Text Generation,0.151825,"We introduce a frustratingly simple, super efficient and surprisingly
effective decoding method, which we call Frustratingly Simple Decoding (FSD),
for neural text generation. The idea behind FSD is straightforward: we build an
anti-LM based on previously generated text and use this anti-LM to penalize
future generation of what has been generated. The anti-LM can be implemented as
simple as an n-gram language model or a vectorized variant. In this way, FSD
introduces no extra model parameters and negligible computational overhead (FSD
can be as fast as greedy search). Despite the simplicity, FSD is surprisingly
effective; Experiments show that FSD can outperform the canonical methods to
date (i.e., nucleus sampling) as well as several strong baselines that were
proposed recently.",None,-1
INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation,0.492952,"Neural machine translation has achieved promising results on many translation
tasks. However, previous studies have shown that neural models induce a
non-smooth representation space, which harms its generalization results.
Recently, kNN-MT has provided an effective paradigm to smooth the prediction
based on neighbor representations during inference. Despite promising results,
kNN-MT usually requires large inference overhead. We propose an effective
training framework INK to directly smooth the representation space via
adjusting representations of kNN neighbors with a small number of new
parameters. The new parameters are then used to refresh the whole
representation datastore to get new kNN knowledge asynchronously. This loop
keeps running until convergence. Experiments on four benchmark datasets show
that \method achieves average gains of 1.99 COMET and 1.0 BLEU, outperforming
the state-of-the-art kNN-MT system with 0.02x memory space and 1.9x inference
speedup.",None,-1
Split-and-Denoise: Protect large language model inference with local differential privacy,0.258871,"Large Language Models (LLMs) excel in natural language understanding by
capturing hidden semantics in vector space. This process enriches the value of
text embeddings for various downstream tasks, thereby fostering the
Embedding-as-a-Service (EaaS) business model. However, the risk of privacy
leakage due to direct text transmission to servers remains a critical concern.
To address this, we introduce Split-N-Denoise (SnD), an private inference
framework that splits the model to execute the token embedding layer on the
client side at minimal computational cost. This allows the client to introduce
noise prior to transmitting the embeddings to the server, and subsequently
receive and denoise the perturbed output embeddings for downstream tasks. Our
approach is designed for the inference stage of LLMs and requires no
modifications to the model parameters. Extensive experiments demonstrate SnD's
effectiveness in optimizing the privacy-utility tradeoff across various LLM
architectures and diverse downstream tasks. The results reveal an improvement
in performance under the same privacy budget compared to the baselines by over
10\% on average, offering clients a privacy-preserving solution for local
privacy protection.",None,-1
Catch Me If You Can: Improving Adversaries in Cyber-Security With Q-Learning Algorithms,0.583138,"The ongoing rise in cyberattacks and the lack of skilled professionals in the
cybersecurity domain to combat these attacks show the need for automated tools
capable of detecting an attack with good performance. Attackers disguise their
actions and launch attacks that consist of multiple actions, which are
difficult to detect. Therefore, improving defensive tools requires their
calibration against a well-trained attacker. In this work, we propose a model
of an attacking agent and environment and evaluate its performance using basic
Q-Learning, Naive Q-learning, and DoubleQ-Learning, all of which are variants
of Q-Learning. The attacking agent is trained with the goal of exfiltrating
data whereby all the hosts in the network have a non-zero detection
probability. Results show that the DoubleQ-Learning agent has the best overall
performance rate by successfully achieving the goal in $70\%$ of the
interactions.",None,-1
Bodily expressed emotion understanding through integrating Laban movement analysis,0.223238,"Body movements carry important information about a person's emotions or
mental state and are essential in daily communication. Enhancing the ability of
machines to understand emotions expressed through body language can improve the
communication of assistive robots with children and elderly users, provide
psychiatric professionals with quantitative diagnostic and prognostic
assistance, and aid law enforcement in identifying deception. This study
develops a high-quality human motor element dataset based on the Laban Movement
Analysis movement coding system and utilizes that to jointly learn about motor
elements and emotions. Our long-term ambition is to integrate knowledge from
computing, psychology, and performing arts to enable automated understanding
and analysis of emotion and mental state through body language. This work
serves as a launchpad for further research into recognizing emotions through
analysis of human movement.",None,-1
CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models,0.764718,"In this paper, we consider the challenge of summarizing patients' medical
progress notes in a limited data setting. For the Problem List Summarization
(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5
fine-tuned to 765 medical clinic notes outperforms other extractive,
abstractive and zero-shot baselines, yielding reasonable baseline systems for
medical note summarization. Further, we introduce Hierarchical Ensemble of
Summarization Models (HESM), consisting of token-level ensembles of diverse
fine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.
Our HESM approach lead to a considerable summarization performance boost, and
when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which
was the best-performing system at the top of the shared task leaderboard.",None,-1
Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives,0.999981,"Intelligent vehicles (IVs) have gained worldwide attention due to their
increased convenience, safety advantages, and potential commercial value.
Despite predictions of commercial deployment by 2025, implementation remains
limited to small-scale validation, with precise tracking controllers and motion
planners being essential prerequisites for IVs. This paper reviews
state-of-the-art motion planning methods for IVs, including pipeline planning
and end-to-end planning methods. The study examines the selection, expansion,
and optimization operations in a pipeline method, while it investigates
training approaches and validation scenarios for driving tasks in end-to-end
methods. Experimental platforms are reviewed to assist readers in choosing
suitable training and validation strategies. A side-by-side comparison of the
methods is provided to highlight their strengths and limitations, aiding
system-level design choices. Current challenges and future perspectives are
also discussed in this survey.",None,-1
Grand Challenge On Detecting Cheapfakes,0.0942796,"Cheapfake is a recently coined term that encompasses non-AI (""cheap"")
manipulations of multimedia content. Cheapfakes are known to be more prevalent
than deepfakes. Cheapfake media can be created using editing software for
image/video manipulations, or even without using any software, by simply
altering the context of an image/video by sharing the media alongside
misleading claims. This alteration of context is referred to as out-of-context
(OOC) misuse of media. OOC media is much harder to detect than fake media,
since the images and videos are not tampered. In this challenge, we focus on
detecting OOC images, and more specifically the misuse of real photographs with
conflicting image captions in news items. The aim of this challenge is to
develop and benchmark models that can be used to detect whether given samples
(news image and associated captions) are OOC, based on the recently compiled
COSMOS dataset.",None,-1
MidMed: Towards Mixed-Type Dialogues for Medical Consultation,0.654771,"Most medical dialogue systems assume that patients have clear goals (medicine
querying, surgical operation querying, etc.) before medical consultation.
However, in many real scenarios, due to the lack of medical knowledge, it is
usually difficult for patients to determine clear goals with all necessary
slots. In this paper, we identify this challenge as how to construct medical
consultation dialogue systems to help patients clarify their goals. To mitigate
this challenge, we propose a novel task and create a human-to-human mixed-type
medical consultation dialogue corpus, termed MidMed, covering five dialogue
types: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded
dialogue, QA, and chitchat. MidMed covers four departments
(otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175
dialogues. Furthermore, we build baselines on MidMed and propose an
instruction-guiding medical dialogue generation framework, termed InsMed, to
address this task. Experimental results show the effectiveness of InsMed.",None,-1
Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency,0.673689,"Monocular 3D object detection has become a mainstream approach in automatic
driving for its easy application. A prominent advantage is that it does not
need LiDAR point clouds during the inference. However, most current methods
still rely on 3D point cloud data for labeling the ground truths used in the
training phase. This inconsistency between the training and inference makes it
hard to utilize the large-scale feedback data and increases the data collection
expenses. To bridge this gap, we propose a new weakly supervised monocular 3D
objection detection method, which can train the model with only 2D labels
marked on images. To be specific, we explore three types of consistency in this
task, i.e. the projection, multi-view and direction consistency, and design a
weakly-supervised architecture based on these consistencies. Moreover, we
propose a new 2D direction labeling method in this task to guide the model for
accurate rotation direction prediction. Experiments show that our
weakly-supervised method achieves comparable performance with some fully
supervised methods. When used as a pre-training method, our model can
significantly outperform the corresponding fully-supervised baseline with only
1/3 3D labels. https://github.com/weakmono3d/weakmono3d",None,-1
Double A3C: Deep Reinforcement Learning on OpenAI Gym Games,0.0548555,"Reinforcement Learning (RL) is an area of machine learning figuring out how
agents take actions in an unknown environment to maximize its rewards. Unlike
classical Markov Decision Process (MDP) in which agent has full knowledge of
its state, rewards, and transitional probability, reinforcement learning
utilizes exploration and exploitation for the model uncertainty. Under the
condition that the model usually has a large state space, a neural network (NN)
can be used to correlate its input state to its output actions to maximize the
agent's rewards. However, building and training an efficient neural network is
challenging. Inspired by Double Q-learning and Asynchronous Advantage
Actor-Critic (A3C) algorithm, we will propose and implement an improved version
of Double A3C algorithm which utilizing the strength of both algorithms to play
OpenAI Gym Atari 2600 games to beat its benchmarks for our project.",None,-1
Orientation-Guided Contrastive Learning for UAV-View Geo-Localisation,0.640985,"Retrieving relevant multimedia content is one of the main problems in a world
that is increasingly data-driven. With the proliferation of drones, high
quality aerial footage is now available to a wide audience for the first time.
Integrating this footage into applications can enable GPS-less geo-localisation
or location correction.
  In this paper, we present an orientation-guided training framework for
UAV-view geo-localisation. Through hierarchical localisation orientations of
the UAV images are estimated in relation to the satellite imagery. We propose a
lightweight prediction module for these pseudo labels which predicts the
orientation between the different views based on the contrastive learned
embeddings. We experimentally demonstrate that this prediction supports the
training and outperforms previous approaches. The extracted pseudo-labels also
enable aligned rotation of the satellite image as augmentation to further
strengthen the generalisation. During inference, we no longer need this
orientation module, which means that no additional computations are required.
We achieve state-of-the-art results on both the University-1652 and
University-160k datasets.",None,-1
Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in AIoT-enabled Vehicular Metaverses with Trajectory Prediction,0.968543,"Avatars, as promising digital assistants in Vehicular Metaverses, can enable
drivers and passengers to immerse in 3D virtual spaces, serving as a practical
emerging example of Artificial Intelligence of Things (AIoT) in intelligent
vehicular environments. The immersive experience is achieved through seamless
human-avatar interaction, e.g., augmented reality navigation, which requires
intensive resources that are inefficient and impractical to process on
intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide
Units (RSUs) or cloud servers for remote execution can effectively reduce
resource consumption. However, the high mobility of vehicles, the dynamic
workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making
avatar migration decisions. To address these challenges, in this paper, we
propose a dynamic migration framework for avatar tasks based on real-time
trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL).
Specifically, we propose a model to predict the future trajectories of
intelligent vehicles based on their historical data, indicating the future
workloads of RSUs.Based on the expected workloads of RSUs, we formulate the
avatar task migration problem as a long-term mixed integer programming problem.
To tackle this problem efficiently, the problem is transformed into a Partially
Observable Markov Decision Process (POMDP) and solved by multiple DRL agents
with hybrid continuous and discrete actions in decentralized. Numerical results
demonstrate that our proposed algorithm can effectively reduce the latency of
executing avatar tasks by around 25% without prediction and 30% with prediction
and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse
(AeVeM).",None,-1
Spectrum-guided Multi-granularity Referring Video Object Segmentation,0.790169,"Current referring video object segmentation (R-VOS) techniques extract
conditional kernels from encoded (low-resolution) vision-language features to
segment the decoded high-resolution features. We discovered that this causes
significant feature drift, which the segmentation kernels struggle to perceive
during the forward computation. This negatively affects the ability of
segmentation kernels. To address the drift problem, we propose a
Spectrum-guided Multi-granularity (SgMg) approach, which performs direct
segmentation on the encoded features and employs visual details to further
optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion
(SCF) to perform intra-frame global interactions in the spectral domain for
effective multimodal representation. Finally, we extend SgMg to perform
multi-object R-VOS, a new paradigm that enables simultaneous segmentation of
multiple referred objects in a video. This not only makes R-VOS faster, but
also more practical. Extensive experiments show that SgMg achieves
state-of-the-art performance on four video benchmark datasets, outperforming
the nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg
enables multi-object R-VOS, runs about 3 times faster while maintaining
satisfactory performance. Code is available at https://github.com/bo-miao/SgMg.",None,-1
EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras,0.559424,"Falls are significant and often fatal for vulnerable populations such as the
elderly. Previous works have addressed the detection of falls by relying on
data capture by a single sensor, images or accelerometers. In this work, we
rely on multimodal descriptors extracted from videos captured by egocentric
cameras. Our proposed method includes a late decision fusion layer that builds
on top of the extracted descriptors. Furthermore, we collect a new dataset on
which we assess our proposed approach. We believe this is the first public
dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects.
We conducted ablation experiments to assess the performance of individual
feature extractors, fusion of visual information, and fusion of both visual and
audio information. Moreover, we experimented with internal and external
cross-validation. Our results demonstrate that the fusion of audio and visual
information through late decision fusion improves detection performance, making
it a promising tool for fall prevention and mitigation.",None,-1
Benchmarking Large Language Model Capabilities for Conditional Generation,0.562593,"Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.",None,-1
Uncertainty-based Detection of Adversarial Attacks in Semantic Segmentation,0.11372,"State-of-the-art deep neural networks have proven to be highly powerful in a
broad range of tasks, including semantic image segmentation. However, these
networks are vulnerable against adversarial attacks, i.e., non-perceptible
perturbations added to the input image causing incorrect predictions, which is
hazardous in safety-critical applications like automated driving. Adversarial
examples and defense strategies are well studied for the image classification
task, while there has been limited research in the context of semantic
segmentation. First works however show that the segmentation outcome can be
severely distorted by adversarial attacks. In this work, we introduce an
uncertainty-based approach for the detection of adversarial attacks in semantic
segmentation. We observe that uncertainty as for example captured by the
entropy of the output distribution behaves differently on clean and perturbed
images and leverage this property to distinguish between the two cases. Our
method works in a light-weight and post-processing manner, i.e., we do not
modify the model or need knowledge of the process used for generating
adversarial examples. In a thorough empirical analysis, we demonstrate the
ability of our approach to detect perturbed images across multiple types of
adversarial attacks.",None,-1
Efficient and Explicit Modelling of Image Hierarchies for Image Restoration,0.999876,"The aim of this paper is to propose a mechanism to efficiently and explicitly
model image hierarchies in the global, regional, and local range for image
restoration. To achieve that, we start by analyzing two important properties of
natural images including cross-scale similarity and anisotropic image features.
Inspired by that, we propose the anchored stripe self-attention which achieves
a good balance between the space and time complexity of self-attention and the
modelling capacity beyond the regional range. Then we propose a new network
architecture dubbed GRL to explicitly model image hierarchies in the Global,
Regional, and Local range via anchored stripe self-attention, window
self-attention, and channel attention enhanced convolution. Finally, the
proposed network is applied to 7 image restoration types, covering both real
and synthetic settings. The proposed method sets the new state-of-the-art for
several of those. Code will be available at
https://github.com/ofsoundof/GRL-Image-Restoration.git.",None,-1
Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,0.925944,"Large language models (LLMs) demonstrate impressive multilingual capability,
but their performance varies substantially across different languages. In this
work, we introduce a simple yet effective method, called cross-lingual-thought
prompting (XLT), to systematically improve the multilingual capability of LLMs.
Specifically, XLT is a generic template prompt that stimulates cross-lingual
and logical reasoning skills to enhance task performance across languages. We
conduct comprehensive evaluations on 7 typical benchmarks related to reasoning,
understanding, and generation tasks, covering both high-resource and
low-resource languages. Experimental results show that XLT not only remarkably
enhances the performance of various multilingual tasks but also significantly
reduces the gap between the average performance and the best performance of
each task in different languages. Notably, XLT brings over 10 points of average
improvement in arithmetic reasoning and open-domain question-answering tasks.",None,-1
Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,0.594047,"Recent model editing techniques promise to mitigate the problem of memorizing
false or outdated associations during LLM training. However, we show that these
techniques can introduce large unwanted side effects which are not detected by
existing specificity benchmarks. We extend the existing CounterFact benchmark
to include a dynamic component and dub our benchmark CounterFact+.
Additionally, we extend the metrics used for measuring specificity by a
principled KL divergence-based metric. We use this improved benchmark to
evaluate recent model editing techniques and find that they suffer from low
specificity. Our findings highlight the need for improved specificity
benchmarks that identify and prevent unwanted side effects.",None,-1
Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation,0.420911,"Self-supervised learning (SSL) can be used to solve complex visual tasks
without human labels. Self-supervised representations encode useful semantic
information about images, and as a result, they have already been used for
tasks such as unsupervised semantic segmentation. In this paper, we investigate
self-supervised representations for instance segmentation without any manual
annotations. We find that the features of different SSL methods vary in their
level of instance-awareness. In particular, DINO features, which are known to
be excellent semantic descriptors, lack behind MAE features in their
sensitivity for separating instances.",None,-1
Towards preserving word order importance through Forced Invalidation,0.0453579,"Large pre-trained language models such as BERT have been widely used as a
framework for natural language understanding (NLU) tasks. However, recent
findings have revealed that pre-trained language models are insensitive to word
order. The performance on NLU tasks remains unchanged even after randomly
permuting the word of a sentence, where crucial syntactic information is
destroyed. To help preserve the importance of word order, we propose a simple
approach called Forced Invalidation (FI): forcing the model to identify
permuted sequences as invalid samples. We perform an extensive evaluation of
our approach on various English NLU and QA based tasks over BERT-based and
attention-based models over word embeddings. Our experiments demonstrate that
Forced Invalidation significantly improves the sensitivity of the models to
word order.",None,-1
"Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation",0.7539,"Interpreting ancient Chinese has been the key to comprehending vast Chinese
literature, tradition, and civilization. In this paper, we propose Erya for
ancient Chinese translation. From a dataset perspective, we collect, clean, and
classify ancient Chinese materials from various sources, forming the most
extensive ancient Chinese resource to date. From a model perspective, we devise
Erya training method oriented towards ancient Chinese. We design two
jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked
language model (DMLM). From an evaluation perspective, we build a benchmark to
judge ancient Chinese translation quality in different scenarios and evaluate
the ancient Chinese translation capacities of various existing models. Our
model exhibits remarkable zero-shot performance across five domains, with over
+12.0 BLEU against GPT-3.5 models and better human evaluation results than
ERNIE Bot. Subsequent fine-tuning further shows the superior transfer
capability of Erya model with +6.2 BLEU gain. We release all the
above-mentioned resources at https://github.com/RUCAIBox/Erya.",None,-1
Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation,0.275396,"A challenge in the Dialogue State Tracking (DST) field is adapting models to
new domains without using any supervised data, zero-shot domain adaptation.
Parameter-Efficient Transfer Learning (PETL) has the potential to address this
problem due to its robustness. However, it has yet to be applied to the
zero-shot scenarios, as it is not clear how to apply it unsupervisedly.
  Our method, Prompter, uses descriptions of target domain slots to generate
dynamic prefixes that are concatenated to the key and values at each layer's
self-attention mechanism. This allows for the use of prefix-tuning in
zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD
benchmarks. In generating prefixes, our analyses find that Prompter not only
utilizes the semantics of slot descriptions but also how often the slots appear
together in conversation. Moreover, Prompter's gains are due to its improved
ability to distinguish ""none""-valued dialogue slots, compared against
baselines.",None,-1
Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data,0.0602113,"Tabular data is the most common format to publish and exchange structured
data online. A clear example is the growing number of open data portals
published by all types of public administrations. However, exploitation of
these data sources is currently limited to technical people able to
programmatically manipulate and digest such data. As an alternative, we propose
the use of chatbots to offer a conversational interface to facilitate the
exploration of tabular data sources. With our approach, any regular citizen can
benefit and leverage them. Moreover, our chatbots are not manually created:
instead, they are automatically generated from the data source itself thanks to
the instantiation of a configurable collection of conversation patterns.",None,-1
Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning,0.0587663,"We propose a model-free reinforcement learning solution, namely the ASAP-Phi
framework, to encourage an agent to fulfill a formal specification ASAP. The
framework leverages a piece-wise reward function that assigns quantitative
semantic reward to traces not satisfying the specification, and a high constant
reward to the remaining. Then, it trains an agent with an actor-critic-based
algorithm, such as soft actor-critic (SAC), or deep deterministic policy
gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that
prioritize fulfilling a specification ASAP. Extensive experiments are run,
including ablation studies, on state-of-the-art benchmarks. Results show that
our framework succeeds in finding sufficiently fast trajectories for up to 97\%
test cases and defeats baselines.",None,-1
DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4,0.199629,"Human preference judgments are pivotal in guiding large language models
(LLMs) to produce outputs that align with human values. Human evaluations are
also used in summarization tasks to compare outputs from various systems,
complementing existing automatic metrics. Despite their significance, however,
there has been limited research probing these pairwise or $k$-wise comparisons.
The collective impact and relative importance of factors such as output length,
informativeness, fluency, and factual consistency are still not well
understood. It is also unclear if there are other hidden factors influencing
human judgments. In this paper, we conduct an in-depth examination of a
collection of pairwise human judgments released by OpenAI. Utilizing the
Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in
these human judgments. We find that the most favored factors vary across tasks
and genres, whereas the least favored factors tend to be consistent, e.g.,
outputs are too brief, contain excessive off-focus content or hallucinated
facts. Our findings have implications on the construction of balanced datasets
in human preference evaluations, which is a crucial step in shaping the
behaviors of future LLMs.",None,-1
Lexical Diversity in Kinship Across Languages and Dialects,0.458546,"Languages are known to describe the world in diverse ways. Across lexicons,
diversity is pervasive, appearing through phenomena such as lexical gaps and
untranslatability. However, in computational resources, such as multilingual
lexical databases, diversity is hardly ever represented. In this paper, we
introduce a method to enrich computational lexicons with content relating to
linguistic diversity. The method is verified through two large-scale case
studies on kinship terminology, a domain known to be diverse across languages
and cultures: one case study deals with seven Arabic dialects, while the other
one with three Indonesian languages. Our results, made available as browseable
and downloadable computational resources, extend prior linguistics research on
kinship terminology, and provide insight into the extent of diversity even
within linguistically and culturally close communities.",None,-1
Incorporating Transformer Designs into Convolutions for Lightweight Image Super-Resolution,0.32162,"In recent years, the use of large convolutional kernels has become popular in
designing convolutional neural networks due to their ability to capture
long-range dependencies and provide large receptive fields. However, the
increase in kernel size also leads to a quadratic growth in the number of
parameters, resulting in heavy computation and memory requirements. To address
this challenge, we propose a neighborhood attention (NA) module that upgrades
the standard convolution with a self-attention mechanism. The NA module
efficiently extracts long-range dependencies in a sliding window pattern,
thereby achieving similar performance to large convolutional kernels but with
fewer parameters.
  Building upon the NA module, we propose a lightweight single image
super-resolution (SISR) network named TCSR. Additionally, we introduce an
enhanced feed-forward network (EFFN) in TCSR to improve the SISR performance.
EFFN employs a parameter-free spatial-shift operation for efficient feature
aggregation. Our extensive experiments and ablation studies demonstrate that
TCSR outperforms existing lightweight SISR methods and achieves
state-of-the-art performance. Our codes are available at
\url{https://github.com/Aitical/TCSR}.",None,-1
LED: A Dataset for Life Event Extraction from Dialogs,0.422646,"Lifelogging has gained more attention due to its wide applications, such as
personalized recommendations or memory assistance. The issues of collecting and
extracting personal life events have emerged. People often share their life
experiences with others through conversations. However, extracting life events
from conversations is rarely explored. In this paper, we present Life Event
Dialog, a dataset containing fine-grained life event annotations on
conversational data. In addition, we initiate a novel conversational life event
extraction task and differentiate the task from the public event extraction or
the life event extraction from other sources like microblogs. We explore three
information extraction (IE) frameworks to address the conversational life event
extraction task: OpenIE, relation extraction, and event extraction. A
comprehensive empirical analysis of the three baselines is established. The
results suggest that the current event extraction model still struggles with
extracting life events from human daily conversations. Our proposed life event
dialog dataset and in-depth analysis of IE frameworks will facilitate future
research on life event extraction from conversations.",None,-1
Multi-Agent Reinforcement Learning Guided by Signal Temporal Logic Specifications,0.663756,"Reward design is a key component of deep reinforcement learning, yet some
tasks and designer's objectives may be unnatural to define as a scalar cost
function. Among the various techniques, formal methods integrated with DRL have
garnered considerable attention due to their expressiveness and flexibility to
define the reward and requirements for different states and actions of the
agent. However, how to leverage Signal Temporal Logic (STL) to guide
multi-agent reinforcement learning reward design remains unexplored. Complex
interactions, heterogeneous goals and critical safety requirements in
multi-agent systems make this problem even more challenging. In this paper, we
propose a novel STL-guided multi-agent reinforcement learning framework. The
STL requirements are designed to include both task specifications according to
the objective of each agent and safety specifications, and the robustness
values of the STL specifications are leveraged to generate rewards. We validate
the advantages of our method through empirical studies. The experimental
results demonstrate significant reward performance improvements compared to
MARL without STL guidance, along with a remarkable increase in the overall
safety rate of the multi-agent systems.",None,-1
Structured Epipolar Matcher for Local Feature Matching,0.168567,"Local feature matching is challenging due to textureless and repetitive
patterns. Existing methods focus on using appearance features and global
interaction and matching, while the importance of geometry priors in local
feature matching has not been fully exploited. Different from these methods, in
this paper, we delve into the importance of geometry prior and propose
Structured Epipolar Matcher (SEM) for local feature matching, which can
leverage the geometric information in an iterative matching way. The proposed
model enjoys several merits. First, our proposed Structured Feature Extractor
can model the relative positional relationship between pixels and
high-confidence anchor points. Second, our proposed Epipolar Attention and
Matching can filter out irrelevant areas by utilizing the epipolar constraint.
Extensive experimental results on five standard benchmarks demonstrate the
superior performance of our SEM compared to state-of-the-art methods. Project
page: https://sem2023.github.io.",None,-1
ColonMapper: topological mapping and localization for colonoscopy,0.0883493,"We propose a topological mapping and localization system able to operate on
real human colonoscopies, despite significant shape and illumination changes.
The map is a graph where each node codes a colon location by a set of real
images, while edges represent traversability between nodes. For close-in-time
images, where scene changes are minor, place recognition can be successfully
managed with the recent transformers-based local feature matching algorithms.
However, under long-term changes -- such as different colonoscopies of the same
patient -- feature-based matching fails. To address this, we train on real
colonoscopies a deep global descriptor achieving high recall with significant
changes in the scene. The addition of a Bayesian filter boosts the accuracy of
long-term place recognition, enabling relocalization in a previously built map.
Our experiments show that ColonMapper is able to autonomously build a map and
localize against it in two important use cases: localization within the same
colonoscopy or within different colonoscopies of the same patient. Code will be
available upon acceptance.",None,-1
Improving Stability in Simultaneous Speech Translation: A Revision-Controllable Decoding Approach,0.371666,"Simultaneous Speech-to-Text translation serves a critical role in real-time
crosslingual communication. Despite the advancements in recent years,
challenges remain in achieving stability in the translation process, a concern
primarily manifested in the flickering of partial results. In this paper, we
propose a novel revision-controllable method designed to address this issue.
Our method introduces an allowed revision window within the beam search pruning
process to screen out candidate translations likely to cause extensive
revisions, leading to a substantial reduction in flickering and, crucially,
providing the capability to completely eliminate flickering. The experiments
demonstrate the proposed method can significantly improve the decoding
stability without compromising substantially on the translation quality.",None,-1
Noise-Robust Dense Retrieval via Contrastive Alignment Post Training,0.103225,"The success of contextual word representations and advances in neural
information retrieval have made dense vector-based retrieval a standard
approach for passage and document ranking. While effective and efficient,
dual-encoders are brittle to variations in query distributions and noisy
queries. Data augmentation can make models more robust but introduces overhead
to training set generation and requires retraining and index regeneration. We
present Contrastive Alignment POst Training (CAPOT), a highly efficient
finetuning method that improves model robustness without requiring index
regeneration, the training set optimization, or alteration. CAPOT enables
robust retrieval by freezing the document encoder while the query encoder
learns to align noisy queries with their unaltered root. We evaluate CAPOT
noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,
finding CAPOT has a similar impact as data augmentation with none of its
overhead.",None,-1
Event Causality Extraction with Event Argument Correlations,0.720348,"Event Causality Identification (ECI), which aims to detect whether a
causality relation exists between two given textual events, is an important
task for event causality understanding. However, the ECI task ignores crucial
event structure and cause-effect causality component information, making it
struggle for downstream applications. In this paper, we explore a novel task,
namely Event Causality Extraction (ECE), aiming to extract the cause-effect
event causality pairs with their structured event information from plain texts.
The ECE task is more challenging since each event can contain multiple event
arguments, posing fine-grained correlations between events to decide the
causeeffect event pair. Hence, we propose a method with a dual grid tagging
scheme to capture the intra- and inter-event argument correlations for ECE.
Further, we devise a event type-enhanced model architecture to realize the dual
grid tagging scheme. Experiments demonstrate the effectiveness of our method,
and extensive analyses point out several future directions for ECE.",None,-1
From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,0.994528,"Understanding emotions during conversation is a fundamental aspect of human
communication, driving NLP research for Emotion Recognition in Conversation
(ERC). While considerable research has focused on discerning emotions of
individual speakers in monolingual dialogues, understanding the emotional
dynamics in code-mixed conversations has received relatively less attention.
This motivates our undertaking of ERC for code-mixed conversations in this
study. Recognizing that emotional intelligence encompasses a comprehension of
worldly knowledge, we propose an innovative approach that integrates
commonsense information with dialogue context to facilitate a deeper
understanding of emotions. To achieve this, we devise an efficient pipeline
that extracts relevant commonsense from existing knowledge graphs based on the
code-mixed input. Subsequently, we develop an advanced fusion technique that
seamlessly combines the acquired commonsense information with the dialogue
representation obtained from a dedicated dialogue understanding module. Our
comprehensive experimentation showcases the substantial performance improvement
obtained through the systematic incorporation of commonsense in ERC. Both
quantitative assessments and qualitative analyses further corroborate the
validity of our hypothesis, reaffirming the pivotal role of commonsense
integration in enhancing ERC.",None,-1
Deriving Language Models from Masked Language Models,0.168826,"Masked language models (MLM) do not explicitly define a distribution over
language, i.e., they are not language models per se. However, recent work has
implicitly treated them as such for the purposes of generation and scoring.
This paper studies methods for deriving explicit joint distributions from MLMs,
focusing on distributions over two tokens, which makes it possible to calculate
exact distributional properties. We find that an approach based on identifying
joints whose conditionals are closest to those of the MLM works well and
outperforms existing Markov random field-based approaches. We further find that
this derived model's conditionals can even occasionally outperform the original
MLM's conditionals.",None,-1
Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,0.639458,"Multi-target multi-camera tracking (MTMCT) of vehicles, i.e. tracking
vehicles across multiple cameras, is a crucial application for the development
of smart city and intelligent traffic system. The main challenges of MTMCT of
vehicles include the intra-class variability of the same vehicle and
inter-class similarity between different vehicles and how to associate the same
vehicle accurately across different cameras under large search space. Previous
methods for MTMCT usually use hierarchical clustering of trajectories to
conduct cross camera association. However, the search space can be large and
does not take spatial and temporal information into consideration. In this
paper, we proposed a transformer-based camera link model with spatial and
temporal filtering to conduct cross camera tracking. Achieving 73.68% IDF1 on
the Nvidia Cityflow V2 dataset test set, showing the effectiveness of our
camera link model on multi-target multi-camera tracking.",None,-1
Clover: Closed-Loop Verifiable Code Generation,0.230659,"The use of large language models for code generation is a rapidly growing
trend in software development. However, without effective methods for ensuring
the correctness of generated code, this trend could lead to any number of
undesirable outcomes. In this paper, we lay out a vision for addressing this
challenge: the Clover paradigm, short for Closed-Loop Verifiable Code
Generation, which reduces correctness checking to the more accessible problem
of consistency checking. At the core of Clover lies a checker that performs
consistency checks among code, docstrings, and formal annotations. The checker
is implemented using a novel integration of formal verification tools and large
language models. We provide a theoretical analysis to support our thesis that
Clover should be effective at consistency checking. We also empirically
investigate its feasibility on a hand-designed dataset (CloverBench) featuring
annotated Dafny programs at a textbook level of difficulty. Experimental
results show that for this dataset, (i) LLMs are reasonably successful at
automatically generating formal specifications; and (ii) our consistency
checker achieves a promising acceptance rate (up to 87%) for correct instances
while maintaining zero tolerance for incorrect ones (no false positives).",None,-1
An Information-Theoretic Perspective on Variance-Invariance-Covariance Regularization,0.432141,"Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised
learning (SSL) method that has shown promising results on a variety of tasks.
However, the fundamental mechanisms underlying VICReg remain unexplored. In
this paper, we present an information-theoretic perspective on the VICReg
objective. We begin by deriving information-theoretic quantities for
deterministic networks as an alternative to unrealistic stochastic network
assumptions. We then relate the optimization of the VICReg objective to mutual
information optimization, highlighting underlying assumptions and facilitating
a constructive comparison with other SSL algorithms and derive a generalization
bound for VICReg, revealing its inherent advantages for downstream tasks.
Building on these results, we introduce a family of SSL methods derived from
information-theoretic principles that outperform existing SSL techniques.",None,-1
Probability-based Global Cross-modal Upsampling for Pansharpening,0.712391,"Pansharpening is an essential preprocessing step for remote sensing image
processing. Although deep learning (DL) approaches performed well on this task,
current upsampling methods used in these approaches only utilize the local
information of each pixel in the low-resolution multispectral (LRMS) image
while neglecting to exploit its global information as well as the cross-modal
information of the guiding panchromatic (PAN) image, which limits their
performance improvement. To address this issue, this paper develops a novel
probability-based global cross-modal upsampling (PGCU) method for
pan-sharpening. Precisely, we first formulate the PGCU method from a
probabilistic perspective and then design an efficient network module to
implement it by fully utilizing the information mentioned above while
simultaneously considering the channel specificity. The PGCU module consists of
three blocks, i.e., information extraction (IE), distribution and expectation
estimation (DEE), and fine adjustment (FA). Extensive experiments verify the
superiority of the PGCU method compared with other popular upsampling methods.
Additionally, experiments also show that the PGCU module can help improve the
performance of existing SOTA deep learning pansharpening methods. The codes are
available at https://github.com/Zeyu-Zhu/PGCU.",None,-1
Deep Learning in Healthcare: An In-Depth Analysis,0.0362029,"Deep learning (DL) along with never-ending advancements in computational
processing and cloud technologies have bestowed us powerful analyzing tools and
techniques in the past decade and enabled us to use and apply them in various
fields of study. Health informatics is not an exception, and conversely, is the
discipline that generates the most amount of data in today's era and can
benefit from DL the most. Extracting features and finding complex patterns from
a huge amount of raw data and transforming them into knowledge is a challenging
task. Besides, various DL architectures have been proposed by researchers
throughout the years to tackle different problems. In this paper, we provide a
review of DL models and their broad application in bioinformatics and
healthcare categorized by their architecture. In addition, we also go over some
of the key challenges that still exist and can show up while conducting DL
research.",None,-1
Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation,0.566415,"Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel ""Fighting Fire with Fire"" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.",None,-1
AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL,0.921708,"LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present
AutoPlanBench, a novel method for automatically converting planning benchmarks
written in PDDL into textual descriptions and offer a benchmark dataset created
with our method. We show that while the best LLM planners do well on some
planning tasks, others remain out of reach of current methods.",None,-1
T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation,0.410935,"Warning: This paper contains several contents that may be toxic, harmful, or
offensive.
  In the last few years, text-to-image generative models have gained remarkable
success in generating images with unprecedented quality accompanied by a
breakthrough of inference speed. Despite their rapid progress, human biases
that manifest in the training examples, particularly with regard to common
stereotypical biases, like gender and skin tone, still have been found in these
generative models. In this work, we seek to measure more complex human biases
exist in the task of text-to-image generations. Inspired by the well-known
Implicit Association Test (IAT) from social psychology, we propose a novel
Text-to-Image Association Test (T2IAT) framework that quantifies the implicit
stereotypes between concepts and valence, and those in the images. We replicate
the previously documented bias tests on generative models, including morally
neutral tests on flowers and insects as well as demographic stereotypical tests
on diverse social attributes. The results of these experiments demonstrate the
presence of complex stereotypical behaviors in image generations.",None,-1
Knowledge-aware Bayesian Co-attention for Multimodal Emotion Recognition,0.617307,"Multimodal emotion recognition is a challenging research area that aims to
fuse different modalities to predict human emotion. However, most existing
models that are based on attention mechanisms have difficulty in learning
emotionally relevant parts on their own. To solve this problem, we propose to
incorporate external emotion-related knowledge in the co-attention based fusion
of pre-trained models. To effectively incorporate this knowledge, we enhance
the co-attention model with a Bayesian attention module (BAM) where a prior
distribution is estimated using the emotion-related knowledge. Experimental
results on the IEMOCAP dataset show that the proposed approach can outperform
several state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).",None,-1
Context-faithful Prompting for Large Language Models,0.210835,"Large language models (LLMs) encode parametric knowledge about world facts
and have shown remarkable performance in knowledge-driven NLP tasks. However,
their reliance on parametric knowledge may cause them to overlook contextual
cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,
knowledge acquisition tasks). In this paper, we seek to assess and enhance
LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction
with abstention. We demonstrate that LLMs' faithfulness can be significantly
improved using carefully designed prompting strategies. In particular, we
identify opinion-based prompts and counterfactual demonstrations as the most
effective methods. Opinion-based prompts reframe the context as a narrator's
statement and inquire about the narrator's opinions, while counterfactual
demonstrations use instances containing false facts to improve faithfulness in
knowledge conflict situations. Neither technique requires additional training.
We conduct experiments on three datasets of two standard NLP tasks, machine
reading comprehension and relation extraction, and the results demonstrate
significant improvement in faithfulness to contexts. Code and data are released
at https://github.com/wzhouad/context-faithful-llm.",None,-1
"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",0.999719,"Conversational artificial intelligence (AI) disrupts how humans interact with
technology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue
model that can converse with its human counterparts with unprecedented
capabilities. ChatGPT has witnessed tremendous attention from the media,
academia, industry, and the general public, attracting more than a million
users within days of its release. However, its explosive adoption for
information search and as an automated decision aid underscores the importance
to understand its limitations and biases. This paper focuses on one of
democratic society's most important decision-making processes: political
elections. Prompting ChatGPT with 630 political statements from two leading
voting advice applications and the nation-agnostic political compass test in
three pre-registered experiments, we uncover ChatGPT's pro-environmental,
left-libertarian ideology. For example, ChatGPT would impose taxes on flights,
restrict rent increases, and legalize abortion. In the 2021 elections, it would
have voted most likely for the Greens both in Germany (B\""undnis 90/Die
Gr\""unen) and in the Netherlands (GroenLinks). Our findings are robust when
negating the prompts, reversing the order of the statements, varying prompt
formality, and across languages (English, German, Dutch, and Spanish). We
conclude by discussing the implications of politically biased conversational AI
on society.",None,-1
Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach,0.97689,"StarCraft II is a challenging benchmark for AI agents due to the necessity of
both precise micro level operations and strategic macro awareness. Previous
works, such as Alphastar and SCC, achieve impressive performance on tackling
StarCraft II , however, still exhibit deficiencies in long term strategic
planning and strategy interpretability. Emerging large language model (LLM)
agents, such as Voyage and MetaGPT, presents the immense potential in solving
intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs
on StarCraft II, a highly complex RTS game.To conveniently take full advantage
of LLMs` reasoning abilities, we first develop textual StratCraft II
environment, called TextStarCraft II, which LLM agent can interact. Secondly,
we propose a Chain of Summarization method, including single frame
summarization for processing raw observations and multi frame summarization for
analyzing game information, providing command recommendations, and generating
strategic decisions. Our experiment consists of two parts: first, an evaluation
by human experts, which includes assessing the LLMs`s mastery of StarCraft II
knowledge and the performance of LLM agents in the game; second, the in game
performance of LLM agents, encompassing aspects like win rate and the impact of
Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the
relevant knowledge and complex planning abilities needed to address StarCraft
II scenarios; 2. Human experts consider the performance of LLM agents to be
close to that of an average player who has played StarCraft II for eight years;
3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)
difficulty level. We have open sourced the code and released demo videos of LLM
agent playing StarCraft II.",None,-1
NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization,0.900466,"Monocular 3D object localization in driving scenes is a crucial task, but
challenging due to its ill-posed nature. Estimating 3D coordinates for each
pixel on the object surface holds great potential as it provides dense 2D-3D
geometric constraints for the underlying PnP problem. However, high-quality
ground truth supervision is not available in driving scenes due to sparsity and
various artifacts of Lidar data, as well as the practical infeasibility of
collecting per-instance CAD models. In this work, we present NeurOCS, a
framework that uses instance masks and 3D boxes as input to learn 3D object
shapes by means of differentiable rendering, which further serves as
supervision for learning dense object coordinates. Our approach rests on
insights in learning a category-level shape prior directly from real driving
scenes, while properly handling single-view ambiguities. Furthermore, we study
and make critical design choices to learn object coordinates more effectively
from an object-centric view. Altogether, our framework leads to new
state-of-the-art in monocular 3D localization that ranks 1st on the
KITTI-Object benchmark among published monocular methods.",None,-1
InstructDiffusion: A Generalist Modeling Interface for Vision Tasks,0.859959,"We present InstructDiffusion, a unifying and generic framework for aligning
computer vision tasks with human instructions. Unlike existing approaches that
integrate prior knowledge and pre-define the output space (e.g., categories and
coordinates) for each vision task, we cast diverse vision tasks into a
human-intuitive image-manipulating process whose output space is a flexible and
interactive pixel space. Concretely, the model is built upon the diffusion
process and is trained to predict pixels according to user instructions, such
as encircling the man's left shoulder in red or applying a blue mask to the
left car. InstructDiffusion could handle a variety of vision tasks, including
understanding tasks (such as segmentation and keypoint detection) and
generative tasks (such as editing and enhancement). It even exhibits the
ability to handle unseen tasks and outperforms prior methods on novel datasets.
This represents a significant step towards a generalist modeling interface for
vision tasks, advancing artificial general intelligence in the field of
computer vision.",None,-1
CipherSniffer: Classifying Cipher Types,0.0419295,"Ciphers are a powerful tool for encrypting communication. There are many
different cipher types, which makes it computationally expensive to solve a
cipher using brute force. In this paper, we frame the decryption task as a
classification problem. We first create a dataset of transpositions,
substitutions, text reversals, word reversals, sentence shifts, and unencrypted
text. Then, we evaluate the performance of various tokenizer-model combinations
on this task.",None,-1
Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning,0.0999024,"Recent advanced methods in Natural Language Understanding for Task-oriented
Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a
large amount of annotated data to achieve competitive performance. In reality,
token-level annotations (slot labels) are time-consuming and difficult to
acquire. In this work, we study the Slot Induction (SI) task whose objective is
to induce slot boundaries without explicit knowledge of token-level slot
annotations. We propose leveraging Unsupervised Pre-trained Language Model
(PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised
semantic knowledge extracted from PLM, and (2) additional sentence-level intent
label signals available from TOD. Our approach is shown to be effective in SI
task and capable of bridging the gaps with token-level supervised models on two
NLU benchmark datasets. When generalized to emerging intents, our SI objectives
also provide enhanced slot label representations, leading to improved
performance on the Slot Filling tasks.",None,-1
Deep Unsupervised Learning Using Spike-Timing-Dependent Plasticity,0.648952,"Spike-Timing-Dependent Plasticity (STDP) is an unsupervised learning
mechanism for Spiking Neural Networks (SNNs) that has received significant
attention from the neuromorphic hardware community. However, scaling such local
learning techniques to deeper networks and large-scale tasks has remained
elusive. In this work, we investigate a Deep-STDP framework where a rate-based
convolutional network, that can be deployed in a neuromorphic setting, is
trained in tandem with pseudo-labels generated by the STDP clustering process
on the network outputs. We achieve $24.56\%$ higher accuracy and $3.5\times$
faster convergence speed at iso-accuracy on a 10-class subset of the Tiny
ImageNet dataset in contrast to a $k$-means clustering approach.",None,-1
TBFormer: Two-Branch Transformer for Image Forgery Localization,0.566443,"Image forgery localization aims to identify forged regions by capturing
subtle traces from high-quality discriminative features. In this paper, we
propose a Transformer-style network with two feature extraction branches for
image forgery localization, and it is named as Two-Branch Transformer
(TBFormer). Firstly, two feature extraction branches are elaborately designed,
taking advantage of the discriminative stacked Transformer layers, for both RGB
and noise domain features. Secondly, an Attention-aware Hierarchical-feature
Fusion Module (AHFM) is proposed to effectively fuse hierarchical features from
two different domains. Although the two feature extraction branches have the
same architecture, their features have significant differences since they are
extracted from different domains. We adopt position attention to embed them
into a unified feature domain for hierarchical feature investigation. Finally,
a Transformer decoder is constructed for feature reconstruction to generate the
predicted mask. Extensive experiments on publicly available datasets
demonstrate the effectiveness of the proposed model.",None,-1
Relevance Feedback with Brain Signals,0.562997,"The Relevance Feedback (RF) process relies on accurate and real-time
relevance estimation of feedback documents to improve retrieval performance.
Since collecting explicit relevance annotations imposes an extra burden on the
user, extensive studies have explored using pseudo-relevance signals and
implicit feedback signals as substitutes. However, such signals are indirect
indicators of relevance and suffer from complex search scenarios where user
interactions are absent or biased.
  Recently, the advances in portable and high-precision brain-computer
interface (BCI) devices have shown the possibility to monitor user's brain
activities during search process. Brain signals can directly reflect user's
psychological responses to search results and thus it can act as additional and
unbiased RF signals. To explore the effectiveness of brain signals in the
context of RF, we propose a novel RF framework that combines BCI-based
relevance feedback with pseudo-relevance signals and implicit signals to
improve the performance of document re-ranking. The experimental results on the
user study dataset show that incorporating brain signals leads to significant
performance improvement in our RF framework. Besides, we observe that brain
signals perform particularly well in several hard search scenarios, especially
when implicit signals as feedback are missing or noisy. This reveals when and
how to exploit brain signals in the context of RF.",None,-1
SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for Document Instance Segmentation,0.734345,"Instance-level segmentation of documents consists in assigning a class-aware
and instance-aware label to each pixel of the image. It is a key step in
document parsing for their understanding. In this paper, we present a unified
transformer encoder-decoder architecture for en-to-end instance segmentation of
complex layouts in document images. The method adapts a contrastive training
with a mixed query selection for anchor initialization in the decoder. Later
on, it performs a dot product between the obtained query embeddings and the
pixel embedding map (coming from the encoder) for semantic reasoning. Extensive
experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical
Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone
achieves better segmentation performance than the existing state-of-the-art
approaches with the average precision of \textbf{93.72}, \textbf{54.39},
\textbf{84.65} and \textbf{98.04} respectively under one billion parameters.
The code is made publicly available at:
\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}",None,-1
Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System,0.446333,"Developing an efficient retriever to retrieve knowledge from a large-scale
knowledge base (KB) is critical for task-oriented dialogue systems to
effectively handle localized and specialized tasks. However, widely used
generative models such as T5 and ChatGPT often struggle to differentiate subtle
differences among the retrieved KB records when generating responses, resulting
in suboptimal quality of generated responses. In this paper, we propose the
application of maximal marginal likelihood to train a perceptive retriever by
utilizing signals from response generation for supervision. In addition, our
approach goes beyond considering solely retrieved entities and incorporates
various meta knowledge to guide the generator, thus improving the utilization
of knowledge. We evaluate our approach on three task-oriented dialogue datasets
using T5 and ChatGPT as the backbone models. The results demonstrate that when
combined with meta knowledge, the response generator can effectively leverage
high-quality knowledge records from the retriever and enhance the quality of
generated responses. The codes and models of this paper are available at
https://github.com/shenwzh3/MK-TOD.",None,-1
I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs,0.938915,"In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene
reconstruction and editing using differentiable Monte Carlo raytracing on
neural signed distance fields (SDFs). Our holistic neural SDF-based framework
jointly recovers the underlying shapes, incident radiance and materials from
multi-view images. We introduce a novel bubble loss for fine-grained small
objects and error-guided adaptive sampling scheme to largely improve the
reconstruction quality on large-scale indoor scenes. Further, we propose to
decompose the neural radiance field into spatially-varying material of the
scene as a neural field through surface-based, differentiable Monte Carlo
raytracing and emitter semantic segmentations, which enables physically based
and photorealistic scene relighting and editing applications. Through a number
of qualitative and quantitative experiments, we demonstrate the superior
quality of our method on indoor scene reconstruction, novel view synthesis, and
scene editing compared to state-of-the-art baselines.",None,-1
An Empirical Analysis of Range for 3D Object Detection,0.755755,"LiDAR-based 3D detection plays a vital role in autonomous navigation.
Surprisingly, although autonomous vehicles (AVs) must detect both near-field
objects (for collision avoidance) and far-field objects (for longer-term
planning), contemporary benchmarks focus only on near-field 3D detection.
However, AVs must detect far-field objects for safe navigation. In this paper,
we present an empirical analysis of far-field 3D detection using the long-range
detection dataset Argoverse 2.0 to better understand the problem, and share the
following insight: near-field LiDAR measurements are dense and optimally
encoded by small voxels, while far-field measurements are sparse and are better
encoded with large voxels. We exploit this observation to build a collection of
range experts tuned for near-vs-far field detection, and propose simple
techniques to efficiently ensemble models for long-range detection that improve
efficiency by 33% and boost accuracy by 3.2% CDS.",None,-1
SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,0.996609,"Building end-to-end task bots and maintaining their integration with new
functionalities using minimal human efforts is a long-standing challenge in
dialog research. Recently large language models (LLMs) have demonstrated
exceptional proficiency in conversational engagement and adherence to
instructions across various downstream tasks. In this work, we introduce
SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems
effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we
instruct fixed LLMs to generate appropriate responses on novel tasks,
circumventing the need for training data. Specifically, SGP-TOD comprises three
components: a LLM for engaging with users, a DST Prompter to aid the LLM with
dialog state tracking, which is then used to retrieve database items, and a
Policy Prompter to elicit proper responses adhering to the provided dialog
policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that
our training-free strategy SGP-TOD, without any task-specific data, yields
state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot
approaches. In a domain-extension setting, SGP-TOD aptly adapts to new
functionalities by merely adding supplementary schema rules. We make our code
and data publicly available.",None,-1
Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture,0.269404,"While deep neural networks have led to major advances in image recognition,
language translation, data mining, and game playing, there are well-known
limits to the paradigm such as lack of explainability, difficulty of
incorporating prior knowledge, and modularity. Neuro symbolic hybrid systems
have recently emerged as a straightforward way to extend deep neural networks
by incorporating ideas from symbolic reasoning such as computational logic. In
this paper, we propose a list desirable criteria for neuro symbolic systems and
examine how some of the existing approaches address these criteria. We then
propose an extension to generalized annotated logic that allows for the
creation of an equivalent neural architecture comprising an alternate neuro
symbolic hybrid. However, unlike previous approaches that rely on continuous
optimization for the training process, our framework is designed as a binarized
neural network that uses discrete optimization. We provide proofs of
correctness and discuss several of the challenges that must be overcome to
realize this framework in an implemented system.",None,-1
Reconstructing Groups of People with Hypergraph Relational Reasoning,0.574279,"Due to the mutual occlusion, severe scale variation, and complex spatial
distribution, the current multi-person mesh recovery methods cannot produce
accurate absolute body poses and shapes in large-scale crowded scenes. To
address the obstacles, we fully exploit crowd features for reconstructing
groups of people from a monocular image. A novel hypergraph relational
reasoning network is proposed to formulate the complex and high-order relation
correlations among individuals and groups in the crowd. We first extract
compact human features and location information from the original
high-resolution image. By conducting the relational reasoning on the extracted
individual features, the underlying crowd collectiveness and interaction
relationship can provide additional group information for the reconstruction.
Finally, the updated individual features and the localization information are
used to regress human meshes in camera coordinates. To facilitate the network
training, we further build pseudo ground-truth on two crowd datasets, which may
also promote future research on pose estimation and human behavior
understanding in crowded scenes. The experimental results show that our
approach outperforms other baseline methods both in crowded and common
scenarios. The code and datasets are publicly available at
https://github.com/boycehbz/GroupRec.",None,-1
Visual Instruction Tuning,1.0,"Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.",None,-1
Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models,0.770145,"Document-level Relation Extraction (DocRE), which aims to extract relations
from a long context, is a critical challenge in achieving fine-grained
structural comprehension and generating interpretable document representations.
Inspired by recent advances in in-context learning capabilities emergent from
large language models (LLMs), such as ChatGPT, we aim to design an automated
annotation method for DocRE with minimum human effort. Unfortunately, vanilla
in-context learning is infeasible for document-level relation extraction due to
the plenty of predefined fine-grained relation types and the uncontrolled
generations of LLMs. To tackle this issue, we propose a method integrating a
large language model (LLM) and a natural language inference (NLI) module to
generate relation triples, thereby augmenting document-level relation datasets.
We demonstrate the effectiveness of our approach by introducing an enhanced
dataset known as DocGNRE, which excels in re-annotating numerous long-tail
relation types. We are confident that our method holds the potential for
broader applications in domain-specific relation type definitions and offers
tangible benefits in advancing generalized language semantic comprehension.",None,-1
Using Auxiliary Tasks In Multimodal Fusion Of Wav2vec 2.0 And BERT For Multimodal Emotion Recognition,0.683391,"The lack of data and the difficulty of multimodal fusion have always been
challenges for multimodal emotion recognition (MER). In this paper, we propose
to use pretrained models as upstream network, wav2vec 2.0 for audio modality
and BERT for text modality, and finetune them in downstream task of MER to cope
with the lack of data. For the difficulty of multimodal fusion, we use a
K-layer multi-head attention mechanism as a downstream fusion module. Starting
from the MER task itself, we design two auxiliary tasks to alleviate the
insufficient fusion between modalities and guide the network to capture and
align emotion-related features. Compared to the previous state-of-the-art
models, we achieve a better performance by 78.42% Weighted Accuracy (WA) and
79.71% Unweighted Accuracy (UA) on the IEMOCAP dataset.",None,-1
Investigation of Architectures and Receptive Fields for Appearance-based Gaze Estimation,0.0898404,"With the rapid development of deep learning technology in the past decade,
appearance-based gaze estimation has attracted great attention from both
computer vision and human-computer interaction research communities.
Fascinating methods were proposed with variant mechanisms including soft
attention, hard attention, two-eye asymmetry, feature disentanglement, rotation
consistency, and contrastive learning. Most of these methods take the
single-face or multi-region as input, yet the basic architecture of gaze
estimation has not been fully explored. In this paper, we reveal the fact that
tuning a few simple parameters of a ResNet architecture can outperform most of
the existing state-of-the-art methods for the gaze estimation task on three
popular datasets. With our extensive experiments, we conclude that the stride
number, input image resolution, and multi-region architecture are critical for
the gaze estimation performance while their effectiveness dependent on the
quality of the input face image. We obtain the state-of-the-art performances on
three datasets with 3.64 on ETH-XGaze, 4.50 on MPIIFaceGaze, and 9.13 on
Gaze360 degrees gaze estimation error by taking ResNet-50 as the backbone.",None,-1
Fairness in Visual Clustering: A Novel Transformer Clustering Approach,0.854114,"Promoting fairness for deep clustering models in unsupervised clustering
settings to reduce demographic bias is a challenging goal. This is because of
the limitation of large-scale balanced data with well-annotated labels for
sensitive or protected attributes. In this paper, we first evaluate demographic
bias in deep clustering models from the perspective of cluster purity, which is
measured by the ratio of positive samples within a cluster to their correlation
degree. This measurement is adopted as an indication of demographic bias. Then,
a novel loss function is introduced to encourage a purity consistency for all
clusters to maintain the fairness aspect of the learned clustering model.
Moreover, we present a novel attention mechanism, Cross-attention, to measure
correlations between multiple clusters, strengthening faraway positive samples
and improving the purity of clusters during the learning process. Experimental
results on a large-scale dataset with numerous attribute settings have
demonstrated the effectiveness of the proposed approach on both clustering
accuracy and fairness enhancement on several sensitive attributes.",None,-1
Red Teaming Language Model Detectors with Language Models,0.700509,"The prevalence and strong capability of large language models (LLMs) present
significant safety and ethical risks if exploited by malicious users. To
prevent the potentially deceptive usage of LLMs, recent works have proposed
algorithms to detect LLM-generated text and protect LLMs. In this paper, we
investigate the robustness and reliability of these LLM detectors under
adversarial attacks. We study two types of attack strategies: 1) replacing
certain words in an LLM's output with their synonyms given the context; 2)
automatically searching for an instructional prompt to alter the writing style
of the generation. In both strategies, we leverage an auxiliary LLM to generate
the word replacements or the instructional prompt. Different from previous
works, we consider a challenging setting where the auxiliary LLM can also be
protected by a detector. Experiments reveal that our attacks effectively
compromise the performance of all detectors in the study with plausible
generations, underscoring the urgent need to improve the robustness of
LLM-generated text detection systems.",None,-1
MNL-Bandit in non-stationary environments,0.326501,"In this paper, we study the MNL-Bandit problem in a non-stationary
environment and present an algorithm with a worst-case expected regret of
$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\;
N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} +
\sqrt{NT}\right\}\right)$. Here $N$ is the number of arms, $L$ is the number of
changes and $\Delta_{\infty}^{K}$ is a variation measure of the unknown
parameters. Furthermore, we show matching lower bounds on the expected regret
(up to logarithmic factors), implying that our algorithm is optimal. Our
approach builds upon the epoch-based algorithm for stationary MNL-Bandit in
Agrawal et al. 2016. However, non-stationarity poses several challenges and we
introduce new techniques and ideas to address these. In particular, we give a
tight characterization for the bias introduced in the estimators due to non
stationarity and derive new concentration bounds.",None,-1
Argumentation Element Annotation Modeling using XLNet,0.773344,"This study demonstrates the effectiveness of XLNet, a transformer-based
language model, for annotating argumentative elements in persuasive essays.
XLNet's architecture incorporates a recurrent mechanism that allows it to model
long-term dependencies in lengthy texts. Fine-tuned XLNet models were applied
to three datasets annotated with different schemes - a proprietary dataset
using the Annotations for Revisions and Reflections on Writing (ARROW) scheme,
the PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNet
models achieved strong performance across all datasets, even surpassing human
agreement levels in some cases. This shows XLNet capably handles diverse
annotation schemes and lengthy essays. Comparisons between the model outputs on
different datasets also revealed insights into the relationships between the
annotation tags. Overall, XLNet's strong performance on modeling argumentative
structures across diverse datasets highlights its suitability for providing
automated feedback on essay organization.",None,-1
AICT: An Adaptive Image Compression Transformer,0.0849373,"Motivated by the efficiency investigation of the Tranformer-based transform
coding framework, namely SwinT-ChARM, we propose to enhance the latter, as
first, with a more straightforward yet effective Tranformer-based channel-wise
auto-regressive prior model, resulting in an absolute image compression
transformer (ICT). Current methods that still rely on ConvNet-based entropy
coding are limited in long-range modeling dependencies due to their local
connectivity and an increasing number of architectural biases and priors. On
the contrary, the proposed ICT can capture both global and local contexts from
the latent representations and better parameterize the distribution of the
quantized latents. Further, we leverage a learnable scaling module with a
sandwich ConvNeXt-based pre/post-processor to accurately extract more compact
latent representation while reconstructing higher-quality images. Extensive
experimental results on benchmark datasets showed that the proposed adaptive
image compression transformer (AICT) framework significantly improves the
trade-off between coding efficiency and decoder complexity over the versatile
video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM.",None,-1
Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination,0.621822,"Enormous hand images with reliable annotations are collected through
marker-based MoCap. Unfortunately, degradations caused by markers limit their
application in hand appearance reconstruction. A clear appearance recovery
insight is an image-to-image translation trained with unpaired data. However,
most frameworks fail because there exists structure inconsistency from a
degraded hand to a bare one. The core of our approach is to first disentangle
the bare hand structure from those degraded images and then wrap the appearance
to this structure with a dual adversarial discrimination (DAD) scheme. Both
modules take full advantage of the semi-supervised learning paradigm: The
structure disentanglement benefits from the modeling ability of ViT, and the
translator is enhanced by the dual discrimination on both translation processes
and translation results. Comprehensive evaluations have been conducted to prove
that our framework can robustly recover photo-realistic hand appearance from
diverse marker-contained and even object-occluded datasets. It provides a novel
avenue to acquire bare hand appearance data for other downstream learning
problems.The codes will be publicly available at https://www.yangangwang.com",None,-1
Choose your Data Wisely: A Framework for Semantic Counterfactuals,0.313124,"Counterfactual explanations have been argued to be one of the most intuitive
forms of explanation. They are typically defined as a minimal set of edits on a
given data sample that, when applied, changes the output of a model on that
sample. However, a minimal set of edits is not always clear and understandable
to an end-user, as it could, for instance, constitute an adversarial example
(which is indistinguishable from the original data sample to an end-user).
Instead, there are recent ideas that the notion of minimality in the context of
counterfactuals should refer to the semantics of the data sample, and not to
the feature space. In this work, we build on these ideas, and propose a
framework that provides counterfactual explanations in terms of knowledge
graphs. We provide an algorithm for computing such explanations (given some
assumptions about the underlying knowledge), and quantitatively evaluate the
framework with a user study.",None,-1
Continual Source-Free Unsupervised Domain Adaptation,0.33108,"Existing Source-free Unsupervised Domain Adaptation (SUDA) approaches
inherently exhibit catastrophic forgetting. Typically, models trained on a
labeled source domain and adapted to unlabeled target data improve performance
on the target while dropping performance on the source, which is not available
during adaptation. In this study, our goal is to cope with the challenging
problem of SUDA in a continual learning setting, i.e., adapting to the
target(s) with varying distributional shifts while maintaining performance on
the source. The proposed framework consists of two main stages: i) a SUDA model
yielding cleaner target labels -- favoring good performance on target, and ii)
a novel method for synthesizing class-conditioned source-style images by
leveraging only the source model and pseudo-labeled target data as a prior. An
extensive pool of experiments on major benchmarks, e.g., PACS, Visda-C, and
DomainNet demonstrates that the proposed Continual SUDA (C-SUDA) framework
enables preserving satisfactory performance on the source domain without
exploiting the source data at all.",None,-1
Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification,0.504005,"Sentiment analysis, widely critiqued for capturing merely the overall tone of
a corpus, falls short in accurately reflecting the latent structures and
political stances within texts. This study introduces topic metrics, dummy
variables converted from extracted topics, as both an alternative and
complement to sentiment metrics in stance classification. By employing three
datasets identified by Bestvater and Monroe (2023), this study demonstrates
BERTopic's proficiency in extracting coherent topics and the effectiveness of
topic metrics in stance classification. The experiment results show that
BERTopic improves coherence scores by 17.07% to 54.20% when compared to
traditional approaches such as Dirichlet Allocation (LDA) and Non-negative
Matrix Factorization (NMF), prevalent in earlier political science research.
Additionally, our results indicate topic metrics outperform sentiment metrics
in stance classification, increasing performance by as much as 18.95%. Our
findings suggest topic metrics are especially effective for context-rich texts
and corpus where stance and sentiment correlations are weak. The combination of
sentiment and topic metrics achieve an optimal performance in most of the
scenarios and can further address the limitations of relying solely on
sentiment as well as the low coherence score of topic metrics.",None,-1
Mastering Strategy Card Game (Hearthstone) with Improved Techniques,0.596312,"Strategy card game is a well-known genre that is demanding on the intelligent
game-play and can be an ideal test-bench for AI. Previous work combines an
end-to-end policy function and an optimistic smooth fictitious play, which
shows promising performances on the strategy card game Legend of Code and
Magic. In this work, we apply such algorithms to Hearthstone, a famous
commercial game that is more complicated in game rules and mechanisms. We
further propose several improved techniques and consequently achieve
significant progress. For a machine-vs-human test we invite a Hearthstone
streamer whose best rank was top 10 of the official league in China region that
is estimated to be of millions of players. Our models defeat the human player
in all Best-of-5 tournaments of full games (including both deck building and
battle), showing a strong capability of decision making.",None,-1
Intelligent Multi-channel Meta-imagers for Accelerating Machine Vision,0.108813,"Rapid developments in machine vision have led to advances in a variety of
industries, from medical image analysis to autonomous systems. These
achievements, however, typically necessitate digital neural networks with heavy
computational requirements, which are limited by high energy consumption and
further hinder real-time decision-making when computation resources are not
accessible. Here, we demonstrate an intelligent meta-imager that is designed to
work in concert with a digital back-end to off-load computationally expensive
convolution operations into high-speed and low-power optics. In this
architecture, metasurfaces enable both angle and polarization multiplexing to
create multiple information channels that perform positive and negatively
valued convolution operations in a single shot. The meta-imager is employed for
object classification, experimentally achieving 98.6% accurate classification
of handwritten digits and 88.8% accuracy in classifying fashion images. With
compactness, high speed, and low power consumption, this approach could find a
wide range of applications in artificial intelligence and machine vision
applications.",None,-1
FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration,0.987484,"We propose an end-to-end deep learning approach to rectify fisheye images and
simultaneously calibrate camera intrinsic and distortion parameters. Our method
consists of two parts: a Quick Image Rectification Module developed with a
Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a
CNN architecture. Our Quick Rectification Network performs robust rectification
with good resolution, making it suitable for constant calibration in
camera-based surveillance equipment. To achieve high-quality calibration, we
use the straightened output from the Quick Rectification Module as a
guidance-like semantic feature map for the Calibration Module to learn the
geometric relationship between the straightened feature and the distorted
feature. We train and validate our method with a large synthesized dataset
labeled with well-simulated parameters applied to a perspective image dataset.
Our solution has achieved robust performance in high-resolution with a
significant PSNR value of 22.343.",None,-1
Mind the Gap: Offline Policy Optimization for Imperfect Rewards,0.299024,"Reward function is essential in reinforcement learning (RL), serving as the
guiding signal to incentivize agents to solve given tasks, however, is also
notoriously difficult to design. In many cases, only imperfect rewards are
available, which inflicts substantial performance loss for RL agents. In this
study, we propose a unified offline policy optimization approach, \textit{RGM
(Reward Gap Minimization)}, which can smartly handle diverse types of imperfect
rewards. RGM is formulated as a bi-level optimization problem: the upper layer
optimizes a reward correction term that performs visitation distribution
matching w.r.t. some expert data; the lower layer solves a pessimistic RL
problem with the corrected rewards. By exploiting the duality of the lower
layer, we derive a tractable algorithm that enables sampled-based learning
without any online interactions. Comprehensive experiments demonstrate that RGM
achieves superior performance to existing methods under diverse settings of
imperfect rewards. Further, RGM can effectively correct wrong or inconsistent
rewards against expert preference and retrieve useful information from biased
rewards.",None,-1
Effects of Human Adversarial and Affable Samples on BERT Generalization,0.119295,"BERT-based models have had strong performance on leaderboards, yet have been
demonstrably worse in real-world settings requiring generalization. Limited
quantities of training data is considered a key impediment to achieving
generalizability in machine learning. In this paper, we examine the impact of
training data quality, not quantity, on a model's generalizability. We consider
two characteristics of training data: the portion of human-adversarial
(h-adversarial), i.e., sample pairs with seemingly minor differences but
different ground-truth labels, and human-affable (h-affable) training samples,
i.e., sample pairs with minor differences but the same ground-truth label. We
find that for a fixed size of training samples, as a rule of thumb, having
10-30% h-adversarial instances improves the precision, and therefore F1, by up
to 20 points in the tasks of text classification and relation extraction.
Increasing h-adversarials beyond this range can result in performance plateaus
or even degradation. In contrast, h-affables may not contribute to a model's
generalizability and may even degrade generalization performance.",None,-1
Visual Story Generation Based on Emotion and Keywords,0.0949594,"Automated visual story generation aims to produce stories with corresponding
illustrations that exhibit coherence, progression, and adherence to characters'
emotional development. This work proposes a story generation pipeline to
co-create visual stories with the users. The pipeline allows the user to
control events and emotions on the generated content. The pipeline includes two
parts: narrative and image generation. For narrative generation, the system
generates the next sentence using user-specified keywords and emotion labels.
For image generation, diffusion models are used to create a visually appealing
image corresponding to each generated sentence. Further, object recognition is
applied to the generated images to allow objects in these images to be
mentioned in future story development.",None,-1
Organizational Governance of Emerging Technologies: AI Adoption in Healthcare,0.890001,"Private and public sector structures and norms refine how emerging technology
is used in practice. In healthcare, despite a proliferation of AI adoption, the
organizational governance surrounding its use and integration is often poorly
understood. What the Health AI Partnership (HAIP) aims to do in this research
is to better define the requirements for adequate organizational governance of
AI systems in healthcare settings and support health system leaders to make
more informed decisions around AI adoption. To work towards this understanding,
we first identify how the standards for the AI adoption in healthcare may be
designed to be used easily and efficiently. Then, we map out the precise
decision points involved in the practical institutional adoption of AI
technology within specific health systems. Practically, we achieve this through
a multi-organizational collaboration with leaders from major health systems
across the United States and key informants from related fields. Working with
the consultancy IDEO [dot] org, we were able to conduct usability-testing
sessions with healthcare and AI ethics professionals. Usability analysis
revealed a prototype structured around mock key decision points that align with
how organizational leaders approach technology adoption. Concurrently, we
conducted semi-structured interviews with 89 professionals in healthcare and
other relevant fields. Using a modified grounded theory approach, we were able
to identify 8 key decision points and comprehensive procedures throughout the
AI adoption lifecycle. This is one of the most detailed qualitative analyses to
date of the current governance structures and processes involved in AI adoption
by health systems in the United States. We hope these findings can inform
future efforts to build capabilities to promote the safe, effective, and
responsible adoption of emerging technologies in healthcare.",None,-1
Hierarchical Neural Memory Network for Low Latency Event Processing,0.85635,"This paper proposes a low latency neural network architecture for event-based
dense prediction tasks. Conventional architectures encode entire scene contents
at a fixed rate regardless of their temporal characteristics. Instead, the
proposed network encodes contents at a proper temporal scale depending on its
movement speed. We achieve this by constructing temporal hierarchy using
stacked latent memories that operate at different rates. Given low latency
event steams, the multi-level memories gradually extract dynamic to static
scene contents by propagating information from the fast to the slow memory
modules. The architecture not only reduces the redundancy of conventional
architectures but also exploits long-term dependencies. Furthermore, an
attention-based event representation efficiently encodes sparse event streams
into the memory cells. We conduct extensive evaluations on three event-based
dense prediction tasks, where the proposed approach outperforms the existing
methods on accuracy and latency, while demonstrating effective event and image
fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",None,-1
AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder,0.841606,"To easily obtain the knowledge about autism spectrum disorder and help its
early screening and diagnosis, we create AsdKB, a Chinese knowledge base on
autism spectrum disorder. The knowledge base is built on top of various
sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical
descriptions on mental and behavioural disorders, 2) the diagnostic knowledge
from DSM-5 and different screening tools recommended by social organizations
and medical institutes, and 3) the expert knowledge on professional physicians
and hospitals from the Web. AsdKB contains both ontological and factual
knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The
potential applications of AsdKB are question answering, auxiliary diagnosis,
and expert recommendation, and we illustrate them with a prototype which can be
accessed at http://asdkb.org.cn/.",None,-1
RBSR: Efficient and Flexible Recurrent Network for Burst Super-Resolution,0.414735,"Burst super-resolution (BurstSR) aims at reconstructing a high-resolution
(HR) image from a sequence of low-resolution (LR) and noisy images, which is
conducive to enhancing the imaging effects of smartphones with limited sensors.
The main challenge of BurstSR is to effectively combine the complementary
information from input frames, while existing methods still struggle with it.
In this paper, we suggest fusing cues frame-by-frame with an efficient and
flexible recurrent network. In particular, we emphasize the role of the
base-frame and utilize it as a key prompt to guide the knowledge acquisition
from other frames in every recurrence. Moreover, we introduce an implicit
weighting loss to improve the model's flexibility in facing input frames with
variable numbers. Extensive experiments on both synthetic and real-world
datasets demonstrate that our method achieves better results than
state-of-the-art ones. Codes and pre-trained models are available at
https://github.com/ZcsrenlongZ/RBSR.",None,-1
Improving neural network representations using human similarity judgments,0.66225,"Deep neural networks have reached human-level performance on many computer
vision tasks. However, the objectives used to train these networks enforce only
that similar images are embedded at similar locations in the representation
space, and do not directly constrain the global structure of the resulting
space. Here, we explore the impact of supervising this global structure by
linearly aligning it with human similarity judgments. We find that a naive
approach leads to large changes in local representational structure that harm
downstream performance. Thus, we propose a novel method that aligns the global
structure of representations while preserving their local structure. This
global-local transform considerably improves accuracy across a variety of
few-shot learning and anomaly detection tasks. Our results indicate that human
visual representations are globally organized in a way that facilitates
learning from few examples, and incorporating this global structure into neural
network representations improves performance on downstream tasks.",None,-1
M2C: Towards Automatic Multimodal Manga Complement,0.461898,"Multimodal manga analysis focuses on enhancing manga understanding with
visual and textual features, which has attracted considerable attention from
both natural language processing and computer vision communities. Currently,
most comics are hand-drawn and prone to problems such as missing pages, text
contamination, and aging, resulting in missing comic text content and seriously
hindering human comprehension. In other words, the Multimodal Manga Complement
(M2C) task has not been investigated, which aims to handle the aforementioned
issues by providing a shared semantic space for vision and language
understanding. To this end, we first propose the Multimodal Manga Complement
task by establishing a new M2C benchmark dataset covering two languages. First,
we design a manga argumentation method called MCoT to mine event knowledge in
comics with large language models. Then, an effective baseline FVP-M$^{2}$
using fine-grained visual prompts is proposed to support manga complement.
Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for
Multimodal Mange Complement.",None,-1
What does BERT learn about prosody?,0.51502,"Language models have become nearly ubiquitous in natural language processing
applications achieving state-of-the-art results in many tasks including
prosody. As the model design does not define predetermined linguistic targets
during training but rather aims at learning generalized representations of the
language, analyzing and interpreting the representations that models implicitly
capture is important in bridging the gap between interpretability and model
performance. Several studies have explored the linguistic information that
models capture providing some insights on their representational capacity.
However, the current studies have not explored whether prosody is part of the
structural information of the language that models learn. In this work, we
perform a series of experiments on BERT probing the representations captured at
different layers. Our results show that information about prosodic prominence
spans across many layers but is mostly focused in middle layers suggesting that
BERT relies mostly on syntactic and semantic information.",None,-1
Interactive Learning of Hierarchical Tasks from Dialog with GPT,0.0582038,"We present a system for interpretable, symbolic, interactive task learning
from dialog using a GPT model as a conversational front-end. The learned tasks
are represented as hierarchical decompositions of predicate-argument structures
with scoped variable arguments. By using a GPT model to convert interactive
dialog into a semantic representation, and then recursively asking for
definitions of unknown steps, we show that hierarchical task knowledge can be
acquired and re-used in a natural and unrestrained conversational environment.
We compare our system to a similar architecture using a more conventional
parser and show that our system tolerates a much wider variety of linguistic
variance.",None,-1
SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM,0.92149,"Integrating CNNs and RNNs to capture spatiotemporal dependencies is a
prevalent strategy for spatiotemporal prediction tasks. However, the property
of CNNs to learn local spatial information decreases their efficiency in
capturing spatiotemporal dependencies, thereby limiting their prediction
accuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which
integrates Swin Transformer blocks and the simplified LSTM, an extension that
replaces the convolutional structure in ConvLSTM with the self-attention
mechanism. Furthermore, we construct a network with SwinLSTM cell as the core
for spatiotemporal prediction. Without using unique tricks, SwinLSTM
outperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and
KTH datasets. In particular, it exhibits a significant improvement in
prediction accuracy compared to ConvLSTM. Our competitive experimental results
demonstrate that learning global spatial dependencies is more advantageous for
models to capture spatiotemporal dependencies. We hope that SwinLSTM can serve
as a solid baseline to promote the advancement of spatiotemporal prediction
accuracy. The codes are publicly available at
https://github.com/SongTang-x/SwinLSTM.",None,-1
Parameter-efficient Modularised Bias Mitigation via AdapterFusion,0.857569,"Large pre-trained language models contain societal biases and carry along
these biases to downstream tasks. Current in-processing bias mitigation
approaches (like adversarial training) impose debiasing by updating a model's
parameters, effectively transferring the model to a new, irreversible debiased
state. In this work, we propose a novel approach to develop stand-alone
debiasing functionalities separate from the model, which can be integrated into
the model on-demand, while keeping the core model untouched. Drawing from the
concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing
with Adapter Modules) - a debiasing approach to first encapsulate arbitrary
bias mitigation functionalities into separate adapters, and then add them to
the model on-demand in order to deliver fairness qualities. We conduct a large
set of experiments on three classification tasks with gender, race, and age as
protected attributes. Our results show that DAM improves or maintains the
effectiveness of bias mitigation, avoids catastrophic forgetting in a
multi-attribute scenario, and maintains on-par task performance, while granting
parameter-efficiency and easy switching between the original and debiased
models.",None,-1
FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder,0.053655,"Latest federated learning (FL) methods started to focus on how to use
unlabeled data in clients for training due to users' privacy concerns, high
labeling costs, or lack of expertise. However, current Federated
Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn
large-scale images because of the limited computing resources of local clients.
In this paper, we introduce a new framework FedMAE, which stands for Federated
Masked AutoEncoder, to address the problem of how to utilize unlabeled
large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked
AutoEncoder (MAE) using large images in lightweight client devices, and then
cascades multiple pre-trained one-block MAEs in the server to build a
multi-block ViT backbone for downstream tasks. Theoretical analysis and
experimental results on image reconstruction and classification show that our
FedMAE achieves superior performance compared to the state-of-the-art FSSL
methods.",None,-1
Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation,0.401611,"This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation.",None,-1
BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations,0.0357884,"Coreference Resolution is a well studied problem in NLP. While widely studied
for English and other resource-rich languages, research on coreference
resolution in Bengali largely remains unexplored due to the absence of relevant
datasets. Bengali, being a low-resource language, exhibits greater
morphological richness compared to English. In this article, we introduce a new
dataset, BenCoref, comprising coreference annotations for Bengali texts
gathered from four distinct domains. This relatively small dataset contains
5200 mention annotations forming 502 mention clusters within 48,569 tokens. We
describe the process of creating this dataset and report performance of
multiple models trained using BenCoref. We expect that our work provides some
valuable insights on the variations in coreference phenomena across several
domains in Bengali and encourages the development of additional resources for
Bengali. Furthermore, we found poor crosslingual performance at zero-shot
setting from English, highlighting the need for more language-specific
resources for this task.",None,-1
Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge,0.156176,"We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge
using Large Language Models (LLMs) as a system of multiple expert agents. Using
the flexibility of LLMs to be prompted to do various novel tasks using
zero-shot, few-shot, context-grounded prompting, we explore the feasibility of
using LLMs to solve the ARC Challenge. We firstly convert the input image into
multiple suitable text-based abstraction spaces. We then utilise the
associative power of LLMs to derive the input-output relationship and map this
to actions in the form of a working program, similar to Voyager / Ghost in the
MineCraft. In addition, we use iterative environmental feedback in order to
guide LLMs to solve the task. Our proposed approach achieves 50 solves out of
111 training set problems (45%) with just three abstraction spaces - grid,
object and pixel - and we believe that with more abstraction spaces and
learnable actions, we will be able to solve more.",None,-1
Distribution-Aligned Diffusion for Human Mesh Recovery,0.696941,"Recovering a 3D human mesh from a single RGB image is a challenging task due
to depth ambiguity and self-occlusion, resulting in a high degree of
uncertainty. Meanwhile, diffusion models have recently seen much success in
generating high-quality outputs by progressively denoising noisy inputs.
Inspired by their capability, we explore a diffusion-based approach for human
mesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which
frames mesh recovery as a reverse diffusion process. We also propose a
Distribution Alignment Technique (DAT) that infuses prior distribution
information into the mesh distribution diffusion process, and provides useful
prior knowledge to facilitate the mesh recovery task. Our method achieves
state-of-the-art performance on three widely used datasets. Project page:
https://gongjia0208.github.io/HMDiff/.",None,-1
Enhancing Explainability in Mobility Data Science through a combination of methods,0.473248,"In the domain of Mobility Data Science, the intricate task of interpreting
models trained on trajectory data, and elucidating the spatio-temporal movement
of entities, has persistently posed significant challenges. Conventional XAI
techniques, although brimming with potential, frequently overlook the distinct
structure and nuances inherent within trajectory data. Observing this
deficiency, we introduced a comprehensive framework that harmonizes pivotal XAI
techniques: LIME (Local Interpretable Model-agnostic Explanations), SHAP
(SHapley Additive exPlanations), Saliency maps, attention mechanisms, direct
trajectory visualization, and Permutation Feature Importance (PFI). Unlike
conventional strategies that deploy these methods singularly, our unified
approach capitalizes on the collective efficacy of these techniques, yielding
deeper and more granular insights for models reliant on trajectory data. In
crafting this synthesis, we effectively address the multifaceted essence of
trajectories, achieving not only amplified interpretability but also a nuanced,
contextually rich comprehension of model decisions. To validate and enhance our
framework, we undertook a survey to gauge preferences and reception among
various user demographics. Our findings underscored a dichotomy: professionals
with academic orientations, particularly those in roles like Data Scientist, IT
Expert, and ML Engineer, showcased a profound, technical understanding and
often exhibited a predilection for amalgamated methods for interpretability.
Conversely, end-users or individuals less acquainted with AI and Data Science
showcased simpler inclinations, such as bar plots indicating timestep
significance or visual depictions pinpointing pivotal segments of a vessel's
trajectory.",None,-1
Targeted Adversarial Attacks against Neural Machine Translation,0.579601,"Neural Machine Translation (NMT) systems are used in various applications.
However, it has been shown that they are vulnerable to very small perturbations
of their inputs, known as adversarial attacks. In this paper, we propose a new
targeted adversarial attack against NMT models. In particular, our goal is to
insert a predefined target keyword into the translation of the adversarial
sentence while maintaining similarity between the original sentence and the
perturbed one in the source domain. To this aim, we propose an optimization
problem, including an adversarial loss term and a similarity term. We use
gradient projection in the embedding space to craft an adversarial sentence.
Experimental results show that our attack outperforms Seq2Sick, the other
targeted adversarial attack against NMT models, in terms of success rate and
decrease in translation quality. Our attack succeeds in inserting a keyword
into the translation for more than 75% of sentences while similarity with the
original sentence stays preserved.",None,-1
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,0.999985,"Theory of mind (ToM) evaluations currently focus on testing models using
passive narratives that inherently lack interactivity. We introduce FANToM, a
new benchmark designed to stress-test ToM within information-asymmetric
conversational contexts via question answering. Our benchmark draws upon
important theoretical requisites from psychology and necessary empirical
considerations when evaluating large language models (LLMs). In particular, we
formulate multiple types of questions that demand the same underlying reasoning
to identify illusory or false sense of ToM capabilities in LLMs. We show that
FANToM is challenging for state-of-the-art LLMs, which perform significantly
worse than humans even with chain-of-thought reasoning or fine-tuning.",None,-1
Fauno: The Italian Large Language Model that will leave you senza parole!,0.58143,"This paper presents Fauno, the first and largest open-source Italian
conversational Large Language Model (LLM). Our goal with Fauno is to
democratize the study of LLMs in Italian, demonstrating that obtaining a
fine-tuned conversational bot with a single GPU is possible. In addition, we
release a collection of datasets for conversational AI in Italian. The datasets
on which we fine-tuned Fauno include various topics such as general question
answering, computer science, and medical questions. We release our code and
datasets on \url{https://github.com/RSTLess-research/Fauno-Italian-LLM}",None,-1
SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation,0.54981,"Recent advances in deep learning-based medical image segmentation studies
achieve nearly human-level performance in fully supervised manner. However,
acquiring pixel-level expert annotations is extremely expensive and laborious
in medical imaging fields. Unsupervised domain adaptation (UDA) can alleviate
this problem, which makes it possible to use annotated data in one imaging
modality to train a network that can successfully perform segmentation on
target imaging modality with no labels. In this work, we propose SDC-UDA, a
simple yet effective volumetric UDA framework for slice-direction continuous
cross-modality medical image segmentation which combines intra- and inter-slice
self-attentive image translation, uncertainty-constrained pseudo-label
refinement, and volumetric self-training. Our method is distinguished from
previous methods on UDA for medical image segmentation in that it can obtain
continuous segmentation in the slice direction, thereby ensuring higher
accuracy and potential in clinical practice. We validate SDC-UDA with multiple
publicly available cross-modality medical image segmentation datasets and
achieve state-of-the-art segmentation performance, not to mention the superior
slice-direction continuity of prediction compared to previous studies.",None,-1
Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation,0.33404,"Large pre-trained language models achieve impressive results across many
tasks. However, recent works point out that pre-trained language models may
memorize a considerable fraction of their training data, leading to the privacy
risk of information leakage. In this paper, we propose a method named Ethicist
for targeted training data extraction through loss smoothed soft prompting and
calibrated confidence estimation, investigating how to recover the suffix in
the training data when given a prefix. To elicit memorization in the attacked
model, we tune soft prompt embeddings while keeping the model fixed. We further
propose a smoothing loss that smooths the loss distribution of the suffix
tokens to make it easier to sample the correct suffix. In order to select the
most probable suffix from a collection of sampled suffixes and estimate the
prediction confidence, we propose a calibrated confidence estimation method,
which normalizes the confidence of the generated suffixes with a local
estimation. We show that Ethicist significantly improves the extraction
performance on a recently proposed public benchmark. We also investigate
several factors influencing the data extraction performance, including decoding
strategy, model scale, prefix length, and suffix length. Our code is available
at https://github.com/thu-coai/Targeted-Data-Extraction.",None,-1
Evaluating the Robustness of Machine Reading Comprehension Models to Low Resource Entity Renaming,0.424571,"Question answering (QA) models have shown compelling results in the task of
Machine Reading Comprehension (MRC). Recently these systems have proved to
perform better than humans on held-out test sets of datasets e.g. SQuAD, but
their robustness is not guaranteed. The QA model's brittleness is exposed when
evaluated on adversarial generated examples by a performance drop. In this
study, we explore the robustness of MRC models to entity renaming, with
entities from low-resource regions such as Africa. We propose EntSwap, a method
for test-time perturbations, to create a test set whose entities have been
renamed. In particular, we rename entities of type: country, person,
nationality, location, organization, and city, to create AfriSQuAD2. Using the
perturbed test set, we evaluate the robustness of three popular MRC models. We
find that compared to base models, large models perform well comparatively on
novel entities. Furthermore, our analysis indicates that entity type person
highly challenges the MRC models' performance.",None,-1
ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals,0.974765,"Motion forecasting is a key module in an autonomous driving system. Due to
the heterogeneous nature of multi-sourced input, multimodality in agent
behavior, and low latency required by onboard deployment, this task is
notoriously challenging. To cope with these difficulties, this paper proposes a
novel agent-centric model with anchor-informed proposals for efficient
multimodal motion prediction. We design a modality-agnostic strategy to
concisely encode the complex input in a unified manner. We generate diverse
proposals, fused with anchors bearing goal-oriented scene context, to induce
multimodal prediction that covers a wide range of future trajectories. Our
network architecture is highly uniform and succinct, leading to an efficient
model amenable for real-world driving deployment. Experiments reveal that our
agent-centric network compares favorably with the state-of-the-art methods in
prediction accuracy, while achieving scene-centric level inference latency.",None,-1
ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,0.851785,"The ability to accurately locate and navigate to a specific object is a
crucial capability for embodied agents that operate in the real world and
interact with objects to complete tasks. Such object navigation tasks usually
require large-scale training in visual environments with labeled objects, which
generalizes poorly to novel objects in unknown environments. In this work, we
present a novel zero-shot object navigation method, Exploration with Soft
Commonsense constraints (ESC), that transfers commonsense knowledge in
pre-trained models to open-world object navigation without any navigation
experience nor any other training on the visual environments. First, ESC
leverages a pre-trained vision and language model for open-world prompt-based
grounding and a pre-trained commonsense language model for room and object
reasoning. Then ESC converts commonsense knowledge into navigation actions by
modeling it as soft logic predicates for efficient exploration. Extensive
experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method
improves significantly over baselines, and achieves new state-of-the-art
results for zero-shot object navigation (e.g., 288% relative Success Rate
improvement than CoW on MP3D).",None,-1
Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking,0.968152,"The biomedical field relies heavily on concept linking in various areas such
as literature mining, graph alignment, information retrieval,
question-answering, data, and knowledge integration. Although large language
models (LLMs) have made significant strides in many natural language processing
tasks, their effectiveness in biomedical concept mapping is yet to be fully
explored. This research investigates a method that exploits the in-context
learning (ICL) capabilities of large models for biomedical concept linking. The
proposed approach adopts a two-stage retrieve-and-rank framework. Initially,
biomedical concepts are embedded using language models, and then embedding
similarity is utilized to retrieve the top candidates. These candidates'
contextual information is subsequently incorporated into the prompt and
processed by a large language model to re-rank the concepts. This approach
achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%
in chemical entity normalization, exhibiting a competitive performance relative
to supervised learning methods. Further, it showed a significant improvement,
with an over 20-point absolute increase in F1 score on an oncology matching
dataset. Extensive qualitative assessments were conducted, and the benefits and
potential shortcomings of using large language models within the biomedical
domain were discussed. were discussed.",None,-1
Developing an Informal-Formal Persian Corpus,0.182572,"Informal language is a style of spoken or written language frequently used in
casual conversations, social media, weblogs, emails and text messages. In
informal writing, the language faces some lexical and/or syntactic changes
varying among different languages. Persian is one of the languages with many
differences between its formal and informal styles of writing, thus developing
informal language processing tools for this language seems necessary. Such a
converter needs a large aligned parallel corpus of colloquial-formal sentences
which can be useful for linguists to extract a regulated grammar and
orthography for colloquial Persian as is done for the formal language. In this
paper we explain our methodology in building a parallel corpus of 50,000
sentence pairs with alignments in the word/phrase level. The sentences were
attempted to cover almost all kinds of lexical and syntactic changes between
informal and formal Persian, therefore both methods of exploring and collecting
from the different resources of informal scripts and following the phonological
and morphological patterns of changes were applied to find as much instances as
possible. The resulting corpus has about 530,000 alignments and a dictionary
containing 49,397 word and phrase pairs.",None,-1
Descriptive Knowledge Graph in Biomedical Domain,0.275798,"We present a novel system that automatically extracts and generates
informative and descriptive sentences from the biomedical corpus and
facilitates the efficient search for relational knowledge. Unlike previous
search engines or exploration systems that retrieve unconnected passages, our
system organizes descriptive sentences as a relational graph, enabling
researchers to explore closely related biomedical entities (e.g., diseases
treated by a chemical) or indirectly connected entities (e.g., potential drugs
for treating a disease). Our system also uses ChatGPT and a fine-tuned relation
synthesis model to generate concise and reliable descriptive sentences from
retrieved information, reducing the need for extensive human reading effort.
With our system, researchers can easily obtain both high-level knowledge and
detailed references and interactively steer to the information of interest. We
spotlight the application of our system in COVID-19 research, illustrating its
utility in areas such as drug repurposing and literature curation.",None,-1
Designing Participatory AI: Creative Professionals' Worries and Expectations about Generative AI,0.756395,"Generative AI, i.e., the group of technologies that automatically generate
visual or written content based on text prompts, has undergone a leap in
complexity and become widely available within just a few years. Such
technologies potentially introduce a massive disruption to creative fields.
This paper presents the results of a qualitative survey ($N$ = 23)
investigating how creative professionals think about generative AI. The results
show that the advancement of these AI models prompts important reflections on
what defines creativity and how creatives imagine using AI to support their
workflows. Based on these reflections, we discuss how we might design
\textit{participatory AI} in the domain of creative expertise with the goal of
empowering creative professionals in their present and future coexistence with
AI.",None,-1
MWaste: A Deep Learning Approach to Manage Household Waste,0.0366138,"Computer vision methods have shown to be effective in classifying garbage
into recycling categories for waste processing, existing methods are costly,
imprecise, and unclear. To tackle this issue, we introduce MWaste, a mobile
application that uses computer vision and deep learning techniques to classify
waste materials as trash, plastic, paper, metal, glass or cardboard. Its
effectiveness was tested on various neural network architectures and real-world
images, achieving an average precision of 92\% on the test set. This app can
help combat climate change by enabling efficient waste processing and reducing
the generation of greenhouse gases caused by incorrect waste disposal.",None,-1
Guided Motion Diffusion for Controllable Human Motion Synthesis,0.929234,"Denoising diffusion models have shown great promise in human motion synthesis
conditioned on natural language descriptions. However, integrating spatial
constraints, such as pre-defined motion trajectories and obstacles, remains a
challenge despite being essential for bridging the gap between isolated human
motion and its surrounding environment. To address this issue, we propose
Guided Motion Diffusion (GMD), a method that incorporates spatial constraints
into the motion generation process. Specifically, we propose an effective
feature projection scheme that manipulates motion representation to enhance the
coherency between spatial information and local poses. Together with a new
imputation formulation, the generated motion can reliably conform to spatial
constraints such as global motion trajectories. Furthermore, given sparse
spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance
approach to turn a sparse signal, which is susceptible to being ignored during
the reverse steps, into denser signals to guide the generated motion to the
given constraints. Our extensive experiments justify the development of GMD,
which achieves a significant improvement over state-of-the-art methods in
text-based motion generation while allowing control of the synthesized motions
with spatial constraints.",None,-1
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks,0.341965,"Medical image segmentation typically necessitates a large and precisely
annotated dataset. However, obtaining pixel-wise annotation is a
labor-intensive task that requires significant effort from domain experts,
making it challenging to obtain in practical clinical scenarios. In such
situations, reducing the amount of annotation required is a more practical
approach. One feasible direction is sparse annotation, which involves
annotating only a few slices, and has several advantages over traditional weak
annotation methods such as bounding boxes and scribbles, as it preserves exact
boundaries. However, learning from sparse annotation is challenging due to the
scarcity of supervision signals. To address this issue, we propose a framework
that can robustly learn from sparse annotation using the cross-teaching of both
3D and 2D networks. Considering the characteristic of these networks, we
develop two pseudo label selection strategies, which are hard-soft confidence
threshold and consistent label fusion. Our experimental results on the MMWHS
dataset demonstrate that our method outperforms the state-of-the-art (SOTA)
semi-supervised segmentation methods. Moreover, our approach achieves results
that are comparable to the fully-supervised upper bound result.",None,-1
Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation,0.998716,"Accurate segmentation of topological tubular structures, such as blood
vessels and roads, is crucial in various fields, ensuring accuracy and
efficiency in downstream tasks. However, many factors complicate the task,
including thin local structures and variable global morphologies. In this work,
we note the specificity of tubular structures and use this knowledge to guide
our DSCNet to simultaneously enhance perception in three stages: feature
extraction, feature fusion, and loss constraint. First, we propose a dynamic
snake convolution to accurately capture the features of tubular structures by
adaptively focusing on slender and tortuous local structures. Subsequently, we
propose a multi-view feature fusion strategy to complement the attention to
features from multiple perspectives during feature fusion, ensuring the
retention of important information from different global morphologies. Finally,
a continuity constraint loss function, based on persistent homology, is
proposed to constrain the topological continuity of the segmentation better.
Experiments on 2D and 3D datasets show that our DSCNet provides better accuracy
and continuity on the tubular structure segmentation task compared with several
methods. Our codes will be publicly available.",None,-1
"Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",0.659174,"We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.",None,-1
Construction of Decision Trees and Acyclic Decision Graphs from Decision Rule Systems,0.0880152,"Decision trees and systems of decision rules are widely used as classifiers,
as a means for knowledge representation, and as algorithms. They are among the
most interpretable models for data analysis. The study of the relationships
between these two models can be seen as an important task of computer science.
Methods for transforming decision trees into systems of decision rules are
simple and well-known. In this paper, we consider the inverse transformation
problem, which is not trivial. We study the complexity of constructing decision
trees and acyclic decision graphs representing decision trees from decision
rule systems, and we discuss the possibility of not building the entire
decision tree, but describing the computation path in this tree for the given
input.",None,-1
Unsupervised Chunking with Hierarchical RNN,0.1506,"In Natural Language Processing (NLP), predicting linguistic structures, such
as parsing and chunking, has mostly relied on manual annotations of syntactic
structures. This paper introduces an unsupervised approach to chunking, a
syntactic task that involves grouping words in a non-hierarchical manner. We
present a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to
model word-to-chunk and chunk-to-sentence compositions. Our approach involves a
two-stage training process: pretraining with an unsupervised parser and
finetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset
reveal a notable improvement over existing unsupervised methods, enhancing
phrase F1 score by up to 6 percentage points. Further, finetuning with
downstream tasks results in an additional performance improvement.
Interestingly, we observe that the emergence of the chunking structure is
transient during the neural model's downstream-task training. This study
contributes to the advancement of unsupervised syntactic structure discovery
and opens avenues for further research in linguistic theory.",None,-1
Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space,0.373096,"Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.",None,-1
On Evaluating Multilingual Compositional Generalization with Translated Datasets,0.341653,"Compositional generalization allows efficient learning and human-like
inductive biases. Since most research investigating compositional
generalization in NLP is done on English, important questions remain
underexplored. Do the necessary compositional generalization abilities differ
across languages? Can models compositionally generalize cross-lingually? As a
first step to answering these questions, recent work used neural machine
translation to translate datasets for evaluating compositional generalization
in semantic parsing. However, we show that this entails critical semantic
distortion. To address this limitation, we craft a faithful rule-based
translation of the MCWQ dataset from English to Chinese and Japanese. Even with
the resulting robust benchmark, which we call MCWQ-R, we show that the
distribution of compositions still suffers due to linguistic divergences, and
that multilingual models still struggle with cross-lingual compositional
generalization. Our dataset and methodology will be useful resources for the
study of cross-lingual compositional generalization in other tasks.",None,-1
DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis,0.797644,"We propose DoE2Vec, a variational autoencoder (VAE)-based methodology to
learn optimization landscape characteristics for downstream meta-learning
tasks, e.g., automated selection of optimization algorithms. Principally, using
large training data sets generated with a random function generator, DoE2Vec
self-learns an informative latent representation for any design of experiments
(DoE). Unlike the classical exploratory landscape analysis (ELA) method, our
approach does not require any feature engineering and is easily applicable for
high dimensional search spaces. For validation, we inspect the quality of
latent reconstructions and analyze the latent representations using different
experiments. The latent representations not only show promising potentials in
identifying similar (cheap-to-evaluate) surrogate functions, but also can
significantly boost performances when being used complementary to the classical
ELA features in classification tasks.",None,-1
FLAG: Fast Label-Adaptive Aggregation for Multi-label Classification in Federated Learning,0.113842,"Federated learning aims to share private data to maximize the data utility
without privacy leakage. Previous federated learning research mainly focuses on
multi-class classification problems. However, multi-label classification is a
crucial research problem close to real-world data properties. Nevertheless, a
limited number of federated learning studies explore this research problem.
Existing studies of multi-label federated learning did not consider the
characteristics of multi-label data, i.e., they used the concept of multi-class
classification to verify their methods' performance, which means it will not be
feasible to apply their methods to real-world applications. Therefore, this
study proposed a new multi-label federated learning framework with a
Clustering-based Multi-label Data Allocation (CMDA) and a novel aggregation
method, Fast Label-Adaptive Aggregation (FLAG), for multi-label classification
in the federated learning environment. The experimental results demonstrate
that our methods only need less than 50\% of training epochs and communication
rounds to surpass the performance of state-of-the-art federated learning
methods.",None,-1
Vanishing Activations: A Symptom of Deep Capsule Networks,0.168993,"Capsule Networks, an extension to Neural Networks utilizing vector or matrix
representations instead of scalars, were initially developed to create a
dynamic parse tree where visual concepts evolve from parts to complete objects.
Early implementations of Capsule Networks achieved and maintain
state-of-the-art results on various datasets. However, recent studies have
revealed shortcomings in the original Capsule Network architecture, notably its
failure to construct a parse tree and its susceptibility to vanishing gradients
when deployed in deeper networks. This paper extends the investigation to a
range of leading Capsule Network architectures, demonstrating that these issues
are not confined to the original design. We argue that the majority of Capsule
Network research has produced architectures that, while modestly divergent from
the original Capsule Network, still retain a fundamentally similar structure.
We posit that this inherent design similarity might be impeding the scalability
of Capsule Networks. Our study contributes to the broader discussion on
improving the robustness and scalability of Capsule Networks.",None,-1
IC3: Image Captioning by Committee Consensus,0.335542,"If you ask a human to describe an image, they might do so in a thousand
different ways. Traditionally, image captioning models are trained to generate
a single ""best"" (most like a reference) image caption. Unfortunately, doing so
encourages captions that are ""informationally impoverished,"" and focus on only
a subset of the possible details, while ignoring other potentially useful
information in the scene. In this work, we introduce a simple, yet novel,
method: ""Image Captioning by Committee Consensus"" (IC3), designed to generate a
single caption that captures high-level details from several annotator
viewpoints. Humans rate captions produced by IC3 at least as helpful as
baseline SOTA models more than two thirds of the time, and IC3 can improve the
performance of SOTA automated recall systems by up to 84%, outperforming single
human-generated reference captions, and indicating significant improvements
over SOTA approaches for visual description. Code is available at
https://davidmchan.github.io/caption-by-committee/",None,-1
Algorithmic Transparency and Manipulation,0.773198,"A series of recent papers raises worries about the manipulative potential of
algorithmic transparency. But while the concern is apt and relevant, it is
based on a fraught understanding of manipulation. Therefore, this paper draws
attention to the indifference view of manipulation, which explains better than
the vulnerability view why algorithmic transparency has manipulative potential.
The paper also raises pertinent research questions for future studies of
manipulation in the context of algorithmic transparency.",None,-1
RPTQ: Reorder-based Post-training Quantization for Large Language Models,0.915786,"Large-scale language models (LLMs) have demonstrated impressive performance,
but their deployment presents challenges due to their significant memory usage.
This issue can be alleviated through quantization. In this paper, we identify
that the challenge in quantizing activations in LLMs arises from varying ranges
across channels, rather than solely the presence of outliers. To address this
challenge, we introduce a quantization method called RPTQ, which utilizes a
reorder-based approach. By rearranging the channels and quantizing them in
clusters, RPTQ effectively mitigates the impact of range differences between
channels. To minimize the overhead of the reorder operation, we fuse it into
the layer norm operation and weights in linear layers. In our experiments, RPTQ
achieved a significant breakthrough by utilizing 3-bit activation in LLMs for
the first time, resulting in a substantial reduction in memory usage. For
instance, quantizing OPT-175b can lead to a memory consumption reduction of up
to 80%.",None,-1
A semantically enhanced dual encoder for aspect sentiment triplet extraction,0.870332,"Aspect sentiment triplet extraction (ASTE) is a crucial subtask of
aspect-based sentiment analysis (ABSA) that aims to comprehensively identify
sentiment triplets. Previous research has focused on enhancing ASTE through
innovative table-filling strategies. However, these approaches often overlook
the multi-perspective nature of language expressions, resulting in a loss of
valuable interaction information between aspects and opinions. To address this
limitation, we propose a framework that leverages both a basic encoder,
primarily based on BERT, and a particular encoder comprising a Bi-LSTM network
and graph convolutional network (GCN ). The basic encoder captures the
surface-level semantics of linguistic expressions, while the particular encoder
extracts deeper semantics, including syntactic and lexical information. By
modeling the dependency tree of comments and considering the part-of-speech and
positional information of words, we aim to capture semantics that are more
relevant to the underlying intentions of the sentences. An interaction strategy
combines the semantics learned by the two encoders, enabling the fusion of
multiple perspectives and facilitating a more comprehensive understanding of
aspect--opinion relationships. Experiments conducted on benchmark datasets
demonstrate the state-of-the-art performance of our proposed framework.",None,-1
Evaluating the Performance of Large Language Models for Spanish Language in Undergraduate Admissions Exams,0.0597474,"This study evaluates the performance of large language models, specifically
GPT-3.5 and BARD (supported by Gemini Pro model), in undergraduate admissions
exams proposed by the National Polytechnic Institute in Mexico. The exams cover
Engineering/Mathematical and Physical Sciences, Biological and Medical
Sciences, and Social and Administrative Sciences. Both models demonstrated
proficiency, exceeding the minimum acceptance scores for respective academic
programs to up to 75% for some academic programs. GPT-3.5 outperformed BARD in
Mathematics and Physics, while BARD performed better in History and questions
related to factual information. Overall, GPT-3.5 marginally surpassed BARD with
scores of 60.94% and 60.42%, respectively.",None,-1
From Chaos to Clarity: Claim Normalization to Empower Fact-Checking,0.673253,"With the rise of social media, users are exposed to many misleading claims.
However, the pervasive noise inherent in these posts presents a challenge in
identifying precise and prominent claims that require verification. Extracting
the important claims from such posts is arduous and time-consuming, yet it is
an underexplored problem. Here, we aim to bridge this gap. We introduce a novel
task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and
noisy social media posts into more straightforward and understandable forms,
termed normalized claims. We propose CACN, a pioneering approach that leverages
chain-of-thought and claim check-worthiness estimation, mimicking human
reasoning processes, to comprehend intricate claims. Moreover, we capitalize on
the in-context learning capabilities of large language models to provide
guidance and to improve claim normalization. To evaluate the effectiveness of
our proposed model, we meticulously compile a comprehensive real-world dataset,
CLAN, comprising more than 6k instances of social media posts alongside their
respective normalized claims. Our experiments demonstrate that CACN outperforms
several baselines across various evaluation measures. Finally, our rigorous
error analysis validates CACN's capabilities and pitfalls.",None,-1
Online POMDP Planning with Anytime Deterministic Guarantees,0.814804,"Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.",None,-1
"If our aim is to build morality into an artificial agent, how might we begin to go about doing so?",0.584772,"As Artificial Intelligence (AI) becomes pervasive in most fields, from
healthcare to autonomous driving, it is essential that we find successful ways
of building morality into our machines, especially for decision-making.
However, the question of what it means to be moral is still debated,
particularly in the context of AI. In this paper, we highlight the different
aspects that should be considered when building moral agents, including the
most relevant moral paradigms and challenges. We also discuss the top-down and
bottom-up approaches to design and the role of emotion and sentience in
morality. We then propose solutions including a hybrid approach to design and a
hierarchical approach to combining moral paradigms. We emphasize how governance
and policy are becoming ever more critical in AI Ethics and in ensuring that
the tasks we set for moral agents are attainable, that ethical behavior is
achieved, and that we obtain good AI.",None,-1
Pruning Compact ConvNets for Efficient Inference,0.342138,"Neural network pruning is frequently used to compress over-parameterized
networks by large amounts, while incurring only marginal drops in
generalization performance. However, the impact of pruning on networks that
have been highly optimized for efficient inference has not received the same
level of attention. In this paper, we analyze the effect of pruning for
computer vision, and study state-of-the-art ConvNets, such as the FBNetV3
family of models. We show that model pruning approaches can be used to further
optimize networks trained through NAS (Neural Architecture Search). The
resulting family of pruned models can consistently obtain better performance
than existing FBNetV3 models at the same level of computation, and thus provide
state-of-the-art results when trading off between computational complexity and
generalization performance on the ImageNet benchmark. In addition to better
generalization performance, we also demonstrate that when limited computation
resources are available, pruning FBNetV3 models incur only a fraction of
GPU-hours involved in running a full-scale NAS.",None,-1
RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation,0.819555,"Grammatical Error Correction (GEC) systems play a vital role in assisting
people with their daily writing tasks. However, users may sometimes come across
a GEC system that initially performs well but fails to correct errors when the
inputs are slightly modified. To ensure an ideal user experience, a reliable
GEC system should have the ability to provide consistent and accurate
suggestions when encountering irrelevant context perturbations, which we refer
to as context robustness. In this paper, we introduce RobustGEC, a benchmark
designed to evaluate the context robustness of GEC systems. RobustGEC comprises
5,000 GEC cases, each with one original error-correct sentence pair and five
variants carefully devised by human annotators. Utilizing RobustGEC, we reveal
that state-of-the-art GEC systems still lack sufficient robustness against
context perturbations. In addition, we propose a simple yet effective method
for remitting this issue.",None,-1
The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation,0.269821,"Continuous-output neural machine translation (CoNMT) replaces the discrete
next-word prediction problem with an embedding prediction. The semantic
structure of the target embedding space (i.e., closeness of related words) is
intuitively believed to be crucial. We challenge this assumption and show that
completely random output embeddings can outperform laboriously pretrained ones,
especially on larger datasets. Further investigation shows this surprising
effect is strongest for rare words, due to the geometry of their embeddings. We
shed further light on this finding by designing a mixed strategy that combines
random and pre-trained embeddings for different tokens.",None,-1
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model,0.721834,"Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as ""The war lasted from the year 1732 to the year 17"", and predict valid
two-digit end years (years > 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.",None,-1
Algebra Error Classification with Large Language Models,0.599918,"Automated feedback as students answer open-ended math questions has
significant potential in improving learning outcomes at large scale. A key part
of automated feedback systems is an error classification component, which
identifies student errors and enables appropriate, predefined feedback to be
deployed. Most existing approaches to error classification use a rule-based
method, which has limited capacity to generalize. Existing data-driven methods
avoid these limitations but specifically require mathematical expressions in
student responses to be parsed into syntax trees. This requirement is itself a
limitation, since student responses are not always syntactically valid and
cannot be converted into trees. In this work, we introduce a flexible method
for error classification using pre-trained large language models. We
demonstrate that our method can outperform existing methods in algebra error
classification, and is able to classify a larger set of student responses.
Additionally, we analyze common classification errors made by our method and
discuss limitations of automated error classification.",None,-1
Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation,0.762063,"Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.",None,-1
SMPConv: Self-moving Point Representations for Continuous Convolution,0.758071,"Continuous convolution has recently gained prominence due to its ability to
handle irregularly sampled data and model long-term dependency. Also, the
promising experimental results of using large convolutional kernels have
catalyzed the development of continuous convolution since they can construct
large kernels very efficiently. Leveraging neural networks, more specifically
multilayer perceptrons (MLPs), is by far the most prevalent approach to
implementing continuous convolution. However, there are a few drawbacks, such
as high computational costs, complex hyperparameter tuning, and limited
descriptive power of filters. This paper suggests an alternative approach to
building a continuous convolution without neural networks, resulting in more
computationally efficient and improved performance. We present self-moving
point representations where weight parameters freely move, and interpolation
schemes are used to implement continuous functions. When applied to construct
convolutional kernels, the experimental results have shown improved performance
with drop-in replacement in the existing frameworks. Due to its lightweight
structure, we are first to demonstrate the effectiveness of continuous
convolution in a large-scale setting, e.g., ImageNet, presenting the
improvements over the prior arts. Our code is available on
https://github.com/sangnekim/SMPConv",None,-1
ABC: Attention with Bilinear Correlation for Infrared Small Target Detection,0.684789,"Infrared small target detection (ISTD) has a wide range of applications in
early warning, rescue, and guidance. However, CNN based deep learning methods
are not effective at segmenting infrared small target (IRST) that it lack of
clear contour and texture features, and transformer based methods also struggle
to achieve significant results due to the absence of convolution induction
bias. To address these issues, we propose a new model called attention with
bilinear correlation (ABC), which is based on the transformer architecture and
includes a convolution linear fusion transformer (CLFT) module with a novel
attention mechanism for feature extraction and fusion, which effectively
enhances target features and suppresses noise. Additionally, our model includes
a u-shaped convolution-dilated convolution (UCDC) module located deeper layers
of the network, which takes advantage of the smaller resolution of deeper
features to obtain finer semantic information. Experimental results on public
datasets demonstrate that our approach achieves state-of-the-art performance.
Code is available at https://github.com/PANPEIWEN/ABC",None,-1
Removing RLHF Protections in GPT-4 via Fine-Tuning,0.837009,"As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the
contrary: fine-tuning allows attackers to remove RLHF protections with as few
as 340 examples and a 95% success rate. These training examples can be
automatically generated with weaker models. We further show that removing RLHF
protections does not decrease usefulness on non-censored outputs, providing
evidence that our fine-tuning strategy does not decrease usefulness despite
using weaker models to generate training data. Our results show the need for
further research on protections on LLMs.",None,-1
Gloss Attention for Gloss-free Sign Language Translation,0.705029,"Most sign language translation (SLT) methods to date require the use of gloss
annotations to provide additional supervision information, however, the
acquisition of gloss is not easy. To solve this problem, we first perform an
analysis of existing models to confirm how gloss annotations make SLT easier.
We find that it can provide two aspects of information for the model, 1) it can
help the model implicitly learn the location of semantic boundaries in
continuous sign language videos, 2) it can help the model understand the sign
language video globally. We then propose \emph{gloss attention}, which enables
the model to keep its attention within video segments that have the same
semantics locally, just as gloss helps existing models do. Furthermore, we
transfer the knowledge of sentence-to-sentence similarity from the natural
language model to our gloss attention SLT network (GASLT) to help it understand
sign language videos at the sentence level. Experimental results on multiple
large-scale sign language datasets show that our proposed GASLT model
significantly outperforms existing methods. Our code is provided in
\url{https://github.com/YinAoXiong/GASLT}.",None,-1
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods,0.998907,"Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called ""machine psychology"". The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.",None,-1
Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network and Self-Attention Mechanism,0.0998625,"Instrument playing technique (IPT) is a key element of musical presentation.
However, most of the existing works for IPT detection only concern monophonic
music signals, yet little has been done to detect IPTs in polyphonic
instrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we
formulate it as a frame-level multi-label classification problem and apply it
to Guzheng, a Chinese plucked string instrument. We create a new dataset,
Guzheng\_Tech99, containing Guzheng recordings and onset, offset, pitch, IPT
annotations of each note. Because different IPTs vary a lot in their lengths,
we propose a new method to solve this problem using multi-scale network and
self-attention. The multi-scale network extracts features from different
scales, and the self-attention mechanism applied to the feature maps at the
coarsest scale further enhances the long-range feature extraction. Our approach
outperforms existing works by a large margin, indicating its effectiveness in
IPT detection.",None,-1
TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation,0.450764,"Recent success of Contrastive Language-Image Pre-training~(CLIP) has shown
great promise in pixel-level open-vocabulary learning tasks. A general paradigm
utilizes CLIP's text and patch embeddings to generate semantic masks. However,
existing models easily misidentify input pixels from unseen classes, thus
confusing novel classes with semantically-similar ones. In our work, we
disentangle the ill-posed optimization problem into two parallel processes: one
performs semantic matching individually, and the other judges reliability for
improving discrimination ability. Motivated by special tokens in language
modeling that represents sentence-level embeddings, we design a trusty token
that decouples the known and novel category prediction tendency. With almost no
extra overhead, we upgrade the pixel-level generalization capacity of existing
models effectively. Our TagCLIP (CLIP adapting with Trusty-guidance) boosts the
IoU of unseen classes by 7.4% and 1.7% on PASCAL VOC 2012 and COCO-Stuff 164K.",None,-1
Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability,0.960111,"Large, multilingual language models exhibit surprisingly good zero- or
few-shot machine translation capabilities, despite having never seen the
intentionally-included translation examples provided to typical neural
translation systems. We investigate the role of incidental bilingualism -- the
unintentional consumption of bilingual signals, including translation examples
-- in explaining the translation capabilities of large language models, taking
the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method
approach to measure and understand incidental bilingualism at scale. We show
that PaLM is exposed to over 30 million translation pairs across at least 44
languages. Furthermore, the amount of incidental bilingual content is highly
correlated with the amount of monolingual in-language content for non-English
languages. We relate incidental bilingual content to zero-shot prompts and show
that it can be used to mine new prompts to improve PaLM's out-of-English
zero-shot translation quality. Finally, in a series of small-scale ablations,
we show that its presence has a substantial impact on translation capabilities,
although this impact diminishes with model scale.",None,-1
Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization,0.843981,"Backdoor defense, which aims to detect or mitigate the effect of malicious
triggers introduced by attackers, is becoming increasingly critical for machine
learning security and integrity. Fine-tuning based on benign data is a natural
defense to erase the backdoor effect in a backdoored model. However, recent
studies show that, given limited benign data, vanilla fine-tuning has poor
defense performance. In this work, we provide a deep study of fine-tuning the
backdoored model from the neuron perspective and find that backdoorrelated
neurons fail to escape the local minimum in the fine-tuning process. Inspired
by observing that the backdoorrelated neurons often have larger norms, we
propose FTSAM, a novel backdoor defense paradigm that aims to shrink the norms
of backdoor-related neurons by incorporating sharpness-aware minimization with
fine-tuning. We demonstrate the effectiveness of our method on several
benchmark datasets and network architectures, where it achieves
state-of-the-art defense performance. Overall, our work provides a promising
avenue for improving the robustness of machine learning models against backdoor
attacks.",None,-1
Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking,0.842551,"Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.",None,-1
Unveiling and unraveling aggregation and dispersion fallacies in group MCDM,0.257449,"Priorities in multi-criteria decision-making (MCDM) convey the relevance
preference of one criterion over another, which is usually reflected by
imposing the non-negativity and unit-sum constraints. The processing of such
priorities is different than other unconstrained data, but this point is often
neglected by researchers, which results in fallacious statistical analysis.
This article studies three prevalent fallacies in group MCDM along with
solutions based on compositional data analysis to avoid misusing statistical
operations. First, we use a compositional approach to aggregate the priorities
of a group of DMs and show that the outcome of the compositional analysis is
identical to the normalized geometric mean, meaning that the arithmetic mean
should be avoided. Furthermore, a new aggregation method is developed, which is
a robust surrogate for the geometric mean. We also discuss the errors in
computing measures of dispersion, including standard deviation and distance
functions. Discussing the fallacies in computing the standard deviation, we
provide a probabilistic criteria ranking by developing proper Bayesian tests,
where we calculate the extent to which a criterion is more important than
another. Finally, we explain the errors in computing the distance between
priorities, and a clustering algorithm is specially tailored based on proper
distance metrics.",None,-1
Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment,0.789691,"The egocentric and exocentric viewpoints of a human activity look
dramatically different, yet invariant representations to link them are
essential for many potential applications in robotics and augmented reality.
Prior work is limited to learning view-invariant features from paired
synchronized viewpoints. We relax that strong data assumption and propose to
learn fine-grained action features that are invariant to the viewpoints by
aligning egocentric and exocentric videos in time, even when not captured
simultaneously or in the same environment. To this end, we propose AE2, a
self-supervised embedding approach with two key designs: (1) an object-centric
encoder that explicitly focuses on regions corresponding to hands and active
objects; and (2) a contrastive-based alignment objective that leverages
temporally reversed frames as negative samples. For evaluation, we establish a
benchmark for fine-grained video understanding in the ego-exo context,
comprising four datasets -- including an ego tennis forehand dataset we
collected, along with dense per-frame labels we annotated for each dataset. On
the four datasets, our AE2 method strongly outperforms prior work in a variety
of fine-grained downstream tasks, both in regular and cross-view settings.",None,-1
Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023,0.573349,"MentalRiskES is a novel challenge that proposes to solve problems related to
early risk detection for the Spanish language. The objective is to detect, as
soon as possible, Telegram users who show signs of mental disorders considering
different tasks. Task 1 involved the users' detection of eating disorders, Task
2 focused on depression detection, and Task 3 aimed at detecting an unknown
disorder. These tasks were divided into subtasks, each one defining a
resolution approach. Our research group participated in subtask A for Tasks 1
and 2: a binary classification problem that evaluated whether the users were
positive or negative. To solve these tasks, we proposed models based on
Transformers followed by a decision policy according to criteria defined by an
early detection framework. One of the models presented an extended vocabulary
with important words for each task to be solved. In addition, we applied a
decision policy based on the history of predictions that the model performs
during user evaluation. For Tasks 1 and 2, we obtained the second-best
performance according to rankings based on classification and latency,
demonstrating the effectiveness and consistency of our approaches for solving
early detection problems in the Spanish language.",None,-1
KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation,0.571333,"In text classification tasks, fine tuning pretrained language models like
BERT and GPT-3 yields competitive accuracy; however, both methods require
pretraining on large text datasets. In contrast, general topic modeling methods
possess the advantage of analyzing documents to extract meaningful patterns of
words without the need of pretraining. To leverage topic modeling's
unsupervised insights extraction on text classification tasks, we develop the
Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires
no pretrained embeddings, few labeled documents and is efficient to train,
making it ideal under resource constrained settings. Across a variety of
datasets, our method outperforms existing supervised topic modeling methods in
classification accuracy, robustness and efficiency and achieves similar
performance compare to state of the art weakly supervised text classification
methods.",None,-1
Class Attention Transfer Based Knowledge Distillation,0.846371,"Previous knowledge distillation methods have shown their impressive
performance on model compression tasks, however, it is hard to explain how the
knowledge they transferred helps to improve the performance of the student
network. In this work, we focus on proposing a knowledge distillation method
that has both high interpretability and competitive performance. We first
revisit the structure of mainstream CNN models and reveal that possessing the
capacity of identifying class discriminative regions of input is critical for
CNN to perform classification. Furthermore, we demonstrate that this capacity
can be obtained and enhanced by transferring class activation maps. Based on
our findings, we propose class attention transfer based knowledge distillation
(CAT-KD). Different from previous KD methods, we explore and present several
properties of the knowledge transferred by our method, which not only improve
the interpretability of CAT-KD but also contribute to a better understanding of
CNN. While having high interpretability, CAT-KD achieves state-of-the-art
performance on multiple benchmarks. Code is available at:
https://github.com/GzyAftermath/CAT-KD.",None,-1
Learning in Cooperative Multiagent Systems Using Cognitive and Machine Models,0.538971,"Developing effective Multi-Agent Systems (MAS) is critical for many
applications requiring collaboration and coordination with humans. Despite the
rapid advance of Multi-Agent Deep Reinforcement Learning (MADRL) in cooperative
MAS, one major challenge is the simultaneous learning and interaction of
independent agents in dynamic environments in the presence of stochastic
rewards. State-of-the-art MADRL models struggle to perform well in Coordinated
Multi-agent Object Transportation Problems (CMOTPs), wherein agents must
coordinate with each other and learn from stochastic rewards. In contrast,
humans often learn rapidly to adapt to nonstationary environments that require
coordination among people. In this paper, motivated by the demonstrated ability
of cognitive models based on Instance-Based Learning Theory (IBLT) to capture
human decisions in many dynamic decision making tasks, we propose three
variants of Multi-Agent IBL models (MAIBL). The idea of these MAIBL algorithms
is to combine the cognitive mechanisms of IBLT and the techniques of MADRL
models to deal with coordination MAS in stochastic environments from the
perspective of independent learners. We demonstrate that the MAIBL models
exhibit faster learning and achieve better coordination in a dynamic CMOTP task
with various settings of stochastic rewards compared to current MADRL models.
We discuss the benefits of integrating cognitive insights into MADRL models.",None,-1
AutoTrial: Prompting Language Models for Clinical Trial Design,0.627051,"Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60% winning rate against the GPT-3.5
baselines via human evaluations.",None,-1
OLISIA: a Cascade System for Spoken Dialogue State Tracking,0.443177,"Though Dialogue State Tracking (DST) is a core component of spoken dialogue
systems, recent work on this task mostly deals with chat corpora, disregarding
the discrepancies between spoken and written language.In this paper, we propose
OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR)
model and a DST model. We introduce several adaptations in the ASR and DST
modules to improve integration and robustness to spoken conversations.With
these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to
evaluate spoken DST. We conduct an in-depth analysis of the results and find
that normalizing the ASR outputs and adapting the DST inputs through data
augmentation, along with increasing the pre-trained models size all play an
important role in reducing the performance discrepancy between written and
spoken conversations.",None,-1
"Point-Query Quadtree for Crowd Counting, Localization, and More",0.823747,"We show that crowd counting can be viewed as a decomposable point querying
process. This formulation enables arbitrary points as input and jointly reasons
whether the points are crowd and where they locate. The querying processing,
however, raises an underlying problem on the number of necessary querying
points. Too few imply underestimation; too many increase computational
overhead. To address this dilemma, we introduce a decomposable structure, i.e.,
the point-query quadtree, and propose a new counting model, termed Point quEry
Transformer (PET). PET implements decomposable point querying via
data-dependent quadtree splitting, where each querying point could split into
four new points when necessary, thus enabling dynamic processing of sparse and
dense regions. Such a querying process yields an intuitive, universal modeling
of crowd as both the input and output are interpretable and steerable. We
demonstrate the applications of PET on a number of crowd-related tasks,
including fully-supervised crowd counting and localization, partial annotation
learning, and point annotation refinement, and also report state-of-the-art
performance. For the first time, we show that a single counting model can
address multiple crowd-related tasks across different learning paradigms. Code
is available at https://github.com/cxliu0/PET.",None,-1
Understanding Expressivity of GNN in Rule Learning,0.318264,"Rule learning is critical to improving knowledge graph (KG) reasoning due to
their ability to provide logical and interpretable explanations. Recently,
Graph Neural Networks (GNNs) with tail entity scoring achieve the
state-of-the-art performance on KG reasoning. However, the theoretical
understandings for these GNNs are either lacking or focusing on
single-relational graphs, leaving what the kind of rules these GNNs can learn
an open problem. We propose to fill the above gap in this paper. Specifically,
GNNs with tail entity scoring are unified into a common framework. Then, we
analyze their expressivity by formally describing the rule structures they can
learn and theoretically demonstrating their superiority. These results further
inspire us to propose a novel labeling strategy to learn more rules in KG
reasoning. Experimental results are consistent with our theoretical findings
and verify the effectiveness of our proposed method. The code is publicly
available at https://github.com/LARS-research/Rule-learning-expressivity.",None,-1
Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,0.49269,"Retaining walls are often built to prevent excessive lateral movements of the
ground surrounding an excavation site. During an excavation, failure of
retaining walls could cause catastrophic accidents and hence their lateral
deformations are monitored regularly. Laser scanning can rapidly acquire the
spatial data of a relatively large area at fine spatial resolutions, which is
ideal for monitoring retaining walls' deformations. This paper attempts to
apply laser scanning to measurements of the lateral deformations of a soil
mixing retaining wall at an ongoing excavation site. Reference measurements by
total station and inclinometer were also conducted to verify those from the
laser scanning. The deformations derived using laser scanning data were
consistent with the reference measurements at the top part of the retaining
wall (i.e., mainly the ring beam of the wall). This research also shows that
the multi-scale-model-to-model method was the most accurate deformation
estimation method on the research data.",None,-1
Rigorously Assessing Natural Language Explanations of Neurons,0.840006,"Natural language is an appealing medium for explaining how large language
models process and store information, but evaluating the faithfulness of such
explanations is challenging. To help address this, we develop two modes of
evaluation for natural language explanations that claim individual neurons
represent a concept in a text input. In the observational mode, we evaluate
claims that a neuron $a$ activates on all and only input strings that refer to
a concept picked out by the proposed explanation $E$. In the intervention mode,
we construe $E$ as a claim that the neuron $a$ is a causal mediator of the
concept denoted by $E$. We apply our framework to the GPT-4-generated
explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the
most confident explanations have high error rates and little to no causal
efficacy. We close the paper by critically assessing whether natural language
is a good choice for explanations and whether neurons are the best level of
analysis.",None,-1
Optimization-Based Eye Tracking using Deflectometric Information,0.766444,"Eye tracking is an important tool with a wide range of applications in
Virtual, Augmented, and Mixed Reality (VR/AR/MR) technologies. State-of-the-art
eye tracking methods are either reflection-based and track reflections of
sparse point light sources, or image-based and exploit 2D features of the
acquired eye image. In this work, we attempt to significantly improve
reflection-based methods by utilizing pixel-dense deflectometric surface
measurements in combination with optimization-based inverse rendering
algorithms. Utilizing the known geometry of our deflectometric setup, we
develop a differentiable rendering pipeline based on PyTorch3D that simulates a
virtual eye under screen illumination. Eventually, we exploit the
image-screen-correspondence information from the captured measurements to find
the eye's rotation, translation, and shape parameters with our renderer via
gradient descent. In general, our method does not require a specific pattern
and can work with ordinary video frames of the main VR/AR/MR screen itself. We
demonstrate real-world experiments with evaluated mean relative gaze errors
below 0.45 degrees at a precision better than 0.11 degrees. Moreover, we show
an improvement of 6X over a representative reflection-based state-of-the-art
method in simulation.",None,-1
Segment Anything Meets Point Tracking,0.999369,"The Segment Anything Model (SAM) has established itself as a powerful
zero-shot image segmentation model, enabled by efficient point-centric
annotation and prompt-based models. While click and brush interactions are both
well explored in interactive image segmentation, the existing methods on videos
focus on mask annotation and propagation. This paper presents SAM-PT, a novel
method for point-centric interactive video segmentation, empowered by SAM and
long-term point tracking. SAM-PT leverages robust and sparse point selection
and propagation techniques for mask generation. Compared to traditional
object-centric mask propagation strategies, we uniquely use point propagation
to exploit local structure information agnostic to object semantics. We
highlight the merits of point-based tracking through direct evaluation on the
zero-shot open-world Unidentified Video Objects (UVO) benchmark. Our
experiments on popular video object segmentation and multi-object segmentation
tracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a
point-based segmentation tracker yields better zero-shot performance and
efficient interactions. We release our code that integrates different point
trackers and video segmentation benchmarks at https://github.com/SysCV/sam-pt.",None,-1
Ontology Revision based on Pre-trained Language Models,0.42607,"Ontology revision aims to seamlessly incorporate a new ontology into an
existing ontology and plays a crucial role in tasks such as ontology evolution,
ontology maintenance, and ontology alignment. Similar to repair single
ontologies, resolving logical incoherence in the task of ontology revision is
also important and meaningful, because incoherence is a main potential factor
to cause inconsistency and reasoning with an inconsistent ontology will obtain
meaningless answers.To deal with this problem, various ontology revision
approaches have been proposed to define revision operators and design ranking
strategies for axioms in an ontology. However, they rarely consider axiom
semantics which provides important information to differentiate axioms. In
addition, pre-trained models can be utilized to encode axiom semantics, and
have been widely applied in many natural language processing tasks and
ontology-related ones in recent years.Therefore, in this paper, we study how to
apply pre-trained models to revise ontologies. We first define four scoring
functions to rank axioms based on a pre-trained model by considering various
information from an ontology. Based on the functions, an ontology revision
algorithm is then proposed to deal with unsatisfiable concepts at once. To
improve efficiency, an adapted revision algorithm is designed to deal with
unsatisfiable concepts group by group. We conduct experiments over 19 ontology
pairs and compare our algorithms and scoring functions with existing ones.
According to the experiments, our algorithms could achieve promising
performance.",None,-1
Multi-Dimensional Evaluation of Text Summarization with In-Context Learning,0.842946,"Evaluation of natural language generation (NLG) is complex and
multi-dimensional. Generated text can be evaluated for fluency, coherence,
factuality, or any other dimensions of interest. Most frameworks that perform
such multi-dimensional evaluation require training on large manually or
synthetically generated datasets. In this paper, we study the efficacy of large
language models as multi-dimensional evaluators using in-context learning,
obviating the need for large training datasets. Our experiments show that
in-context learning-based evaluators are competitive with learned evaluation
frameworks for the task of text summarization, establishing state-of-the-art on
dimensions such as relevance and factual consistency. We then analyze the
effects of factors such as the selection and number of in-context examples on
performance. Finally, we study the efficacy of in-context learning based
evaluators in evaluating zero-shot summaries written by large language models
such as GPT-3.",None,-1
VITAL: Vision Transformer Neural Networks for Accurate Smartphone Heterogeneity Resilient Indoor Localization,0.760349,"Wi-Fi fingerprinting-based indoor localization is an emerging embedded
application domain that leverages existing Wi-Fi access points (APs) in
buildings to localize users with smartphones. Unfortunately, the heterogeneity
of wireless transceivers across diverse smartphones carried by users has been
shown to reduce the accuracy and reliability of localization algorithms. In
this paper, we propose a novel framework based on vision transformer neural
networks called VITAL that addresses this important challenge. Experiments
indicate that VITAL can reduce the uncertainty created by smartphone
heterogeneity while improving localization accuracy from 41% to 68% over the
best-known prior works. We also demonstrate the generalizability of our
approach and propose a data augmentation technique that can be integrated into
most deep learning-based localization frameworks to improve accuracy.",None,-1
Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification,0.149444,"Recent technological advancements have led to a large number of patents in a
diverse range of domains, making it challenging for human experts to analyze
and manage. State-of-the-art methods for multi-label patent classification rely
on deep neural networks (DNNs), which are complex and often considered
black-boxes due to their opaque decision-making processes. In this paper, we
propose a novel deep explainable patent classification framework by introducing
layer-wise relevance propagation (LRP) to provide human-understandable
explanations for predictions. We train several DNN models, including Bi-LSTM,
CNN, and CNN-BiLSTM, and propagate the predictions backward from the output
layer up to the input layer of the model to identify the relevance of words for
individual predictions. Considering the relevance score, we then generate
explanations by visualizing relevant words for the predicted patent class.
Experimental results on two datasets comprising two-million patent texts
demonstrate high performance in terms of various evaluation measures. The
explanations generated for each prediction highlight important relevant words
that align with the predicted class, making the prediction more understandable.
Explainable systems have the potential to facilitate the adoption of complex
AI-enabled methods for patent classification in real-world applications.",None,-1
SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views,0.0754857,"We study to generate novel views of indoor scenes given sparse input views.
The challenge is to achieve both photorealism and view consistency. We present
SparseGNV: a learning framework that incorporates 3D structures and image
generative models to generate novel views with three modules. The first module
builds a neural point cloud as underlying geometry, providing contextual
information and guidance for the target novel view. The second module utilizes
a transformer-based network to map the scene context and the guidance into a
shared latent space and autoregressively decodes the target view in the form of
discrete image tokens. The third module reconstructs the tokens into the image
of the target view. SparseGNV is trained across a large indoor scene dataset to
learn generalizable priors. Once trained, it can efficiently generate novel
views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV
on both real-world and synthetic indoor scenes and demonstrate that it
outperforms state-of-the-art methods based on either neural radiance fields or
conditional image generation.",None,-1
Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features,0.831646,"Recent studies have demonstrated the susceptibility of deep neural networks
to backdoor attacks. Given a backdoored model, its prediction of a poisoned
sample with trigger will be dominated by the trigger information, though
trigger information and benign information coexist. Inspired by the mechanism
of the optical polarizer that a polarizer could pass light waves with
particular polarizations while filtering light waves with other polarizations,
we propose a novel backdoor defense method by inserting a learnable neural
polarizer into the backdoored model as an intermediate layer, in order to
purify the poisoned sample via filtering trigger information while maintaining
benign information. The neural polarizer is instantiated as one lightweight
linear transformation layer, which is learned through solving a well designed
bi-level optimization problem, based on a limited clean dataset. Compared to
other fine-tuning-based defense methods which often adjust all parameters of
the backdoored model, the proposed method only needs to learn one additional
layer, such that it is more efficient and requires less clean data. Extensive
experiments demonstrate the effectiveness and efficiency of our method in
removing backdoors across various neural network architectures and datasets,
especially in the case of very limited clean data.",None,-1
Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature,0.234385,"We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large
language model to engage in meaningful interactions with Astronomy papers using
in-context prompting. To optimize for efficiency, we employ a distillation
technique that effectively reduces the size of the original input paper by
50\%, while maintaining the paragraph structure and overall semantic integrity.
We then explore the model's responses using a multi-document context (ten
distilled documents). Our findings indicate that GPT-4 excels in the
multi-document domain, providing detailed answers contextualized within the
framework of related research findings. Our results showcase the potential of
large language models for the astronomical community, offering a promising
avenue for further exploration, particularly the possibility of utilizing the
models for hypothesis generation.",None,-1
The Algonauts Project 2023 Challenge: UARK-UAlbany Team Solution,0.508964,"This work presents our solutions to the Algonauts Project 2023 Challenge. The
primary objective of the challenge revolves around employing computational
models to anticipate brain responses captured during participants' observation
of intricate natural visual scenes. The goal is to predict brain responses
across the entire visual brain, as it is the region where the most reliable
responses to images have been observed. We constructed an image-based brain
encoder through a two-step training process to tackle this challenge.
Initially, we created a pretrained encoder using data from all subjects. Next,
we proceeded to fine-tune individual subjects. Each step employed different
training strategies, such as different loss functions and objectives, to
introduce diversity. Ultimately, our solution constitutes an ensemble of
multiple unique encoders. The code is available at
https://github.com/uark-cviu/Algonauts2023",None,-1
Redrawing attendance boundaries to promote racial and ethnic diversity in elementary schools,0.486412,"Most US school districts draw ""attendance boundaries"" to define catchment
areas that assign students to schools near their homes, often recapitulating
neighborhood demographic segregation in schools. Focusing on elementary
schools, we ask: how much might we reduce school segregation by redrawing
attendance boundaries? Combining parent preference data with methods from
combinatorial optimization, we simulate alternative boundaries for 98 US school
districts serving over 3 million elementary-aged students, minimizing
White/non-White segregation while mitigating changes to travel times and school
sizes. Across districts, we observe a median 14% relative decrease in
segregation, which we estimate would require approximately 20\% of students to
switch schools and, surprisingly, a slight reduction in travel times. We
release a public dashboard depicting these alternative boundaries
(https://www.schooldiversity.org/) and invite both school boards and their
constituents to evaluate their viability. Our results show the possibility of
greater integration without significant disruptions for families.",None,-1
InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems,0.446333,"Large language models (LLMs) have been used for diverse tasks in natural
language processing (NLP), yet remain under-explored for task-oriented dialogue
systems (TODS), especially for end-to-end TODS. We present InstructTODS, a
novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue
systems that can adapt to diverse domains without fine-tuning. By leveraging
LLMs, InstructTODS generates a proxy belief state that seamlessly translates
user intentions into dynamic queries for efficient interaction with any KB. Our
extensive experiments demonstrate that InstructTODS achieves comparable
performance to fully fine-tuned TODS in guiding dialogues to successful
completion without prior knowledge or task-specific data. Furthermore, a
rigorous human evaluation of end-to-end TODS shows that InstructTODS produces
dialogue responses that notably outperform both the gold responses and the
state-of-the-art TODS in terms of helpfulness, informativeness, and humanness.
Moreover, the effectiveness of LLMs in TODS is further supported by our
comprehensive evaluations on TODS subtasks: dialogue state tracking, intent
classification, and response generation. Code and implementations could be
found here https://github.com/WillyHC22/InstructTODS/",None,-1
Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?,0.492632,"ASR systems are generally built for the spoken 'standard', and their
performance declines for non-standard dialects/varieties. This is a problem for
a language like Irish, where there is no single spoken standard, but rather
three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a
diagnostic to quantify the effect of the speaker's dialect on recognition
performance, 12 ASR systems were trained, firstly using baseline
dialect-balanced training corpora, and then using modified versions of the
baseline corpora, where dialect-specific materials were either subtracted or
added. Results indicate that dialect-balanced corpora do not yield a similar
performance across the dialects: the Ul dialect consistently underperforms,
whereas Mu yields lowest WERs. There is a close relationship between Co and Mu
dialects, but one that is not symmetrical. These results will guide future
corpus collection and system building strategies to optimise for cross-dialect
performance equity.",None,-1
Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild,0.0617196,"Engaging in the deliberate generation of abnormal outputs from large language
models (LLMs) by attacking them is a novel human activity. This paper presents
a thorough exposition of how and why people perform such attacks. Using a
formal qualitative methodology, we interviewed dozens of practitioners from a
broad range of backgrounds, all contributors to this novel work of attempting
to cause LLMs to fail. We relate and connect this activity between its
practitioners' motivations and goals; the strategies and techniques they
deploy; and the crucial role the community plays. As a result, this paper
presents a grounded theory of how and why people attack large language models:
LLM red teaming in the wild.",None,-1
Classification of Cross-cultural News Events,0.819908,"We present a methodology to support the analysis of culture from text such as
news events and demonstrate its usefulness on categorizing news events from
different categories (society, business, health, recreation, science, shopping,
sports, arts, computers, games and home) across different geographical
locations (different places in 117 countries). We group countries based on the
culture that they follow and then filter the news events based on their content
category. The news events are automatically labelled with the help of Hofstedes
cultural dimensions. We present combinations of events across different
categories and check the performances of different classification methods. We
also presents experimental comparison of different number of features in order
to find a suitable set to represent the culture.",None,-1
Towards Legally Enforceable Hate Speech Detection for Public Forums,0.199302,"Hate speech causes widespread and deep-seated societal issues. Proper
enforcement of hate speech laws is key for protecting groups of people against
harmful and discriminatory language. However, determining what constitutes hate
speech is a complex task that is highly open to subjective interpretations.
Existing works do not align their systems with enforceable definitions of hate
speech, which can make their outputs inconsistent with the goals of regulators.
This research introduces a new perspective and task for enforceable hate speech
detection centred around legal definitions, and a dataset annotated on
violations of eleven possible definitions by legal experts. Given the challenge
of identifying clear, legally enforceable instances of hate speech, we augment
the dataset with expert-generated samples and an automatically mined challenge
set. We experiment with grounding the model decision in these definitions using
zero-shot and few-shot prompting. We then report results on several large
language models (LLMs). With this task definition, automatic hate speech
detection can be more closely aligned to enforceable laws, and hence assist in
more rigorous enforcement of legal protections against harmful speech in public
forums.",None,-1
DiffVoice: Text-to-Speech with Latent Diffusion,0.768762,"In this work, we present DiffVoice, a novel text-to-speech model based on
latent diffusion. We propose to first encode speech signals into a phoneme-rate
latent representation with a variational autoencoder enhanced by adversarial
training, and then jointly model the duration and the latent representation
with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS
datasets demonstrate that our method beats the best publicly available systems
in naturalness. By adopting recent generative inverse problem solving
algorithms for diffusion models, DiffVoice achieves the state-of-the-art
performance in text-based speech editing, and zero-shot adaptation.",None,-1
Experimental results from applying GPT-4 to an unpublished formal language,0.210909,"Can large language models be used to complete mathematical tasks that are
traditionally performed either manually or with the aid of theorem provers? To
answer this question, a state-of-the-art system, GPT-4, was provided with a
concise natural language specification for a previously unpublished formal
system and asked to complete a number of tasks, from stating function and type
definitions to proving simple theorems and verifying user-supplied proofs. The
system completed all tasks successfully, showed extensive domain knowledge,
invented helpful new syntax and semantics, and exhibited generalization and
inference abilities. So the answer seems to be: yes.",None,-1
Structuring ontologies in a context of collaborative system modelling,0.127887,"Prospective studies require discussing and collaborating with the
stakeholders to create scenarios of the possible evolution of the studied
value-chain. However, stakeholders don't always use the same words when
referring to one idea. Constructing an ontology and homogenizing vocabularies
is thus crucial to identify key variables which serve in the construction of
the needed scenarios. Nevertheless, it is a very complex and timeconsuming
task. In this paper we present the method we used to manually build ontologies
adapted to the needs of two complementary system-analysis models (namely the
""Godet"" and the ""MyChoice"" models), starting from interviews of the agri-food
system's stakeholders.",None,-1
Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images,0.407987,"Object detection on drone images with low-latency is an important but
challenging task on the resource-constrained unmanned aerial vehicle (UAV)
platform. This paper investigates optimizing the detection head based on the
sparse convolution, which proves effective in balancing the accuracy and
efficiency. Nevertheless, it suffers from inadequate integration of contextual
information of tiny objects as well as clumsy control of the mask ratio in the
presence of foreground with varying scales. To address the issues above, we
propose a novel global context-enhanced adaptive sparse convolutional network
(CEASC). It first develops a context-enhanced group normalization (CE-GN)
layer, by replacing the statistics based on sparsely sampled features with the
global contextual ones, and then designs an adaptive multi-layer masking
strategy to generate optimal mask ratios at distinct scales for compact
foreground coverage, promoting both the accuracy and efficiency. Extensive
experimental results on two major benchmarks, i.e. VisDrone and UAVDT,
demonstrate that CEASC remarkably reduces the GFLOPs and accelerates the
inference procedure when plugging into the typical state-of-the-art detection
frameworks (e.g. RetinaNet and GFL V1) with competitive performance. Code is
available at https://github.com/Cuogeihong/CEASC.",None,-1
Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis,0.829318,"Performances on standard 3D point cloud benchmarks have plateaued, resulting
in oversized models and complex network design to make a fractional
improvement. We present an alternative to enhance existing deep neural networks
without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter
(SN-Adapter). Building on any trained 3D network, we utilize its learned
encoding capability to extract features of the training dataset and summarize
them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter
retrieves k nearest neighbors (k-NN) from the pre-constructed spatial
prototypes and linearly interpolates the k-NN prediction with that of the
original 3D network. By providing complementary characteristics, the proposed
SN-Adapter serves as a plug-and-play module to economically improve performance
in a non-parametric manner. More importantly, our SN-Adapter can be effectively
generalized to various 3D tasks, including shape classification, part
segmentation, and 3D object detection, demonstrating its superiority and
robustness. We hope our approach could show a new perspective for point cloud
analysis and facilitate future research.",None,-1
Equivariant Multi-Modality Image Fusion,0.24753,"Multi-modality image fusion is a technique that combines information from
different sensors or modalities, enabling the fused image to retain
complementary features from each modality, such as functional highlights and
texture details. However, effective training of such fusion models is
challenging due to the scarcity of ground truth fusion data. To tackle this
issue, we propose the Equivariant Multi-Modality imAge fusion (EMMA) paradigm
for end-to-end self-supervised learning. Our approach is rooted in the prior
knowledge that natural imaging responses are equivariant to certain
transformations. Consequently, we introduce a novel training paradigm that
encompasses a fusion module, a pseudo-sensing module, and an equivariant fusion
module. These components enable the net training to follow the principles of
the natural sensing-imaging process while satisfying the equivariant imaging
prior. Extensive experiments confirm that EMMA yields high-quality fusion
results for infrared-visible and medical images, concurrently facilitating
downstream multi-modal segmentation and detection tasks. The code is available
at https://github.com/Zhaozixiang1228/MMIF-EMMA.",None,-1
TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation,0.855161,"Test-time adaptation methods have been gaining attention recently as a
practical solution for addressing source-to-target domain gaps by gradually
updating the model without requiring labels on the target data. In this paper,
we propose a method of test-time adaptation for category-level object pose
estimation called TTA-COPE. We design a pose ensemble approach with a
self-training loss using pose-aware confidence. Unlike previous unsupervised
domain adaptation methods for category-level object pose estimation, our
approach processes the test data in a sequential, online manner, and it does
not require access to the source domain at runtime. Extensive experimental
results demonstrate that the proposed pose ensemble and the self-training loss
improve category-level object pose performance during test time under both
semi-supervised and unsupervised settings. Project page:
https://taeyeop.com/ttacope",None,-1
Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra,0.769243,"To improve the accuracy of color image completion with missing entries, we
present a recovery method based on generalized higher-order scalars. We extend
the traditional second-order matrix model to a more comprehensive higher-order
matrix equivalent, called the ""t-matrix"" model, which incorporates a pixel
neighborhood expansion strategy to characterize the local pixel constraints.
This ""t-matrix"" model is then used to extend some commonly used matrix and
tensor completion algorithms to their higher-order versions. We perform
extensive experiments on various algorithms using simulated data and algorithms
on simulated data and publicly available images and compare their performance.
The results show that our generalized matrix completion model and the
corresponding algorithm compare favorably with their lower-order tensor and
conventional matrix counterparts.",None,-1
ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis,0.616197,"Generative AI has received substantial attention in recent years due to its
ability to synthesize data that closely resembles the original data source.
While Generative Adversarial Networks (GANs) have provided innovative
approaches for histopathological image analysis, they suffer from limitations
such as mode collapse and overfitting in discriminator. Recently, Denoising
Diffusion models have demonstrated promising results in computer vision. These
models exhibit superior stability during training, better distribution
coverage, and produce high-quality diverse images. Additionally, they display a
high degree of resilience to noise and perturbations, making them well-suited
for use in digital pathology, where images commonly contain artifacts and
exhibit significant variations in staining. In this paper, we present a novel
approach, namely ViT-DAE, which integrates vision transformers (ViT) and
diffusion autoencoders for high-quality histopathology image synthesis. This
marks the first time that ViT has been introduced to diffusion autoencoders in
computational pathology, allowing the model to better capture the complex and
intricate details of histopathology images. We demonstrate the effectiveness of
ViT-DAE on three publicly available datasets. Our approach outperforms recent
GAN-based and vanilla DAE methods in generating realistic images.",None,-1
The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students,0.157014,"The recent advancement in Natural Language Processing (NLP) capability has
led to the development of language models (e.g., ChatGPT) that is capable of
generating human-like language. In this study, we explore how language models
can be utilized to help the ideation aspect of creative writing. Our empirical
findings show that language models play different roles in helping student
writers to be more creative, such as the role of a collaborator, a provocateur,
etc",None,-1
DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification,0.0388843,"In this paper, we propose a novel learning paradigm called ""DeepFlorist"" for
flower classification using ensemble learning as a meta-classifier. DeepFlorist
combines the power of deep learning with the robustness of ensemble methods to
achieve accurate and reliable flower classification results. The proposed
network architecture leverages a combination of dense convolutional and
convolutional neural networks (DCNNs and CNNs) to extract high-level features
from flower images, followed by a fully connected layer for classification. To
enhance the performance and generalization of DeepFlorist, an ensemble learning
approach is employed, incorporating multiple diverse models to improve the
classification accuracy. Experimental results on benchmark flower datasets
demonstrate the effectiveness of DeepFlorist, outperforming state-of-the-art
methods in terms of accuracy and robustness. The proposed framework holds
significant potential for automated flower recognition systems in real-world
applications, enabling advancements in plant taxonomy, conservation efforts,
and ecological studies.",None,-1
MRecGen: Multimodal Appropriate Reaction Generator,0.815906,"Verbal and non-verbal human reaction generation is a challenging task, as
different reactions could be appropriate for responding to the same behaviour.
This paper proposes the first multiple and multimodal (verbal and nonverbal)
appropriate human reaction generation framework that can generate appropriate
and realistic human-style reactions (displayed in the form of synchronised
text, audio and video streams) in response to an input user behaviour. This
novel technique can be applied to various human-computer interaction scenarios
by generating appropriate virtual agent/robot behaviours. Our demo is available
at \url{https://github.com/SSYSteve/MRecGen}.",None,-1
CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer,0.760784,"Content affinity loss including feature and pixel affinity is a main problem
which leads to artifacts in photorealistic and video style transfer. This paper
proposes a new framework named CAP-VSTNet, which consists of a new reversible
residual network and an unbiased linear transform module, for versatile style
transfer. This reversible residual network can not only preserve content
affinity but not introduce redundant information as traditional reversible
networks, and hence facilitate better stylization. Empowered by Matting
Laplacian training loss which can address the pixel affinity loss problem led
by the linear transform, the proposed framework is applicable and effective on
versatile style transfer. Extensive experiments show that CAP-VSTNet can
produce better qualitative and quantitative results in comparison with the
state-of-the-art methods.",None,-1
Prediction-Powered Inference,0.871716,"Prediction-powered inference is a framework for performing valid statistical
inference when an experimental dataset is supplemented with predictions from a
machine-learning system. The framework yields simple algorithms for computing
provably valid confidence intervals for quantities such as means, quantiles,
and linear and logistic regression coefficients, without making any assumptions
on the machine-learning algorithm that supplies the predictions. Furthermore,
more accurate predictions translate to smaller confidence intervals.
Prediction-powered inference could enable researchers to draw valid and more
data-efficient conclusions using machine learning. The benefits of
prediction-powered inference are demonstrated with datasets from proteomics,
astronomy, genomics, remote sensing, census analysis, and ecology.",None,-1
Using Text Injection to Improve Recognition of Personal Identifiers in Speech,0.678188,"Accurate recognition of specific categories, such as persons' names, dates or
other identifiers is critical in many Automatic Speech Recognition (ASR)
applications. As these categories represent personal information, ethical use
of this data including collection, transcription, training and evaluation
demands special care. One way of ensuring the security and privacy of
individuals is to redact or eliminate Personally Identifiable Information (PII)
from collection altogether. However, this results in ASR models that tend to
have lower recognition accuracy of these categories. We use text-injection to
improve the recognition of PII categories by including fake textual substitutes
of PII categories in the training data using a text injection method. We
demonstrate substantial improvement to Recall of Names and Dates in medical
notes while improving overall WER. For alphanumeric digit sequences we show
improvements to Character Error Rate and Sentence Accuracy.",None,-1
Robust Deep Reinforcement Learning Scheduling via Weight Anchoring,0.14753,"Questions remain on the robustness of data-driven learning methods when
crossing the gap from simulation to reality. We utilize weight anchoring, a
method known from continual learning, to cultivate and fixate desired behavior
in Neural Networks. Weight anchoring may be used to find a solution to a
learning problem that is nearby the solution of another learning problem.
Thereby, learning can be carried out in optimal environments without neglecting
or unlearning desired behavior. We demonstrate this approach on the example of
learning mixed QoS-efficient discrete resource scheduling with infrequent
priority messages. Results show that this method provides performance
comparable to the state of the art of augmenting a simulation environment,
alongside significantly increased robustness and steerability.",None,-1
Scalable Prompt Generation for Semi-supervised Learning with Language Models,0.652661,"Prompt-based learning methods in semi-supervised learning (SSL) settings have
been shown to be effective on multiple natural language understanding (NLU)
datasets and tasks in the literature. However, manually designing multiple
prompts and verbalizers requires domain knowledge and human effort, making it
difficult and expensive to scale across different datasets. In this paper, we
propose two methods to automatically design multiple prompts and integrate
automatic verbalizer in SSL settings without sacrificing performance. The first
method uses various demonstration examples with learnable continuous prompt
tokens to create diverse prompt models. The second method uses a varying number
of soft prompt tokens to encourage language models to learn different prompts.
For the verbalizer, we use the prototypical verbalizer to replace the manual
one. In summary, we obtained the best average accuracy of 73.2% (a relative
improvement of 2.52% over even the previous state-of-the-art SSL method with
manual prompts and verbalizers) in different few-shot learning settings.",None,-1
Difference of Probability and Information Entropy for Skills Classification and Prediction in Student Learning,0.0261916,"The probability of an event is in the range of [0, 1]. In a sample space S,
the value of probability determines whether an outcome is true or false. The
probability of an event Pr(A) that will never occur = 0. The probability of the
event Pr(B) that will certainly occur = 1. This makes both events A and B thus
a certainty. Furthermore, the sum of probabilities Pr(E1) + Pr(E2) + ... +
Pr(En) of a finite set of events in a given sample space S = 1. Conversely, the
difference of the sum of two probabilities that will certainly occur is 0.
Firstly, this paper discusses Bayes' theorem, then complement of probability
and the difference of probability for occurrences of learning-events, before
applying these in the prediction of learning objects in student learning. Given
the sum total of 1; to make recommendation for student learning, this paper
submits that the difference of argMaxPr(S) and probability of
student-performance quantifies the weight of learning objects for students.
Using a dataset of skill-set, the computational procedure demonstrates: i) the
probability of skill-set events that has occurred that would lead to higher
level learning; ii) the probability of the events that has not occurred that
requires subject-matter relearning; iii) accuracy of decision tree in the
prediction of student performance into class labels; and iv) information
entropy about skill-set data and its implication on student cognitive
performance and recommendation of learning [1].",None,-1
FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning,0.891416,"In this paper, we propose FrameBERT, a RoBERTa-based model that can
explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor
detection. FrameBERT not only achieves better or comparable performance to the
state-of-the-art, but also is more explainable and interpretable compared to
existing models, attributing to its ability of accounting for external
knowledge of FrameNet.",None,-1
Object Discovery from Motion-Guided Tokens,0.212293,"Object discovery -- separating objects from the background without manual
labels -- is a fundamental open challenge in computer vision. Previous methods
struggle to go beyond clustering of low-level cues, whether handcrafted (e.g.,
color, texture) or learned (e.g., from auto-encoders). In this work, we augment
the auto-encoder representation learning framework with two key components:
motion-guidance and mid-level feature tokenization. Although both have been
separately investigated, we introduce a new transformer decoder showing that
their benefits can compound thanks to motion-guided vector quantization. We
show that our architecture effectively leverages the synergy between motion and
tokenization, improving upon the state of the art on both synthetic and real
datasets. Our approach enables the emergence of interpretable object-specific
mid-level features, demonstrating the benefits of motion-guidance (no labeling)
and quantization (interpretability, memory efficiency).",None,-1
DistillCSE: Distilled Contrastive Learning for Sentence Embeddings,0.156515,"This paper proposes the DistillCSE framework, which performs contrastive
learning under the self-training paradigm with knowledge distillation. The
potential advantage of DistillCSE is its self-enhancing feature: using a base
model to provide additional supervision signals, a stronger model may be
learned through knowledge distillation. However, the vanilla DistillCSE through
the standard implementation of knowledge distillation only achieves marginal
improvements due to severe overfitting. The further quantitative analyses
demonstrate the reason that the standard knowledge distillation exhibits a
relatively large variance of the teacher model's logits due to the essence of
contrastive learning. To mitigate the issue induced by high variance, this
paper accordingly proposed two simple yet effective solutions for knowledge
distillation: a Group-P shuffling strategy as an implicit regularization and
the averaging logits from multiple teacher components. Experiments on standard
benchmarks demonstrate that the proposed DistillCSE outperforms many strong
baseline methods and yields a new state-of-the-art performance.",None,-1
A Novel Energy based Model Mechanism for Multi-modal Aspect-Based Sentiment Analysis,0.371938,"Multi-modal aspect-based sentiment analysis (MABSA) has recently attracted
increasing attention. The span-based extraction methods, such as FSUIE,
demonstrate strong performance in sentiment analysis due to their joint
modeling of input sequences and target labels. However, previous methods still
have certain limitations: (i) They ignore the difference in the focus of visual
information between different analysis targets (aspect or sentiment). (ii)
Combining features from uni-modal encoders directly may not be sufficient to
eliminate the modal gap and can cause difficulties in capturing the image-text
pairwise relevance. (iii) Existing span-based methods for MABSA ignore the
pairwise relevance of target span boundaries. To tackle these limitations, we
propose a novel framework called DQPSA for multi-modal sentiment analysis.
Specifically, our model contains a Prompt as Dual Query (PDQ) module that uses
the prompt as both a visual query and a language query to extract prompt-aware
visual information and strengthen the pairwise relevance between visual
information and the analysis target. Additionally, we introduce an Energy-based
Pairwise Expert (EPE) module that models the boundaries pairing of the analysis
target from the perspective of an Energy-based Model. This expert predicts
aspect or sentiment span based on pairwise stability. Experiments on three
widely used benchmarks demonstrate that DQPSA outperforms previous approaches
and achieves a new state-of-the-art performance.",None,-1
Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling,0.979691,"Recent works have proposed to craft adversarial clothes for evading person
detectors, while they are either only effective at limited viewing angles or
very conspicuous to humans. We aim to craft adversarial texture for clothes
based on 3D modeling, an idea that has been used to craft rigid adversarial
objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes
are non-rigid, leading to difficulties in physical realization. In order to
craft natural-looking adversarial clothes that can evade person detectors at
multiple viewing angles, we propose adversarial camouflage textures (AdvCaT)
that resemble one kind of the typical textures of daily clothes, camouflage
textures. We leverage the Voronoi diagram and Gumbel-softmax trick to
parameterize the camouflage textures and optimize the parameters via 3D
modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes
combining topologically plausible projection (TopoProj) and Thin Plate Spline
(TPS) to narrow the gap between digital and real-world objects. We printed the
developed 3D texture pieces on fabric materials and tailored them into T-shirts
and trousers. Experiments show high attack success rates of these clothes
against multiple detectors.",None,-1
ChatGPT-Powered Hierarchical Comparisons for Image Classification,0.433263,"The zero-shot open-vocabulary challenge in image classification is tackled by
pretrained vision-language models like CLIP, which benefit from incorporating
class-specific knowledge from large language models (LLMs) like ChatGPT.
However, biases in CLIP lead to similar descriptions for distinct but related
classes, prompting our novel image classification framework via hierarchical
comparisons: using LLMs to recursively group classes into hierarchies and
classifying images by comparing image-text embeddings at each hierarchy level,
resulting in an intuitive, effective, and explainable approach.",None,-1
Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning,0.685959,"Self-supervised learning (SSL) has made remarkable progress in visual
representation learning. Some studies combine SSL with knowledge distillation
(SSL-KD) to boost the representation learning performance of small models. In
this study, we propose a Multi-mode Online Knowledge Distillation method (MOKD)
to boost self-supervised visual representation learning. Different from
existing SSL-KD methods that transfer knowledge from a static pre-trained
teacher to a student, in MOKD, two different models learn collaboratively in a
self-supervised manner. Specifically, MOKD consists of two distillation modes:
self-distillation and cross-distillation modes. Among them, self-distillation
performs self-supervised learning for each model independently, while
cross-distillation realizes knowledge interaction between different models. In
cross-distillation, a cross-attention feature search strategy is proposed to
enhance the semantic feature alignment between different models. As a result,
the two models can absorb knowledge from each other to boost their
representation learning performance. Extensive experimental results on
different backbones and datasets demonstrate that two heterogeneous models can
benefit from MOKD and outperform their independently trained baseline. In
addition, MOKD also outperforms existing SSL-KD methods for both the student
and teacher models.",None,-1
PerPLM: Personalized Fine-tuning of Pretrained Language Models via Writer-specific Intermediate Learning and Prompts,0.0677078,"The meanings of words and phrases depend not only on where they are used
(contexts) but also on who use them (writers). Pretrained language models
(PLMs) are powerful tools for capturing context, but they are typically
pretrained and fine-tuned for universal use across different writers. This
study aims to improve the accuracy of text understanding tasks by personalizing
the fine-tuning of PLMs for specific writers. We focus on a general setting
where only the plain text from target writers are available for
personalization. To avoid the cost of fine-tuning and storing multiple copies
of PLMs for different users, we exhaustively explore using writer-specific
prompts to personalize a unified PLM. Since the design and evaluation of these
prompts is an underdeveloped area, we introduce and compare different types of
prompts that are possible in our setting. To maximize the potential of
prompt-based personalized fine-tuning, we propose a personalized intermediate
learning based on masked language modeling to extract task-independent traits
of writers' text. Our experiments, using multiple tasks, datasets, and PLMs,
reveal the nature of different prompts and the effectiveness of our
intermediate learning approach.",None,-1
PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination,0.423924,"Zero-shot human-AI coordination holds the promise of collaborating with
humans without human data. Prevailing methods try to train the ego agent with a
population of partners via self-play. However, these methods suffer from two
problems: 1) The diversity of a population with finite partners is limited,
thereby limiting the capacity of the trained ego agent to collaborate with a
novel human; 2) Current methods only provide a common best response for every
partner in the population, which may result in poor zero-shot coordination
performance with a novel partner or humans. To address these issues, we first
propose the policy ensemble method to increase the diversity of partners in the
population, and then develop a context-aware method enabling the ego agent to
analyze and identify the partner's potential policy primitives so that it can
take different actions accordingly. In this way, the ego agent is able to learn
more universal cooperative behaviors for collaborating with diverse partners.
We conduct experiments on the Overcooked environment, and evaluate the
zero-shot human-AI coordination performance of our method with both
behavior-cloned human proxies and real humans. The results demonstrate that our
method significantly increases the diversity of partners and enables ego agents
to learn more diverse behaviors than baselines, thus achieving state-of-the-art
performance in all scenarios. We also open-source a human-AI coordination study
framework on the Overcooked for the convenience of future studies.",None,-1
SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data,0.853472,"Text-to-SQL aims to automate the process of generating SQL queries on a
database from natural language text. In this work, we propose ""SQLPrompt"",
tailored to improve the few-shot prompting capabilities of Text-to-SQL for
Large Language Models (LLMs). Our methods include innovative prompt design,
execution-based consistency decoding strategy which selects the SQL with the
most consistent execution outcome among other SQL proposals, and a method that
aims to improve performance by diversifying the SQL proposals during
consistency selection with different prompt designs (""MixPrompt"") and
foundation models (""MixLLMs""). We show that \emph{SQLPrompt} outperforms
previous approaches for in-context learning with few labeled data by a large
margin, closing the gap with finetuning state-of-the-art with thousands of
labeled data.",None,-1
Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction,0.839892,"Accurate prediction of what types of patents that companies will apply for in
the next period of time can figure out their development strategies and help
them discover potential partners or competitors in advance. Although important,
this problem has been rarely studied in previous research due to the challenges
in modelling companies' continuously evolving preferences and capturing the
semantic correlations of classification codes. To fill in this gap, we propose
an event-based dynamic graph learning framework for patent application trend
prediction. In particular, our method is founded on the memorable
representations of both companies and patent classification codes. When a new
patent is observed, the representations of the related companies and
classification codes are updated according to the historical memories and the
currently encoded messages. Moreover, a hierarchical message passing mechanism
is provided to capture the semantic proximities of patent classification codes
by updating their representations along the hierarchical taxonomy. Finally, the
patent application trend is predicted by aggregating the representations of the
target company and classification codes from static, dynamic, and hierarchical
perspectives. Experiments on real-world data demonstrate the effectiveness of
our approach under various experimental conditions, and also reveal the
abilities of our method in learning semantics of classification codes and
tracking technology developing trajectories of companies.",None,-1
Chinese Spelling Correction as Rephrasing Language Model,0.75813,"This paper studies Chinese Spelling Correction (CSC), which aims to detect
and correct the potential spelling errors in a given sentence. Current
state-of-the-art methods regard CSC as a sequence tagging task and fine-tune
BERT-based models on sentence pairs. However, we note a critical flaw in the
process of tagging one character to another, that the correction is excessively
conditioned on the error. This is opposite from human mindset, where
individuals rephrase the complete sentence based on its semantics, rather than
solely on the error patterns memorized before. Such a counter-intuitive
learning process results in the bottleneck of generalizability and
transferability of machine spelling correction. To address this, we propose
Rephrasing Language Model (ReLM), where the model is trained to rephrase the
entire sentence by infilling additional slots, instead of
character-to-character tagging. This novel training paradigm achieves the new
state-of-the-art results across fine-tuned and zero-shot CSC benchmarks,
outperforming previous counterparts by a large margin. Our method also learns
transferable language representation when CSC is jointly trained with other
tasks.",None,-1
Content-based Unrestricted Adversarial Attack,0.859301,"Unrestricted adversarial attacks typically manipulate the semantic content of
an image (e.g., color or texture) to create adversarial examples that are both
effective and photorealistic, demonstrating their ability to deceive human
perception and deep neural networks with stealth and success. However, current
works usually sacrifice unrestricted degrees and subjectively select some image
content to guarantee the photorealism of unrestricted adversarial examples,
which limits its attack performance. To ensure the photorealism of adversarial
examples and boost attack performance, we propose a novel unrestricted attack
framework called Content-based Unrestricted Adversarial Attack. By leveraging a
low-dimensional manifold that represents natural images, we map the images onto
the manifold and optimize them along its adversarial direction. Therefore,
within this framework, we implement Adversarial Content Attack based on Stable
Diffusion and can generate high transferable unrestricted adversarial examples
with various adversarial contents. Extensive experimentation and visualization
demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art
attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models
and defense methods, respectively.",None,-1
Large Language Models Cannot Self-Correct Reasoning Yet,0.86057,"Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance even
degrades after self-correction. Drawing from these insights, we offer
suggestions for future research and practical applications in this field.",None,-1
FATE in AI: Towards Algorithmic Inclusivity and Accessibility,0.138544,"Artificial Intelligence (AI) is at the forefront of modern technology, and
its effects are felt in many areas of society. To prevent algorithmic
disparities, fairness, accountability, transparency, and ethics (FATE) in AI
are being implemented. However, the current discourse on these issues is
largely dominated by more economically developed countries (MEDC), leaving out
local knowledge, cultural pluralism, and global fairness. This study aims to
address this gap by examining FATE-related desiderata, particularly
transparency and ethics, in areas of the global South that are underserved by
AI. A user study (n=43) and a participatory session (n=30) were conducted to
achieve this goal. The results showed that AI models can encode bias and
amplify stereotypes. To promote inclusivity, a community-led strategy is
proposed to collect and curate representative data for responsible AI design.
This will enable the affected community or individuals to monitor the
increasing use of AI-powered systems. Additionally, recommendations based on
public input are provided to ensure that AI adheres to social values and
context-specific FATE needs.",None,-1
ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories,0.0874346,"Recently, Large Language Models (LLMs) have been serving as general-purpose
interfaces, posing a significant demand for comprehensive visual knowledge.
However, it remains unclear how well current LLMs and their visually augmented
counterparts (VaLMs) can master visual commonsense knowledge. To investigate
this, we propose ImageNetVC, a human-annotated dataset specifically designed
for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet
categories. Utilizing ImageNetVC, we benchmark the fundamental visual
commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze
the factors affecting the visual commonsense knowledge of large-scale models,
providing insights into the development of language models enriched with visual
commonsense knowledge. Our code and dataset are available at
https://github.com/hemingkx/ImageNetVC.",None,-1
Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection,0.843841,"Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.",None,-1
Pay More Attention to Relation Exploration for Knowledge Base Question Answering,0.193465,"Knowledge base question answering (KBQA) is a challenging task that aims to
retrieve correct answers from large-scale knowledge bases. Existing attempts
primarily focus on entity representation and final answer reasoning, which
results in limited supervision for this task. Moreover, the relations, which
empirically determine the reasoning path selection, are not fully considered in
recent advancements. In this study, we propose a novel framework, RE-KBQA, that
utilizes relations in the knowledge base to enhance entity representation and
introduce additional supervision. We explore guidance from relations in three
aspects, including (1) distinguishing similar entities by employing a
variational graph auto-encoder to learn relation importance; (2) exploring
extra supervision by predicting relation distributions as soft labels with a
multi-task scheme; (3) designing a relation-guided re-ranking algorithm for
post-processing. Experimental results on two benchmark datasets demonstrate the
effectiveness and superiority of our framework, improving the F1 score by 5.7%
from 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par
with state-of-the-art methods.",None,-1
Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering,0.0884352,"We train a language model (LM) to robustly answer multistep questions by
generating and answering sub-questions. We propose Chain-of-Questions, a
framework that trains a model to generate sub-questions and sub-answers one at
a time by leveraging human annotated question decomposition meaning
representation (QDMR). The key technical challenge is that QDMR only contains
sub-questions but not answers to those sub-questions, so we treat sub-answers
as latent variables and optimize them using a novel dynamic mixture of Hard-EM
and MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods
by 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA
adversarial set, thus demonstrating the effectiveness and robustness of our
framework.",None,-1
Dual-Alignment Pre-training for Cross-lingual Sentence Embedding,0.808211,"Recent studies have shown that dual encoder models trained with the
sentence-level translation ranking task are effective methods for cross-lingual
sentence embedding. However, our research indicates that token-level alignment
is also crucial in multilingual scenarios, which has not been fully explored
previously. Based on our findings, we propose a dual-alignment pre-training
(DAP) framework for cross-lingual sentence embedding that incorporates both
sentence-level and token-level alignment. To achieve this, we introduce a novel
representation translation learning (RTL) task, where the model learns to use
one-side contextualized token representation to reconstruct its translation
counterpart. This reconstruction objective encourages the model to embed
translation information into the token representation. Compared to other
token-level alignment methods such as translation language modeling, RTL is
more suitable for dual encoder architectures and is computationally efficient.
Extensive experiments on three sentence-level cross-lingual benchmarks
demonstrate that our approach can significantly improve sentence embedding. Our
code is available at https://github.com/ChillingDream/DAP.",None,-1
Measuring axiomatic soundness of counterfactual image models,0.907701,"We present a general framework for evaluating image counterfactuals. The
power and flexibility of deep generative models make them valuable tools for
learning mechanisms in structural causal models. However, their flexibility
makes counterfactual identifiability impossible in the general case. Motivated
by these issues, we revisit Pearl's axiomatic definition of counterfactuals to
determine the necessary constraints of any counterfactual inference model:
composition, reversibility, and effectiveness. We frame counterfactuals as
functions of an input variable, its parents, and counterfactual parents and use
the axiomatic constraints to restrict the set of functions that could represent
the counterfactual, thus deriving distance metrics between the approximate and
ideal functions. We demonstrate how these metrics can be used to compare and
choose between different approximate counterfactual inference models and to
provide insight into a model's shortcomings and trade-offs.",None,-1
Benchmarking the Generation of Fact Checking Explanations,0.538315,"Fighting misinformation is a challenging, yet crucial, task. Despite the
growing number of experts being involved in manual fact-checking, this activity
is time-consuming and cannot keep up with the ever-increasing amount of Fake
News produced daily. Hence, automating this process is necessary to help curb
misinformation. Thus far, researchers have mainly focused on claim veracity
classification. In this paper, instead, we address the generation of
justifications (textual explanation of why a claim is classified as either true
or false) and benchmark it with novel datasets and advanced baselines. In
particular, we focus on summarization approaches over unstructured knowledge
(i.e. news articles) and we experiment with several extractive and abstractive
strategies. We employed two datasets with different styles and structures, in
order to assess the generalizability of our findings. Results show that in
justification production summarization benefits from the claim information,
and, in particular, that a claim-driven extractive step improves abstractive
summarization performances. Finally, we show that although cross-dataset
experiments suffer from performance degradation, a unique model trained on a
combination of the two datasets is able to retain style information in an
efficient manner.",None,-1
Self-Sufficient Framework for Continuous Sign Language Recognition,0.908278,"The goal of this work is to develop self-sufficient framework for Continuous
Sign Language Recognition (CSLR) that addresses key issues of sign language
recognition. These include the need for complex multi-scale features such as
hands, face, and mouth for understanding, and absence of frame-level
annotations. To this end, we propose (1) Divide and Focus Convolution (DFConv)
which extracts both manual and non-manual features without the need for
additional networks or annotations, and (2) Dense Pseudo-Label Refinement
(DPLR) which propagates non-spiky frame-level pseudo-labels by combining the
ground truth gloss sequence labels with the predicted sequence. We demonstrate
that our model achieves state-of-the-art performance among RGB-based methods on
large-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing
comparable results with better efficiency when compared to other approaches
that use multi-modality or extra annotations.",None,-1
Trusting Your Evidence: Hallucinate Less with Context-aware Decoding,0.912161,"Language models (LMs) often struggle to pay enough attention to the input
context, and generate texts that are unfaithful or contain hallucinations. To
mitigate this issue, we present context-aware decoding (CAD), which follows a
contrastive output distribution that amplifies the difference between the
output probabilities when a model is used with and without context. Our
experiments show that CAD, without additional training, significantly improves
the faithfulness of different LM families, including OPT, GPT, LLaMA and
FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality
metrics). Furthermore, CAD is particularly effective in overriding a model's
prior knowledge when it contradicts the provided context, leading to
substantial improvements in tasks where resolving the knowledge conflict is
essential.",None,-1
QVRF: A Quantization-error-aware Variable Rate Framework for Learned Image Compression,0.303119,"Learned image compression has exhibited promising compression performance,
but variable bitrates over a wide range remain a challenge. State-of-the-art
variable rate methods compromise the loss of model performance and require
numerous additional parameters. In this paper, we present a
Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a
univariate quantization regulator a to achieve wide-range variable rates within
a single model. Specifically, QVRF defines a quantization regulator vector
coupled with predefined Lagrange multipliers to control quantization error of
all latent representation for discrete variable rates. Additionally, the
reparameterization method makes QVRF compatible with a round quantizer.
Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods
equipped with QVRF can achieve wide-range continuous variable rates within a
single model without significant performance degradation. Furthermore, QVRF
outperforms contemporary variable-rate methods in rate-distortion performance
with minimal additional parameters.",None,-1
Quantifying Consistency and Information Loss for Causal Abstraction Learning,0.345791,"Structural causal models provide a formalism to express causal relations
between variables of interest. Models and variables can represent a system at
different levels of abstraction, whereby relations may be coarsened and refined
according to the need of a modeller. However, switching between different
levels of abstraction requires evaluating a trade-off between the consistency
and the information loss among different models. In this paper we introduce a
family of interventional measures that an agent may use to evaluate such a
trade-off. We consider four measures suited for different tasks, analyze their
properties, and propose algorithms to evaluate and learn causal abstractions.
Finally, we illustrate the flexibility of our setup by empirically showing how
different measures and algorithmic choices may lead to different abstractions.",None,-1
Internal-External Boundary Attention Fusion for Glass Surface Segmentation,0.623397,"Glass surfaces of transparent objects and mirrors are not able to be uniquely
and explicitly characterized by their visual appearances because they contain
the visual appearance of other reflected or transmitted surfaces as well.
Detecting glass regions from a single-color image is a challenging task. Recent
deep-learning approaches have paid attention to the description of glass
surface boundary where the transition of visual appearances between glass and
non-glass surfaces are observed. In this work, we analytically investigate how
glass surface boundary helps to characterize glass objects. Inspired by prior
semantic segmentation approaches with challenging image types such as X-ray or
CT scans, we propose separated internal-external boundary attention modules
that individually learn and selectively integrate visual characteristics of the
inside and outside region of glass surface from a single color image. Our
proposed method is evaluated on six public benchmarks comparing with
state-of-the-art methods showing promising results.",None,-1
Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization,0.57646,"In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset both to the
public and the market. We conduct a comprehensive study on motion style
transfer in the experiment using the state-of-the-art method, and the results
show the proposed dataset's validity for the motion style transfer task.",None,-1
COVID-19 Spreading Prediction and Impact Analysis by Using Artificial Intelligence for Sustainable Global Health Assessment,0.180092,"The COVID-19 pandemic is considered as the most alarming global health
calamity of this century. COVID-19 has been confirmed to be mutated from
coronavirus family. As stated by the records of The World Health Organization
(WHO at April 18 2020), the present epidemic of COVID-19, has influenced more
than 2,164,111 persons and killed more than 146,198 folks in over 200 countries
across the globe and billions had confronted impacts in lifestyle because of
this virus outbreak. The ongoing overall outbreak of the COVID-19 opened up new
difficulties to the research sectors. Artificial intelligence (AI) driven
strategies can be valuable to predict the parameters, hazards, and impacts of
such an epidemic in a cost-efficient manner. The fundamental difficulties of AI
in this situation is the limited availability of information and the uncertain
nature of the disease. Here in this article, we have tried to integrate AI to
predict the infection outbreak and along with this, we have also tried to test
whether AI with help deep learning can recognize COVID-19 infected chest X-Rays
or not. The global outbreak of the virus posed enormous economic, ecological
and societal challenges into the human population and with help of this paper,
we have tried to give a message that AI can help us to identify certain
features of the disease outbreak that could prove to be essential to protect
the humanity from this deadly disease.",None,-1
TADA: Task-Agnostic Dialect Adapters for English,0.563746,"Large Language Models, the dominant starting point for Natural Language
Processing (NLP) applications, fail at a higher rate for speakers of English
dialects other than Standard American English (SAE). Prior work addresses this
using task-specific data or synthetic data augmentation, both of which require
intervention for each dialect and task pair. This poses a scalability issue
that prevents the broad adoption of robust dialectal English NLP. We introduce
a simple yet effective method for task-agnostic dialect adaptation by aligning
non-SAE dialects using adapters and composing them with task-specific adapters
from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on
4 dialectal variants of the GLUE benchmark without task-specific supervision.",None,-1
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,0.392907,"Large Language Models (LLMs) are evolving at an unprecedented pace and have
exhibited considerable capability in the realm of natural language processing
(NLP) with world knowledge. Benefiting from ultra-large-scale training corpora,
a single LLM can manage typical NLP tasks competently. However, its performance
in executing reasoning tasks is still confined by the limitations of its
internal representations. To push this boundary further, we introduce Corex in
this paper, a suite of novel general-purpose strategies that transform LLMs
into autonomous agents pioneering multi-model collaborations for complex
task-solving. Inspired by human behaviors, Corex is constituted by diverse
collaboration paradigms including Debate, Review, and Retrieve modes, which
collectively work towards enhancing the factuality, faithfulness, and
reliability of the reasoning process. These paradigms foster task-agnostic
approaches that enable LLMs to ''think outside the box,'' thereby overcoming
hallucinations and providing better solutions. Through extensive experiments
across four different types of reasoning tasks, we demonstrate that
orchestrating multiple LLMs to work in concert yields substantially better
performance compared to existing methods. Further results and in-depth analysis
demonstrate the cost-effectiveness of our method, facilitating collaboration
among different LLMs and promoting annotation efficiency.",None,-1
In-context Learning and Gradient Descent Revisited,0.216296,"In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. A recent
line of work suggests that ICL performs gradient descent (GD)-based
optimization implicitly. While appealing, much of the research focuses on
simplified settings, where the parameters of a shallow model are optimized. In
this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks
and models. We find gaps in evaluation, both in terms of problematic metrics
and insufficient baselines. We show that surprisingly, even untrained models
achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,
we explore a major discrepancy in the flow of information throughout the model
between ICL and GD, which we term Layer Causality. We propose a simple GD-based
optimization procedure that respects layer causality, and show it improves
similarity scores significantly.",None,-1
DAG Learning on the Permutahedron,0.890169,"We propose a continuous optimization framework for discovering a latent
directed acyclic graph (DAG) from observational data. Our approach optimizes
over the polytope of permutation vectors, the so-called Permutahedron, to learn
a topological ordering. Edges can be optimized jointly, or learned conditional
on the ordering via a non-differentiable subroutine. Compared to existing
continuous optimization approaches our formulation has a number of advantages
including: 1. validity: optimizes over exact DAGs as opposed to other
relaxations optimizing approximate DAGs; 2. modularity: accommodates any
edge-optimization procedure, edge structural parameterization, and optimization
loss; 3. end-to-end: either alternately iterates between node-ordering and
edge-optimization, or optimizes them jointly. We demonstrate, on real-world
data problems in protein-signaling and transcriptional network discovery, that
our approach lies on the Pareto frontier of two key metrics, the SID and SHD.",None,-1
LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery,0.643008,"Large Language Models (LLMs) have transformed the landscape of artificial
intelligence, while their enormous size presents significant challenges in
terms of computational costs. We introduce LoRAShear, a novel efficient
approach to structurally prune LLMs and recover knowledge. Given general LLMs,
LoRAShear at first creates the dependency graphs over LoRA modules to discover
minimally removal structures and analyze the knowledge distribution. It then
proceeds progressive structured pruning on LoRA adaptors and enables inherent
knowledge transfer to better preserve the information in the redundant
structures. To recover the lost knowledge during pruning, LoRAShear
meticulously studies and proposes a dynamic fine-tuning schemes with dynamic
data adaptors to effectively narrow down the performance gap to the full
models. Numerical results demonstrate that by only using one GPU within a
couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with
only 1.0% performance degradation and significantly outperforms
state-of-the-arts. The source code will be available at
https://github.com/microsoft/lorashear.",None,-1
mPMR: A Multilingual Pre-trained Machine Reader at Scale,0.665015,"We present multilingual Pre-trained Machine Reader (mPMR), a novel method for
multilingual machine reading comprehension (MRC)-style pre-training. mPMR aims
to guide multilingual pre-trained language models (mPLMs) to perform natural
language understanding (NLU) including both sequence classification and span
extraction in multiple languages. To achieve cross-lingual generalization when
only source-language fine-tuning data is available, existing mPLMs solely
transfer NLU capability from a source language to target languages. In
contrast, mPMR allows the direct inheritance of multilingual NLU capability
from the MRC-style pre-training to downstream tasks. Therefore, mPMR acquires
better NLU capability for target languages. mPMR also provides a unified solver
for tackling cross-lingual span extraction and sequence classification, thereby
enabling the extraction of rationales to explain the sentence-pair
classification process.",None,-1
AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes,0.698791,"We propose attribute-aware multimodal entity linking, where the input is a
mention described with a text and image, and the goal is to predict the
corresponding target entity from a multimodal knowledge base (KB) where each
entity is also described with a text description, a visual image and a set of
attributes and values. To support this research, we construct AMELI, a
large-scale dataset consisting of 18,472 reviews and 35,598 products. To
establish baseline performance on AMELI, we experiment with the current
state-of-the-art multimodal entity linking approaches and our enhanced
attribute-aware model and demonstrate the importance of incorporating the
attribute information into the entity linking process. To be best of our
knowledge, we are the first to build benchmark dataset and solutions for the
attribute-aware multimodal entity linking task. Datasets and codes will be made
publicly available.",None,-1
Updated Corpora and Benchmarks for Long-Form Speech Recognition,0.466743,"The vast majority of ASR research uses corpora in which both the training and
test data have been pre-segmented into utterances. In most real-word ASR
use-cases, however, test audio is not segmented, leading to a mismatch between
inference-time conditions and models trained on segmented utterances. In this
paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and
VoxPopuli-en - with updated transcription and alignments to enable their use
for long-form ASR research. We use these reconstituted corpora to study the
train-test mismatch problem for transducers and attention-based
encoder-decoders (AEDs), confirming that AEDs are more susceptible to this
issue. Finally, we benchmark a simple long-form training for these models,
showing its efficacy for model robustness under this domain shift.",None,-1
Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West,0.35998,"Large Language Models (LLMs), now used daily by millions of users, can encode
societal biases, exposing their users to representational harms. A large body
of scholarship on LLM bias exists but it predominantly adopts a Western-centric
frame and attends comparatively less to bias levels and potential harms in the
Global South. In this paper, we quantify stereotypical bias in popular LLMs
according to an Indian-centric frame and compare bias levels between the Indian
and Western contexts. To do this, we develop a novel dataset which we call
Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and
anti-stereotypical examples for caste and religion contexts. We find that the
majority of LLMs tested are strongly biased towards stereotypes in the Indian
context, especially as compared to the Western context. We finally investigate
Instruction Prompting as a simple intervention to mitigate such bias and find
that it significantly reduces both stereotypical and anti-stereotypical biases
in the majority of cases for GPT-3.5. The findings of this work highlight the
need for including more diverse voices when evaluating LLMs.",None,-1
Sequentially Controlled Text Generation,0.111159,"While GPT-2 generates sentences that are remarkably human-like, longer
documents can ramble and do not follow human-like writing structure. We study
the problem of imposing structure on long-range text. We propose a novel
controlled text generation task, sequentially controlled text generation, and
identify a dataset, NewsDiscourse as a starting point for this task. We develop
a sequential controlled text generation pipeline with generation and editing.
We test different degrees of structural awareness and show that, in general,
more structural awareness results in higher control-accuracy, grammaticality,
coherency and topicality, approaching human-level writing performance.",None,-1
LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?,0.230168,"Large language models (LLMs) have exhibited impressive capabilities in
comprehending complex instructions. However, their blind adherence to provided
instructions has led to concerns regarding risks of malicious use. Existing
defence mechanisms, such as model fine-tuning or output censorship using LLMs,
have proven to be fallible, as LLMs can still generate problematic responses.
Commonly employed censorship approaches treat the issue as a machine learning
problem and rely on another LM to detect undesirable content in LLM outputs. In
this paper, we present the theoretical limitations of such semantic censorship
approaches. Specifically, we demonstrate that semantic censorship can be
perceived as an undecidable problem, highlighting the inherent challenges in
censorship that arise due to LLMs' programmatic and instruction-following
capabilities. Furthermore, we argue that the challenges extend beyond semantic
censorship, as knowledgeable attackers can reconstruct impermissible outputs
from a collection of permissible ones. As a result, we propose that the problem
of censorship needs to be reevaluated; it should be treated as a security
problem which warrants the adaptation of security-based approaches to mitigate
potential risks.",None,-1
MemGPT: Towards LLMs as Operating Systems,0.397283,"Large language models (LLMs) have revolutionized AI, but are constrained by
limited context windows, hindering their utility in tasks like extended
conversations and document analysis. To enable using context beyond limited
context windows, we propose virtual context management, a technique drawing
inspiration from hierarchical memory systems in traditional operating systems
that provide the appearance of large memory resources through data movement
between fast and slow memory. Using this technique, we introduce MemGPT
(Memory-GPT), a system that intelligently manages different memory tiers in
order to effectively provide extended context within the LLM's limited context
window, and utilizes interrupts to manage control flow between itself and the
user. We evaluate our OS-inspired design in two domains where the limited
context windows of modern LLMs severely handicaps their performance: document
analysis, where MemGPT is able to analyze large documents that far exceed the
underlying LLM's context window, and multi-session chat, where MemGPT can
create conversational agents that remember, reflect, and evolve dynamically
through long-term interactions with their users. We release MemGPT code and
data for our experiments at https://memgpt.ai.",None,-1
Generalization of Fitness Exercise Recognition from Doppler Measurements by Domain-adaption and Few-Shot Learning,0.998015,"In previous works, a mobile application was developed using an unmodified
commercial off-the-shelf smartphone to recognize whole-body exercises. The
working principle was based on the ultrasound Doppler sensing with the device
built-in hardware. Applying such a lab-environment trained model on realistic
application variations causes a significant drop in performance, and thus
decimate its applicability. The reason of the reduced performance can be
manifold. It could be induced by the user, environment, and device variations
in realistic scenarios. Such scenarios are often more complex and diverse,
which can be challenging to anticipate in the initial training data. To study
and overcome this issue, this paper presents a database with controlled and
uncontrolled subsets of fitness exercises. We propose two concepts to utilize
small adaption data to successfully improve model generalization in an
uncontrolled environment, increasing the recognition accuracy by two to six
folds compared to the baseline for different users.",None,-1
Can Large Language Models assist in Hazard Analysis?,0.659057,"Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable
natural language processing and generation capabilities and have been applied
to a variety tasks, such as source code generation. This paper explores the
potential of integrating LLMs in the hazard analysis for safety-critical
systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a
human analyst interacts with an LLM via a context-aware chat session and uses
the responses to support elicitation of possible hazard causes. In this
experiment, we explore CoHA with three increasingly complex versions of a
simple system, using Open AI's ChatGPT service. The quality of ChatGPT's
responses were systematically assessed to determine the feasibility of CoHA
given the current state of LLM technology. The results suggest that LLMs may be
useful for supporting human analysts performing hazard analysis.",None,-1
Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,0.768949,"Recent advances in large language models (LLMs) have shown impressive ability
in biomedical question-answering, but have not been adequately investigated for
more specific biomedical applications. This study investigates the performance
of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical
tasks beyond question-answering. Because no patient data can be passed to the
OpenAI API public interface, we evaluated model performance with over 10000
samples as proxies for two fundamental tasks in the clinical domain -
classification and reasoning. The first task is classifying whether statements
of clinical and policy recommendations in scientific literature constitute
health advice. The second task is causal relation detection from the biomedical
literature. We compared LLMs with simpler models, such as bag-of-words (BoW)
with logistic regression, and fine-tuned BioBERT models. Despite the excitement
around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks
remained the best strategy. The simple BoW model performed on par with the most
complex LLM prompting. Prompt engineering required significant investment.",None,-1
A Probabilistic Rotation Representation for Symmetric Shapes With an Efficiently Computable Bingham Loss Function,0.145137,"In recent years, a deep learning framework has been widely used for object
pose estimation. While quaternion is a common choice for rotation
representation, it cannot represent the ambiguity of the observation. In order
to handle the ambiguity, the Bingham distribution is one promising solution.
However, it requires complicated calculation when yielding the negative
log-likelihood (NLL) loss. An alternative easy-to-implement loss function has
been proposed to avoid complex computations but has difficulty expressing
symmetric distribution. In this paper, we introduce a fast-computable and
easy-to-implement NLL loss function for Bingham distribution. We also create
the inference network and show that our loss function can capture the symmetric
property of target objects from their point clouds.",None,-1
Back Translation for Speech-to-text Translation Without Transcripts,0.896315,"The success of end-to-end speech-to-text translation (ST) is often achieved
by utilizing source transcripts, e.g., by pre-training with automatic speech
recognition (ASR) and machine translation (MT) tasks, or by introducing
additional ASR and MT data. Unfortunately, transcripts are only sometimes
available since numerous unwritten languages exist worldwide. In this paper, we
aim to utilize large amounts of target-side monolingual data to enhance ST
without transcripts. Motivated by the remarkable success of back translation in
MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo
ST data from monolingual target data. To ease the challenges posed by
short-to-long generation and one-to-many mapping, we introduce self-supervised
discrete units and achieve back translation by cascading a target-to-unit model
and a unit-to-speech model. With our synthetic ST data, we achieve an average
boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments
show that our method is especially effective in low-resource scenarios.",None,-1
Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation,0.127334,"We study the effect of tokenization on gender bias in machine translation, an
aspect that has been largely overlooked in previous works. Specifically, we
focus on the interactions between the frequency of gendered profession names in
training data, their representation in the subword tokenizer's vocabulary, and
gender bias. We observe that female and non-stereotypical gender inflections of
profession names (e.g., Spanish ""doctora"" for ""female doctor"") tend to be split
into multiple subword tokens. Our results indicate that the imbalance of gender
forms in the model's training corpus is a major factor contributing to gender
bias and has a greater impact than subword splitting. We show that analyzing
subword splits provides good estimates of gender-form imbalance in the training
data and can be used even when the corpus is not publicly available. We also
demonstrate that fine-tuning just the token embedding layer can decrease the
gap in gender prediction accuracy between female and male forms without
impairing the translation quality.",None,-1
DeePoint: Visual Pointing Recognition and Direction Estimation,0.067678,"In this paper, we realize automatic visual recognition and direction
estimation of pointing. We introduce the first neural pointing understanding
method based on two key contributions. The first is the introduction of a
first-of-its-kind large-scale dataset for pointing recognition and direction
estimation, which we refer to as the DP Dataset. DP Dataset consists of more
than 2 million frames of 33 people pointing in various styles annotated for
each frame with pointing timings and 3D directions. The second is DeePoint, a
novel deep network model for joint recognition and 3D direction estimation of
pointing. DeePoint is a Transformer-based network which fully leverages the
spatio-temporal coordination of the body parts, not just the hands. Through
extensive experiments, we demonstrate the accuracy and efficiency of DeePoint.
We believe DP Dataset and DeePoint will serve as a sound foundation for visual
human intention understanding.",None,-1
Video-Mined Task Graphs for Keystep Recognition in Instructional Videos,0.390777,"Procedural activity understanding requires perceiving human actions in terms
of a broader task, where multiple keysteps are performed in sequence across a
long video to reach a final goal state -- such as the steps of a recipe or a
DIY fix-it task. Prior work largely treats keystep recognition in isolation of
this broader structure, or else rigidly confines keysteps to align with a
predefined sequential script. We propose discovering a task graph automatically
from how-to videos to represent probabilistically how people tend to execute
keysteps, and then leverage this graph to regularize keystep recognition in
novel videos. On multiple datasets of real-world instructional videos, we show
the impact: more reliable zero-shot keystep localization and improved video
representation learning, exceeding the state of the art.",None,-1
Gradient Domain Diffusion Models for Image Synthesis,0.119047,"Diffusion models are getting popular in generative image and video synthesis.
However, due to the diffusion process, they require a large number of steps to
converge. To tackle this issue, in this paper, we propose to perform the
diffusion process in the gradient domain, where the convergence becomes faster.
There are two reasons. First, thanks to the Poisson equation, the gradient
domain is mathematically equivalent to the original image domain. Therefore,
each diffusion step in the image domain has a unique corresponding gradient
domain representation. Second, the gradient domain is much sparser than the
image domain. As a result, gradient domain diffusion models converge faster.
Several numerical experiments confirm that the gradient domain diffusion models
are more efficient than the original diffusion models. The proposed method can
be applied in a wide range of applications such as image processing, computer
vision and machine learning tasks.",None,-1
Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets,0.105895,"Despite incredible advances, deep learning has been shown to be susceptible
to adversarial attacks. Numerous approaches have been proposed to train robust
networks both empirically and certifiably. However, most of them defend against
only a single type of attack, while recent work takes steps forward in
defending against multiple attacks. In this paper, to understand multi-target
robustness, we view this problem as a bargaining game in which different
players (adversaries) negotiate to reach an agreement on a joint direction of
parameter updating. We identify a phenomenon named player domination in the
bargaining game, namely that the existing max-based approaches, such as MAX and
MSD, do not converge. Based on our theoretical analysis, we design a novel
framework that adjusts the budgets of different adversaries to avoid any player
dominance. Experiments on standard benchmarks show that employing the proposed
framework to the existing approaches significantly advances multi-target
robustness.",None,-1
Revisiting Rotation Averaging: Uncertainties and Robust Losses,0.284427,"In this paper, we revisit the rotation averaging problem applied in global
Structure-from-Motion pipelines. We argue that the main problem of current
methods is the minimized cost function that is only weakly connected with the
input data via the estimated epipolar geometries.We propose to better model the
underlying noise distributions by directly propagating the uncertainty from the
point correspondences into the rotation averaging. Such uncertainties are
obtained for free by considering the Jacobians of two-view refinements.
Moreover, we explore integrating a variant of the MAGSAC loss into the rotation
averaging problem, instead of using classical robust losses employed in current
frameworks. The proposed method leads to results superior to baselines, in
terms of accuracy, on large-scale public benchmarks. The code is public.
https://github.com/zhangganlin/GlobalSfMpy",None,-1
CAROM Air -- Vehicle Localization and Traffic Scene Reconstruction from Aerial Videos,0.601242,"Road traffic scene reconstruction from videos has been desirable by road
safety regulators, city planners, researchers, and autonomous driving
technology developers. However, it is expensive and unnecessary to cover every
mile of the road with cameras mounted on the road infrastructure. This paper
presents a method that can process aerial videos to vehicle trajectory data so
that a traffic scene can be automatically reconstructed and accurately
re-simulated using computers. On average, the vehicle localization error is
about 0.1 m to 0.3 m using a consumer-grade drone flying at 120 meters. This
project also compiles a dataset of 50 reconstructed road traffic scenes from
about 100 hours of aerial videos to enable various downstream traffic analysis
applications and facilitate further road traffic related research. The dataset
is available at https://github.com/duolu/CAROM.",None,-1
Progressively Optimized Local Radiance Fields for Robust View Synthesis,0.852064,"We present an algorithm for reconstructing the radiance field of a
large-scale scene from a single casually captured video. The task poses two
core challenges. First, most existing radiance field reconstruction approaches
rely on accurate pre-estimated camera poses from Structure-from-Motion
algorithms, which frequently fail on in-the-wild videos. Second, using a
single, global radiance field with finite representational capacity does not
scale to longer trajectories in an unbounded scene. For handling unknown poses,
we jointly estimate the camera poses with radiance field in a progressive
manner. We show that progressive optimization significantly improves the
robustness of the reconstruction. For handling large unbounded scenes, we
dynamically allocate new local radiance fields trained with frames within a
temporal window. This further improves robustness (e.g., performs well even
under moderate pose drifts) and allows us to scale to large scenes. Our
extensive evaluation on the Tanks and Temples dataset and our collected outdoor
dataset, Static Hikes, show that our approach compares favorably with the
state-of-the-art.",None,-1
Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities,0.0549248,"This study critically evaluates the efficacy of prompting methods in
enhancing the mathematical reasoning capability of large language models
(LLMs). The investigation uses three prescriptive prompting methods - simple,
persona, and conversational prompting - known for their effectiveness in
enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's
LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and
MMLU datasets, encompassing a broad spectrum of mathematical challenges. A
grading script adapted to each dataset is used to determine the effectiveness
of these prompting interventions in enhancing the model's mathematical analysis
power. Contrary to expectations, our empirical analysis reveals that none of
the investigated methods consistently improves over ChatGPT-3.5's baseline
performance, with some causing significant degradation. Our findings suggest
that prompting strategies do not necessarily generalize to new domains, in this
study failing to enhance mathematical performance.",None,-1
MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions,0.684426,"Audio-driven portrait animation aims to synthesize portrait videos that are
conditioned by given audio. Animating high-fidelity and multimodal video
portraits has a variety of applications. Previous methods have attempted to
capture different motion modes and generate high-fidelity portrait videos by
training different models or sampling signals from given videos. However,
lacking correlation learning between lip-sync and other movements (e.g., head
pose/eye blinking) usually leads to unnatural results. In this paper, we
propose a unified system for multi-person, diverse, and high-fidelity talking
portrait generation. Our method contains three stages, i.e., 1) Mapping-Once
network with Dual Attentions (MODA) generates talking representation from given
audio. In MODA, we design a dual-attention module to encode accurate mouth
movements and diverse modalities. 2) Facial composer network generates dense
and detailed face landmarks, and 3) temporal-guided renderer syntheses stable
videos. Extensive evaluations demonstrate that the proposed system produces
more natural and realistic video portraits compared to previous methods.",None,-1
Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,0.926908,"Most unsupervised image anomaly localization methods suffer from
overgeneralization because of the high generalization abilities of
convolutional neural networks, leading to unreliable predictions. To mitigate
the overgeneralization, this study proposes to collaboratively optimize normal
and abnormal feature distributions with the assistance of synthetic anomalies,
namely collaborative discrepancy optimization (CDO). CDO introduces a margin
optimization module and an overlap optimization module to optimize the two key
factors determining the localization performance, i.e., the margin and the
overlap between the discrepancy distributions (DDs) of normal and abnormal
samples. With CDO, a large margin and a small overlap between normal and
abnormal DDs are obtained, and the prediction reliability is boosted.
Experiments on MVTec2D and MVTec3D show that CDO effectively mitigates the
overgeneralization and achieves great anomaly localization performance with
real-time computation efficiency. A real-world automotive plastic parts
inspection application further demonstrates the capability of the proposed CDO.
Code is available on https://github.com/caoyunkang/CDO.",None,-1
How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases,0.870064,"Accurate syntactic representations are essential for robust generalization in
natural language. Recent work has found that pre-training can teach language
models to rely on hierarchical syntactic features - as opposed to incorrect
linear features - when performing tasks after fine-tuning. We test what aspects
of pre-training are important for endowing encoder-decoder Transformers with an
inductive bias that favors hierarchical syntactic generalizations. We focus on
architectural features (depth, width, and number of parameters), as well as the
genre and size of the pre-training corpus, diagnosing inductive biases using
two syntactic transformation tasks: question formation and passivization, both
in English. We find that the number of parameters alone does not explain
hierarchical generalization: model depth plays greater role than model width.
We also find that pre-training on simpler language, such as child-directed
speech, induces a hierarchical bias using an order-of-magnitude less data than
pre-training on more typical datasets based on web text or Wikipedia; this
suggests that in cognitively plausible language acquisition settings, neural
language models may be more data-efficient than previously thought.",None,-1
Learning Language-Specific Layers for Multilingual Machine Translation,0.721236,"Multilingual Machine Translation promises to improve translation quality
between non-English languages. This is advantageous for several reasons, namely
lower latency (no need to translate twice), and reduced error cascades (e.g.,
avoiding losing gender and formality information when translating through
English). On the downside, adding more languages reduces model capacity per
language, which is usually countered by increasing the overall model size,
making training harder and inference slower. In this work, we introduce
Language-Specific Transformer Layers (LSLs), which allow us to increase model
capacity, while keeping the amount of computation and the number of parameters
used in the forward pass constant. The key idea is to have some layers of the
encoder be source or target language-specific, while keeping the remaining
layers shared. We study the best way to place these layers using a neural
architecture search inspired approach, and achieve an improvement of 1.3 chrF
(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and
1.9 chrF (2.2 spBLEU) on a shared decoder one.",None,-1
Exploring Large-scale Unlabeled Faces to Enhance Facial Expression Recognition,0.770541,"Facial Expression Recognition (FER) is an important task in computer vision
and has wide applications in human-computer interaction, intelligent security,
emotion analysis, and other fields. However, the limited size of FER datasets
limits the generalization ability of expression recognition models, resulting
in ineffective model performance. To address this problem, we propose a
semi-supervised learning framework that utilizes unlabeled face data to train
expression recognition models effectively. Our method uses a dynamic threshold
module (\textbf{DTM}) that can adaptively adjust the confidence threshold to
fully utilize the face recognition (FR) data to generate pseudo-labels, thus
improving the model's ability to model facial expressions. In the ABAW5 EXPR
task, our method achieved excellent results on the official validation set.",None,-1
Is More Always Better? The Effects of Personal Characteristics and Level of Detail on the Perception of Explanations in a Recommender System,0.879326,"Despite the acknowledgment that the perception of explanations may vary
considerably between end-users, explainable recommender systems (RS) have
traditionally followed a one-size-fits-all model, whereby the same explanation
level of detail is provided to each user, without taking into consideration
individual user's context, i.e., goals and personal characteristics. To fill
this research gap, we aim in this paper at a shift from a one-size-fits-all to
a personalized approach to explainable recommendation by giving users agency in
deciding which explanation they would like to see. We developed a transparent
Recommendation and Interest Modeling Application (RIMA) that provides on-demand
personalized explanations of the recommendations, with three levels of detail
(basic, intermediate, advanced) to meet the demands of different types of
end-users. We conducted a within-subject study (N=31) to investigate the
relationship between user's personal characteristics and the explanation level
of detail, and the effects of these two variables on the perception of the
explainable RS with regard to different explanation goals. Our results show
that the perception of explainable RS with different levels of detail is
affected to different degrees by the explanation goal and user type.
Consequently, we suggested some theoretical and design guidelines to support
the systematic design of explanatory interfaces in RS tailored to the user's
context.",None,-1
GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,0.572134,"Current dataset collection methods typically scrape large amounts of data
from the web. While this technique is extremely scalable, data collected in
this way tends to reinforce stereotypical biases, can contain personally
identifiable information, and typically originates from Europe and North
America. In this work, we rethink the dataset collection paradigm and introduce
GeoDE, a geographically diverse dataset with 61,940 images from 40 classes and
6 world regions, and no personally identifiable information, collected through
crowd-sourcing. We analyse GeoDE to understand differences in images collected
in this manner compared to web-scraping. Despite the smaller size of this
dataset, we demonstrate its use as both an evaluation and training dataset,
highlight shortcomings in current models, as well as show improved performances
when even small amounts of GeoDE (1000 - 2000 images per region) are added to a
training dataset. We release the full dataset and code at
https://geodiverse-data-collection.cs.princeton.edu/",None,-1
"Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy",0.135951,"How does one measure ""ability to understand language""? If it is a person's
ability that is being measured, this is a question that almost never poses
itself in an unqualified manner: Whatever formal test is applied, it takes
place on the background of the person's language use in daily social practice,
and what is measured is a specialised variety of language understanding (e.g.,
of a second language; or of written, technical language). Computer programs do
not have this background. What does that mean for the applicability of formal
tests of language understanding? I argue that such tests need to be
complemented with tests of language use embedded in a practice, to arrive at a
more comprehensive evaluation of ""artificial language understanding"". To do
such tests systematically, I propose to use ""Dialogue Games"" -- constructed
activities that provide a situational embedding for language use. I describe a
taxonomy of Dialogue Game types, linked to a model of underlying capabilites
that are tested, and thereby giving an argument for the \emph{construct
validity} of the test. I close with showing how the internal structure of the
taxonomy suggests an ordering from more specialised to more general situational
language understanding, which potentially can provide some strategic guidance
for development in this field.",None,-1
Beyond AUROC & co. for evaluating out-of-distribution detection performance,0.496121,"While there has been a growing research interest in developing
out-of-distribution (OOD) detection methods, there has been comparably little
discussion around how these methods should be evaluated. Given their relevance
for safe(r) AI, it is important to examine whether the basis for comparing OOD
detection methods is consistent with practical needs. In this work, we take a
closer look at the go-to metrics for evaluating OOD detection, and question the
approach of exclusively reducing OOD detection to a binary classification task
with little consideration for the detection threshold. We illustrate the
limitations of current metrics (AUROC & its friends) and propose a new metric -
Area Under the Threshold Curve (AUTC), which explicitly penalizes poor
separation between ID and OOD samples. Scripts and data are available at
https://github.com/glhr/beyond-auroc",None,-1
Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification,0.12137,"Randomly masking text spans in ordinary texts in the pre-training stage
hardly allows models to acquire the ability to generate simple texts. It can
hurt the performance of pre-trained models on text simplification tasks. In
this paper, we propose a new continued pre-training strategy to teach the
pre-trained model to generate simple texts. We continue pre-training BART, a
representative model, to obtain SimpleBART. It consistently and significantly
improves the results on lexical simplification, sentence simplification, and
document-level simplification tasks over BART. At the end, we compare
SimpleBART with several representative large language models (LLMs).",None,-1
Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization,0.442505,"This work investigates the effectiveness of different pseudonymization
techniques, ranging from rule-based substitutions to using pre-trained Large
Language Models (LLMs), on a variety of datasets and models used for two widely
used NLP tasks: text classification and summarization. Our work provides
crucial insights into the gaps between original and anonymized data (focusing
on the pseudonymization technique) and model quality and fosters future
research into higher-quality anonymization techniques to better balance the
trade-offs between data protection and utility preservation. We make our code,
pseudonymized datasets, and downstream models publicly available",None,-1
Is In-hospital Meta-information Useful for Abstractive Discharge Summary Generation?,0.281964,"During the patient's hospitalization, the physician must record daily
observations of the patient and summarize them into a brief document called
""discharge summary"" when the patient is discharged. Automated generation of
discharge summary can greatly relieve the physicians' burden, and has been
addressed recently in the research community. Most previous studies of
discharge summary generation using the sequence-to-sequence architecture focus
on only inpatient notes for input. However, electric health records (EHR) also
have rich structured metadata (e.g., hospital, physician, disease, length of
stay, etc.) that might be useful. This paper investigates the effectiveness of
medical meta-information for summarization tasks. We obtain four types of
meta-information from the EHR systems and encode each meta-information into a
sequence-to-sequence model. Using Japanese EHRs, meta-information encoded
models increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over
the vanilla Longformer. Also, we found that the encoded meta-information
improves the precisions of its related terms in the outputs. Our results showed
the benefit of the use of medical meta-information.",None,-1
Task-Driven Graph Attention for Hierarchical Relational Object Navigation,0.689056,"Embodied AI agents in large scenes often need to navigate to find objects. In
this work, we study a naturally emerging variant of the object navigation task,
hierarchical relational object navigation (HRON), where the goal is to find
objects specified by logical predicates organized in a hierarchical structure -
objects related to furniture and then to rooms - such as finding an apple on
top of a table in the kitchen. Solving such a task requires an efficient
representation to reason about object relations and correlate the relations in
the environment and in the task goal. HRON in large scenes (e.g. homes) is
particularly challenging due to its partial observability and long horizon,
which invites solutions that can compactly store the past information while
effectively exploring the scene. We demonstrate experimentally that scene
graphs are the best-suited representation compared to conventional
representations such as images or 2D maps. We propose a solution that uses
scene graphs as part of its input and integrates graph neural networks as its
backbone, with an integrated task-driven attention mechanism, and demonstrate
its better scalability and learning efficiency than state-of-the-art baselines.",None,-1
Diacritic Recognition Performance in Arabic ASR,0.457022,"We present an analysis of diacritic recognition performance in Arabic
Automatic Speech Recognition (ASR) systems. As most existing Arabic speech
corpora do not contain all diacritical marks, which represent short vowels and
other phonetic information in Arabic script, current state-of-the-art ASR
models do not produce full diacritization in their output. Automatic text-based
diacritization has previously been employed both as a pre-processing step to
train diacritized ASR, or as a post-processing step to diacritize the resulting
ASR hypotheses. It is generally believed that input diacritization degrades ASR
performance, but no systematic evaluation of ASR diacritization performance,
independent of ASR performance, has been conducted to date. In this paper, we
attempt to experimentally clarify whether input diacritiztation indeed degrades
ASR quality, and to compare the diacritic recognition performance against
text-based diacritization as a post-processing step. We start with pre-trained
Arabic ASR models and fine-tune them on transcribed speech data with different
diacritization conditions: manual, automatic, and no diacritization. We isolate
diacritic recognition performance from the overall ASR performance using
coverage and precision metrics. We find that ASR diacritization significantly
outperforms text-based diacritization in post-processing, particularly when the
ASR model is fine-tuned with manually diacritized transcripts.",None,-1
DynaShare: Task and Instance Conditioned Parameter Sharing for Multi-Task Learning,0.0458591,"Multi-task networks rely on effective parameter sharing to achieve robust
generalization across tasks. In this paper, we present a novel parameter
sharing method for multi-task learning that conditions parameter sharing on
both the task and the intermediate feature representations at inference time.
In contrast to traditional parameter sharing approaches, which fix or learn a
deterministic sharing pattern during training and apply the same pattern to all
examples during inference, we propose to dynamically decide which parts of the
network to activate based on both the task and the input instance. Our approach
learns a hierarchical gating policy consisting of a task-specific policy for
coarse layer selection and gating units for individual input instances, which
work together to determine the execution path at inference time. Experiments on
the NYU v2, Cityscapes and MIMIC-III datasets demonstrate the potential of the
proposed approach and its applicability across problem domains.",None,-1
Knowledge Graph Embedding with 3D Compound Geometric Transformations,0.428263,"The cascade of 2D geometric transformations were exploited to model relations
between entities in a knowledge graph (KG), leading to an effective KG
embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was
proposed as a new KGE model, Rotate3D, by leveraging its non-commutative
property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric
transformations, including translation, rotation, scaling, reflection, and
shear and propose a family of KGE models, named CompoundE3D, in this work.
CompoundE3D allows multiple design variants to match rich underlying
characteristics of a KG. Since each variant has its own advantages on a subset
of relations, an ensemble of multiple variants can yield superior performance.
The effectiveness and flexibility of CompoundE3D are experimentally verified on
four popular link prediction datasets.",None,-1
Mark My Words: Dangers of Watermarked Images in ImageNet,0.0686769,"The utilization of pre-trained networks, especially those trained on
ImageNet, has become a common practice in Computer Vision. However, prior
research has indicated that a significant number of images in the ImageNet
dataset contain watermarks, making pre-trained networks susceptible to learning
artifacts such as watermark patterns within their latent spaces. In this paper,
we aim to assess the extent to which popular pre-trained architectures display
such behavior and to determine which classes are most affected. Additionally,
we examine the impact of watermarks on the extracted features. Contrary to the
popular belief that the Chinese logographic watermarks impact the ""carton""
class only, our analysis reveals that a variety of ImageNet classes, such as
""monitor"", ""broom"", ""apron"" and ""safe"" rely on spurious correlations. Finally,
we propose a simple approach to mitigate this issue in fine-tuned networks by
ignoring the encodings from the feature-extractor layer of ImageNet pre-trained
networks that are most susceptible to watermark imprints.",None,-1
Egocentric Audio-Visual Object Localization,0.878794,"Humans naturally perceive surrounding scenes by unifying sound and sight in a
first-person view. Likewise, machines are advanced to approach human
intelligence by learning with multisensory inputs from an egocentric
perspective. In this paper, we explore the challenging egocentric audio-visual
object localization task and observe that 1) egomotion commonly exists in
first-person recordings, even within a short duration; 2) The out-of-view sound
components can be created while wearers shift their attention. To address the
first problem, we propose a geometry-aware temporal aggregation module to
handle the egomotion explicitly. The effect of egomotion is mitigated by
estimating the temporal geometry transformation and exploiting it to update
visual representations. Moreover, we propose a cascaded feature enhancement
module to tackle the second issue. It improves cross-modal localization
robustness by disentangling visually-indicated audio representation. During
training, we take advantage of the naturally available audio-visual temporal
synchronization as the ``free'' self-supervision to avoid costly labeling. We
also annotate and create the Epic Sounding Object dataset for evaluation
purposes. Extensive experiments show that our method achieves state-of-the-art
localization performance in egocentric videos and can be generalized to diverse
audio-visual scenes.",None,-1
Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation,0.388172,"Using a vocabulary that is shared across languages is common practice in
Multilingual Neural Machine Translation (MNMT). In addition to its simple
design, shared tokens play an important role in positive knowledge transfer,
assuming that shared tokens refer to similar meanings across languages.
However, when word overlap is small, especially due to different writing
systems, transfer is inhibited. In this paper, we define word-level information
transfer pathways via word equivalence classes and rely on graph networks to
fuse word embeddings across languages. Our experiments demonstrate the
advantages of our approach: 1) embeddings of words with similar meanings are
better aligned across languages, 2) our method achieves consistent BLEU
improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less
than 1.0\% additional trainable parameters are required with a limited increase
in computational costs, while inference time remains identical to the baseline.
We release the codebase to the community.",None,-1
Rationale-Enhanced Language Models are Better Continual Relation Learners,0.926837,"Continual relation extraction (CRE) aims to solve the problem of catastrophic
forgetting when learning a sequence of newly emerging relations. Recent CRE
studies have found that catastrophic forgetting arises from the model's lack of
robustness against future analogous relations. To address the issue, we
introduce rationale, i.e., the explanations of relation classification results
generated by large language models (LLM), into CRE task. Specifically, we
design the multi-task rationale tuning strategy to help the model learn current
relations robustly. We also conduct contrastive rationale replay to further
distinguish analogous relations. Experimental results on two standard
benchmarks demonstrate that our method outperforms the state-of-the-art CRE
models.",None,-1
"Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection",0.748567,"This paper aims for high-performance offline LiDAR-based 3D object detection.
We first observe that experienced human annotators annotate objects from a
track-centric perspective. They first label the objects with clear shapes in a
track, and then leverage the temporal coherence to infer the annotations of
obscure objects. Drawing inspiration from this, we propose a high-performance
offline detector in a track-centric perspective instead of the conventional
object-centric perspective. Our method features a bidirectional tracking module
and a track-centric learning module. Such a design allows our detector to infer
and refine a complete track once the object is detected at a certain moment. We
refer to this characteristic as ""onCe detecTed, neveR Lost"" and name the
proposed system CTRL. Extensive experiments demonstrate the remarkable
performance of our method, surpassing the human-level annotating accuracy and
the previous state-of-the-art methods in the highly competitive Waymo Open
Dataset without model ensemble. The code will be made publicly available at
https://github.com/tusen-ai/SST.",None,-1
A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network,0.583634,"Aiming at the prediction problem of transport capacity risk caused by the
mismatch between the carrying capacity of rail transit network and passenger
flow demand, this paper proposes an explainable prediction method of rail
transit network transport capacity risk based on linear Gaussian Bayesian
network. This method obtains the training data of the prediction model based on
the simulation model of the rail transit system with a three-layer structure
including rail transit network, train flow and passenger flow. A Bayesian
network structure construction method based on the topology of the rail transit
network is proposed, and the MLE (Maximum Likelihood Estimation) method is used
to realize the parameter learning of the Bayesian network. Finally, the
effectiveness of the proposed method is verified by simulation examples.",None,-1
Reconstructing Animatable Categories from Videos,0.911421,"Building animatable 3D models is challenging due to the need for 3D scans,
laborious registration, and manual rigging, which are difficult to scale to
arbitrary categories. Recently, differentiable rendering provides a pathway to
obtain high-quality 3D models from monocular videos, but these are limited to
rigid categories or single instances. We present RAC that builds category 3D
models from monocular videos while disentangling variations over instances and
motion over time. Three key ideas are introduced to solve this problem: (1)
specializing a skeleton to instances via optimization, (2) a method for latent
space regularization that encourages shared structure across a category while
maintaining instance details, and (3) using 3D background models to disentangle
objects from the background. We show that 3D models of humans, cats, and dogs
can be learned from 50-100 internet videos.",None,-1
Towards Self-Explainability of Deep Neural Networks with Heatmap Captioning and Large-Language Models,0.506483,"Heatmaps are widely used to interpret deep neural networks, particularly for
computer vision tasks, and the heatmap-based explainable AI (XAI) techniques
are a well-researched topic. However, most studies concentrate on enhancing the
quality of the generated heatmap or discovering alternate heatmap generation
techniques, and little effort has been devoted to making heatmap-based XAI
automatic, interactive, scalable, and accessible. To address this gap, we
propose a framework that includes two modules: (1) context modelling and (2)
reasoning. We proposed a template-based image captioning approach for context
modelling to create text-based contextual information from the heatmap and
input data. The reasoning module leverages a large language model to provide
explanations in combination with specialised knowledge. Our qualitative
experiments demonstrate the effectiveness of our framework and heatmap
captioning approach. The code for the proposed template-based heatmap
captioning approach will be publicly available.",None,-1
Herb-Drug Interactions: A Holistic Decision Support System in Healthcare,0.471469,"Complementary and alternative medicine are commonly used concomitantly with
conventional medications leading to adverse drug reactions and even fatality in
some cases. Furthermore, the vast possibility of herb-drug interactions
prevents health professionals from remembering or manually searching them in a
database. Decision support systems are a powerful tool that can be used to
assist clinicians in making diagnostic and therapeutic decisions in patient
care. Therefore, an original and hybrid decision support system was designed to
identify herb-drug interactions, applying artificial intelligence techniques to
identify new possible interactions. Different machine learning models will be
used to strengthen the typical rules engine used in these cases. Thus, using
the proposed system, the pharmacy community, people's first line of contact
within the Healthcare System, will be able to make better and more accurate
therapeutic decisions and mitigate possible adverse events.",None,-1
MDP: A Generalized Framework for Text-Guided Image Editing by Manipulating the Diffusion Path,0.362531,"Image generation using diffusion can be controlled in multiple ways. In this
paper, we systematically analyze the equations of modern generative diffusion
networks to propose a framework, called MDP, that explains the design space of
suitable manipulations. We identify 5 different manipulations, including
intermediate latent, conditional embedding, cross attention maps, guidance, and
predicted noise. We analyze the corresponding parameters of these manipulations
and the manipulation schedule. We show that some previous editing methods fit
nicely into our framework. Particularly, we identified one specific
configuration as a new type of control by manipulating the predicted noise,
which can perform higher-quality edits than previous work for a variety of
local and global edits.",None,-1
Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search,0.56061,"Anytime multi-agent path finding (MAPF) is a promising approach to scalable
path optimization in large-scale multi-agent systems. State-of-the-art anytime
MAPF is based on Large Neighborhood Search (LNS), where a fast initial solution
is iteratively optimized by destroying and repairing a fixed number of parts,
i.e., the neighborhood, of the solution, using randomized destroy heuristics
and prioritized planning. Despite their recent success in various MAPF
instances, current LNS-based approaches lack exploration and flexibility due to
greedy optimization with a fixed neighborhood size which can lead to low
quality solutions in general. So far, these limitations have been addressed
with extensive prior effort in tuning or offline machine learning beyond actual
planning. In this paper, we focus on online learning in LNS and propose
Bandit-based Adaptive LArge Neighborhood search Combined with Exploration
(BALANCE). BALANCE uses a bi-level multi-armed bandit scheme to adapt the
selection of destroy heuristics and neighborhood sizes on the fly during
search. We evaluate BALANCE on multiple maps from the MAPF benchmark set and
empirically demonstrate cost improvements of at least 50% compared to
state-of-the-art anytime MAPF in large-scale scenarios. We find that Thompson
Sampling performs particularly well compared to alternative multi-armed bandit
algorithms.",None,-1
PyReason: Software for Open World Temporal Logic,0.520276,"The growing popularity of neuro symbolic reasoning has led to the adoption of
various forms of differentiable (i.e., fuzzy) first order logic. We introduce
PyReason, a software framework based on generalized annotated logic that both
captures the current cohort of differentiable logics and temporal extensions to
support inference over finite periods of time with capabilities for open world
reasoning. Further, PyReason is implemented to directly support reasoning over
graphical structures (e.g., knowledge graphs, social networks, biological
networks, etc.), produces fully explainable traces of inference, and includes
various practical features such as type checking and a memory-efficient
implementation. This paper reviews various extensions of generalized annotated
logic integrated into our implementation, our modern, efficient Python-based
implementation that conducts exact yet scalable deductive inference, and a
suite of experiments. PyReason is available at: github.com/lab-v2/pyreason.",None,-1
Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing,0.379815,"Recently, Neural Radiance Fields (NeRF) has exhibited significant success in
novel view synthesis, surface reconstruction, etc. However, since no physical
reflection is considered in its rendering pipeline, NeRF mistakes the
reflection in the mirror as a separate virtual scene, leading to the inaccurate
reconstruction of the mirror and multi-view inconsistent reflections in the
mirror. In this paper, we present a novel neural rendering framework, named
Mirror-NeRF, which is able to learn accurate geometry and reflection of the
mirror and support various scene manipulation applications with mirrors, such
as adding new objects or mirrors into the scene and synthesizing the
reflections of these new objects in mirrors, controlling mirror roughness, etc.
To achieve this goal, we propose a unified radiance field by introducing the
reflection probability and tracing rays following the light transport model of
Whitted Ray Tracing, and also develop several techniques to facilitate the
learning process. Experiments and comparisons on both synthetic and real
datasets demonstrate the superiority of our method. The code and supplementary
material are available on the project webpage:
https://zju3dv.github.io/Mirror-NeRF/.",None,-1
Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon,0.452928,"Safety-critical cyber-physical systems require control strategies whose
worst-case performance is robust against adversarial disturbances and modeling
uncertainties. In this paper, we present a framework for approximate control
and learning in partially observed systems to minimize the worst-case
discounted cost over an infinite time horizon. We model disturbances to the
system as finite-valued uncertain variables with unknown probability
distributions. For problems with known system dynamics, we construct a dynamic
programming (DP) decomposition to compute the optimal control strategy. Our
first contribution is to define information states that improve the
computational tractability of this DP without loss of optimality. Then, we
describe a simplification for a class of problems where the incurred cost is
observable at each time instance. Our second contribution is defining an
approximate information state that can be constructed or learned directly from
observed data for problems with observable costs. We derive bounds on the
performance loss of the resulting approximate control strategy and illustrate
the effectiveness of our approach in partially observed decision-making
problems with a numerical example.",None,-1
Referring Camouflaged Object Detection,0.286825,"We consider the problem of referring camouflaged object detection (Ref-COD),
a new task that aims to segment specified camouflaged objects based on a small
set of referring images with salient target objects. We first assemble a
large-scale dataset, called R2C7K, which consists of 7K images covering 64
object categories in real-world scenarios. Then, we develop a simple but strong
dual-branch framework, dubbed R2CNet, with a reference branch embedding the
common representations of target objects from referring images and a
segmentation branch identifying and segmenting camouflaged objects under the
guidance of the common representations. In particular, we design a Referring
Mask Generation module to generate pixel-level prior mask and a Referring
Feature Enrichment module to enhance the capability of identifying specified
camouflaged objects. Extensive experiments show the superiority of our Ref-COD
methods over their COD counterparts in segmenting specified camouflaged objects
and identifying the main body of target objects. Our code and dataset are
publicly available at https://github.com/zhangxuying1004/RefCOD.",None,-1
Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior,0.298778,"Video dehazing aims to recover haze-free frames with high visibility and
contrast. This paper presents a novel framework to effectively explore the
physical haze priors and aggregate temporal information. Specifically, we
design a memory-based physical prior guidance module to encode the
prior-related features into long-range memory. Besides, we formulate a
multi-range scene radiance recovery module to capture space-time dependencies
in multiple space-time ranges, which helps to effectively aggregate temporal
information from adjacent frames. Moreover, we construct the first large-scale
outdoor video dehazing benchmark dataset, which contains videos in various
real-world scenarios. Experimental results on both synthetic and real
conditions show the superiority of our proposed method.",None,-1
Two-Stage Learning For the Flexible Job Shop Scheduling Problem,0.432553,"The Flexible Job-shop Scheduling Problem (FJSP) is an important combinatorial
optimization problem that arises in manufacturing and service settings. FJSP is
composed of two subproblems, an assignment problem that assigns tasks to
machines, and a scheduling problem that determines the starting times of tasks
on their chosen machines. Solving FJSP instances of realistic size and
composition is an ongoing challenge even under simplified, deterministic
assumptions. Motivated by the inevitable randomness and uncertainties in supply
chains, manufacturing, and service operations, this paper investigates the
potential of using a deep learning framework to generate fast and accurate
approximations for FJSP. In particular, this paper proposes a two-stage
learning framework 2SLFJSP that explicitly models the hierarchical nature of
FJSP decisions, uses a confidence-aware branching scheme to generate
appropriate instances for the scheduling stage from the assignment predictions
and leverages a novel symmetry-breaking formulation to improve learnability.
2SL-FJSP is evaluated on instances from the FJSP benchmark library. Results
show that 2SL-FJSP can generate high-quality solutions in milliseconds,
outperforming a state-of-the-art reinforcement learning approach recently
proposed in the literature, and other heuristics commonly used in practice.",None,-1
On Formal Feature Attribution and Its Approximation,0.644615,"Recent years have witnessed the widespread use of artificial intelligence
(AI) algorithms and machine learning (ML) models. Despite their tremendous
success, a number of vital problems like ML model brittleness, their fairness,
and the lack of interpretability warrant the need for the active developments
in explainable artificial intelligence (XAI) and formal ML model verification.
The two major lines of work in XAI include feature selection methods, e.g.
Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their
promise, most of the existing feature selection and attribution approaches are
susceptible to a range of critical issues, including explanation unsoundness
and out-of-distribution sampling. A recent formal approach to XAI (FXAI)
although serving as an alternative to the above and free of these issues
suffers from a few other limitations. For instance and besides the scalability
limitation, the formal approach is unable to tackle the feature attribution
problem. Additionally, a formal explanation despite being formally sound is
typically quite large, which hampers its applicability in practical settings.
Motivated by the above, this paper proposes a way to apply the apparatus of
formal XAI to the case of feature attribution based on formal explanation
enumeration. Formal feature attribution (FFA) is argued to be advantageous over
the existing methods, both formal and non-formal. Given the practical
complexity of the problem, the paper then proposes an efficient technique for
approximating exact FFA. Finally, it offers experimental evidence of the
effectiveness of the proposed approximate FFA in comparison to the existing
feature attribution algorithms not only in terms of feature importance and but
also in terms of their relative order.",None,-1
Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) for Automated Structural Condition Assessment in Visual Inspection,0.40397,"Efficiently monitoring the condition of civil infrastructure requires
automating the structural condition assessment in visual inspection. This paper
proposes an Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) for
automatic structural condition assessment in visual bridge inspection.
AECIF-Net can simultaneously parse structural elements and segment surface
defects on the elements in inspection images. It integrates two task-specific
relearning subnets to extract task-specific features from an overall feature
embedding. A co-interactive feature fusion module further captures the spatial
correlation and facilitates information sharing between tasks. Experimental
results demonstrate that the proposed AECIF-Net outperforms the current
state-of-the-art approaches, achieving promising performance with 92.11% mIoU
for element segmentation and 87.16% mIoU for corrosion segmentation on the test
set of the new benchmark dataset Steel Bridge Condition Inspection Visual
(SBCIV). An ablation study verifies the merits of the designs for AECIF-Net,
and a case study demonstrates its capability to automate structural condition
assessment.",None,-1
Generative Novel View Synthesis with 3D-Aware Diffusion Models,0.996175,"We present a diffusion-based model for 3D-aware generative novel view
synthesis from as few as a single input image. Our model samples from the
distribution of possible renderings consistent with the input and, even in the
presence of ambiguity, is capable of rendering diverse and plausible novel
views. To achieve this, our method makes use of existing 2D diffusion backbones
but, crucially, incorporates geometry priors in the form of a 3D feature
volume. This latent feature field captures the distribution over possible scene
representations and improves our method's ability to generate view-consistent
novel renderings. In addition to generating novel views, our method has the
ability to autoregressively synthesize 3D-consistent sequences. We demonstrate
state-of-the-art results on synthetic renderings and room-scale scenes; we also
show compelling results for challenging, real-world objects.",None,-1
AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image,0.75864,"Multiple Instance Learning (MIL), a powerful strategy for weakly supervised
learning, is able to perform various prediction tasks on gigapixel Whole Slide
Images (WSIs). However, the tens of thousands of patches in WSIs usually incur
a vast computational burden for image augmentation, limiting the MIL model's
improvement in performance. Currently, the feature augmentation-based MIL
framework is a promising solution, while existing methods such as Mixup often
produce unrealistic features. To explore a more efficient and practical
augmentation method, we introduce the Diffusion Model (DM) into MIL for the
first time and propose a feature augmentation framework called AugDiff.
Specifically, we employ the generation diversity of DM to improve the quality
of feature augmentation and the step-by-step generation property to control the
retention of semantic information. We conduct extensive experiments over three
distinct cancer datasets, two different feature extractors, and three prevalent
MIL algorithms to evaluate the performance of AugDiff. Ablation study and
visualization further verify the effectiveness. Moreover, we highlight
AugDiff's higher-quality augmented feature over image augmentation and its
superiority over self-supervised learning. The generalization over external
datasets indicates its broader applications.",None,-1
ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and Classification Models,0.368868,"Artificial intelligence (AI) reasoning and explainable AI (XAI) tasks have
gained popularity recently, enabling users to explain the predictions or
decision processes of AI models. This paper introduces Forest Monkey (FM), a
toolkit designed to reason the outputs of any AI-based defect detection and/or
classification model with data explainability. Implemented as a Python package,
FM takes input in the form of dataset folder paths (including original images,
ground truth labels, and predicted labels) and provides a set of charts and a
text file to illustrate the reasoning results and suggest possible
improvements. The FM toolkit consists of processes such as feature extraction
from predictions to reasoning targets, feature extraction from images to defect
characteristics, and a decision tree-based AI-Reasoner. Additionally, this
paper investigates the time performance of the FM toolkit when applied to four
AI models with different datasets. Lastly, a tutorial is provided to guide
users in performing reasoning tasks using the FM toolkit.",None,-1
$$-augmented pregroups and applications to linguistics,0.134798,"We enrich pregroups with a mapping which allows us to locally apply precyclic
permutations to designated substrings. We prove a normalisation theorem for
such algebraic structures and briefly formalise some known applications of
pregroups to the analysis of clitic pronouns in certain natural languages.",None,-1
GROOT: Learning to Follow Instructions by Watching Gameplay Videos,0.57947,"We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.",None,-1
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,0.628843,"Cross-lingual text classification leverages text classifiers trained in a
high-resource language to perform text classification in other languages with
no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,
cross-lingual text classifiers are typically built on large-scale, multilingual
language models (LMs) pretrained on a variety of languages of interest.
However, the performance of these models vary significantly across languages
and classification tasks, suggesting that the superposition of the language
modelling and classification tasks is not always effective. For this reason, in
this paper we propose revisiting the classic ""translate-and-test"" pipeline to
neatly separate the translation and classification stages. The proposed
approach couples 1) a neural machine translator translating from the targeted
language to a high-resource language, with 2) a text classifier trained in the
high-resource language, but the neural machine translator generates ""soft""
translations to permit end-to-end backpropagation during fine-tuning of the
pipeline. Extensive experiments have been carried out over three cross-lingual
text classification datasets (XNLI, MLDoc and MultiEURLEX), with the results
showing that the proposed approach has significantly improved performance over
a competitive baseline.",None,-1
Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?,0.732865,"Much discussion about large language models and language-and-vision models
has focused on whether these models are intelligent agents. We present an
alternative perspective. We argue that these artificial intelligence models are
cultural technologies that enhance cultural transmission in the modern world,
and are efficient imitation engines. We explore what AI models can tell us
about imitation and innovation by evaluating their capacity to design new tools
and discover novel causal structures, and contrast their responses with those
of human children. Our work serves as a first step in determining which
particular representations and competences, as well as which kinds of knowledge
or skill, can be derived from particular learning techniques and data.
Critically, our findings suggest that machines may need more than large scale
language and images to achieve what a child can do.",None,-1
FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models,0.492267,"Large language models (LLMs) have demonstrated exceptional performance in
various natural language processing tasks, yet their efficacy in more
challenging and domain-specific tasks remains largely unexplored. This paper
presents FinEval, a benchmark specifically designed for the financial domain
knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice
questions covering Finance, Economy, Accounting, and Certificate. It includes
4,661 questions spanning 34 different academic subjects. To ensure a
comprehensive model performance evaluation, FinEval employs a range of prompt
types, including zero-shot and few-shot prompts, as well as answer-only and
chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs
on FinEval, the results show that only GPT-4 achieved an accuracy close to 70%
in different prompt settings, indicating significant growth potential for LLMs
in the financial domain knowledge. Our work offers a more comprehensive
financial knowledge evaluation benchmark, utilizing data of mock exams and
covering a wide range of evaluated LLMs.",None,-1
Weight-based Mask for Domain Adaptation,0.331459,"In computer vision, unsupervised domain adaptation (UDA) is an approach to
transferring knowledge from a label-rich source domain to a fully-unlabeled
target domain. Conventional UDA approaches have two problems. The first problem
is that a class classifier can be biased to the source domain because it is
trained using only source samples. The second is that previous approaches align
image-level features regardless of foreground and background, although the
classifier requires foreground features. To solve these problems, we introduce
Weight-based Mask Network (WEMNet) composed of Domain Ignore Module (DIM) and
Semantic Enhancement Module (SEM). DIM obtains domain-agnostic feature
representations via the weight of the domain discriminator and predicts
categories. In addition, SEM obtains class-related feature representations
using the classifier weight and focuses on the foreground features for domain
adaptation. Extensive experimental results reveal that the proposed WEMNet
outperforms the competitive accuracy on representative UDA datasets.",None,-1
Traj-MAE: Masked Autoencoders for Trajectory Prediction,0.864259,"Trajectory prediction has been a crucial task in building a reliable
autonomous driving system by anticipating possible dangers. One key issue is to
generate consistent trajectory predictions without colliding. To overcome the
challenge, we propose an efficient masked autoencoder for trajectory prediction
(Traj-MAE) that better represents the complicated behaviors of agents in the
driving environment. Specifically, our Traj-MAE employs diverse masking
strategies to pre-train the trajectory encoder and map encoder, allowing for
the capture of social and temporal information among agents while leveraging
the effect of environment from multiple granularities. To address the
catastrophic forgetting problem that arises when pre-training the network with
multiple masking strategies, we introduce a continual pre-training framework,
which can help Traj-MAE learn valuable and diverse information from various
strategies efficiently. Our experimental results in both multi-agent and
single-agent settings demonstrate that Traj-MAE achieves competitive results
with state-of-the-art methods and significantly outperforms our baseline model.",None,-1
Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm,0.619823,"Contextual biasing refers to the problem of biasing the automatic speech
recognition (ASR) systems towards rare entities that are relevant to the
specific user or application scenarios. We propose algorithms for contextual
biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During
beam search, we boost the score of a token extension if it extends matching
into a set of biasing phrases. Our method simulates the classical approaches
often implemented in the weighted finite state transducer (WFST) framework, but
avoids the FST language altogether, with careful considerations on memory
footprint and efficiency on tensor processing units (TPUs) by vectorization.
Without introducing additional model parameters, our method achieves
significant word error rate (WER) reductions on biasing test sets by itself,
and yields further performance gain when combined with a model-based biasing
method.",None,-1
Theory of Mind for Multi-Agent Collaboration via Large Language Models,0.855026,"While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.",None,-1
A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases,0.332776,"Enterprise applications of Large Language Models (LLMs) hold promise for
question answering on enterprise SQL databases. However, the extent to which
LLMs can accurately respond to enterprise questions in such databases remains
unclear, given the absence of suitable Text-to-SQL benchmarks tailored to
enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to
enhance LLM-based question answering by providing business context is not well
understood. This study aims to evaluate the accuracy of LLM-powered question
answering systems in the context of enterprise questions and SQL databases,
while also exploring the role of knowledge graphs in improving accuracy. To
achieve this, we introduce a benchmark comprising an enterprise SQL schema in
the insurance domain, a range of enterprise queries encompassing reporting to
metrics, and a contextual layer incorporating an ontology and mappings that
define a knowledge graph. Our primary finding reveals that question answering
using GPT-4, with zero-shot prompts directly on SQL databases, achieves an
accuracy of 16%. Notably, this accuracy increases to 54% when questions are
posed over a Knowledge Graph representation of the enterprise SQL database.
Therefore, investing in Knowledge Graph provides higher accuracy for LLM
powered question answering systems.",None,-1
Ten New Benchmarks for Optimization,0.0381487,"Benchmarks are used for testing new optimization algorithms and their
variants to evaluate their performance. Most existing benchmarks are smooth
functions. This chapter introduces ten new benchmarks with different
properties, including noise, discontinuity, parameter estimation and unknown
paths.",None,-1
DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models,0.115417,"Recent data-driven image colorization methods have enabled automatic or
reference-based colorization, while still suffering from unsatisfactory and
inaccurate object-level color control. To address these issues, we propose a
new method called DiffColor that leverages the power of pre-trained diffusion
models to recover vivid colors conditioned on a prompt text, without any
additional inputs. DiffColor mainly contains two stages: colorization with
generative color prior and in-context controllable colorization. Specifically,
we first fine-tune a pre-trained text-to-image model to generate colorized
images using a CLIP-based contrastive loss. Then we try to obtain an optimized
text embedding aligning the colorized image and the text prompt, and a
fine-tuned diffusion model enabling high-quality image reconstruction. Our
method can produce vivid and diverse colors with a few iterations, and keep the
structure and background intact while having colors well-aligned with the
target language guidance. Moreover, our method allows for in-context
colorization, i.e., producing different colorization results by modifying
prompt texts without any fine-tuning, and can achieve object-level controllable
colorization results. Extensive experiments and user studies demonstrate that
DiffColor outperforms previous works in terms of visual quality, color
fidelity, and diversity of colorization options.",None,-1
Optimization-Inspired Cross-Attention Transformer for Compressive Sensing,0.852979,"By integrating certain optimization solvers with deep neural networks, deep
unfolding network (DUN) with good interpretability and high performance has
attracted growing attention in compressive sensing (CS). However, existing DUNs
often improve the visual quality at the price of a large number of parameters
and have the problem of feature information loss during iteration. In this
paper, we propose an Optimization-inspired Cross-attention Transformer (OCT)
module as an iterative process, leading to a lightweight OCT-based Unfolding
Framework (OCTUF) for image CS. Specifically, we design a novel Dual Cross
Attention (Dual-CA) sub-module, which consists of an Inertia-Supplied Cross
Attention (ISCA) block and a Projection-Guided Cross Attention (PGCA) block.
ISCA block introduces multi-channel inertia forces and increases the memory
effect by a cross attention mechanism between adjacent iterations. And, PGCA
block achieves an enhanced information interaction, which introduces the
inertia force into the gradient descent step through a cross attention block.
Extensive CS experiments manifest that our OCTUF achieves superior performance
compared to state-of-the-art methods while training lower complexity. Codes are
available at https://github.com/songjiechong/OCTUF.",None,-1
ZhichunRoad at Amazon KDD Cup 2022: MultiTask Pre-Training for E-Commerce Product Search,0.320298,"In this paper, we propose a robust multilingual model to improve the quality
of search results. Our model not only leverage the processed class-balanced
dataset, but also benefit from multitask pre-training that leads to more
general representations. In pre-training stage, we adopt mlm task,
classification task and contrastive learning task to achieve considerably
performance. In fine-tuning stage, we use confident learning, exponential
moving average method (EMA), adversarial training (FGM) and regularized dropout
strategy (R-Drop) to improve the model's generalization and robustness.
Moreover, we use a multi-granular semantic unit to discover the queries and
products textual metadata for enhancing the representation of the model. Our
approach obtained competitive results and ranked top-8 in three tasks. We
release the source code and pre-trained models associated with this work.",None,-1
Contrastive Grouping with Transformer for Referring Image Segmentation,0.867403,"Referring image segmentation aims to segment the target referent in an image
conditioning on a natural language expression. Existing one-stage methods
employ per-pixel classification frameworks, which attempt straightforwardly to
align vision and language at the pixel level, thus failing to capture critical
object-level information. In this paper, we propose a mask classification
framework, Contrastive Grouping with Transformer network (CGFormer), which
explicitly captures object-level information via token-based querying and
grouping strategy. Specifically, CGFormer first introduces learnable query
tokens to represent objects and then alternately queries linguistic features
and groups visual features into the query tokens for object-aware cross-modal
reasoning. In addition, CGFormer achieves cross-level interaction by jointly
updating the query tokens and decoding masks in every two consecutive layers.
Finally, CGFormer cooperates contrastive learning to the grouping strategy to
identify the token and its mask corresponding to the referent. Experimental
results demonstrate that CGFormer outperforms state-of-the-art methods in both
segmentation and generalization settings consistently and significantly.",None,-1
In-Context Learning Unlocked for Diffusion Models,0.509422,"We present Prompt Diffusion, a framework for enabling in-context learning in
diffusion-based generative models. Given a pair of task-specific example
images, such as depth from/to image and scribble from/to image, and a text
guidance, our model automatically understands the underlying task and performs
the same task on a new query image following the text guidance. To achieve
this, we propose a vision-language prompt that can model a wide range of
vision-language tasks and a diffusion model that takes it as input. The
diffusion model is trained jointly over six different tasks using these
prompts. The resulting Prompt Diffusion model is the first diffusion-based
vision-language foundation model capable of in-context learning. It
demonstrates high-quality in-context generation on the trained tasks and
generalizes effectively to new, unseen vision tasks with their respective
prompts. Our model also shows compelling text-guided image editing results. Our
framework aims to facilitate research into in-context learning for computer
vision. We share our code and pre-trained models at
https://github.com/Zhendong-Wang/Prompt-Diffusion.",None,-1
Flows: Building Blocks of Reasoning and Collaborating AI,0.807317,"Recent advances in artificial intelligence (AI) have produced highly capable
and controllable systems. This creates unprecedented opportunities for
structured reasoning as well as collaboration among multiple AI systems and
humans. To fully realize this potential, it is essential to develop a
principled way of designing and studying such structured interactions. For this
purpose, we introduce the conceptual framework Flows. Flows are self-contained
building blocks of computation, with an isolated state, communicating through a
standardized message-based interface. This modular design simplifies the
process of creating Flows by allowing them to be recursively composed into
arbitrarily nested interactions and is inherently concurrency-friendly.
Crucially, any interaction can be implemented using this framework, including
prior work on AI-AI and human-AI interactions, prompt engineering schemes, and
tool augmentation. We demonstrate the potential of Flows on competitive coding,
a challenging task on which even GPT-4 struggles. Our results suggest that
structured reasoning and collaboration substantially improve generalization,
with AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in
terms of solve rate. To support rapid and rigorous research, we introduce the
aiFlows library embodying Flows. The aiFlows library is available at
https://github.com/epfl-dlab/aiflows. Data and Flows for reproducing our
experiments are available at https://github.com/epfl-dlab/cc_flows.",None,-1
Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning,0.660441,"The offline reinforcement learning (RL) paradigm provides a general recipe to
convert static behavior datasets into policies that can perform better than the
policy that collected the data. While policy constraints, conservatism, and
other methods for mitigating distributional shifts have made offline
reinforcement learning more effective, the continuous action setting often
necessitates various approximations for applying these techniques. Many of
these challenges are greatly alleviated in discrete action settings, where
offline RL constraints and regularizers can often be computed more precisely or
even exactly. In this paper, we propose an adaptive scheme for action
quantization. We use a VQ-VAE to learn state-conditioned action quantization,
avoiding the exponential blowup that comes with na\""ive discretization of the
action space. We show that several state-of-the-art offline RL methods such as
IQL, CQL, and BRAC improve in performance on benchmarks when combined with our
proposed discretization scheme. We further validate our approach on a set of
challenging long-horizon complex robotic manipulation tasks in the Robomimic
environment, where our discretized offline RL algorithms are able to improve
upon their continuous counterparts by 2-3x. Our project page is at
https://saqrl.github.io/",None,-1
Joint fMRI Decoding and Encoding with Latent Embedding Alignment,0.670227,"The connection between brain activity and corresponding visual stimuli is
crucial in comprehending the human brain. While deep generative models have
exhibited advancement in recovering brain recordings by generating images
conditioned on fMRI signals, accomplishing high-quality generation with
consistent semantics continues to pose challenges. Moreover, the prediction of
brain activity from visual stimuli remains a formidable undertaking. In this
paper, we introduce a unified framework that addresses both fMRI decoding and
encoding. Commencing with the establishment of two latent spaces capable of
representing and reconstructing fMRI signals and visual images, respectively,
we proceed to align the fMRI signals and visual images within the latent space,
thereby enabling a bidirectional transformation between the two domains. Our
Latent Embedding Alignment (LEA) model concurrently recovers visual stimuli
from fMRI signals and predicts brain activity from images within a unified
framework. The performance of LEA surpasses that of existing methods on
multiple benchmark fMRI decoding and encoding datasets. By integrating fMRI
decoding and encoding, LEA offers a comprehensive solution for modeling the
intricate relationship between brain activity and visual stimuli.",None,-1
VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna,0.757909,"Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and
few-shot capabilities in Named Entity Recognition (NER). However, these models
can only be accessed via online APIs, which may cause data leak and
non-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot
NER framework based on the newly released open-source LLM -- Vicuna. VicunaNER
is a two-phase framework, where each phase leverages multi-turn dialogues with
Vicuna to recognize entities from texts. We name the second phase as
Re-Recognition, which recognizes those entities not recognized in the first
phase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues
in each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot
capacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.
Experimental results demonstrate that VicunaNER achieves superior performance
in both shot settings. Additionally, we conduct comprehensive investigations on
Vicuna from multiple perspectives.",None,-1
Evaluating Self-Supervised Speech Representations for Indigenous American Languages,0.618935,"The application of self-supervision to speech representation learning has
garnered significant interest in recent years, due to its scalability to large
amounts of unlabeled data. However, much progress, both in terms of
pre-training and downstream evaluation, has remained concentrated in
monolingual models that only consider English. Few models consider other
languages, and even fewer consider indigenous ones. In our submission to the
New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR
corpus for Quechua, an indigenous South American Language. We benchmark the
efficacy of large SSL models on Quechua, along with 6 other indigenous
languages such as Guarani and Bribri, on low-resource ASR. Our results show
surprisingly strong performance by state-of-the-art SSL models, showing the
potential generalizability of large-scale models to real-world data.",None,-1
LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding,0.616426,"Visually-rich Document Understanding (VrDU) has attracted much research
attention over the past years. Pre-trained models on a large number of document
images with transformer-based backbones have led to significant performance
gains in this field. The major challenge is how to fusion the different
modalities (text, layout, and image) of the documents in a unified model with
different pre-training tasks. This paper focuses on improving text-layout
interactions and proposes a novel multi-modal pre-training model, LayoutMask.
LayoutMask uses local 1D position, instead of global 1D position, as layout
input and has two pre-training objectives: (1) Masked Language Modeling:
predicting masked tokens with two novel masking strategies; (2) Masked Position
Modeling: predicting masked 2D positions to improve layout representation
learning. LayoutMask can enhance the interactions between text and layout
modalities in a unified model and produce adaptive and robust multi-modal
representations for downstream tasks. Experimental results show that our
proposed method can achieve state-of-the-art results on a wide variety of VrDU
problems, including form understanding, receipt understanding, and document
image classification.",None,-1
RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,0.999399,"Large Language Models (LLMs) have greatly advanced code auto-completion
systems, with a potential for substantial productivity enhancements for
developers. However, current benchmarks mainly focus on single-file tasks,
leaving an assessment gap for more complex, real-world, multi-file programming
scenarios. To fill this gap, we introduce RepoBench, a new benchmark
specifically designed for evaluating repository-level code auto-completion
systems. RepoBench supports both Python and Java and consists of three
interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code
Completion), and RepoBench-P (Pipeline). Each task respectively measures the
system's ability to retrieve the most relevant code snippets from other files
as cross-file context, predict the next line of code with cross-file and
in-file context, and handle complex tasks that require a combination of both
retrieval and next-line prediction. RepoBench aims to facilitate a more
complete comparison of performance and encouraging continuous improvement in
auto-completion systems. RepoBench is publicly available at
https://github.com/Leolty/repobench.",None,-1
"NTIRE 2023 Challenge on Light Field Image Super-Resolution: Dataset, Methods and Results",0.99739,"In this report, we summarize the first NTIRE challenge on light field (LF)
image super-resolution (SR), which aims at super-resolving LF images under the
standard bicubic degradation with a magnification factor of 4. This challenge
develops a new LF dataset called NTIRE-2023 for validation and test, and
provides a toolbox called BasicLFSR to facilitate model development. Compared
with single image SR, the major challenge of LF image SR lies in how to exploit
complementary angular information from plenty of views with varying
disparities. In total, 148 participants have registered the challenge, and 11
teams have successfully submitted results with PSNR scores higher than the
baseline method LF-InterNet \cite{LF-InterNet}. These newly developed methods
have set new state-of-the-art in LF image SR, e.g., the winning method achieves
around 1 dB PSNR improvement over the existing state-of-the-art method DistgSSR
\cite{DistgLF}. We report the solutions proposed by the participants, and
summarize their common trends and useful tricks. We hope this challenge can
stimulate future research and inspire new ideas in LF image SR.",None,-1
What happens before and after: Multi-Event Commonsense in Event Coreference Resolution,0.932627,"Event coreference models cluster event mentions pertaining to the same
real-world event. Recent models rely on contextualized representations to
recognize coreference among lexically or contextually similar mentions.
However, models typically fail to leverage commonsense inferences, which is
particularly limiting for resolving lexically-divergent mentions. We propose a
model that extends event mentions with temporal commonsense inferences. Given a
complex sentence with multiple events, e.g., ""The man killed his wife and got
arrested"", with the target event ""arrested"", our model generates plausible
events that happen before the target event - such as ""the police arrived"", and
after it, such as ""he was sentenced"". We show that incorporating such
inferences into an existing event coreference model improves its performance,
and we analyze the coreferences in which such temporal knowledge is required.",None,-1
Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction,0.453106,"The task of multimodal relation extraction has attracted significant research
attention, but progress is constrained by the scarcity of available training
data. One natural thought is to extend existing datasets with cross-modal
generative models. In this paper, we consider a novel problem setting, where
only unimodal data, either text or image, are available during training. We aim
to train a multimodal classifier from synthetic data that perform well on real
multimodal test data. However, training with synthetic data suffers from two
obstacles: lack of data diversity and label information loss. To alleviate the
issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta
GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to
promote diversity in the generated data and exploits a teacher network to
select valuable training samples with high mutual information with the
ground-truth labels. Comparing our method to direct training on synthetic data,
we observed a significant improvement of 24.06% F1 with synthetic text and
26.42% F1 with synthetic images. Notably, our best model trained on completely
synthetic images outperforms prior state-of-the-art models trained on real
multimodal data by a margin of 3.76% in F1. Our codebase will be made available
upon acceptance.",None,-1
NatCS: Eliciting Natural Customer Support Dialogues,0.0424852,"Despite growing interest in applications based on natural customer support
conversations, there exist remarkably few publicly available datasets that
reflect the expected characteristics of conversations in these settings.
Existing task-oriented dialogue datasets, which were collected to benchmark
dialogue systems mainly in written human-to-bot settings, are not
representative of real customer support conversations and do not provide
realistic benchmarks for systems that are applied to natural data. To address
this gap, we introduce NatCS, a multi-domain collection of spoken customer
service conversations. We describe our process for collecting synthetic
conversations between customers and agents based on natural language phenomena
observed in real conversations. Compared to previous dialogue datasets, the
conversations collected with our approach are more representative of real
human-to-human conversations along multiple metrics. Finally, we demonstrate
potential uses of NatCS, including dialogue act classification and intent
induction from conversations as potential applications, showing that dialogue
act annotations in NatCS provide more effective training data for modeling real
conversations compared to existing synthetic written datasets. We publicly
release NatCS to facilitate research in natural dialog systems",None,-1
Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective,0.384789,"This paper introduces a comprehensive, multi-stage machine learning
methodology that effectively integrates information systems and artificial
intelligence to enhance decision-making processes within the domain of
operations research. The proposed framework adeptly addresses common
limitations of existing solutions, such as the neglect of data-driven
estimation for vital production parameters, exclusive generation of point
forecasts without considering model uncertainty, and lacking explanations
regarding the sources of such uncertainty. Our approach employs Quantile
Regression Forests for generating interval predictions, alongside both local
and global variants of SHapley Additive Explanations for the examined
predictive process monitoring problem. The practical applicability of the
proposed methodology is substantiated through a real-world production planning
case study, emphasizing the potential of prescriptive analytics in refining
decision-making procedures. This paper accentuates the imperative of addressing
these challenges to fully harness the extensive and rich data resources
accessible for well-informed decision-making.",None,-1
Half of an image is enough for quality assessment,0.162634,"Deep networks have demonstrated promising results in the field of Image
Quality Assessment (IQA). However, there has been limited research on
understanding how deep models in IQA work. This study introduces a novel
positional masked transformer for IQA and provides insights into the
contribution of different regions of an image towards its overall quality.
Results indicate that half of an image may play a trivial role in determining
image quality, while the other half is critical. This observation is extended
to several other CNN-based IQA models, revealing that half of the image regions
can significantly impact the overall image quality. To further enhance our
understanding, three semantic measures (saliency, frequency, and objectness)
were derived and found to have high correlation with the importance of image
regions in IQA.",None,-1
A novel efficient Multi-view traffic-related object detection framework,0.684739,"With the rapid development of intelligent transportation system applications,
a tremendous amount of multi-view video data has emerged to enhance vehicle
perception. However, performing video analytics efficiently by exploiting the
spatial-temporal redundancy from video data remains challenging. Accordingly,
we propose a novel traffic-related framework named CEVAS to achieve efficient
object detection using multi-view video data. Briefly, a fine-grained input
filtering policy is introduced to produce a reasonable region of interest from
the captured images. Also, we design a sharing object manager to manage the
information of objects with spatial redundancy and share their results with
other vehicles. We further derive a content-aware model selection policy to
select detection methods adaptively. Experimental results show that our
framework significantly reduces response latency while achieving the same
detection accuracy as the state-of-the-art methods.",None,-1
BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation,0.591861,"A novel 4K video frame interpolator based on bilateral transformer (BiFormer)
is proposed in this paper, which performs three steps: global motion
estimation, local motion refinement, and frame synthesis. First, in global
motion estimation, we predict symmetric bilateral motion fields at a coarse
scale. To this end, we propose BiFormer, the first transformer-based bilateral
motion estimator. Second, we refine the global motion fields efficiently using
blockwise bilateral cost volumes (BBCVs). Third, we warp the input frames using
the refined motion fields and blend them to synthesize an intermediate frame.
Extensive experiments demonstrate that the proposed BiFormer algorithm achieves
excellent interpolation performance on 4K datasets. The source codes are
available at https://github.com/JunHeum/BiFormer.",None,-1
MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection,0.0939712,"Existing cross-domain keypoint detection methods always require accessing the
source data during adaptation, which may violate the data privacy law and pose
serious security concerns. Instead, this paper considers a realistic problem
setting called source-free domain adaptive keypoint detection, where only the
well-trained source model is provided to the target domain. For the challenging
problem, we first construct a teacher-student learning baseline by stabilizing
the predictions under data augmentation and network ensembles. Built on this,
we further propose a unified approach, Mixup Augmentation and Progressive
Selection (MAPS), to fully exploit the noisy pseudo labels of unlabeled target
data during training. On the one hand, MAPS regularizes the model to favor
simple linear behavior in-between the target samples via self-mixup
augmentation, preventing the model from over-fitting to noisy predictions. On
the other hand, MAPS employs the self-paced learning paradigm and progressively
selects pseudo-labeled samples from `easy' to `hard' into the training process
to reduce noise accumulation. Results on four keypoint detection datasets show
that MAPS outperforms the baseline and achieves comparable or even better
results in comparison to previous non-source-free counterparts.",None,-1
OMNI: Open-endedness via Models of human Notions of Interestingness,0.426649,"Open-ended algorithms aim to learn new, interesting behaviors forever. That
requires a vast environment search space, but there are thus infinitely many
possible tasks. Even after filtering for tasks the current agent can learn
(i.e., learning progress), countless learnable yet uninteresting tasks remain
(e.g., minor variations of previously learned tasks). An Achilles Heel of
open-endedness research is the inability to quantify (and thus prioritize)
tasks that are not just learnable, but also $\textit{interesting}$ (e.g.,
worthwhile and novel). We propose solving this problem by
$\textit{Open-endedness via Models of human Notions of Interestingness}$
(OMNI). The insight is that we can utilize foundation models (FMs) as a model
of interestingness (MoI), because they $\textit{already}$ internalize human
concepts of interestingness from training on vast amounts of human-generated
data, where humans naturally write about what they find interesting or boring.
We show that FM-based MoIs improve open-ended learning by focusing on tasks
that are both learnable $\textit{and interesting}$, outperforming baselines
based on uniform task sampling or learning progress alone. This approach has
the potential to dramatically advance the ability to intelligently select which
tasks to focus on next (i.e., auto-curricula), and could be seen as AI
selecting its own next task to learn, facilitating self-improving AI and
AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/",None,-1
InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules,0.173946,"Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant
challenge that existing approaches struggle to address without extensive
modifications to vanilla NeRF framework. We introduce InsertNeRF, a method for
INStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play
HyperNet modules, InsertNeRF dynamically tailors NeRF's weights to specific
reference scenes, transforming multi-scale sampling-aware features into
scene-specific representations. This novel design allows for more accurate and
efficient representations of complex appearances and geometries. Experiments
show that this method not only achieves superior generalization performance but
also provides a flexible pathway for integration with other NeRF-like systems,
even in sparse input settings. Code will be available
https://github.com/bbbbby-99/InsertNeRF.",None,-1
Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks,0.79931,"Children possess the ability to learn multiple cognitive tasks sequentially,
which is a major challenge toward the long-term goal of artificial general
intelligence. Existing continual learning frameworks are usually applicable to
Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired,
energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning
mechanisms during child growth and development, we propose Dynamic Structure
Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive
continual learning. When learning a sequence of tasks, the DSD-SNN dynamically
assigns and grows new neurons to new tasks and prunes redundant neurons,
thereby increasing memory capacity and reducing computational overhead. In
addition, the overlapping shared structure helps to quickly leverage all
acquired knowledge to new tasks, empowering a single network capable of
supporting multiple incremental tasks (without the separate sub-network mask
for each task). We validate the effectiveness of the proposed model on multiple
class incremental learning and task incremental learning benchmarks. Extensive
experiments demonstrated that our model could significantly improve
performance, learning speed and memory capacity, and reduce computational
overhead. Besides, our DSD-SNN model achieves comparable performance with the
DNNs-based methods, and significantly outperforms the state-of-the-art (SOTA)
performance for existing SNNs-based continual learning methods.",None,-1
EM Pre-training for Multi-party Dialogue Response Generation,0.851785,"Dialogue response generation requires an agent to generate a response
according to the current dialogue history, in terms of which two-party
dialogues have been well studied, but leaving a great gap for multi-party
dialogues at the same time. Different from two-party dialogues where each
response is a direct reply to its previous utterance, the addressee of a
response utterance should be specified before it is generated in the
multi-party scenario. Thanks to the huge amount of two-party conversational
data, various pre-trained language models for two-party dialogue response
generation have been proposed. However, due to the lack of annotated addressee
labels in multi-party dialogue datasets, it is hard to use them to pre-train a
response generation model for multi-party dialogues. To tackle this obstacle,
we propose an Expectation-Maximization (EM) approach that iteratively performs
the expectation steps to generate addressee labels, and the maximization steps
to optimize a response generation model. Theoretical analyses and extensive
experiments have justified the feasibility and effectiveness of our proposed
method.",None,-1
Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics,0.586299,"Modern neural-network-based no-reference image- and video-quality metrics
exhibit performance as high as full-reference metrics. These metrics are widely
used to improve visual quality in computer vision methods and compare video
processing methods. However, these metrics are not stable to traditional
adversarial attacks, which can cause incorrect results. Our goal is to
investigate the boundaries of no-reference metrics applicability, and in this
paper, we propose a fast adversarial perturbation attack on no-reference
quality metrics. The proposed attack (FACPA) can be exploited as a
preprocessing step in real-time video processing and compression algorithms.
This research can yield insights to further aid in designing of stable
neural-network-based no-reference quality metrics.",None,-1
Multi-Label Self-Supervised Learning with Scene Images,0.395866,"Self-supervised learning (SSL) methods targeting scene images have seen a
rapid growth recently, and they mostly rely on either a dedicated dense
matching mechanism or a costly unsupervised object discovery module. This paper
shows that instead of hinging on these strenuous operations, quality image
representations can be learned by treating scene/multi-label image SSL simply
as a multi-label classification problem, which greatly simplifies the learning
framework. Specifically, multiple binary pseudo-labels are assigned for each
input image by comparing its embeddings with those in two dictionaries, and the
network is optimized using the binary cross entropy loss. The proposed method
is named Multi-Label Self-supervised learning (MLS). Visualizations
qualitatively show that clearly the pseudo-labels by MLS can automatically find
semantically similar pseudo-positive pairs across different images to
facilitate contrastive learning. MLS learns high quality representations on
MS-COCO and achieves state-of-the-art results on classification, detection and
segmentation benchmarks. At the same time, MLS is much simpler than existing
methods, making it easier to deploy and for further exploration.",None,-1
Maskomaly:Zero-Shot Mask Anomaly Segmentation,0.245672,"We present a simple and practical framework for anomaly segmentation called
Maskomaly. It builds upon mask-based standard semantic segmentation networks by
adding a simple inference-time post-processing step which leverages the raw
mask outputs of such networks. Maskomaly does not require additional training
and only adds a small computational overhead to inference. Most importantly, it
does not require anomalous data at training. We show top results for our method
on SMIYC, RoadAnomaly, and StreetHazards. On the most central benchmark, SMIYC,
Maskomaly outperforms all directly comparable approaches. Further, we introduce
a novel metric that benefits the development of robust anomaly segmentation
methods and demonstrate its informativeness on RoadAnomaly.",None,-1
Unsupervised Object Localization with Representer Point Selection,0.165752,"We propose a novel unsupervised object localization method that allows us to
explain the predictions of the model by utilizing self-supervised pre-trained
models without additional finetuning. Existing unsupervised and self-supervised
object localization methods often utilize class-agnostic activation maps or
self-similarity maps of a pre-trained model. Although these maps can offer
valuable information for localization, their limited ability to explain how the
model makes predictions remains challenging. In this paper, we propose a simple
yet effective unsupervised object localization method based on representer
point selection, where the predictions of the model can be represented as a
linear combination of representer values of training points. By selecting
representer points, which are the most important examples for the model
predictions, our model can provide insights into how the model predicts the
foreground object by providing relevant examples as well as their importance.
Our method outperforms the state-of-the-art unsupervised and self-supervised
object localization methods on various datasets with significant margins and
even outperforms recent weakly supervised and few-shot methods.",None,-1
VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions,0.378421,"Font design is of vital importance in the digital content design and modern
printing industry. Developing algorithms capable of automatically synthesizing
vector fonts can significantly facilitate the font design process. However,
existing methods mainly concentrate on raster image generation, and only a few
approaches can directly synthesize vector fonts. This paper proposes an
end-to-end trainable method, VecFontSDF, to reconstruct and synthesize
high-quality vector fonts using signed distance functions (SDFs). Specifically,
based on the proposed SDF-based implicit shape representation, VecFontSDF
learns to model each glyph as shape primitives enclosed by several parabolic
curves, which can be precisely converted to quadratic B\'ezier curves that are
widely used in vector font products. In this manner, most image generation
methods can be easily extended to synthesize vector fonts. Qualitative and
quantitative experiments conducted on a publicly-available dataset demonstrate
that our method obtains high-quality results on several tasks, including vector
font reconstruction, interpolation, and few-shot vector font synthesis,
markedly outperforming the state of the art.",None,-1
IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation,0.23959,"Knowledge Graph Embedding (KGE) models are used to learn continuous
representations of entities and relations. A key task in the literature is
predicting missing links between entities. However, Knowledge Graphs are not
just sets of links but also have semantics underlying their structure.
Semantics is crucial in several downstream tasks, such as query answering or
reasoning. We introduce the subgraph inference task, where a model has to
generate likely and semantically valid subgraphs. We propose IntelliGraphs, a
set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain
subgraphs with semantics expressed in logical rules for evaluating subgraph
inference. We also present the dataset generator that produced the synthetic
datasets. We designed four novel baseline models, which include three models
based on traditional KGEs. We evaluate their expressiveness and show that these
models cannot capture the semantics. We believe this benchmark will encourage
the development of machine learning models that emphasize semantic
understanding.",None,-1
SGAligner : 3D Scene Alignment with Scene Graphs,0.580902,"Building 3D scene graphs has recently emerged as a topic in scene
representation for several embodied AI applications to represent the world in a
structured and rich manner. With their increased use in solving downstream
tasks (eg, navigation and room rearrangement), can we leverage and recycle them
for creating 3D maps of environments, a pivotal step in agent operation? We
focus on the fundamental problem of aligning pairs of 3D scene graphs whose
overlap can range from zero to partial and can contain arbitrary changes. We
propose SGAligner, the first method for aligning pairs of 3D scene graphs that
is robust to in-the-wild scenarios (ie, unknown overlap -- if any -- and
changes in the environment). We get inspired by multi-modality knowledge graphs
and use contrastive learning to learn a joint, multi-modal embedding space. We
evaluate on the 3RScan dataset and further showcase that our method can be used
for estimating the transformation between pairs of 3D scenes. Since benchmarks
for these tasks are missing, we create them on this dataset. The code,
benchmark, and trained models are available on the project website.",None,-1
Weakly Supervised Headline Dependency Parsing,0.30715,"English news headlines form a register with unique syntactic properties that
have been documented in linguistics literature since the 1930s. However,
headlines have received surprisingly little attention from the NLP syntactic
parsing community. We aim to bridge this gap by providing the first news
headline corpus of Universal Dependencies annotated syntactic dependency trees,
which enables us to evaluate existing state-of-the-art dependency parsers on
news headlines. To improve English news headline parsing accuracies, we develop
a projection method to bootstrap silver training data from unlabeled news
headline-article lead sentence pairs. Models trained on silver headline parses
demonstrate significant improvements in performance over models trained solely
on gold-annotated long-form texts. Ultimately, we find that, although projected
silver training data improves parser performance across different news outlets,
the improvement is moderated by constructions idiosyncratic to outlet.",None,-1
Sequential Integrated Gradients: a simple but effective method for explaining language models,0.445952,"Several explanation methods such as Integrated Gradients (IG) can be
characterised as path-based methods, as they rely on a straight line between
the data and an uninformative baseline. However, when applied to language
models, these methods produce a path for each word of a sentence
simultaneously, which could lead to creating sentences from interpolated words
either having no clear meaning, or having a significantly different meaning
compared to the original sentence. In order to keep the meaning of these
sentences as close as possible to the original one, we propose Sequential
Integrated Gradients (SIG), which computes the importance of each word in a
sentence by keeping fixed every other words, only creating interpolations
between the baseline and the word of interest. Moreover, inspired by the
training procedure of several language models, we also propose to replace the
baseline token ""pad"" with the trained token ""mask"". While being a simple
improvement over the original IG method, we show on various models and datasets
that SIG proves to be a very effective method for explaining language models.",None,-1
A Wide Evaluation of ChatGPT on Affective Computing Tasks,0.832693,"With the rise of foundation models, a new artificial intelligence paradigm
has emerged, by simply using general purpose foundation models with prompting
to solve problems instead of training a separate machine learning model for
each problem. Such models have been shown to have emergent properties of
solving problems that they were not initially trained on. The studies for the
effectiveness of such models are still quite limited. In this work, we widely
study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13
affective computing problems, namely aspect extraction, aspect polarity
classification, opinion extraction, sentiment analysis, sentiment intensity
ranking, emotions intensity ranking, suicide tendency detection, toxicity
detection, well-being assessment, engagement measurement, personality
assessment, sarcasm detection, and subjectivity detection. We introduce a
framework to evaluate the ChatGPT models on regression-based problems, such as
intensity ranking problems, by modelling them as pairwise ranking
classification. We compare ChatGPT against more traditional NLP methods, such
as end-to-end recurrent neural networks and transformers. The results
demonstrate the emergent abilities of the ChatGPT models on a wide range of
affective computing problems, where GPT-3.5 and especially GPT-4 have shown
strong performance on many problems, particularly the ones related to
sentiment, emotions, or toxicity. The ChatGPT models fell short for problems
with implicit signals, such as engagement measurement and subjectivity
detection.",None,-1
On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study,0.540388,"Modern deep models for summarization attains impressive benchmark
performance, but they are prone to generating miscalibrated predictive
uncertainty. This means that they assign high confidence to low-quality
predictions, leading to compromised reliability and trustworthiness in
real-world applications. Probabilistic deep learning methods are common
solutions to the miscalibration problem. However, their relative effectiveness
in complex autoregressive summarization tasks are not well-understood. In this
work, we thoroughly investigate different state-of-the-art probabilistic
methods' effectiveness in improving the uncertainty quality of the neural
summarization models, across three large-scale benchmarks with varying
difficulty. We show that the probabilistic methods consistently improve the
model's generation and uncertainty quality, leading to improved selective
generation performance (i.e., abstaining from low-quality summaries) in
practice. We also reveal notable failure patterns of probabilistic methods
widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout),
cautioning the importance of choosing appropriate method for the data setting.",None,-1
Scaling Vision-Language Models with Sparse Mixture of Experts,0.474456,"The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.",None,-1
Causal Models with Constraints,0.156027,"Causal models have proven extremely useful in offering formal representations
of causal relationships between a set of variables. Yet in many situations,
there are non-causal relationships among variables. For example, we may want
variables $LDL$, $HDL$, and $TOT$ that represent the level of low-density
lipoprotein cholesterol, the level of lipoprotein high-density lipoprotein
cholesterol, and total cholesterol level, with the relation $LDL+HDL=TOT$. This
cannot be done in standard causal models, because we can intervene
simultaneously on all three variables. The goal of this paper is to extend
standard causal models to allow for constraints on settings of variables.
Although the extension is relatively straightforward, to make it useful we have
to define a new intervention operation that $disconnects$ a variable from a
causal equation. We give examples showing the usefulness of this extension, and
provide a sound and complete axiomatization for causal models with constraints.",None,-1
Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability,0.0535544,"Reinforcement learning (RL) is an area of significant research interest, and
safe RL in particular is attracting attention due to its ability to handle
safety-driven constraints that are crucial for real-world applications of RL
algorithms. This work proposes a novel approach to RL training, called control
invariant set (CIS) enhanced RL, which leverages the benefits of CIS to improve
stability guarantees and sampling efficiency. The approach consists of two
learning stages: offline and online. In the offline stage, CIS is incorporated
into the reward design, initial state sampling, and state reset procedures. In
the online stage, RL is retrained whenever the state is outside of CIS, which
serves as a stability criterion. A backup table that utilizes the explicit form
of CIS is obtained to ensure the online stability. To evaluate the proposed
approach, we apply it to a simulated chemical reactor. The results show a
significant improvement in sampling efficiency during offline training and
closed-loop stability in the online implementation.",None,-1
Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute,0.277492,"Retrieval-augmented language models such as Fusion-in-Decoder are powerful,
setting the state of the art on a variety of knowledge-intensive tasks.
However, they are also expensive, due to the need to encode a large number of
retrieved passages. Some work avoids this cost by pre-encoding a text corpus
into a memory and retrieving dense representations directly. However,
pre-encoding memory incurs a severe quality penalty as the memory
representations are not conditioned on the current input. We propose LUMEN, a
hybrid between these two extremes, pre-computing the majority of the retrieval
representation and completing the encoding on the fly using a live encoder that
is conditioned on the question and fine-tuned for the task. We show that LUMEN
significantly outperforms pure memory on multiple question-answering tasks
while being much cheaper than FiD, and outperforms both for any given compute
budget. Moreover, the advantage of LUMEN over FiD increases with model size.",None,-1
Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing,0.25631,"Recent works in end-to-end speech-to-text translation (ST) have proposed
multi-tasking methods with soft parameter sharing which leverage machine
translation (MT) data via secondary encoders that map text inputs to an
eventual cross-modal representation. In this work, we instead propose a ST/MT
multi-tasking framework with hard parameter sharing in which all model
parameters are shared cross-modally. Our method reduces the speech-text
modality gap via a pre-processing stage which converts speech and text inputs
into two discrete token sequences of similar length -- this allows models to
indiscriminately process both modalities simply using a joint vocabulary. With
experiments on MuST-C, we demonstrate that our multi-tasking framework improves
attentional encoder-decoder, Connectionist Temporal Classification (CTC),
transducer, and joint CTC/attention models by an average of +0.5 BLEU without
any external MT data. Further, we show that this framework incorporates
external MT data, yielding +0.8 BLEU, and also improves transfer learning from
pre-trained textual models, yielding +1.8 BLEU.",None,-1
MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,0.979906,"We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of
vision experts to achieve multimodal reasoning and action. In this paper, we
define and explore a comprehensive list of advanced vision tasks that are
intriguing to solve, but may exceed the capabilities of existing vision and
vision-language models. To achieve such advanced visual intelligence, MM-REACT
introduces a textual prompt design that can represent text descriptions,
textualized spatial coordinates, and aligned file names for dense visual
signals such as images and videos. MM-REACT's prompt design allows language
models to accept, associate, and process multimodal information, thereby
facilitating the synergetic combination of ChatGPT and various vision experts.
Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the
specified capabilities of interests and its wide application in different
scenarios that require advanced visual understanding. Furthermore, we discuss
and compare MM-REACT's system paradigm with an alternative approach that
extends language models for multimodal scenarios through joint finetuning.
Code, demo, video, and visualization are available at
https://multimodal-react.github.io/",None,-1
Large-Scale Multi-Hypotheses Cell Tracking Using Ultrametric Contours Maps,0.0757773,"In this work, we describe a method for large-scale 3D cell-tracking through a
segmentation selection approach. The proposed method is effective at tracking
cells across large microscopy datasets on two fronts: (i) It can solve problems
containing millions of segmentation instances in terabyte-scale 3D+t datasets;
(ii) It achieves competitive results with or without deep learning, which
requires 3D annotated data, that is scarce in the fluorescence microscopy
field. The proposed method computes cell tracks and segments using a hierarchy
of segmentation hypotheses and selects disjoint segments by maximizing the
overlap between adjacent frames. We show that this method achieves
state-of-the-art results in 3D images from the cell tracking challenge and has
a faster integer linear programming formulation. Moreover, our framework is
flexible and supports segmentations from off-the-shelf cell segmentation models
and can combine them into an ensemble that improves tracking. The code is
available https://github.com/royerlab/ultrack.",None,-1
SCOPE: Structural Continuity Preservation for Medical Image Segmentation,0.0869219,"Although the preservation of shape continuity and physiological anatomy is a
natural assumption in the segmentation of medical images, it is often neglected
by deep learning methods that mostly aim for the statistical modeling of input
data as pixels rather than interconnected structures. In biological structures,
however, organs are not separate entities; for example, in reality, a severed
vessel is an indication of an underlying problem, but traditional segmentation
models are not designed to strictly enforce the continuity of anatomy,
potentially leading to inaccurate medical diagnoses. To address this issue, we
propose a graph-based approach that enforces the continuity and connectivity of
anatomical topology in medical images. Our method encodes the continuity of
shapes as a graph constraint, ensuring that the network's predictions maintain
this continuity. We evaluate our method on two public benchmarks on retinal
vessel segmentation, showing significant improvements in connectivity metrics
compared to traditional methods while getting better or on-par performance on
segmentation metrics.",None,-1
Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence,0.730899,"Turbulence in fluids, gases, and plasmas remains an open problem of both
practical and fundamental importance. Its irreducible complexity usually cannot
be tackled computationally in a brute-force style. Here, we combine Large Eddy
Simulation (LES) techniques with Machine Learning (ML) to retain only the
largest dynamics explicitly, while small-scale dynamics are described by an
ML-based sub-grid-scale model. Applying this novel approach to self-driven
plasma turbulence allows us to remove large parts of the inertial range,
reducing the computational effort by about three orders of magnitude, while
retaining the statistical physical properties of the turbulent system.",None,-1
A Game of Bundle Adjustment -- Learning Efficient Convergence,0.294678,"Bundle adjustment is the common way to solve localization and mapping. It is
an iterative process in which a system of non-linear equations is solved using
two optimization methods, weighted by a damping factor. In the classic
approach, the latter is chosen heuristically by the Levenberg-Marquardt
algorithm on each iteration. This might take many iterations, making the
process computationally expensive, which might be harmful to real-time
applications. We propose to replace this heuristic by viewing the problem in a
holistic manner, as a game, and formulating it as a reinforcement-learning
task. We set an environment which solves the non-linear equations and train an
agent to choose the damping factor in a learned manner. We demonstrate that our
approach considerably reduces the number of iterations required to reach the
bundle adjustment's convergence, on both synthetic and real-life scenarios. We
show that this reduction benefits the classic approach and can be integrated
with other bundle adjustment acceleration methods.",None,-1
MVKT-ECG: Efficient Single-lead ECG Classification on Multi-Label Arrhythmia by Multi-View Knowledge Transferring,0.655995,"The widespread emergence of smart devices for ECG has sparked demand for
intelligent single-lead ECG-based diagnostic systems. However, it is
challenging to develop a single-lead-based ECG interpretation model for
multiple diseases diagnosis due to the lack of some key disease information. In
this work, we propose inter-lead Multi-View Knowledge Transferring of ECG
(MVKT-ECG) to boost single-lead ECG's ability for multi-label disease
diagnosis. This training strategy can transfer superior disease knowledge from
multiple different views of ECG (e.g. 12-lead ECG) to single-lead-based ECG
interpretation model to mine details in single-lead ECG signals that are easily
overlooked by neural networks. MVKT-ECG allows this lead variety as a
supervision signal within a teacher-student paradigm, where the teacher
observes multi-lead ECG educates a student who observes only single-lead ECG.
Since the mutual disease information between the single-lead ECG and muli-lead
ECG plays a key role in knowledge transferring, we present a new disease-aware
Contrastive Lead-information Transferring(CLT) to improve the mutual disease
information between the single-lead ECG and muli-lead ECG. Moreover, We modify
traditional Knowledge Distillation to multi-label disease Knowledge
Distillation (MKD) to make it applicable for multi-label disease diagnosis. The
comprehensive experiments verify that MVKT-ECG has an excellent performance in
improving the diagnostic effect of single-lead ECG.",None,-1
Navigating Fairness Measures and Trade-Offs,0.770508,"In order to monitor and prevent bias in AI systems we can use a wide range of
(statistical) fairness measures. However, it is mathematically impossible to
optimize for all of these measures at the same time. In addition, optimizing a
fairness measure often greatly reduces the accuracy of the system (Kozodoi et
al, 2022). As a result, we need a substantive theory that informs us how to
make these decisions and for what reasons. I show that by using Rawls' notion
of justice as fairness, we can create a basis for navigating fairness measures
and the accuracy trade-off. In particular, this leads to a principled choice
focusing on both the most vulnerable groups and the type of fairness measure
that has the biggest impact on that group. This also helps to close part of the
gap between philosophical accounts of distributive justice and the fairness
literature that has been observed (Kuppler et al, 2021) and to operationalise
the value of fairness.",None,-1
Online Map Vectorization for Autonomous Driving: A Rasterization Perspective,0.588363,"Vectorized high-definition (HD) map is essential for autonomous driving,
providing detailed and precise environmental information for advanced
perception and planning. However, current map vectorization methods often
exhibit deviations, and the existing evaluation metric for map vectorization
lacks sufficient sensitivity to detect these deviations. To address these
limitations, we propose integrating the philosophy of rasterization into map
vectorization. Specifically, we introduce a new rasterization-based evaluation
metric, which has superior sensitivity and is better suited to real-world
autonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization
via Rasterization), a novel framework that applies differentiable rasterization
to vectorized outputs and then performs precise and geometry-aware supervision
on rasterized HD maps. Notably, MapVR designs tailored rasterization strategies
for various geometric shapes, enabling effective adaptation to a wide range of
map elements. Experiments show that incorporating rasterization into map
vectorization greatly enhances performance with no extra computational cost
during inference, leading to more accurate map perception and ultimately
promoting safer autonomous driving.",None,-1
CONTAIN: A Community-based Algorithm for Network Immunization,0.482194,"Network immunization is an automated task in the field of network analysis
that involves protecting a network (modeled as a graph) from being infected by
an undesired arbitrary diffusion. In this article, we consider the spread of
harmful content in social networks, and we propose CONTAIN, a novel
COmmuNiTy-based Algorithm for network ImmuNization. Our solution uses the
network information to (1) detect harmful content spreaders, and (2) generate
partitions and rank them for immunization using the subgraphs induced by each
spreader, i.e., employing CONTAIN. The experimental results obtained on
real-world datasets show that CONTAIN outperforms state-of-the-art solutions,
i.e., NetShield and SparseShield, by immunizing the network in fewer
iterations, thus, converging significantly faster than the state-of-the-art
algorithms. We also compared our solution in terms of scalability with the
state-of-the-art tree-based mitigation algorithm MCWDST, as well as with
NetShield and SparseShield. We can conclude that our solution outperforms
MCWDST and NetShield.",None,-1
"An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM",0.184367,"Natural Language Processing (NLP) has emerged as a crucial technology for
understanding and generating human language, playing an essential role in tasks
such as machine translation, sentiment analysis, and more pertinently, question
classification. As a subfield within NLP, question classification focuses on
determining the type of information being sought, a fundamental step for
downstream applications like question answering systems. This study presents an
innovative ensemble approach for question classification, combining the
strengths of Electra, GloVe, and LSTM models. Rigorously tested on the
well-regarded TREC dataset, the model demonstrates how the integration of these
disparate technologies can lead to superior results. Electra brings in its
transformer-based capabilities for complex language understanding, GloVe offers
global vector representations for capturing word-level semantics, and LSTM
contributes its sequence learning abilities to model long-term dependencies. By
fusing these elements strategically, our ensemble model delivers a robust and
efficient solution for the complex task of question classification. Through
rigorous comparisons with well-known models like BERT, RoBERTa, and DistilBERT,
the ensemble approach verifies its effectiveness by attaining an 80% accuracy
score on the test dataset.",None,-1
Scalable 3D Captioning with Pretrained Models,0.902648,"We introduce Cap3D, an automatic approach for generating descriptive text for
3D objects. This approach utilizes pretrained models from image captioning,
image-text alignment, and LLM to consolidate captions from multiple views of a
3D asset, completely side-stepping the time-consuming and costly process of
manual annotation. We apply Cap3D to the recently introduced large-scale 3D
dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted
using 41k human annotations from the same dataset, demonstrates that Cap3D
surpasses human-authored descriptions in terms of quality, cost, and speed.
Through effective prompt engineering, Cap3D rivals human performance in
generating geometric descriptions on 17k collected annotations from the ABO
dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,
and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,
and DreamFusion.",None,-1
Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction,0.830708,"Multi-Modal Relation Extraction (MMRE) aims at identifying the relation
between two entities in texts that contain visual clues. Rich visual content is
valuable for the MMRE task, but existing works cannot well model finer
associations among different modalities, failing to capture the truly helpful
visual information and thus limiting relation extraction performance. In this
paper, we propose a novel MMRE framework to better capture the deeper
correlations of text, entity pair, and image/objects, so as to mine more
helpful information for the task, termed as DGF-PT. We first propose a
prompt-based autoregressive encoder, which builds the associations of
intra-modal and inter-modal features related to the task, respectively by
entity-oriented and object-oriented prefixes. To better integrate helpful
visual information, we design a dual-gated fusion module to distinguish the
importance of image/objects and further enrich text representations. In
addition, a generative decoder is introduced with entity type restriction on
relations, better filtering out candidates. Extensive experiments conducted on
the benchmark dataset show that our approach achieves excellent performance
compared to strong competitors, even in the few-shot situation.",None,-1
Transfer Learning of Transformer-based Speech Recognition Models from Czech to Slovak,0.0980239,"In this paper, we are comparing several methods of training the Slovak speech
recognition models based on the Transformers architecture. Specifically, we are
exploring the approach of transfer learning from the existing Czech pre-trained
Wav2Vec 2.0 model into Slovak. We are demonstrating the benefits of the
proposed approach on three Slovak datasets. Our Slovak models scored the best
results when initializing the weights from the Czech model at the beginning of
the pre-training phase. Our results show that the knowledge stored in the Cezch
pre-trained model can be successfully reused to solve tasks in Slovak while
outperforming even much larger public multilingual models.",None,-1
CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training,0.0300982,"Speech or text representation generated by pre-trained models contains
modal-specific information that could be combined for benefiting spoken
language understanding (SLU) tasks. In this work, we propose a novel
pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training
(CIF-PT). It relies on a simple but effective frame-to-token alignment:
continuous integrate-and-fire (CIF) to bridge the representations between
speech and text. It jointly performs speech-to-text training and language model
distillation through CIF as the pre-training (PT). Evaluated on SLU benchmark
SLURP dataset, CIF-PT outperforms the state-of-the-art model by 1.94% of
accuracy and 2.71% of SLU-F1 on the tasks of intent classification and slot
filling, respectively. We also observe the cross-modal representation extracted
by CIF-PT obtains better performance than other neural interfaces for the tasks
of SLU, including the dominant speech representation learned from
self-supervised pre-training.",None,-1
Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method,0.541608,"Theorem proving is a fundamental task in mathematics. With the advent of
large language models (LLMs) and interactive theorem provers (ITPs) like Lean,
there has been growing interest in integrating LLMs and ITPs to automate
theorem proving. In this approach, the LLM generates proof steps (tactics), and
the ITP checks the applicability of the tactics at the current goal. The two
systems work together to complete the proof. In this paper, we introduce
DS-Prover, a novel dynamic sampling method for theorem proving. This method
dynamically determines the number of tactics to apply to expand the current
goal, taking into account the remaining time compared to the total allocated
time for proving a theorem. This makes the proof search process more efficient
by adjusting the balance between exploration and exploitation as time passes.
We also augment the training dataset by decomposing simplification and rewrite
tactics with multiple premises into tactics with single premises. This gives
the model more examples to learn from and helps it to predict the tactics with
premises more accurately. We perform our experiments using the Mathlib dataset
of the Lean theorem prover and report the performance on two standard datasets,
MiniF2F and ProofNet. Our methods achieve significant performance gains on both
datasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the
ProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the
best-reported Pass@1 of 29.6% using Lean.",None,-1
DDP: Diffusion Model for Dense Visual Prediction,0.987893,"We propose a simple, efficient, yet powerful framework for dense visual
predictions based on the conditional diffusion pipeline. Our approach follows a
""noise-to-map"" generative paradigm for prediction by progressively removing
noise from a random Gaussian distribution, guided by the image. The method,
called DDP, efficiently extends the denoising diffusion process into the modern
perception pipeline. Without task-specific design and architecture
customization, DDP is easy to generalize to most dense prediction tasks, e.g.,
semantic segmentation and depth estimation. In addition, DDP shows attractive
properties such as dynamic inference and uncertainty awareness, in contrast to
previous single-step discriminative methods. We show top results on three
representative tasks with six diverse benchmarks, without tricks, DDP achieves
state-of-the-art or competitive performance on each task compared to the
specialist counterparts. For example, semantic segmentation (83.9 mIoU on
Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation
(0.05 REL on KITTI). We hope that our approach will serve as a solid baseline
and facilitate future research",None,-1
Dynamic Voxel Grid Optimization for High-Fidelity RGB-D Supervised Surface Reconstruction,0.194063,"Direct optimization of interpolated features on multi-resolution voxel grids
has emerged as a more efficient alternative to MLP-like modules. However, this
approach is constrained by higher memory expenses and limited representation
capabilities. In this paper, we introduce a novel dynamic grid optimization
method for high-fidelity 3D surface reconstruction that incorporates both RGB
and depth observations. Rather than treating each voxel equally, we optimize
the process by dynamically modifying the grid and assigning more finer-scale
voxels to regions with higher complexity, allowing us to capture more intricate
details. Furthermore, we develop a scheme to quantify the dynamic subdivision
of voxel grid during optimization without requiring any priors. The proposed
approach is able to generate high-quality 3D reconstructions with fine details
on both synthetic and real-world data, while maintaining computational
efficiency, which is substantially faster than the baseline method NeuralRGBD.",None,-1
Visual Watermark Removal Based on Deep Learning,0.244895,"In recent years as the internet age continues to grow, sharing images on
social media has become a common occurrence. In certain cases, watermarks are
used as protection for the ownership of the image, however, in more cases, one
may wish to remove these watermark images to get the original image without
obscuring. In this work, we proposed a deep learning method based technique for
visual watermark removal. Inspired by the strong image translation performance
of the U-structure, an end-to-end deep neural network model named AdvancedUnet
is proposed to extract and remove the visual watermark simultaneously. On the
other hand, we embed some effective RSU module instead of the common residual
block used in UNet, which increases the depth of the whole architecture without
significantly increasing the computational cost. The deep-supervised hybrid
loss guides the network to learn the transformation between the input image and
the ground truth in a multi-scale and three-level hierarchy. Comparison
experiments demonstrate the effectiveness of our method.",None,-1
ZeroPose: CAD-Model-based Zero-Shot Pose Estimation,0.540853,"In this paper, we present a CAD model-based zero-shot pose estimation
pipeline called ZeroPose. Existing pose estimation methods remain to require
expensive training when applied to an unseen object, which greatly hinders
their scalability in the practical application of industry. In contrast, the
proposed method enables the accurate estimation of pose parameters for
previously unseen objects without the need for training. Specifically, we
design a two-step pipeline consisting of CAD model-based zero-shot instance
segmentation and a zero-shot pose estimator. For the first step, there is a
simple but effective way to leverage CAD models and visual foundation models
SAM and Imagebind to segment the interest unseen object at the instance level.
For the second step, we based on the intensive geometric information in the CAD
model of the rigid object to propose a lightweight hierarchical geometric
structure matching mechanism achieving zero-shot pose estimation. Extensive
experimental results on the seven core datasets on the BOP challenge show that
the proposed zero-shot instance segmentation methods achieve comparable
performance with supervised MaskRCNN and the zero-shot pose estimation results
outperform the SOTA pose estimators with better efficiency.",None,-1
Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation,0.549953,"Very large language models (LLMs) perform extremely well on a spectrum of NLP
tasks in a zero-shot setting. However, little is known about their performance
on human-level NLP problems which rely on understanding psychological concepts,
such as assessing personality traits. In this work, we investigate the
zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'
social media posts. Through a set of systematic experiments, we find that
zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA
for broad classification upon injecting knowledge about the trait in the
prompts. However, when prompted to provide fine-grained classification, its
performance drops to close to a simple most frequent class (MFC) baseline. We
further analyze where GPT-3 performs better, as well as worse, than a
pretrained lexical model, illustrating systematic errors that suggest ways to
improve LLMs on human-level NLP tasks.",None,-1
Mastering Strategy Card Game (Legends of Code and Magic) via End-to-End Policy and Optimistic Smooth Fictitious Play,0.66329,"Deep Reinforcement Learning combined with Fictitious Play shows impressive
results on many benchmark games, most of which are, however, single-stage. In
contrast, real-world decision making problems may consist of multiple stages,
where the observation spaces and the action spaces can be completely different
across stages. We study a two-stage strategy card game Legends of Code and
Magic and propose an end-to-end policy to address the difficulties that arise
in multi-stage game. We also propose an optimistic smooth fictitious play
algorithm to find the Nash Equilibrium for the two-player game. Our approach
wins double championships of COG2022 competition. Extensive studies verify and
show the advancement of our approach.",None,-1
Smooth Non-Stationary Bandits,0.611182,"In many applications of online decision making, the environment is
non-stationary and it is therefore crucial to use bandit algorithms that handle
changes. Most existing approaches are designed to protect against non-smooth
changes, constrained only by total variation or Lipschitzness over time, where
they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice
environments are often changing {\bf smoothly}, so such algorithms may incur
higher-than-necessary regret in these settings and do not leverage information
on the rate of change. We study a non-stationary two-armed bandits problem
where we assume that an arm's mean reward is a $\beta$-H\""older function over
(normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously
differentiable. We show the first separation between the smooth and non-smooth
regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$.
We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound
for any integer $\beta\ge 1$, which matches our upper bound for $\beta=2$.",None,-1
Triplet Edge Attention for Algorithmic Reasoning,0.461107,"This work investigates neural algorithmic reasoning to develop neural
networks capable of learning from classical algorithms. The main challenge is
to develop graph neural networks that are expressive enough to predict the
given algorithm outputs while generalizing well to out-of-distribution data. In
this work, we introduce a new graph neural network layer called Triplet Edge
Attention (TEA), an edge-aware graph attention layer. Our algorithm works by
precisely computing edge latent, aggregating multiple triplet messages using
edge-based attention. We empirically validate our TEA layer in the CLRS
benchmark and demonstrate a $5%$ improvement on average. In particular, we
achieve a $30%$ improvement for the string algorithms compared to the
state-of-the-art model.",None,-1
Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction,0.95295,"We study the problem of learning goal-conditioned policies in Minecraft, a
popular, widely accessible yet challenging open-ended environment for
developing human-level multi-task agents. We first identify two main challenges
of learning such policies: 1) the indistinguishability of tasks from the state
distribution, due to the vast scene diversity, and 2) the non-stationary nature
of environment dynamics caused by partial observability. To tackle the first
challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage
the emergence of goal-relevant visual state representations. To tackle the
second challenge, the policy is further fueled by an adaptive horizon
prediction module that helps alleviate the learning uncertainty brought by the
non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method
significantly outperforms the best baseline so far; in many of them, we double
the performance. Our ablation and exploratory studies then explain how our
approach beat the counterparts and also unveil the surprising bonus of
zero-shot generalization to new scenes (biomes). We hope our agent could help
shed some light on learning goal-conditioned, multi-task agents in challenging,
open-ended environments like Minecraft.",None,-1
Estimating temperatures with low-cost infrared cameras using deep neural networks,0.141031,"Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have
space-variant nonuniformity across their detector. Both inaccuracy and
nonuniformity are dependent on the ambient temperature of the camera. The goal
of this work was to estimate temperatures with low-cost infrared cameras, and
rectify the nonuniformity.
  A nonuniformity simulator that accounts for the ambient temperature was
developed. An end-to-end neural network that incorporates both the physical
model of the camera and the ambient camera temperature was introduced. The
neural network was trained with the simulated nonuniformity data to estimate
the object's temperature and correct the nonuniformity, using only a single
image and the ambient temperature measured by the camera itself. Results of the
proposed method significantly improved the mean temperature error compared to
previous works by up to $0.5^\circ C$. In addition, constraining the physical
model of the camera with the network lowered the error by an additional
$0.1^\circ C$.
  The mean temperature error over an extensive validation dataset was
$0.37^\circ C$. The method was verified on real data in the field and produced
equivalent results.",None,-1
Positive Label Is All You Need for Multi-Label Classification,0.416435,"Multi-label classification (MLC) faces challenges from label noise in
training data due to annotating diverse semantic labels for each image. Current
methods mainly target identifying and correcting label mistakes using trained
MLC models, but still struggle with persistent noisy labels during training,
resulting in imprecise recognition and reduced performance. Our paper addresses
label noise in MLC by introducing a positive and unlabeled multi-label
classification (PU-MLC) method. To counteract noisy labels, we directly discard
negative labels, focusing on the abundance of negative labels and the origin of
most noisy labels. PU-MLC employs positive-unlabeled learning, training the
model with only positive labels and unlabeled data. The method incorporates
adaptive re-balance factors and temperature coefficients in the loss function
to address label distribution imbalance and prevent over-smoothing of
probabilities during training. Additionally, we introduce a local-global
convolution module to capture both local and global dependencies in the image
without requiring backbone retraining. PU-MLC proves effective on MLC and MLC
with partial labels (MLC-PL) tasks, demonstrating significant improvements on
MS-COCO and PASCAL VOC datasets with fewer annotations. Code is available at:
https://github.com/TAKELAMAG/PU-MLC.",None,-1
SoccerNet 2023 Tracking Challenge -- 3rd place MOT4MOT Team Technical Report,0.705425,"The SoccerNet 2023 tracking challenge requires the detection and tracking of
soccer players and the ball. In this work, we present our approach to tackle
these tasks separately. We employ a state-of-the-art online multi-object
tracker and a contemporary object detector for player tracking. To overcome the
limitations of our online approach, we incorporate a post-processing stage
using interpolation and appearance-free track merging. Additionally, an
appearance-based track merging technique is used to handle the termination and
creation of tracks far from the image boundaries. Ball tracking is formulated
as single object detection, and a fine-tuned YOLOv8l detector with proprietary
filtering improves the detection precision. Our method achieves 3rd place on
the SoccerNet 2023 tracking challenge with a HOTA score of 66.27.",None,-1
SePaint: Semantic Map Inpainting via Multinomial Diffusion,0.0876145,"Prediction beyond partial observations is crucial for robots to navigate in
unknown environments because it can provide extra information regarding the
surroundings beyond the current sensing range or resolution. In this work, we
consider the inpainting of semantic Bird's-Eye-View maps. We propose SePaint,
an inpainting model for semantic data based on generative multinomial
diffusion. To maintain semantic consistency, we need to condition the
prediction for the missing regions on the known regions. We propose a novel and
efficient condition strategy, Look-Back Condition (LB-Con), which performs
one-step look-back operations during the reverse diffusion process. By doing
so, we are able to strengthen the harmonization between unknown and known
parts, leading to better completion performance. We have conducted extensive
experiments on different datasets, showing our proposed model outperforms
commonly used interpolation methods in various robotic applications.",None,-1
Summarizing Strategy Card Game AI Competition,0.493273,"This paper concludes five years of AI competitions based on Legends of Code
and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal
of supporting research and algorithm development. The game was used in a number
of events, including Community Contests on the CodinGame platform, and Strategy
Card Game AI Competition at the IEEE Congress on Evolutionary Computation and
IEEE Conference on Games. LOCM has been used in a number of publications
related to areas such as game tree search algorithms, neural networks,
evaluation functions, and CCG deckbuilding. We present the rules of the game,
the history of organized competitions, and a listing of the participant and
their approaches, as well as some general advice on organizing AI competitions
for the research community. Although the COG 2022 edition was announced to be
the last one, the game remains available and can be played using an online
leaderboard arena.",None,-1
Few-Shot Defect Image Generation via Defect-Aware Feature Manipulation,0.214683,"The performances of defect inspection have been severely hindered by
insufficient defect images in industries, which can be alleviated by generating
more samples as data augmentation. We propose the first defect image generation
method in the challenging few-shot cases. Given just a handful of defect images
and relatively more defect-free ones, our goal is to augment the dataset with
new defect images. Our method consists of two training stages. First, we train
a data-efficient StyleGAN2 on defect-free images as the backbone. Second, we
attach defect-aware residual blocks to the backbone, which learn to produce
reasonable defect masks and accordingly manipulate the features within the
masked regions by training the added modules on limited defect images.
Extensive experiments on MVTec AD dataset not only validate the effectiveness
of our method in generating realistic and diverse defect images, but also
manifest the benefits it brings to downstream defect inspection tasks. Codes
are available at https://github.com/Ldhlwh/DFMGAN.",None,-1
"The Good, The Bad, and Why: Unveiling Emotions in Generative AI",0.179283,"Emotion significantly impacts our daily behaviors and interactions. While
recent generative AI models, such as large language models, have shown
impressive performance in various tasks, it remains unclear whether they truly
comprehend emotions. This paper aims to address this gap by incorporating
psychological theories to gain a holistic understanding of emotions in
generative AI models. Specifically, we propose three approaches: 1)
EmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI
model performance, and 3) EmotionDecode to explain the effects of emotional
stimuli, both benign and malignant. Through extensive experiments involving
language and multi-modal models on semantic understanding, logical reasoning,
and generation tasks, we demonstrate that both textual and visual EmotionPrompt
can boost the performance of AI models while EmotionAttack can hinder it.
Additionally, EmotionDecode reveals that AI models can comprehend emotional
stimuli akin to the mechanism of dopamine in the human brain. Our work heralds
a novel avenue for exploring psychology to enhance our understanding of
generative AI models.",None,-1
DCFace: Synthetic Face Generation with Dual Condition Diffusion Model,0.993007,"Generating synthetic datasets for training face recognition models is
challenging because dataset generation entails more than creating high fidelity
images. It involves generating multiple images of same subjects under different
factors (\textit{e.g.}, variations in pose, illumination, expression, aging and
occlusion) which follows the real image conditional distribution. Previous
works have studied the generation of synthetic datasets using GAN or 3D models.
In this work, we approach the problem from the aspect of combining subject
appearance (ID) and external factor (style) conditions. These two conditions
provide a direct way to control the inter-class and intra-class variations. To
this end, we propose a Dual Condition Face Generator (DCFace) based on a
diffusion model. Our novel Patch-wise style extractor and Time-step dependent
ID loss enables DCFace to consistently produce face images of the same subject
under different styles with precise control. Face recognition models trained on
synthetic images from the proposed DCFace provide higher verification
accuracies compared to previous works by $6.11\%$ on average in $4$ out of $5$
test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code is available at
https://github.com/mk-minchul/dcface",None,-1
Diffusion Action Segmentation,0.650794,"Temporal action segmentation is crucial for understanding long-form videos.
Previous works on this task commonly adopt an iterative refinement paradigm by
using multi-stage models. We propose a novel framework via denoising diffusion
models, which nonetheless shares the same inherent spirit of such iterative
refinement. In this framework, action predictions are iteratively generated
from random noise with input video features as conditions. To enhance the
modeling of three striking characteristics of human actions, including the
position prior, the boundary ambiguity, and the relational dependency, we
devise a unified masking strategy for the conditioning inputs in our framework.
Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and
Breakfast, are performed and the proposed method achieves superior or
comparable results to state-of-the-art methods, showing the effectiveness of a
generative approach for action segmentation.",None,-1
"Unsupervised Learning on a DIET: Datum IndEx as Target Free of Self-Supervision, Reconstruction, Projector Head",0.0601642,"Costly, noisy, and over-specialized, labels are to be set aside in favor of
unsupervised learning if we hope to learn cheap, reliable, and transferable
models. To that end, spectral embedding, self-supervised learning, or
generative modeling have offered competitive solutions. Those methods however
come with numerous challenges \textit{e.g.} estimating geodesic distances,
specifying projector architectures and anti-collapse losses, or specifying
decoder architectures and reconstruction losses. In contrast, we introduce a
simple explainable alternative -- coined \textbf{DIET} -- to learn
representations from unlabeled data, free of those challenges. \textbf{DIET} is
blatantly simple: take one's favorite classification setup and use the
\textbf{D}atum \textbf{I}nd\textbf{E}x as its \textbf{T}arget class,
\textit{i.e. each sample is its own class}, no further changes needed.
\textbf{DIET} works without a decoder/projector network, is not based on
positive pairs nor reconstruction, introduces no hyper-parameters, and works
out-of-the-box across datasets and architectures. Despite \textbf{DIET}'s
simplicity, the learned representations are of high-quality and often on-par
with the state-of-the-art \textit{e.g.} using a linear classifier on top of
DIET's learned representation reaches $71.4\%$ on CIFAR100 with a Resnet101,
$52.5\%$ on TinyImagenet with a Resnext50.",None,-1
Learning Robust Self-attention Features for Speech Emotion Recognition with Label-adaptive Mixup,0.236994,"Speech Emotion Recognition (SER) is to recognize human emotions in a natural
verbal interaction scenario with machines, which is considered as a challenging
problem due to the ambiguous human emotions. Despite the recent progress in
SER, state-of-the-art models struggle to achieve a satisfactory performance. We
propose a self-attention based method with combined use of label-adaptive mixup
and center loss. By adapting label probabilities in mixup and fitting center
loss to the mixup training scheme, our proposed method achieves a superior
performance to the state-of-the-art methods.",None,-1
Interactive Class-Agnostic Object Counting,0.279156,"We propose a novel framework for interactive class-agnostic object counting,
where a human user can interactively provide feedback to improve the accuracy
of a counter. Our framework consists of two main components: a user-friendly
visualizer to gather feedback and an efficient mechanism to incorporate it. In
each iteration, we produce a density map to show the current prediction result,
and we segment it into non-overlapping regions with an easily verifiable number
of objects. The user can provide feedback by selecting a region with obvious
counting errors and specifying the range for the estimated number of objects
within it. To improve the counting result, we develop a novel adaptation loss
to force the visual counter to output the predicted count within the
user-specified range. For effective and efficient adaptation, we propose a
refinement module that can be used with any density-based visual counter, and
only the parameters in the refinement module will be updated during adaptation.
Our experiments on two challenging class-agnostic object counting benchmarks,
FSCD-LVIS and FSC-147, show that our method can reduce the mean absolute error
of multiple state-of-the-art visual counters by roughly 30% to 40% with minimal
user input. Our project can be found at
https://yifehuang97.github.io/ICACountProjectPage/.",None,-1
Multilingual Alzheimer's Dementia Recognition through Spontaneous Speech: a Signal Processing Grand Challenge,0.97266,"This Signal Processing Grand Challenge (SPGC) targets a difficult automatic
prediction problem of societal and medical relevance, namely, the detection of
Alzheimer's Dementia (AD). Participants were invited to employ signal
processing and machine learning methods to create predictive models based on
spontaneous speech data. The Challenge has been designed to assess the extent
to which predictive models built based on speech in one language (English)
generalise to another language (Greek). To the best of our knowledge no work
has investigated acoustic features of the speech signal in multilingual AD
detection. Our baseline system used conventional machine learning algorithms
with Active Data Representation of acoustic features, achieving accuracy of
73.91% on AD detection, and 4.95 root mean squared error on cognitive score
prediction.",None,-1
Reinforcement Learning with Temporal-Logic-Based Causal Diagrams,0.0339019,"We study a class of reinforcement learning (RL) tasks where the objective of
the agent is to accomplish temporally extended goals. In this setting, a common
approach is to represent the tasks as deterministic finite automata (DFA) and
integrate them into the state-space for RL algorithms. However, while these
machines model the reward function, they often overlook the causal knowledge
about the environment. To address this limitation, we propose the
Temporal-Logic-based Causal Diagram (TL-CD) in RL, which captures the temporal
causal relationships between different properties of the environment. We
exploit the TL-CD to devise an RL algorithm in which an agent requires
significantly less exploration of the environment. To this end, based on a
TL-CD and a task DFA, we identify configurations where the agent can determine
the expected rewards early during an exploration. Through a series of case
studies, we demonstrate the benefits of using TL-CDs, particularly the faster
convergence of the algorithm to an optimal policy due to reduced exploration of
the environment.",None,-1
API-Assisted Code Generation for Question Answering on Varied Table Structures,0.223073,"A persistent challenge to table question answering (TableQA) by generating
executable programs has been adapting to varied table structures, typically
requiring domain-specific logical forms. In response, this paper introduces a
unified TableQA framework that: (1) provides a unified representation for
structured tables as multi-index Pandas data frames, (2) uses Python as a
powerful querying language, and (3) uses few-shot prompting to translate NL
questions into Python programs, which are executable on Pandas data frames.
Furthermore, to answer complex relational questions with extended program
functionality and external knowledge, our framework allows customized APIs that
Python programs can call. We experiment with four TableQA datasets that involve
tables of different structures -- relational, multi-table, and hierarchical
matrix shapes -- and achieve prominent improvements over past state-of-the-art
systems. In ablation studies, we (1) show benefits from our multi-index
representation and APIs over baselines that use only an LLM, and (2)
demonstrate that our approach is modular and can incorporate additional APIs.",None,-1
Learning and Aggregating Lane Graphs for Urban Automated Driving,0.894731,"Lane graph estimation is an essential and highly challenging task in
automated driving and HD map learning. Existing methods using either onboard or
aerial imagery struggle with complex lane topologies, out-of-distribution
scenarios, or significant occlusions in the image space. Moreover, merging
overlapping lane graphs to obtain consistent large-scale graphs remains
difficult. To overcome these challenges, we propose a novel bottom-up approach
to lane graph estimation from aerial imagery that aggregates multiple
overlapping graphs into a single consistent graph. Due to its modular design,
our method allows us to address two complementary tasks: predicting
ego-respective successor lane graphs from arbitrary vehicle positions using a
graph neural network and aggregating these predictions into a consistent global
lane graph. Extensive experiments on a large-scale lane graph dataset
demonstrate that our approach yields highly accurate lane graphs, even in
regions with severe occlusions. The presented approach to graph aggregation
proves to eliminate inconsistent predictions while increasing the overall graph
quality. We make our large-scale urban lane graph dataset and code publicly
available at http://urbanlanegraph.cs.uni-freiburg.de.",None,-1
Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs,0.45925,"As foundation models continue to exponentially scale in size, efficient
methods of adaptation become increasingly critical. Parameter-efficient
fine-tuning (PEFT), a recent class of techniques that require only modifying a
small percentage of the model parameters, is currently the most popular method
for adapting large language models (LLMs). Several PEFT techniques have
recently been proposed with varying tradeoffs. We provide a comprehensive and
uniform benchmark of various PEFT techniques across a representative LLM, the
FLAN-T5 model, and evaluate model performance across different data scales of
classification and generation datasets. Based on this, we provide a framework
for choosing the optimal fine-tuning techniques given the task type and data
availability. Contrary to popular belief, we also empirically prove that PEFT
techniques converge slower than full tuning in low data scenarios, and posit
the amount of data required for PEFT methods to both perform well and converge
efficiently. Lastly, we further optimize these PEFT techniques by selectively
choosing which parts of the model to train, and find that these techniques can
be applied with significantly fewer parameters while maintaining and even
improving performance.",None,-1
Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification,0.953649,"Environmental, Social, and Governance (ESG) has been used as a metric to
measure the negative impacts and enhance positive outcomes of companies in
areas such as the environment, society, and governance. Recently, investors
have increasingly recognized the significance of ESG criteria in their
investment choices, leading businesses to integrate ESG principles into their
operations and strategies. The Multi-Lingual ESG Issue Identification (ML-ESG)
shared task encompasses the classification of news documents into 35 distinct
ESG issue labels. In this study, we explored multiple strategies harnessing
BERT language models to achieve accurate classification of news documents
across these labels. Our analysis revealed that the RoBERTa classifier emerged
as one of the most successful approaches, securing the second-place position
for the English test dataset, and sharing the fifth-place position for the
French test dataset. Furthermore, our SVM-based binary model tailored for the
Chinese language exhibited exceptional performance, earning the second-place
rank on the test dataset.",None,-1
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection,0.512968,"This paper presents the winning system for the zero-shot Spanish framing
detection task, which also achieves competitive places in eight additional
languages. The challenge of the framing detection task lies in identifying a
set of 14 frames when only a few or zero samples are available, i.e., a
multilingual multi-label few- or zero-shot setting. Our developed solution
employs a pre-training procedure based on multilingual Transformers using a
label-aware contrastive loss function. In addition to describing the system, we
perform an embedding space analysis and ablation study to demonstrate how our
pre-training procedure supports framing detection to advance computational
framing analysis.",None,-1
A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles,0.11904,"This work introduces a preference learning method that ensures adherence to
given specifications, with an application to autonomous vehicles. Our approach
incorporates the priority ordering of Signal Temporal Logic (STL) formulas
describing traffic rules into a learning framework. By leveraging Parametric
Weighted Signal Temporal Logic (PWSTL), we formulate the problem of
safety-guaranteed preference learning based on pairwise comparisons and propose
an approach to solve this learning problem. Our approach finds a feasible
valuation for the weights of the given PWSTL formula such that, with these
weights, preferred signals have weighted quantitative satisfaction measures
greater than their non-preferred counterparts. The feasible valuation of
weights given by our approach leads to a weighted STL formula that can be used
in correct-and-custom-by-construction controller synthesis. We demonstrate the
performance of our method with a pilot human subject study in two different
simulated driving scenarios involving a stop sign and a pedestrian crossing.
Our approach yields competitive results compared to existing preference
learning methods in terms of capturing preferences and notably outperforms them
when safety is considered.",None,-1
Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers,0.522978,"Transformers have become a key architecture in speech processing, but our
understanding of how they build up representations of acoustic and linguistic
structure is limited. In this study, we address this gap by investigating how
measures of 'context-mixing' developed for text models can be adapted and
applied to models of spoken language. We identify a linguistic phenomenon that
is ideal for such a case study: homophony in French (e.g. livre vs livres),
where a speech recognition model has to attend to syntactic cues such as
determiners and pronouns in order to disambiguate spoken words with identical
pronunciations and transcribe them while respecting grammatical agreement. We
perform a series of controlled experiments and probing analyses on
Transformer-based speech models. Our findings reveal that representations in
encoder-only models effectively incorporate these cues to identify the correct
transcription, whereas encoders in encoder-decoder models mainly relegate the
task of capturing contextual dependencies to decoder modules.",None,-1
Mutually Guided Few-shot Learning for Relational Triple Extraction,0.373273,"Knowledge graphs (KGs), containing many entity-relation-entity triples,
provide rich information for downstream applications. Although extracting
triples from unstructured texts has been widely explored, most of them require
a large number of labeled instances. The performance will drop dramatically
when only few labeled data are available. To tackle this problem, we propose
the Mutually Guided Few-shot learning framework for Relational Triple
Extraction (MG-FTE). Specifically, our method consists of an entity-guided
relation proto-decoder to classify the relations firstly and a relation-guided
entity proto-decoder to extract entities based on the classified relations. To
draw the connection between entity and relation, we design a proto-level fusion
module to boost the performance of both entity extraction and relation
classification. Moreover, a new cross-domain few-shot triple extraction task is
introduced. Extensive experiments show that our method outperforms many
state-of-the-art methods by 12.6 F1 score on FewRel 1.0 (single-domain) and
20.5 F1 score on FewRel 2.0 (cross-domain).",None,-1
Mask Detection and Classification in Thermal Face Images,0.581183,"Face masks are recommended to reduce the transmission of many viruses,
especially SARS-CoV-2. Therefore, the automatic detection of whether there is a
mask on the face, what type of mask is worn, and how it is worn is an important
research topic. In this work, the use of thermal imaging was considered to
analyze the possibility of detecting (localizing) a mask on the face, as well
as to check whether it is possible to classify the type of mask on the face.
The previously proposed dataset of thermal images was extended and annotated
with the description of a type of mask and a location of a mask within a face.
Different deep learning models were adapted. The best model for face mask
detection turned out to be the Yolov5 model in the ""nano"" version, reaching mAP
higher than 97% and precision of about 95%. High accuracy was also obtained for
mask type classification. The best results were obtained for the convolutional
neural network model built on an autoencoder initially trained in the thermal
image reconstruction problem. The pretrained encoder was used to train a
classifier which achieved an accuracy of 91%.",None,-1
Human Fall Detection- Multimodality Approach,0.0501799,"Falls have become more frequent in recent years, which has been harmful for
senior citizens.Therefore detecting falls have become important and several
data sets and machine learning model have been introduced related to fall
detection. In this project report, a human fall detection method is proposed
using a multi modality approach. We used the UP-FALL detection data set which
is collected by dozens of volunteers using different sensors and two cameras.
We use wrist sensor with acclerometer data keeping labels to binary
classification, namely fall and no fall from the data set.We used fusion of
camera and sensor data to increase performance. The experimental results shows
that using only wrist data as compared to multi sensor for binary
classification did not impact the model prediction performance for fall
detection.",None,-1
Language Model Tokenizers Introduce Unfairness Between Languages,0.718888,"Recent language models have shown impressive multilingual performance, even
when not explicitly trained for it. Despite this, there are concerns about the
quality of their outputs across different languages. In this paper, we show how
disparity in the treatment of different languages arises at the tokenization
stage, well before a model is even invoked. The same text translated into
different languages can have drastically different tokenization lengths, with
differences up to 15 times in some cases. These disparities persist even for
tokenizers that are intentionally trained for multilingual support.
Character-level and byte-level models also exhibit over 4 times the difference
in the encoding length for some language pairs. This induces unfair treatment
for some language communities in regard to the cost of accessing commercial
language services, the processing time and latency, as well as the amount of
content that can be provided as context to the models. Therefore, we make the
case that we should train future language models using multilingually fair
subword tokenizers.",None,-1
Using AI Uncertainty Quantification to Improve Human Decision-Making,0.220133,"AI Uncertainty Quantification (UQ) has the potential to improve human
decision-making beyond AI predictions alone by providing additional
probabilistic information to users. The majority of past research on AI and
human decision-making has concentrated on model explainability and
interpretability, with little focus on understanding the potential impact of UQ
on human decision-making. We evaluated the impact on human decision-making for
instance-level UQ, calibrated using a strict scoring rule, in two online
behavioral experiments. In the first experiment, our results showed that UQ was
beneficial for decision-making performance compared to only AI predictions. In
the second experiment, we found UQ had generalizable benefits for
decision-making across a variety of representations for probabilistic
information. These results indicate that implementing high quality,
instance-level UQ for AI may improve decision-making with real systems compared
to AI predictions alone.",None,-1
Conformal Prediction with Large Language Models for Multi-Choice Question Answering,0.915801,"As large language models continue to be widely developed, robust uncertainty
quantification techniques will become crucial for their safe deployment in
high-stakes scenarios. In this work, we explore how conformal prediction can be
used to provide uncertainty quantification in language models for the specific
task of multiple-choice question-answering. We find that the uncertainty
estimates from conformal prediction are tightly correlated with prediction
accuracy. This observation can be useful for downstream applications such as
selective classification and filtering out low-quality predictions. We also
investigate the exchangeability assumption required by conformal prediction to
out-of-subject questions, which may be a more realistic scenario for many
practical applications. Our work contributes towards more trustworthy and
reliable usage of large language models in safety-critical situations, where
robust guarantees of error rate are required.",None,-1
MetaTKG: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning,0.665309,"Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts
based on given history. One of the key challenges for prediction is to learn
the evolution of facts. Most existing works focus on exploring evolutionary
information in history to obtain effective temporal embeddings for entities and
relations, but they ignore the variation in evolution patterns of facts, which
makes them struggle to adapt to future data with different evolution patterns.
Moreover, new entities continue to emerge along with the evolution of facts
over time. Since existing models highly rely on historical information to learn
embeddings for entities, they perform poorly on such entities with little
historical information. To tackle these issues, we propose a novel Temporal
Meta-learning framework for TKG reasoning, MetaTKG for brevity. Specifically,
our method regards TKG prediction as many temporal meta-tasks, and utilizes the
designed Temporal Meta-learner to learn evolutionary meta-knowledge from these
meta-tasks. The proposed method aims to guide the backbones to learn to adapt
quickly to future data and deal with entities with little historical
information by the learned meta-knowledge. Specially, in temporal meta-learner,
we design a Gating Integration module to adaptively establish temporal
correlations between meta-tasks. Extensive experiments on four widely-used
datasets and three backbones demonstrate that our method can greatly improve
the performance.",None,-1
Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection,0.827955,"Large Language Models (LLMs) can adapt to new tasks via in-context learning
(ICL). ICL is efficient as it does not require any parameter updates to the
trained LLM, but only few annotated examples as input for the LLM. In this
work, we investigate an active learning approach for ICL, where there is a
limited budget for annotating examples. We propose a model-adaptive
optimization-free algorithm, termed AdaICL, which identifies examples that the
model is uncertain about, and performs semantic diversity-based example
selection. Diversity-based sampling improves overall effectiveness, while
uncertainty sampling improves budget efficiency and helps the LLM learn new
information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage
problem, that dynamically adapts based on the model's feedback and can be
approximately solved via greedy algorithms. Extensive experiments on nine
datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy
points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient
than performing annotations uniformly at random, while it outperforms SOTA with
2x fewer ICL examples.",None,-1
She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models,0.968864,"Implicit gender bias in software development is a well-documented issue, such
as the association of technical roles with men. To address this bias, it is
important to understand it in more detail. This study uses data mining
techniques to investigate the extent to which 56 tasks related to software
development, such as assigning GitHub issues and testing, are affected by
implicit gender bias embedded in large language models. We systematically
translated each task from English into a genderless language and back, and
investigated the pronouns associated with each task. Based on translating each
task 100 times in different permutations, we identify a significant disparity
in the gendered pronoun associations with different tasks. Specifically,
requirements elicitation was associated with the pronoun ""he"" in only 6% of
cases, while testing was associated with ""he"" in 100% of cases. Additionally,
tasks related to helping others had a 91% association with ""he"" while the same
association for tasks related to asking coworkers was only 52%. These findings
reveal a clear pattern of gender bias related to software development tasks and
have important implications for addressing this issue both in the training of
large language models and in broader society.",None,-1
"Learning to ""Segment Anything"" in Thermal Infrared Images through Knowledge Distillation with a Large Scale Dataset SATIR",0.970735,"The Segment Anything Model (SAM) is a promptable segmentation model recently
introduced by Meta AI that has demonstrated its prowess across various fields
beyond just image segmentation. SAM can accurately segment images across
diverse fields, and generating various masks. We discovered that this ability
of SAM can be leveraged to pretrain models for specific fields. Accordingly, we
have proposed a framework that utilizes SAM to generate pseudo labels for
pretraining thermal infrared image segmentation tasks. Our proposed framework
can effectively improve the accuracy of segmentation results of specific
categories beyond the SOTA ImageNet pretrained model. Our framework presents a
novel approach to collaborate with models trained with large data like SAM to
address problems in special fields. Also, we generated a large scale thermal
infrared segmentation dataset used for pretaining, which contains over 100,000
images with pixel-annotation labels. This approach offers an effective solution
for working with large models in special fields where label annotation is
challenging. Our code is available at https://github.com/chenjzBUAA/SATIR",None,-1
Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning,0.0437489,"In cross-domain few-shot learning, the core issue is that the model trained
on source domains struggles to generalize to the target domain, especially when
the domain shift is large. Motivated by the observation that the domain shift
between training tasks and target tasks usually can reflect in their style
variation, we propose Task Augmented Meta-Learning (TAML) to conduct style
transfer-based task augmentation to improve the domain generalization ability.
Firstly, Multi-task Interpolation (MTI) is introduced to fuse features from
multiple tasks with different styles, which makes more diverse styles
available. Furthermore, a novel task-augmentation strategy called Multi-Task
Style Transfer (MTST) is proposed to perform style transfer on existing tasks
to learn discriminative style-independent features. We also introduce a Feature
Modulation module (FM) to add random styles and improve generalization of the
model. The proposed TAML increases the diversity of styles of training tasks,
and contributes to training a model with better domain generalization ability.
The effectiveness is demonstrated via theoretical analysis and thorough
experiments on two popular cross-domain few-shot benchmarks.",None,-1
Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model,0.796142,"In this study, we developed an automated short answer grading (ASAG) model
that provided both analytic scores and final holistic scores. Short answer
items typically consist of multiple sub-questions, and providing an analytic
score and the text span relevant to each sub-question can increase the
interpretability of the automated scores. Furthermore, they can be used to
generate actionable feedback for students. Despite these advantages, most
studies have focused on predicting only holistic scores due to the difficulty
in constructing dataset with manual annotations. To address this difficulty, we
used large language model (LLM)-based one-shot prompting and a text similarity
scoring model with domain adaptation using small manually annotated dataset.
The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a
subset of the publicly available ASAG dataset. The model achieved a substantial
improvement over the majority baseline.",None,-1
Human Preference Score: Better Aligning Text-to-Image Models with Human Preference,0.521627,"Recent years have witnessed a rapid growth of deep generative models, with
text-to-image models gaining significant attention from the public. However,
existing models often generate images that do not align well with human
preferences, such as awkward combinations of limbs and facial expressions. To
address this issue, we collect a dataset of human choices on generated images
from the Stable Foundation Discord channel. Our experiments demonstrate that
current evaluation metrics for generative models do not correlate well with
human choices. Thus, we train a human preference classifier with the collected
dataset and derive a Human Preference Score (HPS) based on the classifier.
Using HPS, we propose a simple yet effective method to adapt Stable Diffusion
to better align with human preferences. Our experiments show that HPS
outperforms CLIP in predicting human choices and has good generalization
capability toward images generated from other models. By tuning Stable
Diffusion with the guidance of HPS, the adapted model is able to generate
images that are more preferred by human users. The project page is available
here: https://tgxs002.github.io/align_sd_web/ .",None,-1
Rotation-Scale Equivariant Steerable Filters,0.213176,"Incorporating either rotation equivariance or scale equivariance into CNNs
has proved to be effective in improving models' generalization performance.
However, jointly integrating rotation and scale equivariance into CNNs has not
been widely explored. Digital histology imaging of biopsy tissue can be
captured at arbitrary orientation and magnification and stored at different
resolutions, resulting in cells appearing in different scales. When
conventional CNNs are applied to histopathology image analysis, the
generalization performance of models is limited because 1) a part of the
parameters of filters are trained to fit rotation transformation, thus
decreasing the capability of learning other discriminative features; 2)
fixed-size filters trained on images at a given scale fail to generalize to
those at different scales. To deal with these issues, we propose the
Rotation-Scale Equivariant Steerable Filter (RSESF), which incorporates
steerable filters and scale-space theory. The RSESF contains copies of filters
that are linear combinations of Gaussian filters, whose direction is controlled
by directional derivatives and whose scale parameters are trainable but
constrained to span disjoint scales in successive layers of the network.
Extensive experiments on two gland segmentation datasets demonstrate that our
method outperforms other approaches, with much fewer trainable parameters and
fewer GPU resources required. The source code is available at:
https://github.com/ynulonger/RSESF.",None,-1
Learned Two-Plane Perspective Prior based Image Resampling for Efficient Object Detection,0.106255,"Real-time efficient perception is critical for autonomous navigation and city
scale sensing. Orthogonal to architectural improvements, streaming perception
approaches have exploited adaptive sampling improving real-time detection
performance. In this work, we propose a learnable geometry-guided prior that
incorporates rough geometry of the 3D scene (a ground plane and a plane above)
to resample images for efficient object detection. This significantly improves
small and far-away object detection performance while also being more efficient
both in terms of latency and memory. For autonomous navigation, using the same
detector and scale, our approach improves detection rate by +4.1 $AP_{S}$ or
+39% and in real-time performance by +5.3 $sAP_{S}$ or +63% for small objects
over state-of-the-art (SOTA). For fixed traffic cameras, our approach detects
small objects at image scales other methods cannot. At the same scale, our
approach improves detection of small objects by 195% (+12.5 $AP_{S}$) over
naive-downsampling and 63% (+4.2 $AP_{S}$) over SOTA.",None,-1
Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries,0.129049,"Deep ensembles (DE) have been successful in improving model performance by
learning diverse members via the stochasticity of random initialization. While
recent works have attempted to promote further diversity in DE via
hyperparameters or regularizing loss functions, these methods primarily still
rely on a stochastic approach to explore the hypothesis space. In this work, we
present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse
ensembles by capturing the multiplicity of hypotheses along symmetry axes,
which explore the hypothesis space beyond stochastic perturbations of model
weights and hyperparameters. We leverage recent advances in contrastive
representation learning to create models that separately capture opposing
hypotheses of invariant and equivariant functional classes and present a simple
ensembling approach to efficiently combine appropriate hypotheses for a given
task. We show that MSE effectively captures the multiplicity of conflicting
hypotheses that is often required in large, diverse datasets like ImageNet. As
a result of their inherent diversity, MSE improves classification performance,
uncertainty quantification, and generalization across a series of transfer
tasks.",None,-1
Watch out Venomous Snake Species: A Solution to SnakeCLEF2023,0.779217,"The SnakeCLEF2023 competition aims to the development of advanced algorithms
for snake species identification through the analysis of images and
accompanying metadata. This paper presents a method leveraging utilization of
both images and metadata. Modern CNN models and strong data augmentation are
utilized to learn better representation of images. To relieve the challenge of
long-tailed distribution, seesaw loss is utilized in our method. We also design
a light model to calculate prior probabilities using metadata features
extracted from CLIP in post processing stage. Besides, we attach more
importance to venomous species by assigning venomous species labels to some
examples that model is uncertain about. Our method achieves 91.31% score of the
final metric combined of F1 and other metrics on private leaderboard, which is
the 1st place among the participators. The code is available at
https://github.com/xiaoxsparraw/CLEF2023.",None,-1
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind,0.942417,"Theory of Mind (ToM), the capacity to comprehend the mental states of
distinct individuals, is essential for numerous practical applications. With
the development of large language models (LLMs), there is a heated debate about
whether they are able to perform ToM tasks. Previous studies have used
different tasks and prompts to test the ToM on LLMs and the results are
inconsistent: some studies asserted these models are capable of exhibiting ToM,
while others suggest the opposite. In this study, We present ToMChallenges, a
dataset for comprehensively evaluating the Theory of Mind based on the
Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also
propose an auto-grader to streamline the answer evaluation process. We tested
three models: davinci, turbo, and gpt-4. Our evaluation results and error
analyses show that LLMs have inconsistent behaviors across prompts and tasks.
Performing the ToM tasks robustly remains a challenge for the LLMs. In
addition, our paper wants to raise awareness in evaluating the ToM in LLMs and
we want to invite more discussion on how to design the prompts and tasks for
ToM tasks that can better assess the LLMs' ability.",None,-1
Efficient Enumeration of Markov Equivalent DAGs,0.218245,"Enumerating the directed acyclic graphs (DAGs) of a Markov equivalence class
(MEC) is an important primitive in causal analysis. The central resource from
the perspective of computational complexity is the delay, that is, the time an
algorithm that lists all members of the class requires between two consecutive
outputs. Commonly used algorithms for this task utilize the rules proposed by
Meek (1995) or the transformational characterization by Chickering (1995), both
resulting in superlinear delay. In this paper, we present the first linear-time
delay algorithm. On the theoretical side, we show that our algorithm can be
generalized to enumerate DAGs represented by models that incorporate background
knowledge, such as MPDAGs; on the practical side, we provide an efficient
implementation and evaluate it in a series of experiments. Complementary to the
linear-time delay algorithm, we also provide intriguing insights into Markov
equivalence itself: All members of an MEC can be enumerated such that two
successive DAGs have structural Hamming distance at most three.",None,-1
Robust face anti-spoofing framework with Convolutional Vision Transformer,0.85237,"Owing to the advances in image processing technology and large-scale
datasets, companies have implemented facial authentication processes, thereby
stimulating increased focus on face anti-spoofing (FAS) against realistic
presentation attacks. Recently, various attempts have been made to improve face
recognition performance using both global and local learning on face images;
however, to the best of our knowledge, this is the first study to investigate
whether the robustness of FAS against domain shifts is improved by considering
global information and local cues in face images captured using self-attention
and convolutional layers. This study proposes a convolutional vision
transformer-based framework that achieves robust performance for various unseen
domain data. Our model resulted in 7.3%$p$ and 12.9%$p$ increases in FAS
performance compared to models using only a convolutional neural network or
vision transformer, respectively. It also shows the highest average rank in
sub-protocols of cross-dataset setting over the other nine benchmark models for
domain generalization.",None,-1
Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details,0.257852,"We propose Text2Scene, a method to automatically create realistic textures
for virtual scenes composed of multiple objects. Guided by a reference image
and text descriptions, our pipeline adds detailed texture on labeled 3D
geometries in the room such that the generated colors respect the hierarchical
structure or semantic parts that are often composed of similar materials.
Instead of applying flat stylization on the entire scene at a single step, we
obtain weak semantic cues from geometric segmentation, which are further
clarified by assigning initial colors to segmented parts. Then we add texture
details for individual objects such that their projections on image space
exhibit feature embedding aligned with the embedding of the input. The
decomposition makes the entire pipeline tractable to a moderate amount of
computation resources and memory. As our framework utilizes the existing
resources of image and text embedding, it does not require dedicated datasets
with high-quality textures designed by skillful artists. To the best of our
knowledge, it is the first practical and scalable approach that can create
detailed and realistic textures of the desired style that maintain structural
context for scenes with multiple objects.",None,-1
Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration,0.230302,"The current approach for testing the robustness of object detectors suffers
from serious deficiencies such as improper methods of performing
out-of-distribution detection and using calibration metrics which do not
consider both localisation and classification quality. In this work, we address
these issues, and introduce the Self-Aware Object Detection (SAOD) task, a
unified testing framework which respects and adheres to the challenges that
object detectors face in safety-critical environments such as autonomous
driving. Specifically, the SAOD task requires an object detector to be: robust
to domain shift; obtain reliable uncertainty estimates for the entire scene;
and provide calibrated confidence scores for the detections. We extensively use
our framework, which introduces novel metrics and large scale test datasets, to
test numerous object detectors in two different use-cases, allowing us to
highlight critical insights into their robustness performance. Finally, we
introduce a simple baseline for the SAOD task, enabling researchers to
benchmark future proposed methods and move towards robust object detectors
which are fit for purpose. Code is available at https://github.com/fiveai/saod",None,-1
Distributed Trust Through the Lens of Software Architecture,0.394839,"Distributed trust is a nebulous concept that has evolved from different
perspectives in recent years. While one can attribute its current prominence to
blockchain and cryptocurrency, the distributed trust concept has been
cultivating progress in federated learning, trustworthy and responsible AI in
an ecosystem setting, data sharing, privacy issues across organizational
boundaries, and zero trust cybersecurity. This paper will survey the concept of
distributed trust in multiple disciplines. It will take a system/software
architecture point of view to look at trust redistribution/shift and the
associated tradeoffs in systems and applications enabled by distributed trust
technologies.",None,-1
Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation,0.711215,"Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.",None,-1
Cross-Cultural Transfer Learning for Chinese Offensive Language Detection,0.4725,"Detecting offensive language is a challenging task. Generalizing across
different cultures and languages becomes even more challenging: besides
lexical, syntactic and semantic differences, pragmatic aspects such as cultural
norms and sensitivities, which are particularly relevant in this context, vary
greatly. In this paper, we target Chinese offensive language detection and aim
to investigate the impact of transfer learning using offensive language
detection data from different cultural backgrounds, specifically Korean and
English. We find that culture-specific biases in what is considered offensive
negatively impact the transferability of language models (LMs) and that LMs
trained on diverse cultural data are sensitive to different features in Chinese
offensive language detection. In a few-shot learning scenario, however, our
study shows promising prospects for non-English offensive language detection
with limited resources. Our findings highlight the importance of cross-cultural
transfer learning in improving offensive language detection and promoting
inclusive digital spaces.",None,-1
SparseVSR: Lightweight and Noise Robust Visual Speech Recognition,0.333421,"Recent advances in deep neural networks have achieved unprecedented success
in visual speech recognition. However, there remains substantial disparity
between current methods and their deployment in resource-constrained devices.
In this work, we explore different magnitude-based pruning techniques to
generate a lightweight model that achieves higher performance than its dense
model equivalent, especially under the presence of visual noise. Our sparse
models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and
outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse
model on 7 different visual noise types and achieve an overall absolute
improvement of more than 2% WER compared to the dense equivalent. Our results
confirm that sparse networks are more resistant to noise than dense networks.",None,-1
"A data science axiology: the nature, value, and risks of data science",0.148591,"Data science is not a science. It is a research paradigm with an unfathomed
scope, scale, complexity, and power for knowledge discovery that is not
otherwise possible and can be beyond human reasoning. It is changing our world
practically and profoundly already widely deployed in tens of thousands of
applications in every discipline in an AI Arms Race that, due to its
inscrutability, can lead to unfathomed risks. This paper presents an axiology
of data science, its purpose, nature, importance, risks, and value for problem
solving, by exploring and evaluating its remarkable, definitive features. As
data science is in its infancy, this initial, speculative axiology is intended
to aid in understanding and defining data science to recognize its potential
benefits, risks, and open research challenges. AI based data science is
inherently about uncertainty that may be more realistic than our preference for
the certainty of science. Data science will have impacts far beyond knowledge
discovery and will take us into new ways of understanding the world.",None,-1
A Generalist Dynamics Model for Control,0.599784,"We investigate the use of transformer sequence models as dynamics models
(TDMs) for control. We find that TDMs exhibit strong generalization
capabilities to unseen environments, both in a few-shot setting, where a
generalist TDM is fine-tuned with small amounts of data from the target
environment, and in a zero-shot setting, where a generalist TDM is applied to
an unseen environment without any further training. Here, we demonstrate that
generalizing system dynamics can work much better than generalizing optimal
behavior directly as a policy. Additional results show that TDMs also perform
well in a single-environment learning setting when compared to a number of
baseline models. These properties make TDMs a promising ingredient for a
foundation model of control.",None,-1
Models of reference production: How do they withstand the test of time?,0.0527607,"In recent years, many NLP studies have focused solely on performance
improvement. In this work, we focus on the linguistic and scientific aspects of
NLP. We use the task of generating referring expressions in context
(REG-in-context) as a case study and start our analysis from GREC, a
comprehensive set of shared tasks in English that addressed this topic over a
decade ago. We ask what the performance of models would be if we assessed them
(1) on more realistic datasets, and (2) using more advanced methods. We test
the models using different evaluation metrics and feature selection
experiments. We conclude that GREC can no longer be regarded as offering a
reliable assessment of models' ability to mimic human reference production,
because the results are highly impacted by the choice of corpus and evaluation
metrics. Our results also suggest that pre-trained language models are less
dependent on the choice of corpus than classic Machine Learning models, and
therefore make more robust class predictions.",None,-1
Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation,0.618403,"Simultaneous speech translation is an essential communication task difficult
for humans whereby a translation is generated concurrently with oncoming speech
inputs. For such a streaming task, transformers using block processing to break
an input sequence into segments have achieved state-of-the-art performance at a
reduced cost. Current methods to allow information to propagate across
segments, including left context and memory banks, have faltered as they are
both insufficient representations and unnecessarily expensive to compute. In
this paper, we propose an Implicit Memory Transformer that implicitly retains
memory through a new left context method, removing the need to explicitly
represent memory with memory banks. We generate the left context from the
attention output of the previous segment and include it in the keys and values
of the current segment's attention calculation. Experiments on the MuST-C
dataset show that the Implicit Memory Transformer provides a substantial
speedup on the encoder forward pass with nearly identical translation quality
when compared with the state-of-the-art approach that employs both left context
and memory banks.",None,-1
Siamese DETR,0.209414,"Recent self-supervised methods are mainly designed for representation
learning with the base model, e.g., ResNets or ViTs. They cannot be easily
transferred to DETR, with task-specific Transformer modules. In this work, we
present Siamese DETR, a Siamese self-supervised pretraining approach for the
Transformer architecture in DETR. We consider learning view-invariant and
detection-oriented representations simultaneously through two complementary
tasks, i.e., localization and discrimination, in a novel multi-view learning
framework. Two self-supervised pretext tasks are designed: (i) Multi-View
Region Detection aims at learning to localize regions-of-interest between
augmented views of the input, and (ii) Multi-View Semantic Discrimination
attempts to improve object-level discrimination for each region. The proposed
Siamese DETR achieves state-of-the-art transfer performance on COCO and PASCAL
VOC detection using different DETR variants in all setups. Code is available at
https://github.com/Zx55/SiameseDETR.",None,-1
GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images,0.449405,"In this work, we focus on synthesizing high-fidelity novel view images for
arbitrary human performers, given a set of sparse multi-view images. It is a
challenging task due to the large variation among articulated body poses and
heavy self-occlusions. To alleviate this, we introduce an effective
generalizable framework Generalizable Model-based Neural Radiance Fields
(GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a
geometry-guided attention mechanism to register the appearance code from
multi-view 2D images to a geometry proxy which can alleviate the misalignment
between inaccurate geometry prior and pixel space. On top of that, we further
conduct neural rendering and partial gradient backpropagation for efficient
perceptual supervision and improvement of the perceptual quality of synthesis.
To evaluate our method, we conduct experiments on synthesized datasets
THuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The
results demonstrate that our approach outperforms state-of-the-art methods in
terms of novel view synthesis and geometric reconstruction.",None,-1
Syllable Subword Tokens for Open Vocabulary Speech Recognition in Malayalam,0.0783221,"In a hybrid automatic speech recognition (ASR) system, a pronunciation
lexicon (PL) and a language model (LM) are essential to correctly retrieve
spoken word sequences. Being a morphologically complex language, the vocabulary
of Malayalam is so huge and it is impossible to build a PL and an LM that cover
all diverse word forms. Usage of subword tokens to build PL and LM, and
combining them to form words after decoding, enables the recovery of many out
of vocabulary words. In this work we investigate the impact of using syllables
as subword tokens instead of words in Malayalam ASR, and evaluate the relative
improvement in lexicon size, model memory requirement and word error rate.",None,-1
Prompting Neural Machine Translation with Translation Memories,0.113071,"Improving machine translation (MT) systems with translation memories (TMs) is
of great interest to practitioners in the MT community. However, previous
approaches require either a significant update of the model architecture and/or
additional training efforts to make the models well-behaved when TMs are taken
as additional input. In this paper, we present a simple but effective method to
introduce TMs into neural machine translation (NMT) systems. Specifically, we
treat TMs as prompts to the NMT model at test time, but leave the training
process unchanged. The result is a slight update of an existing NMT system,
which can be implemented in a few hours by anyone who is familiar with NMT.
Experimental results on several datasets demonstrate that our system
significantly outperforms strong baselines.",None,-1
What has ChatGPT read? The origins of archaeological citations used by a generative artificial intelligence application,0.105493,"The public release of ChatGPT has resulted in considerable publicity and has
led to wide-spread discussion of the usefulness and capabilities of generative
AI language models. Its ability to extract and summarise data from textual
sources and present them as human-like contextual responses makes it an
eminently suitable tool to answer questions users might ask. This paper tested
what archaeological literature appears to have been included in ChatGPT's
training phase. While ChatGPT offered seemingly pertinent references, a large
percentage proved to be fictitious. Using cloze analysis to make inferences on
the sources 'memorised' by a generative AI model, this paper was unable to
prove that ChatGPT had access to the full texts of the genuine references. It
can be shown that all references provided by ChatGPT that were found to be
genuine have also been cited on Wikipedia pages. This strongly indicates that
the source base for at least some of the data is found in those pages. The
implications of this in relation to data quality are discussed.",None,-1
Dialogue Shaping: Empowering Agents through NPC Interaction,0.191862,"One major challenge in reinforcement learning (RL) is the large amount of
steps for the RL agent needs to converge in the training process and learn the
optimal policy, especially in text-based game environments where the action
space is extensive. However, non-player characters (NPCs) sometimes hold some
key information about the game, which can potentially help to train RL agents
faster. Thus, this paper explores how to interact and converse with NPC agents
to get the key information using large language models (LLMs), as well as
incorporate this information to speed up RL agent's training using knowledge
graphs (KGs) and Story Shaping.",None,-1
DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning,0.115517,"In recent years, sentiment analysis has gained significant importance in
natural language processing. However, most existing models and datasets for
sentiment analysis are developed for high-resource languages, such as English
and Chinese, leaving low-resource languages, particularly African languages,
largely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this
gap by evaluating sentiment analysis models on low-resource African languages.
In this paper, we present our solution to the shared task, where we employed
different multilingual XLM-R models with classification head trained on various
data, including those retrained in African dialects and fine-tuned on target
languages. Our team achieved the third-best results in Subtask B, Track 16:
Multilingual, demonstrating the effectiveness of our approach. While our model
showed relatively good results on multilingual data, it performed poorly in
some languages. Our findings highlight the importance of developing more
comprehensive datasets and models for low-resource African languages to advance
sentiment analysis research. We also provided the solution on the github
repository.",None,-1
Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios,0.346223,"The abilities to understand the social interaction behaviors between a
vehicle and its surroundings while predicting its trajectory in an urban
environment are critical for road safety in autonomous driving. Social
interactions are hard to explain because of their uncertainty. In recent years,
neural network-based methods have been widely used for trajectory prediction
and have been shown to outperform hand-crafted methods. However, these methods
suffer from their lack of interpretability. In order to overcome this
limitation, we combine the interpretability of a discrete choice model with the
high accuracy of a neural network-based model for the task of vehicle
trajectory prediction in an interactive environment. We implement and evaluate
our model using the INTERACTION dataset and demonstrate the effectiveness of
our proposed architecture to explain its predictions without compromising the
accuracy.",None,-1
Confidence-Aware and Self-Supervised Image Anomaly Localisation,0.178878,"Universal anomaly detection still remains a challenging problem in machine
learning and medical image analysis. It is possible to learn an expected
distribution from a single class of normative samples, e.g., through epistemic
uncertainty estimates, auto-encoding models, or from synthetic anomalies in a
self-supervised way. The performance of self-supervised anomaly detection
approaches is still inferior compared to methods that use examples from known
unknown classes to shape the decision boundary. However, outlier exposure
methods often do not identify unknown unknowns. Here we discuss an improved
self-supervised single-class training strategy that supports the approximation
of probabilistic inference with loosen feature locality constraints. We show
that up-scaling of gradients with histogram-equalised images is beneficial for
recently proposed self-supervision tasks. Our method is integrated into several
out-of-distribution (OOD) detection models and we show evidence that our method
outperforms the state-of-the-art on various benchmark datasets.",None,-1
Active Learning Guided Fine-Tuning for enhancing Self-Supervised Based Multi-Label Classification of Remote Sensing Images,0.301672,"In recent years, deep neural networks (DNNs) have been found very successful
for multi-label classification (MLC) of remote sensing (RS) images.
Self-supervised pre-training combined with fine-tuning on a randomly selected
small training set has become a popular approach to minimize annotation efforts
of data-demanding DNNs. However, fine-tuning on a small and biased training set
may limit model performance. To address this issue, we investigate the
effectiveness of the joint use of self-supervised pre-training with active
learning (AL). The considered AL strategy aims at guiding the MLC fine-tuning
of a self-supervised model by selecting informative training samples to
annotate in an iterative manner. Experimental results show the effectiveness of
applying AL-guided fine-tuning (particularly for the case where strong
class-imbalance is present in MLC problems) compared to the application of
fine-tuning using a randomly constructed small training set.",None,-1
GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer,0.245875,"The recently proposed Vision transformers (ViTs) have shown very impressive
empirical performance in various computer vision tasks, and they are viewed as
an important type of foundation model. However, ViTs are typically constructed
with large-scale sizes, which then severely hinder their potential deployment
in many practical resources-constrained applications. To mitigate this
challenging problem, structured pruning is a promising solution to compress
model size and enable practical efficiency. However, unlike its current
popularity for CNNs and RNNs, structured pruning for ViT models is little
explored.
  In this paper, we propose GOHSP, a unified framework of Graph and
Optimization-based Structured Pruning for ViT models. We first develop a
graph-based ranking for measuring the importance of attention heads, and the
extracted importance information is further integrated to an optimization-based
procedure to impose the heterogeneous structured sparsity patterns on the ViT
models. Experimental results show that our proposed GOHSP demonstrates
excellent compression performance. On CIFAR-10 dataset, our approach can bring
40% parameters reduction with no accuracy loss for ViT-Small model. On ImageNet
dataset, with 30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models,
our approach achieves 1.65% and 0.76% accuracy increase over the existing
structured pruning methods, respectively.",None,-1
MADiff: Offline Multi-agent Learning with Diffusion Models,0.749543,"Diffusion model (DM) recently achieved huge success in various scenarios
including offline reinforcement learning, where the diffusion planner learn to
generate desired trajectories during online evaluations. However, despite the
effectiveness in single-agent learning, it remains unclear how DMs can operate
in multi-agent problems, where agents can hardly complete teamwork without good
coordination by independently modeling each agent's trajectories. In this
paper, we propose MADiff, a novel generative multi-agent learning framework to
tackle this problem. MADiff is realized with an attention-based diffusion model
to model the complex coordination among behaviors of multiple agents. To the
best of our knowledge, MADiff is the first diffusion-based multi-agent learning
framework, which behaves as both a decentralized policy and a centralized
controller. During decentralized executions, MADiff simultaneously performs
teammate modeling, and the centralized controller can also be applied in
multi-agent trajectory predictions. Our experiments show the superior
performance of MADiff compared to baseline algorithms in a wide range of
multi-agent learning tasks, which emphasizes the effectiveness of MADiff in
modeling complex multi-agent interactions. Our code is available at
https://github.com/zbzhu99/madiff.",None,-1
Instructed Language Models with Retrievers Are Powerful Entity Linkers,0.784527,"Generative approaches powered by large language models (LLMs) have
demonstrated emergent abilities in tasks that require complex reasoning
abilities. Yet the generative nature still makes the generated content suffer
from hallucinations, thus unsuitable for entity-centric tasks like entity
linking (EL) requiring precise entity predictions over a large knowledge base.
We present Instructed Generative Entity Linker (INSGENEL), the first approach
that enables casual language models to perform entity linking over knowledge
bases. Several methods to equip language models with EL capability were
proposed in this work, including (i) a sequence-to-sequence training EL
objective with instruction-tuning, (ii) a novel generative EL framework based
on a light-weight potential mention retriever that frees the model from heavy
and non-parallelizable decoding, achieving 4$\times$ speedup without compromise
on linking metrics. INSGENEL outperforms previous generative alternatives with
+6.8 F1 points gain on average, also with a huge advantage in training data
efficiency and training compute consumption. In addition, our skillfully
engineered in-context learning (ICL) framework for EL still lags behind
INSGENEL significantly, reaffirming that the EL task remains a persistent
hurdle for general LLMs.",None,-1
3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,0.260848,"The proliferation of non-cooperative resident space objects (RSOs) in orbit
has spurred the demand for active space debris removal, on-orbit servicing
(OOS), classification, and functionality identification of these RSOs. Recent
advances in computer vision have enabled high-definition 3D modeling of objects
based on a set of 2D images captured from different viewing angles. This work
adapts Instant NeRF and D-NeRF, variations of the neural radiance field (NeRF)
algorithm to the problem of mapping RSOs in orbit for the purposes of
functionality identification and assisting with OOS. The algorithms are
evaluated for 3D reconstruction quality and hardware requirements using
datasets of images of a spacecraft mock-up taken under two different lighting
and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing
and Navigation (ORION) Laboratory at Florida Institute of Technology. Instant
NeRF is shown to learn high-fidelity 3D models with a computational cost that
could feasibly be trained on on-board computers.",None,-1
Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions,0.051246,"The success of transformer models trained with a language modeling objective
brings a promising opportunity to the reinforcement learning framework.
Decision Transformer is a step towards this direction, showing how to train
transformers with a similar next-step prediction objective on offline data.
Another important development in this area is the recent emergence of
large-scale datasets collected from the internet, such as the ones composed of
tutorial videos with captions where people talk about what they are doing. To
take advantage of this language component, we propose a novel method for
unifying language reasoning with actions in a single policy. Specifically, we
augment a transformer policy with word outputs, so it can generate textual
captions interleaved with actions. When tested on the most challenging task in
BabyAI, with captions describing next subgoals, our reasoning policy
consistently outperforms the caption-free baseline.",None,-1
BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval,0.612375,"Efficient information retrieval (IR) from building information models (BIMs)
poses significant challenges due to the necessity for deep BIM knowledge or
extensive engineering efforts for automation. We introduce BIM-GPT, a
prompt-based virtual assistant (VA) framework integrating BIM and generative
pre-trained transformer (GPT) technologies to support NL-based IR. A prompt
manager and dynamic template generate prompts for GPT models, enabling
interpretation of NL queries, summarization of retrieved information, and
answering BIM-related questions. In tests on a BIM IR dataset, our approach
achieved 83.5% and 99.5% accuracy rates for classifying NL queries with no data
and 2% data incorporated in prompts, respectively. Additionally, we validated
the functionality of BIM-GPT through a VA prototype for a hospital building.
This research contributes to the development of effective and versatile VAs for
BIM IR in the construction industry, significantly enhancing BIM accessibility
and reducing engineering efforts and training data requirements for processing
NL queries.",None,-1
Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,0.0698235,"Recent pre-trained language models (PLMs) equipped with foundation reasoning
skills have shown remarkable performance on downstream complex tasks. However,
the significant structure reasoning skill has been rarely studied, which
involves modeling implicit structure information within the text and performing
explicit logical reasoning over them to deduce the conclusion. This paper
proposes a unified learning framework that combines explicit structure
reasoning and language pre-training to endow PLMs with the structure reasoning
skill. It first identifies several elementary structures within contexts to
construct structured queries and performs step-by-step reasoning along the
queries to identify the answer entity. The fusion of textual semantics and
structure reasoning is achieved by using contextual representations learned by
PLMs to initialize the representation space of structures, and performing
stepwise reasoning on this semantic representation space. Experimental results
on four datasets demonstrate that the proposed model achieves significant
improvements in complex reasoning tasks involving diverse structures, and shows
transferability to downstream tasks with limited training data and
effectiveness for complex reasoning of KGs modality.",None,-1
Edit Everything: A Text-Guided Generative System for Images Editing,0.603802,"We introduce a new generative system called Edit Everything, which can take
image and text inputs and produce image outputs. Edit Everything allows users
to edit images using simple text instructions. Our system designs prompts to
guide the visual module in generating requested images. Experiments demonstrate
that Edit Everything facilitates the implementation of the visual aspects of
Stable Diffusion with the use of Segment Anything model and CLIP. Our system is
publicly available at https://github.com/DefengXie/Edit_Everything.",None,-1
CEN-HDR: Computationally Efficient neural Network for real-time High Dynamic Range imaging,0.149544,"High dynamic range (HDR) imaging is still a challenging task in modern
digital photography. Recent research proposes solutions that provide
high-quality acquisition but at the cost of a very large number of operations
and a slow inference time that prevent the implementation of these solutions on
lightweight real-time systems. In this paper, we propose CEN-HDR, a new
computationally efficient neural network by providing a novel architecture
based on a light attention mechanism and sub-pixel convolution operations for
real-time HDR imaging. We also provide an efficient training scheme by applying
network compression using knowledge distillation. We performed extensive
qualitative and quantitative comparisons to show that our approach produces
competitive results in image quality while being faster than state-of-the-art
solutions, allowing it to be practically deployed under real-time constraints.
Experimental results show our method obtains a score of 43.04 mu-PSNR on the
Kalantari2017 dataset with a framerate of 33 FPS using a Macbook M1 NPU.",None,-1
Document Flattening: Beyond Concatenating Context for Document-Level Neural Machine Translation,0.901852,"Existing work in document-level neural machine translation commonly
concatenates several consecutive sentences as a pseudo-document, and then
learns inter-sentential dependencies. This strategy limits the model's ability
to leverage information from distant context. We overcome this limitation with
a novel Document Flattening (DocFlat) technique that integrates Flat-Batch
Attention (FBA) and Neural Context Gate (NCG) into Transformer model to utilize
information beyond the pseudo-document boundaries. FBA allows the model to
attend to all the positions in the batch and learns the relationships between
positions explicitly and NCG identifies the useful information from the distant
context. We conduct comprehensive experiments and analyses on three benchmark
datasets for English-German translation, and validate the effectiveness of two
variants of DocFlat. Empirical results show that our approach outperforms
strong baselines with statistical significance on BLEU, COMET and accuracy on
the contrastive test set. The analyses highlight that DocFlat is highly
effective in capturing the long-range information.",None,-1
Counting Crowds in Bad Weather,0.683705,"Crowd counting has recently attracted significant attention in the field of
computer vision due to its wide applications to image understanding. Numerous
methods have been proposed and achieved state-of-the-art performance for
real-world tasks. However, existing approaches do not perform well under
adverse weather such as haze, rain, and snow since the visual appearances of
crowds in such scenes are drastically different from those images in clear
weather of typical datasets. In this paper, we propose a method for robust
crowd counting in adverse weather scenarios. Instead of using a two-stage
approach that involves image restoration and crowd counting modules, our model
learns effective features and adaptive queries to account for large appearance
variations. With these weather queries, the proposed model can learn the
weather information according to the degradation of the input image and
optimize with the crowd counting module simultaneously. Experimental results
show that the proposed algorithm is effective in counting crowds under
different weather types on benchmark datasets. The source code and trained
models will be made available to the public.",None,-1
ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,0.770769,"ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods.",None,-1
Slot-VAE: Object-Centric Scene Generation with Slot Attention,0.651369,"Slot attention has shown remarkable object-centric representation learning
performance in computer vision tasks without requiring any supervision. Despite
its object-centric binding ability brought by compositional modelling, as a
deterministic module, slot attention lacks the ability to generate novel
scenes. In this paper, we propose the Slot-VAE, a generative model that
integrates slot attention with the hierarchical VAE framework for
object-centric structured scene generation. For each image, the model
simultaneously infers a global scene representation to capture high-level scene
structure and object-centric slot representations to embed individual object
components. During generation, slot representations are generated from the
global scene representation to ensure coherent scene structures. Our extensive
evaluation of the scene generation ability indicates that Slot-VAE outperforms
slot representation-based generative baselines in terms of sample quality and
scene structure accuracy.",None,-1
LDFA: Latent Diffusion Face Anonymization for Self-driving Applications,0.419717,"In order to protect vulnerable road users (VRUs), such as pedestrians or
cyclists, it is essential that intelligent transportation systems (ITS)
accurately identify them. Therefore, datasets used to train perception models
of ITS must contain a significant number of vulnerable road users. However,
data protection regulations require that individuals are anonymized in such
datasets. In this work, we introduce a novel deep learning-based pipeline for
face anonymization in the context of ITS. In contrast to related methods, we do
not use generative adversarial networks (GANs) but build upon recent advances
in diffusion models. We propose a two-stage method, which contains a face
detection model followed by a latent diffusion model to generate realistic face
in-paintings. To demonstrate the versatility of anonymized images, we train
segmentation methods on anonymized data and evaluate them on non-anonymized
data. Our experiment reveal that our pipeline is better suited to anonymize
data for segmentation than naive methods and performes comparably with recent
GAN-based methods. Moreover, face detectors achieve higher mAP scores for faces
anonymized by our method compared to naive or recent GAN-based methods.",None,-1
Mimicking the Thinking Process for Emotion Recognition in Conversation with Prompts and Paraphrasing,0.797573,"Emotion recognition in conversation, which aims to predict the emotion for
all utterances, has attracted considerable research attention in recent years.
It is a challenging task since the recognition of the emotion in one utterance
involves many complex factors, such as the conversational context, the
speaker's background, and the subtle difference between emotion labels. In this
paper, we propose a novel framework which mimics the thinking process when
modeling these factors. Specifically, we first comprehend the conversational
context with a history-oriented prompt to selectively gather information from
predecessors of the target utterance. We then model the speaker's background
with an experience-oriented prompt to retrieve the similar utterances from all
conversations. We finally differentiate the subtle label semantics with a
paraphrasing mechanism to elicit the intrinsic label related knowledge. We
conducted extensive experiments on three benchmarks. The empirical results
demonstrate the superiority of our proposed framework over the state-of-the-art
baselines.",None,-1
Question Decomposition Tree for Answering Complex Questions over Knowledge Bases,0.633472,"Knowledge base question answering (KBQA) has attracted a lot of interest in
recent years, especially for complex questions which require multiple facts to
answer. Question decomposition is a promising way to answer complex questions.
Existing decomposition methods split the question into sub-questions according
to a single compositionality type, which is not sufficient for questions
involving multiple compositionality types. In this paper, we propose Question
Decomposition Tree (QDT) to represent the structure of complex questions.
Inspired by recent advances in natural language generation (NLG), we present a
two-staged method called Clue-Decipher to generate QDT. It can leverage the
strong ability of NLG model and simultaneously preserve the original questions.
To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA
system called QDTQA. Extensive experiments show that QDTQA outperforms previous
state-of-the-art methods on ComplexWebQuestions dataset. Besides, our
decomposition method improves an existing KBQA system by 12% and sets a new
state-of-the-art on LC-QuAD 1.0.",None,-1
S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction,0.637307,"In this work we introduce S-TREK, a novel local feature extractor that
combines a deep keypoint detector, which is both translation and rotation
equivariant by design, with a lightweight deep descriptor extractor. We train
the S-TREK keypoint detector within a framework inspired by reinforcement
learning, where we leverage a sequential procedure to maximize a reward
directly related to keypoint repeatability. Our descriptor network is trained
following a ""detect, then describe"" approach, where the descriptor loss is
evaluated only at those locations where keypoints have been selected by the
already trained detector. Extensive experiments on multiple benchmarks confirm
the effectiveness of our proposed method, with S-TREK often outperforming other
state-of-the-art methods in terms of repeatability and quality of the recovered
poses, especially when dealing with in-plane rotations.",None,-1
Self-supervised representations in speech-based depression detection,0.972758,"This paper proposes handling training data sparsity in speech-based automatic
depression detection (SDD) using foundation models pre-trained with
self-supervised learning (SSL). An analysis of SSL representations derived from
different layers of pre-trained foundation models is first presented for SDD,
which provides insight to suitable indicator for depression detection.
Knowledge transfer is then performed from automatic speech recognition (ASR)
and emotion recognition to SDD by fine-tuning the foundation models. Results
show that the uses of oracle and ASR transcriptions yield similar SDD
performance when the hidden representations of the ASR model is incorporated
along with the ASR textual information. By integrating representations from
multiple foundation models, state-of-the-art SDD results based on real ASR were
achieved on the DAIC-WOZ dataset.",None,-1
Fast Matrix Multiplication Without Tears: A Constraint Programming Approach,0.178464,"It is known that the multiplication of an $N \times M$ matrix with an $M
\times P$ matrix can be performed using fewer multiplications than what the
naive $NMP$ approach suggests. The most famous instance of this is Strassen's
algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8
multiplications. This gives rise to the constraint satisfaction problem of fast
matrix multiplication, where a set of $R < NMP$ multiplication terms must be
chosen and combined such that they satisfy correctness constraints on the
output matrix. Despite its highly combinatorial nature, this problem has not
been exhaustively examined from that perspective, as evidenced for example by
the recent deep reinforcement learning approach of AlphaTensor. In this work,
we propose a simple yet novel Constraint Programming approach to find
non-commutative algorithms for fast matrix multiplication or provide proof of
infeasibility otherwise. We propose a set of symmetry-breaking constraints and
valid inequalities that are particularly helpful in proving infeasibility. On
the feasible side, we find that exploiting solver performance variability in
conjunction with a sparsity-based problem decomposition enables finding
solutions for larger (feasible) instances of fast matrix multiplication. Our
experimental results using CP Optimizer demonstrate that we can find fast
matrix multiplication algorithms for matrices up to $3\times 3$ in a short
amount of time.",None,-1
PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents,0.214524,"Strategies such as chain-of-thought prompting improve the performance of
large language models (LLMs) on complex reasoning tasks by decomposing input
examples into intermediate steps. However, it remains unclear how to apply such
methods to reason over long input documents, in which both the decomposition
and the output of each intermediate step are non-trivial to obtain. In this
work, we propose PEARL, a prompting framework to improve reasoning over long
documents, which consists of three stages: action mining, plan formulation, and
plan execution. More specifically, given a question about a long document,
PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,
FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain
the answer. Each stage of PEARL is implemented via zero-shot or few-shot
prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate
PEARL on a challenging subset of the QuALITY dataset, which contains questions
that require complex reasoning over long narrative texts. PEARL outperforms
zero-shot and chain-of-thought prompting on this dataset, and ablation
experiments show that each stage of PEARL is critical to its performance.
Overall, PEARL is a first step towards leveraging LLMs to reason over long
documents.",None,-1
Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised Style Extractor and Hierarchical Modeling in Speech Synthesis,0.542959,"Cross-speaker style transfer in speech synthesis aims at transferring a style
from source speaker to synthesized speech of a target speaker's timbre. In most
previous methods, the synthesized fine-grained prosody features often represent
the source speaker's average style, similar to the one-to-many problem(i.e.,
multiple prosody variations correspond to the same text). In response to this
problem, a strength-controlled semi-supervised style extractor is proposed to
disentangle the style from content and timbre, improving the representation and
interpretability of the global style embedding, which can alleviate the
one-to-many mapping and data imbalance problems in prosody prediction. A
hierarchical prosody predictor is proposed to improve prosody modeling. We find
that better style transfer can be achieved by using the source speaker's
prosody features that are easily predicted. Additionally, a
speaker-transfer-wise cycle consistency loss is proposed to assist the model in
learning unseen style-timbre combinations during the training phase.
Experimental results show that the method outperforms the baseline. We provide
a website with audio samples.",None,-1
Large language models in medicine: the potentials and pitfalls,0.505043,"Large language models (LLMs) have been applied to tasks in healthcare,
ranging from medical exam questions to responding to patient questions. With
increasing institutional partnerships between companies producing LLMs and
healthcare systems, real world clinical application is coming closer to
reality. As these models gain traction, it is essential for healthcare
practitioners to understand what LLMs are, their development, their current and
potential applications, and the associated pitfalls when utilized in medicine.
This review and accompanying tutorial aim to give an overview of these topics
to aid healthcare practitioners in understanding the rapidly changing landscape
of LLMs as applied to medicine.",None,-1
Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses,0.917285,"In the field of Japanese-Chinese translation linguistics, the issue of
correctly translating attributive clauses has persistently proven to be
challenging. Present-day machine translation tools often fail to accurately
translate attributive clauses from Japanese to Chinese. In light of this, this
paper investigates the linguistic problem underlying such difficulties, namely
how does the semantic role of the modified noun affect the selection of
translation patterns for attributive clauses, from a linguistic perspective. To
ad-dress these difficulties, a pre-edit scheme is proposed, which aims to
enhance the accuracy of translation. Furthermore, we propose a novel two-step
prompt strategy, which combines this pre-edit scheme with ChatGPT, currently
the most widely used large language model. This prompt strategy is capable of
optimizing translation input in zero-shot scenarios and has been demonstrated
to improve the average translation accuracy score by over 35%.",None,-1
Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter,0.180921,"Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a
promising paradigm for institutions with limited language resources. This
approach allows multiple institutions to act as clients and train a unified
model through model synchronization, rather than collecting sensitive data for
centralized training. This significantly reduces the cost of corpus collection
and preserves data privacy. However, as pre-trained language models (PLMs)
continue to increase in size, the communication cost for transmitting
parameters during synchronization has become a training speed bottleneck. In
this paper, we propose a communication-efficient Fed-MNMT framework that
addresses this issue by keeping PLMs frozen and only transferring lightweight
adapter modules between clients. Since different language pairs exhibit
substantial discrepancies in data distributions, adapter parameters of clients
may conflict with each other. To tackle this, we explore various clustering
strategies to group parameters for integration and mitigate the negative
effects of conflicting parameters. Experimental results demonstrate that our
framework reduces communication cost by over 98% while achieving similar or
even better performance compared to competitive baselines. Further analysis
reveals that clustering strategies effectively solve the problem of linguistic
discrepancy and pruning adapter modules further improves communication
efficiency.",None,-1
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers,0.547752,"Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element's contribution to the prediction
results.",None,-1
A Game-Theoretic Framework for Joint Forecasting and Planning,0.874188,"Planning safe robot motions in the presence of humans requires reliable
forecasts of future human motion. However, simply predicting the most likely
motion from prior interactions does not guarantee safety. Such forecasts fail
to model the long tail of possible events, which are rarely observed in limited
datasets. On the other hand, planning for worst-case motions leads to overtly
conservative behavior and a ""frozen robot"". Instead, we aim to learn forecasts
that predict counterfactuals that humans guard against. We propose a novel
game-theoretic framework for joint planning and forecasting with the payoff
being the performance of the planner against the demonstrator, and present
practical algorithms to train models in an end-to-end fashion. We demonstrate
that our proposed algorithm results in safer plans in a crowd navigation
simulator and real-world datasets of pedestrian motion. We release our code at
https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.",None,-1
Student Classroom Behavior Detection based on YOLOv7-BRA and Multi-Model Fusion,0.296422,"Accurately detecting student behavior in classroom videos can aid in
analyzing their classroom performance and improving teaching effectiveness.
However, the current accuracy rate in behavior detection is low. To address
this challenge, we propose the Student Classroom Behavior Detection system
based on based on YOLOv7-BRA (YOLOv7 with Bi-level Routing Attention ). We
identified eight different behavior patterns, including standing, sitting,
speaking, listening, walking, raising hands, reading, and writing. We
constructed a dataset, which contained 11,248 labels and 4,001 images, with an
emphasis on the common behavior of raising hands in a classroom setting
(Student Classroom Behavior dataset, SCB-Dataset). To improve detection
accuracy, we added the biformer attention module to the YOLOv7 network.
Finally, we fused the results from YOLOv7 CrowdHuman, SlowFast, and DeepSort
models to obtain student classroom behavior data. We conducted experiments on
the SCB-Dataset, and YOLOv7-BRA achieved an mAP@0.5 of 87.1%, resulting in a
2.2% improvement over previous results. Our SCB-dataset can be downloaded from:
https://github.com/Whiffe/SCB-datase",None,-1
Large Language Models and Explainable Law: a Hybrid Methodology,0.160989,"The paper advocates for LLMs to enhance the accessibility, usage and
explainability of rule-based legal systems, contributing to a democratic and
stakeholder-oriented view of legal technology. A methodology is developed to
explore the potential use of LLMs for translating the explanations produced by
rule-based systems, from high-level programming languages to natural language,
allowing all users a fast, clear, and accessible interaction with such
technologies. The study continues by building upon these explanations to
empower laypeople with the ability to execute complex juridical tasks on their
own, using a Chain of Prompts for the autonomous legal comparison of different
rule-based inferences, applied to the same factual case.",None,-1
Pruning Pre-trained Language Models with Principled Importance and Self-regularization,0.0447912,"Iterative pruning is one of the most effective compression methods for
pre-trained language models. We discovered that finding the optimal pruning
decision is an equality-constrained 0-1 Integer Linear Programming problem. The
solution to this optimization problem leads to a principled importance
criterion which we use to rank parameters during iterative model pruning. To
mitigate the poor generalization at high sparsity levels, we propose a
self-regularization scheme where model prediction is regularized by the latest
checkpoint with increasing sparsity throughout pruning. Our experiments on
natural language understanding, question-answering, named entity recognition,
and data-to-text generation with various Transformer-based PLMs show the
effectiveness of the approach at various sparsity levels.",None,-1
Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance,0.239742,"Retrieval augmentation enhances performance of traditional language models by
incorporating additional context. However, the computational demands for
retrieval augmented large language models (LLMs) pose a challenge when applying
them to real-time tasks, such as composition assistance. To address this
limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG)
framework, a novel approach that efficiently combines a cloud-based LLM with a
smaller, client-side, language model through retrieval augmented memory. This
integration enables the client model to generate effective responses,
benefiting from the LLM's capabilities and contextual information.
Additionally, through an asynchronous memory update mechanism, the client model
can deliver real-time completions swiftly to user inputs without the need to
wait for responses from the cloud. Our experiments on five benchmark datasets
demonstrate that HybridRAG significantly improves utility over client-only
models while maintaining low latency.",None,-1
When 3D Bounding-Box Meets SAM: Point Cloud Instance Segmentation with Weak-and-Noisy Supervision,0.628217,"Learning from bounding-boxes annotations has shown great potential in
weakly-supervised 3D point cloud instance segmentation. However, we observed
that existing methods would suffer severe performance degradation with
perturbed bounding box annotations. To tackle this issue, we propose a
complementary image prompt-induced weakly-supervised point cloud instance
segmentation (CIP-WPIS) method. CIP-WPIS leverages pretrained knowledge
embedded in the 2D foundation model SAM and 3D geometric prior to achieve
accurate point-wise instance labels from the bounding box annotations.
Specifically, CP-WPIS first selects image views in which 3D candidate points of
an instance are fully visible. Then, we generate complementary background and
foreground prompts from projections to obtain SAM 2D instance mask predictions.
According to these, we assign the confidence values to points indicating the
likelihood of points belonging to the instance. Furthermore, we utilize 3D
geometric homogeneity provided by superpoints to decide the final instance
label assignments. In this fashion, we achieve high-quality 3D point-wise
instance labels. Extensive experiments on both Scannet-v2 and S3DIS benchmarks
demonstrate that our method is robust against noisy 3D bounding-box annotations
and achieves state-of-the-art performance.",None,-1
DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models,0.185887,"In this study, we aim to extend the capabilities of diffusion-based
text-to-image (T2I) generation models by incorporating diverse modalities
beyond textual description, such as sketch, box, color palette, and style
embedding, within a single model. We thus design a multimodal T2I diffusion
model, coined as DiffBlender, by separating the channels of conditions into
three types, i.e., image forms, spatial tokens, and non-spatial tokens. The
unique architecture of DiffBlender facilitates adding new input modalities,
pioneering a scalable framework for conditional image generation. Notably, we
achieve this without altering the parameters of the existing generative model,
Stable Diffusion, only with updating partial components. Our study establishes
new benchmarks in multimodal generation through quantitative and qualitative
comparisons with existing conditional generation methods. We demonstrate that
DiffBlender faithfully blends all the provided information and showcase its
various applications in the detailed image synthesis.",None,-1
Recovering from Privacy-Preserving Masking with Large Language Models,0.497833,"Model adaptation is crucial to handle the discrepancy between proxy training
data and actual users data received. To effectively perform adaptation, textual
data of users is typically stored on servers or their local devices, where
downstream natural language processing (NLP) models can be directly trained
using such in-domain data. However, this might raise privacy and security
concerns due to the extra risks of exposing user information to adversaries.
Replacing identifying information in textual data with a generic marker has
been recently explored. In this work, we leverage large language models (LLMs)
to suggest substitutes of masked tokens and have their effectiveness evaluated
on downstream language modeling tasks. Specifically, we propose multiple
pre-trained and fine-tuned LLM-based approaches and perform empirical studies
on various datasets for the comparison of these methods. Experimental results
show that models trained on the obfuscation corpora are able to achieve
comparable performance with the ones trained on the original data without
privacy-preserving token masking.",None,-1
Exploring Continual Learning of Diffusion Models,0.316374,"Diffusion models have achieved remarkable success in generating high-quality
images thanks to their novel training procedures applied to unprecedented
amounts of data. However, training a diffusion model from scratch is
computationally expensive. This highlights the need to investigate the
possibility of training these models iteratively, reusing computation while the
data distribution changes. In this study, we take the first step in this
direction and evaluate the continual learning (CL) properties of diffusion
models. We begin by benchmarking the most common CL methods applied to
Denoising Diffusion Probabilistic Models (DDPMs), where we note the strong
performance of the experience replay with the reduced rehearsal coefficient.
Furthermore, we provide insights into the dynamics of forgetting, which exhibit
diverse behavior across diffusion timesteps. We also uncover certain pitfalls
of using the bits-per-dimension metric for evaluating CL.",None,-1
RADIO: Reference-Agnostic Dubbing Video Synthesis,0.193295,"One of the most challenging problems in audio-driven talking head generation
is achieving high-fidelity detail while ensuring precise synchronization. Given
only a single reference image, extracting meaningful identity attributes
becomes even more challenging, often causing the network to mirror the facial
and lip structures too closely. To address these issues, we introduce RADIO, a
framework engineered to yield high-quality dubbed videos regardless of the pose
or expression in reference images. The key is to modulate the decoder layers
using latent space composed of audio and reference features. Additionally, we
incorporate ViT blocks into the decoder to emphasize high-fidelity details,
especially in the lip region. Our experimental results demonstrate that RADIO
displays high synchronization without the loss of fidelity. Especially in harsh
scenarios where the reference frame deviates significantly from the ground
truth, our method outperforms state-of-the-art methods, highlighting its
robustness.",None,-1
Entity Tracking in Language Models,0.39283,"Keeping track of how states of entities change as a text or dialog unfolds is
a key prerequisite to discourse understanding. Yet, there have been few
systematic investigations into the ability of large language models (LLMs) to
track discourse entities. In this work, we present a task probing to what
extent a language model can infer the final state of an entity given an English
description of the initial state and a series of state-changing operations. We
use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track
the state of entities, and find that only GPT-3.5 models, which have been
pretrained on large amounts of code, exhibit this ability. We then investigate
whether smaller models pretrained primarily on text can learn to track
entities, through finetuning T5 on several training/evaluation splits. While
performance degrades for more complex splits, we find that even when evaluated
on a different set of entities from training or longer operation sequences, a
finetuned model can perform non-trivial entity tracking. Taken together, these
results suggest that language models can learn to track entities but
pretraining on text corpora alone does not make this capacity surface.",None,-1
Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement,0.249498,"We commonly use agreement measures to assess the utility of judgements made
by human annotators in Natural Language Processing (NLP) tasks. While
inter-annotator agreement is frequently used as an indication of label
reliability by measuring consistency between annotators, we argue for the
additional use of intra-annotator agreement to measure label stability over
time. However, in a systematic review, we find that the latter is rarely
reported in this field. Calculating these measures can act as important quality
control and provide insights into why annotators disagree. We propose
exploratory annotation experiments to investigate the relationships between
these measures and perceptions of subjectivity and ambiguity in text items.",None,-1
Elementary Sets for Logic Programs,0.390698,"By introducing the concepts of a loop and a loop formula, Lin and Zhao showed
that the answer sets of a nondisjunctive logic program are exactly the models
of its Clark's completion that satisfy the loop formulas of all loops.
Recently, Gebser and Schaub showed that the Lin-Zhao theorem remains correct
even if we restrict loop formulas to a special class of loops called
``elementary loops.'' In this paper, we simplify and generalize the notion of
an elementary loop, and clarify its role. We propose the notion of an
elementary set, which is almost equivalent to the notion of an elementary loop
for nondisjunctive programs, but is simpler, and, unlike elementary loops, can
be extended to disjunctive programs without producing unintuitive results. We
show that the maximal unfounded elementary sets for the ``relevant'' part of a
program are exactly the minimal sets among the nonempty unfounded sets. We also
present a graph-theoretic characterization of elementary sets for
nondisjunctive programs, which is simpler than the one proposed in (Gebser &
Schaub 2005). Unlike the case of nondisjunctive programs, we show that the
problem of deciding an elementary set is coNP-complete for disjunctive
programs.",None,-1
Garment Recovery with Shape and Deformation Priors,0.581501,"While modeling people wearing tight-fitting clothing has made great strides
in recent years, loose-fitting clothing remains a challenge. We propose a
method that delivers realistic garment models from real-world images,
regardless of garment shape or deformation. To this end, we introduce a fitting
approach that utilizes shape and deformation priors learned from synthetic data
to accurately capture garment shapes and deformations, including large ones.
Not only does our approach recover the garment geometry accurately, it also
yields models that can be directly used by downstream applications such as
animation and simulation.",None,-1
Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?,0.0813632,"Compositionality is a pivotal property of symbolic reasoning. However, how
well recent neural models capture compositionality remains underexplored in the
symbolic reasoning tasks. This study empirically addresses this question by
systematically examining recently published pre-trained seq2seq models with a
carefully controlled dataset of multi-hop arithmetic symbolic reasoning. We
introduce a skill tree on compositionality in arithmetic symbolic reasoning
that defines the hierarchical levels of complexity along with three
compositionality dimensions: systematicity, productivity, and substitutivity.
Our experiments revealed that among the three types of composition, the models
struggled most with systematicity, performing poorly even with relatively
simple compositions. That difficulty was not resolved even after training the
models with intermediate reasoning steps.",None,-1
STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection,0.993273,"Recently, deep learning-based facial landmark detection has achieved
significant improvement. However, the semantic ambiguity problem degrades
detection performance. Specifically, the semantic ambiguity causes inconsistent
annotation and negatively affects the model's convergence, leading to worse
accuracy and instability prediction. To solve this problem, we propose a
Self-adapTive Ambiguity Reduction (STAR) loss by exploiting the properties of
semantic ambiguity. We find that semantic ambiguity results in the anisotropic
predicted distribution, which inspires us to use predicted distribution to
represent semantic ambiguity. Based on this, we design the STAR loss that
measures the anisotropism of the predicted distribution. Compared with the
standard regression loss, STAR loss is encouraged to be small when the
predicted distribution is anisotropic and thus adaptively mitigates the impact
of semantic ambiguity. Moreover, we propose two kinds of eigenvalue restriction
methods that could avoid both distribution's abnormal change and the model's
premature convergence. Finally, the comprehensive experiments demonstrate that
STAR loss outperforms the state-of-the-art methods on three benchmarks, i.e.,
COFW, 300W, and WFLW, with negligible computation overhead. Code is at
https://github.com/ZhenglinZhou/STAR.",None,-1
Probabilistic Circuits That Know What They Don't Know,0.665424,"Probabilistic circuits (PCs) are models that allow exact and tractable
probabilistic inference. In contrast to neural networks, they are often assumed
to be well-calibrated and robust to out-of-distribution (OOD) data. In this
paper, we show that PCs are in fact not robust to OOD data, i.e., they don't
know what they don't know. We then show how this challenge can be overcome by
model uncertainty quantification. To this end, we propose tractable dropout
inference (TDI), an inference procedure to estimate uncertainty by deriving an
analytical solution to Monte Carlo dropout (MCD) through variance propagation.
Unlike MCD in neural networks, which comes at the cost of multiple network
evaluations, TDI provides tractable sampling-free uncertainty estimates in a
single forward pass. TDI improves the robustness of PCs to distribution shift
and OOD data, demonstrated through a series of experiments evaluating the
classification confidence and uncertainty estimates on real-world data.",None,-1
gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction,0.714349,"Signed distance functions (SDFs) is an attractive framework that has recently
shown promising results for 3D shape reconstruction from images. SDFs
seamlessly generalize to different shape resolutions and topologies but lack
explicit modelling of the underlying 3D geometry. In this work, we exploit the
hand structure and use it as guidance for SDF-based shape reconstruction. In
particular, we address reconstruction of hands and manipulated objects from
monocular RGB images. To this end, we estimate poses of hands and objects and
use them to guide 3D reconstruction. More specifically, we predict kinematic
chains of pose transformations and align SDFs with highly-articulated hand
poses. We improve the visual features of 3D points with geometry alignment and
further leverage temporal information to enhance the robustness to occlusion
and motion blurs. We conduct extensive experiments on the challenging ObMan and
DexYCB benchmarks and demonstrate significant improvements of the proposed
method over the state of the art.",None,-1
"Large Language Models for Scientific Synthesis, Inference and Explanation",0.999967,"Large language models are a form of artificial intelligence systems whose
primary knowledge consists of the statistical patterns, semantic relationships,
and syntactical structures of language1. Despite their limited forms of
""knowledge"", these systems are adept at numerous complex tasks including
creative writing, storytelling, translation, question-answering, summarization,
and computer code generation. However, they have yet to demonstrate advanced
applications in natural science. Here we show how large language models can
perform scientific synthesis, inference, and explanation. We present a method
for using general-purpose large language models to make inferences from
scientific datasets of the form usually associated with special-purpose machine
learning algorithms. We show that the large language model can augment this
""knowledge"" by synthesizing from the scientific literature. When a conventional
machine learning system is augmented with this synthesized and inferred
knowledge it can outperform the current state of the art across a range of
benchmark tasks for predicting molecular properties. This approach has the
further advantage that the large language model can explain the machine
learning system's predictions. We anticipate that our framework will open new
avenues for AI to accelerate the pace of scientific discovery.",None,-1
Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target,0.0592906,"Spoken Language Understanding (SLU) is a task that aims to extract semantic
information from spoken utterances. Previous research has made progress in
end-to-end SLU by using paired speech-text data, such as pre-trained Automatic
Speech Recognition (ASR) models or paired text as intermediate targets.
However, acquiring paired transcripts is expensive and impractical for
unwritten languages. On the other hand, Textless SLU extracts semantic
information from speech without utilizing paired transcripts. However, the
absence of intermediate targets and training guidance for textless SLU often
results in suboptimal performance. In this work, inspired by the
content-disentangled discrete units from self-supervised speech models, we
proposed to use discrete units as intermediate guidance to improve textless SLU
performance. Our method surpasses the baseline method on five SLU benchmark
corpora. Additionally, we find that unit guidance facilitates few-shot learning
and enhances the model's ability to handle noise.",None,-1
Diffusion Models for Interferometric Satellite Aperture Radar,0.834891,"Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.",None,-1
Hate Speech Targets Detection in Parler using BERT,0.337135,"Online social networks have become a fundamental component of our everyday
life. Unfortunately, these platforms are also a stage for hate speech. Popular
social networks have regularized rules against hate speech. Consequently,
social networks like Parler and Gab advocating and claiming to be free speech
platforms have evolved. These platforms have become a district for hate speech
against diverse targets. We present in our paper a pipeline for detecting hate
speech and its targets and use it for creating Parler hate targets'
distribution. The pipeline consists of two models; one for hate speech
detection and the second for target classification, both based on BERT with
Back-Translation and data pre-processing for improved results. The source code
used in this work, as well as other relevant sources, are available at:
https://github.com/NadavSc/HateRecognition.git",None,-1
Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation,0.556283,"In today's digital era, the rapid spread of misinformation poses threats to
public well-being and societal trust. As online misinformation proliferates,
manual verification by fact checkers becomes increasingly challenging. We
introduce FACT-GPT (Fact-checking Augmentation with Claim matching
Task-oriented Generative Pre-trained Transformer), a framework designed to
automate the claim matching phase of fact-checking using Large Language Models
(LLMs). This framework identifies new social media content that either supports
or contradicts claims previously debunked by fact-checkers. Our approach
employs GPT-4 to generate a labeled dataset consisting of simulated social
media posts. This data set serves as a training ground for fine-tuning more
specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media
content related to public health. The results indicate that our fine-tuned LLMs
rival the performance of larger pre-trained LLMs in claim matching tasks,
aligning closely with human annotations. This study achieves three key
milestones: it provides an automated framework for enhanced fact-checking;
demonstrates the potential of LLMs to complement human expertise; offers public
resources, including datasets and models, to further research and applications
in the fact-checking domain.",None,-1
DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition,0.594126,"Graph Convolutional Networks (GCNs) have been widely used in skeleton-based
human action recognition. In GCN-based methods, the spatio-temporal graph is
fundamental for capturing motion patterns. However, existing approaches ignore
the physical dependency and synchronized spatio-temporal correlations between
joints, which limits the representation capability of GCNs. To solve these
problems, we construct the directed diffusion graph for action modeling and
introduce the activity partition strategy to optimize the weight sharing
mechanism of graph convolution kernels. In addition, we present the
spatio-temporal synchronization encoder to embed synchronized spatio-temporal
semantics. Finally, we propose Directed Diffusion Graph Convolutional Network
(DD-GCN) for action recognition, and the experiments on three public datasets:
NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art
performance of our method.",None,-1
Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8,0.937686,"This paper introduces a software architecture for real-time object detection
using machine learning (ML) in an augmented reality (AR) environment. Our
approach uses the recent state-of-the-art YOLOv8 network that runs onboard on
the Microsoft HoloLens 2 head-mounted display (HMD). The primary motivation
behind this research is to enable the application of advanced ML models for
enhanced perception and situational awareness with a wearable, hands-free AR
platform. We show the image processing pipeline for the YOLOv8 model and the
techniques used to make it real-time on the resource-limited edge computing
platform of the headset. The experimental results demonstrate that our solution
achieves real-time processing without needing offloading tasks to the cloud or
any other external servers while retaining satisfactory accuracy regarding the
usual mAP metric and measured qualitative performance",None,-1
DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices,0.75417,"Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
https://github.com/ismailnejjar/DARE-GRAM.",None,-1
VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking,0.999998,"3D object detectors usually rely on hand-crafted proxies, e.g., anchors or
centers, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel
features need to be densified and processed by dense prediction heads, which
inevitably costs extra computation. In this paper, we instead propose VoxelNext
for fully sparse 3D object detection. Our core insight is to predict objects
directly based on sparse voxel features, without relying on hand-crafted
proxies. Our strong sparse convolutional network VoxelNeXt detects and tracks
3D objects through voxel features entirely. It is an elegant and efficient
framework, with no need for sparse-to-dense conversion or NMS post-processing.
Our method achieves a better speed-accuracy trade-off than other mainframe
detectors on the nuScenes dataset. For the first time, we show that a fully
sparse voxel-based representation works decently for LIDAR 3D object detection
and tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2
benchmarks validate the effectiveness of our approach. Without bells and
whistles, our model outperforms all existing LIDAR methods on the nuScenes
tracking test benchmark.",None,-1
IvyGPT: InteractiVe Chinese pathwaY language model in medical domain,0.213911,"General large language models (LLMs) such as ChatGPT have shown remarkable
success. However, such LLMs have not been widely adopted for medical purposes,
due to poor accuracy and inability to provide medical advice. We propose
IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality
medical question-answer (QA) instances and Reinforcement Learning from Human
Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn
conversation capabilities, but it cannot perform like a doctor in other
aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output
richer diagnosis and treatment answers that are closer to human. In the
training, we used QLoRA to train 33 billion parameters on a small number of
NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed
other medical GPT models.",None,-1
"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety",0.151271,"Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.",None,-1
Language Models Trained on Media Diets Can Predict Public Opinion,0.880255,"Public opinion reflects and shapes societal behavior, but the traditional
survey-based tools to measure it are limited. We introduce a novel approach to
probe media diet models -- language models adapted to online news, TV
broadcast, or radio show content -- that can emulate the opinions of
subpopulations that have consumed a set of media. To validate this method, we
use as ground truth the opinions expressed in U.S. nationally representative
surveys on COVID-19 and consumer confidence. Our studies indicate that this
approach is (1) predictive of human judgements found in survey response
distributions and robust to phrasing and channels of media exposure, (2) more
accurate at modeling people who follow media more closely, and (3) aligned with
literature on which types of opinions are affected by media consumption.
Probing language models provides a powerful new method for investigating media
effects, has practical applications in supplementing polls and forecasting
public opinion, and suggests a need for further study of the surprising
fidelity with which neural language models can predict human responses.",None,-1
Dynamic Large Language Models on Blockchains,0.131313,"Training and deploying the large language models requires a large mount of
computational resource because the language models contain billions of
parameters and the text has thousands of tokens. Another problem is that the
large language models are static. They are fixed after the training process. To
tackle these issues, in this paper, we propose to train and deploy the dynamic
large language model on blockchains, which have high computation performance
and are distributed across a network of computers. A blockchain is a secure,
decentralized, and transparent system that allows for the creation of a
tamper-proof ledger for transactions without the need for intermediaries. The
dynamic large language models can continuously learn from the user input after
the training process. Our method provides a new way to develop the large
language models and also sheds a light on the next generation artificial
intelligence systems.",None,-1
TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,0.301368,"Deep saliency prediction algorithms complement the object recognition
features, they typically rely on additional information, such as scene context,
semantic relationships, gaze direction, and object dissimilarity. However, none
of these models consider the temporal nature of gaze shifts during image
observation. We introduce a novel saliency prediction model that learns to
output saliency maps in sequential time intervals by exploiting human temporal
attention patterns. Our approach locally modulates the saliency predictions by
combining the learned temporal maps. Our experiments show that our method
outperforms the state-of-the-art models, including a multi-duration saliency
model, on the SALICON benchmark. Our code will be publicly available on GitHub.",None,-1
Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,0.206324,"Despite the success of diffusion models (DMs), we still lack a thorough
understanding of their latent space. While image editing with GANs builds upon
latent space, DMs rely on editing the conditions such as text prompts. We
present an unsupervised method to discover interpretable editing directions for
the latent variables $\mathbf{x}_t \in \mathcal{X}$ of DMs. Our method adopts
Riemannian geometry between $\mathcal{X}$ and the intermediate feature maps
$\mathcal{H}$ of the U-Nets to provide a deep understanding over the
geometrical structure of $\mathcal{X}$. The discovered semantic latent
directions mostly yield disentangled attribute changes, and they are globally
consistent across different samples. Furthermore, editing in earlier timesteps
edits coarse attributes, while ones in later timesteps focus on high-frequency
details. We define the curvedness of a line segment between samples to show
that $\mathcal{X}$ is a curved manifold. Experiments on different baselines and
datasets demonstrate the effectiveness of our method even on Stable Diffusion.
Our source code will be publicly available for the future researchers.",None,-1
Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue,0.699011,"Open-domain dialogue system usually requires different sources of knowledge
to generate more informative and evidential responses. However, existing
knowledge-grounded dialogue systems either focus on a single knowledge source
or overlook the dependency between multiple sources of knowledge, which may
result in generating inconsistent or even paradoxical responses. To incorporate
multiple knowledge sources and dependencies between them, we propose SAFARI, a
novel framework that leverages the exceptional capabilities of large language
models (LLMs) in planning, understanding, and incorporating under both
supervised and unsupervised settings. Specifically, SAFARI decouples the
knowledge grounding into multiple sources and response generation, which allows
easy extension to various knowledge sources including the possibility of not
using any sources. To study the problem, we construct a personalized
knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind
\textbf{P}ersona}~(\textbf{KBP}), which is the first to consider the dependency
between persona and implicit knowledge. Experimental results on the KBP dataset
demonstrate that the SAFARI framework can effectively produce
persona-consistent and knowledge-enhanced responses.",None,-1
SSCBench: Monocular 3D Semantic Scene Completion Benchmark in Street Views,0.505358,"Monocular scene understanding is a foundational component of autonomous
systems. Within the spectrum of monocular perception topics, one crucial and
useful task for holistic 3D scene understanding is semantic scene completion
(SSC), which jointly completes semantic information and geometric details from
RGB input. However, progress in SSC, particularly in large-scale street views,
is hindered by the scarcity of high-quality datasets. To address this issue, we
introduce SSCBench, a comprehensive benchmark that integrates scenes from
widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo).
SSCBench follows an established setup and format in the community, facilitating
the easy exploration of SSC methods in various street views. We benchmark
models using monocular, trinocular, and point cloud input to assess the
performance gap resulting from sensor coverage and modality. Moreover, we have
unified semantic labels across diverse datasets to simplify cross-domain
generalization testing. We commit to including more datasets and SSC models to
drive further advancements in this field.",None,-1
Multi-Temporal Lip-Audio Memory for Visual Speech Recognition,0.536859,"Visual Speech Recognition (VSR) is a task to predict a sentence or word from
lip movements. Some works have been recently presented which use audio signals
to supplement visual information. However, existing methods utilize only
limited information such as phoneme-level features and soft labels of Automatic
Speech Recognition (ASR) networks. In this paper, we present a Multi-Temporal
Lip-Audio Memory (MTLAM) that makes the best use of audio signals to complement
insufficient information of lip movements. The proposed method is mainly
composed of two parts: 1) MTLAM saves multi-temporal audio features produced
from short- and long-term audio signals, and the MTLAM memorizes a
visual-to-audio mapping to load stored multi-temporal audio features from
visual features at the inference phase. 2) We design an audio temporal model to
produce multi-temporal audio features capturing the context of neighboring
words. In addition, to construct effective visual-to-audio mapping, the audio
temporal models can generate audio features time-aligned with visual features.
Through extensive experiments, we validate the effectiveness of the MTLAM
achieving state-of-the-art performances on two public VSR datasets.",None,-1
Streaming Punctuation: A Novel Punctuation Technique Leveraging Bidirectional Context for Continuous Speech Recognition,0.354588,"While speech recognition Word Error Rate (WER) has reached human parity for
English, continuous speech recognition scenarios such as voice typing and
meeting transcriptions still suffer from segmentation and punctuation problems,
resulting from irregular pausing patterns or slow speakers. Transformer
sequence tagging models are effective at capturing long bi-directional context,
which is crucial for automatic punctuation. Automatic Speech Recognition (ASR)
production systems, however, are constrained by real-time requirements, making
it hard to incorporate the right context when making punctuation decisions.
Context within the segments produced by ASR decoders can be helpful but
limiting in overall punctuation performance for a continuous speech session. In
this paper, we propose a streaming approach for punctuation or re-punctuation
of ASR output using dynamic decoding windows and measure its impact on
punctuation and segmentation accuracy across scenarios. The new system tackles
over-segmentation issues, improving segmentation F0.5-score by 13.9%. Streaming
punctuation achieves an average BLEUscore improvement of 0.66 for the
downstream task of Machine Translation (MT).",None,-1
Protect Your Prompts: Protocols for IP Protection in LLM Applications,0.359454,"With the rapid adoption of AI in the form of large language models (LLMs),
the potential value of carefully engineered prompts has become significant.
However, to realize this potential, prompts should be tradable on an open
market. Since prompts are, at present, generally economically non-excludable,
by virtue of their nature as text, no general competitive market has yet been
established. This note discusses two protocols intended to provide protection
of prompts, elevating their status as intellectual property, thus confirming
the intellectual property rights of prompt engineers, and potentially
supporting the flourishing of an open market for LLM prompts.",None,-1
Semiconductor Fab Scheduling with Self-Supervised and Reinforcement Learning,0.506828,"Semiconductor manufacturing is a notoriously complex and costly multi-step
process involving a long sequence of operations on expensive and
quantity-limited equipment. Recent chip shortages and their impacts have
highlighted the importance of semiconductors in the global supply chains and
how reliant on those our daily lives are. Due to the investment cost,
environmental impact, and time scale needed to build new factories, it is
difficult to ramp up production when demand spikes.
  This work introduces a method to successfully learn to schedule a
semiconductor manufacturing facility more efficiently using deep reinforcement
and self-supervised learning. We propose the first adaptive scheduling approach
to handle complex, continuous, stochastic, dynamic, modern semiconductor
manufacturing models. Our method outperforms the traditional hierarchical
dispatching strategies typically used in semiconductor manufacturing plants,
substantially reducing each order's tardiness and time until completion. As a
result, our method yields a better allocation of resources in the semiconductor
manufacturing process.",None,-1
Extrinsic Factors Affecting the Accuracy of Biomedical NER,0.375919,"Biomedical named entity recognition (NER) is a critial task that aims to
identify structured information in clinical text, which is often replete with
complex, technical terms and a high degree of variability. Accurate and
reliable NER can facilitate the extraction and analysis of important biomedical
information, which can be used to improve downstream applications including the
healthcare system. However, NER in the biomedical domain is challenging due to
limited data availability, as the high expertise, time, and expenses are
required to annotate its data. In this paper, by using the limited data, we
explore various extrinsic factors including the corpus annotation scheme, data
augmentation techniques, semi-supervised learning and Brill transformation, to
improve the performance of a NER model on a clinical text dataset (i2b2 2012,
\citet{sun-rumshisky-uzuner:2013}). Our experiments demonstrate that these
approaches can significantly improve the model's F1 score from original 73.74
to 77.55. Our findings suggest that considering different extrinsic factors and
combining these techniques is a promising approach for improving NER
performance in the biomedical domain where the size of data is limited.",None,-1
Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,1.0,"We propose a cross-lingual neural codec language model, VALL-E X, for
cross-lingual speech synthesis. Specifically, we extend VALL-E and train a
multi-lingual conditional codec language model to predict the acoustic token
sequences of the target language speech by using both the source language
speech and the target language text as prompts. VALL-E X inherits strong
in-context learning capabilities and can be applied for zero-shot cross-lingual
text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
Experimental results show that it can generate high-quality speech in the
target language via just one speech utterance in the source language as a
prompt while preserving the unseen speaker's voice, emotion, and acoustic
environment. Moreover, VALL-E X effectively alleviates the foreign accent
problems, which can be controlled by a language ID. Audio samples are available
at \url{https://aka.ms/vallex}.",None,-1
Class-Incremental Learning based on Label Generation,0.776989,"Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.",None,-1
A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification,0.101032,"Cross-domain text classification aims to adapt models to a target domain that
lacks labeled data. It leverages or reuses rich labeled data from the different
but related source domain(s) and unlabeled data from the target domain. To this
end, previous work focuses on either extracting domain-invariant features or
task-agnostic features, ignoring domain-aware features that may be present in
the target domain and could be useful for the downstream task. In this paper,
we propose a two-stage framework for cross-domain text classification. In the
first stage, we finetune the model with mask language modeling (MLM) and
labeled data from the source domain. In the second stage, we further fine-tune
the model with self-supervised distillation (SSD) and unlabeled data from the
target domain. We evaluate its performance on a public cross-domain text
classification benchmark and the experiment results show that our method
achieves new state-of-the-art results for both single-source domain adaptations
(94.17% $\uparrow$1.03%) and multi-source domain adaptations (95.09%
$\uparrow$1.34%).",None,-1
HRDoc: Dataset and Baseline Method Toward Hierarchical Reconstruction of Document Structures,0.519773,"The problem of document structure reconstruction refers to converting digital
or scanned documents into corresponding semantic structures. Most existing
works mainly focus on splitting the boundary of each element in a single
document page, neglecting the reconstruction of semantic structure in
multi-page documents. This paper introduces hierarchical reconstruction of
document structures as a novel task suitable for NLP and CV fields. To better
evaluate the system performance on the new task, we built a large-scale dataset
named HRDoc, which consists of 2,500 multi-page documents with nearly 2 million
semantic units. Every document in HRDoc has line-level annotations including
categories and relations obtained from rule-based extractors and human
annotators. Moreover, we proposed an encoder-decoder-based hierarchical
document structure parsing system (DSPS) to tackle this problem. By adopting a
multi-modal bidirectional encoder and a structure-aware GRU decoder with
soft-mask operation, the DSPS model surpass the baseline method by a large
margin. All scripts and datasets will be made publicly available at
https://github.com/jfma-USTC/HRDoc.",None,-1
Floaters No More: Radiance Field Gradient Scaling for Improved Near-Camera Training,0.480185,"NeRF acquisition typically requires careful choice of near planes for the
different cameras or suffers from background collapse, creating floating
artifacts on the edges of the captured scene. The key insight of this work is
that background collapse is caused by a higher density of samples in regions
near cameras. As a result of this sampling imbalance, near-camera volumes
receive significantly more gradients, leading to incorrect density buildup. We
propose a gradient scaling approach to counter-balance this sampling imbalance,
removing the need for near planes, while preventing background collapse. Our
method can be implemented in a few lines, does not induce any significant
overhead, and is compatible with most NeRF implementations.",None,-1
Control Risk for Potential Misuse of Artificial Intelligence in Science,0.885085,"The expanding application of Artificial Intelligence (AI) in scientific
fields presents unprecedented opportunities for discovery and innovation.
However, this growth is not without risks. AI models in science, if misused,
can amplify risks like creation of harmful substances, or circumvention of
established regulations. In this study, we aim to raise awareness of the
dangers of AI misuse in science, and call for responsible AI development and
use in this domain. We first itemize the risks posed by AI in scientific
contexts, then demonstrate the risks by highlighting real-world examples of
misuse in chemical science. These instances underscore the need for effective
risk management strategies. In response, we propose a system called SciGuard to
control misuse risks for AI models in science. We also propose a red-teaming
benchmark SciMT-Safety to assess the safety of different systems. Our proposed
SciGuard shows the least harmful impact in the assessment without compromising
performance in benign tests. Finally, we highlight the need for a
multidisciplinary and collaborative effort to ensure the safe and ethical use
of AI models in science. We hope that our study can spark productive
discussions on using AI ethically in science among researchers, practitioners,
policymakers, and the public, to maximize benefits and minimize the risks of
misuse.",None,-1
On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers,0.607661,"State-of-the-art rehearsal-free continual learning methods exploit the
peculiarities of Vision Transformers to learn task-specific prompts,
drastically reducing catastrophic forgetting. However, there is a tradeoff
between the number of learned parameters and the performance, making such
models computationally expensive. In this work, we aim to reduce this cost
while maintaining competitive performance. We achieve this by revisiting and
extending a simple transfer learning idea: learning task-specific normalization
layers. Specifically, we tune the scale and bias parameters of LayerNorm for
each continual learning task, selecting them at inference time based on the
similarity between task-specific keys and the output of the pre-trained model.
To make the classifier robust to incorrect selection of parameters during
inference, we introduce a two-stage training procedure, where we first optimize
the task-specific parameters and then train the classifier with the same
selection procedure of the inference time. Experiments on ImageNet-R and
CIFAR-100 show that our method achieves results that are either superior or on
par with {the state of the art} while being computationally cheaper.",None,-1
Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity,0.114879,"Previous work has shown that the representations output by contextual
language models are more anisotropic than static type embeddings, and typically
display outlier dimensions. This seems to be true for both monolingual and
multilingual models, although much less work has been done on the multilingual
context. Why these outliers occur and how they affect the representations is
still an active area of research. We investigate outlier dimensions and their
relationship to anisotropy in multiple pre-trained multilingual language
models. We focus on cross-lingual semantic similarity tasks, as these are
natural tasks for evaluating multilingual representations. Specifically, we
examine sentence representations. Sentence transformers which are fine-tuned on
parallel resources (that are not always available) perform better on this task,
and we show that their representations are more isotropic. However, we aim to
improve multilingual representations in general. We investigate how much of the
performance difference can be made up by only transforming the embedding space
without fine-tuning, and visualise the resulting spaces. We test different
operations: Removing individual outlier dimensions, cluster-based isotropy
enhancement, and ZCA whitening. We publish our code for reproducibility.",None,-1
An Empirical Comparison of LM-based Question and Answer Generation Methods,0.51331,"Question and answer generation (QAG) consists of generating a set of
question-answer pairs given a context (e.g. a paragraph). This task has a
variety of applications, such as data augmentation for question answering (QA)
models, information retrieval and education. In this paper, we establish
baselines with three different QAG methodologies that leverage
sequence-to-sequence language model (LM) fine-tuning. Experiments show that an
end-to-end QAG model, which is computationally light at both training and
inference times, is generally robust and outperforms other more convoluted
approaches. However, there are differences depending on the underlying
generative LM. Finally, our analysis shows that QA models fine-tuned solely on
generated question-answer pairs can be competitive when compared to supervised
QA models trained on human-labeled data.",None,-1
Response: Emergent analogical reasoning in large language models,0.0788329,"In their recent Nature Human Behaviour paper, ""Emergent analogical reasoning
in large language models,"" (Webb, Holyoak, and Lu, 2023) the authors argue that
""large language models such as GPT-3 have acquired an emergent ability to find
zero-shot solutions to a broad range of analogy problems."" In this response, we
provide counterexamples of the letter string analogies. In our tests, GPT-3
fails to solve simplest variations of the original tasks, whereas human
performance remains consistently high across all modified versions. Zero-shot
reasoning is an extraordinary claim that requires extraordinary evidence. We do
not see that evidence in our experiments. To strengthen claims of humanlike
reasoning such as zero-shot reasoning, it is important that the field develop
approaches that rule out data memorization.",None,-1
Implicit Temporal Reasoning for Evidence-Based Fact-Checking,0.0669758,"Leveraging contextual knowledge has become standard practice in automated
claim verification, yet the impact of temporal reasoning has been largely
overlooked. Our study demonstrates that time positively influences the claim
verification process of evidence-based fact-checking. The temporal aspects and
relations between claims and evidence are first established through grounding
on shared timelines, which are constructed using publication dates and time
expressions extracted from their text. Temporal information is then provided to
RNN-based and Transformer-based classifiers before or after claim and evidence
encoding. Our time-aware fact-checking models surpass base models by up to 9%
Micro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They also
outperform prior methods that explicitly model temporal relations between
evidence. Our findings show that the presence of temporal information and the
manner in which timelines are constructed greatly influence how fact-checking
models determine the relevance and supporting or refuting character of evidence
documents.",None,-1
AdvFAS: A robust face anti-spoofing framework against adversarial examples,0.7578,"Ensuring the reliability of face recognition systems against presentation
attacks necessitates the deployment of face anti-spoofing techniques. Despite
considerable advancements in this domain, the ability of even the most
state-of-the-art methods to defend against adversarial examples remains
elusive. While several adversarial defense strategies have been proposed, they
typically suffer from constrained practicability due to inevitable trade-offs
between universality, effectiveness, and efficiency. To overcome these
challenges, we thoroughly delve into the coupled relationship between
adversarial detection and face anti-spoofing. Based on this, we propose a
robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled
scores to accurately distinguish between correctly detected and wrongly
detected face images. Extensive experiments demonstrate the effectiveness of
our framework in a variety of settings, including different attacks, datasets,
and backbones, meanwhile enjoying high accuracy on clean examples. Moreover, we
successfully apply the proposed method to detect real-world adversarial
examples.",None,-1
Deep Learning Mental Health Dialogue System,0.942598,"Mental health counseling remains a major challenge in modern society due to
cost, stigma, fear, and unavailability. We posit that generative artificial
intelligence (AI) models designed for mental health counseling could help
improve outcomes by lowering barriers to access. To this end, we have developed
a deep learning (DL) dialogue system called Serena. The system consists of a
core generative model and post-processing algorithms. The core generative model
is a 2.7 billion parameter Seq2Seq Transformer fine-tuned on thousands of
transcripts of person-centered-therapy (PCT) sessions. The series of
post-processing algorithms detects contradictions, improves coherency, and
removes repetitive answers. Serena is implemented and deployed on
\url{https://serena.chat}, which currently offers limited free services. While
the dialogue system is capable of responding in a qualitatively empathetic and
engaging manner, occasionally it displays hallucination and long-term
incoherence. Overall, we demonstrate that a deep learning mental health
dialogue system has the potential to provide a low-cost and effective
complement to traditional human counselors with less barriers to access.",None,-1
Target-Side Augmentation for Document-Level Machine Translation,0.696954,"Document-level machine translation faces the challenge of data sparsity due
to its long input length and a small amount of training data, increasing the
risk of learning spurious patterns. To address this challenge, we propose a
target-side augmentation method, introducing a data augmentation (DA) model to
generate many potential translations for each source document. Learning on
these wider range translations, an MT model can learn a smoothed distribution,
thereby reducing the risk of data sparsity. We demonstrate that the DA model,
which estimates the posterior distribution, largely improves the MT
performance, outperforming the previous best system by 2.30 s-BLEU on News and
achieving new state-of-the-art on News and Europarl benchmarks. Our code is
available at https://github.com/baoguangsheng/target-side-augmentation.",None,-1
Learnability with PAC Semantics for Multi-agent Beliefs,0.0866735,"The tension between deduction and induction is perhaps the most fundamental
issue in areas such as philosophy, cognition and artificial intelligence. In an
influential paper, Valiant recognised that the challenge of learning should be
integrated with deduction. In particular, he proposed a semantics to capture
the quality possessed by the output of Probably Approximately Correct (PAC)
learning algorithms when formulated in a logic. Although weaker than classical
entailment, it allows for a powerful model-theoretic framework for answering
queries. In this paper, we provide a new technical foundation to demonstrate
PAC learning with multi-agent epistemic logics. To circumvent the negative
results in the literature on the difficulty of robust learning with the PAC
semantics, we consider so-called implicit learning where we are able to
incorporate observations to the background theory in service of deciding the
entailment of an epistemic query. We prove correctness of the learning
procedure and discuss results on the sample complexity, that is how many
observations we will need to provably assert that the query is entailed given a
user-specified error bound. Finally, we investigate under what circumstances
this algorithm can be made efficient. On the last point, given that reasoning
in epistemic logics especially in multi-agent epistemic logics is
PSPACE-complete, it might seem like there is no hope for this problem. We
leverage some recent results on the so-called Representation Theorem explored
for single-agent and multi-agent epistemic logics with the only knowing
operator to reduce modal reasoning to propositional reasoning.",None,-1
MixTeacher: Mining Promising Labels with Mixed Scale Teacher for Semi-Supervised Object Detection,0.677103,"Scale variation across object instances remains a key challenge in object
detection task. Despite the remarkable progress made by modern detection
models, this challenge is particularly evident in the semi-supervised case.
While existing semi-supervised object detection methods rely on strict
conditions to filter high-quality pseudo labels from network predictions, we
observe that objects with extreme scale tend to have low confidence, resulting
in a lack of positive supervision for these objects. In this paper, we propose
a novel framework that addresses the scale variation problem by introducing a
mixed scale teacher to improve pseudo label generation and scale-invariant
learning. Additionally, we propose mining pseudo labels using score promotion
of predictions across scales, which benefits from better predictions from mixed
scale features. Our extensive experiments on MS COCO and PASCAL VOC benchmarks
under various semi-supervised settings demonstrate that our method achieves new
state-of-the-art performance. The code and models are available at
\url{https://github.com/lliuz/MixTeacher}.",None,-1
Multi-lingual and Multi-cultural Figurative Language Understanding,0.468803,"Figurative language permeates human communication, but at the same time is
relatively understudied in NLP. Datasets have been created in English to
accelerate progress towards measuring and improving figurative language
processing in language models (LMs). However, the use of figurative language is
an expression of our cultural and societal experiences, making it difficult for
these phrases to be universally applicable. In this work, we create a
figurative language inference dataset, \datasetname, for seven diverse
languages associated with a variety of cultures: Hindi, Indonesian, Javanese,
Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language
relies on cultural and regional concepts for figurative expressions, with the
highest overlap between languages originating from the same region. We assess
multilingual LMs' abilities to interpret figurative language in zero-shot and
few-shot settings. All languages exhibit a significant deficiency compared to
English, with variations in performance reflecting the availability of
pre-training and fine-tuning data, emphasizing the need for LMs to be exposed
to a broader range of linguistic and cultural variation during training.",None,-1
A Suite of Fairness Datasets for Tabular Classification,0.315653,"There have been many papers with algorithms for improving fairness of
machine-learning classifiers for tabular data. Unfortunately, most use only
very few datasets for their experimental evaluation. We introduce a suite of
functions for fetching 20 fairness datasets and providing associated fairness
metadata. Hopefully, these will lead to more rigorous experimental evaluations
in future fairness-aware machine learning research.",None,-1
ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection,0.55537,"Fanfiction, a popular form of creative writing set within established
fictional universes, has gained a substantial online following. However,
ensuring the well-being and safety of participants has become a critical
concern in this community. The detection of triggering content, material that
may cause emotional distress or trauma to readers, poses a significant
challenge. In this paper, we describe our approach for the Trigger Detection
shared task at PAN CLEF 2023, where we want to detect multiple triggering
content in a given Fanfiction document. For this, we build a hierarchical model
that uses recurrence over Transformer-based language models. In our approach,
we first split long documents into smaller sized segments and use them to
fine-tune a Transformer model. Then, we extract feature embeddings from the
fine-tuned Transformer model, which are used as input in the training of
multiple LSTM models for trigger detection in a multi-label setting. Our model
achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the
validation set, which are higher than the baseline results shared at PAN CLEF
2023.",None,-1
Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory,0.999999,"The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular ""ObtainDiamond"" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the ""ObtainDiamond"" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the ""ObtainDiamond"" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.",None,-1
Investigating Pre-trained Audio Encoders in the Low-Resource Condition,0.0752444,"Pre-trained speech encoders have been central to pushing state-of-the-art
results across various speech understanding and generation tasks. Nonetheless,
the capabilities of these encoders in low-resource settings are yet to be
thoroughly explored. To address this, we conduct a comprehensive set of
experiments using a representative set of 3 state-of-the-art encoders
(Wav2vec2, WavLM, Whisper) in the low-resource setting across 7 speech
understanding and generation tasks. We provide various quantitative and
qualitative analyses on task performance, convergence speed, and
representational properties of the encoders. We observe a connection between
the pre-training protocols of these encoders and the way in which they capture
information in their internal layers. In particular, we observe the Whisper
encoder exhibits the greatest low-resource capabilities on content-driven tasks
in terms of performance and convergence speed.",None,-1
Stochastic Multi-Person 3D Motion Forecasting,0.889615,"This paper aims to deal with the ignored real-world complexities in prior
work on human motion forecasting, emphasizing the social properties of
multi-person motion, the diversity of motion and social interactions, and the
complexity of articulated motion. To this end, we introduce a novel task of
stochastic multi-person 3D motion forecasting. We propose a dual-level
generative modeling framework that separately models independent individual
motion at the local level and social interactions at the global level. Notably,
this dual-level modeling mechanism can be achieved within a shared generative
model, through introducing learnable latent codes that represent intents of
future motion and switching the codes' modes of operation at different levels.
Our framework is general; we instantiate it with different generative models,
including generative adversarial networks and diffusion models, and various
multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,
and SoMoF benchmarks show that our approach produces diverse and accurate
multi-person predictions, significantly outperforming the state of the art.",None,-1
AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation,0.451543,"Open-set Unsupervised Video Domain Adaptation (OUVDA) deals with the task of
adapting an action recognition model from a labelled source domain to an
unlabelled target domain that contains ""target-private"" categories, which are
present in the target but absent in the source. In this work we deviate from
the prior work of training a specialized open-set classifier or weighted
adversarial learning by proposing to use pre-trained Language and Vision Models
(CLIP). The CLIP is well suited for OUVDA due to its rich representation and
the zero-shot recognition capabilities. However, rejecting target-private
instances with the CLIP's zero-shot protocol requires oracle knowledge about
the target-private label names. To circumvent the impossibility of the
knowledge of label names, we propose AutoLabel that automatically discovers and
generates object-centric compositional candidate target-private class names.
Despite its simplicity, we show that CLIP when equipped with AutoLabel can
satisfactorily reject the target-private instances, thereby facilitating better
alignment between the shared classes of the two domains. The code is available.",None,-1
Hierarchical Neural Coding for Controllable CAD Model Generation,0.149513,"This paper presents a novel generative model for Computer Aided Design (CAD)
that 1) represents high-level design concepts of a CAD model as a three-level
hierarchical tree of neural codes, from global part arrangement down to local
curve geometry; and 2) controls the generation or completion of CAD models by
specifying the target design using a code tree. Concretely, a novel variant of
a vector quantized VAE with ""masked skip connection"" extracts design variations
as neural codebooks at three levels. Two-stage cascaded auto-regressive
transformers learn to generate code trees from incomplete CAD models and then
complete CAD models following the intended design. Extensive experiments
demonstrate superior performance on conventional tasks such as random
generation while enabling novel interaction capabilities on conditional
generation tasks. The code is available at
https://github.com/samxuxiang/hnc-cad.",None,-1
Reading Between the Lanes: Text VideoQA on the Road,0.0560544,"Text and signs around roads provide crucial information for drivers, vital
for safe navigation and situational awareness. Scene text recognition in motion
is a challenging problem, while textual cues typically appear for a short time
span, and early detection at a distance is necessary. Systems that exploit such
information to assist the driver should not only extract and incorporate visual
and textual cues from the video stream but also reason over time. To address
this issue, we introduce RoadTextVQA, a new dataset for the task of video
question answering (VideoQA) in the context of driver assistance. RoadTextVQA
consists of $3,222$ driving videos collected from multiple countries, annotated
with $10,500$ questions, all based on text or road signs present in the driving
videos. We assess the performance of state-of-the-art video question answering
models on our RoadTextVQA dataset, highlighting the significant potential for
improvement in this domain and the usefulness of the dataset in advancing
research on in-vehicle support systems and text-aware multimodal question
answering. The dataset is available at
http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtextvqa",None,-1
Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten,0.47037,"The Right to Explanation and the Right to be Forgotten are two important
principles outlined to regulate algorithmic decision making and data usage in
real-world applications. While the right to explanation allows individuals to
request an actionable explanation for an algorithmic decision, the right to be
forgotten grants them the right to ask for their data to be deleted from all
the databases and models of an organization. Intuitively, enforcing the right
to be forgotten may trigger model updates which in turn invalidate previously
provided explanations, thus violating the right to explanation. In this work,
we investigate the technical implications arising due to the interference
between the two aforementioned regulatory principles, and propose the first
algorithmic framework to resolve the tension between them. To this end, we
formulate a novel optimization problem to generate explanations that are robust
to model updates due to the removal of training data instances by data deletion
requests. We then derive an efficient approximation algorithm to handle the
combinatorial complexity of this optimization problem. We theoretically
demonstrate that our method generates explanations that are provably robust to
worst-case data deletion requests with bounded costs in case of linear models
and certain classes of non-linear models. Extensive experimentation with
real-world datasets demonstrates the efficacy of the proposed framework.",None,-1
Continual Learning with Scaled Gradient Projection,0.428807,"In neural networks, continual learning results in gradient interference among
sequential tasks, leading to catastrophic forgetting of old tasks while
learning new ones. This issue is addressed in recent methods by storing the
important gradient spaces for old tasks and updating the model orthogonally
during new tasks. However, such restrictive orthogonal gradient updates hamper
the learning capability of the new tasks resulting in sub-optimal performance.
To improve new learning while minimizing forgetting, in this paper we propose a
Scaled Gradient Projection (SGP) method, where we combine the orthogonal
gradient projections with scaled gradient steps along the important gradient
spaces for the past tasks. The degree of gradient scaling along these spaces
depends on the importance of the bases spanning them. We propose an efficient
method for computing and accumulating importance of these bases using the
singular value decomposition of the input representations for each task. We
conduct extensive experiments ranging from continual image classification to
reinforcement learning tasks and report better performance with less training
overhead than the state-of-the-art approaches.",None,-1
CLIP the Gap: A Single Domain Generalization Approach for Object Detection,0.6474,"Single Domain Generalization (SDG) tackles the problem of training a model on
a single source domain so that it generalizes to any unseen target domain.
While this has been well studied for image classification, the literature on
SDG object detection remains almost non-existent. To address the challenges of
simultaneously learning robust object localization and representation, we
propose to leverage a pre-trained vision-language model to introduce semantic
domain concepts via textual prompts. We achieve this via a semantic
augmentation strategy acting on the features extracted by the detector
backbone, as well as a text-based classification loss. Our experiments evidence
the benefits of our approach, outperforming by 10% the only existing SDG object
detection method, Single-DGOD [49], on their own diverse weather-driving
benchmark.",None,-1
Correct Like Humans: Progressive Learning Framework for Chinese Text Error Correction,0.952779,"Chinese Text Error Correction (CTEC) aims to detect and correct errors in the
input text, which benefits human daily life and various downstream tasks.
Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolve
CTEC. Although PLMs have achieved remarkable success in CTEC, we argue that
previous studies still overlook the importance of human thinking patterns. To
enhance the development of PLMs for CTEC, inspired by humans' daily
error-correcting behavior, we propose a novel model-agnostic progressive
learning framework, named ProTEC, which guides PLMs-based CTEC models to learn
to correct like humans. During the training process, ProTEC guides the model to
learn text error correction by incorporating these sub-tasks into a progressive
paradigm. During the inference process, the model completes these sub-tasks in
turn to generate the correction results. Extensive experiments and detailed
analyses demonstrate the effectiveness and efficiency of our proposed
model-agnostic ProTEC framework.",None,-1
Towards Hierarchical Regional Transformer-based Multiple Instance Learning,0.387053,"The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.",None,-1
AGI: Artificial General Intelligence for Education,0.562375,"Artificial general intelligence (AGI) has gained global recognition as a
future technology due to the emergence of breakthrough large language models
and chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional
AI models, typically designed for a limited range of tasks, demand significant
amounts of domain-specific data for training and may not always consider
intricate interpersonal dynamics in education. AGI, driven by the recent large
pre-trained models, represents a significant leap in the capability of machines
to perform tasks that require human-level intelligence, such as reasoning,
problem-solving, decision-making, and even understanding human emotions and
social interactions. This position paper reviews AGI's key concepts,
capabilities, scope, and potential within future education, including achieving
future educational goals, designing pedagogy and curriculum, and performing
assessments. It highlights that AGI can significantly improve intelligent
tutoring systems, educational assessment, and evaluation procedures. AGI
systems can adapt to individual student needs, offering tailored learning
experiences. They can also provide comprehensive feedback on student
performance and dynamically adjust teaching methods based on student progress.
The paper emphasizes that AGI's capabilities extend to understanding human
emotions and social interactions, which are critical in educational settings.
The paper discusses that ethical issues in education with AGI include data
bias, fairness, and privacy and emphasizes the need for codes of conduct to
ensure responsible AGI use in academic settings like homework, teaching, and
recruitment. We also conclude that the development of AGI necessitates
interdisciplinary collaborations between educators and AI engineers to advance
research and application efforts.",None,-1
Multimodal Chain-of-Thought Reasoning in Language Models,0.964243,"Large language models (LLMs) have shown impressive performance on complex
reasoning by leveraging chain-of-thought (CoT) prompting to generate
intermediate reasoning chains as the rationale to infer the answer. However,
existing CoT studies have primarily focused on the language modality. We
propose Multimodal-CoT that incorporates language (text) and vision (images)
modalities into a two-stage framework that separates rationale generation and
answer inference. In this way, answer inference can leverage better generated
rationales that are based on multimodal information. Experimental results on
ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed
approach. With Multimodal-CoT, our model under 1 billion parameters achieves
state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates
that Multimodal-CoT offers the advantages of mitigating hallucination and
enhancing convergence speed. Code is publicly available at
https://github.com/amazon-science/mm-cot.",None,-1
AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,0.748095,"The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.",None,-1
The sample complexity of multi-distribution learning,0.550959,"Multi-distribution learning generalizes the classic PAC learning to handle
data coming from multiple distributions. Given a set of $k$ data distributions
and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis
that minimizes the maximum population loss over $k$ distributions, up to
$\epsilon$ additive error. In this paper, we settle the sample complexity of
multi-distribution learning by giving an algorithm of sample complexity
$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the
lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem
of Awasthi, Haghtalab and Zhao [AHZ23].",None,-1
InstructDET: Diversifying Referring Object Detection with Generalized Instructions,0.205529,"We propose InstructDET, a data-centric method for referring object detection
(ROD) that localizes target objects based on user instructions. While deriving
from referring expressions (REC), the instructions we leverage are greatly
diversified to encompass common user intentions related to object detection.
For one image, we produce tremendous instructions that refer to every single
object and different combinations of multiple objects. Each instruction and its
corresponding object bounding boxes (bbxs) constitute one training data pair.
In order to encompass common detection expressions, we involve emerging
vision-language model (VLM) and large language model (LLM) to generate
instructions guided by text prompts and object bbxs, as the generalizations of
foundation models are effective to produce human-like expressions (e.g.,
describing object property, category, and relationship). We name our
constructed dataset as InDET. It contains images, bbxs and generalized
instructions that are from foundation models. Our InDET is developed from
existing REC datasets and object detection datasets, with the expanding
potential that any image with object bbxs can be incorporated through using our
InstructDET method. By using our InDET dataset, we show that a conventional ROD
model surpasses existing methods on standard REC datasets and our InDET test
set. Our data-centric method InstructDET, with automatic data expansion by
leveraging foundation models, directs a promising field that ROD can be greatly
diversified to execute common object detection instructions.",None,-1
Amodal Intra-class Instance Segmentation: Synthetic Datasets and Benchmark,0.542788,"Images of realistic scenes often contain intra-class objects that are heavily
occluded from each other, making the amodal perception task that requires
parsing the occluded parts of the objects challenging. Although important for
downstream tasks such as robotic grasping systems, the lack of large-scale
amodal datasets with detailed annotations makes it difficult to model
intra-class occlusions explicitly. This paper introduces two new amodal
datasets for image amodal completion tasks, which contain a total of over 267K
images of intra-class occlusion scenarios, annotated with multiple masks,
amodal bounding boxes, dual order relations and full appearance for instances
and background. We also present a point-supervised scheme with layer priors for
amodal instance segmentation specifically designed for intra-class occlusion
scenarios. Experiments show that our weakly supervised approach outperforms the
SOTA fully supervised methods, while our layer priors design exhibits
remarkable performance improvements in the case of intra-class occlusion in
both synthetic and real images.",None,-1
Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping,0.556607,"This paper presents a novel visual-LiDAR odometry and mapping method with
low-drift characteristics. The proposed method is based on two popular
approaches, ORB-SLAM and A-LOAM, with monocular scale correction and
visual-bootstrapped LiDAR poses initialization modifications. The scale
corrector calculates the proportion between the depth of image keypoints
recovered by triangulation and that provided by LiDAR, using an outlier
rejection process for accuracy improvement. Concerning LiDAR poses
initialization, the visual odometry approach gives the initial guesses of LiDAR
motions for better performance. This methodology is not only applicable to
high-resolution LiDAR but can also adapt to low-resolution LiDAR. To evaluate
the proposed SLAM system's robustness and accuracy, we conducted experiments on
the KITTI Odometry and S3E datasets. Experimental results illustrate that our
method significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore,
regarding the accuracy of visual odometry with scale correction, our method
performs similarly to the stereo-mode ORB-SLAM2.",None,-1
Multi-Scale Occ: 4th Place Solution for CVPR 2023 3D Occupancy Prediction Challenge,0.400175,"In this report, we present the 4th place solution for CVPR 2023 3D occupancy
prediction challenge. We propose a simple method called Multi-Scale Occ for
occupancy prediction based on lift-splat-shoot framework, which introduces
multi-scale image features for generating better multi-scale 3D voxel features
with temporal fusion of multiple past frames. Post-processing including model
ensemble, test-time augmentation, and class-wise thresh are adopted to further
boost the final performance. As shown on the leaderboard, our proposed
occupancy prediction method ranks the 4th place with 49.36 mIoU.",None,-1
Contrastive Learning Is Spectral Clustering On Similarity Graph,0.409073,"Contrastive learning is a powerful self-supervised learning method, but we
have a limited theoretical understanding of how it works and why it works. In
this paper, we prove that contrastive learning with the standard InfoNCE loss
is equivalent to spectral clustering on the similarity graph. Using this
equivalence as the building block, we extend our analysis to the CLIP model and
rigorously characterize how similar multi-modal objects are embedded together.
Motivated by our theoretical insights, we introduce the Kernel-InfoNCE loss,
incorporating mixtures of kernel functions that outperform the standard
Gaussian kernel on several vision datasets. The code is available at
https://github.com/yifanzhang-pro/Kernel-InfoNCE.",None,-1
Diffusion-based 3D Object Detection with Random Boxes,0.47813,"3D object detection is an essential task for achieving autonomous driving.
Existing anchor-based detection methods rely on empirical heuristics setting of
anchors, which makes the algorithms lack elegance. In recent years, we have
witnessed the rise of several generative models, among which diffusion models
show great potential for learning the transformation of two distributions. Our
proposed Diff3Det migrates the diffusion model to proposal generation for 3D
object detection by considering the detection boxes as generative targets.
During training, the object boxes diffuse from the ground truth boxes to the
Gaussian distribution, and the decoder learns to reverse this noise process. In
the inference stage, the model progressively refines a set of random boxes to
the prediction results. We provide detailed experiments on the KITTI benchmark
and achieve promising performance compared to classical anchor-based 3D
detection methods.",None,-1
CIRO: COVID-19 infection risk ontology,0.333431,"Public health authorities perform contact tracing for highly contagious
agents to identify close contacts with the infected cases. However, during the
pandemic caused by coronavirus disease 2019 (COVID-19), this operation was not
employed in countries with high patient volumes. Meanwhile, the Japanese
government conducted this operation, thereby contributing to the control of
infections, at the cost of arduous manual labor by public health officials. To
ease the burden of the officials, this study attempted to automate the
assessment of each person's infection risk through an ontology, called COVID-19
Infection Risk Ontology (CIRO). This ontology expresses infection risks of
COVID-19 formulated by the Japanese government, toward automated assessment of
infection risks of individuals, using Resource Description Framework (RDF) and
SPARQL (SPARQL Protocol and RDF Query Language) queries. For evaluation, we
demonstrated that the knowledge graph built could infer the risks, formulated
by the government. Moreover, we conducted reasoning experiments to analyze the
computational efficiency. The experiments demonstrated usefulness of the
knowledge processing, and identified issues left for deployment.",None,-1
SyncDreamer: Generating Multiview-consistent Images from a Single-view Image,0.999947,"In this paper, we present a novel diffusion model called that generates
multiview-consistent images from a single-view image. Using pretrained
large-scale 2D diffusion models, recent work Zero123 demonstrates the ability
to generate plausible novel views from a single-view image of an object.
However, maintaining consistency in geometry and colors for the generated
images remains a challenge. To address this issue, we propose a synchronized
multiview diffusion model that models the joint probability distribution of
multiview images, enabling the generation of multiview-consistent images in a
single reverse process. SyncDreamer synchronizes the intermediate states of all
the generated images at every step of the reverse process through a 3D-aware
feature attention mechanism that correlates the corresponding features across
different views. Experiments show that SyncDreamer generates images with high
consistency across different views, thus making it well-suited for various 3D
generation tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.",None,-1
Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,0.520159,"The large number of ReLU non-linearity operations in existing deep neural
networks makes them ill-suited for latency-efficient private inference (PI).
Existing techniques to reduce ReLU operations often involve manual effort and
sacrifice significant accuracy. In this paper, we first present a novel measure
of non-linearity layers' ReLU sensitivity, enabling mitigation of the
time-consuming manual efforts in identifying the same. Based on this
sensitivity, we then present SENet, a three-stage training method that for a
given ReLU budget, automatically assigns per-layer ReLU counts, decides the
ReLU locations for each layer's activation map, and trains a model with
significantly fewer ReLUs to potentially yield latency and communication
efficient PI. Experimental evaluations with multiple models on various datasets
show SENet's superior performance both in terms of reduced ReLUs and improved
classification accuracy compared to existing alternatives. In particular, SENet
can yield models that require up to ~2x fewer ReLUs while yielding similar
accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved
classification accuracy, evaluated on CIFAR-100.",None,-1
Multi-Scale Prototypical Transformer for Whole Slide Image Classification,0.837679,"Whole slide image (WSI) classification is an essential task in computational
pathology. Despite the recent advances in multiple instance learning (MIL) for
WSI classification, accurate classification of WSIs remains challenging due to
the extreme imbalance between the positive and negative instances in bags, and
the complicated pre-processing to fuse multi-scale information of WSI. To this
end, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI
classification, which includes a prototypical Transformer (PT) module and a
multi-scale feature fusion module (MFFM). The PT is developed to reduce
redundant instances in bags by integrating prototypical learning into the
Transformer architecture. It substitutes all instances with cluster prototypes,
which are then re-calibrated through the self-attention mechanism of the
Trans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypes
of different scales, which employs MLP-Mixer to enhance the information
communication between prototypes. The experimental results on two public WSI
datasets demonstrate that the proposed MSPT outperforms all the compared
algorithms, suggesting its potential applications.",None,-1
Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations,0.465649,"The rapid development and application of foundation models have
revolutionized the field of artificial intelligence. Large diffusion models
have gained significant attention for their ability to generate photorealistic
images and support various tasks. On-device deployment of these models provides
benefits such as lower server costs, offline functionality, and improved user
privacy. However, common large diffusion models have over 1 billion parameters
and pose challenges due to restricted computational and memory resources on
devices. We present a series of implementation optimizations for large
diffusion models that achieve the fastest reported inference latency to-date
(under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung
S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile
devices. These enhancements broaden the applicability of generative AI and
improve the overall user experience across a wide range of devices.",None,-1
A Comparative Analysis of Pretrained Language Models for Text-to-Speech,0.151134,"State-of-the-art text-to-speech (TTS) systems have utilized pretrained
language models (PLMs) to enhance prosody and create more natural-sounding
speech. However, while PLMs have been extensively researched for natural
language understanding (NLU), their impact on TTS has been overlooked. In this
study, we aim to address this gap by conducting a comparative analysis of
different PLMs for two TTS tasks: prosody prediction and pause prediction.
Firstly, we trained a prosody prediction model using 15 different PLMs. Our
findings revealed a logarithmic relationship between model size and quality, as
well as significant performance differences between neutral and expressive
prosody. Secondly, we employed PLMs for pause prediction and found that the
task was less sensitive to small models. We also identified a strong
correlation between our empirical results and the GLUE scores obtained for
these language models. To the best of our knowledge, this is the first study of
its kind to investigate the impact of different PLMs on TTS.",None,-1
Learning Attention as Disentangler for Compositional Zero-shot Learning,0.309626,"Compositional zero-shot learning (CZSL) aims at learning visual concepts
(i.e., attributes and objects) from seen compositions and combining concept
knowledge into unseen compositions. The key to CZSL is learning the
disentanglement of the attribute-object composition. To this end, we propose to
exploit cross-attentions as compositional disentanglers to learn disentangled
concept embeddings. For example, if we want to recognize an unseen composition
""yellow flower"", we can learn the attribute concept ""yellow"" and object concept
""flower"" from different yellow objects and different flowers respectively. To
further constrain the disentanglers to learn the concept of interest, we employ
a regularization at the attention level. Specifically, we adapt the earth
mover's distance (EMD) as a feature similarity metric in the cross-attention
module. Moreover, benefiting from concept disentanglement, we improve the
inference process and tune the prediction score by combining multiple concept
probabilities. Comprehensive experiments on three CZSL benchmark datasets
demonstrate that our method significantly outperforms previous works in both
closed- and open-world settings, establishing a new state-of-the-art.",None,-1
GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer,0.731505,"Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.",None,-1
Reinforcement Learning with Human Feedback for Realistic Traffic Simulation,0.50789,"In light of the challenges and costs of real-world testing, autonomous
vehicle developers often rely on testing in simulation for the creation of
reliable systems. A key element of effective simulation is the incorporation of
realistic traffic models that align with human knowledge, an aspect that has
proven challenging due to the need to balance realism and diversity. This works
aims to address this by developing a framework that employs reinforcement
learning with human preference (RLHF) to enhance the realism of existing
traffic models. This study also identifies two main challenges: capturing the
nuances of human preferences on realism and the unification of diverse traffic
simulation models. To tackle these issues, we propose using human feedback for
alignment and employ RLHF due to its sample efficiency. We also introduce the
first dataset for realism alignment in traffic modeling to support such
research. Our framework, named TrafficRLHF, demonstrates its proficiency in
generating realistic traffic scenarios that are well-aligned with human
preferences, as corroborated by comprehensive evaluations on the nuScenes
dataset.",None,-1
CrossKD: Cross-Head Knowledge Distillation for Object Detection,0.174006,"Knowledge Distillation (KD) has been validated as an effective model
compression technique for learning compact object detectors. Existing
state-of-the-art KD methods for object detection are mostly based on feature
imitation. In this paper, we present a general and effective prediction
mimicking distillation scheme, called CrossKD, which delivers the intermediate
features of the student's detection head to the teacher's detection head. The
resulting cross-head predictions are then forced to mimic the teacher's
predictions. This manner relieves the student's head from receiving
contradictory supervision signals from the annotations and the teacher's
predictions, greatly improving the student's detection performance. Moreover,
as mimicking the teacher's predictions is the target of KD, CrossKD offers more
task-oriented information in contrast with feature imitation. On MS COCO, with
only prediction mimicking losses applied, our CrossKD boosts the average
precision of GFL ResNet-50 with 1x training schedule from 40.2 to 43.7,
outperforming all existing KD methods. In addition, our method also works well
when distilling detectors with heterogeneous backbones. Code is available at
https://github.com/jbwang1997/CrossKD.",None,-1
RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters,0.069541,"Novel view synthesis (NVS) is a challenging task in computer vision that
involves synthesizing new views of a scene from a limited set of input images.
Neural Radiance Fields (NeRF) have emerged as a powerful approach to address
this problem, but they require accurate knowledge of camera \textit{intrinsic}
and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM)
and multi-view stereo (MVS) approaches have been used to extract camera
parameters, but these methods can be unreliable and may fail in certain cases.
In this paper, we propose a novel technique that leverages unposed images from
dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera
parameters directly from data. Our approach is highly extensible and can be
integrated into existing NeRF architectures with minimal modifications. We
demonstrate the effectiveness of our method on a variety of static and dynamic
scenes and show that it outperforms traditional SfM and MVS approaches. The
code for our method is publicly available at
\href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}.
Our approach offers a promising new direction for improving the accuracy and
robustness of NVS using NeRF, and we anticipate that it will be a valuable tool
for a wide range of applications in computer vision and graphics.",None,-1
Test-Time Training on Nearest Neighbors for Large Language Models,0.0393618,"Many recent efforts augment language models with retrieval, by adding
retrieved data to the input context. For this approach to succeed, the
retrieved data must be added at both training and test time. Moreover, as input
length grows linearly with the size of retrieved data, cost in computation and
memory grows quadratically for modern Transformers. To avoid these
complications, we simply fine-tune the model on retrieved data at test time,
using its standard training setup. We build a large-scale distributed index
based on text embeddings of the Pile dataset. For each test input, our system
retrieves its neighbors and fine-tunes the model on their text. Surprisingly,
retrieving and training on as few as 20 neighbors, each for only one gradient
iteration, drastically improves performance across more than 20 language
modeling tasks in the Pile. For example, test-time training with nearest
neighbors significantly narrows the performance gap between a small GPT-2 and a
GPT-Neo model more than 10 times larger. Sufficient index quality and size,
however, are necessary. Our work establishes a first baseline of test-time
training for language modeling.",None,-1
Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation,0.6673,"Medical image segmentation is a fundamental task in the community of medical
image analysis. In this paper, a novel network architecture, referred to as
Convolution, Transformer, and Operator (CTO), is proposed. CTO employs a
combination of Convolutional Neural Networks (CNNs), Vision Transformer (ViT),
and an explicit boundary detection operator to achieve high recognition
accuracy while maintaining an optimal balance between accuracy and efficiency.
The proposed CTO follows the standard encoder-decoder segmentation paradigm,
where the encoder network incorporates a popular CNN backbone for capturing
local semantic information, and a lightweight ViT assistant for integrating
long-range dependencies. To enhance the learning capacity on boundary, a
boundary-guided decoder network is proposed that uses a boundary mask obtained
from a dedicated boundary detection operator as explicit supervision to guide
the decoding learning process. The performance of the proposed method is
evaluated on six challenging medical image segmentation datasets, demonstrating
that CTO achieves state-of-the-art accuracy with a competitive model
complexity.",None,-1
"Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",0.699482,"The use of large language models (LLMs) in healthcare is gaining popularity,
but their practicality and safety in clinical settings have not been thoroughly
assessed. In high-stakes environments like medical settings, trust and safety
are critical issues for LLMs. To address these concerns, we present an approach
to evaluate the performance and trustworthiness of a GPT3.5 model for medical
image protocol assignment. We compare it with a fine-tuned BERT model and a
radiologist. In addition, we have a radiologist review the GPT3.5 output to
evaluate its decision-making process. Our evaluation dataset consists of 4,700
physician entries across 11 imaging protocol classes spanning the entire head.
Our findings suggest that the GPT3.5 performance falls behind BERT and a
radiologist. However, GPT3.5 outperforms BERT in its ability to explain its
decision, detect relevant word indicators, and model calibration. Furthermore,
by analyzing the explanations of GPT3.5 for misclassifications, we reveal
systematic errors that need to be resolved to enhance its safety and
suitability for clinical use.",None,-1
PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,0.853552,"In this work, instead of directly predicting the pixel-level segmentation
masks, the problem of referring image segmentation is formulated as sequential
polygon generation, and the predicted polygons can be later converted into
segmentation masks. This is enabled by a new sequence-to-sequence framework,
Polygon Transformer (PolyFormer), which takes a sequence of image patches and
text query tokens as input, and outputs a sequence of polygon vertices
autoregressively. For more accurate geometric localization, we propose a
regression-based decoder, which predicts the precise floating-point coordinates
directly, without any coordinate quantization error. In the experiments,
PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52%
absolute improvements on the challenging RefCOCO+ and RefCOCOg datasets. It
also shows strong generalization ability when evaluated on the referring video
segmentation task without fine-tuning, e.g., achieving competitive 61.5% J&F on
the Ref-DAVIS17 dataset.",None,-1
4D Panoptic Segmentation as Invariant and Equivariant Field Prediction,0.506944,"In this paper, we develop rotation-equivariant neural networks for 4D
panoptic segmentation. 4D panoptic segmentation is a benchmark task for
autonomous driving that requires recognizing semantic classes and object
instances on the road based on LiDAR scans, as well as assigning temporally
consistent IDs to instances across time. We observe that the driving scenario
is symmetric to rotations on the ground plane. Therefore, rotation-equivariance
could provide better generalization and more robust feature learning.
Specifically, we review the object instance clustering strategies and restate
the centerness-based approach and the offset-based approach as the prediction
of invariant scalar fields and equivariant vector fields. Other sub-tasks are
also unified from this perspective, and different invariant and equivariant
layers are designed to facilitate their predictions. Through evaluation on the
standard 4D panoptic segmentation benchmark of SemanticKITTI, we show that our
equivariant models achieve higher accuracy with lower computational costs
compared to their non-equivariant counterparts. Moreover, our method sets the
new state-of-the-art performance and achieves 1st place on the SemanticKITTI 4D
Panoptic Segmentation leaderboard.",None,-1
A Systems-Theoretical Formalization of Closed Systems,0.31263,"There is a lack of formalism for some key foundational concepts in systems
engineering. One of the most recently acknowledged deficits is the inadequacy
of systems engineering practices for engineering intelligent systems. In our
previous works, we proposed that closed systems precepts could be used to
accomplish a required paradigm shift for the systems engineering of intelligent
systems. However, to enable such a shift, formal foundations for closed systems
precepts that expand the theory of systems engineering are needed. The concept
of closure is a critical concept in the formalism underlying closed systems
precepts. In this paper, we provide formal, systems- and information-theoretic
definitions of closure to identify and distinguish different types of closed
systems. Then, we assert a mathematical framework to evaluate the subjective
formation of the boundaries and constraints of such systems. Finally, we argue
that engineering an intelligent system can benefit from appropriate closed and
open systems paradigms on multiple levels of abstraction of the system. In the
main, this framework will provide the necessary fundamentals to aid in systems
engineering of intelligent systems.",None,-1
Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation,0.419153,"Recent approaches to empathetic response generation try to incorporate
commonsense knowledge or reasoning about the causes of emotions to better
understand the user's experiences and feelings. However, these approaches
mainly focus on understanding the causalities of context from the user's
perspective, ignoring the system's perspective. In this paper, we propose a
commonsense-based causality explanation approach for diverse empathetic
response generation that considers both the user's perspective (user's desires
and reactions) and the system's perspective (system's intentions and
reactions). We enhance ChatGPT's ability to reason for the system's perspective
by integrating in-context learning with commonsense knowledge. Then, we
integrate the commonsense-based causality explanation with both ChatGPT and a
T5-based model. Experimental evaluations demonstrate that our method
outperforms other comparable methods on both automatic and human evaluations.",None,-1
Single Frame Semantic Segmentation Using Multi-Modal Spherical Images,0.147475,"In recent years, the research community has shown a lot of interest to
panoramic images that offer a 360-degree directional perspective. Multiple data
modalities can be fed, and complimentary characteristics can be utilized for
more robust and rich scene interpretation based on semantic segmentation, to
fully realize the potential. Existing research, however, mostly concentrated on
pinhole RGB-X semantic segmentation. In this study, we propose a
transformer-based cross-modal fusion architecture to bridge the gap between
multi-modal fusion and omnidirectional scene perception. We employ
distortion-aware modules to address extreme object deformations and panorama
distortions that result from equirectangular representation. Additionally, we
conduct cross-modal interactions for feature rectification and information
exchange before merging the features in order to communicate long-range
contexts for bi-modal and tri-modal feature streams. In thorough tests using
combinations of four different modality types in three indoor panoramic-view
datasets, our technique achieved state-of-the-art mIoU performance: 60.60% on
Stanford2D3DS (RGB-HHA), 71.97% Structured3D (RGB-D-N), and 35.92% Matterport3D
(RGB-D). We plan to release all codes and trained models soon.",None,-1
"Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility",0.474602,"Deploying machine learning (ML) models often requires both fairness and
privacy guarantees. Both of these objectives present unique trade-offs with the
utility (e.g., accuracy) of the model. However, the mutual interactions between
fairness, privacy, and utility are less well-understood. As a result, often
only one objective is optimized, while the others are tuned as
hyper-parameters. Because they implicitly prioritize certain objectives, such
designs bias the model in pernicious, undetectable ways. To address this, we
adopt impartiality as a principle: design of ML pipelines should not favor one
objective over another. We propose impartially-specified models, which provide
us with accurate Pareto frontiers that show the inherent trade-offs between the
objectives. Extending two canonical ML frameworks for privacy-preserving
learning, we provide two methods (FairDP-SGD and FairPATE) to train
impartially-specified models and recover the Pareto frontier. Through
theoretical privacy analysis and a comprehensive empirical study, we provide an
answer to the question of where fairness mitigation should be integrated within
a privacy-aware ML pipeline.",None,-1
"Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models",0.601129,"Modern recommender systems lie at the heart of complex ecosystems that couple
the behavior of users, content providers, advertisers, and other actors.
Despite this, the focus of the majority of recommender research -- and most
practical recommenders of any import -- is on the local, myopic optimization of
the recommendations made to individual users. This comes at a significant cost
to the long-term utility that recommenders could generate for its users. We
argue that explicitly modeling the incentives and behaviors of all actors in
the system -- and the interactions among them induced by the recommender's
policy -- is strictly necessary if one is to maximize the value the system
brings to these actors and improve overall ecosystem ""health"". Doing so
requires: optimization over long horizons using techniques such as
reinforcement learning; making inevitable tradeoffs in the utility that can be
generated for different actors using the methods of social choice; reducing
information asymmetry, while accounting for incentives and strategic behavior,
using the tools of mechanism design; better modeling of both user and
item-provider behaviors by incorporating notions from behavioral economics and
psychology; and exploiting recent advances in generative and foundation models
to make these mechanisms interpretable and actionable. We propose a conceptual
framework that encompasses these elements, and articulate a number of research
challenges that emerge at the intersection of these different disciplines.",None,-1
AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets,0.198083,"High-quality data is essential for conversational recommendation systems and
serves as the cornerstone of the network architecture development and training
strategy design. Existing works contribute heavy human efforts to manually
labeling or designing and extending recommender dialogue templates. However,
they suffer from (i) the limited number of human annotators results in that
datasets can hardly capture rich and large-scale cases in the real world, (ii)
the limited experience and knowledge of annotators account for the
uninformative corpus and inappropriate recommendations. In this paper, we
propose a novel automatic dataset synthesis approach that can generate both
large-scale and high-quality recommendation dialogues through a data2text
generation process, where unstructured recommendation conversations are
generated from structured graphs based on user-item information from the real
world. In doing so, we comprehensively exploit: (i) rich personalized user
profiles from traditional recommendation datasets, (ii) rich external knowledge
from knowledge graphs, and (iii) the conversation ability contained in
human-to-human conversational recommendation datasets. Extensive experiments
validate the benefit brought by the automatically synthesized data under
low-resource scenarios and demonstrate the promising potential to facilitate
the development of a more effective conversational recommendation system.",None,-1
Protecting Federated Learning from Extreme Model Poisoning Attacks via Multidimensional Time Series Anomaly Detection,0.0555686,"Current defense mechanisms against model poisoning attacks in federated
learning (FL) systems have proven effective up to a certain threshold of
malicious clients. In this work, we introduce FLANDERS, a novel pre-aggregation
filter for FL resilient to large-scale model poisoning attacks, i.e., when
malicious clients far exceed legitimate participants. FLANDERS treats the
sequence of local models sent by clients in each FL round as a matrix-valued
time series. Then, it identifies malicious client updates as outliers in this
time series by comparing actual observations with estimates generated by a
matrix autoregressive forecasting model maintained by the server. Experiments
conducted in several non-iid FL setups show that FLANDERS significantly
improves robustness across a wide spectrum of attacks when paired with standard
and robust existing aggregation methods.",None,-1
Automatically Identifying Relations Between Self-Admitted Technical Debt Across Different Sources,0.345399,"Self-Admitted Technical Debt or SATD can be found in various sources, such as
source code comments, commit messages, issue tracking systems, and pull
requests. Previous research has established the existence of relations between
SATD items in different sources; such relations can be useful for investigating
and improving SATD management. However, there is currently a lack of approaches
for automatically detecting these SATD relations. To address this, we proposed
and evaluated approaches for automatically identifying SATD relations across
different sources. Our findings show that our approach outperforms baseline
approaches by a large margin, achieving an average F1-score of 0.829 in
identifying relations between SATD items. Moreover, we explored the
characteristics of SATD relations in 103 open-source projects and describe nine
major cases in which related SATD is documented in a second source, and give a
quantitative overview of 26 kinds of relations.",None,-1
Knowledge Enhanced Semantic Communication Receiver,0.796956,"In recent years, with the rapid development of deep learning and natural
language processing technologies, semantic communication has become a topic of
great interest in the field of communication. Although existing deep
learning-based semantic communication approaches have shown many advantages,
they still do not make sufficient use of prior knowledge. Moreover, most
existing semantic communication methods focus on the semantic encoding at the
transmitter side, while we believe that the semantic decoding capability of the
receiver should also be concerned. In this paper, we propose a knowledge
enhanced semantic communication framework in which the receiver can more
actively utilize the facts in the knowledge base for semantic reasoning and
decoding, on the basis of only affecting the parameters rather than the
structure of the neural networks at the transmitter side. Specifically, we
design a transformer-based knowledge extractor to find relevant factual triples
for the received noisy signal. Extensive simulation results on the WebNLG
dataset demonstrate that the proposed receiver yields superior performance on
top of the knowledge graph enhanced decoding.",None,-1
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,0.524583,"Real-world fact verification task aims to verify the factuality of a claim by
retrieving evidence from the source document. The quality of the retrieved
evidence plays an important role in claim verification. Ideally, the retrieved
evidence should be faithful (reflecting the model's decision-making process in
claim verification) and plausible (convincing to humans), and can improve the
accuracy of verification task. Although existing approaches leverage the
similarity measure of semantic or surface form between claims and documents to
retrieve evidence, they all rely on certain heuristics that prevent them from
satisfying all three requirements. In light of this, we propose a fact
verification model named ReRead to retrieve evidence and verify claim that: (1)
Train the evidence retriever to obtain interpretable evidence (i.e.,
faithfulness and plausibility criteria); (2) Train the claim verifier to
revisit the evidence retrieved by the optimized evidence retriever to improve
the accuracy. The proposed system is able to achieve significant improvements
upon best-reported models under different settings.",None,-1
Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,0.828616,"ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that
are slated to promise different applications in diverse areas. In education,
these AI technologies have been tested for applications in assessment and
teaching. In assessment, AI has long been used in automated essay scoring and
automated item generation. One psychometric property that these tools must have
to assist or replace humans in assessment is high reliability in terms of
agreement between AI scores and human raters. In this paper, we measure the
reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and
trained humans in perceiving and rating the complexity of writing prompts.
Intraclass correlation (ICC) as a performance metric showed that the
inter-reliability of both the OpenAI ChatGPT and the Google Bard were low
against the gold standard of human ratings.",None,-1
LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,0.800991,"Layout-to-image generation refers to the task of synthesizing photo-realistic
images based on semantic layouts. In this paper, we propose LayoutDiffuse that
adapts a foundational diffusion model pretrained on large-scale image or
text-image datasets for layout-to-image generation. By adopting a novel neural
adaptor based on layout attention and task-aware prompts, our method trains
efficiently, generates images with both high perceptual quality and layout
alignment, and needs less data. Experiments on three datasets show that our
method significantly outperforms other 10 generative models based on GANs,
VQ-VAE, and diffusion models.",None,-1
Predictable MDP Abstraction for Unsupervised Model-Based RL,0.390385,"A key component of model-based reinforcement learning (RL) is a dynamics
model that predicts the outcomes of actions. Errors in this predictive model
can degrade the performance of model-based controllers, and complex Markov
decision processes (MDPs) can present exceptionally difficult prediction
problems. To mitigate this issue, we propose predictable MDP abstraction (PMA):
instead of training a predictive model on the original MDP, we train a model on
a transformed MDP with a learned action space that only permits predictable,
easy-to-model actions, while covering the original state-action space as much
as possible. As a result, model learning becomes easier and more accurate,
which allows robust, stable model-based planning or model-based RL. This
transformation is learned in an unsupervised manner, before any task is
specified by the user. Downstream tasks can then be solved with model-based
control in a zero-shot fashion, without additional environment interactions. We
theoretically analyze PMA and empirically demonstrate that PMA leads to
significant improvements over prior unsupervised model-based RL approaches in a
range of benchmark environments. Our code and videos are available at
https://seohong.me/projects/pma/",None,-1
Linguistic ambiguity analysis in ChatGPT,0.335841,"Linguistic ambiguity is and has always been one of the main challenges in
Natural Language Processing (NLP) systems. Modern Transformer architectures
like BERT, T5 or more recently InstructGPT have achieved some impressive
improvements in many NLP fields, but there is still plenty of work to do.
Motivated by the uproar caused by ChatGPT, in this paper we provide an
introduction to linguistic ambiguity, its varieties and their relevance in
modern NLP, and perform an extensive empiric analysis. ChatGPT strengths and
weaknesses are revealed, as well as strategies to get the most of this model.",None,-1
Physics-Aware Semi-Supervised Underwater Image Enhancement,0.512716,"Underwater images normally suffer from degradation due to the transmission
medium of water bodies. Both traditional prior-based approaches and deep
learning-based methods have been used to address this problem. However, the
inflexible assumption of the former often impairs their effectiveness in
handling diverse underwater scenes, while the generalization of the latter to
unseen images is usually weakened by insufficient data. In this study, we
leverage both the physics-based underwater Image Formation Model (IFM) and deep
learning techniques for Underwater Image Enhancement (UIE). To this end, we
propose a novel Physics-Aware Dual-Stream Underwater Image Enhancement Network,
i.e., PA-UIENet, which comprises a Transmission Estimation Steam (T-Stream) and
an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE
task by explicitly estimating the degradation parameters of the IFM. We also
adopt an IFM-inspired semi-supervised learning framework, which exploits both
the labeled and unlabeled images, to address the issue of insufficient data.
Our method performs better than, or at least comparably to, eight baselines
across five testing sets in the degradation estimation and UIE tasks. This
should be due to the fact that it not only can model the degradation but also
can learn the characteristics of diverse underwater scenes.",None,-1
EventNet-ITA: Italian Frame Parsing for Events,0.316809,"This paper introduces EventNet-ITA, a large, multi-domain corpus annotated
full-text with event frames for Italian. Moreover, we present and thoroughly
evaluate an efficient multi-label sequence labeling approach for Frame Parsing.
Covering a wide range of individual, social and historical phenomena, with more
than 53,000 annotated sentences and over 200 modeled frames, EventNet-ITA
constitutes the first systematic attempt to provide the Italian language with a
publicly available resource for Frame Parsing of events, useful for a broad
spectrum of research and application tasks. Our approach achieves a promising
0.9 strict F1-score for frame classification and 0.72 for frame element
classification, on top of minimizing computational requirements. The annotated
corpus and the frame parsing model are released under open license.",None,-1
Language Models are Universal Embedders,0.0192019,"In the large language model (LLM) revolution, embedding is a key component of
various systems. For example, it is used to retrieve knowledge or memories for
LLMs, to build content moderation filters, etc. As such cases span from English
to other natural or programming languages, from retrieval to classification and
beyond, it is desirable to build a unified embedding model rather than
dedicated ones for each scenario. In this work, we make an initial step towards
this goal, demonstrating that multiple languages (both natural and programming)
pre-trained transformer decoders can embed universally when finetuned on
limited English data. We provide a comprehensive practice with thorough
evaluations. On English MTEB, our models achieve competitive performance on
different embedding tasks by minimal training data. On other benchmarks, such
as multilingual classification and code search, our models (without any
supervision) perform comparably to, or even surpass heavily supervised
baselines and/or APIs. These results provide evidence of a promising path
towards building powerful unified embedders that can be applied across tasks
and languages.",None,-1
Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization,0.698806,"Recent computational approaches for combating online hate speech involve the
automatic generation of counter narratives by adapting Pretrained
Transformer-based Language Models (PLMs) with human-curated data. This process,
however, can produce in-domain overfitting, resulting in models generating
acceptable narratives only for hatred similar to training data, with little
portability to other targets or to real-world toxic language. This paper
introduces novel attention regularization methodologies to improve the
generalization capabilities of PLMs for counter narratives generation.
Overfitting to training-specific terms is then discouraged, resulting in more
diverse and richer narratives. We experiment with two attention-based
regularization techniques on a benchmark English dataset. Regularized models
produce better counter narratives than state-of-the-art approaches in most
cases, both in terms of automatic metrics and human evaluation, especially when
hateful targets are not present in the training data. This work paves the way
for better and more flexible counter-speech generation models, a task for which
datasets are highly challenging to produce.",None,-1
What does CLIP know about a red circle? Visual prompt engineering for VLMs,0.723542,"Large-scale Vision-Language Models, such as CLIP, learn powerful image-text
representations that have found numerous applications, from zero-shot
classification to text-to-image generation. Despite that, their capabilities
for solving novel discriminative tasks via prompting fall behind those of large
language models, such as GPT-3. Here we explore the idea of visual prompt
engineering for solving computer vision tasks beyond classification by editing
in image space instead of text. In particular, we discover an emergent ability
of CLIP, where, by simply drawing a red circle around an object, we can direct
the model's attention to that region, while also maintaining global
information. We show the power of this simple approach by achieving
state-of-the-art in zero-shot referring expressions comprehension and strong
performance in keypoint localization tasks. Finally, we draw attention to some
potential ethical concerns of large language-vision models.",None,-1
GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,0.847993,"Relation extraction (RE) is a crucial task in natural language processing
(NLP) that aims to identify and classify relationships between entities
mentioned in text. In the financial domain, relation extraction plays a vital
role in extracting valuable information from financial documents, such as news
articles, earnings reports, and company filings. This paper describes our
solution to relation extraction on one such dataset REFinD. The dataset was
released along with shared task as a part of the Fourth Workshop on Knowledge
Discovery from Unstructured Data in Financial Services, co-located with SIGIR
2023. In this paper, we employed OpenAI models under the framework of
in-context learning (ICL). We utilized two retrieval strategies to find top K
relevant in-context learning demonstrations / examples from training data for a
given test example. The first retrieval mechanism, we employed, is a
learning-free dense retriever and the other system is a learning-based
retriever. We were able to achieve 3rd rank overall. Our best F1-score is
0.718.",None,-1
Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training,0.130355,"Parallel training of neural networks at scale is challenging due to
significant overheads arising from communication. Recently, deep learning
researchers have developed a variety of pruning algorithms that are capable of
pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to
yield sparse subnetworks that equal the accuracy of the unpruned parent
network. In this work, we propose a novel approach that exploits these sparse
subnetworks to optimize the memory utilization and communication in two popular
algorithms for parallel deep learning namely -- data and inter-layer
parallelism. We integrate our approach into AxoNN, a highly scalable framework
for parallel deep learning that relies on data and inter-layer parallelism, and
demonstrate the reduction in communication time and memory utilization. On 512
NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7
billion parameter model by 74%, and the total communication time by 40%, thus
providing an overall speedup of 34% over AxoNN, 32% over DeepSpeed-3D and 46%
over Sputnik, a sparse matrix computation baseline.",None,-1
Improving CNN-based Person Re-identification using score Normalization,0.976372,"Person re-identification (PRe-ID) is a crucial task in security,
surveillance, and retail analysis, which involves identifying an individual
across multiple cameras and views. However, it is a challenging task due to
changes in illumination, background, and viewpoint. Efficient feature
extraction and metric learning algorithms are essential for a successful PRe-ID
system. This paper proposes a novel approach for PRe-ID, which combines a
Convolutional Neural Network (CNN) based feature extraction method with
Cross-view Quadratic Discriminant Analysis (XQDA) for metric learning.
Additionally, a matching algorithm that employs Mahalanobis distance and a
score normalization process to address inconsistencies between camera scores is
implemented. The proposed approach is tested on four challenging datasets,
including VIPeR, GRID, CUHK01, and PRID450S, and promising results are
obtained. For example, without normalization, the rank-20 rate accuracies of
the GRID, CUHK01, VIPeR and PRID450S datasets were 61.92%, 83.90%, 92.03%,
96.22%; however, after score normalization, they have increased to 64.64%,
89.30%, 92.78%, and 98.76%, respectively. Accordingly, the promising results on
four challenging datasets indicate the effectiveness of the proposed approach.",None,-1
Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,0.897062,"As large language models (LLMs) have become the norm in NLP, demonstrating
good performance in generation and reasoning tasks, one of its most fatal
disadvantages is the lack of factual correctness. Generating unfactual texts
not only leads to lower performances but also degrades the trust and validity
of their applications. Chain-of-Thought (CoT) prompting improves trust and
model performance on complex reasoning tasks by generating interpretable
reasoning chains, but still suffers from factuality concerns in
knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit
framework for CoT prompting, which seeks to increase prediction factuality by
post-editing reasoning chains according to external knowledge. Building on top
of GPT-3, our framework lead to accuracy improvements in multiple open-domain
question-answering tasks.",None,-1
A Hierarchical Destroy and Repair Approach for Solving Very Large-Scale Travelling Salesman Problem,0.414693,"For prohibitively large-scale Travelling Salesman Problems (TSPs), existing
algorithms face big challenges in terms of both computational efficiency and
solution quality. To address this issue, we propose a hierarchical
destroy-and-repair (HDR) approach, which attempts to improve an initial
solution by applying a series of carefully designed destroy-and-repair
operations. A key innovative concept is the hierarchical search framework,
which recursively fixes partial edges and compresses the input instance into a
small-scale TSP under some equivalence guarantee. This neat search framework is
able to deliver highly competitive solutions within a reasonable time. Fair
comparisons based on nineteen famous large-scale instances (with 10,000 to
10,000,000 cities) show that HDR is highly competitive against existing
state-of-the-art TSP algorithms, in terms of both efficiency and solution
quality. Notably, on two large instances with 3,162,278 and 10,000,000 cities,
HDR breaks the world records (i.e., best-known results regardless of
computation time), which were previously achieved by LKH and its variants,
while HDR is completely independent of LKH. Finally, ablation studies are
performed to certify the importance and validity of the hierarchical search
framework.",None,-1
Advances and Challenges in Multimodal Remote Sensing Image Registration,0.988702,"Over the past few decades, with the rapid development of global aerospace and
aerial remote sensing technology, the types of sensors have evolved from the
traditional monomodal sensors (e.g., optical sensors) to the new generation of
multimodal sensors [e.g., multispectral, hyperspectral, light detection and
ranging (LiDAR) and synthetic aperture radar (SAR) sensors]. These advanced
devices can dynamically provide various and abundant multimodal remote sensing
images with different spatial, temporal, and spectral resolutions according to
different application requirements. Since then, it is of great scientific
significance to carry out the research of multimodal remote sensing image
registration, which is a crucial step for integrating the complementary
information among multimodal data and making comprehensive observations and
analysis of the Earths surface. In this work, we will present our own
contributions to the field of multimodal image registration, summarize the
advantages and limitations of existing multimodal image registration methods,
and then discuss the remaining challenges and make a forward-looking prospect
for the future development of the field.",None,-1
Probabilistic Prompt Learning for Dense Prediction,0.560982,"Recent progress in deterministic prompt learning has become a promising
alternative to various downstream vision tasks, enabling models to learn
powerful visual representations with the help of pre-trained vision-language
models. However, this approach results in limited performance for dense
prediction tasks that require handling more complex and diverse objects, since
a single and deterministic description cannot sufficiently represent the entire
image. In this paper, we present a novel probabilistic prompt learning to fully
exploit the vision-language knowledge in dense prediction tasks. First, we
introduce learnable class-agnostic attribute prompts to describe universal
attributes across the object class. The attributes are combined with class
information and visual-context knowledge to define the class-specific textual
distribution. Text representations are sampled and used to guide the dense
prediction task using the probabilistic pixel-text matching loss, enhancing the
stability and generalization capability of the proposed method. Extensive
experiments on different dense prediction tasks and ablation studies
demonstrate the effectiveness of our proposed method.",None,-1
Writing Assistants Should Model Social Factors of Language,0.113538,"Intelligent writing assistants powered by large language models (LLMs) are
more popular today than ever before, but their further widespread adoption is
precluded by sub-optimal performance. In this position paper, we argue that a
major reason for this sub-optimal performance and adoption is a singular focus
on the information content of language while ignoring its social aspects. We
analyze the different dimensions of these social factors in the context of
writing assistants and propose their incorporation into building smarter, more
effective, and truly personalized writing assistants that would enrich the user
experience and contribute to increased user adoption.",None,-1
Why We Don't Have AGI Yet,0.0662221,"The original vision of AI was re-articulated in 2002 via the term 'Artificial
General Intelligence' or AGI. This vision is to build 'Thinking Machines' -
computer systems that can learn, reason, and solve problems similar to the way
humans do. This is in stark contrast to the 'Narrow AI' approach practiced by
almost everyone in the field over the many decades. While several large-scale
efforts have nominally been working on AGI (most notably DeepMind), the field
of pure focused AGI development has not been well funded or promoted. This is
surprising given the fantastic value that true AGI can bestow on humanity. In
addition to the dearth of effort in this field, there are also several
theoretical and methodical missteps that are hampering progress. We highlight
why purely statistical approaches are unlikely to lead to AGI, and identify
several crucial cognitive abilities required to achieve human-like adaptability
and autonomous learning. We conclude with a survey of socio-technical factors
that have undoubtedly slowed progress towards AGI.",None,-1
Multilingual Event Extraction from Historical Newspaper Adverts,0.371531,"NLP methods can aid historians in analyzing textual materials in greater
volumes than manually feasible. Developing such methods poses substantial
challenges though. First, acquiring large, annotated historical datasets is
difficult, as only domain experts can reliably label them. Second, most
available off-the-shelf NLP models are trained on modern language texts,
rendering them significantly less effective when applied to historical corpora.
This is particularly problematic for less well studied tasks, and for languages
other than English. This paper addresses these challenges while focusing on the
under-explored task of event extraction from a novel domain of historical
texts. We introduce a new multilingual dataset in English, French, and Dutch
composed of newspaper ads from the early modern colonial period reporting on
enslaved people who liberated themselves from enslavement. We find that: 1)
even with scarce annotated data, it is possible to achieve surprisingly good
results by formulating the problem as an extractive QA task and leveraging
existing datasets and models for modern languages; and 2) cross-lingual
low-resource learning for historical languages is highly challenging, and
machine translation of the historical datasets to the considered target
languages is, in practice, often the best-performing solution.",None,-1
Modern Bayesian Experimental Design,0.963316,"Bayesian experimental design (BED) provides a powerful and general framework
for optimizing the design of experiments. However, its deployment often poses
substantial computational challenges that can undermine its practical use. In
this review, we outline how recent advances have transformed our ability to
overcome these challenges and thus utilize BED effectively, before discussing
some key areas for future development in the field.",None,-1
RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture,0.803171,"The techniques for 3D indoor scene capturing are widely used, but the meshes
produced leave much to be desired. In this paper, we propose ""RoomDreamer"",
which leverages powerful natural language to synthesize a new room with a
different style. Unlike existing image synthesis methods, our work addresses
the challenge of synthesizing both geometry and texture aligned to the input
scene structure and prompt simultaneously. The key insight is that a scene
should be treated as a whole, taking into account both scene texture and
geometry. The proposed framework consists of two significant components:
Geometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for
3D Scene guarantees the consistency of the scene style by applying the 2D prior
to the entire scene simultaneously. Mesh Optimization improves the geometry and
texture jointly and eliminates the artifacts in the scanned scene. To validate
the proposed method, real indoor scenes scanned with smartphones are used for
extensive experiments, through which the effectiveness of our method is
demonstrated.",None,-1
Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking,0.325169,"Tracking dialogue states is an essential topic in task-oriented dialogue
systems, which involve filling in the necessary information in pre-defined
slots corresponding to a schema. While general pre-trained language models have
been shown effective in slot-filling, their performance is limited when applied
to specific domains. We propose a graph-based framework that learns
domain-specific prompts by incorporating the dialogue schema. Specifically, we
embed domain-specific schema encoded by a graph neural network into the
pre-trained language model, which allows for relations in the schema to guide
the model for better adaptation to the specific domain. Our experiments
demonstrate that the proposed graph-based method outperforms other multi-domain
DST approaches while using similar or fewer trainable parameters. We also
conduct a comprehensive study of schema graph architectures, parameter usage,
and module ablation that demonstrate the effectiveness of our model on
multi-domain dialogue state tracking.",None,-1
3D-aware Blending with Generative NeRFs,0.489039,"Image blending aims to combine multiple images seamlessly. It remains
challenging for existing 2D-based methods, especially when input images are
misaligned due to differences in 3D camera poses and object shapes. To tackle
these issues, we propose a 3D-aware blending method using generative Neural
Radiance Fields (NeRF), including two key components: 3D-aware alignment and
3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of
the reference image with respect to generative NeRFs and then perform 3D local
alignment for each part. To further leverage 3D information of the generative
NeRF, we propose 3D-aware blending that directly blends images on the NeRF's
latent representation space, rather than raw pixel space. Collectively, our
method outperforms existing 2D baselines, as validated by extensive
quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.",None,-1
Data Association Aware POMDP Planning with Hypothesis Pruning Performance Guarantees,0.11938,"Autonomous agents that operate in the real world must often deal with partial
observability, which is commonly modeled as partially observable Markov
decision processes (POMDPs). However, traditional POMDP models rely on the
assumption of complete knowledge of the observation source, known as fully
observable data association. To address this limitation, we propose a planning
algorithm that maintains multiple data association hypotheses, represented as a
belief mixture, where each component corresponds to a different data
association hypothesis. However, this method can lead to an exponential growth
in the number of hypotheses, resulting in significant computational overhead.
To overcome this challenge, we introduce a pruning-based approach for planning
with ambiguous data associations. Our key contribution is to derive bounds
between the value function based on the complete set of hypotheses and the
value function based on a pruned-subset of the hypotheses, enabling us to
establish a trade-off between computational efficiency and performance. We
demonstrate how these bounds can both be used to certify any pruning heuristic
in retrospect and propose a novel approach to determine which hypotheses to
prune in order to ensure a predefined limit on the loss. We evaluate our
approach in simulated environments and demonstrate its efficacy in handling
multi-modal belief hypotheses with ambiguous data associations.",None,-1
Contrastive Learning of Sentence Embeddings from Scratch,0.436229,"Contrastive learning has been the dominant approach to train state-of-the-art
sentence embeddings. Previous studies have typically learned sentence
embeddings either through the use of human-annotated natural language inference
(NLI) data or via large-scale unlabeled sentences in an unsupervised manner.
However, even in the case of unlabeled data, their acquisition presents
challenges in certain domains due to various reasons. To address these issues,
we present SynCSE, a contrastive learning framework that trains sentence
embeddings with synthesized data. Specifically, we explore utilizing large
language models to synthesize the required data samples for contrastive
learning, including (1) producing positive and negative annotations given
unlabeled sentences (SynCSE-partial), and (2) generating sentences along with
their corresponding annotations from scratch (SynCSE-scratch). Experimental
results on sentence similarity and reranking tasks indicate that both
SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines,
and SynCSE-partial even achieves comparable performance to the supervised
models in most settings.",None,-1
Mixture of Soft Prompts for Controllable Data Generation,0.497039,"Large language models (LLMs) effectively generate fluent text when the target
output follows natural language patterns. However, structured prediction tasks
confine the output format to a limited ontology, causing even very large models
to struggle since they were never trained with such restrictions in mind. The
difficulty of using LLMs for direct prediction is exacerbated in few-shot
learning scenarios, which commonly arise due to domain shift and resource
limitations. We flip the problem on its head by leveraging the LLM as a tool
for data augmentation rather than direct prediction. Our proposed Mixture of
Soft Prompts (MSP) serves as a parameter-efficient procedure for generating
data in a controlled manner. Denoising mechanisms are further applied to
improve the quality of synthesized data. Automatic metrics show our method is
capable of producing diverse and natural text, while preserving label
semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks
when compared against strong baselines. Our method offers an alternate
data-centric approach for applying LLMs to complex prediction tasks.",None,-1
The AI Revolution: Opportunities and Challenges for the Finance Sector,0.60828,"This report examines Artificial Intelligence (AI) in the financial sector,
outlining its potential to revolutionise the industry and identify its
challenges. It underscores the criticality of a well-rounded understanding of
AI, its capabilities, and its implications to effectively leverage its
potential while mitigating associated risks. The potential of AI potential
extends from augmenting existing operations to paving the way for novel
applications in the finance sector. The application of AI in the financial
sector is transforming the industry. Its use spans areas from customer service
enhancements, fraud detection, and risk management to credit assessments and
high-frequency trading. However, along with these benefits, AI also presents
several challenges. These include issues related to transparency,
interpretability, fairness, accountability, and trustworthiness. The use of AI
in the financial sector further raises critical questions about data privacy
and security. A further issue identified in this report is the systemic risk
that AI can introduce to the financial sector. Being prone to errors, AI can
exacerbate existing systemic risks, potentially leading to financial crises.
Regulation is crucial to harnessing the benefits of AI while mitigating its
potential risks. Despite the global recognition of this need, there remains a
lack of clear guidelines or legislation for AI use in finance. This report
discusses key principles that could guide the formation of effective AI
regulation in the financial sector, including the need for a risk-based
approach, the inclusion of ethical considerations, and the importance of
maintaining a balance between innovation and consumer protection. The report
provides recommendations for academia, the finance industry, and regulators.",None,-1
Epsilon-Identifiability of Causal Quantities,0.230531,"Identifying the effects of causes and causes of effects is vital in virtually
every scientific field. Often, however, the needed probabilities may not be
fully identifiable from the data sources available. This paper shows how
partial identifiability is still possible for several probabilities of
causation. We term this epsilon-identifiability and demonstrate its usefulness
in cases where the behavior of certain subpopulations can be restricted to
within some narrow bounds. In particular, we show how unidentifiable causal
effects and counterfactual probabilities can be narrowly bounded when such
allowances are made. Often those allowances are easily measured and reasonably
assumed. Finally, epsilon-identifiability is applied to the unit selection
problem.",None,-1
Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning,0.577294,"Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph
(KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial
task that aims to predict future facts based on historical occurrences. The key
challenge lies in uncovering structural dependencies within historical
subgraphs and temporal patterns. Most existing approaches model TKGs relying on
entity modeling, as nodes in the graph play a crucial role in knowledge
representation. However, the real-world scenario often involves an extensive
number of entities, with new entities emerging over time. This makes it
challenging for entity-dependent methods to cope with extensive volumes of
entities, and effectively handling newly emerging entities also becomes a
significant challenge. Therefore, we propose Temporal Inductive Path Neural
Network (TiPNN), which models historical information in an entity-independent
perspective. Specifically, TiPNN adopts a unified graph, namely history
temporal graph, to comprehensively capture and encapsulate information from
history. Subsequently, we utilize the defined query-aware temporal paths on a
history temporal graph to model historical path information related to queries
for reasoning. Extensive experiments illustrate that the proposed model not
only attains significant performance enhancements but also handles inductive
settings, while additionally facilitating the provision of reasoning evidence
through history temporal graphs.",None,-1
Hierarchical State Abstraction Based on Structural Information Principles,0.279878,"State abstraction optimizes decision-making by ignoring irrelevant
environmental information in reinforcement learning with rich observations.
Nevertheless, recent approaches focus on adequate representational capacities
resulting in essential information loss, affecting their performances on
challenging tasks. In this article, we propose a novel mathematical Structural
Information principles-based State Abstraction framework, namely SISA, from the
information-theoretic perspective. Specifically, an unsupervised, adaptive
hierarchical state clustering method without requiring manual assistance is
presented, and meanwhile, an optimal encoding tree is generated. On each
non-root tree node, a new aggregation function and condition structural entropy
are designed to achieve hierarchical state abstraction and compensate for
sampling-induced essential information loss in state abstraction. Empirical
evaluations on a visual gridworld domain and six continuous control benchmarks
demonstrate that, compared with five SOTA state abstraction approaches, SISA
significantly improves mean episode reward and sample efficiency up to 18.98
and 44.44%, respectively. Besides, we experimentally show that SISA is a
general framework that can be flexibly integrated with different
representation-learning objectives to improve their performances further.",None,-1
Zero-Shot Image Harmonization with Generative Model Prior,0.416548,"We propose a zero-shot approach to image harmonization, aiming to overcome
the reliance on large amounts of synthetic composite images in existing
methods. These methods, while showing promising results, involve significant
training expenses and often struggle with generalization to unseen images. To
this end, we introduce a fully modularized framework inspired by human
behavior. Leveraging the reasoning capabilities of recent foundation models in
language and vision, our approach comprises three main stages. Initially, we
employ a pretrained vision-language model (VLM) to generate descriptions for
the composite image. Subsequently, these descriptions guide the foreground
harmonization direction of a text-to-image generative model (T2I). We refine
text embeddings for enhanced representation of imaging conditions and employ
self-attention and edge maps for structure preservation. Following each
harmonization iteration, an evaluator determines whether to conclude or modify
the harmonization direction. The resulting framework, mirroring human behavior,
achieves harmonious results without the need for extensive training. We present
compelling visual results across diverse scenes and objects, along with a user
study validating the effectiveness of our approach.",None,-1
Image-text Retrieval via Preserving Main Semantics of Vision,0.0570601,"Image-text retrieval is one of the major tasks of cross-modal retrieval.
Several approaches for this task map images and texts into a common space to
create correspondences between the two modalities. However, due to the content
(semantics) richness of an image, redundant secondary information in an image
may cause false matches. To address this issue, this paper presents a semantic
optimization approach, implemented as a Visual Semantic Loss (VSL), to assist
the model in focusing on an image's main content. This approach is inspired by
how people typically annotate the content of an image by describing its main
content. Thus, we leverage the annotated texts corresponding to an image to
assist the model in capturing the main content of the image, reducing the
negative impact of secondary content. Extensive experiments on two benchmark
datasets (MSCOCO and Flickr30K) demonstrate the superior performance of our
method. The code is available at: https://github.com/ZhangXu0963/VSL.",None,-1
Marathi-English Code-mixed Text Generation,0.0650973,"Code-mixing, the blending of linguistic elements from distinct languages to
form meaningful sentences, is common in multilingual settings, yielding hybrid
languages like Hinglish and Minglish. Marathi, India's third most spoken
language, often integrates English for precision and formality. Developing
code-mixed language systems, like Marathi-English (Minglish), faces resource
constraints. This research introduces a Marathi-English code-mixed text
generation algorithm, assessed with Code Mixing Index (CMI) and Degree of Code
Mixing (DCM) metrics. Across 2987 code-mixed questions, it achieved an average
CMI of 0.2 and an average DCM of 7.4, indicating effective and comprehensible
code-mixed sentences. These results offer potential for enhanced NLP tools,
bridging linguistic gaps in multilingual societies.",None,-1
Assessing the Impact of Noise on Quantum Neural Networks: An Experimental Analysis,0.401511,"In the race towards quantum computing, the potential benefits of quantum
neural networks (QNNs) have become increasingly apparent. However, Noisy
Intermediate-Scale Quantum (NISQ) processors are prone to errors, which poses a
significant challenge for the execution of complex algorithms or quantum
machine learning. To ensure the quality and security of QNNs, it is crucial to
explore the impact of noise on their performance. This paper provides a
comprehensive analysis of the impact of noise on QNNs, examining the Mottonen
state preparation algorithm under various noise models and studying the
degradation of quantum states as they pass through multiple layers of QNNs.
Additionally, the paper evaluates the effect of noise on the performance of
pre-trained QNNs and highlights the challenges posed by noise models in quantum
computing. The findings of this study have significant implications for the
development of quantum software, emphasizing the importance of prioritizing
stability and noise-correction measures when developing QNNs to ensure reliable
and trustworthy results. This paper contributes to the growing body of
literature on quantum computing and quantum machine learning, providing new
insights into the impact of noise on QNNs and paving the way towards the
development of more robust and efficient quantum algorithms.",None,-1
System 2 Attention (is something you might need too),0.710487,"Soft attention in Transformer-based Large Language Models (LLMs) is
susceptible to incorporating irrelevant information from the context into its
latent representations, which adversely affects next token generations. To help
rectify these issues, we introduce System 2 Attention (S2A), which leverages
the ability of LLMs to reason in natural language and follow instructions in
order to decide what to attend to. S2A regenerates the input context to only
include the relevant portions, before attending to the regenerated context to
elicit the final response. In experiments, S2A outperforms standard
attention-based LLMs on three tasks containing opinion or irrelevant
information, QA, math word problems and longform generation, where S2A
increases factuality and objectivity, and decreases sycophancy.",None,-1
Class-Balancing Diffusion Models,0.185954,"Diffusion-based models have shown the merits of generating high-quality
visual data while preserving better diversity in recent studies. However, such
observation is only justified with curated data distribution, where the data
samples are nicely pre-processed to be uniformly distributed in terms of their
labels. In practice, a long-tailed data distribution appears more common and
how diffusion models perform on such class-imbalanced data remains unknown. In
this work, we first investigate this problem and observe significant
degradation in both diversity and fidelity when the diffusion model is trained
on datasets with class-imbalanced distributions. Especially in tail classes,
the generations largely lose diversity and we observe severe mode-collapse
issues. To tackle this problem, we set from the hypothesis that the data
distribution is not class-balanced, and propose Class-Balancing Diffusion
Models (CBDM) that are trained with a distribution adjustment regularizer as a
solution. Experiments show that images generated by CBDM exhibit higher
diversity and quality in both quantitative and qualitative ways. Our method
benchmarked the generation results on CIFAR100/CIFAR100LT dataset and shows
outstanding performance on the downstream recognition task.",None,-1
Character-LLM: A Trainable Agent for Role-Playing,0.982278,"Large language models (LLMs) can be used to serve as agents to simulate human
behaviors, given the powerful ability to understand human instructions and
provide high-quality generated texts. Such ability stimulates us to wonder
whether LLMs can simulate a person in a higher form than simple human
behaviors. Therefore, we aim to train an agent with the profile, experience,
and emotional states of a specific person instead of using limited prompts to
instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs
to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,
etc. Our method focuses on editing profiles as experiences of a certain
character and training models to be personal simulacra with these experiences.
To assess the effectiveness of our approach, we build a test playground that
interviews trained agents and evaluates whether the agents \textit{memorize}
their characters and experiences. Experimental results show interesting
observations that help build future simulacra of humankind.",None,-1
Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification,0.471349,"Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuning
techniques designed to make the training of language models more efficient.
Previous results demonstrated that these methods can even improve performance
on some classification tasks. This paper complements the existing research by
investigating how these techniques influence the classification performance and
computation costs compared to full fine-tuning when applied to multilingual
text classification tasks (genre, framing, and persuasion techniques detection;
with different input lengths, number of predicted classes and classification
difficulty), some of which have limited training data. In addition, we conduct
in-depth analyses of their efficacy across different training scenarios
(training on the original multilingual data; on the translations into English;
and on a subset of English-only data) and different languages. Our findings
provide valuable insights into the applicability of the parameter-efficient
fine-tuning techniques, particularly to complex multilingual and multilabel
classification tasks.",None,-1
SegGPT Meets Co-Saliency Scene,0.277547,"Co-salient object detection targets at detecting co-existed salient objects
among a group of images. Recently, a generalist model for segmenting everything
in context, called SegGPT, is gaining public attention. In view of its
breakthrough for segmentation, we can hardly wait to probe into its
contribution to the task of co-salient object detection. In this report, we
first design a framework to enable SegGPT for the problem of co-salient object
detection. Proceed to the next step, we evaluate the performance of SegGPT on
the problem of co-salient object detection on three available datasets. We
achieve a finding that co-saliency scenes challenges SegGPT due to context
discrepancy within a group of co-saliency images.",None,-1
Exploring Partial Knowledge Base Inference in Biomedical Entity Linking,0.437878,"Biomedical entity linking (EL) consists of named entity recognition (NER) and
named entity disambiguation (NED). EL models are trained on corpora labeled by
a predefined KB. However, it is a common scenario that only entities within a
subset of the KB are precious to stakeholders. We name this scenario partial
knowledge base inference: training an EL model with one KB and inferring on the
part of it without further training. In this work, we give a detailed
definition and evaluation procedures for this practically valuable but
significantly understudied scenario and evaluate methods from three
representative EL paradigms. We construct partial KB inference benchmarks and
witness a catastrophic degradation in EL performance due to dramatically
precision drop. Our findings reveal these EL paradigms can not correctly handle
unlinkable mentions (NIL), so they are not robust to partial KB inference. We
also propose two simple-and-effective redemption methods to combat the NIL
issue with little computational overhead. Codes are released at
https://github.com/Yuanhy1997/PartialKB-EL.",None,-1
"Syntax and Semantics Meet in the ""Middle"": Probing the Syntax-Semantics Interface of LMs Through Agentivity",0.417362,"Recent advances in large language models have prompted researchers to examine
their abilities across a variety of linguistic tasks, but little has been done
to investigate how models handle the interactions in meaning across words and
larger syntactic forms -- i.e. phenomena at the intersection of syntax and
semantics. We present the semantic notion of agentivity as a case study for
probing such interactions. We created a novel evaluation dataset by utilitizing
the unique linguistic properties of a subset of optionally transitive English
verbs. This dataset was used to prompt varying sizes of three model classes to
see if they are sensitive to agentivity at the lexical level, and if they can
appropriately employ these word-level priors given a specific syntactic
context. Overall, GPT-3 text-davinci-003 performs extremely well across all
experiments, outperforming all other models tested by far. In fact, the results
are even better correlated with human judgements than both syntactic and
semantic corpus statistics. This suggests that LMs may potentially serve as
more useful tools for linguistic annotation, theory testing, and discovery than
select corpora for certain tasks. Code is available at
https://github.com/lindiatjuatja/lm_sem",None,-1
DIFFQG: Generating Questions to Summarize Factual Changes,0.15028,"Identifying the difference between two versions of the same article is useful
to update knowledge bases and to understand how articles evolve. Paired texts
occur naturally in diverse situations: reporters write similar news stories and
maintainers of authoritative websites must keep their information up to date.
We propose representing factual changes between paired documents as
question-answer pairs, where the answer to the same question differs between
two versions. We find that question-answer pairs can flexibly and concisely
capture the updated contents. Provided with paired documents, annotators
identify questions that are answered by one passage but answered differently or
cannot be answered by the other. We release DIFFQG which consists of 759 QA
pairs and 1153 examples of paired passages with no factual change. These
questions are intended to be both unambiguous and information-seeking and
involve complex edits, pushing beyond the capabilities of current question
generation and factual change detection systems. Our dataset summarizes the
changes between two versions of the document as questions and answers, studying
automatic update summarization in a novel way.",None,-1
AMR Parsing with Instruction Fine-tuned Pre-trained Language Models,0.239704,"Instruction fine-tuned language models on a collection of instruction
annotated datasets (FLAN) have shown highly effective to improve model
performance and generalization to unseen tasks. However, a majority of standard
parsing tasks including abstract meaning representation (AMR), universal
dependency (UD), semantic role labeling (SRL) has been excluded from the FLAN
collections for both model training and evaluations. In this paper, we take one
of such instruction fine-tuned pre-trained language models, i.e. FLAN-T5, and
fine-tune them for AMR parsing. Our extensive experiments on various AMR
parsing tasks including AMR2.0, AMR3.0 and BioAMR indicate that FLAN-T5
fine-tuned models out-perform previous state-of-the-art models across all
tasks. In addition, full fine-tuning followed by the parameter efficient
fine-tuning, LoRA, further improves the model performances, setting new
state-of-the-arts in Smatch on AMR2.0 (86.4), AMR3.0 (84.9) and BioAMR (82.3).",None,-1
Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning,0.375229,"Hateful memes have emerged as a significant concern on the Internet.
Detecting hateful memes requires the system to jointly understand the visual
and textual modalities. Our investigation reveals that the embedding space of
existing CLIP-based systems lacks sensitivity to subtle differences in memes
that are vital for correct hatefulness classification. We propose constructing
a hatefulness-aware embedding space through retrieval-guided contrastive
training. Our approach achieves state-of-the-art performance on the
HatefulMemes dataset with an AUROC of 87.0, outperforming much larger
fine-tuned large multimodal models. We demonstrate a retrieval-based hateful
memes detection system, which is capable of identifying hatefulness based on
data unseen in training. This allows developers to update the hateful memes
detection system by simply adding new examples without retraining, a desirable
feature for real services in the constantly evolving landscape of hateful memes
on the Internet.",None,-1
NeuroX Library for Neuron Analysis of Deep NLP Models,0.741346,"Neuron analysis provides insights into how knowledge is structured in
representations and discovers the role of neurons in the network. In addition
to developing an understanding of our models, neuron analysis enables various
applications such as debiasing, domain adaptation and architectural search. We
present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis
of natural language processing models. It implements various interpretation
methods under a unified API, and provides a framework for data processing and
evaluation, thus making it easier for researchers and practitioners to perform
neuron analysis. The Python toolkit is available at
https://www.github.com/fdalvi/NeuroX. Demo Video available at
https://youtu.be/mLhs2YMx4u8.",None,-1
Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields,0.415134,"Synthesizing photo-realistic images from a point cloud is challenging because
of the sparsity of point cloud representation. Recent Neural Radiance Fields
and extensions are proposed to synthesize realistic images from 2D input. In
this paper, we present Point2Pix as a novel point renderer to link the 3D
sparse point clouds with 2D dense image pixels. Taking advantage of the point
cloud 3D prior and NeRF rendering pipeline, our method can synthesize
high-quality images from colored point clouds, generally for novel indoor
scenes. To improve the efficiency of ray sampling, we propose point-guided
sampling, which focuses on valid samples. Also, we present Point Encoding to
build Multi-scale Radiance Fields that provide discriminative 3D point
features. Finally, we propose Fusion Encoding to efficiently synthesize
high-quality images. Extensive experiments on the ScanNet and ArkitScenes
datasets demonstrate the effectiveness and generalization.",None,-1
iDML: Incentivized Decentralized Machine Learning,0.313745,"With the rising emergence of decentralized and opportunistic approaches to
machine learning, end devices are increasingly tasked with training deep
learning models on-devices using crowd-sourced data that they collect
themselves. These approaches are desirable from a resource consumption
perspective and also from a privacy preservation perspective. When the devices
benefit directly from the trained models, the incentives are implicit -
contributing devices' resources are incentivized by the availability of the
higher-accuracy model that results from collaboration. However, explicit
incentive mechanisms must be provided when end-user devices are asked to
contribute their resources (e.g., computation, communication, and data) to a
task performed primarily for the benefit of others, e.g., training a model for
a task that a neighbor device needs but the device owner is uninterested in. In
this project, we propose a novel blockchain-based incentive mechanism for
completely decentralized and opportunistic learning architectures. We leverage
a smart contract not only for providing explicit incentives to end devices to
participate in decentralized learning but also to create a fully decentralized
mechanism to inspect and reflect on the behavior of the learning architecture.",None,-1
Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?,0.526926,"The rapid advancement of Large Language Models (LLMs) has spurred discussions
about their potential to enhance quantitative trading strategies. LLMs excel in
analyzing sentiments about listed companies from financial news, providing
critical insights for trading decisions. However, the performance of LLMs in
this task varies substantially due to their inherent characteristics. This
paper introduces a standardized experimental procedure for comprehensive
evaluations. We detail the methodology using three distinct LLMs, each
embodying a unique approach to performance enhancement, applied specifically to
the task of sentiment factor extraction from large volumes of Chinese news
summaries. Subsequently, we develop quantitative trading strategies using these
sentiment factors and conduct back-tests in realistic scenarios. Our results
will offer perspectives about the performances of Large Language Models applied
to extracting sentiments from Chinese news texts.",None,-1
The Hardness of Reasoning about Probabilities and Causality,0.77687,"We study formal languages which are capable of fully expressing quantitative
probabilistic reasoning and do-calculus reasoning for causal effects, from a
computational complexity perspective. We focus on satisfiability problems whose
instance formulas allow expressing many tasks in probabilistic and causal
inference. The main contribution of this work is establishing the exact
computational complexity of these satisfiability problems. We introduce a new
natural complexity class, named succ$\exists$R, which can be viewed as a
succinct variant of the well-studied class $\exists$R, and show that the
problems we consider are complete for succ$\exists$R. Our results imply even
stronger algorithmic limitations than were proven by Fagin, Halpern, and
Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of
the standard languages used commonly in probabilistic and causal inference.",None,-1
DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning,0.666813,"In this paper, we introduce self-distillation and online clustering for
self-supervised speech representation learning (DinoSR) which combines masked
language modeling, self-distillation, and online clustering. We show that these
concepts complement each other and result in a strong representation learning
model for speech. DinoSR first extracts contextualized embeddings from the
input audio with a teacher network, then runs an online clustering system on
the embeddings to yield a machine-discovered phone inventory, and finally uses
the discretized tokens to guide a student network. We show that DinoSR
surpasses previous state-of-the-art performance in several downstream tasks,
and provide a detailed analysis of the model and the learned discrete units.",None,-1
Is Writing Prompts Really Making Art?,0.565379,"In recent years Generative Machine Learning systems have advanced
significantly. A current wave of generative systems use text prompts to create
complex imagery, video, even 3D datasets. The creators of these systems claim a
revolution in bringing creativity and art to anyone who can type a prompt. In
this position paper, we question the basis for these claims, dividing our
analysis into three areas: the limitations of linguistic descriptions,
implications of the dataset, and lastly, matters of materiality and embodiment.
We conclude with an analysis of the creative possibilities enabled by
prompt-based systems, asking if they can be considered a new artistic medium.",None,-1
Low-Shot Learning for Fictional Claim Verification,0.0613599,"In this paper, we study the problem of claim verification in the context of
claims about fictional stories in a low-shot learning setting. To this end, we
generate two synthetic datasets and then develop an end-to-end pipeline and
model that is tested on both benchmarks. To test the efficacy of our pipeline
and the difficulty of benchmarks, we compare our models' results against human
and random assignment results. Our code is available at
https://github.com/Derposoft/plot_hole_detection.",None,-1
Emergent Communication with Attention,0.437692,"To develop computational agents that better communicate using their own
emergent language, we endow the agents with an ability to focus their attention
on particular concepts in the environment. Humans often understand an object or
scene as a composite of concepts and those concepts are further mapped onto
words. We implement this intuition as cross-modal attention mechanisms in
Speaker and Listener agents in a referential game and show attention leads to
more compositional and interpretable emergent language. We also demonstrate how
attention aids in understanding the learned communication protocol by
investigating the attention weights associated with each message symbol and the
alignment of attention weights between Speaker and Listener agents. Overall,
our results suggest that attention is a promising mechanism for developing more
human-like emergent language.",None,-1
Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources,0.195585,"To address the data scarcity issue in Conversational question answering
(ConvQA), a dialog inpainting method, which utilizes documents to generate
ConvQA datasets, has been proposed. However, the original dialog inpainting
model is trained solely on the dialog reconstruction task, resulting in the
generation of questions with low contextual relevance due to insufficient
learning of question-answer alignment. To overcome this limitation, we propose
a novel framework called Dialogizer, which has the capability to automatically
generate ConvQA datasets with high contextual relevance from textual sources.
The framework incorporates two training tasks: question-answer matching (QAM)
and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted
during the inference phase based on the contextual relevance of the generated
questions. Using our framework, we produce four ConvQA datasets by utilizing
documents from multiple domains as the primary source. Through automatic
evaluation using diverse metrics, as well as human evaluation, we validate that
our proposed framework exhibits the ability to generate datasets of higher
quality compared to the baseline dialog inpainting model.",None,-1
Compressing Context to Enhance Inference Efficiency of Large Language Models,0.142808,"Large language models (LLMs) achieved remarkable performance across various
tasks. However, they face challenges in managing long documents and extended
conversations, due to significantly increased computational requirements, both
in memory and inference time, and potential context truncation when the input
exceeds the LLM's fixed context length. This paper proposes a method called
Selective Context that enhances the inference efficiency of LLMs by identifying
and pruning redundancy in the input context to make the input more compact. We
test our approach using common data sources requiring long context processing:
arXiv papers, news articles, and long conversations, on tasks of summarisation,
question answering, and response generation. Experimental results show that
Selective Context significantly reduces memory cost and decreases generation
latency while maintaining comparable performance compared to that achieved when
full context is used. Specifically, we achieve a 50\% reduction in context
cost, resulting in a 36\% reduction in inference memory usage and a 32\%
reduction in inference time, while observing only a minor drop of .023 in
BERTscore and .038 in faithfulness on four downstream applications, indicating
that our method strikes a good balance between efficiency and performance.",None,-1
Vision Transformer for Action Units Detection,0.963505,"Facial Action Units detection (FAUs) represents a fine-grained classification
problem that involves identifying different units on the human face, as defined
by the Facial Action Coding System. In this paper, we present a simple yet
efficient Vision Transformer-based approach for addressing the task of Action
Units (AU) detection in the context of Affective Behavior Analysis in-the-wild
(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to
capture the temporal facial change in the video. Besides, to reduce massive
size of the Vision Transformers model, we replace the ViViT feature extraction
layers with the CNN backbone (Regnet). Our model outperform the baseline model
of ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,
the achieved results are comparable to those of the top three teams in the
previous ABAW 2022 challenge.",None,-1
Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected,0.848982,"Meta AI Research has recently released SAM (Segment Anything Model) which is
trained on a large segmentation dataset of over 1 billion masks. As a
foundation model in the field of computer vision, SAM (Segment Anything Model)
has gained attention for its impressive performance in generic object
segmentation. Despite its strong capability in a wide range of zero-shot
transfer tasks, it remains unknown whether SAM can detect things in challenging
setups like transparent objects. In this work, we perform an empirical
evaluation of two glass-related challenging scenarios: mirror and transparent
objects. We found that SAM often fails to detect the glass in both scenarios,
which raises concern for deploying the SAM in safety-critical situations that
have various forms of glass.",None,-1
Tracking Progress in Multi-Agent Path Finding,0.89821,"Multi-Agent Path Finding (MAPF) is an important core problem for many new and
emerging industrial applications. Many works appear on this topic each year,
and a large number of substantial advancements and performance improvements
have been reported. Yet measuring overall progress in MAPF is difficult: there
are many potential competitors, and the computational burden for comprehensive
experimentation is prohibitively large. Moreover, detailed data from past
experimentation is usually unavailable. In this work, we introduce a set of
methodological and visualisation tools which can help the community establish
clear indicators for state-of-the-art MAPF performance and which can facilitate
large-scale comparisons between MAPF solvers. Our objectives are to lower the
barrier of entry for new researchers and to further promote the study of MAPF,
since progress in the area and the main challenges are made much clearer.",None,-1
When to Trust AI: Advances and Challenges for Certification of Neural Networks,0.906671,"Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.",None,-1
From task structures to world models: What do LLMs know?,0.513251,"In what sense does a large language model have knowledge? The answer to this
question extends beyond the capabilities of a particular AI system, and
challenges our assumptions about the nature of knowledge and intelligence. We
answer by granting LLMs ""instrumental knowledge""; knowledge defined by a
certain set of abilities. We then ask how such knowledge is related to the more
ordinary, ""worldly"" knowledge exhibited by human agents, and explore this in
terms of the degree to which instrumental knowledge can be said to incorporate
the structured world models of cognitive science. We discuss ways LLMs could
recover degrees of worldly knowledge, and suggest such recovery will be
governed by an implicit, resource-rational tradeoff between world models and
task demands.",None,-1
SFD2: Semantic-guided Feature Detection and Description,0.90005,"Visual localization is a fundamental task for various applications including
autonomous driving and robotics. Prior methods focus on extracting large
amounts of often redundant locally reliable features, resulting in limited
efficiency and accuracy, especially in large-scale environments under
challenging conditions. Instead, we propose to extract globally reliable
features by implicitly embedding high-level semantics into both the detection
and description processes. Specifically, our semantic-aware detector is able to
detect keypoints from reliable regions (e.g. building, traffic lane) and
suppress unreliable areas (e.g. sky, car) implicitly instead of relying on
explicit semantic labels. This boosts the accuracy of keypoint matching by
reducing the number of features sensitive to appearance changes and avoiding
the need of additional segmentation networks at test time. Moreover, our
descriptors are augmented with semantics and have stronger discriminative
ability, providing more inliers at test time. Particularly, experiments on
long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons
datasets demonstrate that our model outperforms previous local features and
gives competitive accuracy to advanced matchers but is about 2 and 3 times
faster when using 2k and 4k keypoints, respectively.",None,-1
Multimodality of AI for Education: Towards Artificial General Intelligence,0.888273,"This paper presents a comprehensive examination of how multimodal artificial
intelligence (AI) approaches are paving the way towards the realization of
Artificial General Intelligence (AGI) in educational contexts. It scrutinizes
the evolution and integration of AI in educational systems, emphasizing the
crucial role of multimodality, which encompasses auditory, visual, kinesthetic,
and linguistic modes of learning. This research delves deeply into the key
facets of AGI, including cognitive frameworks, advanced knowledge
representation, adaptive learning mechanisms, strategic planning, sophisticated
language processing, and the integration of diverse multimodal data sources. It
critically assesses AGI's transformative potential in reshaping educational
paradigms, focusing on enhancing teaching and learning effectiveness, filling
gaps in existing methodologies, and addressing ethical considerations and
responsible usage of AGI in educational settings. The paper also discusses the
implications of multimodal AI's role in education, offering insights into
future directions and challenges in AGI development. This exploration aims to
provide a nuanced understanding of the intersection between AI, multimodality,
and education, setting a foundation for future research and development in AGI.",None,-1
Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler,0.236994,"We propose a neuralized undirected graphical model called Neural-Hidden-CRF
to solve the weakly-supervised sequence labeling problem. Under the umbrella of
probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded
with a hidden CRF layer models the variables of word sequence, latent ground
truth sequence, and weak label sequence with the global perspective that
undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can
capitalize on the powerful language model BERT or other deep models to provide
rich contextual semantic knowledge to the latent ground truth sequence, and use
the hidden CRF layer to capture the internal label dependencies.
Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains
new state-of-the-art results on one crowdsourcing benchmark and three
weak-supervision benchmarks, including outperforming the recent advanced model
CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and
inference performance, respectively.",None,-1
ENTL: Embodied Navigation Trajectory Learner,0.239324,"We propose Embodied Navigation Trajectory Learner (ENTL), a method for
extracting long sequence representations for embodied navigation. Our approach
unifies world modeling, localization and imitation learning into a single
sequence prediction task. We train our model using vector-quantized predictions
of future states conditioned on current states and actions. ENTL's generic
architecture enables sharing of the spatio-temporal sequence encoder for
multiple challenging embodied tasks. We achieve competitive performance on
navigation tasks using significantly less data than strong baselines while
performing auxiliary tasks such as localization and future frame prediction (a
proxy for world modeling). A key property of our approach is that the model is
pre-trained without any explicit reward signal, which makes the resulting model
generalizable to multiple tasks and environments.",None,-1
Improving User Controlled Table-To-Text Generation Robustness,0.417825,"In this work we study user controlled table-to-text generation where users
explore the content in a table by selecting cells and reading a natural
language description thereof automatically produce by a natural language
generator. Such generation models usually learn from carefully selected cell
combinations (clean cell selections); however, in practice users may select
unexpected, redundant, or incoherent cell combinations (noisy cell selections).
In experiments, we find that models perform well on test sets coming from the
same distribution as the train data but their performance drops when evaluated
on realistic noisy user inputs. We propose a fine-tuning regime with additional
user-simulated noisy cell selections. Models fine-tuned with the proposed
regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test
cases; and achieve comparable state-of-the-art performance on the ToTTo
dataset.",None,-1
AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling,0.699167,"Business optimisation refers to the process of finding and implementing
efficient and cost-effective means of operation to bring a competitive
advantage for businesses. Synthesizing problem formulations is an integral part
of business optimisation, which relies on human expertise to construct problem
formulations using optimisation languages. Interestingly, with advancements in
Large Language Models (LLMs), the human expertise needed in problem formulation
can be minimized. However, developing an LLM for problem formulation is
challenging, due to training data, token limitations, and lack of appropriate
performance metrics. For the requirement of training data, recent attention has
been directed towards fine-tuning pre-trained LLMs for downstream tasks rather
than training an LLM from scratch for a specific task. In this paper, we adopt
an LLM fine-tuning approach and propose an AI-Copilot for business optimisation
problem formulation. For token limitations, we introduce modularization and
prompt engineering techniques to synthesize complex problem formulations as
modules that fit into the token limits of LLMs. Additionally, we design
performance evaluation metrics that are better suited for assessing the
accuracy and quality of problem formulations. The experiment results
demonstrate that with this approach we can synthesize complex and large problem
formulations for a typical business optimisation problem in production
scheduling.",None,-1
Everyone Deserves A Reward: Learning Customized Human Preferences,0.750768,"Reward models (RMs) are essential for aligning large language models (LLMs)
with human preferences to improve interaction quality. However, the real world
is pluralistic, which leads to diversified human preferences with respect to
different religions, politics, cultures, etc. Moreover, each individual can
have their unique preferences on various topics. Neglecting the diversity of
human preferences, current human feedback aligning methods only consider a
general reward model, which is below satisfaction for customized or
personalized application scenarios. To explore customized preference learning,
we collect a domain-specific preference (DSP) dataset, which includes preferred
responses for each given query from four practical domains. Besides, from the
perspective of data efficiency, we propose a three-stage customized RM learning
scheme, then empirically verify its effectiveness on both general preference
datasets and our DSP set. Furthermore, we test multiple training and data
strategies on the three learning stages. We find several ways to better
preserve the general preferring ability while training the customized RMs,
especially general preference enrichment, and customized preference imitation
learning. The DSP dataset and code are available at
https://github.com/Linear95/DSP.",None,-1
Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation,0.526661,"Interpretability and efficiency are two important considerations for the
adoption of neural automatic metrics. In this work, we develop
strong-performing automatic metrics for reference-based summarization
evaluation, based on a two-stage evaluation pipeline that first extracts basic
information units from one text sequence and then checks the extracted units in
another sequence. The metrics we developed include two-stage metrics that can
provide high interpretability at both the fine-grained unit level and summary
level, and one-stage metrics that achieve a balance between efficiency and
interpretability. We make the developed tools publicly available at
https://github.com/Yale-LILY/AutoACU.",None,-1
Unsupervised augmentation optimization for few-shot medical image segmentation,0.103636,"The augmentation parameters matter to few-shot semantic segmentation since
they directly affect the training outcome by feeding the networks with varying
perturbated samples. However, searching optimal augmentation parameters for
few-shot segmentation models without annotations is a challenge that current
methods fail to address. In this paper, we first propose a framework to
determine the ``optimal'' parameters without human annotations by solving a
distribution-matching problem between the intra-instance and intra-class
similarity distribution, with the intra-instance similarity describing the
similarity between the original sample of a particular anatomy and its
augmented ones and the intra-class similarity representing the similarity
between the selected sample and the others in the same class. Extensive
experiments demonstrate the superiority of our optimized augmentation in
boosting few-shot segmentation models. We greatly improve the top competing
method by 1.27\% and 1.11\% on Abd-MRI and Abd-CT datasets, respectively, and
even achieve a significant improvement for SSL-ALP on the left kidney by 3.39\%
on the Abd-CT dataset.",None,-1
Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design,0.66509,"Most of the current hypergraph learning methodologies and benchmarking
datasets in the hypergraph realm are obtained by lifting procedures from their
graph analogs, leading to overshadowing specific characteristics of
hypergraphs. This paper attempts to confront some pending questions in that
regard: Q1 Can the concept of homophily play a crucial role in Hypergraph
Neural Networks (HNNs)? Q2 Is there room for improving current HNN
architectures by carefully addressing specific characteristics of higher-order
networks? Q3 Do existing datasets provide a meaningful benchmark for HNNs? To
address them, we first introduce a novel conceptualization of homophily in
higher-order networks based on a Message Passing (MP) scheme, unifying both the
analytical examination and the modeling of higher-order networks. Further, we
investigate some natural, yet mostly unexplored, strategies for processing
higher-order structures within HNNs such as keeping hyperedge-dependent node
representations, or performing node/hyperedge stochastic samplings, leading us
to the most general MP formulation up to date -MultiSet-, as well as to an
original architecture design, MultiSetMixer. Finally, we conduct an extensive
set of experiments that contextualize our proposals and successfully provide
insights about our inquiries.",None,-1
"If consciousness is dynamically relevant, artificial intelligence isn't conscious",0.035565,"We demonstrate that if consciousness is relevant for the temporal evolution
of a system's states--that is, if it is dynamically relevant--then AI systems
cannot be conscious. That is because AI systems run on CPUs, GPUs, TPUs or
other processors which have been designed and verified to adhere to
computational dynamics that systematically preclude or suppress deviations. The
design and verification preclude or suppress, in particular, potential
consciousness-related dynamical effects, so that if consciousness is
dynamically relevant, AI systems cannot be conscious.",None,-1
Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA,0.292794,"Large Language Models (LLMs) have shown outstanding performance across wide
range of downstream tasks. This competency is attributed to their substantial
parameter size and pre-training on extensive corpus. Moreover, LLMs have
exhibited enhanced reasoning capabilities in tackling complex reasoning tasks,
owing to the utilization of a method named ``Chain-of-Thought (CoT)
prompting''. This method is designed to generate intermediate reasoning steps
that guide the inference of the final answer. However, it is essential to
highlight that these advanced reasoning abilities appear to emerge in models
with a minimum of 10 billion parameters, thereby limiting its efficacy in
situations where computational resources are constrained. In this paper, we
investigate the possibility of transferring the reasoning capabilities of LLMs
to smaller models via knowledge distillation. Specifically, we propose Sci-CoT,
a two-stage framework that separates the processes of generating rationales and
inferring answers. This method enables a more efficient use of rationales
during the answer inference stage, leading to improved performance on
scientific question-answering tasks. Utilizing Sci-CoT, our 80-million
parameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy
dataset under the few shot setting.",None,-1
Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Clouds,0.215863,"We build rearticulable models for arbitrary everyday man-made objects
containing an arbitrary number of parts that are connected together in
arbitrary ways via 1 degree-of-freedom joints. Given point cloud videos of such
everyday objects, our method identifies the distinct object parts, what parts
are connected to what other parts, and the properties of the joints connecting
each part pair. We do this by jointly optimizing the part segmentation,
transformation, and kinematics using a novel energy minimization framework. Our
inferred animatable models, enables retargeting to novel poses with sparse
point correspondences guidance. We test our method on a new articulating robot
dataset, and the Sapiens dataset with common daily objects, as well as
real-world scans. Experiments show that our method outperforms two leading
prior works on various metrics.",None,-1
Explicit Visual Prompting for Low-Level Structure Segmentations,0.999731,"We consider the generic problem of detecting low-level structures in images,
which includes segmenting the manipulated parts, identifying out-of-focus
pixels, separating shadow regions, and detecting concealed objects. Whereas
each such topic has been typically addressed with a domain-specific solution,
we show that a unified approach performs well across all of them. We take
inspiration from the widely-used pre-training and then prompt tuning protocols
in NLP and propose a new visual prompting model, named Explicit Visual
Prompting (EVP). Different from the previous visual prompting which is
typically a dataset-level implicit embedding, our key insight is to enforce the
tunable parameters focusing on the explicit visual content from each individual
image, i.e., the features from frozen patch embeddings and the input's
high-frequency components. The proposed EVP significantly outperforms other
parameter-efficient tuning protocols under the same amount of tunable
parameters (5.7% extra trainable parameters of each task). EVP also achieves
state-of-the-art performances on diverse low-level structure segmentation tasks
compared to task-specific solutions. Our code is available at:
https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",None,-1
Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model,0.658321,"We study the incentivized information acquisition problem, where a principal
hires an agent to gather information on her behalf. Such a problem is modeled
as a Stackelberg game between the principal and the agent, where the principal
announces a scoring rule that specifies the payment, and then the agent then
chooses an effort level that maximizes her own profit and reports the
information. We study the online setting of such a problem from the principal's
perspective, i.e., designing the optimal scoring rule by repeatedly interacting
with the strategic agent. We design a provably sample efficient algorithm that
tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a
sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a
delicate estimation procedure for the optimal profit of the principal, and a
conservative correction scheme that ensures the desired agent's actions are
incentivized. Furthermore, a key feature of our regret bound is that it is
independent of the number of states of the environment.",None,-1
Text-to-Image Diffusion Models are Zero-Shot Classifiers,0.60156,"The excellent generative capabilities of text-to-image diffusion models
suggest they learn informative representations of image-text data. However,
what knowledge their representations capture is not fully understood, and they
have not been thoroughly explored on downstream tasks. We investigate diffusion
models by proposing a method for evaluating them as zero-shot classifiers. The
key idea is using a diffusion model's ability to denoise a noised image given a
text description of a label as a proxy for that label's likelihood. We apply
our method to Stable Diffusion and Imagen, using it to probe fine-grained
aspects of the models' knowledge and comparing them with CLIP's zero-shot
abilities. They perform competitively with CLIP on a wide range of zero-shot
image classification datasets. Additionally, they achieve state-of-the-art
results on shape/texture bias tests and can successfully perform attribute
binding while CLIP cannot. Although generative pre-training is prevalent in
NLP, visual foundation models often use other methods such as contrastive
learning. Based on our findings, we argue that generative pre-training should
be explored as a compelling alternative for vision-language tasks.",None,-1
The Pipeline for the Continuous Development of Artificial Intelligence Models -- Current State of Research and Practice,0.617448,"Companies struggle to continuously develop and deploy AI models to complex
production systems due to AI characteristics while assuring quality. To ease
the development process, continuous pipelines for AI have become an active
research area where consolidated and in-depth analysis regarding the
terminology, triggers, tasks, and challenges is required. This paper includes a
Multivocal Literature Review where we consolidated 151 relevant formal and
informal sources. In addition, nine-semi structured interviews with
participants from academia and industry verified and extended the obtained
information. Based on these sources, this paper provides and compares
terminologies for DevOps and CI/CD for AI, MLOps, (end-to-end) lifecycle
management, and CD4ML. Furthermore, the paper provides an aggregated list of
potential triggers for reiterating the pipeline, such as alert systems or
schedules. In addition, this work uses a taxonomy creation strategy to present
a consolidated pipeline comprising tasks regarding the continuous development
of AI. This pipeline consists of four stages: Data Handling, Model Learning,
Software Development and System Operations. Moreover, we map challenges
regarding pipeline implementation, adaption, and usage for the continuous
development of AI to these four stages.",None,-1
Integrating Audio-Visual Features for Multimodal Deepfake Detection,0.904969,"Deepfakes are AI-generated media in which an image or video has been
digitally modified. The advancements made in deepfake technology have led to
privacy and security issues. Most deepfake detection techniques rely on the
detection of a single modality. Existing methods for audio-visual detection do
not always surpass that of the analysis based on single modalities. Therefore,
this paper proposes an audio-visual-based method for deepfake detection, which
integrates fine-grained deepfake identification with binary classification. We
categorize the samples into four types by combining labels specific to each
single modality. This method enhances the detection under intra-domain and
cross-domain testing.",None,-1
Empathetic Response Generation via Emotion Cause Transition Graph,0.53496,"Empathetic dialogue is a human-like behavior that requires the perception of
both affective factors (e.g., emotion status) and cognitive factors (e.g.,
cause of the emotion). Besides concerning emotion status in early work, the
latest approaches study emotion causes in empathetic dialogue. These approaches
focus on understanding and duplicating emotion causes in the context to show
empathy for the speaker. However, instead of only repeating the contextual
causes, the real empathic response often demonstrate a logical and
emotion-centered transition from the causes in the context to those in the
responses. In this work, we propose an emotion cause transition graph to
explicitly model the natural transition of emotion causes between two adjacent
turns in empathetic dialogue. With this graph, the concept words of the emotion
causes in the next turn can be predicted and used by a specifically designed
concept-aware decoder to generate the empathic response. Automatic and human
experimental results on the benchmark dataset demonstrate that our method
produces more empathetic, coherent, informative, and specific responses than
existing models.",None,-1
The moral authority of ChatGPT,0.909972,"ChatGPT is not only fun to chat with, but it also searches information,
answers questions, and gives advice. With consistent moral advice, it might
improve the moral judgment and decisions of users, who often hold contradictory
moral beliefs. Unfortunately, ChatGPT turns out highly inconsistent as a moral
advisor. Nonetheless, it influences users' moral judgment, we find in an
experiment, even if they know they are advised by a chatting bot, and they
underestimate how much they are influenced. Thus, ChatGPT threatens to corrupt
rather than improves users' judgment. These findings raise the question of how
to ensure the responsible use of ChatGPT and similar AI. Transparency is often
touted but seems ineffective. We propose training to improve digital literacy.",None,-1
Neural Microfacet Fields for Inverse Rendering,0.344812,"We present Neural Microfacet Fields, a method for recovering materials,
geometry, and environment illumination from images of a scene. Our method uses
a microfacet reflectance model within a volumetric setting by treating each
sample along the ray as a (potentially non-opaque) surface. Using surface-based
Monte Carlo rendering in a volumetric setting enables our method to perform
inverse rendering efficiently by combining decades of research in surface-based
light transport with recent advances in volume rendering for view synthesis.
Our approach outperforms prior work in inverse rendering, capturing high
fidelity geometry and high frequency illumination details; its novel view
synthesis results are on par with state-of-the-art methods that do not recover
illumination or materials.",None,-1
GAM Coach: Towards Interactive and User-centered Algorithmic Recourse,0.196239,"Machine learning (ML) recourse techniques are increasingly used in
high-stakes domains, providing end users with actions to alter ML predictions,
but they assume ML developers understand what input variables can be changed.
However, a recourse plan's actionability is subjective and unlikely to match
developers' expectations completely. We present GAM Coach, a novel open-source
system that adapts integer linear programming to generate customizable
counterfactual explanations for Generalized Additive Models (GAMs), and
leverages interactive visualizations to enable end users to iteratively
generate recourse plans meeting their needs. A quantitative user study with 41
participants shows our tool is usable and useful, and users prefer personalized
recourse plans over generic plans. Through a log analysis, we explore how users
discover satisfactory recourse plans, and provide empirical evidence that
transparency can lead to more opportunities for everyday users to discover
counterintuitive patterns in ML models. GAM Coach is available at:
https://poloclub.github.io/gam-coach/.",None,-1
"Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media",0.405171,"We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.",None,-1
Kernel Density Bayesian Inverse Reinforcement Learning,0.46325,"Inverse reinforcement learning~(IRL) is a powerful framework to infer an
agent's reward function by observing its behavior, but IRL algorithms that
learn point estimates of the reward function can be misleading because there
may be several functions that describe an agent's behavior equally well. A
Bayesian approach to IRL models a distribution over candidate reward functions,
alleviating the shortcomings of learning a point estimate. However, several
Bayesian IRL algorithms use a $Q$-value function in place of the likelihood
function. The resulting posterior is computationally intensive to calculate,
has few theoretical guarantees, and the $Q$-value function is often a poor
approximation for the likelihood. We introduce kernel density Bayesian IRL
(KD-BIRL), which uses conditional kernel density estimation to directly
approximate the likelihood, providing an efficient framework that, with a
modified reward function parameterization, is applicable to environments with
complex and infinite state spaces. We demonstrate KD-BIRL's benefits through a
series of experiments in Gridworld environments and a simulated sepsis
treatment task.",None,-1
Building a Parallel Corpus and Training Translation Models Between Luganda and English,0.491924,"Neural machine translation (NMT) has achieved great successes with large
datasets, so NMT is more premised on high-resource languages. This continuously
underpins the low resource languages such as Luganda due to the lack of
high-quality parallel corpora, so even 'Google translate' does not serve
Luganda at the time of this writing. In this paper, we build a parallel corpus
with 41,070 pairwise sentences for Luganda and English which is based on three
different open-sourced corpora. Then, we train NMT models with hyper-parameter
search on the dataset. Experiments gave us a BLEU score of 21.28 from Luganda
to English and 17.47 from English to Luganda. Some translation examples show
high quality of the translation. We believe that our model is the first
Luganda-English NMT model. The bilingual dataset we built will be available to
the public.",None,-1
All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation,0.0764723,"Fairness in Language Models (LMs) remains a longstanding challenge, given the
inherent biases in training data that can be perpetuated by models and affect
the downstream tasks. Recent methods employ expensive retraining or attempt
debiasing during inference by constraining model outputs to contrast from a
reference set of biased templates or exemplars. Regardless, they dont address
the primary goal of fairness to maintain equitability across different
demographic groups. In this work, we posit that inferencing LMs to generate
unbiased output for one demographic under a context ensues from being aware of
outputs for other demographics under the same context. To this end, we propose
Counterfactually Aware Fair InferencE (CAFIE), a framework that dynamically
compares the model understanding of diverse demographics to generate more
equitable sentences. We conduct an extensive empirical evaluation using base
LMs of varying sizes and across three diverse datasets and found that CAFIE
outperforms strong baselines. CAFIE produces fairer text and strikes the best
balance between fairness and language modeling capability",None,-1
DFormer: Diffusion-guided Transformer for Universal Image Segmentation,0.318751,"This paper introduces an approach, named DFormer, for universal image
segmentation. The proposed DFormer views universal image segmentation task as a
denoising process using a diffusion model. DFormer first adds various levels of
Gaussian noise to ground-truth masks, and then learns a model to predict
denoising masks from corrupted masks. Specifically, we take deep pixel-level
features along with the noisy masks as inputs to generate mask features and
attention masks, employing diffusion-based decoder to perform mask prediction
gradually. At inference, our DFormer directly predicts the masks and
corresponding categories from a set of randomly-generated masks. Extensive
experiments reveal the merits of our proposed contributions on different image
segmentation tasks: panoptic segmentation, instance segmentation, and semantic
segmentation. Our DFormer outperforms the recent diffusion-based panoptic
segmentation method Pix2Seq-D with a gain of 3.6% on MS COCO val2017 set.
Further, DFormer achieves promising semantic segmentation performance
outperforming the recent diffusion-based method by 2.2% on ADE20K val set. Our
source code and models will be publicly on https://github.com/cp3wan/DFormer",None,-1
"Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science",0.726076,"This chapter presents some of the fundamental assumptions and principles that
could form the philosophical foundation of GeoAI and spatial data science.
Instead of reviewing the well-established characteristics of spatial data
(analysis), including interaction, neighborhoods, and autocorrelation, the
chapter highlights themes such as sustainability, bias in training data,
diversity in schema knowledge, and the (potential lack of) neutrality of GeoAI
systems from a unifying ethical perspective. Reflecting on our profession's
ethical implications will assist us in conducting potentially disruptive
research more responsibly, identifying pitfalls in designing, training, and
deploying GeoAI-based systems, and developing a shared understanding of the
benefits but also potential dangers of artificial intelligence and machine
learning research across academic fields, all while sharing our unique
(geo)spatial perspective with others.",None,-1
Impact of translation on biomedical information extraction from real-life clinical notes,0.54724,"The objective of our study is to determine whether using English tools to
extract and normalize French medical concepts on translations provides
comparable performance to French models trained on a set of annotated French
clinical notes. We compare two methods: a method involving French language
models and a method involving English language models. For the native French
method, the Named Entity Recognition (NER) and normalization steps are
performed separately. For the translated English method, after the first
translation step, we compare a two-step method and a terminology-oriented
method that performs extraction and normalization at the same time. We used
French, English and bilingual annotated datasets to evaluate all steps (NER,
normalization and translation) of our algorithms. Concerning the results, the
native French method performs better than the translated English one with a
global f1 score of 0.51 [0.47;0.55] against 0.39 [0.34;0.44] and 0.38
[0.36;0.40] for the two English methods tested. In conclusion, despite the
recent improvement of the translation models, there is a significant
performance difference between the two approaches in favor of the native French
method which is more efficient on French medical texts, even with few annotated
documents.",None,-1
"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",0.919375,"Large language models (LLMs) demonstrate impressive reasoning abilities, but
translating reasoning into actions in the real world remains challenging. In
particular, it remains unclear how to complete a given task provably within a
minimum number of interactions with the external environment, e.g., through an
internal mechanism of reasoning. To this end, we propose a principled framework
with provable regret guarantees to orchestrate reasoning and acting, which we
call ""reason for future, act for now"" (\texttt{RAFA}). Specifically, we design
a prompt template for reasoning that learns from the memory buffer and plans a
future trajectory over a long horizon (""reason for future""). At each step, the
LLM agent takes the initial action of the planned trajectory (""act for now""),
stores the collected feedback in the memory buffer, and reinvokes the reasoning
routine to replan the future trajectory from the new state.
  The key idea is to cast reasoning in LLMs as learning and planning in
Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt
LLMs to form an updated posterior of the unknown environment from the memory
buffer (learning) and generate an optimal trajectory for multiple future steps
that maximizes a value function (planning). The learning and planning
subroutines are performed in an ""in-context"" manner to emulate the actor-critic
update for MDPs. Our theoretical analysis proves that the novel combination of
long-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. Here,
$T$ denotes the number of online interactions. In particular, the regret bound
highlights an intriguing interplay between the prior knowledge obtained through
pretraining and the uncertainty reduction achieved by reasoning and acting. Our
empirical validation shows that it outperforms various existing frameworks and
achieves nearly perfect scores on a few benchmarks.",None,-1
Efficient Large-scale Scene Representation with a Hybrid of High-resolution Grid and Plane Features,0.521483,"Existing neural radiance fields (NeRF) methods for large-scale scene modeling
require days of training using multiple GPUs, hindering their applications in
scenarios with limited computing resources. Despite fast optimization NeRF
variants have been proposed based on the explicit dense or hash grid features,
their effectivenesses are mainly demonstrated in object-scale scene
representation. In this paper, we point out that the low feature resolution in
explicit representation is the bottleneck for large-scale unbounded scene
representation. To address this problem, we introduce a new and efficient
hybrid feature representation for NeRF that fuses the 3D hash-grids and
high-resolution 2D dense plane features. Compared with the dense-grid
representation, the resolution of a dense 2D plane can be scaled up more
efficiently. Based on this hybrid representation, we propose a fast
optimization NeRF variant, called GP-NeRF, that achieves better rendering
results while maintaining a compact model size. Extensive experiments on
multiple large-scale unbounded scene datasets show that our model can converge
in 1.5 hours using a single GPU while achieving results comparable to or even
better than the existing method that requires about one day's training with 8
GPUs.",None,-1
Estimation of the qualification and behavior of a contributor and aggregation of his answers in a crowdsourcing context,0.747399,"Crowdsourcing is the outsourcing of tasks to a crowd of contributors on a
dedicated platform. The crowd on these platforms is very diversified and
includes various profiles of contributors which generates data of uneven
quality. However, majority voting, which is the aggregating method commonly
used in platforms, gives equal weight to each contribution. To overcome this
problem, we propose a method, MONITOR, which estimates the contributor's
profile and aggregates the collected data by taking into account their possible
imperfections thanks to the theory of belief functions. To do so, MONITOR
starts by estimating the profile of the contributor through his qualification
for the task and his behavior.Crowdsourcing campaigns have been carried out to
collect the necessary data to test MONITOR on real data in order to compare it
to existing approaches. The results of the experiments show that thanks to the
use of the MONITOR method, we obtain a better rate of correct answer after
aggregation of the contributions compared to the majority voting. Our
contributions in this article are for the first time the proposal of a model
that takes into account both the qualification of the contributor and his
behavior in the estimation of his profile. For the second one, the weakening
and the aggregation of the answers according to the estimated profiles.",None,-1
Laplacian ICP for Progressive Registration of 3D Human Head Meshes,0.399427,"We present a progressive 3D registration framework that is a highly-efficient
variant of classical non-rigid Iterative Closest Points (N-ICP). Since it uses
the Laplace-Beltrami operator for deformation regularisation, we view the
overall process as Laplacian ICP (L-ICP). This exploits a `small deformation
per iteration' assumption and is progressively coarse-to-fine, employing an
increasingly flexible deformation model, an increasing number of correspondence
sets, and increasingly sophisticated correspondence estimation. Correspondence
matching is only permitted within predefined vertex subsets derived from
domain-specific feature extractors. Additionally, we present a new benchmark
and a pair of evaluation metrics for 3D non-rigid registration, based on
annotation transfer. We use this to evaluate our framework on a
publicly-available dataset of 3D human head scans (Headspace). The method is
robust and only requires a small fraction of the computation time compared to
the most popular classical approach, yet has comparable registration
performance.",None,-1
Rotation-Invariant Completion Network,0.408424,"Real-world point clouds usually suffer from incompleteness and display
different poses. While current point cloud completion methods excel in
reproducing complete point clouds with consistent poses as seen in the training
set, their performance tends to be unsatisfactory when handling point clouds
with diverse poses. We propose a network named Rotation-Invariant Completion
Network (RICNet), which consists of two parts: a Dual Pipeline Completion
Network (DPCNet) and an enhancing module. Firstly, DPCNet generates a coarse
complete point cloud. The feature extraction module of DPCNet can extract
consistent features, no matter if the input point cloud has undergone rotation
or translation. Subsequently, the enhancing module refines the fine-grained
details of the final generated point cloud. RICNet achieves better rotation
invariance in feature extraction and incorporates structural relationships in
man-made objects. To assess the performance of RICNet and existing methods on
point clouds with various poses, we applied random transformations to the point
clouds in the MVP dataset and conducted experiments on them. Our experiments
demonstrate that RICNet exhibits superior completion performance compared to
existing methods.",None,-1
3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining,0.871314,"Masked autoencoders (MAE) have recently been introduced to 3D self-supervised
pretraining for point clouds due to their great success in NLP and computer
vision. Unlike MAEs used in the image domain, where the pretext task is to
restore features at the masked pixels, such as colors, the existing 3D MAE
works reconstruct the missing geometry only, i.e, the location of the masked
points. In contrast to previous studies, we advocate that point location
recovery is inessential and restoring intrinsic point features is much
superior. To this end, we propose to ignore point position reconstruction and
recover high-order features at masked points including surface normals and
surface variations, through a novel attention-based decoder which is
independent of the encoder design. We validate the effectiveness of our pretext
task and decoder design using different encoder structures for 3D training and
demonstrate the advantages of our pretrained networks on various point cloud
analysis tasks.",None,-1
Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution,0.819605,"Entity Resolution (ER) is the problem of semi-automatically determining when
two entities refer to the same underlying entity, with applications ranging
from healthcare to e-commerce. Traditional ER solutions required considerable
manual expertise, including domain-specific feature engineering, as well as
identification and curation of training data. Recently released large language
models (LLMs) provide an opportunity to make ER more seamless and
domain-independent. However, it is also well known that LLMs can pose risks,
and that the quality of their outputs can depend on how prompts are engineered.
Unfortunately, a systematic experimental study on the effects of different
prompting methods for addressing unsupervised ER, using LLMs like ChatGPT, has
been lacking thus far. This paper aims to address this gap by conducting such a
study. We consider some relatively simple and cost-efficient ER prompt
engineering methods and apply them to ER on two real-world datasets widely used
in the community. We use an extensive set of experimental results to show that
an LLM like GPT3.5 is viable for high-performing unsupervised ER, and
interestingly, that more complicated and detailed (and hence, expensive)
prompting methods do not necessarily outperform simpler approaches. We provide
brief discussions on qualitative and error analysis, including a study of the
inter-consistency of different prompting methods to determine whether they
yield stable outputs. Finally, we consider some limitations of LLMs when
applied to ER.",None,-1
LightGlue: Local Feature Matching at Light Speed,1.0,"We introduce LightGlue, a deep neural network that learns to match local
features across images. We revisit multiple design decisions of SuperGlue, the
state of the art in sparse matching, and derive simple but effective
improvements. Cumulatively, they make LightGlue more efficient - in terms of
both memory and computation, more accurate, and much easier to train. One key
property is that LightGlue is adaptive to the difficulty of the problem: the
inference is much faster on image pairs that are intuitively easy to match, for
example because of a larger visual overlap or limited appearance change. This
opens up exciting prospects for deploying deep matchers in latency-sensitive
applications like 3D reconstruction. The code and trained models are publicly
available at https://github.com/cvg/LightGlue.",None,-1
Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes,0.979713,"Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT
and Bard, are becoming available to users that have the potential to
revolutionize the quality of decision-making achieved by the public. In this
context, we set out to investigate how such systems perform in the personal
finance domain, where financial inclusion has been an overarching stated aim of
banks for decades. We asked 13 questions representing banking products in
personal finance: bank account, credit card, and certificate of deposits and
their inter-product interactions, and decisions related to high-value
purchases, payment of bank dues, and investment advice, and in different
dialects and languages (English, African American Vernacular English, and
Telugu). We find that although the outputs of the chatbots are fluent and
plausible, there are still critical gaps in providing accurate and reliable
financial information using LLM-based chatbots.",None,-1
Going faster to see further: GPU-accelerated value iteration and simulation for perishable inventory control using JAX,0.396407,"Value iteration can find the optimal replenishment policy for a perishable
inventory problem, but is computationally demanding due to the large state
spaces that are required to represent the age profile of stock. The parallel
processing capabilities of modern GPUs can reduce the wall time required to run
value iteration by updating many states simultaneously. The adoption of
GPU-accelerated approaches has been limited in operational research relative to
other fields like machine learning, in which new software frameworks have made
GPU programming widely accessible. We used the Python library JAX to implement
value iteration and simulators of the underlying Markov decision processes in a
high-level API, and relied on this library's function transformations and
compiler to efficiently utilize GPU hardware. Our method can extend use of
value iteration to settings that were previously considered infeasible or
impractical. We demonstrate this on example scenarios from three recent studies
which include problems with over 16 million states and additional problem
features, such as substitution between products, that increase computational
complexity. We compare the performance of the optimal replenishment policies to
heuristic policies, fitted using simulation optimization in JAX which allowed
the parallel evaluation of multiple candidate policy parameters on thousands of
simulated years. The heuristic policies gave a maximum optimality gap of 2.49%.
Our general approach may be applicable to a wide range of problems in
operational research that would benefit from large-scale parallel computation
on consumer-grade GPU hardware.",None,-1
"What Should Be Balanced in a ""Balanced"" Face Recognition Dataset?",0.663357,"The issue of demographic disparities in face recognition accuracy has
attracted increasing attention in recent years. Various face image datasets
have been proposed as 'fair' or 'balanced' to assess the accuracy of face
recognition algorithms across demographics. These datasets typically balance
the number of identities and images across demographics. It is important to
note that the number of identities and images in an evaluation dataset are {\em
not} driving factors for 1-to-1 face matching accuracy. Moreover, balancing the
number of identities and images does not ensure balance in other factors known
to impact accuracy, such as head pose, brightness, and image quality. We
demonstrate these issues using several recently proposed datasets. To improve
the ability to perform less biased evaluations, we propose a bias-aware toolkit
that facilitates creation of cross-demographic evaluation datasets balanced on
factors mentioned in this paper.",None,-1
Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules,0.27274,"Automated game design (AGD), the study of automatically generating game
rules, has a long history in technical games research. AGD approaches generally
rely on approximations of human play, either objective functions or AI agents.
Despite this, the majority of these approximators are static, meaning they do
not reflect human player's ability to learn and improve in a game. In this
paper, we investigate the application of Reinforcement Learning (RL) as an
approximator for human play for rule generation. We recreate the classic AGD
environment Mechanic Maker in Unity as a new, open-source rule generation
framework. Our results demonstrate that RL produces distinct sets of rules from
an A* agent baseline, which may be more usable by humans.",None,-1
Unlearn What You Want to Forget: Efficient Unlearning for LLMs,0.901668,"Large language models (LLMs) have achieved significant progress from
pre-training on and memorizing a wide range of textual data, however, this
process might suffer from privacy issues and violations of data protection
regulations. As a result, the ability to easily remove data related to
individual users from such models while not deteriorating their predictive
quality after the removal becomes increasingly important. To address these
issues, in this work, we propose an efficient unlearning framework that could
efficiently update LLMs without having to retrain the whole model after data
removals, by introducing lightweight unlearning layers learned with a selective
teacher-student objective into the transformers. In addition, we introduce a
fusion mechanism to effectively combine different unlearning layers that learns
to forget different sets of data to handle a sequence of forgetting operations.
Experiments on classification and generation tasks demonstrate the
effectiveness of our proposed methods compared to the state-of-the-art
baselines.",None,-1
TwinExplainer: Explaining Predictions of an Automotive Digital Twin,0.329873,"Vehicles are complex Cyber Physical Systems (CPS) that operate in a variety
of environments, and the likelihood of failure of one or more subsystems, such
as the engine, transmission, brakes, and fuel, can result in unscheduled
downtime and incur high maintenance or repair costs. In order to prevent these
issues, it is crucial to continuously monitor the health of various subsystems
and identify abnormal sensor channel behavior. Data-driven Digital Twin (DT)
systems are capable of such a task. Current DT technologies utilize various
Deep Learning (DL) techniques that are constrained by the lack of justification
or explanation for their predictions. This inability of these opaque systems
can influence decision-making and raises user trust concerns. This paper
presents a solution to this issue, where the TwinExplainer system, with its
three-layered architectural pipeline, explains the predictions of an automotive
DT. Such a system can assist automotive stakeholders in understanding the
global scale of the sensor channels and how they contribute towards generic DT
predictions. TwinExplainer can also visualize explanations for both normal and
abnormal local predictions computed by the DT.",None,-1
Solar Irradiance Anticipative Transformer,0.522922,"This paper proposes an anticipative transformer-based model for short-term
solar irradiance forecasting. Given a sequence of sky images, our proposed
vision transformer encodes features of consecutive images, feeding into a
transformer decoder to predict irradiance values associated with future unseen
sky images. We show that our model effectively learns to attend only to
relevant features in images in order to forecast irradiance. Moreover, the
proposed anticipative transformer captures long-range dependencies between sky
images to achieve a forecasting skill of 21.45 % on a 15 minute ahead
prediction for a newly introduced dataset of all-sky images when compared to a
smart persistence model.",None,-1
The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions,0.333486,"Recent progress in Large Language Models (LLMs) has produced models that
exhibit remarkable performance across a variety of NLP tasks. However, it
remains unclear whether the existing focus of NLP research accurately captures
the genuine requirements of human users. This paper provides a comprehensive
analysis of the divergence between current NLP research and the needs of
real-world NLP applications via a large-scale collection of user-GPT
conversations. We analyze a large-scale collection of real user queries to GPT.
We compare these queries against existing NLP benchmark tasks and identify a
significant gap between the tasks that users frequently request from LLMs and
the tasks that are commonly studied in academic research. For example, we find
that tasks such as ``design'' and ``planning'' are prevalent in user
interactions but are largely neglected or different from traditional NLP
benchmarks. We investigate these overlooked tasks, dissect the practical
challenges they pose, and provide insights toward a roadmap to make LLMs better
aligned with user needs.",None,-1
CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,0.701527,"In this paper, we propose CGI-Stereo, a novel neural network architecture
that can concurrently achieve real-time performance, competitive accuracy, and
strong generalization ability. The core of our CGI-Stereo is a Context and
Geometry Fusion (CGF) block which adaptively fuses context and geometry
information for more effective cost aggregation and meanwhile provides feedback
to feature learning to guide more effective contextual feature extraction. The
proposed CGF can be easily embedded into many existing stereo matching
networks, such as PSMNet, GwcNet and ACVNet. The resulting networks show a
significant improvement in accuracy. Specially, the model which incorporates
our CGF with ACVNet ranks $1^{st}$ on the KITTI 2012 and 2015 leaderboards
among all the published methods. We further propose an informative and concise
cost volume, named Attention Feature Volume (AFV), which exploits a correlation
volume as attention weights to filter a feature volume. Based on CGF and AFV,
the proposed CGI-Stereo outperforms all other published real-time methods on
KITTI benchmarks and shows better generalization ability than other real-time
methods. Code is available at https://github.com/gangweiX/CGI-Stereo.",None,-1
QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing,0.607163,"Questions Under Discussion (QUD) is a versatile linguistic framework in which
discourse progresses as continuously asking questions and answering them.
Automatic parsing of a discourse to produce a QUD structure thus entails a
complex question generation task: given a document and an answer sentence,
generate a question that satisfies linguistic constraints of QUD and can be
grounded in an anchor sentence in prior context. These questions are known to
be curiosity-driven and open-ended. This work introduces the first framework
for the automatic evaluation of QUD parsing, instantiating the theoretical
constraints of QUD in a concrete protocol. We present QUDeval, a dataset of
fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned
systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD
is still challenging for modern LLMs, and that existing evaluation metrics
poorly approximate parser quality. Encouragingly, human-authored QUDs are
scored highly by our human evaluators, suggesting that there is headroom for
further progress on language modeling to improve both QUD parsing and QUD
evaluation.",None,-1
Deep Learning-Enabled Sleep Staging From Vital Signs and Activity Measured Using a Near-Infrared Video Camera,0.713048,"Conventional sleep monitoring is time-consuming, expensive and uncomfortable,
requiring a large number of contact sensors to be attached to the patient.
Video data is commonly recorded as part of a sleep laboratory assessment. If
accurate sleep staging could be achieved solely from video, this would overcome
many of the problems of traditional methods. In this work we use heart rate,
breathing rate and activity measures, all derived from a near-infrared video
camera, to perform sleep stage classification. We use a deep transfer learning
approach to overcome data scarcity, by using an existing contact-sensor dataset
to learn effective representations from the heart and breathing rate time
series. Using a dataset of 50 healthy volunteers, we achieve an accuracy of
73.4\% and a Cohen's kappa of 0.61 in four-class sleep stage classification,
establishing a new state-of-the-art for video-based sleep staging.",None,-1
ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER,0.292782,"Prompt-based language models have produced encouraging results in numerous
applications, including Named Entity Recognition (NER) tasks. NER aims to
identify entities in a sentence and provide their types. However, the strong
performance of most available NER approaches is heavily dependent on the design
of discrete prompts and a verbalizer to map the model-predicted outputs to
entity categories, which are complicated undertakings. To address these
challenges, we present ContrastNER, a prompt-based NER framework that employs
both discrete and continuous tokens in prompts and uses a contrastive learning
approach to learn the continuous prompts and forecast entity types. The
experimental results demonstrate that ContrastNER obtains competitive
performance to the state-of-the-art NER methods in high-resource settings and
outperforms the state-of-the-art models in low-resource circumstances without
requiring extensive manual prompt engineering and verbalizer design.",None,-1
Can LLMs facilitate interpretation of pre-trained language models?,0.544927,"Work done to uncover the knowledge encoded within pre-trained language models
rely on annotated corpora or human-in-the-loop methods. However, these
approaches are limited in terms of scalability and the scope of interpretation.
We propose using a large language model, ChatGPT, as an annotator to enable
fine-grained interpretation analysis of pre-trained language models. We
discover latent concepts within pre-trained language models by applying
agglomerative hierarchical clustering over contextualized representations and
then annotate these concepts using ChatGPT. Our findings demonstrate that
ChatGPT produces accurate and semantically richer annotations compared to
human-annotated concepts. Additionally, we showcase how GPT-based annotations
empower interpretation analysis methodologies of which we demonstrate two:
probing frameworks and neuron interpretation. To facilitate further exploration
and experimentation in the field, we make available a substantial ConceptNet
dataset (TCN) comprising 39,000 annotated concepts.",None,-1
Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer,0.168143,"Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success in
domain adaptation tasks by integrating pre-trained Neural Machine Translation
(NMT) models with domain-specific token-level retrieval. However, the reasons
underlying its success have not been thoroughly investigated. In this paper, we
comprehensively analyze $k$NN-MT through theoretical and empirical studies.
Initially, we provide new insights into the working mechanism of $k$NN-MT as an
efficient technique to implicitly execute gradient descent on the output
projection layer of NMT, indicating that it is a specific case of model
fine-tuning. Subsequently, we conduct multi-domain experiments and word-level
analysis to examine the differences in performance between $k$NN-MT and
entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT
with adapters yields comparable translation performance to fine-tuning on
in-domain test sets, while achieving better performance on out-of-domain test
sets; (2) Fine-tuning significantly outperforms $k$NN-MT on the recall of
in-domain low-frequency words, but this gap could be bridged by optimizing the
context representations with additional adapter layers.",None,-1
Crowdsourcing on Sensitive Data with Privacy-Preserving Text Rewriting,0.140147,"Most tasks in NLP require labeled data. Data labeling is often done on
crowdsourcing platforms due to scalability reasons. However, publishing data on
public platforms can only be done if no privacy-relevant information is
included. Textual data often contains sensitive information like person names
or locations. In this work, we investigate how removing personally identifiable
information (PII) as well as applying differential privacy (DP) rewriting can
enable text with privacy-relevant information to be used for crowdsourcing. We
find that DP-rewriting before crowdsourcing can preserve privacy while still
leading to good label quality for certain tasks and data. PII-removal led to
good label quality in all examined tasks, however, there are no privacy
guarantees given.",None,-1
Morphosyntactic probing of multilingual BERT models,0.352921,"We introduce an extensive dataset for multilingual probing of morphological
information in language models (247 tasks across 42 languages from 10
families), each consisting of a sentence with a target word and a morphological
tag as the desired label, derived from the Universal Dependencies treebanks. We
find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features
that attain strong performance across these tasks. We then apply two methods to
locate, for each probing task, where the disambiguating information resides in
the input. The first is a new perturbation method that masks various parts of
context; the second is the classical method of Shapley values. The most
intriguing finding that emerges is a strong tendency for the preceding context
to hold more information relevant to the prediction than the following context.",None,-1
Rethinking the BERT-like Pretraining for DNA Sequences,0.692554,"With the success of large-scale pretraining in NLP, there is an increasing
trend of applying it to the domain of life sciences. In particular, pretraining
methods based on DNA sequences have garnered growing attention due to their
potential to capture generic information about genes. However, existing
pretraining methods for DNA sequences largely rely on direct adoptions of BERT
pretraining from NLP, lacking a comprehensive understanding and a specifically
tailored approach. To address this research gap, we first conducted a series of
exploratory experiments and gained several insightful observations: 1) In the
fine-tuning phase of downstream tasks, when using K-mer overlapping
tokenization instead of K-mer non-overlapping tokenization, both overlapping
and non-overlapping pretraining weights show consistent performance
improvement.2) During the pre-training process, using K-mer overlapping
tokenization quickly produces clear K-mer embeddings and reduces the loss to a
very low level, while using K-mer non-overlapping tokenization results in less
distinct embeddings and continuously decreases the loss. 3) Using overlapping
tokenization causes the self-attention in the intermediate layers of
pre-trained models to tend to overly focus on certain tokens, reflecting that
these layers are not adequately optimized. In summary, overlapping tokenization
can benefit the fine-tuning of downstream tasks but leads to inadequate
pretraining with fast convergence. To unleash the pretraining potential, we
introduce a novel approach called RandomMask, which gradually increases the
task difficulty of BERT-like pretraining by continuously expanding its mask
boundary, forcing the model to learn more knowledge. RandomMask is simple but
effective, achieving top-tier performance across 26 datasets of 28 datasets
spanning 7 downstream tasks.",None,-1
DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis,0.32226,"We present DiffuScene for indoor 3D scene synthesis based on a novel scene
configuration denoising diffusion model. It generates 3D instance properties
stored in an unordered object set and retrieves the most similar geometry for
each object configuration, which is characterized as a concatenation of
different attributes, including location, size, orientation, semantics, and
geometry features. We introduce a diffusion network to synthesize a collection
of 3D indoor objects by denoising a set of unordered object attributes.
Unordered parametrization simplifies and eases the joint distribution
approximation. The shape feature diffusion facilitates natural object
placements, including symmetries. Our method enables many downstream
applications, including scene completion, scene arrangement, and
text-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that
our method can synthesize more physically plausible and diverse indoor scenes
than state-of-the-art methods. Extensive ablation studies verify the
effectiveness of our design choice in scene diffusion models.",None,-1
Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes,0.611931,"The BioNLP Workshop 2023 initiated the launch of a shared task on Problem
List Summarization (ProbSum) in January 2023. The aim of this shared task is to
attract future research efforts in building NLP models for real-world
diagnostic decision support applications, where a system generating relevant
and accurate diagnoses will augment the healthcare providers decision-making
process and improve the quality of care for patients. The goal for participants
is to develop models that generated a list of diagnoses and problems using
input from the daily care notes collected from the hospitalization of
critically ill patients. Eight teams submitted their final systems to the
shared task leaderboard. In this paper, we describe the tasks, datasets,
evaluation metrics, and baseline systems. Additionally, the techniques and
results of the evaluation of the different approaches tried by the
participating teams are summarized.",None,-1
Semi-Supervised SAR ATR Framework with Transductive Auxiliary Segmentation,0.999866,"Convolutional neural networks (CNNs) have achieved high performance in
synthetic aperture radar (SAR) automatic target recognition (ATR). However, the
performance of CNNs depends heavily on a large amount of training data. The
insufficiency of labeled training SAR images limits the recognition performance
and even invalidates some ATR methods. Furthermore, under few labeled training
data, many existing CNNs are even ineffective. To address these challenges, we
propose a Semi-supervised SAR ATR Framework with transductive Auxiliary
Segmentation (SFAS). The proposed framework focuses on exploiting the
transductive generalization on available unlabeled samples with an auxiliary
loss serving as a regularizer. Through auxiliary segmentation of unlabeled SAR
samples and information residue loss (IRL) in training, the framework can
employ the proposed training loop process and gradually exploit the information
compilation of recognition and segmentation to construct a helpful inductive
bias and achieve high performance. Experiments conducted on the MSTAR dataset
have shown the effectiveness of our proposed SFAS for few-shot learning. The
recognition performance of 94.18\% can be achieved under 20 training samples in
each class with simultaneous accurate segmentation results. Facing variances of
EOCs, the recognition ratios are higher than 88.00\% when 10 training samples
each class.",None,-1
Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation,0.416548,"Given a composite image, image harmonization aims to adjust the foreground
illumination to be consistent with background. Previous methods have explored
transforming foreground features to achieve competitive performance. In this
work, we show that using global information to guide foreground feature
transformation could achieve significant improvement. Besides, we propose to
transfer the foreground-background relation from real images to composite
images, which can provide intermediate supervision for the transformed encoder
features. Additionally, considering the drawbacks of existing harmonization
datasets, we also contribute a ccHarmony dataset which simulates the natural
illumination variation. Extensive experiments on iHarmony4 and our contributed
dataset demonstrate the superiority of our method. Our ccHarmony dataset is
released at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.",None,-1
A Formal Perspective on Byte-Pair Encoding,0.739265,"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in
NLP, despite being devised initially as a compression method. BPE appears to be
a greedy algorithm at face value, but the underlying optimization problem that
BPE seeks to solve has not yet been laid down. We formalize BPE as a
combinatorial optimization problem. Via submodular functions, we prove that the
iterative greedy version is a
$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation
of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the
total backward curvature with respect to the optimal merge sequence
$\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is
$\approx 0.37$.
  We provide a faster implementation of BPE which improves the runtime
complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log
M\right)$, where $N$ is the sequence length and $M$ is the merge count.
Finally, we optimize the brute-force algorithm for optimal BPE using
memoization.",None,-1
AutoHall: Automated Hallucination Dataset Generation for Large Language Models,0.0617634,"While Large language models (LLMs) have garnered widespread applications
across various domains due to their powerful language understanding and
generation capabilities, the detection of non-factual or hallucinatory content
generated by LLMs remains scarce. Currently, one significant challenge in
hallucination detection is the laborious task of time-consuming and expensive
manual annotation of the hallucinatory generation. To address this issue, this
paper first introduces a method for automatically constructing model-specific
hallucination datasets based on existing fact-checking datasets called
AutoHall. Furthermore, we propose a zero-resource and black-box hallucination
detection method based on self-contradiction. We conduct experiments towards
prevalent open-/closed-source LLMs, achieving superior hallucination detection
performance compared to extant baselines. Moreover, our experiments reveal
variations in hallucination proportions and types among different models.",None,-1
Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models,0.765612,"Parameter-efficient tuning (PET) methods fit pre-trained language models
(PLMs) to downstream tasks by either computing a small compressed update for a
subset of model parameters, or appending and fine-tuning a small number of new
model parameters to the pre-trained network. Hand-designed PET architectures
from the literature perform well in practice, but have the potential to be
improved via automated neural architecture search (NAS). We propose an
efficient NAS method for learning PET architectures via structured and
unstructured pruning. We present experiments on GLUE demonstrating the
effectiveness of our algorithm and discuss how PET architectural design choices
affect performance in practice.",None,-1
DiffCap: Exploring Continuous Diffusion on Image Captioning,0.208906,"Current image captioning works usually focus on generating descriptions in an
autoregressive manner. However, there are limited works that focus on
generating descriptions non-autoregressively, which brings more decoding
diversity. Inspired by the success of diffusion models on generating
natural-looking images, we propose a novel method DiffCap to apply continuous
diffusions on image captioning. Unlike image generation where the output is
fixed-size and continuous, image description length varies with discrete
tokens. Our method transforms discrete tokens in a natural way and applies
continuous diffusion on them to successfully fuse extracted image features for
diffusion caption generation. Our experiments on COCO dataset demonstrate that
our method uses a much simpler structure to achieve comparable results to the
previous non-autoregressive works. Apart from quality, an intriguing property
of DiffCap is its high diversity during generation, which is missing from many
autoregressive models. We believe our method on fusing multimodal features in
diffusion language generation will inspire more researches on multimodal
language generation tasks for its simplicity and decoding flexibility.",None,-1
Extrinsically-Focused Evaluation of Omissions in Medical Summarization,0.152583,"The goal of automated summarization techniques (Paice, 1990; Kupiec et al,
1995) is to condense text by focusing on the most critical information.
Generative large language models (LLMs) have shown to be robust summarizers,
yet traditional metrics struggle to capture resulting performance (Goyal et al,
2022) in more powerful LLMs. In safety-critical domains such as medicine, more
rigorous evaluation is required, especially given the potential for LLMs to
omit important information in the resulting summary. We propose MED-OMIT, a new
omission benchmark for medical summarization. Given a doctor-patient
conversation and a generated summary, MED-OMIT categorizes the chat into a set
of facts and identifies which are omitted from the summary. We further propose
to determine fact importance by simulating the impact of each fact on a
downstream clinical task: differential diagnosis (DDx) generation. MED-OMIT
leverages LLM prompt-based approaches which categorize the importance of facts
and cluster them as supporting or negating evidence to the diagnosis. We
evaluate MED-OMIT on a publicly-released dataset of patient-doctor
conversations and find that MED-OMIT captures omissions better than alternative
metrics.",None,-1
Egocentric Hierarchical Visual Semantics,0.0848544,"We are interested in aligning how people think about objects and what
machines perceive, meaning by this the fact that object recognition, as
performed by a machine, should follow a process which resembles that followed
by humans when thinking of an object associated with a certain concept. The
ultimate goal is to build systems which can meaningfully interact with their
users, describing what they perceive in the users' own terms. As from the field
of Lexical Semantics, humans organize the meaning of words in hierarchies where
the meaning of, e.g., a noun, is defined in terms of the meaning of a more
general noun, its genus, and of one or more differentiating properties, its
differentia. The main tenet of this paper is that object recognition should
implement a hierarchical process which follows the hierarchical semantic
structure used to define the meaning of words. We achieve this goal by
implementing an algorithm which, for any object, recursively recognizes its
visual genus and its visual differentia. In other words, the recognition of an
object is decomposed in a sequence of steps where the locally relevant visual
features are recognized. This paper presents the algorithm and a first
evaluation.",None,-1
Temporal Segment Transformer for Action Segmentation,0.0578989,"Recognizing human actions from untrimmed videos is an important task in
activity understanding, and poses unique challenges in modeling long-range
temporal relations. Recent works adopt a predict-and-refine strategy which
converts an initial prediction to action segments for global context modeling.
However, the generated segment representations are often noisy and exhibit
inaccurate segment boundaries, over-segmentation and other problems. To deal
with these issues, we propose an attention based approach which we call
\textit{temporal segment transformer}, for joint segment relation modeling and
denoising. The main idea is to denoise segment representations using attention
between segment and frame representations, and also use inter-segment attention
to capture temporal correlations between segments. The refined segment
representations are used to predict action labels and adjust segment
boundaries, and a final action segmentation is produced based on voting from
segment masks. We show that this novel architecture achieves state-of-the-art
accuracy on the popular 50Salads, GTEA and Breakfast benchmarks. We also
conduct extensive ablations to demonstrate the effectiveness of different
components of our design.",None,-1
Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,0.74298,"Large language models (LLMs) such as ChatGPT are increasingly being used for
various use cases, including text content generation at scale. Although
detection methods for such AI-generated text exist already, we investigate
ChatGPT's performance as a detector on such AI-generated text, inspired by
works that use ChatGPT as a data labeler or annotator. We evaluate the
zero-shot performance of ChatGPT in the task of human-written vs. AI-generated
text detection, and perform experiments on publicly available datasets. We
empirically investigate if ChatGPT is symmetrically effective in detecting
AI-generated or human-written text. Our findings provide insight on how ChatGPT
and similar LLMs may be leveraged in automated detection pipelines by simply
focusing on solving a specific aspect of the problem and deriving the rest from
that solution. All code and data is available at
https://github.com/AmritaBh/ChatGPT-as-Detector.",None,-1
Disproving XAI Myths with Formal Methods -- Initial Results,0.406486,"The advances in Machine Learning (ML) in recent years have been both
impressive and far-reaching. However, the deployment of ML models is still
impaired by a lack of trust in how the best-performing ML models make
predictions. The issue of lack of trust is even more acute in the uses of ML
models in high-risk or safety-critical domains. eXplainable artificial
intelligence (XAI) is at the core of ongoing efforts for delivering trustworthy
AI. Unfortunately, XAI is riddled with critical misconceptions, that foster
distrust instead of building trust. This paper details some of the most visible
misconceptions in XAI, and shows how formal methods have been used, both to
disprove those misconceptions, but also to devise practically effective
alternatives.",None,-1
Slovo: Russian Sign Language Dataset,0.55192,"One of the main challenges of the sign language recognition task is the
difficulty of collecting a suitable dataset due to the gap between
hard-of-hearing and hearing societies. In addition, the sign language in each
country differs significantly, which obliges the creation of new data for each
of them. This paper presents the Russian Sign Language (RSL) video dataset
Slovo, produced using crowdsourcing platforms. The dataset contains 20,000
FullHD recordings, divided into 1,000 classes of isolated RSL gestures received
by 194 signers. We also provide the entire dataset creation pipeline, from data
collection to video annotation, with the following demo application. Several
neural networks are trained and evaluated on the Slovo to demonstrate its
teaching ability. Proposed data and pre-trained models are publicly available.",None,-1
Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models,0.625143,"We address the challenge of ensuring differential privacy (DP) guarantees in
training deep retrieval systems. Training these systems often involves the use
of contrastive-style losses, which are typically non-per-example decomposable,
making them difficult to directly DP-train with since common techniques require
per-example gradients. To address this issue, we propose an approach that
prioritizes ensuring query privacy prior to training a deep retrieval system.
Our method employs DP language models (LMs) to generate private synthetic
queries representative of the original data. These synthetic queries can be
used in downstream retrieval system training without compromising privacy. Our
approach demonstrates a significant enhancement in retrieval quality compared
to direct DP-training, all while maintaining query-level privacy guarantees.
This work highlights the potential of harnessing LMs to overcome limitations in
standard DP-training methods.",None,-1
ControlMat: A Controlled Generative Approach to Material Capture,0.934765,"Material reconstruction from a photograph is a key component of 3D content
creation democratization. We propose to formulate this ill-posed problem as a
controlled synthesis one, leveraging the recent progress in generative deep
networks. We present ControlMat, a method which, given a single photograph with
uncontrolled illumination as input, conditions a diffusion model to generate
plausible, tileable, high-resolution physically-based digital materials. We
carefully analyze the behavior of diffusion models for multi-channel outputs,
adapt the sampling process to fuse multi-scale information and introduce rolled
diffusion to enable both tileability and patched diffusion for high-resolution
outputs. Our generative approach further permits exploration of a variety of
materials which could correspond to the input image, mitigating the unknown
lighting conditions. We show that our approach outperforms recent inference and
latent-space-optimization methods, and carefully validate our diffusion process
design choices. Supplemental materials and additional details are available at:
https://gvecchio.com/controlmat/.",None,-1
Enhancing conversational quality in language learning chatbots: An evaluation of GPT4 for ASR error correction,0.0431152,"The integration of natural language processing (NLP) technologies into
educational applications has shown promising results, particularly in the
language learning domain. Recently, many spoken open-domain chatbots have been
used as speaking partners, helping language learners improve their language
skills. However, one of the significant challenges is the high word-error-rate
(WER) when recognizing non-native/non-fluent speech, which interrupts
conversation flow and leads to disappointment for learners. This paper explores
the use of GPT4 for ASR error correction in conversational settings. In
addition to WER, we propose to use semantic textual similarity (STS) and next
response sensibility (NRS) metrics to evaluate the impact of error correction
models on the quality of the conversation. We find that transcriptions
corrected by GPT4 lead to higher conversation quality, despite an increase in
WER. GPT4 also outperforms standard error correction methods without the need
for in-domain training data.",None,-1
From Database Repairs to Causality in Databases and Beyond,0.705425,"We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.",None,-1
Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation,0.801062,"We propose a meta-ability decoupling (MAD) paradigm, which brings together
various object navigation methods in an architecture system, allowing them to
mutually enhance each other and evolve together. Based on the MAD paradigm, we
design a multiple thinking (MT) model that leverages distinct thinking to
abstract various meta-abilities. Our method decouples meta-abilities from three
aspects: input, encoding, and reward while employing the multiple thinking
collaboration (MTC) module to promote mutual cooperation between thinking. MAD
introduces a novel qualitative and quantitative interpretability system for
object navigation. Through extensive experiments on AI2-Thor and RoboTHOR, we
demonstrate that our method outperforms state-of-the-art (SOTA) methods on both
typical and zero-shot object navigation tasks.",None,-1
GATE: A Challenge Set for Gender-Ambiguous Translation Examples,0.93362,"Although recent years have brought significant progress in improving
translation of unambiguously gendered sentences, translation of ambiguously
gendered input remains relatively unexplored. When source gender is ambiguous,
machine translation models typically default to stereotypical gender roles,
perpetuating harmful bias. Recent work has led to the development of ""gender
rewriters"" that generate alternative gender translations on such ambiguous
inputs, but such systems are plagued by poor linguistic coverage. To encourage
better performance on this task we present and release GATE, a linguistically
diverse corpus of gender-ambiguous source sentences along with multiple
alternative target language translations. We also provide tools for evaluation
and system analysis when using GATE and use them to evaluate our translation
rewriter system.",None,-1
Curriculum Learning for ab initio Deep Learned Refractive Optics,0.329597,"Deep optical optimization has recently emerged as a new paradigm for
designing computational imaging systems using only the output image as the
objective. However, it has been limited to either simple optical systems
consisting of a single element such as a diffractive optical element (DOE) or
metalens, or the fine-tuning of compound lenses from good initial designs. Here
we present a DeepLens design method based on curriculum learning, which is able
to learn optical designs of compound lenses ab initio from randomly initialized
surfaces without human intervention, therefore overcoming the need for a good
initial design. We demonstrate the effectiveness of our approach by fully
automatically designing both classical imaging lenses and a large field-of-view
extended depth-of-field computational lens in a cellphone-style form factor,
with highly aspheric surfaces and a short back focal length.",None,-1
dacl10k: Benchmark for Semantic Bridge Damage Segmentation,0.737927,"Reliably identifying reinforced concrete defects (RCDs)plays a crucial role
in assessing the structural integrity, traffic safety, and long-term durability
of concrete bridges, which represent the most common bridge type worldwide.
Nevertheless, available datasets for the recognition of RCDs are small in terms
of size and class variety, which questions their usability in real-world
scenarios and their role as a benchmark. Our contribution to this problem is
""dacl10k"", an exceptionally diverse RCD dataset for multi-label semantic
segmentation comprising 9,920 images deriving from real-world bridge
inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge
components that play a key role in the building assessment and recommending
actions, such as restoration works, traffic load limitations or bridge
closures. In addition, we examine baseline models for dacl10k which are
subsequently evaluated. The best model achieves a mean intersection-over-union
of 0.42 on the test set. dacl10k, along with our baselines, will be openly
accessible to researchers and practitioners, representing the currently biggest
dataset regarding number of images and class diversity for semantic
segmentation in the bridge inspection domain.",None,-1
Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering,0.40027,"Whereas the recent emergence of large language models (LLMs) like ChatGPT has
exhibited impressive general performance, it still has a large gap with
fully-supervised models on specific tasks such as multi-span question
answering. Previous researches found that in-context learning is an effective
approach to exploiting LLM, by using a few task-related labeled data as
demonstration examples to construct a few-shot prompt for answering new
questions. A popular implementation is to concatenate a few questions and their
correct answers through simple templates, informing LLM of the desired output.
In this paper, we propose a novel way of employing labeled data such that it
also informs LLM of some undesired output, by extending demonstration examples
with feedback about answers predicted by an off-the-shelf model, e.g., correct,
incorrect, or incomplete. Experiments on three multi-span question answering
datasets as well as a keyphrase extraction dataset show that our new prompting
strategy consistently improves LLM's in-context learning performance.",None,-1
Improving Medical Dialogue Generation with Abstract Meaning Representations,0.618325,"Medical Dialogue Generation serves a critical role in telemedicine by
facilitating the dissemination of medical expertise to patients. Existing
studies focus on incorporating textual representations, which have limited
their ability to represent the semantics of text, such as ignoring important
medical entities. To enhance the model's understanding of the textual semantics
and the medical knowledge including entities and relations, we introduce the
use of Abstract Meaning Representations (AMR) to construct graphical
representations that delineate the roles of language constituents and medical
entities within the dialogues. In this paper, We propose a novel framework that
models dialogues between patients and healthcare professionals using AMR
graphs, where the neural networks incorporate textual and graphical knowledge
with a dual attention mechanism. Experimental results show that our framework
outperforms strong baseline models in medical dialogue generation,
demonstrating the effectiveness of AMR graphs in enhancing the representations
of medical knowledge and logical relationships. Furthermore, to support future
research in this domain, we provide the corresponding source code at
https://github.com/Bernard-Yang/MedDiaAMR.",None,-1
SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities,0.88527,"The rapid advance in artificial intelligence technology has facilitated the
prosperity of digital humanities research. Against such backdrop, research
methods need to be transformed in the intelligent processing of ancient texts,
which is a crucial component of digital humanities research, so as to adapt to
new development trends in the wave of AIGC. In this study, we propose a GPT
model called SikuGPT based on the corpus of Siku Quanshu. The model's
performance in tasks such as intralingual translation and text classification
exceeds that of other GPT-type models aimed at processing ancient texts.
SikuGPT's ability to process traditional Chinese ancient texts can help promote
the organization of ancient information and knowledge services, as well as the
international dissemination of Chinese ancient culture.",None,-1
AutoAD: Movie Description in Context,0.447745,"The objective of this paper is an automatic Audio Description (AD) model that
ingests movies and outputs AD in text form. Generating high-quality movie AD is
challenging due to the dependency of the descriptions on context, and the
limited amount of training data available. In this work, we leverage the power
of pretrained foundation models, such as GPT and CLIP, and only train a mapping
network that bridges the two models for visually-conditioned text generation.
In order to obtain high-quality AD, we make the following four contributions:
(i) we incorporate context from the movie clip, AD from previous clips, as well
as the subtitles; (ii) we address the lack of training data by pretraining on
large-scale datasets, where visual or contextual information is unavailable,
e.g. text-only AD without movies or visual captioning datasets without context;
(iii) we improve on the currently available AD datasets, by removing label
noise in the MAD dataset, and adding character naming information; and (iv) we
obtain strong results on the movie AD task compared with previous methods.",None,-1
Controllable Visual-Tactile Synthesis,0.256891,"Deep generative models have various content creation applications such as
graphic design, e-commerce, and virtual Try-on. However, current works mainly
focus on synthesizing realistic visual outputs, often ignoring other sensory
modalities, such as touch, which limits physical interaction with users. In
this work, we leverage deep generative models to create a multi-sensory
experience where users can touch and see the synthesized object when sliding
their fingers on a haptic surface. The main challenges lie in the significant
scale discrepancy between vision and touch sensing and the lack of explicit
mapping from touch sensing data to a haptic rendering device. To bridge this
gap, we collect high-resolution tactile data with a GelSight sensor and create
a new visuotactile clothing dataset. We then develop a conditional generative
model that synthesizes both visual and tactile outputs from a single sketch. We
evaluate our method regarding image quality and tactile rendering accuracy.
Finally, we introduce a pipeline to render high-quality visual and tactile
outputs on an electroadhesion-based haptic device for an immersive experience,
allowing for challenging materials and editable sketch inputs.",None,-1
HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection,0.536631,"Very-high-resolution (VHR) remote sensing (RS) image change detection (CD)
has been a challenging task for its very rich spatial information and sample
imbalance problem. In this paper, we have proposed a hierarchical change
guiding map network (HCGMNet) for change detection. The model uses hierarchical
convolution operations to extract multiscale features, continuously merges
multi-scale features layer by layer to improve the expression of global and
local information, and guides the model to gradually refine edge features and
comprehensive performance by a change guide module (CGM), which is a
self-attention with changing guide map. Extensive experiments on two CD
datasets show that the proposed HCGMNet architecture achieves better CD
performance than existing state-of-the-art (SOTA) CD methods.",None,-1
Inter-Annotator Agreement in the Wild: Uncovering Its Emerging Roles and Considerations in Real-World Scenarios,0.388692,"Inter-Annotator Agreement (IAA) is commonly used as a measure of label
consistency in natural language processing tasks. However, in real-world
scenarios, IAA has various roles and implications beyond its traditional usage.
In this paper, we not only consider IAA as a measure of consistency but also as
a versatile tool that can be effectively utilized in practical applications.
Moreover, we discuss various considerations and potential concerns when
applying IAA and suggest strategies for effectively navigating these
challenges.",None,-1
"General Policies, Subgoal Structure, and Planning Width",0.0761464,"It has been observed that many classical planning domains with atomic goals
can be solved by means of a simple polynomial exploration procedure, called IW,
that runs in time exponential in the problem width, which in these cases is
bounded and small. Yet, while the notion of width has become part of
state-of-the-art planning algorithms such as BFWS, there is no good explanation
for why so many benchmark domains have bounded width when atomic goals are
considered. In this work, we address this question by relating bounded width
with the existence of general optimal policies that in each planning instance
are represented by tuples of atoms of bounded size. We also define the notions
of (explicit) serializations and serialized width that have a broader scope as
many domains have a bounded serialized width but no bounded width. Such
problems are solved non-optimally in polynomial time by a suitable variant of
the Serialized IW algorithm. Finally, the language of general policies and the
semantics of serializations are combined to yield a simple, meaningful, and
expressive language for specifying serializations in compact form in the form
of sketches, which can be used for encoding domain control knowledge by hand or
for learning it from small examples. Sketches express general problem
decompositions in terms of subgoals, and sketches of bounded width express
problem decompositions that can be solved in polynomial time.",None,-1
MVPSNet: Fast Generalizable Multi-view Photometric Stereo,0.980765,"We propose a fast and generalizable solution to Multi-view Photometric Stereo
(MVPS), called MVPSNet. The key to our approach is a feature extraction network
that effectively combines images from the same view captured under multiple
lighting conditions to extract geometric features from shading cues for stereo
matching. We demonstrate these features, termed `Light Aggregated Feature Maps'
(LAFM), are effective for feature matching even in textureless regions, where
traditional multi-view stereo methods fail. Our method produces similar
reconstruction results to PS-NeRF, a state-of-the-art MVPS method that
optimizes a neural network per-scene, while being 411$\times$ faster (105
seconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic
dataset for MVPS, sMVPS, which is shown to be effective to train a
generalizable MVPS method.",None,-1
A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech,0.973373,"Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have
achieved near human-level naturalness. The diversity of human speech, however,
often goes beyond the coverage of these corpora. We believe the ability to
handle such diversity is crucial for AI systems to achieve human-level
communication. Our work explores the use of more abundant real-world data for
building speech synthesizers. We train TTS systems using real-world speech from
YouTube and podcasts. We observe the mismatch between training and inference
alignments in mel-spectrogram based autoregressive models, leading to
unintelligible synthesis, and demonstrate that learned discrete codes within
multiple code groups effectively resolves this issue. We introduce our MQTTS
system whose architecture is designed for multiple code generation and
monotonic alignment, along with the use of a clean silence prompt to improve
synthesis quality. We conduct ablation analyses to identify the efficacy of our
methods. We show that MQTTS outperforms existing TTS systems in several
objective and subjective measures.",None,-1
Cultivated Wildness: Technodiversity and Wildness in Machines,0.329466,"This paper investigates the idea of cultivated wildness at the intersection
of landscape design and artificial intelligence. The paper posits that
contemporary landscape practices should overcome the potentially single
understanding on wilderness, and instead explore landscape strategies to
cultivate new forms of wild places via ideas and concerns in contemporary
Environmental Humanities, Science and Technology Studies, Ecological Sciences,
and Landscape Architecture. Drawing cases in environmental engineering,
computer science, and landscape architecture research, this paper explores a
framework to construct wild places with intelligent machines. In this
framework, machines are not understood as a layer of ""digital infrastructure""
that is used to extend localized human intelligence and agency. Rather machines
are conceptualized as active agents who can participate in the intelligence of
co-production. Recent developments in cybernetic technologies such as sensing
networks, artificial intelligence, and cyberphysical systems can also
contribute to establishing the framework. At the heart of this framework is
""technodiversity,"" in parallel with biodiversity, since a singular vision on
technological development driven by optimization and efficiency reinforces a
monocultural approach that eliminates other possible relationships to construct
with the environment. Thus, cultivated wildness is also about recognizing
""wildness"" in machines.",None,-1
Adapting a Language Model While Preserving its General Knowledge,0.467276,"Domain-adaptive pre-training (or DA-training for short), also known as
post-training, aims to train a pre-trained general-purpose language model (LM)
using an unlabeled corpus of a particular domain to adapt the LM so that
end-tasks in the domain can give improved performances. However, existing
DA-training methods are in some sense blind as they do not explicitly identify
what knowledge in the LM should be preserved and what should be changed by the
domain corpus. This paper shows that the existing methods are suboptimal and
proposes a novel method to perform a more informed adaptation of the knowledge
in the LM by (1) soft-masking the attention heads based on their importance to
best preserve the general knowledge in the LM and (2) contrasting the
representations of the general and the full (both general and domain knowledge)
to learn an integrated representation with both general and domain-specific
knowledge. Experimental results will demonstrate the effectiveness of the
proposed approach.",None,-1
VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders,0.249271,"Large-scale text-to-image diffusion models have shown impressive capabilities
for generative tasks by leveraging strong vision-language alignment from
pre-training. However, most vision-language discriminative tasks require
extensive fine-tuning on carefully-labeled datasets to acquire such alignment,
with great cost in time and computing resources. In this work, we explore
directly applying a pre-trained generative diffusion model to the challenging
discriminative task of visual grounding without any fine-tuning and additional
training dataset. Specifically, we propose VGDiffZero, a simple yet effective
zero-shot visual grounding framework based on text-to-image diffusion models.
We also design a comprehensive region-scoring method considering both global
and local contexts of each isolated proposal. Extensive experiments on RefCOCO,
RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance on
zero-shot visual grounding. Our code is available at
https://github.com/xuyang-liu16/VGDiffZero.",None,-1
Region-Aware Portrait Retouching with Sparse Interactive Guidance,0.277925,"Portrait retouching aims to improve the aesthetic quality of input portrait
photos and especially requires human-region priority. The deep learning-based
methods largely elevate the retouching efficiency and provide promising
retouched results. However, existing portrait retouching methods focus on
automatic retouching, which treats all human-regions equally and ignores users'
preferences for specific individuals, thus suffering from limited flexibility
in interactive scenarios. In this work, we emphasize the importance of users'
intents and explore the interactive portrait retouching task. Specifically, we
propose a region-aware retouching framework with two branches: an automatic
branch and an interactive branch. The automatic branch involves an
encoding-decoding process, which searches region candidates and performs
automatic region-aware retouching without user guidance. The interactive branch
encodes sparse user guidance into a priority condition vector and modulates
latent features with a region selection module to further emphasize the
user-specified regions. Experimental results show that our interactive branch
effectively captures users' intents and generalizes well to unseen scenes with
sparse user guidance, while our automatic branch also outperforms the
state-of-the-art retouching methods due to improved region-awareness.",None,-1
Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and Denoising,0.0132777,"Due to the low accuracy of object detection and recognition in many
intelligent surveillance systems at nighttime, the quality of night images is
crucial. Compared with the corresponding daytime image, nighttime image is
characterized as low brightness, low contrast and high noise. In this paper, a
bio-inspired image enhancement algorithm is proposed to convert a low
illuminance image to a brighter and clear one. Different from existing
bio-inspired algorithm, the proposed method doesn't use any training sequences,
we depend on a novel chain of contrast enhancement and denoising algorithms
without using any forms of recursive functions. Our method can largely improve
the brightness and contrast of night images, besides, suppress noise. Then we
implement on real experiment, and simulation experiment to test our algorithms.
Both results show the advantages of proposed algorithm over contrast pair,
Meylan and Retinex.",None,-1
Large Language Models can Learn Rules,0.981307,"When prompted with a few examples and intermediate steps, large language
models (LLMs) have demonstrated impressive performance in various reasoning
tasks. However, prompting methods that rely on implicit knowledge in an LLM
often generate incorrect answers when the implicit knowledge is wrong or
inconsistent with the task. To tackle this problem, we present
Hypotheses-to-Theories (HtT), a framework that learns a rule library for
reasoning with LLMs. HtT contains two stages, an induction stage and a
deduction stage. In the induction stage, an LLM is first asked to generate and
verify rules over a set of training examples. Rules that appear and lead to
correct answers sufficiently often are collected to form a rule library. In the
deduction stage, the LLM is then prompted to employ the learned rule library to
perform reasoning to answer test questions. Experiments on relational
reasoning, numerical reasoning and concept learning problems show that HtT
improves existing prompting methods, with an absolute gain of 10-30% in
accuracy. The learned rules are also transferable to different models and to
different forms of the same problem.",None,-1
Exploring the Benefits of Visual Prompting in Differential Privacy,0.755532,"Visual Prompting (VP) is an emerging and powerful technique that allows
sample-efficient adaptation to downstream tasks by engineering a well-trained
frozen source model. In this work, we explore the benefits of VP in
constructing compelling neural network classifiers with differential privacy
(DP). We explore and integrate VP into canonical DP training methods and
demonstrate its simplicity and efficiency. In particular, we discover that VP
in tandem with PATE, a state-of-the-art DP training method that leverages the
knowledge transfer from an ensemble of teachers, achieves the state-of-the-art
privacy-utility trade-off with minimum expenditure of privacy budget. Moreover,
we conduct additional experiments on cross-domain image classification with a
sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we
also conduct extensive ablation studies to validate the effectiveness and
contribution of VP under DP consideration. Our code is available at
(https://github.com/EzzzLi/Prompt-PATE).",None,-1
K-ESConv: Knowledge Injection for Emotional Support Dialogue Systems via Prompt Learning,0.611104,"Automatic psychological counseling requires mass of professional knowledge
that can be found in online counseling forums. Motivated by this, we propose
K-ESConv, a novel prompt learning based knowledge injection method for
emotional support dialogue system, transferring forum knowledge to response
generation. We evaluate our model on an emotional support dataset ESConv, where
the model retrieves and incorporates knowledge from external professional
emotional Q\&A forum. Experiment results show that the proposed method
outperforms existing baselines on both automatic evaluation and human
evaluation, which shows that our approach significantly improves the
correlation and diversity of responses and provides more comfort and better
suggestion for the seeker.",None,-1
InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent,0.882771,"This research paper delves into the integration of OpenAI's ChatGPT into
embodied agent systems, evaluating its influence on interactive decision-making
benchmark. Drawing a parallel to the concept of people assuming roles according
to their unique strengths, we introduce InterAct. In this approach, we feed
ChatGPT with varied prompts, assigning it a numerous roles like a checker and a
sorter, then integrating them with the original language model. Our research
shows a remarkable success rate of 98% in AlfWorld, which consists of 6
different tasks in a simulated household environment, emphasizing the
significance of proficient prompt engineering. The results highlight ChatGPT's
competence in comprehending and performing intricate tasks effectively in
real-world settings, thus paving the way for further advancements in task
planning.",None,-1
Large-Scale Knowledge Synthesis and Complex Information Retrieval from Biomedical Documents,0.298113,"Recent advances in the healthcare industry have led to an abundance of
unstructured data, making it challenging to perform tasks such as efficient and
accurate information retrieval at scale. Our work offers an all-in-one scalable
solution for extracting and exploring complex information from large-scale
research documents, which would otherwise be tedious. First, we briefly explain
our knowledge synthesis process to extract helpful information from
unstructured text data of research documents. Then, on top of the knowledge
extracted from the documents, we perform complex information retrieval using
three major components- Paragraph Retrieval, Triplet Retrieval from Knowledge
Graphs, and Complex Question Answering (QA). These components combine lexical
and semantic-based methods to retrieve paragraphs and triplets and perform
faceted refinement for filtering these search results. The complexity of
biomedical queries and documents necessitates using a QA system capable of
handling queries more complex than factoid queries, which we evaluate
qualitatively on the COVID-19 Open Research Dataset (CORD-19) to demonstrate
the effectiveness and value-add.",None,-1
Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures,0.0222361,"During the past decade, Deep Learning (DL) algorithms, programming systems
and hardware have converged with the High Performance Computing (HPC)
counterparts. Nevertheless, the programming methodology of DL and HPC systems
is stagnant, relying on highly-optimized, yet platform-specific and inflexible
vendor-optimized libraries. Such libraries provide close-to-peak performance on
specific platforms, kernels and shapes thereof that vendors have dedicated
optimizations efforts, while they underperform in the remaining use-cases,
yielding non-portable codes with performance glass-jaws. This work introduces a
framework to develop efficient, portable DL and HPC kernels for modern CPU
architectures. We decompose the kernel development in two steps: 1) Expressing
the computational core using Tensor Processing Primitives (TPPs): a compact,
versatile set of 2D-tensor operators, 2) Expressing the logical loops around
TPPs in a high-level, declarative fashion whereas the exact instantiation
(ordering, tiling, parallelization) is determined via simple knobs. We
demonstrate the efficacy of our approach using standalone kernels and
end-to-end workloads that outperform state-of-the-art implementations on
diverse CPU platforms.",None,-1
I2I: Initializing Adapters with Improvised Knowledge,0.343971,"Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.",None,-1
Arabic Dialect Identification under Scrutiny: Limitations of Single-label Classification,0.972463,"Automatic Arabic Dialect Identification (ADI) of text has gained great
popularity since it was introduced in the early 2010s. Multiple datasets were
developed, and yearly shared tasks have been running since 2018. However, ADI
systems are reported to fail in distinguishing between the micro-dialects of
Arabic. We argue that the currently adopted framing of the ADI task as a
single-label classification problem is one of the main reasons for that. We
highlight the limitation of the incompleteness of the Dialect labels and
demonstrate how it impacts the evaluation of ADI systems. A manual error
analysis for the predictions of an ADI, performed by 7 native speakers of
different Arabic dialects, revealed that $\approx$ 66% of the validated errors
are not true errors. Consequently, we propose framing ADI as a multi-label
classification task and give recommendations for designing new ADI datasets.",None,-1
Fast Feedforward Networks,0.260387,"We break the linear link between the layer size and its inference cost by
introducing the fast feedforward (FFF) architecture, a log-time alternative to
feedforward networks. We demonstrate that FFFs are up to 220x faster than
feedforward networks, up to 6x faster than mixture-of-experts networks, and
exhibit better training properties than mixtures of experts thanks to noiseless
conditional execution. Pushing FFFs to the limit, we show that they can use as
little as 1% of layer neurons for inference in vision transformers while
preserving 94.2% of predictive performance.",None,-1
On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence,0.999999,"Large pre-trained models, also known as foundation models (FMs), are trained
in a task-agnostic manner on large-scale data and can be adapted to a wide
range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.
Despite their successes in language and vision tasks, we have yet seen an
attempt to develop foundation models for geospatial artificial intelligence
(GeoAI). In this work, we explore the promises and challenges of developing
multimodal foundation models for GeoAI. We first investigate the potential of
many existing FMs by testing their performances on seven tasks across multiple
geospatial subdomains including Geospatial Semantics, Health Geography, Urban
Geography, and Remote Sensing. Our results indicate that on several geospatial
tasks that only involve text modality such as toponym recognition, location
description recognition, and US state-level/county-level dementia time series
forecasting, these task-agnostic LLMs can outperform task-specific
fully-supervised models in a zero-shot or few-shot learning setting. However,
on other geospatial tasks, especially tasks that involve multiple data
modalities (e.g., POI-based urban function classification, street view
image-based urban noise intensity classification, and remote sensing image
scene classification), existing foundation models still underperform
task-specific models. Based on these observations, we propose that one of the
major challenges of developing a FM for GeoAI is to address the multimodality
nature of geospatial tasks. After discussing the distinct challenges of each
geospatial data modality, we suggest the possibility of a multimodal foundation
model which can reason over various types of geospatial data through geospatial
alignments. We conclude this paper by discussing the unique risks and
challenges to develop such a model for GeoAI.",None,-1
Fine-Tashkeel: Finetuning Byte-Level Models for Accurate Arabic Text Diacritization,0.524996,"Most of previous work on learning diacritization of the Arabic language
relied on training models from scratch. In this paper, we investigate how to
leverage pre-trained language models to learn diacritization. We finetune
token-free pre-trained multilingual models (ByT5) to learn to predict and
insert missing diacritics in Arabic text, a complex task that requires
understanding the sentence semantics and the morphological structure of the
tokens. We show that we can achieve state-of-the-art on the diacritization task
with minimal amount of training and no feature engineering, reducing WER by
40%. We release our finetuned models for the greater benefit of the researchers
in the community.",None,-1
Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization,0.563211,"The rapid advancements in computer vision have stimulated remarkable progress
in face forgery techniques, capturing the dedicated attention of researchers
committed to detecting forgeries and precisely localizing manipulated areas.
Nonetheless, with limited fine-grained pixel-wise supervision labels, deepfake
detection models perform unsatisfactorily on precise forgery detection and
localization. To address this challenge, we introduce the well-trained vision
segmentation foundation model, i.e., Segment Anything Model (SAM) in face
forgery detection and localization. Based on SAM, we propose the Detect Any
Deepfakes (DADF) framework with the Multiscale Adapter, which can capture
short- and long-range forgery contexts for efficient fine-tuning. Moreover, to
better identify forged traces and augment the model's sensitivity towards
forgery regions, Reconstruction Guided Attention (RGA) module is proposed. The
proposed framework seamlessly integrates end-to-end forgery localization and
detection optimization. Extensive experiments on three benchmark datasets
demonstrate the superiority of our approach for both forgery detection and
localization. The codes will be released soon at
https://github.com/laiyingxin2/DADF.",None,-1
Universal Agent Mixtures and the Geometry of Intelligence,0.133282,"Inspired by recent progress in multi-agent Reinforcement Learning (RL), in
this work we examine the collective intelligent behaviour of theoretical
universal agents by introducing a weighted mixture operation. Given a weighted
set of agents, their weighted mixture is a new agent whose expected total
reward in any environment is the corresponding weighted average of the original
agents' expected total rewards in that environment. Thus, if RL agent
intelligence is quantified in terms of performance across environments, the
weighted mixture's intelligence is the weighted average of the original agents'
intelligences. This operation enables various interesting new theorems that
shed light on the geometry of RL agent intelligence, namely: results about
symmetries, convex agent-sets, and local extrema. We also show that any RL
agent intelligence measure based on average performance across environments,
subject to certain weak technical conditions, is identical (up to a constant
factor) to performance within a single environment dependent on said
intelligence measure.",None,-1
A Mathematical Guide to Operator Learning,0.871258,"Operator learning aims to discover properties of an underlying dynamical
system or partial differential equation (PDE) from data. Here, we present a
step-by-step guide to operator learning. We explain the types of problems and
PDEs amenable to operator learning, discuss various neural network
architectures, and explain how to employ numerical PDE solvers effectively. We
also give advice on how to create and manage training data and conduct
optimization. We offer intuition behind the various neural network
architectures employed in operator learning by motivating them from the
point-of-view of numerical linear algebra.",None,-1
Silver Syntax Pre-training for Cross-Domain Relation Extraction,0.231544,"Relation Extraction (RE) remains a challenging task, especially when
considering realistic out-of-domain evaluations. One of the main reasons for
this is the limited training size of current RE datasets: obtaining
high-quality (manually annotated) data is extremely expensive and cannot
realistically be repeated for each new domain. An intermediate training step on
data from related tasks has shown to be beneficial across many NLP
tasks.However, this setup still requires supplementary annotated data, which is
often not available. In this paper, we investigate intermediate pre-training
specifically for RE. We exploit the affinity between syntactic structure and
semantic RE, and identify the syntactic relations which are closely related to
RE by being on the shortest dependency path between two entities. We then take
advantage of the high accuracy of current syntactic parsers in order to
automatically obtain large amounts of low-cost pre-training data. By
pre-training our RE model on the relevant syntactic relations, we are able to
outperform the baseline in five out of six cross-domain setups, without any
additional annotated data.",None,-1
Context-Gloss Augmentation for Improving Arabic Target Sense Verification,0.770509,"Arabic language lacks semantic datasets and sense inventories. The most
common semantically-labeled dataset for Arabic is the ArabGlossBERT, a
relatively small dataset that consists of 167K context-gloss pairs (about 60K
positive and 107K negative pairs), collected from Arabic dictionaries. This
paper presents an enrichment to the ArabGlossBERT dataset, by augmenting it
using (Arabic-English-Arabic) machine back-translation. Augmentation increased
the dataset size to 352K pairs (149K positive and 203K negative pairs). We
measure the impact of augmentation using different data configurations to
fine-tune BERT on target sense verification (TSV) task. Overall, the accuracy
ranges between 78% to 84% for different data configurations. Although our
approach performed at par with the baseline, we did observe some improvements
for some POS tags in some experiments. Furthermore, our fine-tuned models are
trained on a larger dataset covering larger vocabulary and contexts. We provide
an in-depth analysis of the accuracy for each part-of-speech (POS).",None,-1
TransFlow: Transformer as Flow Learner,0.974688,"Optical flow is an indispensable building block for various important
computer vision tasks, including motion estimation, object tracking, and
disparity measurement. In this work, we propose TransFlow, a pure transformer
architecture for optical flow estimation. Compared to dominant CNN-based
methods, TransFlow demonstrates three advantages. First, it provides more
accurate correlation and trustworthy matching in flow estimation by utilizing
spatial self-attention and cross-attention mechanisms between adjacent frames
to effectively capture global dependencies; Second, it recovers more
compromised information (e.g., occlusion and motion blur) in flow estimation
through long-range temporal association in dynamic scenes; Third, it enables a
concise self-learning paradigm and effectively eliminate the complex and
laborious multi-stage pre-training procedures. We achieve the state-of-the-art
results on the Sintel, KITTI-15, as well as several downstream tasks, including
video object detection, interpolation and stabilization. For its efficacy, we
hope TransFlow could serve as a flexible baseline for optical flow estimation.",None,-1
Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,0.749927,"Although large language models (LLMs) are often pre-trained on large-scale
multilingual texts, their reasoning abilities and real-world knowledge are
mainly evaluated based on English datasets. Assessing LLM capabilities beyond
English is increasingly vital but hindered due to the lack of suitable
datasets. In this work, we introduce IndoMMLU, the first multi-task language
understanding benchmark for Indonesian culture and languages, which consists of
questions from primary school to university entrance exams in Indonesia. By
employing professional teachers, we obtain 14,981 questions across 64 tasks and
education levels, with 46% of the questions focusing on assessing proficiency
in the Indonesian language and knowledge of nine local languages and cultures
in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass
the Indonesian primary school level, with limited knowledge of local Indonesian
languages and culture. Other smaller models such as BLOOMZ and Falcon perform
at even lower levels.",None,-1
Balancing Logit Variation for Long-tailed Semantic Segmentation,0.53828,"Semantic segmentation usually suffers from a long-tail data distribution. Due
to the imbalanced number of samples across categories, the features of those
tail classes may get squeezed into a narrow area in the feature space. Towards
a balanced feature distribution, we introduce category-wise variation into the
network predictions in the training phase such that an instance is no longer
projected to a feature point, but a small region instead. Such a perturbation
is highly dependent on the category scale, which appears as assigning smaller
variation to head classes and larger variation to tail classes. In this way, we
manage to close the gap between the feature areas of different categories,
resulting in a more balanced representation. It is noteworthy that the
introduced variation is discarded at the inference stage to facilitate a
confident prediction. Although with an embarrassingly simple implementation,
our method manifests itself in strong generalizability to various datasets and
task settings. Extensive experiments suggest that our plug-in design lends
itself well to a range of state-of-the-art approaches and boosts the
performance on top of them.",None,-1
Denoising Diffusion Autoencoders are Unified Self-supervised Learners,0.643802,"Inspired by recent advances in diffusion models, which are reminiscent of
denoising autoencoders, we investigate whether they can acquire discriminative
representations for classification via generative pre-training. This paper
shows that the networks in diffusion models, namely denoising diffusion
autoencoders (DDAE), are unified self-supervised learners: by pre-training on
unconditional image generation, DDAE has already learned strongly
linear-separable representations within its intermediate layers without
auxiliary encoders, thus making diffusion pre-training emerge as a general
approach for generative-and-discriminative dual learning. To validate this, we
conduct linear probe and fine-tuning evaluations. Our diffusion-based approach
achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and
Tiny-ImageNet, respectively, and is comparable to contrastive learning and
masked autoencoders for the first time. Transfer learning from ImageNet also
confirms the suitability of DDAE for Vision Transformers, suggesting the
potential to scale DDAEs as unified foundation models. Code is available at
github.com/FutureXiang/ddae.",None,-1
Patton: Language Model Pretraining on Text-Rich Networks,0.699437,"A real-world text corpus sometimes comprises not only text documents but also
semantic links between them (e.g., academic papers in a bibliographic network
are linked by citations and co-authorships). Text documents and semantic
connections form a text-rich network, which empowers a wide range of downstream
tasks such as classification and retrieval. However, pretraining methods for
such structures are still lacking, making it difficult to build one generic
model that can be adapted to various tasks on text-rich networks. Current
pretraining objectives, such as masked language modeling, purely model texts
and do not take inter-document structure information into consideration. To
this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.
Patton includes two pretraining strategies: network-contextualized masked
language modeling and masked node prediction, to capture the inherent
dependency between textual attributes and network structure. We conduct
experiments on four downstream tasks in five datasets from both academic and
e-commerce domains, where Patton outperforms baselines significantly and
consistently.",None,-1
3D Registration with Maximal Cliques,0.974826,"As a fundamental problem in computer vision, 3D point cloud registration
(PCR) aims to seek the optimal pose to align a point cloud pair. In this paper,
we present a 3D registration method with maximal cliques (MAC). The key insight
is to loosen the previous maximum clique constraint, and mine more local
consensus information in a graph for accurate pose hypotheses generation: 1) A
compatibility graph is constructed to render the affinity relationship between
initial correspondences. 2) We search for maximal cliques in the graph, each of
which represents a consensus set. We perform node-guided clique selection then,
where each node corresponds to the maximal clique with the greatest graph
weight. 3) Transformation hypotheses are computed for the selected cliques by
the SVD algorithm and the best hypothesis is used to perform registration.
Extensive experiments on U3M, 3DMatch, 3DLoMatch and KITTI demonstrate that MAC
effectively increases registration accuracy, outperforms various
state-of-the-art methods and boosts the performance of deep-learned methods.
MAC combined with deep-learned methods achieves state-of-the-art registration
recall of 95.7% / 78.9% on 3DMatch / 3DLoMatch.",None,-1
TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental Implant Position Prediction,0.845083,"In implant prosthesis treatment, the surgical guide of implant is used to
ensure accurate implantation. However, such design heavily relies on the manual
location of the implant position. When deep neural network has been proposed to
assist the dentist in locating the implant position, most of them take a single
slice as input, which do not fully explore 3D contextual information and
ignoring the influence of implant slope. In this paper, we design a Text Guided
3D Context and Slope Aware Triple Network (TCSloT) which enables the perception
of contextual information from multiple adjacent slices and awareness of
variation of implant slopes. A Texture Variation Perception (TVP) module is
correspondingly elaborated to process the multiple slices and capture the
texture variation among slices and a Slope-Aware Loss (SAL) is proposed to
dynamically assign varying weights for the regression head. Additionally, we
design a conditional text guidance (CTG) module to integrate the text condition
(i.e., left, middle and right) from the CLIP for assisting the implant position
prediction. Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed TCSloT achieves superior
performance than existing methods.",None,-1
Discovering Universal Geometry in Embeddings with ICA,0.760512,"This study utilizes Independent Component Analysis (ICA) to unveil a
consistent semantic structure within embeddings of words or images. Our
approach extracts independent semantic components from the embeddings of a
pre-trained model by leveraging anisotropic information that remains after the
whitening process in Principal Component Analysis (PCA). We demonstrate that
each embedding can be expressed as a composition of a few intrinsic
interpretable axes and that these semantic axes remain consistent across
different languages, algorithms, and modalities. The discovery of a universal
semantic structure in the geometric patterns of embeddings enhances our
understanding of the representations in embeddings.",None,-1
Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers,0.79087,"In recent years machine translation has become very successful for
high-resource language pairs. This has also sparked new interest in research on
the automatic translation of low-resource languages, including Indigenous
languages. However, the latter are deeply related to the ethnic and cultural
groups that speak (or used to speak) them. The data collection, modeling and
deploying machine translation systems thus result in new ethical questions that
must be addressed. Motivated by this, we first survey the existing literature
on ethical considerations for the documentation, translation, and general
natural language processing for Indigenous languages. Afterward, we conduct and
analyze an interview study to shed light on the positions of community leaders,
teachers, and language activists regarding ethical concerns for the automatic
translation of their languages. Our results show that the inclusion, at
different degrees, of native speakers and community members is vital to
performing better and more ethical research on Indigenous languages.",None,-1
Physical Deep Reinforcement Learning: Safety and Unknown Unknowns,0.265044,"In this paper, we propose the Phy-DRL: a physics-model-regulated deep
reinforcement learning framework for safety-critical autonomous systems. The
Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training,
ii) conjunctive residual control (i.e., integration of data-driven control and
physics-model-based control) and safety- \& stability-sensitive reward, and
iii) physics-model-based neural network editing, including link editing and
activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1)
tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable
safety and stability, and 3) strictly comply with physical knowledge pertaining
to Bellman equation and reward. The effectiveness of the Phy-DRL is finally
validated by an inverted pendulum and a quadruped robot. The experimental
results demonstrate that compared with purely data-driven DRL, Phy-DRL features
remarkably fewer learning parameters, accelerated training and enlarged reward,
while offering enhanced model robustness and safety assurance.",None,-1
A Study on the Impact of Face Image Quality on Face Recognition in the Wild,0.166082,"Deep learning has received increasing interests in face recognition recently.
Large quantities of deep learning methods have been proposed to handle various
problems appeared in face recognition. Quite a lot deep methods claimed that
they have gained or even surpassed human-level face verification performance in
certain databases. As we know, face image quality poses a great challenge to
traditional face recognition methods, e.g. model-driven methods with
hand-crafted features. However, a little research focus on the impact of face
image quality on deep learning methods, and even human performance. Therefore,
we raise a question: Is face image quality still one of the challenges for deep
learning based face recognition, especially in unconstrained condition. Based
on this, we further investigate this problem on human level. In this paper, we
partition face images into three different quality sets to evaluate the
performance of deep learning methods on cross-quality face images in the wild,
and then design a human face verification experiment on these cross-quality
data. The result indicates that quality issue still needs to be studied
thoroughly in deep learning, human own better capability in building the
relations between different face images with large quality gaps, and saying
deep learning method surpasses human-level is too optimistic.",None,-1
Implicit Stacked Autoregressive Model for Video Prediction,0.656266,"Future frame prediction has been approached through two primary methods:
autoregressive and non-autoregressive. Autoregressive methods rely on the
Markov assumption and can achieve high accuracy in the early stages of
prediction when errors are not yet accumulated. However, their performance
tends to decline as the number of time steps increases. In contrast,
non-autoregressive methods can achieve relatively high performance but lack
correlation between predictions for each time step. In this paper, we propose
an Implicit Stacked Autoregressive Model for Video Prediction (IAM4VP), which
is an implicit video prediction model that applies a stacked autoregressive
method. Like non-autoregressive methods, stacked autoregressive methods use the
same observed frame to estimate all future frames. However, they use their own
predictions as input, similar to autoregressive methods. As the number of time
steps increases, predictions are sequentially stacked in the queue. To evaluate
the effectiveness of IAM4VP, we conducted experiments on three common future
frame prediction benchmark datasets and weather\&climate prediction benchmark
datasets. The results demonstrate that our proposed model achieves
state-of-the-art performance.",None,-1
Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification,0.468754,"Clothes-invariant feature extraction is critical to the clothes-changing
person re-identification (CC-ReID). It can provide discriminative identity
features and eliminate the negative effects caused by the confounder--clothing
changes. But we argue that there exists a strong spurious correlation between
clothes and human identity, that restricts the common likelihood-based ReID
method P(Y|X) to extract clothes-irrelevant features. In this paper, we propose
a new Causal Clothes-Invariant Learning (CCIL) method to achieve
clothes-invariant feature learning by modeling causal intervention P(Y|do(X)).
This new causality-based model is inherently invariant to the confounder in the
causal view, which can achieve the clothes-invariant features and avoid the
barrier faced by the likelihood-based methods. Extensive experiments on three
CC-ReID benchmarks, including PRCC, LTCC, and VC-Clothes, demonstrate the
effectiveness of our approach, which achieves a new state of the art.",None,-1
X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification,0.358756,"Currently, most existing person re-identification methods use Instance-Level
features, which are extracted only from a single image. However, these
Instance-Level features can easily ignore the discriminative information due to
the appearance of each identity varies greatly in different images. Thus, it is
necessary to exploit Identity-Level features, which can be shared across
different images of each identity. In this paper, we propose to promote
Instance-Level features to Identity-Level features by employing cross-attention
to incorporate information from one image to another of the same identity, thus
more unified and discriminative pedestrian information can be obtained. We
propose a novel training framework named X-ReID. Specifically, a Cross
Intra-Identity Instances module (IntraX) fuses different intra-identity
instances to transfer Identity-Level knowledge and make Instance-Level features
more compact. A Cross Inter-Identity Instances module (InterX) involves hard
positive and hard negative instances to improve the attention response to the
same identity instead of different identity, which minimizes intra-identity
variation and maximizes inter-identity variation. Extensive experiments on
benchmark datasets show the superiority of our method over existing works.
Particularly, on the challenging MSMT17, our proposed method gains 1.1% mAP
improvements when compared to the second place.",None,-1
FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views,0.727817,"We present FlexNeRF, a method for photorealistic freeviewpoint rendering of
humans in motion from monocular videos. Our approach works well with sparse
views, which is a challenging scenario when the subject is exhibiting
fast/complex motions. We propose a novel approach which jointly optimizes a
canonical time and pose configuration, with a pose-dependent motion field and
pose-independent temporal deformations complementing each other. Thanks to our
novel temporal and cyclic consistency constraints along with additional losses
on intermediate representation such as segmentation, our approach provides high
quality outputs as the observed views become sparser. We empirically
demonstrate that our method significantly outperforms the state-of-the-art on
public benchmark datasets as well as a self-captured fashion dataset. The
project page is available at: https://flex-nerf.github.io/",None,-1
GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems,0.78489,"There has been considerable divergence of opinion on the reasoning abilities
of Large Language Models (LLMs). While the initial optimism that reasoning
might emerge automatically with scale has been tempered thanks to a slew of
counterexamples, a wide spread belief in their iterative self-critique
capabilities persists. In this paper, we set out to systematically investigate
the effectiveness of iterative prompting of LLMs in the context of Graph
Coloring, a canonical NP-complete reasoning problem that is related to
propositional satisfiability as well as practical problems like scheduling and
allocation. We present a principled empirical study of the performance of GPT4
in solving graph coloring instances or verifying the correctness of candidate
colorings. In iterative modes, we experiment with the model critiquing its own
answers and an external correct reasoner verifying proposed solutions. In both
cases, we analyze whether the content of the criticisms actually affects bottom
line performance. The study seems to indicate that (i) LLMs are bad at solving
graph coloring instances (ii) they are no better at verifying a solution--and
thus are not effective in iterative modes with LLMs critiquing LLM-generated
solutions (iii) the correctness and content of the criticisms--whether by LLMs
or external solvers--seems largely irrelevant to the performance of iterative
prompting. We show that the observed increase in effectiveness is largely due
to the correct solution being fortuitously present in the top-k completions of
the prompt (and being recognized as such by an external verifier). Our results
thus call into question claims about the self-critiquing capabilities of state
of the art LLMs.",None,-1
Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation,0.614387,"Human evaluation is critical for validating the performance of text-to-image
generative models, as this highly cognitive process requires deep comprehension
of text and images. However, our survey of 37 recent papers reveals that many
works rely solely on automatic measures (e.g., FID) or perform poorly described
human evaluations that are not reliable or repeatable. This paper proposes a
standardized and well-defined human evaluation protocol to facilitate
verifiable and reproducible human evaluation in future works. In our pilot data
collection, we experimentally show that the current automatic measures are
incompatible with human perception in evaluating the performance of the
text-to-image generation results. Furthermore, we provide insights for
designing human evaluation experiments reliably and conclusively. Finally, we
make several resources publicly available to the community to facilitate easy
and fast implementations.",None,-1
Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic-Structural Constraints,0.377871,"Ensuring the realism of computer-generated synthetic images is crucial to
deep neural network (DNN) training. Due to different semantic distributions
between synthetic and real-world captured datasets, there exists semantic
mismatch between synthetic and refined images, which in turn results in the
semantic distortion. Recently, contrastive learning (CL) has been successfully
used to pull correlated patches together and push uncorrelated ones apart. In
this work, we exploit semantic and structural consistency between synthetic and
refined images and adopt CL to reduce the semantic distortion. Besides, we
incorporate hard negative mining to improve the performance furthermore. We
compare the performance of our method with several other benchmarking methods
using qualitative and quantitative measures and show that our method offers the
state-of-the-art performance.",None,-1
Considering Layerwise Importance in the Lottery Ticket Hypothesis,0.19677,"The Lottery Ticket Hypothesis (LTH) showed that by iteratively training a
model, removing connections with the lowest global weight magnitude and
rewinding the remaining connections, sparse networks can be extracted.
  This global comparison removes context information between connections within
a layer. Here we study means for recovering some of this layer distributional
context and generalise the LTH to consider weight importance values rather than
global weight magnitudes.
  We find that given a repeatable training procedure, applying different
importance metrics leads to distinct performant lottery tickets with little
overlapping connections. This strongly suggests that lottery tickets are not
unique",None,-1
Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case,0.676124,"After the pandemic, artificial intelligence (AI) powered support for mental
health care has become increasingly important. The breadth and complexity of
significant challenges required to provide adequate care involve: (a)
Personalized patient understanding, (b) Safety-constrained and medically
validated chatbot patient interactions, and (c) Support for continued
feedback-based refinements in design using chatbot-patient interactions. We
propose Alleviate, a chatbot designed to assist patients suffering from mental
health challenges with personalized care and assist clinicians with
understanding their patients better. Alleviate draws from an array of publicly
available clinically valid mental-health texts and databases, allowing
Alleviate to make medically sound and informed decisions. In addition,
Alleviate's modular design and explainable decision-making lends itself to
robust and continued feedback-based refinements to its design. In this paper,
we explain the different modules of Alleviate and submit a short video
demonstrating Alleviate's capabilities to help patients and clinicians
understand each other better to facilitate optimal care strategies.",None,-1
Can We Revitalize Interventional Healthcare with AI-XR Surgical Metaverses?,0.19339,"Recent advancements in technology, particularly in machine learning (ML),
deep learning (DL), and the metaverse, offer great potential for
revolutionizing surgical science. The combination of artificial intelligence
and extended reality (AI-XR) technologies has the potential to create a
surgical metaverse, a virtual environment where surgeries can be planned and
performed. This paper aims to provide insight into the various potential
applications of an AI-XR surgical metaverse and the challenges that must be
addressed to bring its full potential to fruition. It is important for the
community to focus on these challenges to fully realize the potential of the
AI-XR surgical metaverses. Furthermore, to emphasize the need for secure and
robust AI-XR surgical metaverses and to demonstrate the real-world implications
of security threats to the AI-XR surgical metaverses, we present a case study
in which the ``an immersive surgical attack'' on incision point localization is
performed in the context of preoperative planning in a surgical metaverse.",None,-1
Using Artificial Populations to Study Psychological Phenomena in Neural Models,0.0579342,"The recent proliferation of research into transformer based natural language
processing has led to a number of studies which attempt to detect the presence
of human-like cognitive behavior in the models. We contend that, as is true of
human psychology, the investigation of cognitive behavior in language models
must be conducted in an appropriate population of an appropriate size for the
results to be meaningful. We leverage work in uncertainty estimation in a novel
approach to efficiently construct experimental populations. The resultant tool,
PopulationLM, has been made open source. We provide theoretical grounding in
the uncertainty estimation literature and motivation from current cognitive
work regarding language models. We discuss the methodological lessons from
other scientific communities and attempt to demonstrate their application to
two artificial population studies. Through population based experimentation we
find that language models exhibit behavior consistent with typicality effects
among categories highly represented in training. However, we find that language
models don't tend to exhibit structural priming effects. Generally, our results
show that single models tend to over estimate the presence of cognitive
behaviors in neural models.",None,-1
DiffusionAD: Norm-guided One-step Denoising Diffusion for Anomaly Detection,0.0776775,"Anomaly detection has garnered extensive applications in real industrial
manufacturing due to its remarkable effectiveness and efficiency. However,
previous generative-based models have been limited by suboptimal reconstruction
quality, hampering their overall performance. A fundamental enhancement lies in
our reformulation of the reconstruction process using a diffusion model into a
noise-to-norm paradigm. Here, anomalous regions are perturbed with Gaussian
noise and reconstructed as normal, overcoming the limitations of previous
models by facilitating anomaly-free restoration. Additionally, we propose a
rapid one-step denoising paradigm, significantly faster than the traditional
iterative denoising in diffusion models. Furthermore, the introduction of the
norm-guided paradigm elevates the accuracy and fidelity of reconstructions. The
segmentation sub-network predicts pixel-level anomaly scores using the input
image and its anomaly-free restoration. Comprehensive evaluations on four
standard and challenging benchmarks reveal that DiffusionAD outperforms current
state-of-the-art approaches, demonstrating the effectiveness and broad
applicability of the proposed pipeline.",None,-1
A Dynamic Multi-Scale Voxel Flow Network for Video Prediction,0.99586,"The performance of video prediction has been greatly boosted by advanced deep
neural networks. However, most of the current methods suffer from large model
sizes and require extra inputs, e.g., semantic/depth maps, for promising
performance. For efficiency consideration, in this paper, we propose a Dynamic
Multi-scale Voxel Flow Network (DMVFN) to achieve better video prediction
performance at lower computational costs with only RGB images, than previous
methods. The core of our DMVFN is a differentiable routing module that can
effectively perceive the motion scales of video frames. Once trained, our DMVFN
selects adaptive sub-networks for different inputs at the inference stage.
Experiments on several benchmarks demonstrate that our DMVFN is an order of
magnitude faster than Deep Voxel Flow and surpasses the state-of-the-art
iterative-based OPT on generated image quality. Our code and demo are available
at https://huxiaotaostasy.github.io/DMVFN/.",None,-1
Skeleton-based action analysis for ADHD diagnosis,0.188169,"Attention Deficit Hyperactivity Disorder (ADHD) is a common neurobehavioral
disorder worldwide. While extensive research has focused on machine learning
methods for ADHD diagnosis, most research relies on high-cost equipment, e.g.,
MRI machine and EEG patch. Therefore, low-cost diagnostic methods based on the
action characteristics of ADHD are desired. Skeleton-based action recognition
has gained attention due to the action-focused nature and robustness. In this
work, we propose a novel ADHD diagnosis system with a skeleton-based action
recognition framework, utilizing a real multi-modal ADHD dataset and
state-of-the-art detection algorithms. Compared to conventional methods, the
proposed method shows cost-efficiency and significant performance improvement,
making it more accessible for a broad range of initial ADHD diagnoses. Through
the experiment results, the proposed method outperforms the conventional
methods in accuracy and AUC. Meanwhile, our method is widely applicable for
mass screening.",None,-1
Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation,0.963985,"The ability to collect a large dataset of human preferences from
text-to-image users is usually limited to companies, making such datasets
inaccessible to the public. To address this issue, we create a web app that
enables text-to-image users to generate images and specify their preferences.
Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image
prompts and real users' preferences over generated images. We leverage this
dataset to train a CLIP-based scoring function, PickScore, which exhibits
superhuman performance on the task of predicting human preferences. Then, we
test PickScore's ability to perform model evaluation and observe that it
correlates better with human rankings than other automatic evaluation metrics.
Therefore, we recommend using PickScore for evaluating future text-to-image
generation models, and using Pick-a-Pic prompts as a more relevant dataset than
MS-COCO. Finally, we demonstrate how PickScore can enhance existing
text-to-image models via ranking.",None,-1
Multistage Spatial Context Models for Learned Image Compression,0.621367,"Recent state-of-the-art Learned Image Compression methods feature spatial
context models, achieving great rate-distortion improvements over hyperprior
methods. However, the autoregressive context model requires serial decoding,
limiting runtime performance. The Checkerboard context model allows parallel
decoding at a cost of reduced RD performance. We present a series of multistage
spatial context models allowing both fast decoding and better RD performance.
We split the latent space into square patches and decode serially within each
patch while different patches are decoded in parallel. The proposed method
features a comparable decoding speed to Checkerboard while reaching the RD
performance of Autoregressive and even also outperforming Autoregressive.
Inside each patch, the decoding order must be carefully decided as a bad order
negatively impacts performance; therefore, we also propose a decoding order
optimization algorithm.",None,-1
P+: Extended Textual Conditioning in Text-to-Image Generation,0.918145,"We introduce an Extended Textual Conditioning space in text-to-image models,
referred to as $P+$. This space consists of multiple textual conditions,
derived from per-layer prompts, each corresponding to a layer of the denoising
U-net of the diffusion model.
  We show that the extended space provides greater disentangling and control
over image synthesis. We further introduce Extended Textual Inversion (XTI),
where the images are inverted into $P+$, and represented by per-layer tokens.
  We show that XTI is more expressive and precise, and converges faster than
the original Textual Inversion (TI) space. The extended inversion method does
not involve any noticeable trade-off between reconstruction and editability and
induces more regular inversions.
  We conduct a series of extensive experiments to analyze and understand the
properties of the new space, and to showcase the effectiveness of our method
for personalizing text-to-image models. Furthermore, we utilize the unique
properties of this space to achieve previously unattainable results in
object-style mixing using text-to-image models. Project page:
https://prompt-plus.github.io",None,-1
Visual-Language Prompt Tuning with Knowledge-guided Context Optimization,0.864916,"Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.",None,-1
Improving Contrastive Learning of Sentence Embeddings from AI Feedback,0.73365,"Contrastive learning has become a popular approach in natural language
processing, particularly for the learning of sentence embeddings. However, the
discrete nature of natural language makes it difficult to ensure the quality of
positive and negative sample pairs generated through data augmentation methods.
Although supervised contrastive learning can produce more accurate sample pairs
with human feedback labels, it still lacks fine-grained training signals. In
this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of
sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our
method utilizes AI feedback from large pre-trained language models (LLMs) to
construct sample pairs with fine-grained sample similarity scores to improve
contrastive learning. Besides, we combine human feedback and AI feedback to
provide better supervision signals for supervised contrastive learning of
sentence embeddings. Experimental results show that our method achieves
state-of-the-art performance on several semantic textual similarity (STS) and
transfer learning tasks compared to other unsupervised and supervised
contrastive learning methods.",None,-1
Arukikata Travelogue Dataset,0.0425294,"We have constructed Arukikata Travelogue Dataset and released it free of
charge for academic research. This dataset is a Japanese text dataset with a
total of over 31 million words, comprising 4,672 Japanese domestic travelogues
and 9,607 overseas travelogues. Before providing our dataset, there was a
scarcity of widely available travelogue data for research purposes, and each
researcher had to prepare their own data. This hinders the replication of
existing studies and fair comparative analysis of experimental results. Our
dataset enables any researchers to conduct investigation on the same data and
to ensure transparency and reproducibility in research. In this paper, we
describe the academic significance, characteristics, and prospects of our
dataset.",None,-1
A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI),0.808963,"Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.",None,-1
Detecting Backdoors in Pre-trained Encoders,0.480161,"Self-supervised learning in computer vision trains on unlabeled data, such as
images or (image, text) pairs, to obtain an image encoder that learns
high-quality embeddings for input data. Emerging backdoor attacks towards
encoders expose crucial vulnerabilities of self-supervised learning, since
downstream classifiers (even further trained on clean data) may inherit
backdoor behaviors from encoders. Existing backdoor detection methods mainly
focus on supervised learning settings and cannot handle pre-trained encoders
especially when input labels are not available. In this paper, we propose
DECREE, the first backdoor detection approach for pre-trained encoders,
requiring neither classifier headers nor input labels. We evaluate DECREE on
over 400 encoders trojaned under 3 paradigms. We show the effectiveness of our
method on image encoders pre-trained on ImageNet and OpenAI's CLIP 400 million
image-text pairs. Our method consistently has a high detection accuracy even if
we have only limited or no access to the pre-training dataset.",None,-1
Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,0.327369,"With the rapid evolution of large language models (LLMs), new and
hard-to-predict harmful capabilities are emerging. This requires developers to
be able to identify risks through the evaluation of ""dangerous capabilities"" in
order to responsibly deploy LLMs. In this work, we collect the first
open-source dataset to evaluate safeguards in LLMs, and deploy safer
open-source LLMs at a low cost. Our dataset is curated and filtered to consist
only of instructions that responsible language models should not follow. We
annotate and assess the responses of six popular LLMs to these instructions.
Based on our annotation, we proceed to train several BERT-like classifiers, and
find that these small classifiers can achieve results that are comparable with
GPT-4 on automatic safety evaluation. Warning: this paper contains example data
that may be offensive, harmful, or biased.",None,-1
HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer,0.492002,"Visual localization is critical to many applications in computer vision and
robotics. To address single-image RGB localization, state-of-the-art
feature-based methods match local descriptors between a query image and a
pre-built 3D model. Recently, deep neural networks have been exploited to
regress the mapping between raw pixels and 3D coordinates in the scene, and
thus the matching is implicitly performed by the forward pass through the
network. However, in a large and ambiguous environment, learning such a
regression task directly can be difficult for a single network. In this work,
we present a new hierarchical scene coordinate network to predict pixel scene
coordinates in a coarse-to-fine manner from a single RGB image. The proposed
method, which is an extension of HSCNet, allows us to train compact models
which scale robustly to large environments. It sets a new state-of-the-art for
single-image localization on the 7-Scenes, 12 Scenes, Cambridge Landmarks
datasets, and the combined indoor scenes.",None,-1
Dynamic Coarse-to-Fine Learning for Oriented Tiny Object Detection,0.624324,"Detecting arbitrarily oriented tiny objects poses intense challenges to
existing detectors, especially for label assignment. Despite the exploration of
adaptive label assignment in recent oriented object detectors, the extreme
geometry shape and limited feature of oriented tiny objects still induce severe
mismatch and imbalance issues. Specifically, the position prior, positive
sample feature, and instance are mismatched, and the learning of extreme-shaped
objects is biased and unbalanced due to little proper feature supervision. To
tackle these issues, we propose a dynamic prior along with the coarse-to-fine
assigner, dubbed DCFL. For one thing, we model the prior, label assignment, and
object representation all in a dynamic manner to alleviate the mismatch issue.
For another, we leverage the coarse prior matching and finer posterior
constraint to dynamically assign labels, providing appropriate and relatively
balanced supervision for diverse instances. Extensive experiments on six
datasets show substantial improvements to the baseline. Notably, we obtain the
state-of-the-art performance for one-stage detectors on the DOTA-v1.5,
DOTA-v2.0, and DIOR-R datasets under single-scale training and testing. Codes
are available at https://github.com/Chasel-Tsui/mmrotate-dcfl.",None,-1
Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming,0.89483,"The rapid advancements in artificial intelligence (AI) have led to a growing
trend of human-AI teaming (HAT) in various fields. As machines continue to
evolve from mere automation to a state of autonomy, they are increasingly
exhibiting unexpected behaviors and human-like cognitive/intelligent
capabilities, including situation awareness (SA). This shift has the potential
to enhance the performance of mixed human-AI teams over all-human teams,
underscoring the need for a better understanding of the dynamic SA interactions
between humans and machines. To this end, we provide a review of leading SA
theoretical models and a new framework for SA in the HAT context based on the
key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA)
framework unifies human and AI behavior, and involves bidirectional, and
dynamic interaction. The framework is based on the individual and team SA
models and elaborates on the cognitive mechanisms for modeling HAT. Similar
perceptual cycles are adopted for the individual (including both human and AI)
and the whole team, which is tailored to the unique requirements of the HAT
context. ATSA emphasizes cohesive and effective HAT through structures and
components, including teaming understanding, teaming control, and the world, as
well as adhesive transactive part. We further propose several future research
directions to expand on the distinctive contributions of ATSA and address the
specific and pressing next steps.",None,-1
Implementation of a noisy hyperlink removal system: A semantic and relatedness approach,0.446838,"As the volume of data on the web grows, the web structure graph, which is a
graph representation of the web, continues to evolve. The structure of this
graph has gradually shifted from content-based to non-content-based.
Furthermore, spam data, such as noisy hyperlinks, in the web structure graph
adversely affect the speed and efficiency of information retrieval and link
mining algorithms. Previous works in this area have focused on removing noisy
hyperlinks using structural and string approaches. However, these approaches
may incorrectly remove useful links or be unable to detect noisy hyperlinks in
certain circumstances. In this paper, a data collection of hyperlinks is
initially constructed using an interactive crawler. The semantic and
relatedness structure of the hyperlinks is then studied through semantic web
approaches and tools such as the DBpedia ontology. Finally, the removal process
of noisy hyperlinks is carried out using a reasoner on the DBpedia ontology.
Our experiments demonstrate the accuracy and ability of semantic web
technologies to remove noisy hyperlinks",None,-1
Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,0.535989,"Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research.",None,-1
DoNet: Deep De-overlapping Network for Cytology Instance Segmentation,0.514231,"Cell instance segmentation in cytology images has significant importance for
biology analysis and cancer screening, while remains challenging due to 1) the
extensive overlapping translucent cell clusters that cause the ambiguous
boundaries, and 2) the confusion of mimics and debris as nuclei. In this work,
we proposed a De-overlapping Network (DoNet) in a decompose-and-recombined
strategy. A Dual-path Region Segmentation Module (DRM) explicitly decomposes
the cell clusters into intersection and complement regions, followed by a
Semantic Consistency-guided Recombination Module (CRM) for integration. To
further introduce the containment relationship of the nucleus in the cytoplasm,
we design a Mask-guided Region Proposal Strategy (MRP) that integrates the cell
attention maps for inner-cell instance prediction. We validate the proposed
approach on ISBI2014 and CPS datasets. Experiments show that our proposed DoNet
significantly outperforms other state-of-the-art (SOTA) cell instance
segmentation methods. The code is available at
https://github.com/DeepDoNet/DoNet.",None,-1
Fairness for Workers Who Pull the Arms: An Index Based Policy for Allocation of Restless Bandit Tasks,0.649635,"Motivated by applications such as machine repair, project monitoring, and
anti-poaching patrol scheduling, we study intervention planning of stochastic
processes under resource constraints. This planning problem has previously been
modeled as restless multi-armed bandits (RMAB), where each arm is an
intervention-dependent Markov Decision Process. However, the existing
literature assumes all intervention resources belong to a single uniform pool,
limiting their applicability to real-world settings where interventions are
carried out by a set of workers, each with their own costs, budgets, and
intervention effects. In this work, we consider a novel RMAB setting, called
multi-worker restless bandits (MWRMAB) with heterogeneous workers. The goal is
to plan an intervention schedule that maximizes the expected reward while
satisfying budget constraints on each worker as well as fairness in terms of
the load assigned to each worker. Our contributions are two-fold: (1) we
provide a multi-worker extension of the Whittle index to tackle heterogeneous
costs and per-worker budget and (2) we develop an index-based scheduling policy
to achieve fairness. Further, we evaluate our method on various cost structures
and show that our method significantly outperforms other baselines in terms of
fairness without sacrificing much in reward accumulated.",None,-1
Masking and Mixing Adversarial Training,0.0698079,"While convolutional neural networks (CNNs) have achieved excellent
performances in various computer vision tasks, they often misclassify with
malicious samples, a.k.a. adversarial examples. Adversarial training is a
popular and straightforward technique to defend against the threat of
adversarial examples. Unfortunately, CNNs must sacrifice the accuracy of
standard samples to improve robustness against adversarial examples when
adversarial training is used. In this work, we propose Masking and Mixing
Adversarial Training (M2AT) to mitigate the trade-off between accuracy and
robustness. We focus on creating diverse adversarial examples during training.
Specifically, our approach consists of two processes: 1) masking a perturbation
with a binary mask and 2) mixing two partially perturbed images. Experimental
results on CIFAR-10 dataset demonstrate that our method achieves better
robustness against several adversarial attacks than previous methods.",None,-1
GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models,0.503615,"Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",None,-1
Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation,0.443066,"Uncertainty estimation is an important research area to make deep neural
networks (DNNs) more trustworthy. While extensive research on uncertainty
estimation has been conducted with unimodal data, uncertainty estimation for
multimodal data remains a challenge. Neural processes (NPs) have been
demonstrated to be an effective uncertainty estimation method for unimodal data
by providing the reliability of Gaussian processes with efficient and powerful
DNNs. While NPs hold significant potential for multimodal uncertainty
estimation, the adaptation of NPs for multimodal data has not been carefully
studied. To bridge this gap, we propose Multimodal Neural Processes (MNPs) by
generalising NPs for multimodal uncertainty estimation. Based on the framework
of NPs, MNPs consist of several novel and principled mechanisms tailored to the
characteristics of multimodal data. In extensive empirical evaluation, our
method achieves state-of-the-art multimodal uncertainty estimation performance,
showing its appealing robustness against noisy samples and reliability in
out-of-distribution detection with faster computation time compared to the
current state-of-the-art multimodal uncertainty estimation method.",None,-1
HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition,0.445753,"State-of-the-art ASR systems have achieved promising results by modeling
local and global interactions separately. While the former can be computed
efficiently, global interactions are usually modeled via attention mechanisms,
which are expensive for long input sequences. Here, we address this by
extending HyperMixer, an efficient alternative to attention exhibiting linear
complexity, to the Conformer architecture for speech recognition, leading to
HyperConformer. In particular, multi-head HyperConformer achieves comparable or
higher recognition performance while being more efficient than Conformer in
terms of inference speed, memory, parameter count, and available training data.
HyperConformer achieves a word error rate of 2.9% on Librispeech test-clean
with less than 8M neural parameters and a peak memory during training of 5.7GB,
hence trainable with accessible hardware. Encoder speed is between 38% on
mid-length speech and 56% on long speech faster than an equivalent Conformer.
(The HyperConformer recipe is publicly available in:
https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech/ASR/transformer/)",None,-1
Language models are not naysayers: An analysis of language models on negation benchmarks,0.147473,"Negation has been shown to be a major bottleneck for masked language models,
such as BERT. However, whether this finding still holds for larger-sized
auto-regressive language models (``LLMs'') has not been studied
comprehensively. With the ever-increasing volume of research and applications
of LLMs, we take a step back to evaluate the ability of current-generation LLMs
to handle negation, a fundamental linguistic phenomenon that is central to
language understanding. We evaluate different LLMs -- including the open-source
GPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks.
Through systematic experimentation with varying model sizes and prompts, we
show that LLMs have several limitations including insensitivity to the presence
of negation, an inability to capture the lexical semantics of negation, and a
failure to reason under negation.",None,-1
CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning,0.454876,"Commonsense reasoning, aiming at endowing machines with a human-like ability
to make situational presumptions, is extremely challenging to generalize. For
someone who barely knows about ""meditation,"" while is knowledgeable about
""singing,"" he can still infer that ""meditation makes people relaxed"" from the
existing knowledge that ""singing makes people relaxed"" by first conceptualizing
""singing"" as a ""relaxing event"" and then instantiating that event to
""meditation."" This process, known as conceptual induction and deduction, is
fundamental to commonsense reasoning while lacking both labeled data and
methodologies to enhance commonsense modeling. To fill such a research gap, we
propose CAT (Contextualized ConceptuAlization and InsTantiation), a
semi-supervised learning framework that integrates event conceptualization and
instantiation to conceptualize commonsense knowledge bases at scale. Extensive
experiments show that our framework achieves state-of-the-art performances on
two conceptualization tasks, and the acquired abstract commonsense knowledge
can significantly improve commonsense inference modeling. Our code, data, and
fine-tuned models are publicly available at
https://github.com/HKUST-KnowComp/CAT.",None,-1
dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning,0.0661958,"We present dPASP, a novel declarative probabilistic logic programming
framework for differentiable neuro-symbolic reasoning. The framework allows for
the specification of discrete probabilistic models with neural predicates,
logic constraints and interval-valued probabilistic choices, thus supporting
models that combine low-level perception (images, texts, etc), common-sense
reasoning, and (vague) statistical knowledge. To support all such features, we
discuss the several semantics for probabilistic logic programs that can express
nondeterministic, contradictory, incomplete and/or statistical knowledge. We
also discuss how gradient-based learning can be performed with neural
predicates and probabilistic choices under selected semantics. We then describe
an implemented package that supports inference and learning in the language,
along with several example programs. The package requires minimal user
knowledge of deep learning system's inner workings, while allowing end-to-end
training of rather sophisticated models and loss functions.",None,-1
Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),0.809814,"Recently, there has been a remarkable increase in the interest towards
skeleton-based action recognition within the research community, owing to its
various advantageous features, including computational efficiency,
representative features, and illumination invariance. Despite this, researchers
continue to explore and investigate the most optimal way to represent human
actions through skeleton representation and the extracted features. As a
result, the growth and availability of human action recognition datasets have
risen substantially. In addition, deep learning-based algorithms have gained
widespread popularity due to the remarkable advancements in various computer
vision tasks. Most state-of-the-art contributions in skeleton-based action
recognition incorporate a Graph Neural Network (GCN) architecture for
representing the human body and extracting features. Our research demonstrates
that Convolutional Neural Networks (CNNs) can attain comparable results to GCN,
provided that the proper training techniques, augmentations, and optimizers are
applied. Our approach has been rigorously validated, and we have achieved a
score of 95% on the NTU-60 dataset",None,-1
Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning,0.733599,"We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a
novel self-supervised learning framework for learning accurate and dense vision
representation. To extract spatial-sensitive information, ADCLR introduces
query patches for contrasting in addition with global contrasting. Compared
with previous dense contrasting methods, ADCLR mainly enjoys three merits: i)
achieving both global-discriminative and spatial-sensitive representation, ii)
model-efficient (no extra parameters in addition to the global contrasting
baseline), and iii) correspondence-free and thus simpler to implement. Our
approach achieves new state-of-the-art performance for contrastive methods. On
classification tasks, for ViT-S, ADCLR achieves 77.5% top-1 accuracy on
ImageNet with linear probing, outperforming our baseline (DINO) without our
devised techniques as plug-in, by 0.5%. For ViT-B, ADCLR achieves 79.8%, 84.0%
accuracy on ImageNet by linear probing and finetune, outperforming iBOT by
0.3%, 0.2% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant
improvements of 44.3% AP on object detection, 39.7% AP on instance
segmentation, outperforming previous SOTA method SelfPatch by 2.2% and 1.2%,
respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0% mIoU, 1.2% mAcc on
the segme",None,-1
Unsupervised Semantic Correspondence Using Stable Diffusion,0.845484,"Text-to-image diffusion models are now capable of generating images that are
often indistinguishable from real images. To generate such images, these models
must understand the semantics of the objects they are asked to generate. In
this work we show that, without any training, one can leverage this semantic
knowledge within diffusion models to find semantic correspondences - locations
in multiple images that have the same semantic meaning. Specifically, given an
image, we optimize the prompt embeddings of these models for maximum attention
on the regions of interest. These optimized embeddings capture semantic
information about the location, which can then be transferred to another image.
By doing so we obtain results on par with the strongly supervised state of the
art on the PF-Willow dataset and significantly outperform (20.9% relative for
the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow,
CUB-200 and SPair-71k datasets.",None,-1
ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters,0.580708,"We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via
the use of language adapters (LAs). Most of the earlier works have explored
training with adapter of a single source (often English), and testing either
using the target LA or LA of another related language. Training target LA
requires unlabeled data, which may not be readily available for low resource
unseen languages: those that are neither seen by the underlying multilingual
language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data
for them. We posit that for more effective cross-lingual transfer, instead of
just one source LA, we need to leverage LAs of multiple (linguistically or
geographically related) source languages, both at train and test-time - which
we investigate via our novel neural architecture, ZGUL. Extensive
experimentation across four language groups, covering 15 unseen target
languages, demonstrates improvements of up to 3.2 average F1 points over
standard fine-tuning and other strong baselines on POS tagging and NER tasks.
We also extend ZGUL to settings where either (1) some unlabeled data or (2)
few-shot training examples are available for the target language. We find that
ZGUL continues to outperform baselines in these settings too.",None,-1
WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models,0.364572,"The rapid advancement of generative models, facilitating the creation of
hyper-realistic images from textual descriptions, has concurrently escalated
critical societal concerns such as misinformation. Although providing some
mitigation, traditional fingerprinting mechanisms fall short in attributing
responsibility for the malicious use of synthetic images. This paper introduces
a novel approach to model fingerprinting that assigns responsibility for the
generated images, thereby serving as a potential countermeasure to model
misuse. Our method modifies generative models based on each user's unique
digital fingerprint, imprinting a unique identifier onto the resultant content
that can be traced back to the user. This approach, incorporating fine-tuning
into Text-to-Image (T2I) tasks using the Stable Diffusion Model, demonstrates
near-perfect attribution accuracy with a minimal impact on output quality.
Through extensive evaluation, we show that our method outperforms baseline
methods with an average improvement of 11\% in handling image post-processes.
Our method presents a promising and novel avenue for accountable model
distribution and responsible use. Our code is available in
\url{https://github.com/kylemin/WOUAF}.",None,-1
Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data,0.612433,"Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., ""an image of a <category>"", and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.",None,-1
Analogue and Physical Reservoir Computing Using Water Waves,0.652678,"More than 3.5 billion people live in rural areas, where water and water
energy resources play an important role in ensuring sustainable and productive
rural economies. This article reviews and critically analyses the recent
advances in the field of analogue and reservoir computing that have been driven
by unique physical properties and energy of water waves. It also demonstrates
that analogue and reservoir computing hold the potential to bring artificial
intelligence closer to people living outside large cities, thus enabling them
to enjoy the benefits of novel technologies that already work in large cities
but are not readily available and suitable for regional communities.",None,-1
Loop Closure Detection Based on Object-level Spatial Layout and Semantic Consistency,0.334523,"Visual simultaneous localization and mapping (SLAM) systems face challenges
in detecting loop closure under the circumstance of large viewpoint changes. In
this paper, we present an object-based loop closure detection method based on
the spatial layout and semanic consistency of the 3D scene graph. Firstly, we
propose an object-level data association approach based on the semantic
information from semantic labels, intersection over union (IoU), object color,
and object embedding. Subsequently, multi-view bundle adjustment with the
associated objects is utilized to jointly optimize the poses of objects and
cameras. We represent the refined objects as a 3D spatial graph with semantics
and topology. Then, we propose a graph matching approach to select
correspondence objects based on the structure layout and semantic property
similarity of vertices' neighbors. Finally, we jointly optimize camera
trajectories and object poses in an object-level pose graph optimization, which
results in a globally consistent map. Experimental results demonstrate that our
proposed data association approach can construct more accurate 3D semantic
maps, and our loop closure method is more robust than point-based and
object-based methods in circumstances with large viewpoint changes.",None,-1
SCB-dataset: A Dataset for Detecting Student Classroom Behavior,0.622878,"The use of deep learning methods for automatic detection of students'
classroom behavior is a promising approach to analyze their class performance
and enhance teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose a Student Classroom Behavior dataset
(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248
labels and 4,003 images, with a focus on hand-raising behavior. We evaluated
the dataset using the YOLOv7 algorithm, achieving a mean average precision
(map) of up to 85.3%. We believe that our dataset can serve as a robust
foundation for future research in the field of student behavior detection and
promote further advancements in this area.Our SCB-dataset can be downloaded
from: https://github.com/Whiffe/SCB-dataset",None,-1
Knowledge Distillation for Feature Extraction in Underwater VSLAM,0.272063,"In recent years, learning-based feature detection and matching have
outperformed manually-designed methods in in-air cases. However, it is
challenging to learn the features in the underwater scenario due to the absence
of annotated underwater datasets. This paper proposes a cross-modal knowledge
distillation framework for training an underwater feature detection and
matching network (UFEN). In particular, we use in-air RGBD data to generate
synthetic underwater images based on a physical underwater imaging formation
model and employ these as the medium to distil knowledge from a teacher model
SuperPoint pretrained on in-air images. We embed UFEN into the ORB-SLAM3
framework to replace the ORB feature by introducing an additional binarization
layer. To test the effectiveness of our method, we built a new underwater
dataset with groundtruth measurements named EASI
(https://github.com/Jinghe-mel/UFEN-SLAM), recorded in an indoor water tank for
different turbidity levels. The experimental results on the existing dataset
and our new dataset demonstrate the effectiveness of our method.",None,-1
Density-based clustering with fully-convolutional networks for crowd flow detection from drones,0.689559,"Crowd analysis from drones has attracted increasing attention in recent times
due to the ease of use and affordable cost of these devices. However, how this
technology can provide a solution to crowd flow detection is still an
unexplored research question. To this end, we propose a crowd flow detection
method for video sequences shot by a drone. The method is based on a
fully-convolutional network that learns to perform crowd clustering in order to
detect the centroids of crowd-dense areas and track their movement in
consecutive frames. The proposed method proved effective and efficient when
tested on the Crowd Counting datasets of the VisDrone challenge, characterized
by video sequences rather than still images. The encouraging results show that
the proposed method could open up new ways of analyzing high-level crowd
behavior from drones.",None,-1
NovPhy: A Testbed for Physical Reasoning in Open-world Environments,0.0457542,"Due to the emergence of AI systems that interact with the physical
environment, there is an increased interest in incorporating physical reasoning
capabilities into those AI systems. But is it enough to only have physical
reasoning capabilities to operate in a real physical environment? In the real
world, we constantly face novel situations we have not encountered before. As
humans, we are competent at successfully adapting to those situations.
Similarly, an agent needs to have the ability to function under the impact of
novelties in order to properly operate in an open-world physical environment.
To facilitate the development of such AI systems, we propose a new testbed,
NovPhy, that requires an agent to reason about physical scenarios in the
presence of novelties and take actions accordingly. The testbed consists of
tasks that require agents to detect and adapt to novelties in physical
scenarios. To create tasks in the testbed, we develop eight novelties
representing a diverse novelty space and apply them to five commonly
encountered scenarios in a physical environment. According to our testbed
design, we evaluate two capabilities of an agent: the performance on a novelty
when it is applied to different physical scenarios and the performance on a
physical scenario when different novelties are applied to it. We conduct a
thorough evaluation with human players, learning agents, and heuristic agents.
Our evaluation shows that humans' performance is far beyond the agents'
performance. Some agents, even with good normal task performance, perform
significantly worse when there is a novelty, and the agents that can adapt to
novelties typically adapt slower than humans. We promote the development of
intelligent agents capable of performing at the human level or above when
operating in open-world physical environments. Testbed website:
https://github.com/phy-q/novphy",None,-1
"Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models",0.782911,"The introduction of diffusion models in anomaly detection has paved the way
for more effective and accurate image reconstruction in pathologies. However,
the current limitations in controlling noise granularity hinder diffusion
models' ability to generalize across diverse anomaly types and compromise the
restoration of healthy tissues. To overcome these challenges, we propose
AutoDDPM, a novel approach that enhances the robustness of diffusion models.
AutoDDPM utilizes diffusion models to generate initial likelihood maps of
potential anomalies and seamlessly integrates them with the original image.
Through joint noised distribution re-sampling, AutoDDPM achieves harmonization
and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in
replacing anomalous regions while preserving healthy tissues, considerably
surpassing diffusion models' limitations. It also contributes valuable insights
and analysis on the limitations of current diffusion models, promoting robust
and interpretable anomaly detection in medical imaging - an essential aspect of
building autonomous clinical decision systems with higher interpretability.",None,-1
"Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment",0.758037,"Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.",None,-1
Let's reward step by step: Step-Level reward model as the Navigators for Reasoning,0.50468,"Recent years have seen considerable advancements in multi-step reasoning with
Large Language Models (LLMs). The previous studies have elucidated the merits
of integrating feedback or search mechanisms during model inference to improve
the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically
furnishes LLMs with step-by-step feedback during the training phase, akin to
Proximal Policy Optimization (PPO) or reject sampling. Our objective is to
examine the efficacy of PRM in the inference phase to help discern the optimal
solution paths for multi-step tasks such as mathematical reasoning and code
generation. To this end, we propose a heuristic greedy search algorithm that
employs the step-level feedback from PRM to optimize the reasoning pathways
explored by LLMs. This tailored PRM demonstrated enhanced results compared to
the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH.
Additionally, to explore the versatility of our approach, we develop a novel
method to automatically generate step-level reward dataset for coding tasks and
observed similar improved performance in the code generation tasks. Thus
highlighting the robust nature of our reward-model-based approach to inference
for reasoning tasks.",None,-1
4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees,0.270654,"We introduce an encoding for parsing as sequence labeling that can represent
any projective dependency tree as a sequence of 4-bit labels, one per word. The
bits in each word's label represent (1) whether it is a right or left
dependent, (2) whether it is the outermost (left/right) dependent of its
parent, (3) whether it has any left children and (4) whether it has any right
children. We show that this provides an injective mapping from trees to labels
that can be encoded and decoded in linear time. We then define a 7-bit
extension that represents an extra plane of arcs, extending the coverage to
almost full non-projectivity (over 99.9% empirical arc coverage). Results on a
set of diverse treebanks show that our 7-bit encoding obtains substantial
accuracy gains over the previously best-performing sequence labeling encodings.",None,-1
Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks,0.703159,"In this study, we investigated whether self-supervised pretraining could
produce a neural network feature extractor applicable to multiple
classification tasks in B-mode lung ultrasound analysis. When fine-tuning on
three lung ultrasound tasks, pretrained models resulted in an improvement of
the average across-task area under the receiver operating curve (AUC) by 0.032
and 0.061 on local and external test sets respectively. Compact nonlinear
classifiers trained on features outputted by a single pretrained model did not
improve performance across all tasks; however, they did reduce inference time
by 49% compared to serial execution of separate fine-tuned models. When
training using 1% of the available labels, pretrained models consistently
outperformed fully supervised models, with a maximum observed test AUC increase
of 0.396 for the task of view classification. Overall, the results indicate
that self-supervised pretraining is useful for producing initial weights for
lung ultrasound classifiers.",None,-1
"Trustworthy, responsible, ethical AI in manufacturing and supply chains: synthesis and emerging research questions",0.29026,"While the increased use of AI in the manufacturing sector has been widely
noted, there is little understanding on the risks that it may raise in a
manufacturing organisation. Although various high level frameworks and
definitions have been proposed to consolidate potential risks, practitioners
struggle with understanding and implementing them.
  This lack of understanding exposes manufacturing to a multitude of risks,
including the organisation, its workers, as well as suppliers and clients. In
this paper, we explore and interpret the applicability of responsible, ethical,
and trustworthy AI within the context of manufacturing. We then use a broadened
adaptation of a machine learning lifecycle to discuss, through the use of
illustrative examples, how each step may result in a given AI trustworthiness
concern. We additionally propose a number of research questions to the
manufacturing research community, in order to help guide future research so
that the economic and societal benefits envisaged by AI in manufacturing are
delivered safely and responsibly.",None,-1
Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters,0.79919,"Identifying logical errors in complex, incomplete or even contradictory and
overall heterogeneous data like students' experimentation protocols is
challenging. Recognizing the limitations of current evaluation methods, we
investigate the potential of Large Language Models (LLMs) for automatically
identifying student errors and streamlining teacher assessments. Our aim is to
provide a foundation for productive, personalized feedback. Using a dataset of
65 student protocols, an Artificial Intelligence (AI) system based on the
GPT-3.5 and GPT-4 series was developed and tested against human raters. Our
results indicate varying levels of accuracy in error detection between the AI
system and human raters. The AI system can accurately identify many fundamental
student errors, for instance, the AI system identifies when a student is
focusing the hypothesis not on the dependent variable but solely on an expected
observation (acc. = 0.90), when a student modifies the trials in an ongoing
investigation (acc. = 1), and whether a student is conducting valid test trials
(acc. = 0.82) reliably. The identification of other, usually more complex
errors, like whether a student conducts a valid control trial (acc. = .60),
poses a greater challenge. This research explores not only the utility of AI in
educational settings, but also contributes to the understanding of the
capabilities of LLMs in error detection in inquiry-based learning like
experimentation.",None,-1
THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech,0.598444,"Detecting harmful content on social media, such as Twitter, is made difficult
by the fact that the seemingly simple yes/no classification conceals a
significant amount of complexity. Unfortunately, while several datasets have
been collected for training classifiers in hate and offensive speech, there is
a scarcity of datasets labeled with a finer granularity of target classes and
specific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets
manually labeled with fine-grained annotations about the target of the message.
We demonstrate that this dataset makes it feasible to train classifiers, based
on Large Language Models, to perform classification at this level of
granularity.",None,-1
RecycleGPT: An Autoregressive Language Model with Recyclable Module,0.0778871,"Existing large language models have to run K times to generate a sequence of
K tokens. In this paper, we present RecycleGPT, a generative language model
with fast decoding speed by recycling pre-generated model states without
running the whole model in multiple steps. Our approach relies on the
observation that adjacent tokens in a sequence usually have strong correlations
and the next token in a sequence can be reasonably guessed or inferred based on
the preceding ones. Experiments and analysis demonstrate the effectiveness of
our approach in lowering inference latency, achieving up to 1.4x speedup while
preserving high performance.",None,-1
BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis,0.0916664,"Recently, diffusion-based deep generative models (e.g., Stable Diffusion)
have shown impressive results in text-to-image synthesis. However, current
text-to-image models often require multiple passes of prompt engineering by
humans in order to produce satisfactory results for real-world applications. We
propose BeautifulPrompt, a deep generative model to produce high-quality
prompts from very simple raw descriptions, which enables diffusion-based models
to generate more beautiful images. In our work, we first fine-tuned the
BeautifulPrompt model over low-quality and high-quality collecting prompt
pairs. Then, to ensure that our generated prompts can generate more beautiful
images, we further propose a Reinforcement Learning with Visual AI Feedback
technique to fine-tune our model to maximize the reward values of the generated
prompts, where the reward values are calculated based on the PickScore and the
Aesthetic Scores. Our results demonstrate that learning from visual AI feedback
promises the potential to improve the quality of generated prompts and images
significantly. We further showcase the integration of BeautifulPrompt to a
cloud-native AI platform to provide better text-to-image generation service in
the cloud.",None,-1
Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware Sound Separation,0.867137,"The audio-visual sound separation field assumes visible sources in videos,
but this excludes invisible sounds beyond the camera's view. Current methods
struggle with such sounds lacking visible cues. This paper introduces a novel
""Audio-Visual Scene-Aware Separation"" (AVSA-Sep) framework. It includes a
semantic parser for visible and invisible sounds and a separator for
scene-informed separation. AVSA-Sep successfully separates both sound types,
with joint training and cross-modal alignment enhancing effectiveness.",None,-1
Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT,0.722989,"The abundance of information on social media has increased the necessity of
accurate real-time rumour detection. Manual techniques of identifying and
verifying fake news generated by AI tools are impracticable and time-consuming
given the enormous volume of information generated every day. This has sparked
an increase in interest in creating automated systems to find fake news on the
Internet. The studies in this research demonstrate that the BERT and RobertA
models with fine-tuning had the best success in detecting AI generated news.
With a score of 98%, tweaked RobertA in particular showed excellent precision.
In conclusion, this study has shown that neural networks can be used to
identify bogus news AI generation news created by ChatGPT. The RobertA and BERT
models' excellent performance indicates that these models can play a critical
role in the fight against misinformation.",None,-1
Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy,0.381381,"Learning meaningful protein representation is important for a variety of
biological downstream tasks such as structure-based drug design. Having
witnessed the success of protein sequence pretraining, pretraining for
structural data which is more informative has become a promising research
topic. However, there are three major challenges facing protein structure
pretraining: insufficient sample diversity, physically unrealistic modeling,
and the lack of protein-specific pretext tasks. To try to address these
challenges, we present the 3D Geometric Pretraining. In this paper, we propose
a unified framework for protein pretraining and a 3D geometric-based,
data-efficient, and protein-specific pretext task: RefineDiff (Refine the
Diffused Protein Structure Decoy). After pretraining our geometric-aware model
with this task on limited data(less than 1% of SOTA models), we obtained
informative protein representations that can achieve comparable performance for
various downstream tasks.",None,-1
Applying Large Language Models for Causal Structure Learning in Non Small Cell Lung Cancer,0.656434,"Causal discovery is becoming a key part in medical AI research. These methods
can enhance healthcare by identifying causal links between biomarkers,
demographics, treatments and outcomes. They can aid medical professionals in
choosing more impactful treatments and strategies. In parallel, Large Language
Models (LLMs) have shown great potential in identifying patterns and generating
insights from text data. In this paper we investigate applying LLMs to the
problem of determining the directionality of edges in causal discovery.
Specifically, we test our approach on a deidentified set of Non Small Cell Lung
Cancer(NSCLC) patients that have both electronic health record and genomic
panel data. Graphs are validated using Bayesian Dirichlet estimators using
tabular data. Our result shows that LLMs can accurately predict the
directionality of edges in causal graphs, outperforming existing
state-of-the-art methods. These findings suggests that LLMs can play a
significant role in advancing causal discovery and help us better understand
complex systems.",None,-1
MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning,0.719917,"This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
Detection. We used a multi-label contrastive loss for fine-tuning large
pre-trained language models in a multi-lingual setting, achieving very
competitive results: our system was ranked first on the official test set and
on the official shared task leaderboard for five of the six languages for which
we had training data and for which we could perform fine-tuning. Here, we
describe our experimental setup, as well as various ablation studies. The code
of our system is available at https://github.com/QishengL/SemEval2023",None,-1
Relighting Neural Radiance Fields with Shadow and Highlight Hints,0.839207,"This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.",None,-1
ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes,0.99998,"We present ScanNet++, a large-scale dataset that couples together capture of
high-quality and commodity-level geometry and color of indoor scenes. Each
scene is captured with a high-end laser scanner at sub-millimeter resolution,
along with registered 33-megapixel images from a DSLR camera, and RGB-D streams
from an iPhone. Scene reconstructions are further annotated with an open
vocabulary of semantics, with label-ambiguous scenarios explicitly annotated
for comprehensive semantic understanding. ScanNet++ enables a new real-world
benchmark for novel view synthesis, both from high-quality RGB capture, and
importantly also from commodity-level images, in addition to a new benchmark
for 3D semantic scene understanding that comprehensively encapsulates diverse
and ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460
scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.",None,-1
"Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework for Domain Adaptive Semantic Segmentation",0.563396,"To overcome the domain gap between synthetic and real-world datasets,
unsupervised domain adaptation methods have been proposed for semantic
segmentation. Majority of the previous approaches have attempted to reduce the
gap either at the pixel or feature level, disregarding the fact that the two
components interact positively. To address this, we present CONtrastive FEaTure
and pIxel alignment (CONFETI) for bridging the domain gap at both the pixel and
feature levels using a unique contrastive formulation. We introduce
well-estimated prototypes by including category-wise cross-domain information
to link the two alignments: the pixel-level alignment is achieved using the
jointly trained style transfer module with the prototypical semantic
consistency, while the feature-level alignment is enforced to cross-domain
features with the \textbf{pixel-to-prototype contrast}. Our extensive
experiments demonstrate that our method outperforms existing state-of-the-art
methods using DeepLabV2. Our code is available at
https://github.com/cxa9264/CONFETI",None,-1
Memory augment is All You Need for image restoration,0.163482,"Image restoration is a low-level vision task, most CNN methods are designed
as a black box, lacking transparency and internal aesthetics. Although some
methods combining traditional optimization algorithms with DNNs have been
proposed, they all have some limitations. In this paper, we propose a
three-granularity memory layer and contrast learning named MemoryNet,
specifically, dividing the samples into positive, negative, and actual three
samples for contrastive learning, where the memory layer is able to preserve
the deep features of the image and the contrastive learning converges the
learned features to balance. Experiments on Derain/Deshadow/Deblur task
demonstrate that these methods are effective in improving restoration
performance. In addition, this paper's model obtains significant PSNR, SSIM
gain on three datasets with different degradation types, which is a strong
proof that the recovered images are perceptually realistic. The source code of
MemoryNet can be obtained from https://github.com/zhangbaijin/MemoryNet",None,-1
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,0.497749,"Cross-lingual semantic parsing transfers parsing capability from a
high-resource language (e.g., English) to low-resource languages with scarce
training data. Previous work has primarily considered silver-standard data
augmentation or zero-shot methods, however, exploiting few-shot gold data is
comparatively unexplored. We propose a new approach to cross-lingual semantic
parsing by explicitly minimizing cross-lingual divergence between probabilistic
latent variables using Optimal Transport. We demonstrate how this direct
guidance improves parsing from natural languages using fewer examples and less
training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL,
establishing state-of-the-art results under a few-shot cross-lingual regime.
Ablation studies further reveal that our method improves performance even
without parallel input translations. In addition, we show that our model better
captures cross-lingual structure in the latent space to improve semantic
representation similarity.",None,-1
TherapyView: Visualizing Therapy Sessions with Temporal Topic Modeling and AI-Generated Arts,0.76477,"We present the TherapyView, a demonstration system to help therapists
visualize the dynamic contents of past treatment sessions, enabled by the
state-of-the-art neural topic modeling techniques to analyze the topical
tendencies of various psychiatric conditions and deep learning-based image
generation engine to provide a visual summary. The system incorporates temporal
modeling to provide a time-series representation of topic similarities at a
turn-level resolution and AI-generated artworks given the dialogue segments to
provide a concise representations of the contents covered in the session,
offering interpretable insights for therapists to optimize their strategies and
enhance the effectiveness of psychotherapy. This system provides a proof of
concept of AI-augmented therapy tools with e in-depth understanding of the
patient's mental state and enabling more effective treatment.",None,-1
Automatic Readability Assessment for Closely Related Languages,0.906605,"In recent years, the main focus of research on automatic readability
assessment (ARA) has shifted towards using expensive deep learning-based
methods with the primary goal of increasing models' accuracy. This, however, is
rarely applicable for low-resource languages where traditional handcrafted
features are still widely used due to the lack of existing NLP tools to extract
deeper linguistic representations. In this work, we take a step back from the
technical component and focus on how linguistic aspects such as mutual
intelligibility or degree of language relatedness can improve ARA in a
low-resource setting. We collect short stories written in three languages in
the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment
models and explore the interaction of data and features in various
cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel
specialized feature exploiting n-gram overlap applied to languages with high
mutual intelligibility, significantly improves the performance of ARA models
compared to the use of off-the-shelf large multilingual language models alone.
Consequently, when both linguistic representations are combined, we achieve
state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA
in Bikol.",None,-1
"Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection",0.866978,"Fake news detection has been a critical task for maintaining the health of
the online news ecosystem. However, very few existing works consider the
temporal shift issue caused by the rapidly-evolving nature of news data in
practice, resulting in significant performance degradation when training on
past data and testing on future data. In this paper, we observe that the
appearances of news events on the same topic may display discernible patterns
over time, and posit that such patterns can assist in selecting training
instances that could make the model adapt better to future data. Specifically,
we design an effective framework FTT (Forecasting Temporal Trends), which could
forecast the temporal distribution patterns of news data and then guide the
detector to fast adapt to future distribution. Experiments on the real-world
temporally split dataset demonstrate the superiority of our proposed framework.
The code is available at https://github.com/ICTMCG/FTT-ACL23.",None,-1
SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models,0.83607,"Interpreting remote sensing imagery enables numerous downstream applications
ranging from land-use planning to deforestation monitoring. Robustly
classifying this data is challenging due to the Earth's geographic diversity.
While many distinct satellite and aerial image classification datasets exist,
there is yet to be a benchmark curated that suitably covers this diversity. In
this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from
27 existing remotely sensed datasets, and comprehensively evaluate the
zero-shot transfer classification capabilities of a broad range of
vision-language (VL) models on SATIN. We find SATIN to be a challenging
benchmark-the strongest method we evaluate achieves a classification accuracy
of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public
leaderboard}}$ to guide and track the progress of VL models in this important
domain.",None,-1
AFPN: Asymptotic Feature Pyramid Network for Object Detection,0.829378,"Multi-scale features are of great importance in encoding objects with scale
variance in object detection tasks. A common strategy for multi-scale feature
extraction is adopting the classic top-down and bottom-up feature pyramid
networks. However, these approaches suffer from the loss or degradation of
feature information, impairing the fusion effect of non-adjacent levels. This
paper proposes an asymptotic feature pyramid network (AFPN) to support direct
interaction at non-adjacent levels. AFPN is initiated by fusing two adjacent
low-level features and asymptotically incorporates higher-level features into
the fusion process. In this way, the larger semantic gap between non-adjacent
levels can be avoided. Given the potential for multi-object information
conflicts to arise during feature fusion at each spatial location, adaptive
spatial fusion operation is further utilized to mitigate these inconsistencies.
We incorporate the proposed AFPN into both two-stage and one-stage object
detection frameworks and evaluate with the MS-COCO 2017 validation and test
datasets. Experimental evaluation shows that our method achieves more
competitive results than other state-of-the-art feature pyramid networks. The
code is available at
\href{https://github.com/gyyang23/AFPN}{https://github.com/gyyang23/AFPN}.",None,-1
Rig Inversion by Training a Differentiable Rig Function,0.0995843,"Rig inversion is the problem of creating a method that can find the rig
parameter vector that best approximates a given input mesh. In this paper we
propose to solve this problem by first obtaining a differentiable rig function
by training a multi layer perceptron to approximate the rig function. This
differentiable rig function can then be used to train a deep learning model of
rig inversion.",None,-1
GREC: Generalized Referring Expression Comprehension,0.481225,"The objective of Classic Referring Expression Comprehension (REC) is to
produce a bounding box corresponding to the object mentioned in a given textual
description. Commonly, existing datasets and techniques in classic REC are
tailored for expressions that pertain to a single target, meaning a sole
expression is linked to one specific object. Expressions that refer to multiple
targets or involve no specific target have not been taken into account. This
constraint hinders the practical applicability of REC. This study introduces a
new benchmark termed as Generalized Referring Expression Comprehension (GREC).
This benchmark extends the classic REC by permitting expressions to describe
any number of target objects. To achieve this goal, we have built the first
large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of
expressions: those referring to multiple targets, expressions with no specific
target, and the single-target expressions. The design of GREC and gRefCOCO
ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a
GREC method implementation code, and GREC evaluation code are available at
https://github.com/henghuiding/gRefCOCO.",None,-1
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,0.999393,"There is a growing interest in applying pre-trained large language models
(LLMs) to planning problems. However, methods that use LLMs directly as
planners are currently impractical due to several factors, including limited
correctness of plans, strong reliance on feedback from interactions with
simulators or even the actual environment, and the inefficiency in utilizing
human feedback. In this work, we introduce a novel alternative paradigm that
constructs an explicit world (domain) model in planning domain definition
language (PDDL) and then uses it to plan with sound domain-independent
planners. To address the fact that LLMs may not generate a fully functional
PDDL model initially, we employ LLMs as an interface between PDDL and sources
of corrective feedback, such as PDDL validators and humans. For users who lack
a background in PDDL, we show that LLMs can translate PDDL into natural
language and effectively encode corrective feedback back to the underlying
domain model. Our framework not only enjoys the correctness guarantee offered
by the external planners but also reduces human involvement by allowing users
to correct domain models at the beginning, rather than inspecting and
correcting (through interactive prompting) every generated plan as in previous
work. On two IPC domains and a Household domain that is more complicated than
commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be
leveraged to produce high-quality PDDL models for over 40 actions, and the
corrected PDDL models are then used to successfully solve 48 challenging
planning tasks. Resources, including the source code, are released at:
https://guansuns.github.io/pages/llm-dm.",None,-1
Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following,0.344509,"In this paper, we present our finding that prepending a Task-Agnostic Prefix
Prompt (TAPP) to the input improves the instruction-following ability of
various Large Language Models (LLMs) during inference. TAPP is different from
canonical prompts for LLMs in that it is a fixed prompt prepended to the
beginning of every input regardless of the target task for zero-shot
generalization. We observe that both base LLMs (i.e. not fine-tuned to follow
instructions) and instruction-tuned models benefit from TAPP, resulting in
34.58% and 12.26% improvement on average, respectively. This implies that the
instruction-following ability of LLMs can be improved during inference time
with a fixed prompt constructed with simple heuristics. We hypothesize that
TAPP assists language models to better estimate the output distribution by
focusing more on the instruction of the target task during inference. In other
words, such ability does not seem to be sufficiently activated in not only base
LLMs but also many instruction-fine-tuned LLMs. All experiments are
reproducible from https://github.com/seonghyeonye/TAPP.",None,-1
Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles,0.866629,"Modern hierarchical vision transformers have added several vision-specific
components in the pursuit of supervised classification performance. While these
components lead to effective accuracies and attractive FLOP counts, the added
complexity actually makes these transformers slower than their vanilla ViT
counterparts. In this paper, we argue that this additional bulk is unnecessary.
By pretraining with a strong visual pretext task (MAE), we can strip out all
the bells-and-whistles from a state-of-the-art multi-stage vision transformer
without losing accuracy. In the process, we create Hiera, an extremely simple
hierarchical vision transformer that is more accurate than previous models
while being significantly faster both at inference and during training. We
evaluate Hiera on a variety of tasks for image and video recognition. Our code
and models are available at https://github.com/facebookresearch/hiera.",None,-1
Japanese SimCSE Technical Report,0.109091,"We report the development of Japanese SimCSE, Japanese sentence embedding
models fine-tuned with SimCSE. Since there is a lack of sentence embedding
models for Japanese that can be used as a baseline in sentence embedding
research, we conducted extensive experiments on Japanese sentence embeddings
involving 24 pre-trained Japanese or multilingual language models, five
supervised datasets, and four unsupervised datasets. In this report, we provide
the detailed training setup for Japanese SimCSE and their evaluation results.",None,-1
Uncertainty-Aware AB3DMOT by Variational 3D Object Detection,0.112084,"Autonomous driving needs to rely on high-quality 3D object detection to
ensure safe navigation in the world. Uncertainty estimation is an effective
tool to provide statistically accurate predictions, while the associated
detection uncertainty can be used to implement a more safe navigation protocol
or include the user in the loop. In this paper, we propose a Variational Neural
Network-based TANet 3D object detector to generate 3D object detections with
uncertainty and introduce these detections to an uncertainty-aware AB3DMOT
tracker. This is done by applying a linear transformation to the estimated
uncertainty matrix, which is subsequently used as a measurement noise for the
adopted Kalman filter. We implement two ways to estimate output uncertainty,
i.e., internally, by computing the variance of the CNN outputs and then
propagating the uncertainty through the post-processing, and externally, by
associating the final predictions of different samples and computing the
covariance of each predicted box. In experiments, we show that the external
uncertainty estimation leads to better results, outperforming both internal
uncertainty estimation and classical tracking approaches. Furthermore, we
propose a method to initialize the Variational 3D object detector with a
pretrained TANet model, which leads to the best performing models.",None,-1
An Experiment in Retrofitting Competency Questions for Existing Ontologies,0.612317,"Competency Questions (CQs) are a form of ontology functional requirements
expressed as natural language questions. Inspecting CQs together with the
axioms in an ontology provides critical insights into the intended scope and
applicability of the ontology. CQs also underpin a number of tasks in the
development of ontologies e.g. ontology reuse, ontology testing, requirement
specification, and the definition of patterns that implement such requirements.
Although CQs are integral to the majority of ontology engineering
methodologies, the practice of publishing CQs alongside the ontological
artefacts is not widely observed by the community. In this context, we present
an experiment in retrofitting CQs from existing ontologies. We propose
RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using
Generative AI. In the paper we present the pipeline that facilitates the
extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its
application to a number of existing ontologies.",None,-1
DReg-NeRF: Deep Registration for Neural Radiance Fields,0.436607,"Although Neural Radiance Fields (NeRF) is popular in the computer vision
community recently, registering multiple NeRFs has yet to gain much attention.
Unlike the existing work, NeRF2NeRF, which is based on traditional optimization
methods and needs human annotated keypoints, we propose DReg-NeRF to solve the
NeRF registration problem on object-centric scenes without human intervention.
After training NeRF models, our DReg-NeRF first extracts features from the
occupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformer
architecture with self-attention and cross-attention layers to learn the
relations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)
point cloud registration methods, the decoupled correspondences are supervised
by surface fields without any ground truth overlapping labels. We construct a
novel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse to
train our network. When evaluated on the test set, our proposed method beats
the SOTA point cloud registration methods by a large margin, with a mean
$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.
  Our code is available at https://github.com/AIBluefisher/DReg-NeRF.",None,-1
Prompt Engineering a Prompt Engineer,0.637058,"Prompt engineering is a challenging yet crucial task for optimizing the
performance of large language models on customized tasks. It requires complex
reasoning to examine the model's errors, hypothesize what is missing or
misleading in the current prompt, and communicate the task with clarity. While
recent works indicate that large language models can be meta-prompted to
perform automatic prompt engineering, we argue that their potential is limited
due to insufficient guidance for complex reasoning in the meta-prompt. We fill
this gap by infusing into the meta-prompt three key components: detailed
descriptions, context specification, and a step-by-step reasoning template. The
resulting method, named PE2, showcases remarkable versatility across diverse
language tasks. It finds prompts that outperform ""let's think step by step"" by
6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on
counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted
prompt edits, rectify erroneous prompts, and induce multi-step plans for
complex tasks.",None,-1
Industrial Memories: Exploring the Findings of Government Inquiries with Neural Word Embedding and Machine Learning,0.039059,"We present a text mining system to support the exploration of large volumes
of text detailing the findings of government inquiries. Despite their
historical significance and potential societal impact, key findings of
inquiries are often hidden within lengthy documents and remain inaccessible to
the general public. We transform the findings of the Irish government's inquiry
into industrial schools and through the use of word embedding, text
classification and visualisation, present an interactive web-based platform
that enables the exploration of the text to uncover new historical insights.",None,-1
Behind the Scenes: Density Fields for Single View Reconstruction,0.79327,"Inferring a meaningful geometric scene representation from a single image is
a fundamental problem in computer vision. Approaches based on traditional depth
map prediction can only reason about areas that are visible in the image.
Currently, neural radiance fields (NeRFs) can capture true 3D including color,
but are too complex to be generated from a single image. As an alternative, we
propose to predict implicit density fields. A density field maps every location
in the frustum of the input image to volumetric density. By directly sampling
color from the available views instead of storing color in the density field,
our scene representation becomes significantly less complex compared to NeRFs,
and a neural network can predict it in a single forward pass. The prediction
network is trained through self-supervision from only video data. Our
formulation allows volume rendering to perform both depth prediction and novel
view synthesis. Through experiments, we show that our method is able to predict
meaningful geometry for regions that are occluded in the input image.
Additionally, we demonstrate the potential of our approach on three datasets
for depth prediction and novel-view synthesis.",None,-1
Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization,0.65134,"Multi-document summarization (MDS) aims to generate a summary for a number of
related documents. We propose HGSUM, an MDS model that extends an
encoder-decoder architecture, to incorporate a heterogeneous graph to represent
different semantic units (e.g., words and sentences) of the documents. This
contrasts with existing MDS models which do not consider different edge types
of graphs and as such do not capture the diversity of relationships in the
documents. To preserve only key information and relationships of the documents
in the heterogeneous graph, HGSUM uses graph pooling to compress the input
graph. And to guide HGSUM to learn compression, we introduce an additional
objective that maximizes the similarity between the compressed graph and the
graph constructed from the ground-truth summary during training. HGSUM is
trained end-to-end with graph similarity and standard cross-entropy objectives.
Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM
outperforms state-of-the-art MDS models. The code for our model and experiments
is available at: https://github.com/oaimli/HGSum.",None,-1
Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations,0.999941,"In this paper, we present an innovative process-oriented math process reward
model called \textbf{Math-Shepherd}, which assigns a reward score to each step
of math problem solutions. The training of Math-Shepherd is achieved using
automatically constructed process-wise supervision data, breaking the
bottleneck of heavy reliance on manual annotation in existing work. We explore
the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}:
Math-Shepherd is utilized for reranking multiple outputs generated by Large
Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is
employed to reinforce LLMs with step-by-step Proximal Policy Optimization
(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates
exceptional performance. For instance, the step-by-step PPO with Math-Shepherd
significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K
and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\%
and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd,
respectively. We believe that automatic process supervision holds significant
potential for the future evolution of LLMs.",None,-1
Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures,0.841617,"Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.",None,-1
STRONG -- Structure Controllable Legal Opinion Summary Generation,0.111323,"We propose an approach for the structure controllable summarization of long
legal opinions that considers the argument structure of the document. Our
approach involves using predicted argument role information to guide the model
in generating coherent summaries that follow a provided structure pattern. We
demonstrate the effectiveness of our approach on a dataset of legal opinions
and show that it outperforms several strong baselines with respect to ROUGE,
BERTScore, and structure similarity.",None,-1
NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis,0.605777,"This paper describes our system developed for the SemEval-2023 Task 12
""Sentiment Analysis for Low-resource African Languages using Twitter Dataset"".
Sentiment analysis is one of the most widely studied applications in natural
language processing. However, most prior work still focuses on a small number
of high-resource languages. Building reliable sentiment analysis systems for
low-resource languages remains challenging, due to the limited training data in
this task. In this work, we propose to leverage language-adaptive and
task-adaptive pretraining on African texts and study transfer learning with
source language selection on top of an African language-centric pretrained
language model. Our key findings are: (1) Adapting the pretrained model to the
target language and task using a small yet relevant corpus improves performance
remarkably by more than 10 F1 score points. (2) Selecting source languages with
positive transfer gains during training can avoid harmful interference from
dissimilar languages, leading to better results in multilingual and
cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
and, in particular, performs best in the multilingual evaluation.",None,-1
Towards a Holodeck-style Simulation Game,0.434794,"We introduce Infinitia, a simulation game system that uses generative image
and language models at play time to reshape all aspects of the setting and NPCs
based on a short description from the player, in a way similar to how settings
are created on the fictional Holodeck. Building off the ideas of the Generative
Agents paper, our system introduces gameplay elements, such as infinite
generated fantasy worlds, controllability of NPC behavior, humorous dialogue,
cost & time efficiency, collaboration between players and elements of
non-determinism among in-game events. Infinitia is implemented in the Unity
engine with a server-client architecture, facilitating the addition of exciting
features by community developers in the future. Furthermore, it uses a
multiplayer framework to allow humans to be present and interact in the
simulation. The simulation will be available in open-alpha shortly at
https://infinitia.ai/ and we are looking forward to building upon it with the
community.",None,-1
Segment Anything,1.0,"We introduce the Segment Anything (SA) project: a new task, model, and
dataset for image segmentation. Using our efficient model in a data collection
loop, we built the largest segmentation dataset to date (by far), with over 1
billion masks on 11M licensed and privacy respecting images. The model is
designed and trained to be promptable, so it can transfer zero-shot to new
image distributions and tasks. We evaluate its capabilities on numerous tasks
and find that its zero-shot performance is impressive -- often competitive with
or even superior to prior fully supervised results. We are releasing the
Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and
11M images at https://segment-anything.com to foster research into foundation
models for computer vision.",None,-1
Distractor generation for multiple-choice questions with predictive prompting and large language models,0.328664,"Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable
performance across various tasks and have garnered significant attention from
both researchers and practitioners. However, in an educational context, we
still observe a performance gap in generating distractors -- i.e., plausible
yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In
this study, we propose a strategy for guiding LLMs such as ChatGPT, in
generating relevant distractors by prompting them with question items
automatically retrieved from a question bank as well-chosen in-context
examples. We evaluate our LLM-based solutions using a quantitative assessment
on an existing test set, as well as through quality annotations by human
experts, i.e., teachers. We found that on average 53% of the generated
distractors presented to the teachers were rated as high-quality, i.e.,
suitable for immediate use as is, outperforming the state-of-the-art model. We
also show the gains of our approach 1 in generating high-quality distractors by
comparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with
static examples.",None,-1
Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models,0.893902,"We present Text2Room, a method for generating room-scale textured 3D meshes
from a given text prompt as input. To this end, we leverage pre-trained 2D
text-to-image models to synthesize a sequence of images from different poses.
In order to lift these outputs into a consistent 3D scene representation, we
combine monocular depth estimation with a text-conditioned inpainting model.
The core idea of our approach is a tailored viewpoint selection such that the
content of each image can be fused into a seamless, textured 3D mesh. More
specifically, we propose a continuous alignment strategy that iteratively fuses
scene frames with the existing geometry to create a seamless mesh. Unlike
existing works that focus on generating single objects or zoom-out trajectories
from text, our method generates complete 3D scenes with multiple objects and
explicit 3D geometry. We evaluate our approach using qualitative and
quantitative metrics, demonstrating it as the first method to generate
room-scale 3D geometry with compelling textures from only text as input.",None,-1
"Knowledge is Power, Understanding is Impact: Utility and Beyond Goals, Explanation Quality, and Fairness in Path Reasoning Recommendation",0.415414,"Path reasoning is a notable recommendation approach that models high-order
user-product relations, based on a Knowledge Graph (KG). This approach can
extract reasoning paths between recommended products and already experienced
products and, then, turn such paths into textual explanations for the user.
Unfortunately, evaluation protocols in this field appear heterogeneous and
limited, making it hard to contextualize the impact of the existing methods. In
this paper, we replicated three state-of-the-art relevant path reasoning
recommendation methods proposed in top-tier conferences. Under a common
evaluation protocol, based on two public data sets and in comparison with other
knowledge-aware methods, we then studied the extent to which they meet
recommendation utility and beyond objectives, explanation quality, and consumer
and provider fairness. Our study provides a picture of the progress in this
field, highlighting open issues and future directions. Source code:
\url{https://github.com/giacoballoccu/rep-path-reasoning-recsys}.",None,-1
Generalized Semantic Segmentation by Self-Supervised Source Domain Projection and Multi-Level Contrastive Learning,0.185474,"Deep networks trained on the source domain show degraded performance when
tested on unseen target domain data. To enhance the model's generalization
ability, most existing domain generalization methods learn domain invariant
features by suppressing domain sensitive features. Different from them, we
propose a Domain Projection and Contrastive Learning (DPCL) approach for
generalized semantic segmentation, which includes two modules: Self-supervised
Source Domain Projection (SSDP) and Multi-level Contrastive Learning (MLCL).
SSDP aims to reduce domain gap by projecting data to the source domain, while
MLCL is a learning scheme to learn discriminative and generalizable features on
the projected data. During test time, we first project the target data by SSDP
to mitigate domain shift, then generate the segmentation results by the learned
segmentation network based on MLCL. At test time, we can update the projected
data by minimizing our proposed pixel-to-pixel contrastive loss to obtain
better results. Extensive experiments for semantic segmentation demonstrate the
favorable generalization capability of our method on benchmark datasets.",None,-1
Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT,0.320401,"The rapid proliferation of ChatGPT has incited debates regarding its impact
on human writing. Amid concerns about declining writing standards, this study
investigates the role of ChatGPT in facilitating academic writing, especially
among language learners. Using a case study approach, this study examines the
experiences of Kailing, a doctoral student, who integrates ChatGPT throughout
their academic writing process. The study employs activity theory as a lens for
understanding writing with generative AI tools and data analyzed includes
semi-structured interviews, writing samples, and GPT logs. Results indicate
that Kailing effectively collaborates with ChatGPT across various writing
stages while preserving her distinct authorial voice and agency. This
underscores the potential of AI tools such as ChatGPT to enhance academic
writing for language learners without overshadowing individual authenticity.
This case study offers a critical exploration of how ChatGPT is utilized in the
academic writing process and the preservation of a student's authentic voice
when engaging with the tool.",None,-1
You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis,0.317934,"Expressing attitude or stance toward entities and concepts is an integral
part of human behavior and personality. Recently, evaluative language data has
become more accessible with social media's rapid growth, enabling large-scale
opinion analysis. However, surprisingly little research examines the
relationship between personality and evaluative language. To bridge this gap,
we introduce the notion of evaluative topics, obtained by applying topic models
to pre-filtered evaluative text from social media. We then link evaluative
topics to individual text authors to build their evaluative profiles. We apply
evaluative profiling to Reddit comments labeled with personality scores and
conduct an exploratory study on the relationship between evaluative topics and
Big Five personality facets, aiming for a more interpretable, facet-level
analysis. Finally, we validate our approach by observing correlations
consistent with prior research in personality psychology.",None,-1
Comparing Psychometric and Behavioral Predictors of Compliance During Human-AI Interactions,0.0963258,"Optimization of human-AI teams hinges on the AI's ability to tailor its
interaction to individual human teammates. A common hypothesis in adaptive AI
research is that minor differences in people's predisposition to trust can
significantly impact their likelihood of complying with recommendations from
the AI. Predisposition to trust is often measured with self-report inventories
that are administered before interactions. We benchmark a popular measure of
this kind against behavioral predictors of compliance. We find that the
inventory is a less effective predictor of compliance than the behavioral
measures in datasets taken from three previous research projects. This suggests
a general property that individual differences in initial behavior are more
predictive than differences in self-reported trust attitudes. This result also
shows a potential for easily accessible behavioral measures to provide an AI
with more accurate models without the use of (often costly) survey instruments.",None,-1
Glancing Future for Simultaneous Machine Translation,0.541831,"Simultaneous machine translation (SiMT) outputs translation while reading the
source sentence. Unlike conventional sequence-to-sequence (seq2seq) training,
existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training,
where the model predicts target tokens based on partial source tokens. However,
the prefix2prefix training diminishes the ability of the model to capture
global information and introduces forced predictions due to the absence of
essential source information. Consequently, it is crucial to bridge the gap
between the prefix2prefix training and seq2seq training to enhance the
translation capability of the SiMT model. In this paper, we propose a novel
method that glances future in curriculum learning to achieve the transition
from the seq2seq training to prefix2prefix training. Specifically, we gradually
reduce the available source information from the whole sentence to the prefix
corresponding to that latency. Our method is applicable to a wide range of SiMT
methods and experiments demonstrate that our method outperforms strong
baselines.",None,-1
Kattis vs. ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence,0.650209,"AI-powered education technologies can support students and teachers in
computer science education. However, with the recent developments in generative
AI, and especially the increasingly emerging popularity of ChatGPT, the
effectiveness of using large language models for solving programming tasks has
been underexplored. The present study examines ChatGPT's ability to generate
code solutions at different difficulty levels for introductory programming
courses. We conducted an experiment where ChatGPT was tested on 127 randomly
selected programming problems provided by Kattis, an automatic software grading
tool for computer science programs, often used in higher education. The results
showed that ChatGPT independently could solve 19 out of 127 programming tasks
generated and assessed by Kattis. Further, ChatGPT was found to be able to
generate accurate code solutions for simple problems but encountered
difficulties with more complex programming tasks. The results contribute to the
ongoing debate on the utility of AI-powered tools in programming education.",None,-1
CoReFace: Sample-Guided Contrastive Regularization for Deep Face Recognition,0.588239,"The discriminability of feature representation is the key to open-set face
recognition. Previous methods rely on the learnable weights of the
classification layer that represent the identities. However, the evaluation
process learns no identity representation and drops the classifier from
training. This inconsistency could confuse the feature encoder in understanding
the evaluation goal and hinder the effect of identity-based methods. To
alleviate the above problem, we propose a novel approach namely Contrastive
Regularization for Face recognition (CoReFace) to apply image-level
regularization in feature representation learning. Specifically, we employ
sample-guided contrastive learning to regularize the training with the
image-image relationship directly, which is consistent with the evaluation
process. To integrate contrastive learning into face recognition, we augment
embeddings instead of images to avoid the image quality degradation. Then, we
propose a novel contrastive loss for the representation distribution by
incorporating an adaptive margin and a supervised contrastive mask to generate
steady loss values and avoid the collision with the classification supervision
signal. Finally, we discover and solve the semantically repetitive signal
problem in contrastive learning by exploring new pair coupling protocols.
Extensive experiments demonstrate the efficacy and efficiency of our CoReFace
which is highly competitive with the state-of-the-art approaches.",None,-1
Multilingual Word Error Rate Estimation: e-WER3,0.595366,"The success of the multilingual automatic speech recognition systems
empowered many voice-driven applications. However, measuring the performance of
such systems remains a major challenge, due to its dependency on manually
transcribed speech data in both mono- and multilingual scenarios. In this
paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on
acoustic and lexical representation to estimate word error rate. We demonstrate
the effectiveness of eWER3 to (i) predict WER without using any internal states
from the ASR and (ii) use the multilingual shared latent space to push the
performance of the close-related languages. We show our proposed multilingual
model outperforms the previous monolingual word error rate estimation method
(eWER2) by an absolute 9\% increase in Pearson correlation coefficient (PCC),
with better overall estimation between the predicted and reference WER.",None,-1
Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,0.881903,"Prompt tuning, in which a base pretrained model is adapted to each task via
conditioning on learned prompt vectors, has emerged as a promising approach for
efficiently adapting large language models to multiple downstream tasks.
However, existing methods typically learn soft prompt vectors from scratch, and
it has not been clear how to exploit the rich cross-task knowledge with prompt
vectors in a multitask learning setting. We propose multitask prompt tuning
(MPT), which first learns a single transferable prompt by distilling knowledge
from multiple task-specific source prompts. We then learn multiplicative low
rank updates to this shared prompt to efficiently adapt it to each downstream
target task. Extensive experiments on 23 NLP datasets demonstrate that our
proposed approach outperforms the state-of-the-art methods, including the full
finetuning baseline in some cases, despite only tuning 0.035% as many
task-specific parameters.",None,-1
Investigations on convergence behaviour of Physics Informed Neural Networks across spectral ranges and derivative orders,0.256368,"An important inference from Neural Tangent Kernel (NTK) theory is the
existence of spectral bias (SB), that is, low frequency components of the
target function of a fully connected Artificial Neural Network (ANN) being
learnt significantly faster than the higher frequencies during training. This
is established for Mean Square Error (MSE) loss functions with very low
learning rate parameters. Physics Informed Neural Networks (PINNs) are designed
to learn the solutions of differential equations (DE) of arbitrary orders; in
PINNs the loss functions are obtained as the residues of the conservative form
of the DEs and represent the degree of dissatisfaction of the equations. So
there has been an open question whether (a) PINNs also exhibit SB and (b) if
so, how does this bias vary across the orders of the DEs. In this work, a
series of numerical experiments are conducted on simple sinusoidal functions of
varying frequencies, compositions and equation orders to investigate these
issues. It is firmly established that under normalized conditions, PINNs do
exhibit strong spectral bias, and this increases with the order of the
differential equation.",None,-1
See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data,0.944897,"Zero-shot point cloud segmentation aims to make deep models capable of
recognizing novel objects in point cloud that are unseen in the training phase.
Recent trends favor the pipeline which transfers knowledge from seen classes
with labels to unseen classes without labels. They typically align visual
features with semantic features obtained from word embedding by the supervision
of seen classes' annotations. However, point cloud contains limited information
to fully match with semantic features. In fact, the rich appearance information
of images is a natural complement to the textureless point cloud, which is not
well explored in previous literature. Motivated by this, we propose a novel
multi-modal zero-shot learning method to better utilize the complementary
information of point clouds and images for more accurate visual-semantic
alignment. Extensive experiments are performed in two popular benchmarks, i.e.,
SemanticKITTI and nuScenes, and our method outperforms current SOTA methods
with 52% and 49% improvement on average for unseen class mIoU, respectively.",None,-1
BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives,0.457191,"Implicit neural representations have become pivotal in robotic perception,
enabling robots to comprehend 3D environments from 2D images. Given a set of
camera poses and associated images, the models can be trained to synthesize
novel, unseen views. To successfully navigate and interact in dynamic settings,
robots require the understanding of their spatial surroundings driven by
unassisted reconstruction of 3D scenes and camera poses from real-time video
footage. Existing approaches like COLMAP and bundle-adjusting neural radiance
field methods take hours to days to process due to the high computational
demands of feature matching, dense point sampling, and training of a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP) which leverages accelerated sampling and
hash encoding to expedite automatic pose refinement/estimation and 3D scene
reconstruction. Experimental results demonstrate 10 to 20 x speed improvement
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation. The github repository can be found
here https://github.com/IntelLabs/baa-ngp.",None,-1
A Theory of Bounded Inductive Rationality,0.811881,"The dominant theories of rational choice assume logical omniscience. That is,
they assume that when facing a decision problem, an agent can perform all
relevant computations and determine the truth value of all relevant
logical/mathematical claims. This assumption is unrealistic when, for example,
we offer bets on remote digits of pi or when an agent faces a computationally
intractable planning problem. Furthermore, the assumption of logical
omniscience creates contradictions in cases where the environment can contain
descriptions of the agent itself. Importantly, strategic interactions as
studied in game theory are decision problems in which a rational agent is
predicted by its environment (the other players). In this paper, we develop a
theory of rational decision making that does not assume logical omniscience. We
consider agents who repeatedly face decision problems (including ones like
betting on digits of pi or games against other agents). The main contribution
of this paper is to provide a sensible theory of rationality for such agents.
Roughly, we require that a boundedly rational inductive agent tests each
efficiently computable hypothesis infinitely often and follows those hypotheses
that keep their promises of high rewards. We then prove that agents that are
rational in this sense have other desirable properties. For example, they learn
to value random and pseudo-random lotteries at their expected reward. Finally,
we consider strategic interactions between different agents and prove a folk
theorem for what strategies bounded rational inductive agents can converge to.",None,-1
Do large language models solve verbal analogies like children do?,0.585665,"Analogy-making lies at the heart of human cognition. Adults solve analogies
such as \textit{Horse belongs to stable like chicken belongs to ...?} by
mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In
contrast, children often use association, e.g., answering \textit{egg}. This
paper investigates whether large language models (LLMs) solve verbal analogies
in A:B::C:? form using associations, similar to what children do. We use verbal
analogies extracted from an online adaptive learning environment, where 14,002
7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six
tested Dutch monolingual and multilingual LLMs performed around the same level
as children, with MGPT performing worst, around the 7-year-old level, and XLM-V
and GPT-3 the best, slightly above the 11-year-old level. However, when we
control for associative processes this picture changes and each model's
performance level drops 1-2 years. Further experiments demonstrate that
associative processes often underlie correctly solved analogies. We conclude
that the LLMs we tested indeed tend to solve verbal analogies by association
with C like children do.",None,-1
Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data,0.424263,"Chatbots have become popular in educational settings, revolutionizing how
students interact with material and how teachers teach. We present
Curriculum-Driven EduBot, a framework for developing a chatbot that combines
the interactive features of chatbots with the systematic material of English
textbooks to assist students in enhancing their conversational skills. We begin
by extracting pertinent topics from textbooks and then using large language
models to generate dialogues related to these topics. We then fine-tune an
open-source LLM using our generated conversational data to create our
curriculum-driven chatbot. User studies demonstrate that our chatbot
outperforms ChatGPT in leading curriculum-based dialogues and adapting its
dialogue to match the user's English proficiency level. By combining
traditional textbook methodologies with conversational AI, our approach offers
learners an interactive tool that aligns with their curriculum and provides
user-tailored conversation practice. This facilitates meaningful student-bot
dialogues and enriches the overall learning experience within the curriculum's
pedagogical framework.",None,-1
GNN-based Passenger Request Prediction,0.777338,"Passenger request prediction is essential for operations planning, control,
and management in ride-sharing platforms. While the demand prediction problem
has been studied extensively, the Origin-Destination (OD) flow prediction of
passengers has received less attention from the research community. This paper
develops a Graph Neural Network framework along with the Attention Mechanism to
predict the OD flow of passengers. The proposed framework exploits various
linear and non-linear dependencies that arise among requests originating from
different locations and captures the repetition pattern and the contextual data
of that place. Moreover, the optimal size of the grid cell that covers the road
network and preserves the complexity and accuracy of the model is determined.
Extensive simulations are conducted to examine the characteristics of our
proposed approach and its various components. The results show the superior
performance of our proposed model compared to the existing baselines.",None,-1
Unveiling the Potential of Counterfactuals Explanations in Employability,0.402957,"In eXplainable Artificial Intelligence (XAI), counterfactual explanations are
known to give simple, short, and comprehensible justifications for complex
model decisions. However, we are yet to see more applied studies in which they
are applied in real-world cases. To fill this gap, this study focuses on
showing how counterfactuals are applied to employability-related problems which
involve complex machine learning algorithms. For these use cases, we use real
data obtained from a public Belgian employment institution (VDAB). The use
cases presented go beyond the mere application of counterfactuals as
explanations, showing how they can enhance decision support, comply with legal
requirements, guide controlled changes, and analyze novel insights.",None,-1
Neural Refinement for Absolute Pose Regression with Feature Synthesis,0.42724,"Absolute Pose Regression (APR) methods use deep neural networks to directly
regress camera poses from RGB images. However, the predominant APR
architectures only rely on 2D operations during inference, resulting in limited
accuracy of pose estimation due to the lack of 3D geometry constraints or
priors. In this work, we propose a test-time refinement pipeline that leverages
implicit geometric constraints using a robust feature field to enhance the
ability of APR methods to use 3D information during inference. We also
introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D
geometric features during training and directly renders dense novel view
features at test time to refine APR methods. To enhance the robustness of our
model, we introduce a feature fusion module and a progressive training
strategy. Our proposed method achieves state-of-the-art single-image APR
accuracy on indoor and outdoor datasets.",None,-1
RealFusion: 360 Reconstruction of Any Object from a Single Image,1.0,"We consider the problem of reconstructing a full 360{\deg} photographic model
of an object from a single image of it. We do so by fitting a neural radiance
field to the image, but find this problem to be severely ill-posed. We thus
take an off-the-self conditional image generator based on diffusion and
engineer a prompt that encourages it to ""dream up"" novel views of the object.
Using an approach inspired by DreamFields and DreamFusion, we fuse the given
input view, the conditional prior, and other regularizers in a final,
consistent reconstruction. We demonstrate state-of-the-art reconstruction
results on benchmark images when compared to prior methods for monocular 3D
reconstruction of objects. Qualitatively, our reconstructions provide a
faithful match of the input view and a plausible extrapolation of its
appearance and 3D shape, including to the side of the object not visible in the
image.",None,-1
Generative AI in the Construction Industry: Opportunities & Challenges,0.996744,"In the last decade, despite rapid advancements in artificial intelligence
(AI) transforming many industry practices, construction largely lags in
adoption. Recently, the emergence and rapid adoption of advanced large language
models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown
great potential and sparked considerable global interest. However, the current
surge lacks a study investigating the opportunities and challenges of
implementing Generative AI (GenAI) in the construction sector, creating a
critical knowledge gap for researchers and practitioners. This underlines the
necessity to explore the prospects and complexities of GenAI integration.
Bridging this gap is fundamental to optimizing GenAI's early-stage adoption
within the construction sector. Given GenAI's unprecedented capabilities to
generate human-like content based on learning from existing content, we reflect
on two guiding questions: What will the future bring for GenAI in the
construction industry? What are the potential opportunities and challenges in
implementing GenAI in the construction industry? This study delves into
reflected perception in literature, analyzes the industry perception using
programming-based word cloud and frequency analysis, and integrates authors'
opinions to answer these questions. This paper recommends a conceptual GenAI
implementation framework, provides practical recommendations, summarizes future
research questions, and builds foundational literature to foster subsequent
research expansion in GenAI within the construction and its allied architecture
& engineering domains.",None,-1
LLMLight: Large Language Models as Traffic Signal Control Agents,0.457935,"Traffic Signal Control (TSC) is a crucial component in urban traffic
management, aiming to optimize road network efficiency and reduce congestion.
Traditional methods in TSC, primarily based on transportation engineering and
reinforcement learning (RL), often exhibit limitations in generalization across
varied traffic scenarios and lack interpretability. This paper presents
LLMLight, a novel framework employing Large Language Models (LLMs) as
decision-making agents for TSC. Specifically, the framework begins by
instructing the LLM with a knowledgeable prompt detailing real-time traffic
conditions. Leveraging the advanced generalization capabilities of LLMs,
LLMLight engages a reasoning and decision-making process akin to human
intuition for effective traffic control. Moreover, we build LightGPT, a
specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic
patterns and control strategies, LightGPT enhances the LLMLight framework
cost-effectively. Extensive experiments on nine real-world and synthetic
datasets showcase the remarkable effectiveness, generalization ability, and
interpretability of LLMLight against nine transportation-based and RL-based
baselines.",None,-1
Temporally Consistent Online Depth Estimation Using Point-Based Fusion,0.441213,"Depth estimation is an important step in many computer vision problems such
as 3D reconstruction, novel view synthesis, and computational photography. Most
existing work focuses on depth estimation from single frames. When applied to
videos, the result lacks temporal consistency, showing flickering and swimming
artifacts. In this paper we aim to estimate temporally consistent depth maps of
video streams in an online setting. This is a difficult problem as future
frames are not available and the method must choose between enforcing
consistency and correcting errors from previous estimations. The presence of
dynamic objects further complicates the problem. We propose to address these
challenges by using a global point cloud that is dynamically updated each
frame, along with a learned fusion approach in image space. Our approach
encourages consistency while simultaneously allowing updates to handle errors
and dynamic objects. Qualitative and quantitative results show that our method
achieves state-of-the-art quality for consistent video depth estimation.",None,-1
Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,0.624026,"Recent learning-based video quality assessment (VQA) algorithms are expensive
to implement due to the cost of data collection of human quality opinions, and
are less robust across various scenarios due to the biases of these opinions.
This motivates our exploration on opinion-unaware (a.k.a zero-shot) VQA
approaches. Existing approaches only considers low-level naturalness in spatial
or temporal domain, without considering impacts from high-level semantics. In
this work, we introduce an explicit semantic affinity index for opinion-unaware
VQA using text-prompts in the contrastive language-image pre-training (CLIP)
model. We also aggregate it with different traditional low-level naturalness
indexes through gaussian normalization and sigmoid rescaling strategies.
Composed of aggregated semantic and technical metrics, the proposed Blind
Unified Opinion-Unaware Video Quality Index via Semantic and Technical Metric
Aggregation (BUONA-VISTA) outperforms existing opinion-unaware VQA methods by
at least 20% improvements, and is more robust than opinion-aware approaches.",None,-1
Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition,0.697824,"Micro-expressions are spontaneous, rapid and subtle facial movements that can
neither be forged nor suppressed. They are very important nonverbal
communication clues, but are transient and of low intensity thus difficult to
recognize. Recently deep learning based methods have been developed for
micro-expression (ME) recognition using feature extraction and fusion
techniques, however, targeted feature learning and efficient feature fusion
still lack further study according to the ME characteristics. To address these
issues, we propose a novel framework Feature Representation Learning with
adaptive Displacement Generation and Transformer fusion (FRL-DGT), in which a
convolutional Displacement Generation Module (DGM) with self-supervised
learning is used to extract dynamic features from onset/apex frames targeted to
the subsequent ME recognition task, and a well-designed Transformer Fusion
mechanism composed of three Transformer-based fusion modules (local, global
fusions based on AU regions and full-face fusion) is applied to extract the
multi-level informative features after DGM for the final ME prediction. The
extensive experiments with solid leave-one-subject-out (LOSO) evaluation
results have demonstrated the superiority of our proposed FRL-DGT to
state-of-the-art methods.",None,-1
Optimized Custom Dataset for Efficient Detection of Underwater Trash,0.580041,"Accurately quantifying and removing submerged underwater waste plays a
crucial role in safeguarding marine life and preserving the environment. While
detecting floating and surface debris is relatively straightforward,
quantifying submerged waste presents significant challenges due to factors like
light refraction, absorption, suspended particles, and color distortion. This
paper addresses these challenges by proposing the development of a custom
dataset and an efficient detection approach for submerged marine debris. The
dataset encompasses diverse underwater environments and incorporates
annotations for precise labeling of debris instances. Ultimately, the primary
objective of this custom dataset is to enhance the diversity of litter
instances and improve their detection accuracy in deep submerged environments
by leveraging state-of-the-art deep learning architectures.",None,-1
shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation,0.628541,"Instruction-tuned generative Large language models (LLMs) like ChatGPT and
Bloomz possess excellent generalization abilities, but they face limitations in
understanding radiology reports, particularly in the task of generating the
IMPRESSIONS section from the FINDINGS section. They tend to generate either
verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to
medical text data during training. We present a system which leverages
large-scale medical text data for domain-adaptive pre-training of
instruction-tuned LLMs to enhance its medical knowledge and performance on
specific medical tasks. We show that this system performs better in a zero-shot
setting than a number of pretrain-and-finetune adaptation methods on the
IMPRESSIONS generation task, and ranks 1st among participating systems in Task
1B: Radiology Report Summarization at the BioNLP 2023 workshop.",None,-1
The ACL OCL Corpus: Advancing Open Science in Computational Linguistics,0.340051,"We present ACL OCL, a scholarly corpus derived from the ACL Anthology to
assist Open scientific research in the Computational Linguistics domain.
Integrating and enhancing the previous versions of the ACL Anthology, the ACL
OCL contributes metadata, PDF files, citation graphs and additional structured
full texts with sections, figures, and links to a large knowledge resource
(Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers,
alongside 210K figures.
  We spotlight how ACL OCL applies to observe trends in computational
linguistics. By detecting paper topics with a supervised neural model, we note
that interest in ""Syntax: Tagging, Chunking and Parsing"" is waning and ""Natural
Language Generation"" is resurging. Our dataset is available from HuggingFace
(https://huggingface.co/datasets/WINGNUS/ACL-OCL).",None,-1
Errors are Robustly Tamed in Cumulative Knowledge Processes,0.249199,"We study processes of societal knowledge accumulation, where the validity of
a new unit of knowledge depends both on the correctness of its derivation and
on the validity of the units it depends on. A fundamental question in this
setting is: If a constant fraction of the new derivations is wrong, can
investing a constant fraction, bounded away from one, of effort ensure that a
constant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer,
Mossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model to
analyze such questions and showed an affirmative answer to this question. Their
study, however, focuses on the simple case where each new unit depends on just
one existing unit, and units attach according to a $\textit{preferential
attachment rule}$.
  In this work, we consider much more general families of cumulative knowledge
processes, where new units may attach according to varied attachment mechanisms
and depend on multiple existing units. We also allow a (random) fraction of
insertions of adversarial nodes.
  We give a robust affirmative answer to the above question by showing that for
$\textit{all}$ of these models, as long as many of the units follow simple
heuristics for checking a bounded number of units they depend on, all errors
will be eventually eliminated. Our results indicate that preserving the quality
of large interdependent collections of units of knowledge is feasible, as long
as careful but not too costly checks are performed when new units are
derived/deposited.",None,-1
Learning from Children: Improving Image-Caption Pretraining via Curriculum,0.0330592,"Image-caption pretraining has been quite successfully used for downstream
vision tasks like zero-shot image classification and object detection. However,
image-caption pretraining is still a hard problem -- it requires multiple
concepts (nouns) from captions to be aligned to several objects in images. To
tackle this problem, we go to the roots -- the best learner, children. We take
inspiration from cognitive science studies dealing with children's language
learning to propose a curriculum learning framework. The learning begins with
easy-to-align image caption pairs containing one concept per caption. The
difficulty is progressively increased with each new phase by adding one more
concept per caption. Correspondingly, the knowledge acquired in each learning
phase is utilized in subsequent phases to effectively constrain the learning
problem to aligning one new concept-object pair in each phase. We show that
this learning strategy improves over vanilla image-caption training in various
settings -- pretraining from scratch, using a pretrained image or/and
pretrained text encoder, low data regime etc.",None,-1
Exploiting Inductive Bias in Transformer for Point Cloud Classification and Segmentation,0.200436,"Discovering inter-point connection for efficient high-dimensional feature
extraction from point coordinate is a key challenge in processing point cloud.
Most existing methods focus on designing efficient local feature extractors
while ignoring global connection, or vice versa. In this paper, we design a new
Inductive Bias-aided Transformer (IBT) method to learn 3D inter-point
relations, which considers both local and global attentions. Specifically,
considering local spatial coherence, local feature learning is performed
through Relative Position Encoding and Attentive Feature Pooling. We
incorporate the learned locality into the Transformer module. The local feature
affects value component in Transformer to modulate the relationship between
channels of each point, which can enhance self-attention mechanism with
locality based channel interaction. We demonstrate its superiority
experimentally on classification and segmentation tasks. The code is available
at: https://github.com/jiamang/IBT",None,-1
Grandma Karl is 27 years old -- research agenda for pseudonymization of research data,0.825978,"Accessibility of research data is critical for advances in many research
fields, but textual data often cannot be shared due to the personal and
sensitive information which it contains, e.g names or political opinions.
General Data Protection Regulation (GDPR) suggests pseudonymization as a
solution to secure open access to research data, but we need to learn more
about pseudonymization as an approach before adopting it for manipulation of
research data. This paper outlines a research agenda within pseudonymization,
namely need of studies into the effects of pseudonymization on unstructured
data in relation to e.g. readability and language assessment, as well as the
effectiveness of pseudonymization as a way of protecting writer identity, while
also exploring different ways of developing context-sensitive algorithms for
detection, labelling and replacement of personal information in unstructured
data. The recently granted project on pseudonymization Grandma Karl is 27 years
old addresses exactly those challenges.",None,-1
SegGPT: Segmenting Everything In Context,0.982141,"We present SegGPT, a generalist model for segmenting everything in context.
We unify various segmentation tasks into a generalist in-context learning
framework that accommodates different kinds of segmentation data by
transforming them into the same format of images. The training of SegGPT is
formulated as an in-context coloring problem with random color mapping for each
data sample. The objective is to accomplish diverse tasks according to the
context, rather than relying on specific colors. After training, SegGPT can
perform arbitrary segmentation tasks in images or videos via in-context
inference, such as object instance, stuff, part, contour, and text. SegGPT is
evaluated on a broad range of tasks, including few-shot semantic segmentation,
video object segmentation, semantic segmentation, and panoptic segmentation.
Our results show strong capabilities in segmenting in-domain and out-of-domain
targets, either qualitatively or quantitatively.",None,-1
ZScribbleSeg: Zen and the Art of Scribble Supervised Medical Image Segmentation,0.310968,"Curating a large scale fully-annotated dataset can be both labour-intensive
and expertise-demanding, especially for medical images. To alleviate this
problem, we propose to utilize solely scribble annotations for weakly
supervised segmentation. Existing solutions mainly leverage selective losses
computed solely on annotated areas and generate pseudo gold standard
segmentation by propagating labels to adjacent areas. However, these methods
could suffer from the inaccurate and sometimes unrealistic pseudo segmentation
due to the insufficient supervision and incomplete shape features. Different
from previous efforts, we first investigate the principle of ''good scribble
annotations'', which leads to efficient scribble forms via supervision
maximization and randomness simulation. Furthermore, we introduce
regularization terms to encode the spatial relationship and shape prior, where
a new formulation is developed to estimate the mixture ratios of label classes.
These ratios are critical in identifying the unlabeled pixels for each class
and correcting erroneous predictions, thus the accurate estimation lays the
foundation for the incorporation of spatial prior. Finally, we integrate the
efficient scribble supervision with the prior into a unified framework, denoted
as ZScribbleSeg, and apply the method to multiple scenarios. Leveraging only
scribble annotations, ZScribbleSeg set new state-of-the-arts on four
segmentation tasks using ACDC, MSCMRseg, MyoPS and PPSS datasets.",None,-1
Carbon-Efficient Neural Architecture Search,0.220568,"This work presents a novel approach to neural architecture search (NAS) that
aims to reduce energy costs and increase carbon efficiency during the model
design process. The proposed framework, called carbon-efficient NAS (CE-NAS),
consists of NAS evaluation algorithms with different energy requirements, a
multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS
dynamically balances energy-efficient sampling and energy-consuming evaluation
tasks based on current carbon emissions. Using a recent NAS benchmark dataset
and two carbon traces, our trace-driven simulations demonstrate that CE-NAS
achieves better carbon and search efficiency than the three baselines.",None,-1
ADaPT: As-Needed Decomposition and Planning with Language Models,0.263209,"Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.",None,-1
PIVOINE: Instruction Tuning for Open-world Information Extraction,0.660573,"We consider the problem of Open-world Information Extraction (Open-world IE),
which extracts comprehensive entity profiles from unstructured texts. Different
from the conventional closed-world setting of Information Extraction (IE),
Open-world IE considers a more general situation where entities and relations
could be beyond a predefined ontology. More importantly, we seek to develop a
large language model (LLM) that is able to perform Open-world IE to extract
desirable entity profiles characterized by (possibly fine-grained) natural
language instructions. We achieve this by finetuning LLMs using instruction
tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction
tuning dataset for Open-world IE enriched with a comprehensive corpus,
extensive annotations, and diverse instructions. We finetune the pretrained
BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE
with strong instruction-following capabilities. Our experiments demonstrate
that PIVOINE significantly outperforms traditional closed-world methods and
other LLM baselines, displaying impressive generalization capabilities on both
unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as
a promising solution to tackle the open-world challenge in IE effectively.",None,-1
Can we trust the evaluation on ChatGPT?,0.796242,"ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.",None,-1
How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions,0.0443385,"While recent advancements in large language models (LLMs) bring us closer to
achieving artificial general intelligence, the question persists: Do LLMs truly
understand language, or do they merely mimic comprehension through pattern
recognition? This study seeks to explore this question through the lens of
syntax, a crucial component of sentence comprehension. Adopting a natural
language question-answering (Q&A) scheme, we craft questions targeting nine
syntactic knowledge points that are most closely related to sentence
comprehension. Experiments conducted on 24 LLMs suggest that most have a
limited grasp of syntactic knowledge, exhibiting notable discrepancies across
different syntactic knowledge points. In particular, questions involving
prepositional phrase attachment pose the greatest challenge, whereas those
concerning adjectival modifier and indirect object are relatively easier for
LLMs to handle. Furthermore, a case study on the training dynamics of the LLMs
reveals that the majority of syntactic knowledge is learned during the initial
stages of training, hinting that simply increasing the number of training
tokens may not be the `silver bullet' for improving the comprehension ability
of LLMs.",None,-1
A Unified One-Step Solution for Aspect Sentiment Quad Prediction,0.29926,"Aspect sentiment quad prediction (ASQP) is a challenging yet significant
subtask in aspect-based sentiment analysis as it provides a complete
aspect-level sentiment structure. However, existing ASQP datasets are usually
small and low-density, hindering technical advancement. To expand the capacity,
in this paper, we release two new datasets for ASQP, which contain the
following characteristics: larger size, more words per sample, and higher
density. With such datasets, we unveil the shortcomings of existing strong ASQP
baselines and therefore propose a unified one-step solution for ASQP, namely
One-ASQP, to detect the aspect categories and to identify the
aspect-opinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds
several unique advantages: (1) by separating ASQP into two subtasks and solving
them independently and simultaneously, we can avoid error propagation in
pipeline-based methods and overcome slow training and inference in
generation-based methods; (2) by introducing sentiment-specific horns tagging
schema in a token-pair-based two-dimensional matrix, we can exploit deeper
interactions between sentiment elements and efficiently decode the AOS
triplets; (3) we design ``[NULL]'' token can help us effectively identify the
implicit aspects or opinions. Experiments on two benchmark datasets and our
released two datasets demonstrate the advantages of our One-ASQP. The two new
datasets are publicly released at
\url{https://www.github.com/Datastory-CN/ASQP-Datasets}.",None,-1
Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,0.890582,"This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)
framework. It emphasizes spatio-temporal continuity and integrates both past
and future reasoning for tracked objects. Thus, we name it ""Past-and-Future
reasoning for Tracking"" (PF-Track). Specifically, our method adapts the
""tracking by attention"" framework and represents tracked instances coherently
over time with object queries. To explicitly use historical cues, our ""Past
Reasoning"" module learns to refine the tracks and enhance the object features
by cross-attending to queries from previous frames and other objects. The
""Future Reasoning"" module digests historical information and predicts robust
future trajectories. In the case of long-term occlusions, our method maintains
the object positions and enables re-association by integrating motion
predictions. On the nuScenes dataset, our method improves AMOTA by a large
margin and remarkably reduces ID-Switches by 90% compared to prior approaches,
which is an order of magnitude less. The code and models are made available at
https://github.com/TRI-ML/PF-Track.",None,-1
On the Importance of Noise Scheduling for Diffusion Models,0.69145,"We empirically study the effect of noise scheduling strategies for denoising
diffusion generative models. There are three findings: (1) the noise scheduling
is crucial for the performance, and the optimal one depends on the task (e.g.,
image sizes), (2) when increasing the image size, the optimal noise scheduling
shifts towards a noisier one (due to increased redundancy in pixels), and (3)
simply scaling the input data by a factor of $b$ while keeping the noise
schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a
good strategy across image sizes. This simple recipe, when combined with
recently proposed Recurrent Interface Network (RIN), yields state-of-the-art
pixel-based diffusion models for high-resolution images on ImageNet, enabling
single-stage, end-to-end generation of diverse and high-fidelity images at
1024$\times$1024 resolution (without upsampling/cascades).",None,-1
S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,0.224242,"Many point cloud classification methods are developed under the assumption
that all point clouds in the dataset are well aligned with the canonical axes
so that the 3D Cartesian point coordinates can be employed to learn features.
When input point clouds are not aligned, the classification performance drops
significantly. In this work, we focus on a mathematically transparent point
cloud classification method called PointHop, analyze its reason for failure due
to pose variations, and solve the problem by replacing its pose dependent
modules with rotation invariant counterparts. The proposed method is named
SO(3)-Invariant PointHop (or S3I-PointHop in short). We also significantly
simplify the PointHop pipeline using only one single hop along with multiple
spatial aggregation techniques. The idea of exploiting more spatial information
is novel. Experiments on the ModelNet40 dataset demonstrate the superiority of
S3I-PointHop over traditional PointHop-like methods.",None,-1
Generative Prompt Model for Weakly Supervised Object Localization,0.992687,"Weakly supervised object localization (WSOL) remains challenging when
learning object localization models from image category labels. Conventional
methods that discriminatively train activation models ignore representative yet
less discriminative object parts. In this study, we propose a generative prompt
model (GenPromp), defining the first generative pipeline to localize less
discriminative object parts by formulating WSOL as a conditional image
denoising procedure. During training, GenPromp converts image category labels
to learnable prompt embeddings which are fed to a generative model to
conditionally recover the input image with noise and learn representative
embeddings. During inference, enPromp combines the representative embeddings
with discriminative embeddings (queried from an off-the-shelf vision-language
model) for both representative and discriminative capacity. The combined
embeddings are finally used to generate multi-scale high-quality attention
maps, which facilitate localizing full object extent. Experiments on
CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best
discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline
for WSOL with the generative model. Code is available at
https://github.com/callsys/GenPromp.",None,-1
Is Machine Learning Unsafe and Irresponsible in Social Sciences? Paradoxes and Reconsidering from Recidivism Prediction Tasks,0.303666,"The paper addresses some fundamental and hotly debated issues for high-stakes
event predictions underpinning the computational approach to social sciences.
We question several prevalent views against machine learning and outline a new
paradigm that highlights the promises and promotes the infusion of
computational methods and conventional social science approaches.",None,-1
A negation detection assessment of GPTs: analysis with the xNot360 dataset,0.16387,"Negation is a fundamental aspect of natural language, playing a critical role
in communication and comprehension. Our study assesses the negation detection
performance of Generative Pre-trained Transformer (GPT) models, specifically
GPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in
natural language using a zero-shot prediction approach applied to our custom
xNot360 dataset. Our approach examines sentence pairs labeled to indicate
whether the second sentence negates the first. Our findings expose a
considerable performance disparity among the GPT models, with GPT-4 surpassing
its counterparts and GPT-3.5 displaying a marked performance reduction. The
overall proficiency of the GPT models in negation detection remains relatively
modest, indicating that this task pushes the boundaries of their natural
language understanding capabilities. We not only highlight the constraints of
GPT models in handling negation but also emphasize the importance of logical
reliability in high-stakes domains such as healthcare, science, and law.",None,-1
Rather a Nurse than a Physician -- Contrastive Explanations under Investigation,0.740519,"Contrastive explanations, where one decision is explained in contrast to
another, are supposed to be closer to how humans explain a decision than
non-contrastive explanations, where the decision is not necessarily referenced
to an alternative. This claim has never been empirically validated. We analyze
four English text-classification datasets (SST2, DynaSent, BIOS and
DBpedia-Animals). We fine-tune and extract explanations from three different
models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three
post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore
collect and release human rationale annotations for a subset of 100 samples
from the BIOS dataset for contrastive and non-contrastive settings. A
cross-comparison between model-based rationales and human annotations, both in
contrastive and non-contrastive settings, yields a high agreement between the
two settings for models as well as for humans. Moreover, model-based
explanations computed in both settings align equally well with human
rationales. Thus, we empirically find that humans do not necessarily explain in
a contrastive manner.9 pages, long paper at ACL 2022 proceedings.",None,-1
Methodology for generating synthetic labeled datasets for visual container inspection,0.235579,"Nowadays, containerized freight transport is one of the most important
transportation systems that is undergoing an automation process due to the Deep
Learning success. However, it suffers from a lack of annotated data in order to
incorporate state-of-the-art neural network models to its systems. In this
paper we present an innovative methodology to generate a realistic, varied,
balanced, and labelled dataset for visual inspection task of containers in a
dock environment. In addition, we validate this methodology with multiple
visual tasks recurrently found in the state of the art. We prove that the
generated synthetic labelled dataset allows to train a deep neural network that
can be used in a real world scenario. On the other side, using this methodology
we provide the first open synthetic labelled dataset called SeaFront available
in: https://datasets.vicomtech.org/di21-seafront/readme.txt.",None,-1
Fair Decision-making Under Uncertainty,0.756516,"There has been concern within the artificial intelligence (AI) community and
the broader society regarding the potential lack of fairness of AI-based
decision-making systems. Surprisingly, there is little work quantifying and
guaranteeing fairness in the presence of uncertainty which is prevalent in many
socially sensitive applications, ranging from marketing analytics to actuarial
analysis and recidivism prediction instruments. To this end, we study a
longitudinal censored learning problem subject to fairness constraints, where
we require that algorithmic decisions made do not affect certain individuals or
social groups negatively in the presence of uncertainty on class label due to
censorship. We argue that this formulation has a broader applicability to
practical scenarios concerning fairness. We show how the newly devised fairness
notions involving censored information and the general framework for fair
predictions in the presence of censorship allow us to measure and mitigate
discrimination under uncertainty that bridges the gap with real-world
applications. Empirical evaluations on real-world discriminated datasets with
censorship demonstrate the practicality of our approach.",None,-1
Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and Model Lineage Analysis,0.721556,"The generation of high-quality images has become widely accessible and is a
rapidly evolving process. As a result, anyone can generate images that are
indistinguishable from real ones. This leads to a wide range of applications,
including malicious usage with deceptive intentions. Despite advances in
detection techniques for generated images, a robust detection method still
eludes us. Furthermore, model personalization techniques might affect the
detection capabilities of existing methods. In this work, we utilize the
architectural properties of convolutional neural networks (CNNs) to develop a
new detection method. Our method can detect images from a known generative
model and enable us to establish relationships between fine-tuned generative
models. We tested the method on images produced by both Generative Adversarial
Networks (GANs) and recent large text-to-image models (LTIMs) that rely on
Diffusion Models. Our approach outperforms others trained under identical
conditions and achieves comparable performance to state-of-the-art pre-trained
detection methods on images generated by Stable Diffusion and MidJourney, with
significantly fewer required train samples.",None,-1
Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance,0.542139,"We propose the use of conversational GPT models for easy and quick few-shot
text classification in the financial domain using the Banking77 dataset. Our
approach involves in-context learning with GPT-3.5 and GPT-4, which minimizes
the technical expertise required and eliminates the need for expensive GPU
computing while yielding quick and accurate results. Additionally, we fine-tune
other pre-trained, masked language models with SetFit, a recent contrastive
learning technique, to achieve state-of-the-art results both in full-data and
few-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can
outperform fine-tuned, non-generative models even with fewer examples. However,
subscription fees associated with these solutions may be considered costly for
small organizations. Lastly, we find that generative models perform better on
the given task when shown representative samples selected by a human expert
rather than when shown random ones. We conclude that a) our proposed methods
offer a practical solution for few-shot tasks in datasets with limited label
availability, and b) our state-of-the-art results can inspire future work in
the area.",None,-1
Professional Basketball Player Behavior Synthesis via Planning with Diffusion,0.917915,"Dynamically planning in multi-agent systems has been explored to improve
decision-making in various domains. Professional basketball serves as a
compelling example of a dynamic spatio-temporal game, encompassing both
concealed strategic policies and decision-making. However, processing the
diverse on-court signals and navigating the vast space of potential actions and
outcomes makes it difficult for existing approaches to swiftly identify optimal
strategies in response to evolving circumstances. In this study, we first
formulate the sequential decision-making process as a conditional trajectory
generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis),
a method for enhancing player decision-making. We extend the state-of-the-art
generative model, diffusion probabilistic model, to learn challenging
multi-agent environmental dynamics from historical National Basketball
Association (NBA) player motion tracking data. To incorporate data-driven
strategies, an auxiliary value function is trained using the play-by-play data
with corresponding rewards acting as the plan guidance. To accomplish
reward-guided trajectory generation, conditional sampling is introduced to
condition the diffusion model on the value function and conduct
classifier-guided sampling. We validate the effectiveness of PLAYBEST via
comprehensive simulation studies from real-world data, contrasting the
generated trajectories and play strategies with those employed by professional
basketball teams. Our results reveal that the model excels at generating
high-quality basketball trajectories that yield efficient plays, surpassing
conventional planning techniques in terms of adaptability, flexibility, and
overall performance. Moreover, the synthesized play strategies exhibit a
remarkable alignment with professional tactics, highlighting the model's
capacity to capture the intricate dynamics of basketball games.",None,-1
WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents,0.764767,"In this paper, we introduce WeLayout, a novel system for segmenting the
layout of corporate documents, which stands for WeChat Layout Analysis System.
Our approach utilizes a sophisticated ensemble of DINO and YOLO models,
specifically developed for the ICDAR 2023 Competition on Robust Layout
Segmentation. Our method significantly surpasses the baseline, securing a top
position on the leaderboard with a mAP of 70.0. To achieve this performance, we
concentrated on enhancing various aspects of the task, such as dataset
augmentation, model architecture, bounding box refinement, and model ensemble
techniques. Additionally, we trained the data separately for each document
category to ensure a higher mean submission score. We also developed an
algorithm for cell matching to further improve our performance. To identify the
optimal weights and IoU thresholds for our model ensemble, we employed a
Bayesian optimization algorithm called the Tree-Structured Parzen Estimator.
Our approach effectively demonstrates the benefits of combining query-based and
anchor-free models for achieving robust layout segmentation in corporate
documents.",None,-1
PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction,0.36636,"Speech-to-text errors made by automatic speech recognition (ASR) systems
negatively impact downstream models. Error correction models as a
post-processing text editing method have been recently developed for refining
the ASR outputs. However, efficient models that meet the low latency
requirements of industrial grade production systems have not been well studied.
We propose PATCorrect-a novel non-autoregressive (NAR) approach based on
multi-modal fusion leveraging representations from both text and phoneme
modalities, to reduce word error rate (WER) and perform robustly with varying
input transcription quality. We demonstrate that PATCorrect consistently
outperforms state-of-the-art NAR method on English corpus across different
upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to
9.46% WERR achieved by other methods using text only modality. Besides, its
inference latency is at tens of milliseconds, making it ideal for systems with
low latency requirements.",None,-1
LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation,0.956689,"Empowering chatbots in the field of mental health is receiving increasing
amount of attention, while there still lacks exploration in developing and
evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus
on exploring the potential of ChatGPT in powering chatbots for psychiatrist and
patient simulation. We collaborate with psychiatrists to identify objectives
and iteratively develop the dialogue system to closely align with real-world
scenarios. In the evaluation experiments, we recruit real psychiatrists and
patients to engage in diagnostic conversations with the chatbots, collecting
their ratings for assessment. Our findings demonstrate the feasibility of using
ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of
prompt designs on chatbot behavior and user experience.",None,-1
EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design,0.406039,"We present a hardware-efficient architecture of convolutional neural network,
which has a repvgg-like architecture. Flops or parameters are traditional
metrics to evaluate the efficiency of networks which are not sensitive to
hardware including computing ability and memory bandwidth. Thus, how to design
a neural network to efficiently use the computing ability and memory bandwidth
of hardware is a critical problem. This paper proposes a method how to design
hardware-aware neural network. Based on this method, we designed EfficientRep
series convolutional networks, which are high-computation hardware(e.g. GPU)
friendly and applied in YOLOv6 object detection framework. YOLOv6 has published
YOLOv6N/YOLOv6S/YOLOv6M/YOLOv6L models in v1 and v2 versions.",None,-1
Give Me More Details: Improving Fact-Checking with Latent Retrieval,0.361114,"Evidence plays a crucial role in automated fact-checking. When verifying
real-world claims, existing fact-checking systems either assume the evidence
sentences are given or use the search snippets returned by the search engine.
Such methods ignore the challenges of collecting evidence and may not provide
sufficient information to verify real-world claims. Aiming at building a better
fact-checking system, we propose to incorporate full text from source documents
as evidence and introduce two enriched datasets. The first one is a
multilingual dataset, while the second one is monolingual (English). We further
develop a latent variable model to jointly extract evidence sentences from
documents and perform claim verification. Experiments indicate that including
source documents can provide sufficient contextual clues even when gold
evidence sentences are not annotated. The proposed system is able to achieve
significant improvements upon best-reported models under different settings.",None,-1
UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity,0.738807,"Image reconstruction and captioning from brain activity evoked by visual
stimuli allow researchers to further understand the connection between the
human brain and the visual perception system. While deep generative models have
recently been employed in this field, reconstructing realistic captions and
images with both low-level details and high semantic fidelity is still a
challenging problem. In this work, we propose UniBrain: Unify Image
Reconstruction and Captioning All in One Diffusion Model from Human Brain
Activity. For the first time, we unify image reconstruction and captioning from
visual-evoked functional magnetic resonance imaging (fMRI) through a latent
diffusion model termed Versatile Diffusion. Specifically, we transform fMRI
voxels into text and image latent for low-level information and guide the
backward diffusion process through fMRI-based image and text conditions derived
from CLIP to generate realistic captions and images. UniBrain outperforms
current methods both qualitatively and quantitatively in terms of image
reconstruction and reports image captioning results for the first time on the
Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and
functional region-of-interest (ROI) analysis further exhibit the superiority of
UniBrain and provide comprehensive insight for visual-evoked brain decoding.",None,-1
NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning,0.314086,"This paper introduces NoRefER, a novel referenceless quality metric for
automatic speech recognition (ASR) systems. Traditional reference-based metrics
for evaluating ASR systems require costly ground-truth transcripts. NoRefER
overcomes this limitation by fine-tuning a multilingual language model for
pair-wise ranking ASR hypotheses using contrastive learning with Siamese
network architecture. The self-supervised NoRefER exploits the known quality
relationships between hypotheses from multiple compression levels of an ASR for
learning to rank intra-sample hypotheses by quality, which is essential for
model comparisons. The semi-supervised version also uses a referenced dataset
to improve its inter-sample quality ranking, which is crucial for selecting
potentially erroneous samples. The results indicate that NoRefER correlates
highly with reference-based metrics and their intra-sample ranks, indicating a
high potential for referenceless ASR evaluation or a/b testing.",None,-1
LDM3D: Latent Diffusion Model for 3D,0.514319,"This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that
generates both image and depth map data from a given text prompt, allowing
users to generate RGBD images from text prompts. The LDM3D model is fine-tuned
on a dataset of tuples containing an RGB image, depth map and caption, and
validated through extensive experiments. We also develop an application called
DepthFusion, which uses the generated RGB images and depth maps to create
immersive and interactive 360-degree-view experiences using TouchDesigner. This
technology has the potential to transform a wide range of industries, from
entertainment and gaming to architecture and design. Overall, this paper
presents a significant contribution to the field of generative AI and computer
vision, and showcases the potential of LDM3D and DepthFusion to revolutionize
content creation and digital experiences. A short video summarizing the
approach can be found at https://t.ly/tdi2.",None,-1
Generalist: Decoupling Natural and Robust Generalization,0.450991,"Deep neural networks obtained by standard training have been constantly
plagued by adversarial examples. Although adversarial training demonstrates its
capability to defend against adversarial examples, unfortunately, it leads to
an inevitable drop in the natural generalization. To address the issue, we
decouple the natural generalization and the robust generalization from joint
training and formulate different training strategies for each one.
Specifically, instead of minimizing a global loss on the expectation over these
two generalization errors, we propose a bi-expert framework called
\emph{Generalist} where we simultaneously train base learners with task-aware
strategies so that they can specialize in their own fields. The parameters of
base learners are collected and combined to form a global learner at intervals
during the training process. The global learner is then distributed to the base
learners as initialized parameters for continued training. Theoretically, we
prove that the risks of Generalist will get lower once the base learners are
well trained. Extensive experiments verify the applicability of Generalist to
achieve high accuracy on natural examples while maintaining considerable
robustness to adversarial ones. Code is available at
https://github.com/PKU-ML/Generalist.",None,-1
A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment,0.665795,"When communicating with elders with cognitive impairment, cognitive
stimulation (CS) help to maintain the cognitive health of elders. Data sparsity
is the main challenge in building CS-based dialogue systems, particularly in
the Chinese language. To fill this gap, we construct a Chinese CS conversation
(CSConv) dataset, which contains about 2.6K groups of dialogues with CS
principles and emotional support strategy labels. Making chit chat while
providing emotional support is overlooked by the majority of existing cognitive
dialogue systems. In this paper, we propose a multi-source knowledge fusion
method for CS dialogue (CSD), to generate open-ended responses guided by the CS
principle and emotional support strategy. We first use a progressive mask
method based on external knowledge to learn encoders as effective classifiers,
which is the prerequisite to predict the CS principle and emotional support
strategy of the target response. Then a decoder interacts with the perceived CS
principle and emotional support strategy to generate responses. Extensive
experiments conducted on the CSConv dataset demonstrate the effectiveness of
the proposed method, while there is still a large space for improvement
compared to human performance.",None,-1
Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification,0.331872,"The success of deep learning models on multi-hop fact verification has
prompted researchers to understand the behavior behind their veracity. One
possible way is erasure search: obtaining the rationale by entirely removing a
subset of input without compromising the veracity prediction. Although
extensively explored, existing approaches fall within the scope of the
single-granular (tokens or sentences) explanation, which inevitably leads to
explanation redundancy and inconsistency. To address such issues, this paper
explores the viability of multi-granular rationale extraction with consistency
and faithfulness for explainable multi-hop fact verification. In particular,
given a pretrained veracity prediction model, both the token-level explainer
and sentence-level explainer are trained simultaneously to obtain
multi-granular rationales via differentiable masking. Meanwhile, three
diagnostic properties (fidelity, consistency, salience) are introduced and
applied to the training process, to ensure that the extracted rationales
satisfy faithfulness and consistency. Experimental results on three multi-hop
fact verification datasets show that the proposed approach outperforms some
state-of-the-art baselines.",None,-1
Depression detection in social media posts using affective and social norm features,0.0339652,"We propose a deep architecture for depression detection from social media
posts. The proposed architecture builds upon BERT to extract language
representations from social media posts and combines these representations
using an attentive bidirectional GRU network. We incorporate affective
information, by augmenting the text representations with features extracted
from a pretrained emotion classifier. Motivated by psychological literature we
propose to incorporate profanity and morality features of posts and words in
our architecture using a late fusion scheme. Our analysis indicates that
morality and profanity can be important features for depression detection. We
apply our model for depression detection on Reddit posts on the Pirina dataset,
and further consider the setting of detecting depressed users, given multiple
posts per user, proposed in the Reddit RSDD dataset. The inclusion of the
proposed features yields state-of-the-art results in both settings, namely
2.65% and 6.73% absolute improvement in F1 score respectively. Index Terms:
Depression detection, BERT, Feature fusion, Emotion recognition, profanity,
morality",None,-1
Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities,0.912985,"Unsupervised video-based object-centric learning is a promising avenue to
learn structured representations from large, unlabeled video collections, but
previous approaches have only managed to scale to real-world datasets in
restricted domains. Recently, it was shown that the reconstruction of
pre-trained self-supervised features leads to object-centric representations on
unconstrained real-world image datasets. Building on this approach, we propose
a novel way to use such pre-trained features in the form of a temporal feature
similarity loss. This loss encodes semantic and temporal correlations between
image patches and is a natural way to introduce a motion bias for object
discovery. We demonstrate that this loss leads to state-of-the-art performance
on the challenging synthetic MOVi datasets. When used in combination with the
feature reconstruction loss, our model is the first object-centric video model
that scales to unconstrained video datasets such as YouTube-VIS.",None,-1
Characterizing Financial Market Coverage using Artificial Intelligence,0.377661,"This paper scrutinizes a database of over 4900 YouTube videos to characterize
financial market coverage. Financial market coverage generates a large number
of videos. Therefore, watching these videos to derive actionable insights could
be challenging and complex. In this paper, we leverage Whisper, a
speech-to-text model from OpenAI, to generate a text corpus of market coverage
videos from Bloomberg and Yahoo Finance. We employ natural language processing
to extract insights regarding language use from the market coverage. Moreover,
we examine the prominent presence of trending topics and their evolution over
time, and the impacts that some individuals and organizations have on the
financial market. Our characterization highlights the dynamics of the financial
market coverage and provides valuable insights reflecting broad discussions
regarding recent financial events and the world economy.",None,-1
HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining,0.734559,"Human-centric perceptions include a variety of vision tasks, which have
widespread industrial applications, including surveillance, autonomous driving,
and the metaverse. It is desirable to have a general pretrain model for
versatile human-centric downstream tasks. This paper forges ahead along this
path from the aspects of both benchmark and pretraining methods. Specifically,
we propose a \textbf{HumanBench} based on existing datasets to comprehensively
evaluate on the common ground the generalization abilities of different
pretraining methods on 19 datasets from 6 diverse downstream tasks, including
person ReID, pose estimation, human parsing, pedestrian attribute recognition,
pedestrian detection, and crowd counting. To learn both coarse-grained and
fine-grained knowledge in human bodies, we further propose a \textbf{P}rojector
\textbf{A}ssis\textbf{T}ed \textbf{H}ierarchical pretraining method
(\textbf{PATH}) to learn diverse knowledge at different granularity levels.
Comprehensive evaluations on HumanBench show that our PATH achieves new
state-of-the-art results on 17 downstream datasets and on-par results on the
other 2 datasets. The code will be publicly at
\href{https://github.com/OpenGVLab/HumanBench}{https://github.com/OpenGVLab/HumanBench}.",None,-1
CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis,0.947794,"In this work, we focus on a novel task of category-level functional
hand-object manipulation synthesis covering both rigid and articulated object
categories. Given an object geometry, an initial human hand pose as well as a
sparse control sequence of object poses, our goal is to generate a physically
reasonable hand-object manipulation sequence that performs like human beings.
To address such a challenge, we first design CAnonicalized Manipulation Spaces
(CAMS), a two-level space hierarchy that canonicalizes the hand poses in an
object-centric and contact-centric view. Benefiting from the representation
capability of CAMS, we then present a two-stage framework for synthesizing
human-like manipulation animations. Our framework achieves state-of-the-art
performance for both rigid and articulated categories with impressive visual
effects. Codes and video results can be found at our project homepage:
https://cams-hoi.github.io/",None,-1
Linearity of Relation Decoding in Transformer Language Models,0.797188,"Much of the knowledge encoded in transformer language models (LMs) may be
expressed in terms of relations: relations between words and their synonyms,
entities and their attributes, etc. We show that, for a subset of relations,
this computation is well-approximated by a single linear transformation on the
subject representation. Linear relation representations may be obtained by
constructing a first-order approximation to the LM from a single prompt, and
they exist for a variety of factual, commonsense, and linguistic relations.
However, we also identify many cases in which LM predictions capture relational
knowledge accurately, but this knowledge is not linearly encoded in their
representations. Our results thus reveal a simple, interpretable, but
heterogeneously deployed knowledge representation strategy in transformer LMs.",None,-1
PatFig: Generating Short and Long Captions for Patent Figures,0.264859,"This paper introduces Qatent PatFig, a novel large-scale patent figure
dataset comprising 30,000+ patent figures from over 11,000 European patent
applications. For each figure, this dataset provides short and long captions,
reference numerals, their corresponding terms, and the minimal claim set that
describes the interactions between the components of the image. To assess the
usability of the dataset, we finetune an LVLM model on Qatent PatFig to
generate short and long descriptions, and we investigate the effects of
incorporating various text-based cues at the prediction stage of the patent
figure captioning process.",None,-1
Towards Compute-Optimal Transfer Learning,0.0354698,"The field of transfer learning is undergoing a significant shift with the
introduction of large pretrained models which have demonstrated strong
adaptability to a variety of downstream tasks. However, the high computational
and memory requirements to finetune or use these models can be a hindrance to
their widespread use. In this study, we present a solution to this issue by
proposing a simple yet effective way to trade computational efficiency for
asymptotic performance which we define as the performance a learning algorithm
achieves as compute tends to infinity. Specifically, we argue that zero-shot
structured pruning of pretrained models allows them to increase compute
efficiency with minimal reduction in performance. We evaluate our method on the
Nevis'22 continual learning benchmark that offers a diverse set of transfer
scenarios. Our results show that pruning convolutional filters of pretrained
models can lead to more than 20% performance improvement in low computational
regimes.",None,-1
On the Local Cache Update Rules in Streaming Federated Learning,0.121348,"In this study, we address the emerging field of Streaming Federated Learning
(SFL) and propose local cache update rules to manage dynamic data distributions
and limited cache capacity. Traditional federated learning relies on fixed data
sets, whereas in SFL, data is streamed, and its distribution changes over time,
leading to discrepancies between the local training dataset and long-term
distribution. To mitigate this problem, we propose three local cache update
rules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR),
and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache of
each client while considering the limited cache capacity. Furthermore, we
derive a convergence bound for our proposed SFL algorithm as a function of the
distribution discrepancy between the long-term data distribution and the
client's local training dataset. We then evaluate our proposed algorithm on two
datasets: a network traffic classification dataset and an image classification
dataset. Our experimental results demonstrate that our proposed local cache
update rules significantly reduce the distribution discrepancy and outperform
the baseline methods. Our study advances the field of SFL and provides
practical cache management solutions in federated learning.",None,-1
Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT,0.980705,"Recently, ChatGPT has attracted great attention, as it can generate fluent
and high-quality responses to human inquiries. Several prior studies have shown
that ChatGPT attains remarkable generation ability compared with existing
models. However, the quantitative analysis of ChatGPT's understanding ability
has been given little attention. In this report, we explore the understanding
ability of ChatGPT by evaluating it on the most popular GLUE benchmark, and
comparing it with 4 representative fine-tuned BERT-style models. We find that:
1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT
outperforms all BERT models on inference tasks by a large margin; 3) ChatGPT
achieves comparable performance compared with BERT on sentiment analysis and
question-answering tasks. Additionally, by combining some advanced prompting
strategies, we show that the understanding ability of ChatGPT can be further
improved.",None,-1
Procedural content generation of puzzle games using conditional generative adversarial networks,0.31433,"In this article, we present an experimental approach to using parameterized
Generative Adversarial Networks (GANs) to produce levels for the puzzle game
Lily's Garden. We extract two condition vectors from the real levels in an
effort to control the details of the GAN's outputs. While the GANs perform well
in approximating the first condition (map shape), they struggle to approximate
the second condition (piece distribution). We hypothesize that this might be
improved by trying out alternative architectures for both the Generator and
Discriminator of the GANs.",None,-1
Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,0.604992,"Current popular backbones in computer vision, such as Vision Transformers
(ViT) and ResNets are trained to perceive the world from 2D images. However, to
more effectively understand 3D structural priors in 2D backbones, we propose
Mask3D to leverage existing large-scale RGB-D data in a self-supervised
pre-training to embed these 3D priors into 2D learned feature representations.
In contrast to traditional 3D contrastive learning paradigms requiring 3D
reconstructions or multi-view correspondences, our approach is simple: we
formulate a pre-text reconstruction task by masking RGB and depth patches in
individual RGB-D frames. We demonstrate the Mask3D is particularly effective in
embedding 3D priors into the powerful 2D ViT backbone, enabling improved
representation learning for various scene understanding tasks, such as semantic
segmentation, instance segmentation and object detection. Experiments show that
Mask3D notably outperforms existing self-supervised 3D pre-training approaches
on ScanNet, NYUv2, and Cityscapes image understanding tasks, with an
improvement of +6.5% mIoU against the state-of-the-art Pri3D on ScanNet image
semantic segmentation.",None,-1
TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement,0.362689,"As demand for robotics manipulation application increases, accurate
vision-based 6D pose estimation becomes essential for autonomous operations.
Convolutional Neural Networks (CNNs) based approaches for pose estimation have
been previously introduced. However, the quest for better performance still
persists especially for accurate robotics manipulation. This quest extends to
the Agri-robotics domain. In this paper, we propose TransPose, an improved
Transformer-based 6D pose estimation with a depth refinement module. The
architecture takes in only an RGB image as input with no additional
supplementing modalities such as depth or thermal images. The architecture
encompasses an innovative lighter depth estimation network that estimates depth
from an RGB image using feature pyramid with an up-sampling method. A
transformer-based detection network with additional prediction heads is
proposed to directly regress the object's centre and predict the 6D pose of the
target. A novel depth refinement module is then used alongside the predicted
centers, 6D poses and depth patches to refine the accuracy of the estimated 6D
pose. We extensively compared our results with other state-of-the-art methods
and analysed our results for fruit-picking applications. The results we
achieved show that our proposed technique outperforms the other methods
available in the literature.",None,-1
CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models,0.611815,"We propose CHiLL (Crafting High-Level Latents), an approach for
natural-language specification of features for linear models. CHiLL prompts
LLMs with expert-crafted queries to generate interpretable features from health
records. The resulting noisy labels are then used to train a simple linear
classifier. Generating features based on queries to an LLM can empower
physicians to use their domain expertise to craft features that are clinically
meaningful for a downstream task of interest, without having to manually
extract these from raw EHR. We are motivated by a real-world risk prediction
task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and
standard predictive tasks (e.g., 30-day readmission) to evaluate this approach.
We find that linear models using automatically extracted features are
comparably performant to models using reference features, and provide greater
interpretability than linear models using ""Bag-of-Words"" features. We verify
that learned feature weights align well with clinical expectations.",None,-1
A Refutation of Shapley Values for Explainability,0.103204,"Recent work demonstrated the existence of Boolean functions for which Shapley
values provide misleading information about the relative importance of features
in rule-based explanations. Such misleading information was broadly categorized
into a number of possible issues. Each of those issues relates with features
being relevant or irrelevant for a prediction, and all are significant
regarding the inadequacy of Shapley values for rule-based explainability. This
earlier work devised a brute-force approach to identify Boolean functions,
defined on small numbers of features, and also associated instances, which
displayed such inadequacy-revealing issues, and so served as evidence to the
inadequacy of Shapley values for rule-based explainability. However, an
outstanding question is how frequently such inadequacy-revealing issues can
occur for Boolean functions with arbitrary large numbers of features. It is
plain that a brute-force approach would be unlikely to provide insights on how
to tackle this question. This paper answers the above question by proving that,
for any number of features, there exist Boolean functions that exhibit one or
more inadequacy-revealing issues, thereby contributing decisive arguments
against the use of Shapley values as the theoretical underpinning of
feature-attribution methods in explainability.",None,-1
Neural Spectro-polarimetric Fields,0.85507,"Modeling the spatial radiance distribution of light rays in a scene has been
extensively explored for applications, including view synthesis. Spectrum and
polarization, the wave properties of light, are often neglected due to their
integration into three RGB spectral bands and their non-perceptibility to human
vision. However, these properties are known to encompass substantial material
and geometric information about a scene. Here, we propose to model
spectro-polarimetric fields, the spatial Stokes-vector distribution of any
light ray at an arbitrary wavelength. We present Neural Spectro-polarimetric
Fields (NeSpoF), a neural representation that models the physically-valid
Stokes vector at given continuous variables of position, direction, and
wavelength. NeSpoF manages inherently noisy raw measurements, showcases memory
efficiency, and preserves physically vital signals - factors that are crucial
for representing the high-dimensional signal of a spectro-polarimetric field.
To validate NeSpoF, we introduce the first multi-view
hyperspectral-polarimetric image dataset, comprised of both synthetic and
real-world scenes. These were captured using our compact
hyperspectral-polarimetric imaging system, which has been calibrated for
robustness against system imperfections. We demonstrate the capabilities of
NeSpoF on diverse scenes.",None,-1
A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities,0.224911,"Cities play an important role in achieving sustainable development goals
(SDGs) to promote economic growth and meet social needs. Especially satellite
imagery is a potential data source for studying sustainable urban development.
However, a comprehensive dataset in the United States (U.S.) covering multiple
cities, multiple years, multiple scales, and multiple indicators for SDG
monitoring is lacking. To support the research on SDGs in U.S. cities, we
develop a satellite imagery dataset using deep learning models for five SDGs
containing 25 sustainable development indicators. The proposed dataset covers
the 100 most populated U.S. cities and corresponding Census Block Groups from
2014 to 2023. Specifically, we collect satellite imagery and identify objects
with state-of-the-art object detection and semantic segmentation models to
observe cities' bird's-eye view. We further gather population, nighttime light,
survey, and built environment data to depict SDGs regarding poverty, health,
education, inequality, and living environment. We anticipate the dataset to
help urban policymakers and researchers to advance SDGs-related studies,
especially applying satellite imagery to monitor long-term and multi-scale SDGs
in cities.",None,-1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,0.453255,"We present a data-driven generative framework for synthesizing blood vessel
3D geometry. This is a challenging task due to the complexity of vascular
systems, which are highly variating in shape, size, and structure. Existing
model-based methods provide some degree of control and variation in the
structures produced, but fail to capture the diversity of actual anatomical
data. We developed VesselVAE, a recursive variational Neural Network that fully
exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface. After training, the VesselVAE latent
space can be sampled to generate new vessel geometries. To the best of our
knowledge, this work is the first to utilize this technique for synthesizing
blood vessels. We achieve similarities of synthetic and real data for radius
(.97), length (.95), and tortuosity (.96). By leveraging the power of deep
neural networks, we generate 3D models of blood vessels that are both accurate
and diverse, which is crucial for medical and surgical training, hemodynamic
simulations, and many other purposes.",None,-1
Generating Realistic Images from In-the-wild Sounds,0.26564,"Representing wild sounds as images is an important but challenging task due
to the lack of paired datasets between sound and images and the significant
differences in the characteristics of these two modalities. Previous studies
have focused on generating images from sound in limited categories or music. In
this paper, we propose a novel approach to generate images from in-the-wild
sounds. First, we convert sound into text using audio captioning. Second, we
propose audio attention and sentence attention to represent the rich
characteristics of sound and visualize the sound. Lastly, we propose a direct
sound optimization with CLIPscore and AudioCLIP and generate images with a
diffusion-based model. In experiments, it shows that our model is able to
generate high quality images from wild sounds and outperforms baselines in both
quantitative and qualitative evaluations on wild audio datasets.",None,-1
Towards Local Visual Modeling for Image Captioning,0.738976,"In this paper, we study the local visual modeling with grid features for
image captioning, which is critical for generating accurate and detailed
captions. To achieve this target, we propose a Locality-Sensitive Transformer
Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention
(LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer
interaction in Transformer via modeling the relationship between each grid and
its neighbors. It reduces the difficulty of local object recognition during
captioning. LSF is used for inter-layer information fusion, which aggregates
the information of different encoder layers for cross-layer semantical
complementarity. With these two novel designs, the proposed LSTNet can model
the local visual information of grid features to improve the captioning
quality. To validate LSTNet, we conduct extensive experiments on the
competitive MS-COCO benchmark. The experimental results show that LSTNet is not
only capable of local visual modeling, but also outperforms a bunch of
state-of-the-art captioning models on offline and online testings, i.e., 134.8
CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is
also verified on the Flickr8k and Flickr30k datasets",None,-1
Collision Avoidance Detour for Multi-Agent Trajectory Forecasting,0.430642,"We present our approach, Collision Avoidance Detour (CAD), which won the 3rd
place award in the 2023 Waymo Open Dataset Challenge - Sim Agents, held at the
2023 CVPR Workshop on Autonomous Driving. To satisfy the motion prediction
factorization requirement, we partition all the valid objects into three
mutually exclusive sets: Autonomous Driving Vehicle (ADV),
World-tracks-to-predict, and World-others. We use different motion models to
forecast their future trajectories independently. Furthermore, we also apply
collision avoidance detour resampling, additive Gaussian noise, and
velocity-based heading estimation to improve the realism of our simulation
result.",None,-1
Diversity-Measurable Anomaly Detection,0.802379,"Reconstruction-based anomaly detection models achieve their purpose by
suppressing the generalization ability for anomaly. However, diverse normal
patterns are consequently not well reconstructed as well. Although some efforts
have been made to alleviate this problem by modeling sample diversity, they
suffer from shortcut learning due to undesired transmission of abnormal
information. In this paper, to better handle the tradeoff problem, we propose
Diversity-Measurable Anomaly Detection (DMAD) framework to enhance
reconstruction diversity while avoid the undesired generalization on anomalies.
To this end, we design Pyramid Deformation Module (PDM), which models diverse
normals and measures the severity of anomaly by estimating multi-scale
deformation fields from reconstructed reference to original input. Integrated
with an information compression module, PDM essentially decouples deformation
from prototypical embedding and makes the final anomaly score more reliable.
Experimental results on both surveillance videos and industrial images
demonstrate the effectiveness of our method. In addition, DMAD works equally
well in front of contaminated data and anomaly-like normal samples.",None,-1
Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,0.887438,"Semantic segmentation of point clouds in autonomous driving datasets requires
techniques that can process large numbers of points efficiently. Sparse 3D
convolutions have become the de-facto tools to construct deep neural networks
for this task: they exploit point cloud sparsity to reduce the memory and
computational loads and are at the core of today's best methods. In this paper,
we propose an alternative method that reaches the level of state-of-the-art
methods without requiring sparse convolutions. We actually show that such level
of performance is achievable by relying on tools a priori unfit for large scale
and high-performing 3D perception. In particular, we propose a novel 3D
backbone, WaffleIron, made almost exclusively of MLPs and dense 2D convolutions
and present how to train it to reach high performance on SemanticKITTI and
nuScenes. We believe that WaffleIron is a compelling alternative to backbones
using sparse 3D convolutions, especially in frameworks and on hardware where
those convolutions are not readily available.",None,-1
Regularization of polynomial networks for image recognition,0.148266,"Deep Neural Networks (DNNs) have obtained impressive performance across
tasks, however they still remain as black boxes, e.g., hard to theoretically
analyze. At the same time, Polynomial Networks (PNs) have emerged as an
alternative method with a promising performance and improved interpretability
but have yet to reach the performance of the powerful DNN baselines. In this
work, we aim to close this performance gap. We introduce a class of PNs, which
are able to reach the performance of ResNet across a range of six benchmarks.
We demonstrate that strong regularization is critical and conduct an extensive
study of the exact regularization schemes required to match performance. To
further motivate the regularization schemes, we introduce D-PolyNets that
achieve a higher-degree of expansion than previously proposed polynomial
networks. D-PolyNets are more parameter-efficient while achieving a similar
performance as other polynomial networks. We expect that our new models can
lead to an understanding of the role of elementwise activation functions (which
are no longer required for training PNs). The source code is available at
https://github.com/grigorisg9gr/regularized_polynomials.",None,-1
A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment,0.957882,"Entity alignment is the task of identifying corresponding entities across
different knowledge graphs (KGs). Although recent embedding-based entity
alignment methods have shown significant advancements, they still struggle to
fully utilize KG structural information. In this paper, we introduce FGWEA, an
unsupervised entity alignment framework that leverages the Fused
Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of
entity semantics and KG structures within a joint optimization framework. To
address the computational challenges associated with optimizing FGW, we devise
a three-stage progressive optimization algorithm. It starts with a basic
semantic embedding matching, proceeds to approximate cross-KG structural and
relational similarity matching based on iterative updates of high-confidence
entity links, and ultimately culminates in a global structural comparison
between KGs. We perform extensive experiments on four entity alignment datasets
covering 14 distinct KGs across five languages. Without any supervision or
hyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including
cutting-edge supervised entity alignment methods. Our code is available at
https://github.com/squareRoot3/FusedGW-Entity-Alignment.",None,-1
Unsupervised Learning of Robust Spectral Shape Matching,0.801059,"We propose a novel learning-based approach for robust 3D shape matching. Our
method builds upon deep functional maps and can be trained in a fully
unsupervised manner. Previous deep functional map methods mainly focus on
predicting optimised functional maps alone, and then rely on off-the-shelf
post-processing to obtain accurate point-wise maps during inference. However,
this two-stage procedure for obtaining point-wise maps often yields sub-optimal
performance. In contrast, building upon recent insights about the relation
between functional maps and point-wise maps, we propose a novel unsupervised
loss to couple the functional maps and point-wise maps, and thereby directly
obtain point-wise maps without any post-processing. Our approach obtains
accurate correspondences not only for near-isometric shapes, but also for more
challenging non-isometric shapes and partial shapes, as well as shapes with
different discretisation or topological noise. Using a total of nine diverse
datasets, we extensively evaluate the performance and demonstrate that our
method substantially outperforms previous state-of-the-art methods, even
compared to recent supervised methods. Our code is available at
https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.",None,-1
Improving Continual Relation Extraction by Distinguishing Analogous Semantics,0.628304,"Continual relation extraction (RE) aims to learn constantly emerging
relations while avoiding forgetting the learned relations. Existing works store
a small number of typical samples to re-train the model for alleviating
forgetting. However, repeatedly replaying these samples may cause the
overfitting problem. We conduct an empirical study on existing works and
observe that their performance is severely affected by analogous relations. To
address this issue, we propose a novel continual extraction model for analogous
relations. Specifically, we design memory-insensitive relation prototypes and
memory augmentation to overcome the overfitting problem. We also introduce
integrated training and focal knowledge distillation to enhance the performance
on analogous relations. Experimental results show the superiority of our model
and demonstrate its effectiveness in distinguishing analogous relations and
overcoming overfitting.",None,-1
Synthcity: facilitating innovative use cases of synthetic data in different data modalities,0.796039,"Synthcity is an open-source software package for innovative use cases of
synthetic data in ML fairness, privacy and augmentation across diverse tabular
data modalities, including static data, regular and irregular time series, data
with censoring, multi-source data, composite data, and more. Synthcity provides
the practitioners with a single access point to cutting edge research and tools
in synthetic data. It also offers the community a playground for rapid
experimentation and prototyping, a one-stop-shop for SOTA benchmarks, and an
opportunity for extending research impact. The library can be accessed on
GitHub (https://github.com/vanderschaarlab/synthcity) and pip
(https://pypi.org/project/synthcity/). We warmly invite the community to join
the development effort by providing feedback, reporting bugs, and contributing
code.",None,-1
DDS3D: Dense Pseudo-Labels with Dynamic Threshold for Semi-Supervised 3D Object Detection,0.815199,"In this paper, we present a simple yet effective semi-supervised 3D object
detector named DDS3D. Our main contributions have two-fold. On the one hand,
different from previous works using Non-Maximal Suppression (NMS) or its
variants for obtaining the sparse pseudo labels, we propose a dense
pseudo-label generation strategy to get dense pseudo-labels, which can retain
more potential supervision information for the student network. On the other
hand, instead of traditional fixed thresholds, we propose a dynamic threshold
manner to generate pseudo-labels, which can guarantee the quality and quantity
of pseudo-labels during the whole training process. Benefiting from these two
components, our DDS3D outperforms the state-of-the-art semi-supervised 3d
object detection with mAP of 3.1% on the pedestrian and 2.1% on the cyclist
under the same configuration of 1% samples. Extensive ablation studies on the
KITTI dataset demonstrate the effectiveness of our DDS3D. The code and models
will be made publicly available at https://github.com/hust-jy/DDS3D",None,-1
NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online,0.0790606,"Online texts with toxic content are a threat in social media that might cause
cyber harassment. Although many platforms applied measures, such as machine
learning-based hate-speech detection systems, to diminish their effect, those
toxic content publishers can still evade the system by modifying the spelling
of toxic words. Those modified words are also known as human-written text
perturbations. Many research works developed certain techniques to generate
adversarial samples to help the machine learning models obtain the ability to
recognize those perturbations. However, there is still a gap between those
machine-generated perturbations and human-written perturbations. In this paper,
we introduce a benchmark test set containing human-written perturbations online
for toxic speech detection models. We also recruited a group of workers to
evaluate the quality of this test set and dropped low-quality samples.
Meanwhile, to check if our perturbation can be normalized to its clean version,
we applied spell corrector algorithms on this dataset. Finally, we test this
data on state-of-the-art language models, such as BERT and RoBERTa, and black
box APIs, such as perspective API, to demonstrate the adversarial attack with
real human-written perturbations is still effective.",None,-1
Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems,0.412486,"Transferring the knowledge of large language models (LLMs) is a promising
technique to incorporate linguistic knowledge into end-to-end automatic speech
recognition (ASR) systems. However, existing works only transfer a single
representation of LLM (e.g. the last layer of pretrained BERT), while the
representation of a text is inherently non-unique and can be obtained variously
from different layers, contexts and models. In this work, we explore a wide
range of techniques to obtain and transfer multiple representations of LLMs
into a transducer-based ASR system. While being conceptually simple, we show
that transferring multiple representations of LLMs can be an effective
alternative to transferring only a single representation.",None,-1
DreamEditor: Text-Driven 3D Scene Editing with Neural Fields,0.976824,"Neural fields have achieved impressive advancements in view synthesis and
scene reconstruction. However, editing these neural fields remains challenging
due to the implicit encoding of geometry and texture information. In this
paper, we propose DreamEditor, a novel framework that enables users to perform
controlled editing of neural fields using text prompts. By representing scenes
as mesh-based neural fields, DreamEditor allows localized editing within
specific regions. DreamEditor utilizes the text encoder of a pretrained
text-to-Image diffusion model to automatically identify the regions to be
edited based on the semantics of the text prompts. Subsequently, DreamEditor
optimizes the editing region and aligns its geometry and texture with the text
prompts through score distillation sampling [29]. Extensive experiments have
demonstrated that DreamEditor can accurately edit neural fields of real-world
scenes according to the given text prompts while ensuring consistency in
irrelevant areas. DreamEditor generates highly realistic textures and geometry,
significantly surpassing previous works in both quantitative and qualitative
evaluations.",None,-1
Pure Monte Carlo Counterfactual Regret Minimization,0.859653,"Counterfactual Regret Minimization (CFR) and its variants are the best
algorithms so far for solving large-scale incomplete information games.
However, we believe that there are two problems with CFR: First, matrix
multiplication is required in CFR iteration, and the time complexity of one
iteration is too high; Secondly, the game characteristics in the real world are
different. Just using one CFR algorithm will not be perfectly suitable for all
game problems.
  For these two problems, this paper proposes a new algorithm called Pure CFR
(PCFR) based on CFR. PCFR can be seen as a combination of CFR and Fictitious
Play (FP), inheriting the concept of counterfactual regret (value) from CFR,
and using the best response strategy instead of the regret matching strategy
for the next iteration. This algorithm has three advantages. First, PCFR can be
combined with any CFR variant. The resulting Pure MCCFR (PMCCFR) can
significantly reduce the time and space complexity of one iteration. Secondly,
our experiments show that the convergence speed of the PMCCFR is 2$\sim$3 times
that of the MCCFR. Finally, there is a type of game that is very suitable for
PCFR. We call this type of game clear-game, which is characterized by a high
proportion of dominated strategies. Experiments show that in clear-game, the
convergence rate of PMCCFR is two orders of magnitude higher than that of
MCCFR.",None,-1
"Mathematics, word problems, common sense, and artificial intelligence",0.878225,"The paper discusses the capacities and limitations of current artificial
intelligence (AI) technology to solve word problems that combine elementary
knowledge with commonsense reasoning. No existing AI systems can solve these
reliably. We review three approaches that have been developed, using AI natural
language technology: outputting the answer directly, outputting a computer
program that solves the problem, and outputting a formalized representation
that can be input to an automated theorem verifier. We review some benchmarks
that have been developed to evaluate these systems and some experimental
studies. We discuss the limitations of the existing technology at solving these
kinds of problems. We argue that it is not clear whether these kinds of
limitations will be important in developing AI technology for pure mathematical
research, but that they will be important in applications of mathematics, and
may well be important in developing programs capable of reading and
understanding mathematical content written by humans.",None,-1
Evaluating Transformer's Ability to Learn Mildly Context-Sensitive Languages,0.0936313,"Despite the fact that Transformers perform well in NLP tasks, recent studies
suggest that self-attention is theoretically limited in learning even some
regular and context-free languages. These findings motivated us to think about
their implications in modeling natural language, which is hypothesized to be
mildly context-sensitive. We test the Transformer's ability to learn mildly
context-sensitive languages of varying complexities, and find that they
generalize well to unseen in-distribution data, but their ability to
extrapolate to longer strings is worse than that of LSTMs. Our analyses show
that the learned self-attention patterns and representations modeled dependency
relations and demonstrated counting behavior, which may have helped the models
solve the languages.",None,-1
Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-shot Learning with Hyperspherical Embeddings,0.296712,"Distance-based classification is frequently used in transductive few-shot
learning (FSL). However, due to the high-dimensionality of image
representations, FSL classifiers are prone to suffer from the hubness problem,
where a few points (hubs) occur frequently in multiple nearest neighbour lists
of other points. Hubness negatively impacts distance-based classification when
hubs from one class appear often among the nearest neighbors of points from
another class, degrading the classifier's performance. To address the hubness
problem in FSL, we first prove that hubness can be eliminated by distributing
representations uniformly on the hypersphere. We then propose two new
approaches to embed representations on the hypersphere, which we prove optimize
a tradeoff between uniformity and local similarity preservation -- reducing
hubness while retaining class structure. Our experiments show that the proposed
methods reduce hubness, and significantly improves transductive FSL accuracy
for a wide range of classifiers.",None,-1
Retrosynthetic Planning with Dual Value Networks,0.638891,"Retrosynthesis, which aims to find a route to synthesize a target molecule
from commercially available starting materials, is a critical task in drug
discovery and materials design. Recently, the combination of ML-based
single-step reaction predictors with multi-step planners has led to promising
results. However, the single-step predictors are mostly trained offline to
optimize the single-step accuracy, without considering complete routes. Here,
we leverage reinforcement learning (RL) to improve the single-step predictor,
by using a tree-shaped MDP to optimize complete routes. Specifically, we
propose a novel online training algorithm, called Planning with Dual Value
Networks (PDVN), which alternates between the planning phase and updating
phase. In PDVN, we construct two separate value networks to predict the
synthesizability and cost of molecules, respectively. To maintain the
single-step accuracy, we design a two-branch network structure for the
single-step predictor. On the widely-used USPTO dataset, our PDVN algorithm
improves the search success rate of existing multi-step planners (e.g.,
increasing the success rate from 85.79% to 98.95% for Retro*, and reducing the
number of model calls by half while solving 99.47% molecules for RetroGraph).
Additionally, PDVN helps find shorter synthesis routes (e.g., reducing the
average route length from 5.76 to 4.83 for Retro*, and from 5.63 to 4.78 for
RetroGraph). Our code is available at \url{https://github.com/DiXue98/PDVN}.",None,-1
Strategize Before Teaching: A Conversational Tutoring System with Pedagogy Self-Distillation,0.728035,"Conversational tutoring systems (CTSs) aim to help students master
educational material with natural language interaction in the form of a dialog.
CTSs have become a key pillar in educational data mining research. A key
challenge in CTSs is to engage the student in the conversation while exposing
them to a diverse set of teaching strategies, akin to a human teacher, thereby,
helping them learn in the process. Different from previous work that generates
responses given the strategies as input, we propose to jointly predict teaching
strategies and generate tutor responses accordingly, which fits a more
realistic application scenario. We benchmark several competitive models on
three dialog tutoring datasets and propose a unified framework that combines
teaching response generation and pedagogical strategy prediction, where a
self-distillation mechanism is adopted to guide the teaching strategy learning
and facilitate tutor response generation. Our experiments and analyses shed
light on how teaching strategies affect dialog tutoring.",None,-1
Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis,0.702697,"Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various
fine-grained sentiment analysis tasks on a target domain by transferring
knowledge from a source domain. Since labeled data only exists in the source
domain, a model is expected to bridge the domain gap for tackling cross-domain
ABSA. Though domain adaptation methods have proven to be effective, most of
them are based on a discriminative model, which needs to be specifically
designed for different ABSA tasks. To offer a more general solution, we propose
a unified bidirectional generative framework to tackle various cross-domain
ABSA tasks. Specifically, our framework trains a generative model in both
text-to-label and label-to-text directions. The former transforms each task
into a unified format to learn domain-agnostic features, and the latter
generates natural sentences from noisy labels for data augmentation, with which
a more accurate model can be trained. To investigate the effectiveness and
generality of our framework, we conduct extensive experiments on four
cross-domain ABSA tasks and present new state-of-the-art results on all tasks.
Our data and code are publicly available at
\url{https://github.com/DAMO-NLP-SG/BGCA}.",None,-1
"Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs",0.134282,"We address the integration of storytelling and Large Language Models (LLMs)
to develop engaging and believable Social Chatbots (SCs) in community settings.
Motivated by the potential of fictional characters to enhance social
interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept
of story engineering to transform fictional game characters into ""live"" social
entities within player communities. Our story engineering process includes
three steps: (1) Character and story creation, defining the SC's personality
and worldview, (2) Presenting Live Stories to the Community, allowing the SC to
recount challenges and seek suggestions, and (3) Communication with community
members, enabling interaction between the SC and users. We employed the LLM
GPT-3 to drive our SSC prototypes, ""David"" and ""Catherine,"" and evaluated their
performance in an online gaming community, ""DE (Alias),"" on Discord. Our
mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with
community members, reveals that storytelling significantly enhances the
engagement and believability of SCs in community settings.",None,-1
Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering,0.510248,"The nodes in the commonsense knowledge graph (CSKG) are normally represented
by free-form short text (e.g., word or phrase). Different nodes may represent
the same concept. This leads to the problems of edge sparsity and node
redundancy, which challenges CSKG representation and completion. On the one
hand, edge sparsity limits the performance of graph representation learning; On
the other hand, node redundancy makes different nodes corresponding to the same
concept have inconsistent relations with other nodes. To address the two
problems, we propose a new CSKG completion framework based on Contrastive
Pretraining and Node Clustering (CPNC). Contrastive Pretraining constructs
positive and negative head-tail node pairs on CSKG and utilizes contrastive
learning to obtain better semantic node representation. Node Clustering
aggregates nodes with the same concept into a latent concept, assisting the
task of CSKG completion. We evaluate our CPNC approach on two CSKG completion
benchmarks (CN-100K and ATOMIC), where CPNC outperforms the state-of-the-art
methods. Extensive experiments demonstrate that both Contrastive Pretraining
and Node Clustering can significantly improve the performance of CSKG
completion. The source code of CPNC is publicly available on
\url{https://github.com/NUSTM/CPNC}.",None,-1
Mimetic Initialization of Self-Attention Layers,0.61913,"It is notoriously difficult to train Transformers on small datasets;
typically, large pre-trained models are instead used as the starting point. We
explore the weights of such pre-trained Transformers (particularly for vision)
to attempt to find reasons for this discrepancy. Surprisingly, we find that
simply initializing the weights of self-attention layers so that they ""look""
more like their pre-trained counterparts allows us to train vanilla
Transformers faster and to higher final accuracies, particularly on vision
tasks such as CIFAR-10 and ImageNet classification, where we see gains in
accuracy of over 5% and 4%, respectively. Our initialization scheme is closed
form, learning-free, and very simple: we set the product of the query and key
weights to be approximately the identity, and the product of the value and
projection weights to approximately the negative identity. As this mimics the
patterns we saw in pre-trained Transformers, we call the technique ""mimetic
initialization"".",None,-1
Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie,0.585334,"Accurate neural models are much less efficient than non-neural models and are
useless for processing billions of social media posts or handling user queries
in real time with a limited budget. This study revisits the fastest
pattern-based NLP methods to make them as accurate as possible, thus yielding a
strikingly simple yet surprisingly accurate morphological analyzer for
Japanese. The proposed method induces reliable patterns from a morphological
dictionary and annotated data. Experimental results on two standard datasets
confirm that the method exhibits comparable accuracy to learning-based
baselines, while boasting a remarkable throughput of over 1,000,000 sentences
per second on a single modern CPU. The source code is available at
https://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jagger/",None,-1
Expanding Scope: Adapting English Adversarial Attacks to Chinese,0.248114,"Recent studies have revealed that NLP predictive models are vulnerable to
adversarial attacks. Most existing studies focused on designing attacks to
evaluate the robustness of NLP models in the English language alone. Literature
has seen an increasing need for NLP solutions for other languages. We,
therefore, ask one natural question: whether state-of-the-art (SOTA) attack
methods generalize to other languages. This paper investigates how to adapt
SOTA adversarial attack algorithms in English to the Chinese language. Our
experiments show that attack methods previously applied to English NLP can
generate high-quality adversarial examples in Chinese when combined with proper
text segmentation and linguistic constraints. In addition, we demonstrate that
the generated adversarial examples can achieve high fluency and semantic
consistency by focusing on the Chinese language's morphology and phonology,
which in turn can be used to improve the adversarial robustness of Chinese NLP
models.",None,-1
Consistent Multimodal Generation via A Unified GAN Framework,0.0459328,"We investigate how to generate multimodal image outputs, such as RGB, depth,
and surface normals, with a single generative model. The challenge is to
produce outputs that are realistic, and also consistent with each other. Our
solution builds on the StyleGAN3 architecture, with a shared backbone and
modality-specific branches in the last layers of the synthesis network, and we
propose per-modality fidelity discriminators and a cross-modality consistency
discriminator. In experiments on the Stanford2D3D dataset, we demonstrate
realistic and consistent generation of RGB, depth, and normal images. We also
show a training recipe to easily extend our pretrained model on a new domain,
even with a few pairwise data. We further evaluate the use of synthetically
generated RGB and depth pairs for training or fine-tuning depth estimators.
Code will be available at https://github.com/jessemelpolio/MultimodalGAN.",None,-1
Hyper-Decision Transformer for Efficient Online Policy Adaptation,0.965957,"Decision Transformers (DT) have demonstrated strong performances in offline
reinforcement learning settings, but quickly adapting to unseen novel tasks
remains challenging. To address this challenge, we propose a new framework,
called Hyper-Decision Transformer (HDT), that can generalize to novel tasks
from a handful of demonstrations in a data- and parameter-efficient manner. To
achieve such a goal, we propose to augment the base DT with an adaptation
module, whose parameters are initialized by a hyper-network. When encountering
unseen tasks, the hyper-network takes a handful of demonstrations as inputs and
initializes the adaptation module accordingly. This initialization enables HDT
to efficiently adapt to novel tasks by only fine-tuning the adaptation module.
We validate HDT's generalization capability on object manipulation tasks. We
find that with a single expert demonstration and fine-tuning only 0.5% of DT
parameters, HDT adapts faster to unseen tasks than fine-tuning the whole DT
model. Finally, we explore a more challenging setting where expert actions are
not available, and we show that HDT outperforms state-of-the-art baselines in
terms of task success rates by a large margin.",None,-1
D-Separation for Causal Self-Explanation,0.412553,"Rationalization is a self-explaining framework for NLP models. Conventional
work typically uses the maximum mutual information (MMI) criterion to find the
rationale that is most indicative of the target label. However, this criterion
can be influenced by spurious features that correlate with the causal rationale
or the target label. Instead of attempting to rectify the issues of the MMI
criterion, we propose a novel criterion to uncover the causal rationale, termed
the Minimum Conditional Dependence (MCD) criterion, which is grounded on our
finding that the non-causal features and the target label are
\emph{d-separated} by the causal rationale. By minimizing the dependence
between the unselected parts of the input and the target label conditioned on
the selected rationale candidate, all the causes of the label are compelled to
be selected. In this study, we employ a simple and practical measure of
dependence, specifically the KL-divergence, to validate our proposed MCD
criterion. Empirically, we demonstrate that MCD improves the F1 score by up to
$13.7\%$ compared to previous state-of-the-art MMI-based methods. Our code is
available at: \url{https://github.com/jugechengzi/Rationalization-MCD}.",None,-1
Interpretable Deep Learning for Forecasting Online Advertising Costs: Insights from the Competitive Bidding Landscape,0.317826,"As advertisers increasingly shift their budgets toward digital advertising,
forecasting advertising costs is essential for making budget plans to optimize
marketing campaign returns. In this paper, we perform a comprehensive study
using a variety of time-series forecasting methods to predict daily average
cost-per-click (CPC) in the online advertising market. We show that forecasting
advertising costs would benefit from multivariate models using covariates from
competitors' CPC development identified through time-series clustering. We
further interpret the results by analyzing feature importance and temporal
attention. Finally, we show that our approach has several advantages over
models that individual advertisers might build based solely on their collected
data.",None,-1
MG-GNN: Multigrid Graph Neural Networks for Learning Multilevel Domain Decomposition Methods,0.616566,"Domain decomposition methods (DDMs) are popular solvers for discretized
systems of partial differential equations (PDEs), with one-level and multilevel
variants. These solvers rely on several algorithmic and mathematical
parameters, prescribing overlap, subdomain boundary conditions, and other
properties of the DDM. While some work has been done on optimizing these
parameters, it has mostly focused on the one-level setting or special cases
such as structured-grid discretizations with regular subdomain construction. In
this paper, we propose multigrid graph neural networks (MG-GNN), a novel GNN
architecture for learning optimized parameters in two-level DDMs\@. We train
MG-GNN using a new unsupervised loss function, enabling effective training on
small problems that yields robust performance on unstructured grids that are
orders of magnitude larger than those in the training set. We show that MG-GNN
outperforms popular hierarchical graph network architectures for this
optimization and that our proposed loss function is critical to achieving this
improved performance.",None,-1
Multi Modal Facial Expression Recognition with Transformer-Based Fusion Networks and Dynamic Sampling,0.770541,"Facial expression recognition is an essential task for various applications,
including emotion detection, mental health analysis, and human-machine
interactions. In this paper, we propose a multi-modal facial expression
recognition method that exploits audio information along with facial images to
provide a crucial clue to differentiate some ambiguous facial expressions.
Specifically, we introduce a Modal Fusion Module (MFM) to fuse audio-visual
information, where image and audio features are extracted from Swin
Transformer. Additionally, we tackle the imbalance problem in the dataset by
employing dynamic data resampling. Our model has been evaluated in the
Affective Behavior in-the-wild (ABAW) challenge of CVPR 2023.",None,-1
PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction,0.73799,"In the era of information explosion, spatio-temporal data mining serves as a
critical part of urban management. Considering the various fields demanding
attention, e.g., traffic state, human activity, and social event, predicting
multiple spatio-temporal attributes simultaneously can alleviate regulatory
pressure and foster smart city construction. However, current research can not
handle the spatio-temporal multi-attribute prediction well due to the complex
relationships between diverse attributes. The key challenge lies in how to
address the common spatio-temporal patterns while tackling their distinctions.
In this paper, we propose an effective solution for spatio-temporal
multi-attribute prediction, PromptST. We devise a spatio-temporal transformer
and a parameter-sharing training scheme to address the common knowledge among
different spatio-temporal attributes. Then, we elaborate a spatio-temporal
prompt tuning strategy to fit the specific attributes in a lightweight manner.
Through the pretrain and prompt tuning phases, our PromptST is able to enhance
the specific spatio-temoral characteristic capture by prompting the backbone
model to fit the specific target attribute while maintaining the learned common
knowledge. Extensive experiments on real-world datasets verify that our
PromptST attains state-of-the-art performance. Furthermore, we also prove
PromptST owns good transferability on unseen spatio-temporal attributes, which
brings promising application potential in urban computing. The implementation
code is available to ease reproducibility.",None,-1
Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization,0.473286,"Prompt tuning is one of the successful approaches for parameter-efficient
tuning of pre-trained language models. Despite being arguably the most
parameter-efficient (tuned soft prompts constitute <0.1% of total parameters),
it typically performs worse than other efficient tuning methods and is quite
sensitive to hyper-parameters. In this work, we introduce Residual Prompt
Tuning - a simple and efficient method that significantly improves the
performance and stability of prompt tuning. We propose to reparameterize soft
prompt embeddings using a shallow network with a residual connection. Our
experiments show that Residual Prompt Tuning significantly outperforms prompt
tuning on SuperGLUE benchmark. Notably, our method reaches +7 points
improvement over prompt tuning with T5-Base and allows to reduce the prompt
length by 10x without hurting performance. In addition, we show that our
approach is robust to the choice of learning rate and prompt initialization,
and is effective in few-shot settings.",None,-1
CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields,0.480495,"Neural Radiance Fields (NeRF) have the potential to be a major representation
of media. Since training a NeRF has never been an easy task, the protection of
its model copyright should be a priority. In this paper, by analyzing the pros
and cons of possible copyright protection solutions, we propose to protect the
copyright of NeRF models by replacing the original color representation in NeRF
with a watermarked color representation. Then, a distortion-resistant rendering
scheme is designed to guarantee robust message extraction in 2D renderings of
NeRF. Our proposed method can directly protect the copyright of NeRF models
while maintaining high rendering quality and bit accuracy when compared among
optional solutions.",None,-1
Solving Quantum-Inspired Perfect Matching Problems via Tutte's Theorem-Based Hybrid Boolean Constraints,0.491822,"Determining the satisfiability of Boolean constraint-satisfaction problems
with different types of constraints, that is hybrid constraints, is a
well-studied problem with important applications. We study here a new
application of hybrid Boolean constraints, which arises in quantum computing.
The problem relates to constrained perfect matching in edge-colored graphs.
While general-purpose hybrid constraint solvers can be powerful, we show that
direct encodings of the constrained-matching problem as hybrid constraints
scale poorly and special techniques are still needed. We propose a novel
encoding based on Tutte's Theorem in graph theory as well as optimization
techniques. Empirical results demonstrate that our encoding, in suitable
languages with advanced SAT solvers, scales significantly better than a number
of competing approaches on constrained-matching benchmarks. Our study
identifies the necessity of designing problem-specific encodings when applying
powerful general-purpose constraint solvers.",None,-1
A Vision for Semantically Enriched Data Science,0.0577402,"The recent efforts in automation of machine learning or data science has
achieved success in various tasks such as hyper-parameter optimization or model
selection. However, key areas such as utilizing domain knowledge and data
semantics are areas where we have seen little automation. Data Scientists have
long leveraged common sense reasoning and domain knowledge to understand and
enrich data for building predictive models. In this paper we discuss important
shortcomings of current data science and machine learning solutions. We then
envision how leveraging ""semantic"" understanding and reasoning on data in
combination with novel tools for data science automation can help with
consistent and explainable data augmentation and transformation. Additionally,
we discuss how semantics can assist data scientists in a new manner by helping
with challenges related to trust, bias, and explainability in machine learning.
Semantic annotation can also help better explore and organize large data
sources.",None,-1
Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks,0.999999,"Intuitive psychology is a pillar of common-sense reasoning. The replication
of this reasoning in machine intelligence is an important stepping-stone on the
way to human-like artificial intelligence. Several recent tasks and benchmarks
for examining this reasoning in Large-Large Models have focused in particular
on belief attribution in Theory-of-Mind tasks. These tasks have shown both
successes and failures. We consider in particular a recent purported success
case, and show that small variations that maintain the principles of ToM turn
the results on their head. We argue that in general, the zero-hypothesis for
model evaluation in intuitive psychology should be skeptical, and that outlying
failure cases should outweigh average success rates. We also consider what
possible future successes on Theory-of-Mind tasks by more powerful LLMs would
mean for ToM tasks with people.",None,-1
Benchmarking the Robustness of Quantized Models,0.171419,"Quantization has emerged as an essential technique for deploying deep neural
networks (DNNs) on devices with limited resources. However, quantized models
exhibit vulnerabilities when exposed to various noises in real-world
applications. Despite the importance of evaluating the impact of quantization
on robustness, existing research on this topic is limited and often disregards
established principles of robustness evaluation, resulting in incomplete and
inconclusive findings. To address this gap, we thoroughly evaluated the
robustness of quantized models against various noises (adversarial attacks,
natural corruptions, and systematic noises) on ImageNet. Extensive experiments
demonstrate that lower-bit quantization is more resilient to adversarial
attacks but is more susceptible to natural corruptions and systematic noises.
Notably, our investigation reveals that impulse noise (in natural corruptions)
and the nearest neighbor interpolation (in systematic noises) have the most
significant impact on quantized models. Our research contributes to advancing
the robust quantization of models and their deployment in real-world scenarios.",None,-1
Strivec: Sparse Tri-Vector Radiance Fields,0.83269,"We propose Strivec, a novel neural representation that models a 3D scene as a
radiance field with sparsely distributed and compactly factorized local tensor
feature grids. Our approach leverages tensor decomposition, following the
recent work TensoRF, to model the tensor grids. In contrast to TensoRF which
uses a global tensor and focuses on their vector-matrix decomposition, we
propose to utilize a cloud of local tensors and apply the classic
CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple
vectors that express local feature distributions along spatial axes and
compactly encode a local neural field. We also apply multi-scale tensor grids
to discover the geometry and appearance commonalities and exploit spatial
coherence with the tri-vector factorization at multiple local scales. The final
radiance field properties are regressed by aggregating neural features from
multiple local tensors across all scales. Our tri-vector tensors are sparsely
distributed around the actual scene surface, discovered by a fast coarse
reconstruction, leveraging the sparsity of a 3D scene. We demonstrate that our
model can achieve better rendering quality while using significantly fewer
parameters than previous methods, including TensoRF and Instant-NGP.",None,-1
Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences,0.0766216,"Pre-trained language models have led to a new state-of-the-art in many NLP
tasks. However, for topic modeling, statistical generative models such as LDA
are still prevalent, which do not easily allow incorporating contextual word
vectors. They might yield topics that do not align well with human judgment. In
this work, we propose a novel topic modeling and inference algorithm. We
suggest a bag of sentences (BoS) approach using sentences as the unit of
analysis. We leverage pre-trained sentence embeddings by combining generative
process models and clustering. We derive a fast inference algorithm based on
expectation maximization, hard assignments, and an annealing process. The
evaluation shows that our method yields state-of-the art results with
relatively little computational demands. Our method is also more flexible
compared to prior works leveraging word embeddings, since it provides the
possibility to customize topic-document distributions using priors. Code and
data is at \url{https://github.com/JohnTailor/BertSenClu}.",None,-1
Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization,0.321263,"In domain generalization (DG), the target domain is unknown when the model is
being trained, and the trained model should successfully work on an arbitrary
(and possibly unseen) target domain during inference. This is a difficult
problem, and despite active studies in recent years, it remains a great
challenge. In this paper, we take a simple yet effective approach to tackle
this issue. We propose test-time style shifting, which shifts the style of the
test sample (that has a large style gap with the source domains) to the nearest
source domain that the model is already familiar with, before making the
prediction. This strategy enables the model to handle any target domains with
arbitrary style statistics, without additional model update at test-time.
Additionally, we propose style balancing, which provides a great platform for
maximizing the advantage of test-time style shifting by handling the
DG-specific imbalance issues. The proposed ideas are easy to implement and
successfully work in conjunction with various other DG schemes. Experimental
results on different datasets show the effectiveness of our methods.",None,-1
Sampling to Distill: Knowledge Transfer from Open-World Data,0.601225,"Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train
high-performance student models using only the teacher network without original
training data. Despite encouraging results, existing DFKD methods rely heavily
on generation modules with high computational costs. Meanwhile, they ignore the
fact that the generated and original data exist domain shifts due to the lack
of supervision information. Moreover, knowledge is transferred through each
example, ignoring the implicit relationship among multiple examples. To this
end, we propose a novel Open-world Data Sampling Distillation (ODSD) method
without a redundant generation process. First, we try to sample open-world data
close to the original data's distribution by an adaptive sampling module. Then,
we introduce a low-noise representation to alleviate the domain shifts and
build a structured relationship of multiple data examples to exploit data
knowledge. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet
show that our ODSD method achieves state-of-the-art performance. Especially, we
improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with the
existing results.",None,-1
Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving,0.790069,"In recent years there have been remarkable advancements in autonomous
driving. While autonomous vehicles demonstrate high performance in closed-set
conditions, they encounter difficulties when confronted with unexpected
situations. At the same time, world models emerged in the field of model-based
reinforcement learning as a way to enable agents to predict the future
depending on potential actions. This led to outstanding results in sparse
reward and complex control tasks. This work provides an overview of how world
models can be leveraged to perform anomaly detection in the domain of
autonomous driving. We provide a characterization of world models and relate
individual components to previous works in anomaly detection to facilitate
further research in the field.",None,-1
Improving Fairness in Adaptive Social Exergames via Shapley Bandits,0.356117,"Algorithmic fairness is an essential requirement as AI becomes integrated in
society. In the case of social applications where AI distributes resources,
algorithms often must make decisions that will benefit a subset of users,
sometimes repeatedly or exclusively, while attempting to maximize specific
outcomes. How should we design such systems to serve users more fairly? This
paper explores this question in the case where a group of users works toward a
shared goal in a social exergame called Step Heroes. We identify adverse
outcomes in traditional multi-armed bandits (MABs) and formalize the Greedy
Bandit Problem. We then propose a solution based on a new type of
fairness-aware multi-armed bandit, Shapley Bandits. It uses the Shapley Value
for increasing overall player participation and intervention adherence rather
than the maximization of total group output, which is traditionally achieved by
favoring only high-performing participants. We evaluate our approach via a user
study (n=46). Our results indicate that our Shapley Bandits effectively
mediates the Greedy Bandit Problem and achieves better user retention and
motivation across the participants.",None,-1
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,0.0735987,"Language models are typically evaluated on their success at predicting the
distribution of specific words in specific contexts. Yet linguistic knowledge
also encodes relationships between contexts, allowing inferences between word
distributions. We investigate the degree to which pre-trained Transformer-based
large language models (LLMs) represent such relationships, focusing on the
domain of argument structure. We find that LLMs perform well in generalizing
the distribution of a novel noun argument between related contexts that were
seen during pre-training (e.g., the active object and passive subject of the
verb spray), succeeding by making use of the semantically-organized structure
of the embedding space for word embeddings. However, LLMs fail at
generalizations between related contexts that have not been observed during
pre-training, but which instantiate more abstract, but well-attested structural
generalizations (e.g., between the active object and passive subject of an
arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on
linear order. This finding points to a limitation with current models and
points to a reason for which their training is data-intensive.s reported here
are available at https://github.com/clay-lab/structural-alternations.",None,-1
DeDrift: Robust Similarity Search under Content Drift,0.811387,"The statistical distribution of content uploaded and searched on media
sharing sites changes over time due to seasonal, sociological and technical
factors. We investigate the impact of this ""content drift"" for large-scale
similarity search tools, based on nearest neighbor search in embedding space.
Unless a costly index reconstruction is performed frequently, content drift
degrades the search accuracy and efficiency. The degradation is especially
severe since, in general, both the query and database distributions change.
  We introduce and analyze real-world image and video datasets for which
temporal information is available over a long time period. Based on the
learnings, we devise DeDrift, a method that updates embedding quantizers to
continuously adapt large-scale indexing structures on-the-fly. DeDrift almost
eliminates the accuracy degradation due to the query and database content drift
while being up to 100x faster than a full index reconstruction.",None,-1
Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction,0.782246,"Relation extraction (RE) aims to extract potential relations according to the
context of two entities, thus, deriving rational contexts from sentences plays
an important role. Previous works either focus on how to leverage the entity
information (e.g., entity types, entity verbalization) to inference relations,
but ignore context-focused content, or use counterfactual thinking to remove
the model's bias of potential relations in entities, but the relation reasoning
process will still be hindered by irrelevant content. Therefore, how to
preserve relevant content and remove noisy segments from sentences is a crucial
task. In addition, retained content needs to be fluent enough to maintain
semantic coherence and interpretability. In this work, we propose a novel
rationale extraction framework named RE2, which leverages two continuity and
sparsity factors to obtain relevant and coherent rationales from sentences. To
solve the problem that the gold rationales are not labeled, RE2 applies an
optimizable binary mask to each token in the sentence, and adjust the
rationales that need to be selected according to the relation label.
Experiments on four datasets show that RE2 surpasses baselines.",None,-1
Learning Disentangled Representation with Mutual Information Maximization for Real-Time UAV Tracking,0.308766,"Efficiency has been a critical problem in UAV tracking due to limitations in
computation resources, battery capacity, and unmanned aerial vehicle maximum
load. Although discriminative correlation filters (DCF)-based trackers prevail
in this field for their favorable efficiency, some recently proposed
lightweight deep learning (DL)-based trackers using model compression
demonstrated quite remarkable CPU efficiency as well as precision.
Unfortunately, the model compression methods utilized by these works, though
simple, are still unable to achieve satisfying tracking precision with higher
compression rates. This paper aims to exploit disentangled representation
learning with mutual information maximization (DR-MIM) to further improve
DL-based trackers' precision and efficiency for UAV tracking. The proposed
disentangled representation separates the feature into an identity-related and
an identity-unrelated features. Only the latter is used, which enhances the
effectiveness of the feature representation for subsequent classification and
regression tasks. Extensive experiments on four UAV benchmarks, including
UAV123@10fps, DTB70, UAVDT and VisDrone2018, show that our DR-MIM tracker
significantly outperforms state-of-the-art UAV tracking methods.",None,-1
UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective,0.33026,"We propose a new paradigm for universal information extraction (IE) that is
compatible with any schema format and applicable to a list of IE tasks, such as
named entity recognition, relation extraction, event extraction and sentiment
analysis. Our approach converts the text-based IE tasks as the token-pair
problem, which uniformly disassembles all extraction targets into joint span
detection, classification and association problems with a unified extractive
framework, namely UniEX. UniEX can synchronously encode schema-based prompt and
textual information, and collaboratively learn the generalized knowledge from
pre-defined information using the auto-encoder language models. We develop a
traffine attention mechanism to integrate heterogeneous factors including
tasks, labels and inside tokens, and obtain the extraction target via a scoring
matrix. Experiment results show that UniEX can outperform generative universal
IE models in terms of performance and inference-speed on $14$ benchmarks IE
datasets with the supervised setting. The state-of-the-art performance in
low-resource scenarios also verifies the transferability and effectiveness of
UniEX.",None,-1
Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes,0.946003,"Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0",None,-1
InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning,0.979533,"Recent advances in personalized image generation allow a pre-trained
text-to-image model to learn a new concept from a set of images. However,
existing personalization approaches usually require heavy test-time finetuning
for each concept, which is time-consuming and difficult to scale. We propose
InstantBooth, a novel approach built upon pre-trained text-to-image models that
enables instant text-guided image personalization without any test-time
finetuning. We achieve this with several major components. First, we learn the
general concept of the input images by converting them to a textual token with
a learnable image encoder. Second, to keep the fine details of the identity, we
learn rich visual feature representation by introducing a few adapter layers to
the pre-trained model. We train our components only on text-image pairs without
using paired images of the same concept. Compared to test-time finetuning-based
methods like DreamBooth and Textual-Inversion, our model can generate
competitive results on unseen concepts concerning language-image alignment,
image fidelity, and identity preservation while being 100 times faster.",None,-1
Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach,0.198318,"Many decision-making problems feature multiple objectives. In such problems,
it is not always possible to know the preferences of a decision-maker for
different objectives. However, it is often possible to observe the behavior of
decision-makers. In multi-objective decision-making, preference inference is
the process of inferring the preferences of a decision-maker for different
objectives. This research proposes a Dynamic Weight-based Preference Inference
(DWPI) algorithm that can infer the preferences of agents acting in
multi-objective decision-making problems, based on observed behavior
trajectories in the environment. The proposed method is evaluated on three
multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item
Gathering. The performance of the proposed DWPI approach is compared to two
existing preference inference methods from the literature, and empirical
results demonstrate significant improvements compared to the baseline
algorithms, in terms of both time requirements and accuracy of the inferred
preferences. The Dynamic Weight-based Preference Inference algorithm also
maintains its performance when inferring preferences for sub-optimal behavior
demonstrations. In addition to its impressive performance, the Dynamic
Weight-based Preference Inference algorithm does not require any interactions
during training with the agent whose preferences are inferred, all that is
required is a trajectory of observed behavior.",None,-1
Taming Detection Transformers for Medical Object Detection,0.151271,"The accurate detection of suspicious regions in medical images is an
error-prone and time-consuming process required by many routinely performed
diagnostic procedures. To support clinicians during this difficult task,
several automated solutions were proposed relying on complex methods with many
hyperparameters. In this study, we investigate the feasibility of DEtection
TRansformer (DETR) models for volumetric medical object detection. In contrast
to previous works, these models directly predict a set of objects without
relying on the design of anchors or manual heuristics such as
non-maximum-suppression to detect objects. We show by conducting extensive
experiments with three models, namely DETR, Conditional DETR, and DINO DETR on
four data sets (CADA, RibFrac, KiTS19, and LIDC) that these set prediction
models can perform on par with or even better than currently existing methods.
DINO DETR, the best-performing model in our experiments demonstrates this by
outperforming a strong anchor-based one-stage detector, Retina U-Net, on three
out of four data sets.",None,-1
Principal-Agent Reward Shaping in MDPs,0.342994,"Principal-agent problems arise when one party acts on behalf of another,
leading to conflicts of interest. The economic literature has extensively
studied principal-agent problems, and recent work has extended this to more
complex scenarios such as Markov Decision Processes (MDPs). In this paper, we
further explore this line of research by investigating how reward shaping under
budget constraints can improve the principal's utility. We study a two-player
Stackelberg game where the principal and the agent have different reward
functions, and the agent chooses an MDP policy for both players. The principal
offers an additional reward to the agent, and the agent picks their policy
selfishly to maximize their reward, which is the sum of the original and the
offered reward. Our results establish the NP-hardness of the problem and offer
polynomial approximation algorithms for two classes of instances: Stochastic
trees and deterministic decision processes with a finite horizon.",None,-1
Addressing Cold Start Problem for End-to-end Automatic Speech Scoring,0.266786,"Integrating automatic speech scoring/assessment systems has become a critical
aspect of second-language speaking education. With self-supervised learning
advancements, end-to-end speech scoring approaches have exhibited promising
results. However, this study highlights the significant decrease in the
performance of speech scoring systems in new question contexts, thereby
identifying this as a cold start problem in terms of items. With the finding of
cold-start phenomena, this paper seeks to alleviate the problem by following
methods: 1) prompt embeddings, 2) question context embeddings using BERT or
CLIP models, and 3) choice of the pretrained acoustic model. Experiments are
conducted on TOEIC speaking test datasets collected from
English-as-a-second-language (ESL) learners rated by professional TOEIC
speaking evaluators. The results demonstrate that the proposed framework not
only exhibits robustness in a cold-start environment but also outperforms the
baselines for known content.",None,-1
Passive Radio Frequency-based 3D Indoor Positioning System via Ensemble Learning,0.832189,"Passive radio frequency (PRF)-based indoor positioning systems (IPS) have
attracted researchers' attention due to their low price, easy and customizable
configuration, and non-invasive design. This paper proposes a PRF-based
three-dimensional (3D) indoor positioning system (PIPS), which is able to use
signals of opportunity (SoOP) for positioning and also capture a scenario
signature. PIPS passively monitors SoOPs containing scenario signatures through
a single receiver. Moreover, PIPS leverages the Dynamic Data Driven
Applications System (DDDAS) framework to devise and customize the sampling
frequency, enabling the system to use the most impacted frequency band as the
rated frequency band. Various regression methods within three ensemble learning
strategies are used to train and predict the receiver position. The PRF
spectrum of 60 positions is collected in the experimental scenario, and three
criteria are applied to evaluate the performance of PIPS. Experimental results
show that the proposed PIPS possesses the advantages of high accuracy,
configurability, and robustness.",None,-1
Large Language Model Enhanced Multi-Agent Systems for 6G Communications,0.897881,"The rapid development of the Large Language Model (LLM) presents huge
opportunities for 6G communications, e.g., network optimization and management
by allowing users to input task requirements to LLMs by nature language.
However, directly applying native LLMs in 6G encounters various challenges,
such as a lack of private communication data and knowledge, limited logical
reasoning, evaluation, and refinement abilities. Integrating LLMs with the
capabilities of retrieval, planning, memory, evaluation and reflection in
agents can greatly enhance the potential of LLMs for 6G communications. To this
end, we propose a multi-agent system with customized communication knowledge
and tools for solving communication related tasks using natural language,
comprising three components: (1) Multi-agent Data Retrieval (MDR), which
employs the condensate and inference agents to refine and summarize
communication knowledge from the knowledge base, expanding the knowledge
boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning
(MCP), which utilizes multiple planning agents to generate feasible solutions
for the communication related task from different perspectives based on the
retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which
utilizes the evaluation agent to assess the solutions, and applies the
reflexion agent and refinement agent to provide improvement suggestions for
current solutions. Finally, we validate the effectiveness of the proposed
multi-agent system by designing a semantic communication system, as a case
study of 6G communications.",None,-1
Metacognitive threshold: a computational account,0.640985,"This paper will explore ways of computationally accounting for the
metacognitive threshold -- the minimum amount of stimulus needed for a mental
state to be perceived -- and discuss potential cognitive mechanisms by which
this threshold can be influenced through metacognitive training and meditation.",None,-1
Market-Aware Models for Efficient Cross-Market Recommendation,0.245433,"We consider the cross-market recommendation (CMR) task, which involves
recommendation in a low-resource target market using data from a richer,
auxiliary source market. Prior work in CMR utilised meta-learning to improve
recommendation performance in target markets; meta-learning however can be
complex and resource intensive. In this paper, we propose market-aware (MA)
models, which directly model a market via market embeddings instead of
meta-learning across markets. These embeddings transform item representations
into market-specific representations. Our experiments highlight the
effectiveness and efficiency of MA models both in a pairwise setting with a
single target-source market, as well as a global model trained on all markets
in unison. In the former pairwise setting, MA models on average outperform
market-unaware models in 85% of cases on nDCG@10, while being time-efficient -
compared to meta-learning models, MA models require only 15% of the training
time. In the global setting, MA models outperform market-unaware models
consistently for some markets, while outperforming meta-learning-based methods
for all but one market. We conclude that MA models are an efficient and
effective alternative to meta-learning, especially in the global setting.",None,-1
When SAM Meets Shadow Detection,0.308191,"As a promptable generic object segmentation model, segment anything model
(SAM) has recently attracted significant attention, and also demonstrates its
powerful performance. Nevertheless, it still meets its Waterloo when
encountering several tasks, e.g., medical image segmentation, camouflaged
object detection, etc. In this report, we try SAM on an unexplored popular
task: shadow detection. Specifically, four benchmarks were chosen and evaluated
with widely used metrics. The experimental results show that the performance
for shadow detection using SAM is not satisfactory, especially when comparing
with the elaborate models. Code is available at
https://github.com/LeipingJie/SAMSh.",None,-1
Event Camera Data Pre-training,0.879907,"This paper proposes a pre-trained neural network for handling event camera
data. Our model is a self-supervised learning framework, and uses paired event
camera data and natural RGB images for training.
  Our method contains three modules connected in a sequence: i) a family of
event data augmentations, generating meaningful event images for
self-supervised training; ii) a conditional masking strategy to sample
informative event patches from event images, encouraging our model to capture
the spatial layout of a scene and accelerating training; iii) a contrastive
learning approach, enforcing the similarity of embeddings between matching
event images, and between paired event and RGB images. An embedding projection
loss is proposed to avoid the model collapse when enforcing the event image
embedding similarities. A probability distribution alignment loss is proposed
to encourage the event image to be consistent with its paired RGB image in the
feature space.
  Transfer learning performance on downstream tasks shows the superiority of
our method over state-of-the-art methods. For example, we achieve top-1
accuracy at 64.83% on the N-ImageNet dataset.",None,-1
Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective,0.110063,"The COVID-19 pandemic has undoubtedly changed the standards and affected all
aspects of our lives, especially social communication. It has forced people to
extensively wear medical face masks, in order to prevent transmission. This
face occlusion can strongly irritate emotional reading from the face and urges
us to incorporate the whole body as an emotional cue. In this paper, we conduct
insightful studies about the effect of face occlusion on emotion recognition
performance, and showcase the superiority of full body input over the plain
masked face. We utilize a deep learning model based on the Temporal Segment
Network framework, and aspire to fully overcome the face mask consequences.
Although facial and bodily features can be learned from a single input, this
may lead to irrelevant information confusion. By processing those features
separately and fusing their prediction scores, we are more effectively taking
advantage of both modalities. This framework also naturally supports temporal
modeling, by mingling information among neighboring frames. In combination,
these techniques form an effective system capable of tackling emotion
recognition difficulties, caused by safety protocols applied in crucial areas.",None,-1
Challenges in Domain-Specific Abstractive Summarization and How to Overcome them,0.568393,"Large Language Models work quite well with general-purpose data and many
tasks in Natural Language Processing. However, they show several limitations
when used for a task such as domain-specific abstractive text summarization.
This paper identifies three of those limitations as research problems in the
context of abstractive text summarization: 1) Quadratic complexity of
transformer-based models with respect to the input text length; 2) Model
Hallucination, which is a model's ability to generate factually incorrect text;
and 3) Domain Shift, which happens when the distribution of the model's
training and test corpus is not the same. Along with a discussion of the open
research questions, this paper also provides an assessment of existing
state-of-the-art techniques relevant to domain-specific text summarization to
address the research gaps.",None,-1
CLIP-GCD: Simple Language Guided Generalized Category Discovery,0.509399,"Generalized Category Discovery (GCD) requires a model to both classify known
categories and cluster unknown categories in unlabeled data. Prior methods
leveraged self-supervised pre-training combined with supervised fine-tuning on
the labeled data, followed by simple clustering methods. In this paper, we
posit that such methods are still prone to poor performance on
out-of-distribution categories, and do not leverage a key ingredient: Semantic
relationships between object categories. We therefore propose to leverage
multi-modal (vision and language) models, in two complementary ways. First, we
establish a strong baseline by replacing uni-modal features with CLIP, inspired
by its zero-shot performance. Second, we propose a novel retrieval-based
mechanism that leverages CLIP's aligned vision-language representations by
mining text descriptions from a text corpus for the labeled and unlabeled set.
We specifically use the alignment between CLIP's visual encoding of the image
and textual encoding of the corpus to retrieve top-k relevant pieces of text
and incorporate their embeddings to perform joint image+text semi-supervised
clustering. We perform rigorous experimentation and ablations (including on
where to retrieve from, how much to retrieve, and how to combine information),
and validate our results on several datasets including out-of-distribution
domains, demonstrating state-of-art results.",None,-1
Privacy-Preserving Tree-Based Inference with TFHE,0.148955,"Privacy enhancing technologies (PETs) have been proposed as a way to protect
the privacy of data while still allowing for data analysis. In this work, we
focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for
arbitrary computations to be performed on encrypted data. FHE has received lots
of attention in the past few years and has reached realistic execution times
and correctness.
  More precisely, we explain in this paper how we apply FHE to tree-based
models and get state-of-the-art solutions over encrypted tabular data. We show
that our method is applicable to a wide range of tree-based models, including
decision trees, random forests, and gradient boosted trees, and has been
implemented within the Concrete-ML library, which is open-source at
https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we
demonstrate that our FHE version is very close to the unprotected version in
terms of accuracy.",None,-1
SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning,0.401716,"As previous representations for reinforcement learning cannot effectively
incorporate a human-intuitive understanding of the 3D environment, they usually
suffer from sub-optimal performances. In this paper, we present Semantic-aware
Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly
optimizes semantic-aware neural radiance fields (NeRF) with a convolutional
encoder to learn 3D-aware neural implicit representation from multi-view
images. We introduce 3D semantic and distilled feature fields in parallel to
the RGB radiance fields in NeRF to learn semantic and object-centric
representation for reinforcement learning. SNeRL outperforms not only previous
pixel-based representations but also recent 3D-aware representations both in
model-free and model-based reinforcement learning.",None,-1
nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla Sentiment Analysis,0.559276,"In this paper, we discuss the nlpBDpatriots entry to the shared task on
Sentiment Analysis of Bangla Social Media Posts organized at the first workshop
on Bangla Language Processing (BLP) co-located with EMNLP. The main objective
of this task is to identify the polarity of social media content using a Bangla
dataset annotated with positive, neutral, and negative labels provided by the
shared task organizers. Our best system for this task is a transfer learning
approach with data augmentation which achieved a micro F1 score of 0.71. Our
best system ranked 12th among 30 teams that participated in the competition.",None,-1
Federated Learning for Large-Scale Scene Modeling with Neural Radiance Fields,0.374097,"We envision a system to continuously build and maintain a map based on
earth-scale neural radiance fields (NeRF) using data collected from vehicles
and drones in a lifelong learning manner. However, existing large-scale
modeling by NeRF has problems in terms of scalability and maintainability when
modeling earth-scale environments. Therefore, to address these problems, we
propose a federated learning pipeline for large-scale modeling with NeRF. We
tailor the model aggregation pipeline in federated learning for NeRF, thereby
allowing local updates of NeRF. In the aggregation step, the accuracy of the
clients' global pose is critical. Thus, we also propose global pose alignment
to align the noisy global pose of clients before the aggregation step. In
experiments, we show the effectiveness of the proposed pose alignment and the
federated learning pipeline on the large-scale scene dataset, Mill19.",None,-1
R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces,0.264442,"This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific
self-supervision method for speaker and noise-invariant speech representations
by learning discrete acoustic units with speaker-invariant clustering (Spin).
R-Spin resolves Spin's issues and enhances content representations by learning
to predict acoustic pieces. R-Spin offers a 12X reduction in computational
resources compared to previous state-of-the-art methods while outperforming
them in severely distorted speech scenarios. This paper provides detailed
analyses to show how discrete units contribute to speech encoder training and
improving robustness in diverse acoustic environments.",None,-1
Householder Projector for Unsupervised Latent Semantics Discovery,0.733369,"Generative Adversarial Networks (GANs), especially the recent style-based
generators (StyleGANs), have versatile semantics in the structured latent
space. Latent semantics discovery methods emerge to move around the latent code
such that only one factor varies during the traversal. Recently, an
unsupervised method proposed a promising direction to directly use the
eigenvectors of the projection matrix that maps latent codes to features as the
interpretable directions. However, one overlooked fact is that the projection
matrix is non-orthogonal and the number of eigenvectors is too large. The
non-orthogonality would entangle semantic attributes in the top few
eigenvectors, and the large dimensionality might result in meaningless
variations among the directions even if the matrix is orthogonal. To avoid
these issues, we propose Householder Projector, a flexible and general low-rank
orthogonal matrix representation based on Householder transformations, to
parameterize the projection matrix. The orthogonality guarantees that the
eigenvectors correspond to disentangled interpretable semantics, while the
low-rank property encourages that each identified direction has meaningful
variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and
evaluate the models on several benchmarks. Within only $1\%$ of the original
training steps for fine-tuning, our projector helps StyleGANs to discover more
disentangled and precise semantic attributes without sacrificing image
fidelity.",None,-1
SPEED: Speculative Pipelined Execution for Efficient Decoding,0.4657,"Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.",None,-1
Humanoid Agents: Platform for Simulating Human-like Generative Agents,0.697962,"Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents",None,-1
Characterizing Nexus of Similarity within Knowledge Bases: A Logic-based Framework and its Computational Complexity Aspects,0.151157,"Similarities between entities occur frequently in many real-world scenarios.
For over a century, researchers in different fields have proposed a range of
approaches to measure the similarity between entities. More recently, inspired
by ""Google Sets"", significant academic and commercial efforts have been devoted
to expanding a given set of entities with similar ones. As a result, existing
approaches nowadays are able to take into account properties shared by
entities, hereinafter called nexus of similarity. Accordingly, machines are
largely able to deal with both similarity measures and set expansions. To the
best of our knowledge, however, there is no way to characterize nexus of
similarity between entities, namely identifying such nexus in a formal and
comprehensive way so that they are both machine- and human-readable; moreover,
there is a lack of consensus on evaluating existing approaches for weakly
similar entities. As a first step towards filling these gaps, we aim to
complement existing literature by developing a novel logic-based framework to
formally and automatically characterize nexus of similarity between tuples of
entities within a knowledge base. Furthermore, we analyze computational
complexity aspects of this framework.",None,-1
Depth-NeuS: Neural Implicit Surfaces Learning for Multi-view Reconstruction Based on Depth Information Optimization,0.228761,"Recently, methods for neural surface representation and rendering, for
example NeuS, have shown that learning neural implicit surfaces through volume
rendering is becoming increasingly popular and making good progress. However,
these methods still face some challenges. Existing methods lack a direct
representation of depth information, which makes object reconstruction
unrestricted by geometric features, resulting in poor reconstruction of objects
with texture and color features. This is because existing methods only use
surface normals to represent implicit surfaces without using depth information.
Therefore, these methods cannot model the detailed surface features of objects
well. To address this problem, we propose a neural implicit surface learning
method called Depth-NeuS based on depth information optimization for multi-view
reconstruction. In this paper, we introduce depth loss to explicitly constrain
SDF regression and introduce geometric consistency loss to optimize for
low-texture areas. Specific experiments show that Depth-NeuS outperforms
existing technologies in multiple scenarios and achieves high-quality surface
reconstruction in multiple scenarios.",None,-1
Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction,0.629887,"Crosslingual conditional generation (e.g., machine translation) has long
enjoyed the benefits of scaling. Nonetheless, there are still issues that scale
alone may not overcome. A source query in one language, for instance, may yield
several translation options in another language without any extra context. Only
one translation could be acceptable however, depending on the translator's
preferences and goals. Choosing the incorrect option might significantly affect
translation usefulness and quality. We propose a novel method interactive-chain
prompting -- a series of question, answering and generation intermediate steps
between a Translator model and a User model -- that reduces translations into a
list of subproblems addressing ambiguities and then resolving such subproblems
before producing the final text to be translated. To check ambiguity resolution
capabilities and evaluate translation quality, we create a dataset exhibiting
different linguistic phenomena which leads to ambiguities at inference for four
languages. To encourage further exploration in this direction, we release all
datasets. We note that interactive-chain prompting, using eight interactions as
exemplars, consistently surpasses prompt-based methods with direct access to
background information to resolve ambiguities.",None,-1
Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering,0.297078,"The final frontier for simulation is the accurate representation of complex,
real-world social systems. While agent-based modeling (ABM) seeks to study the
behavior and interactions of agents within a larger system, it is unable to
faithfully capture the full complexity of human-driven behavior. Large language
models (LLMs), like ChatGPT, have emerged as a potential solution to this
bottleneck by enabling researchers to explore human-driven interactions in
previously unimaginable ways. Our research investigates simulations of human
interactions using LLMs. Through prompt engineering, inspired by Park et al.
(2023), we present two simulations of believable proxies of human behavior: a
two-agent negotiation and a six-agent murder mystery game.",None,-1
From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data,0.703824,"Large Language Models (LLMs) exhibit exceptional abilities for causal
analysis between concepts in numerous societally impactful domains, including
medicine, science, and law. Recent research on LLM performance in various
causal discovery and inference tasks has given rise to a new ladder in the
classical three-stage framework of causality. In this paper, we advance the
current research of LLM-driven causal discovery by proposing a novel framework
that combines knowledge-based LLM causal analysis with data-driven causal
structure learning. To make LLM more than a query tool and to leverage its
power in discovering natural and new laws of causality, we integrate the
valuable LLM expertise on existing causal mechanisms into statistical analysis
of objective data to build a novel and practical baseline for causal structure
learning.
  We introduce a universal set of prompts designed to extract causal graphs
from given variables and assess the influence of LLM prior causality on
recovering causal structures from data. We demonstrate the significant
enhancement of LLM expertise on the quality of recovered causal structures from
data, while also identifying critical challenges and issues, along with
potential approaches to address them. As a pioneering study, this paper aims to
emphasize the new frontier that LLMs are opening for classical causal discovery
and inference, and to encourage the widespread adoption of LLM capabilities in
data-driven causal analysis.",None,-1
MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR,0.602812,"Based on powerful text-to-image diffusion models, text-to-3D generation has
made significant progress in generating compelling geometry and appearance.
However, existing methods still struggle to recover high-fidelity object
materials, either only considering Lambertian reflectance, or failing to
disentangle BRDF materials from the environment lights. In this work, we
propose Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR
(\textbf{MATLABER}) that leverages a novel latent BRDF auto-encoder for
material generation. We train this auto-encoder with large-scale real-world
BRDF collections and ensure the smoothness of its latent space, which
implicitly acts as a natural distribution of materials. During appearance
modeling in text-to-3D generation, the latent BRDF embeddings, rather than BRDF
parameters, are predicted via a material network. Through exhaustive
experiments, our approach demonstrates the superiority over existing ones in
generating realistic and coherent object materials. Moreover, high-quality
materials naturally enable multiple downstream tasks such as relighting and
material editing. Code and model will be publicly available at
\url{https://sheldontsui.github.io/projects/Matlaber}.",None,-1
AutoRecon: Automated 3D Object Discovery and Reconstruction,0.920092,"A fully automated object reconstruction pipeline is crucial for digital
content creation. While the area of 3D reconstruction has witnessed profound
developments, the removal of background to obtain a clean object model still
relies on different forms of manual labor, such as bounding box labeling, mask
annotations, and mesh manipulations. In this paper, we propose a novel
framework named AutoRecon for the automated discovery and reconstruction of an
object from multi-view images. We demonstrate that foreground objects can be
robustly located and segmented from SfM point clouds by leveraging
self-supervised 2D vision transformer features. Then, we reconstruct decomposed
neural scene representations with dense supervision provided by the decomposed
point clouds, resulting in accurate object reconstruction and segmentation.
Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the
effectiveness and robustness of AutoRecon.",None,-1
CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation,0.544497,"Beyond novel view synthesis, Neural Radiance Fields are useful for
applications that interact with the real world. In this paper, we use them as
an implicit map of a given scene and propose a camera relocalization algorithm
tailored for this representation. The proposed method enables to compute in
real-time the precise position of a device using a single RGB camera, during
its navigation. In contrast with previous work, we do not rely on pose
regression or photometric alignment but rather use dense local features
obtained through volumetric rendering which are specialized on the scene with a
self-supervised objective. As a result, our algorithm is more accurate than
competitors, able to operate in dynamic outdoor environments with changing
lightning conditions and can be readily integrated in any volumetric neural
renderer.",None,-1
Unbiased Image Synthesis via Manifold Guidance in Diffusion Models,0.0532466,"Diffusion Models are a potent class of generative models capable of producing
high-quality images. However, they often inadvertently favor certain data
attributes, undermining the diversity of generated images. This issue is
starkly apparent in skewed datasets like CelebA, where the initial dataset
disproportionately favors females over males by 57.9%, this bias amplified in
generated data where female representation outstrips males by 148%. In
response, we propose a plug-and-play method named Manifold Guidance Sampling,
which is also the first unsupervised method to mitigate bias issue in DDPMs.
Leveraging the inherent structure of the data manifold, this method steers the
sampling process towards a more uniform distribution, effectively dispersing
the clustering of biased data. Without the need for modifying the existing
model or additional training, it significantly mitigates data bias and enhances
the quality and unbiasedness of the generated images.",None,-1
Solving Multi-Agent Target Assignment and Path Finding with a Single Constraint Tree,0.312727,"Combined Target-Assignment and Path-Finding problem (TAPF) requires
simultaneously assigning targets to agents and planning collision-free paths
for agents from their start locations to their assigned targets. As a leading
approach to address TAPF, Conflict-Based Search with Target Assignment (CBS-TA)
leverages both K-best target assignments to create multiple search trees and
Conflict-Based Search (CBS) to resolve collisions in each search tree. While
being able to find an optimal solution, CBS-TA suffers from scalability due to
the duplicated collision resolution in multiple trees and the expensive
computation of K-best assignments. We therefore develop Incremental Target
Assignment CBS (ITA-CBS) to bypass these two computational bottlenecks. ITA-CBS
generates only a single search tree and avoids computing K-best assignments by
incrementally computing new 1-best assignments during the search. We show that,
in theory, ITA-CBS is guaranteed to find an optimal solution and, in practice,
is computationally efficient.",None,-1
Multiparticle Kalman filter for object localization in symmetric environments,0.397605,"This study considers the object localization problem and proposes a novel
multiparticle Kalman filter to solve it in complex and symmetric environments.
Two well-known classes of filtering algorithms to solve the localization
problem are Kalman filter-based methods and particle filter-based methods. We
consider these classes, demonstrate their complementary properties, and propose
a novel filtering algorithm that takes the best from two classes. We evaluate
the multiparticle Kalman filter in symmetric and noisy environments. Such
environments are especially challenging for both classes of classical methods.
We compare the proposed approach with the particle filter since only this
method is feasible if the initial state is unknown. In the considered
challenging environments, our method outperforms the particle filter in terms
of both localization error and runtime.",None,-1
Diagnosing and Rectifying Vision Models using Language,0.648431,"Recent multi-modal contrastive learning models have demonstrated the ability
to learn an embedding space suitable for building strong vision classifiers, by
leveraging the rich information in large-scale image-caption datasets. Our work
highlights a distinct advantage of this multi-modal embedding space: the
ability to diagnose vision classifiers through natural language. The
traditional process of diagnosing model behaviors in deployment settings
involves labor-intensive data acquisition and annotation. Our proposed method
can discover high-error data slices, identify influential attributes and
further rectify undesirable model behaviors, without requiring any visual data.
Through a combination of theoretical explanation and empirical verification, we
present conditions under which classifiers trained on embeddings from one
modality can be equivalently applied to embeddings from another modality. On a
range of image datasets with known error slices, we demonstrate that our method
can effectively identify the error slices and influential attributes, and can
further use language to rectify failure modes of the classifier.",None,-1
SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings,0.798104,"Contextual spelling correction models are an alternative to shallow fusion to
improve automatic speech recognition (ASR) quality given user vocabulary. To
deal with large user vocabularies, most of these models include candidate
retrieval mechanisms, usually based on minimum edit distance between fragments
of ASR hypothesis and user phrases. However, the edit-distance approach is
slow, non-trainable, and may have low recall as it relies only on common
letters. We propose: 1) a novel algorithm for candidate retrieval, based on
misspelled n-gram mappings, which gives up to 90% recall with just the top 10
candidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on
BERT architecture, where the initial transcript and ten candidates are combined
into one input. The experiments on Spoken Wikipedia show 21.4% word error rate
improvement compared to a baseline ASR system.",None,-1
A System for Human-AI collaboration for Online Customer Support,0.0784541,"AI enabled chat bots have recently been put to use to answer customer service
queries, however it is a common feedback of users that bots lack a personal
touch and are often unable to understand the real intent of the user's
question. To this end, it is desirable to have human involvement in the
customer servicing process. In this work, we present a system where a human
support agent collaborates in real-time with an AI agent to satisfactorily
answer customer queries. We describe the user interaction elements of the
solution, along with the machine learning techniques involved in the AI agent.",None,-1
ISP: Multi-Layered Garment Draping with Implicit Sewing Patterns,0.974197,"Many approaches to draping individual garments on human body models are
realistic, fast, and yield outputs that are differentiable with respect to the
body shape on which they are draped. However, they are either unable to handle
multi-layered clothing, which is prevalent in everyday dress, or restricted to
bodies in T-pose. In this paper, we introduce a parametric garment
representation model that addresses these limitations. As in models used by
clothing designers, each garment consists of individual 2D panels. Their 2D
shape is defined by a Signed Distance Function and 3D shape by a 2D to 3D
mapping. The 2D parameterization enables easy detection of potential collisions
and the 3D parameterization handles complex shapes effectively. We show that
this combination is faster and yields higher quality reconstructions than
purely implicit surface representations, and makes the recovery of layered
garments from images possible thanks to its differentiability. Furthermore, it
supports rapid editing of garment shapes and texture by modifying individual 2D
panels.",None,-1
From Fake to Hyperpartisan News Detection Using Domain Adaptation,0.105839,"Unsupervised Domain Adaptation (UDA) is a popular technique that aims to
reduce the domain shift between two data distributions. It was successfully
applied in computer vision and natural language processing. In the current
work, we explore the effects of various unsupervised domain adaptation
techniques between two text classification tasks: fake and hyperpartisan news
detection. We investigate the knowledge transfer from fake to hyperpartisan
news detection without involving target labels during training. Thus, we
evaluate UDA, cluster alignment with a teacher, and cross-domain contrastive
learning. Extensive experiments show that these techniques improve performance,
while including data augmentation further enhances the results. In addition, we
combine clustering and topic modeling algorithms with UDA, resulting in
improved performances compared to the initial UDA setup.",None,-1
Template-free Articulated Neural Point Clouds for Reposable View Synthesis,0.147617,"Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when
synthesizing novel views of time-evolving 3D scenes. However, the common
reliance on backward deformation fields makes reanimation of the captured
object poses challenging. Moreover, the state of the art dynamic models are
often limited by low visual fidelity, long reconstruction time or specificity
to narrow application domains. In this paper, we present a novel method
utilizing a point-based representation and Linear Blend Skinning (LBS) to
jointly learn a Dynamic NeRF and an associated skeletal model from even sparse
multi-view video. Our forward-warping approach achieves state-of-the-art visual
fidelity when synthesizing novel views and poses while significantly reducing
the necessary learning time when compared to existing work. We demonstrate the
versatility of our representation on a variety of articulated objects from
common datasets and obtain reposable 3D reconstructions without the need of
object-specific skeletal templates. Code will be made available at
https://github.com/lukasuz/Articulated-Point-NeRF.",None,-1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images,0.216436,"This paper introduces vox2vec - a contrastive method for self-supervised
learning (SSL) of voxel-level representations. vox2vec representations are
modeled by a Feature Pyramid Network (FPN): a voxel representation is a
concatenation of the corresponding feature vectors from different pyramid
levels. The FPN is pre-trained to produce similar representations for the same
voxel in different augmented contexts and distinctive representations for
different voxels. This results in unified multi-scale representations that
capture both global semantics (e.g., body part) and local semantics (e.g.,
different small organs or healthy versus tumor tissue). We use vox2vec to
pre-train a FPN on more than 6500 publicly available computed tomography
images. We evaluate the pre-trained representations by attaching simple heads
on top of them and training the resulting models for 22 segmentation tasks. We
show that vox2vec outperforms existing medical imaging SSL techniques in three
evaluation setups: linear and non-linear probing and end-to-end fine-tuning.
Moreover, a non-linear head trained on top of the frozen vox2vec
representations achieves competitive performance with the FPN trained from
scratch while having 50 times fewer trainable parameters. The code is available
at https://github.com/mishgon/vox2vec .",None,-1
Interactive Segment Anything NeRF with Feature Imitation,0.64827,"This paper investigates the potential of enhancing Neural Radiance Fields
(NeRF) with semantics to expand their applications. Although NeRF has been
proven useful in real-world applications like VR and digital creation, the lack
of semantics hinders interaction with objects in complex scenes. We propose to
imitate the backbone feature of off-the-shelf perception models to achieve
zero-shot semantic segmentation with NeRF. Our framework reformulates the
segmentation process by directly rendering semantic features and only applying
the decoder from perception models. This eliminates the need for expensive
backbones and benefits 3D consistency. Furthermore, we can project the learned
semantics onto extracted mesh surfaces for real-time interaction. With the
state-of-the-art Segment Anything Model (SAM), our framework accelerates
segmentation by 16 times with comparable mask quality. The experimental results
demonstrate the efficacy and computational advantages of our approach. Project
page: \url{https://me.kiui.moe/san/}.",None,-1
Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,0.973821,"We introduce a method to measure uncertainty in large language models. For
tasks like question answering, it is essential to know when we can trust the
natural language outputs of foundation models. We show that measuring
uncertainty in natural language is challenging because of ""semantic
equivalence"" -- different sentences can mean the same thing. To overcome these
challenges we introduce semantic entropy -- an entropy which incorporates
linguistic invariances created by shared meanings. Our method is unsupervised,
uses only a single model, and requires no modifications to off-the-shelf
language models. In comprehensive ablation studies we show that the semantic
entropy is more predictive of model accuracy on question answering data sets
than comparable baselines.",None,-1
SMATCH++: Standardized and Extended Evaluation of Semantic Graphs,0.741204,"The Smatch metric is a popular method for evaluating graph distances, as is
necessary, for instance, to assess the performance of semantic graph parsing
systems. However, we observe some issues in the metric that jeopardize
meaningful evaluation. E.g., opaque pre-processing choices can affect results,
and current graph-alignment solvers do not provide us with upper-bounds.
Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,
adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)
are spread out, and lack a unifying framework.
  For better inspection, we divide the metric into three modules:
pre-processing, alignment, and scoring. Examining each module, we specify its
goals and diagnose potential issues, for which we discuss and test mitigation
strategies. For pre-processing, we show how to fully conform to annotation
guidelines that allow structurally deviating but valid graphs. For safer and
enhanced alignment, we show the feasibility of optimal alignment in a standard
evaluation setup, and develop a lossless graph compression method that shrinks
the search space and significantly increases efficiency. For improved scoring,
we propose standardized and extended metric calculation of fine-grained
sub-graph meaning aspects. Our code is available at
https://github.com/flipz357/smatchpp",None,-1
"Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",0.999703,"The capability of Large Language Models (LLMs) like ChatGPT to comprehend
user intent and provide reasonable responses has made them extremely popular
lately. In this paper, we focus on assessing the overall ability of ChatGPT
using 7 fine-grained information extraction (IE) tasks. Specially, we present
the systematically analysis by measuring ChatGPT's performance, explainability,
calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT
or domain experts. Our findings reveal that ChatGPT's performance in
Standard-IE setting is poor, but it surprisingly exhibits excellent performance
in the OpenIE setting, as evidenced by human evaluation. In addition, our
research indicates that ChatGPT provides high-quality and trustworthy
explanations for its decisions. However, there is an issue of ChatGPT being
overconfident in its predictions, which resulting in low calibration.
Furthermore, ChatGPT demonstrates a high level of faithfulness to the original
text in the majority of cases. We manually annotate and release the test sets
of 7 fine-grained IE tasks contains 14 datasets to further promote the
research. The datasets and code are available at
https://github.com/pkuserc/ChatGPT_for_IE.",None,-1
Improving Link Prediction in Social Networks Using Local and Global Features: A Clustering-based Approach,0.185646,"Link prediction problem has increasingly become prominent in many domains
such as social network analyses, bioinformatics experiments, transportation
networks, criminal investigations and so forth. A variety of techniques has
been developed for link prediction problem, categorized into 1) similarity
based approaches which study a set of features to extract similar nodes; 2)
learning based approaches which extract patterns from the input data; 3)
probabilistic statistical approaches which optimize a set of parameters to
establish a model which can best compute formation probability. However,
existing literatures lack approaches which utilize strength of each approach by
integrating them to achieve a much more productive one. To tackle the link
prediction problem, we propose an approach based on the combination of first
and second group methods; the existing studied works use just one of these
categories. Our two-phase developed method firstly determines new features
related to the position and dynamic behavior of nodes, which enforce the
approach more efficiency compared to approaches using mere measures. Then, a
subspace clustering algorithm is applied to group social objects based on the
computed similarity measures which differentiate the strength of clusters;
basically, the usage of local and global indices and the clustering information
plays an imperative role in our link prediction process. Some extensive
experiments held on real datasets including Facebook, Brightkite and HepTh
indicate good performances of our proposal method. Besides, we have
experimentally verified our approach with some previous techniques in the area
to prove the supremacy of ours.",None,-1
Structured Dialogue Discourse Parsing,0.799105,"Dialogue discourse parsing aims to uncover the internal structure of a
multi-participant conversation by finding all the discourse~\emph{links} and
corresponding~\emph{relations}. Previous work either treats this task as a
series of independent multiple-choice problems, in which the link existence and
relations are decoded separately, or the encoding is restricted to only local
interaction, ignoring the holistic structural information. In contrast, we
propose a principled method that improves upon previous work from two
perspectives: encoding and decoding. From the encoding side, we perform
structured encoding on the adjacency matrix followed by the matrix-tree
learning algorithm, where all discourse links and relations in the dialogue are
jointly optimized based on latent tree-level distribution. From the decoding
side, we perform structured inference using the modified Chiu-Liu-Edmonds
algorithm, which explicitly generates the labeled multi-root non-projective
spanning tree that best captures the discourse structure. In addition, unlike
in previous work, we do not rely on hand-crafted features; this improves the
model's robustness. Experiments show that our method achieves new
state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on
Molweni (F1 scores). \footnote{Code released
at~\url{https://github.com/chijames/structured_dialogue_discourse_parsing}.}",None,-1
Agent Instructs Large Language Models to be General Zero-Shot Reasoners,0.436075,"We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.",None,-1
Artificial General Intelligence for Medical Imaging,0.727174,"In this review, we explore the potential applications of Artificial General
Intelligence (AGI) models in healthcare, focusing on foundational Large
Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We
emphasize the importance of integrating clinical expertise, domain knowledge,
and multimodal capabilities into AGI models. In addition, we lay out key
roadmaps that guide the development and deployment of healthcare AGI models.
Throughout the review, we provide critical perspectives on the potential
challenges and pitfalls associated with deploying large-scale AGI models in the
medical field. This comprehensive review aims to offer insights into the future
implications of AGI in medical imaging, healthcare and beyond.",None,-1
Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition,0.425078,"In recent years, Large Language Models such as GPT-3 showed remarkable
capabilities in performing NLP tasks in the zero and few shot settings. On the
other hand, the experiments highlighted the difficulty of GPT-3 in carrying out
tasks that require a certain degree of reasoning, such as arithmetic
operations. In this paper we evaluate the ability of Transformer Language
Models to perform arithmetic operations following a pipeline that, before
performing computations, decomposes numbers in units, tens, and so on. We
denote the models fine-tuned with this pipeline with the name Calculon and we
test them in the task of performing additions, subtractions and multiplications
on the same test sets of GPT-3. Results show an increase of accuracy of 63% in
the five-digit addition task. Moreover, we demonstrate the importance of the
decomposition pipeline introduced, since fine-tuning the same Language Model
without decomposing numbers results in 0% accuracy in the five-digit addition
task.",None,-1
CNN-BiLSTM model for English Handwriting Recognition: Comprehensive Evaluation on the IAM Dataset,0.59811,"We present a CNN-BiLSTM system for the problem of offline English handwriting
recognition, with extensive evaluations on the public IAM dataset, including
the effects of model size, data augmentation and the lexicon. Our best model
achieves 3.59\% CER and 9.44\% WER using CNN-BiLSTM network with CTC layer.
Test time augmentation with rotation and shear transformations applied to the
input image, is proposed to increase recognition of difficult cases and found
to reduce the word error rate by 2.5\% points. We also conduct an error
analysis of our proposed method on IAM dataset, show hard cases of handwriting
images and explore samples with erroneous labels. We provide our source code as
public-domain, to foster further research to encourage scientific
reproducibility.",None,-1
A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3,0.832124,"LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art
language model GPT-3, fine-tuned for the legal domain. The system is designed
to provide legal assistance to users in a conversational manner, helping them
with tasks such as answering legal questions, generating legal documents, and
providing legal advice. In this paper, we provide a brief overview of LawGPT
1.0, its architecture, and its performance on a set of legal benchmark tasks.
Please note that the detailed information about the model is protected by a
non-disclosure agreement (NDA) and cannot be disclosed in this report.",None,-1
Diverse Inpainting and Editing with GAN Inversion,0.793123,"Recent inversion methods have shown that real images can be inverted into
StyleGAN's latent space and numerous edits can be achieved on those images
thanks to the semantically rich feature representations of well-trained GAN
models. However, extensive research has also shown that image inversion is
challenging due to the trade-off between high-fidelity reconstruction and
editability. In this paper, we tackle an even more difficult task, inverting
erased images into GAN's latent space for realistic inpaintings and editings.
Furthermore, by augmenting inverted latent codes with different latent samples,
we achieve diverse inpaintings. Specifically, we propose to learn an encoder
and mixing network to combine encoded features from erased images with
StyleGAN's mapped features from random samples. To encourage the mixing network
to utilize both inputs, we train the networks with generated data via a novel
set-up. We also utilize higher-rate features to prevent color inconsistencies
between the inpainted and unerased parts. We run extensive experiments and
compare our method with state-of-the-art inversion and inpainting methods.
Qualitative metrics and visual comparisons show significant improvements.",None,-1
L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models,0.101411,"The exploration of sentiment analysis in low-resource languages, such as
Marathi, has been limited due to the availability of suitable datasets. In this
work, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis
dataset, with four different domains - movie reviews, general tweets, TV show
subtitles, and political tweets. The dataset consists of around 60,000 manually
tagged samples covering 3 distinct sentiments - positive, negative, and
neutral. We create a sub-dataset for each domain comprising 15k samples. The
MahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset
within the Indic sentiment landscape. We fine-tune different monolingual and
multilingual BERT models on these datasets and report the best accuracy with
the MahaBERT model. We also present an extensive in-domain and cross-domain
analysis thus highlighting the need for low-resource multi-domain datasets. The
data and models are available at https://github.com/l3cube-pune/MarathiNLP .",None,-1
KG-BERTScore: Incorporating Knowledge Graph into BERTScore for Reference-Free Machine Translation Evaluation,0.401574,"BERTScore is an effective and robust automatic metric for referencebased
machine translation evaluation. In this paper, we incorporate multilingual
knowledge graph into BERTScore and propose a metric named KG-BERTScore, which
linearly combines the results of BERTScore and bilingual named entity matching
for reference-free machine translation evaluation. From the experimental
results on WMT19 QE as a metric without references shared tasks, our metric
KG-BERTScore gets higher overall correlation with human judgements than the
current state-of-the-art metrics for reference-free machine translation
evaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore
and the parameter for linear combination are also studied in this paper.",None,-1
HDR image watermarking using saliency detection and quantization index modulation,0.229214,"High-dynamic range (HDR) images are circulated rapidly over the internet with
risks of being exploited for unauthorized usage. To protect these images, some
HDR image based watermarking (HDR-IW) methods were put forward. However, they
inherited the same problem faced by conventional IW methods for standard
dynamic range (SDR) images, where only trade-offs among conflicting
requirements are managed instead of simultaneous improvement. In this paper, a
novel saliency (eye-catching object) detection based trade-off independent
HDR-IW is proposed, to simultaneously improve robustness, imperceptibility and
payload. First, the host image goes through our proposed salient object
detection model to produce a saliency map, which is, in turn, exploited to
segment the foreground and background of the host image. Next, the binary
watermark is partitioned into the foregrounds and backgrounds using the same
mask and scrambled using a random permutation algorithm. Finally, the watermark
segments are embedded into selected bit-plane of the corresponding host
segments using quantized indexed modulation. Experimental results suggest that
the proposed work outperforms state-of-the-art methods in terms of improving
the conflicting requirements.",None,-1
eXplainable Artificial Intelligence (XAI) in aging clock models,0.50448,"eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the ""aging clocks"" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.",None,-1
"To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing",0.036312,"NLP is in a period of disruptive change that is impacting our methodologies,
funding sources, and public perception. In this work, we seek to understand how
to shape our future by better understanding our past. We study factors that
shape NLP as a field, including culture, incentives, and infrastructure by
conducting long-form interviews with 26 NLP researchers of varying seniority,
research area, institution, and social identity. Our interviewees identify
cyclical patterns in the field, as well as new shifts without historical
parallel, including changes in benchmark culture and software infrastructure.
We complement this discussion with quantitative analysis of citation,
authorship, and language use in the ACL Anthology over time. We conclude by
discussing shared visions, concerns, and hopes for the future of NLP. We hope
that this study of our field's past and present can prompt informed discussion
of our community's implicit norms and more deliberate action to consciously
shape the future.",None,-1
Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task,0.337034,"The evolution of Generative Pre-trained Transformer (GPT) models has led to
significant advancements in various natural language processing applications,
particularly in legal textual entailment. We present an analysis of GPT-3.5
(ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominent
benchmark in this domain. The study encompasses data from Heisei 18 (2006) to
Reiwa 3 (2021), exploring the models' abilities to discern entailment
relationships within Japanese statute law across different periods. Our
preliminary experimental results unveil intriguing insights into the models'
strengths and weaknesses in handling legal textual entailment tasks, as well as
the patterns observed in model performance. In the context of proprietary
models with undisclosed architectures and weights, black-box analysis becomes
crucial for evaluating their capabilities. We discuss the influence of training
data distribution and the implications on the models' generalizability. This
analysis serves as a foundation for future research, aiming to optimize
GPT-based models and enable their successful adoption in legal information
extraction and entailment applications.",None,-1
Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps,0.744144,"The biodiversity of our planet is under threat, with approximately one
million species expected to become extinct within decades. The reason; negative
human actions, which include hunting, overfishing, pollution, and the
conversion of land for urbanisation and agricultural purposes. Despite
significant investment from charities and governments for activities that
benefit nature, global wildlife populations continue to decline. Local wildlife
guardians have historically played a critical role in global conservation
efforts and have shown their ability to achieve sustainability at various
levels. In 2021, COP26 recognised their contributions and pledged US$1.7
billion per year; however, this is a fraction of the global biodiversity budget
available (between US$124 billion and US$143 billion annually) given they
protect 80% of the planets biodiversity. This paper proposes a radical new
solution based on ""Interspecies Money,"" where animals own their own money.
Creating a digital twin for each species allows animals to dispense funds to
their guardians for the services they provide. For example, a rhinoceros may
release a payment to its guardian each time it is detected in a camera trap as
long as it remains alive and well. To test the efficacy of this approach 27
camera traps were deployed over a 400km2 area in Welgevonden Game Reserve in
Limpopo Province in South Africa. The motion-triggered camera traps were
operational for ten months and, using deep learning, we managed to capture
images of 12 distinct animal species. For each species, a makeshift bank
account was set up and credited with {\pounds}100. Each time an animal was
captured in a camera and successfully classified, 1 penny (an arbitrary amount
- mechanisms still need to be developed to determine the real value of species)
was transferred from the animal account to its associated guardian.",None,-1
Line Graphics Digitization: A Step Towards Full Automation,0.36625,"The digitization of documents allows for wider accessibility and
reproducibility. While automatic digitization of document layout and text
content has been a long-standing focus of research, this problem in regard to
graphical elements, such as statistical plots, has been under-explored. In this
paper, we introduce the task of fine-grained visual understanding of
mathematical graphics and present the Line Graphics (LG) dataset, which
includes pixel-wise annotations of 5 coarse and 10 fine-grained categories. Our
dataset covers 520 images of mathematical graphics collected from 450 documents
from different disciplines. Our proposed dataset can support two different
computer vision tasks, i.e., semantic segmentation and object detection. To
benchmark our LG dataset, we explore 7 state-of-the-art models. To foster
further research on the digitization of statistical graphs, we will make the
dataset, code, and models publicly available to the community.",None,-1
MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model,0.900333,"Due to patient privacy protection concerns, machine learning research in
healthcare has been undeniably slower and limited than in other application
domains. High-quality, realistic, synthetic electronic health records (EHRs)
can be leveraged to accelerate methodological developments for research
purposes while mitigating privacy concerns associated with data sharing. The
current state-of-the-art model for synthetic EHR generation is generative
adversarial networks, which are notoriously difficult to train and can suffer
from mode collapse. Denoising Diffusion Probabilistic Models, a class of
generative models inspired by statistical thermodynamics, have recently been
shown to generate high-quality synthetic samples in certain domains. It is
unknown whether these can generalize to generation of large-scale,
high-dimensional EHRs. In this paper, we present a novel generative model based
on diffusion models that is the first successful application on electronic
health records. Our model proposes a mechanism to perform class-conditional
sampling to preserve label information. We also introduce a new sampling
strategy to accelerate the inference speed. We empirically show that our model
outperforms existing state-of-the-art synthetic EHR generation methods.",None,-1
Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations,0.406351,"Learned Image Compression (LIC) has recently become the trending technique
for image transmission due to its notable performance. Despite its popularity,
the robustness of LIC with respect to the quality of image reconstruction
remains under-explored. In this paper, we introduce an imperceptible attack
approach designed to effectively degrade the reconstruction quality of LIC,
resulting in the reconstructed image being severely disrupted by noise where
any object in the reconstructed images is virtually impossible. More
specifically, we generate adversarial examples by introducing a Frobenius
norm-based loss function to maximize the discrepancy between original images
and reconstructed adversarial examples. Further, leveraging the insensitivity
of high-frequency components to human vision, we introduce Imperceptibility
Constraint (IC) to ensure that the perturbations remain inconspicuous.
Experiments conducted on the Kodak dataset using various LIC models demonstrate
effectiveness. In addition, we provide several findings and suggestions for
designing future defenses.",None,-1
An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development,0.361627,"Causal networks are widely used in many fields, including epidemiology,
social science, medicine, and engineering, to model the complex relationships
between variables. While it can be convenient to algorithmically infer these
models directly from observational data, the resulting networks are often
plagued with erroneous edges. Auditing and correcting these networks may
require domain expertise frequently unavailable to the analyst. We propose the
use of large language models such as ChatGPT as an auditor for causal networks.
Our method presents ChatGPT with a causal network, one edge at a time, to
produce insights about edge directionality, possible confounders, and mediating
variables. We ask ChatGPT to reflect on various aspects of each causal link and
we then produce visualizations that summarize these viewpoints for the human
analyst to direct the edge, gather more data, or test further hypotheses. We
envision a system where large language models, automated causal inference, and
the human analyst and domain expert work hand in hand as a team to derive
holistic and comprehensive causal models for any given case scenario. This
paper presents first results obtained with an emerging prototype.",None,-1
Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline,0.448432,"Large language models (LLMs) have revolutionized the field of AI,
demonstrating unprecedented capacity across various tasks. However, the
inference process for LLMs comes with significant computational costs. In this
paper, we propose an efficient LLM inference pipeline that harnesses the power
of LLMs. Our approach begins by tapping into the potential of LLMs to
accurately perceive and predict the response length with minimal overhead. By
leveraging this information, we introduce an efficient sequence scheduling
technique that groups queries with similar response lengths into micro-batches.
We evaluate our approach on real-world instruction datasets using the
LLaMA-based model, and our results demonstrate an impressive 86% improvement in
inference throughput without compromising effectiveness. Notably, our method is
orthogonal to other inference acceleration techniques, making it a valuable
addition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM
inference.",None,-1
Predicting Privacy Preferences for Smart Devices as Norms,0.463227,"Smart devices, such as smart speakers, are becoming ubiquitous, and users
expect these devices to act in accordance with their preferences. In
particular, since these devices gather and manage personal data, users expect
them to adhere to their privacy preferences. However, the current approach of
gathering these preferences consists in asking the users directly, which
usually triggers automatic responses failing to capture their true preferences.
In response, in this paper we present a collaborative filtering approach to
predict user preferences as norms. These preference predictions can be readily
adopted or can serve to assist users in determining their own preferences.
Using a dataset of privacy preferences of smart assistant users, we test the
accuracy of our predictions.",None,-1
MSdocTr-Lite: A Lite Transformer for Full Page Multi-script Handwriting Recognition,0.510608,"The Transformer has quickly become the dominant architecture for various
pattern recognition tasks due to its capacity for long-range representation.
However, transformers are data-hungry models and need large datasets for
training. In Handwritten Text Recognition (HTR), collecting a massive amount of
labeled data is a complicated and expensive task. In this paper, we propose a
lite transformer architecture for full-page multi-script handwriting
recognition. The proposed model comes with three advantages: First, to solve
the common problem of data scarcity, we propose a lite transformer model that
can be trained on a reasonable amount of data, which is the case of most HTR
public datasets, without the need for external data. Second, it can learn the
reading order at page-level thanks to a curriculum learning strategy, allowing
it to avoid line segmentation errors, exploit a larger context and reduce the
need for costly segmentation annotations. Third, it can be easily adapted to
other scripts by applying a simple transfer-learning process using only
page-level labeled images. Extensive experiments on different datasets with
different scripts (French, English, Spanish, and Arabic) show the effectiveness
of the proposed model.",None,-1
DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration,0.148624,"Existing All-In-One image restoration (IR) methods usually lack flexible
modeling on various types of degradation, thus impeding the restoration
performance. To achieve All-In-One IR with higher task dexterity, this work
proposes an efficient Dynamic Reference Modeling paradigm (DRM-IR), which
consists of task-adaptive degradation modeling and model-based image restoring.
Specifically, these two subtasks are formalized as a pair of entangled
reference-based maximum a posteriori (MAP) inferences, which are optimized
synchronously in an unfolding-based manner. With the two cascaded subtasks,
DRM-IR first dynamically models the task-specific degradation based on a
reference image pair and further restores the image with the collected
degradation statistics. Besides, to bridge the semantic gap between the
reference and target degraded images, we further devise a Degradation Prior
Transmitter (DPT) that restrains the instance-specific feature differences.
DRM-IR explicitly provides superior flexibility for All-in-One IR while being
interpretable. Extensive experiments on multiple benchmark datasets show that
our DRM-IR achieves state-of-the-art in All-In-One IR.",None,-1
Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation,0.742214,"Conversational Recommendation System (CRS) is a rapidly growing research area
that has gained significant attention alongside advancements in language
modelling techniques. However, the current state of conversational
recommendation faces numerous challenges due to its relative novelty and
limited existing contributions. In this study, we delve into benchmark datasets
for developing CRS models and address potential biases arising from the
feedback loop inherent in multi-turn interactions, including selection bias and
multiple popularity bias variants. Drawing inspiration from the success of
generative data via using language models and data augmentation techniques, we
present two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model
performance while mitigating biases. Through extensive experiments on ReDial
and TG-ReDial benchmark datasets, we show a consistent improvement of CRS
techniques with our data augmentation approaches and offer additional insights
on addressing multiple newly formulated biases.",None,-1
On the Robustness of Latent Diffusion Models,0.264196,"Latent diffusion models achieve state-of-the-art performance on a variety of
generative tasks, such as image synthesis and image editing. However, the
robustness of latent diffusion models is not well studied. Previous works only
focus on the adversarial attacks against the encoder or the output image under
white-box settings, regardless of the denoising process. Therefore, in this
paper, we aim to analyze the robustness of latent diffusion models more
thoroughly. We first study the influence of the components inside latent
diffusion models on their white-box robustness. In addition to white-box
scenarios, we evaluate the black-box robustness of latent diffusion models via
transfer attacks, where we consider both prompt-transfer and model-transfer
settings and possible defense mechanisms. However, all these explorations need
a comprehensive benchmark dataset, which is missing in the literature.
Therefore, to facilitate the research of the robustness of latent diffusion
models, we propose two automatic dataset construction pipelines for two kinds
of image editing models and release the whole dataset. Our code and dataset are
available at \url{https://github.com/jpzhang1810/LDM-Robustness}.",None,-1
Detecting Agreement in Multi-party Conversational AI,0.350768,"Today, conversational systems are expected to handle conversations in
multi-party settings, especially within Socially Assistive Robots (SARs).
However, practical usability remains difficult as there are additional
challenges to overcome, such as speaker recognition, addressee recognition, and
complex turn-taking. In this paper, we present our work on a multi-party
conversational system, which invites two users to play a trivia quiz game. The
system detects users' agreement or disagreement on a final answer and responds
accordingly. Our evaluation includes both performance and user assessment
results, with a focus on detecting user agreement. Our annotated transcripts
and the code for the proposed system have been released open-source on GitHub.",None,-1
NExT-GPT: Any-to-Any Multimodal LLM,0.986175,"While recently Multimodal Large Language Models (MM-LLMs) have made exciting
strides, they mostly fall prey to the limitation of only input-side multimodal
understanding, without the ability to produce content in multiple modalities.
As we humans always perceive the world and communicate with people through
various modalities, developing any-to-any MM-LLMs capable of accepting and
delivering content in any modality becomes essential to human-level AI. To fill
the gap, we present an end-to-end general-purpose any-to-any MM-LLM system,
NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion
decoders, enabling NExT-GPT to perceive inputs and generate outputs in
arbitrary combinations of text, images, videos, and audio. By leveraging the
existing well-trained highly-performing encoders and decoders, NExT-GPT is
tuned with only a small amount of parameter (1%) of certain projection layers,
which not only benefits low-cost training and also facilitates convenient
expansion to more potential modalities. Moreover, we introduce a
modality-switching instruction tuning (MosIT) and manually curate a
high-quality dataset for MosIT, based on which NExT-GPT is empowered with
complex cross-modal semantic understanding and content generation. Overall, our
research showcases the promising possibility of building an AI agent capable of
modeling universal modalities, paving the way for more human-like AI research
in the community. Project page: https://next-gpt.github.io/",None,-1
Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy,0.363631,"Real-world deployment of computer vision systems, including in the discovery
processes of biomedical research, requires causal representations that are
invariant to contextual nuisances and generalize to new data. Leveraging the
internal replicate structure of two novel single-cell fluorescent microscopy
datasets, we propose generally applicable tests to assess the extent to which
models learn causal representations across increasingly challenging levels of
OOD-generalization. We show that despite seemingly strong performance, as
assessed by other established metrics, both naive and contemporary baselines
designed to ward against confounding, collapse on these tests. We introduce a
new method, Interventional Style Transfer (IST), that substantially improves
OOD generalization by generating interventional training distributions in which
spurious correlations between biological causes and nuisances are mitigated. We
publish our code and datasets.",None,-1
Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation,0.0564116,"An ideal length-extrapolatable Transformer language model can handle
sequences longer than the training length without any fine-tuning. Such
long-context utilization capability relies heavily on a flexible positional
embedding design. Upon investigating the flexibility of existing large
pre-trained Transformer language models, we find that the T5 family deserves a
closer look, as its positional embeddings capture rich and flexible attention
patterns. However, T5 suffers from the dispersed attention issue: the longer
the input sequence, the flatter the attention distribution. To alleviate the
issue, we propose two attention alignment strategies via temperature scaling.
Our findings show improvement on the long-context utilization capability of T5
on language modeling, retrieval, multi-document question answering, and code
completion tasks without any fine-tuning. This suggests that a flexible
positional embedding design and attention alignment can go a long way toward
Transformer length extrapolation.",None,-1
Explainable Predictive Maintenance,0.766345,"Explainable Artificial Intelligence (XAI) fills the role of a critical
interface fostering interactions between sophisticated intelligent systems and
diverse individuals, including data scientists, domain experts, end-users, and
more. It aids in deciphering the intricate internal mechanisms of ``black box''
Machine Learning (ML), rendering the reasons behind their decisions more
understandable. However, current research in XAI primarily focuses on two
aspects; ways to facilitate user trust, or to debug and refine the ML model.
The majority of it falls short of recognising the diverse types of explanations
needed in broader contexts, as different users and varied application areas
necessitate solutions tailored to their specific needs.
  One such domain is Predictive Maintenance (PdM), an exploding area of
research under the Industry 4.0 \& 5.0 umbrella. This position paper highlights
the gap between existing XAI methodologies and the specific requirements for
explanations within industrial applications, particularly the Predictive
Maintenance field. Despite explainability's crucial role, this subject remains
a relatively under-explored area, making this paper a pioneering attempt to
bring relevant challenges to the research community's attention. We provide an
overview of predictive maintenance tasks and accentuate the need and varying
purposes for corresponding explanations. We then list and describe XAI
techniques commonly employed in the literature, discussing their suitability
for PdM tasks. Finally, to make the ideas and claims more concrete, we
demonstrate XAI applied in four specific industrial use cases: commercial
vehicles, metro trains, steel plants, and wind farms, spotlighting areas
requiring further research.",None,-1
Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning,0.999377,"The Mixture of Experts (MoE) is a widely known neural architecture where an
ensemble of specialized sub-models optimizes overall performance with a
constant computational cost. However, conventional MoEs pose challenges at
scale due to the need to store all experts in memory. In this paper, we push
MoE to the limit. We propose extremely parameter-efficient MoE by uniquely
combining MoE architecture with lightweight experts.Our MoE architecture
outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on
par with full fine-tuning by only updating the lightweight experts -- less than
1% of an 11B parameters model. Furthermore, our method generalizes to unseen
tasks as it does not depend on any prior task knowledge. Our research
underscores the versatility of the mixture of experts architecture, showcasing
its ability to deliver robust performance even when subjected to rigorous
parameter constraints. Our code used in all the experiments is publicly
available here: https://github.com/for-ai/parameter-efficient-moe.",None,-1
Multi-Architecture Multi-Expert Diffusion Models,0.210236,"In this paper, we address the performance degradation of efficient diffusion
models by introducing Multi-architecturE Multi-Expert diffusion models (MEME).
We identify the need for tailored operations at different time-steps in
diffusion processes and leverage this insight to create compact yet
high-performing models. MEME assigns distinct architectures to different
time-step intervals, balancing convolution and self-attention operations based
on observed frequency characteristics. We also introduce a soft interval
assignment strategy for comprehensive training. Empirically, MEME operates 3.3
times faster than baselines while improving image generation quality (FID
scores) by 0.62 (FFHQ) and 0.37 (CelebA). Though we validate the effectiveness
of assigning more optimal architecture per time-step, where efficient models
outperform the larger models, we argue that MEME opens a new design choice for
diffusion models that can be easily applied in other scenarios, such as large
multi-expert models.",None,-1
Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models,0.291648,"Recent studies in natural language processing (NLP) have focused on modern
languages and achieved state-of-the-art results in many tasks. Meanwhile,
little attention has been paid to ancient texts and related tasks. Classical
Chinese first came to Japan approximately 2,000 years ago. It was gradually
adapted to a Japanese form called Kanbun-Kundoku (Kanbun) in Japanese reading
and translating methods, which has significantly impacted Japanese literature.
However, compared to the rich resources for ancient texts in mainland China,
Kanbun resources remain scarce in Japan. To solve this problem, we construct
the first Classical-Chinese-to-Kanbun dataset in the world. Furthermore, we
introduce two tasks, character reordering and machine translation, both of
which play a significant role in Kanbun comprehension. We also test the current
language models on these tasks and discuss the best evaluation method by
comparing the results with human scores. We release our code and dataset on
GitHub.",None,-1
Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation,0.789349,"Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a ""Game-of-Thoughts"". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.",None,-1
Personal Protective Equipment Detection in Extreme Construction Conditions,0.398769,"Object detection has been widely applied for construction safety management,
especially personal protective equipment (PPE) detection. Though the existing
PPE detection models trained on conventional datasets have achieved excellent
results, their performance dramatically declines in extreme construction
conditions. A robust detection model NST-YOLOv5 is developed by combining the
neural style transfer (NST) and YOLOv5 technologies. Five extreme conditions
are considered and simulated via the NST module to endow the detection model
with excellent robustness, including low light, intense light, sand dust, fog,
and rain. Experiments show that the NST has great potential as a tool for
extreme data synthesis since it is better at simulating extreme conditions than
other traditional image processing algorithms and helps the NST-YOLOv5 achieve
0.141 and 0.083 mAP_(05:95) improvements in synthesized and real-world extreme
data. This study provides a new feasible way to obtain a more robust detection
model for extreme construction conditions.",None,-1
PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking,0.999102,"We introduce PointOdyssey, a large-scale synthetic dataset, and data
generation framework, for the training and evaluation of long-term fine-grained
tracking algorithms. Our goal is to advance the state-of-the-art by placing
emphasis on long videos with naturalistic motion. Toward the goal of
naturalism, we animate deformable characters using real-world motion capture
data, we build 3D scenes to match the motion capture environments, and we
render camera viewpoints using trajectories mined via structure-from-motion on
real videos. We create combinatorial diversity by randomizing character
appearance, motion profiles, materials, lighting, 3D assets, and atmospheric
effects. Our dataset currently includes 104 videos, averaging 2,000 frames
long, with orders of magnitude more correspondence annotations than prior work.
We show that existing methods can be trained from scratch in our dataset and
outperform the published variants. Finally, we introduce modifications to the
PIPs point tracking method, greatly widening its temporal receptive field,
which improves its performance on PointOdyssey as well as on two real-world
benchmarks. Our data and code are publicly available at:
https://pointodyssey.com",None,-1
Application-Agnostic Language Modeling for On-Device ASR,0.639628,"On-device automatic speech recognition systems face several challenges
compared to server-based systems. They have to meet stricter constraints in
terms of speed, disk size and memory while maintaining the same accuracy. Often
they have to serve several applications with different distributions at once,
such as communicating with a virtual assistant and speech-to-text. The simplest
solution to serve multiple applications is to build application-specific
(language) models, but this leads to an increase in memory. Therefore, we
explore different data- and architecture-driven language modeling approaches to
build a single application-agnostic model. We propose two novel feed-forward
architectures that find an optimal trade off between different on-device
constraints. In comparison to the application-specific solution, one of our
novel approaches reduces the disk size by half, while maintaining speed and
accuracy of the original model.",None,-1
Text2shape Deep Retrieval Model: Generating Initial Cases for Mechanical Part Redesign under the Context of Case-Based Reasoning,0.0888435,"Retrieving the similar solutions from the historical case base for new design
requirements is the first step in mechanical part redesign under the context of
case-based reasoning. However, the manual retrieving method has the problem of
low efficiency when the case base is large. Additionally, it is difficult for
simple reasoning algorithms (e.g., rule-based reasoning, decision tree) to
cover all the features in complicated design solutions. In this regard, a
text2shape deep retrieval model is established in order to support text
description-based mechanical part shapes retrieval, where the texts are for
describing the structural features of the target mechanical parts. More
specifically, feature engineering is applied to identify the key structural
features of the target mechanical parts. Based on the identified key structural
features, a training set of 1000 samples was constructed, where each sample
consisted of a paragraph of text description of a group of structural features
and the corresponding 3D shape of the structural features. RNN and 3D CNN
algorithms were customized to build the text2shape deep retrieval model.
Orthogonal experiments were used for modeling turning. Eventually, the highest
accuracy of the model was 0.98; therefore, the model can be effective for
retrieving initial cases for mechanical part redesign.",None,-1
Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages,0.250751,"The study investigates the effectiveness of utilizing multimodal information
in Neural Machine Translation (NMT). While prior research focused on using
multimodal data in low-resource scenarios, this study examines how image
features impact translation when added to a large-scale, pre-trained unimodal
NMT system. Surprisingly, the study finds that images might be redundant in
this context. Additionally, the research introduces synthetic noise to assess
whether images help the model deal with textual noise. Multimodal models
slightly outperform text-only models in noisy settings, even with random
images. The study's experiments translate from English to Hindi, Bengali, and
Malayalam, outperforming state-of-the-art benchmarks significantly.
Interestingly, the effect of visual context varies with source text noise: no
visual context works best for non-noisy translations, cropped image features
are optimal for low noise, and full image features work better in high-noise
scenarios. This sheds light on the role of visual context, especially in noisy
settings, opening up a new research direction for Noisy Neural Machine
Translation in multimodal setups. The research emphasizes the importance of
combining visual and textual information for improved translation in various
environments.",None,-1
Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion,0.846209,"Although generative AI has been successful in many areas, its ability to
model geospatial data is still underexplored. Urban flow, a typical kind of
geospatial data, is critical for a wide range of urban applications. Existing
studies mostly focus on predictive modeling of urban flow that predicts the
future flow based on historical flow data, which may be unavailable in
data-sparse areas or newly planned regions. Some other studies aim to predict
OD flow among regions but they fail to model dynamic changes of urban flow over
time. In this work, we study a new problem of urban flow generation that
generates dynamic urban flow for regions without historical flow data. To
capture the effect of multiple factors on urban flow, such as region features
and urban environment, we employ diffusion model to generate urban flow for
regions under different conditions. We first construct an urban knowledge graph
(UKG) to model the urban environment and relationships between regions, based
on which we design a knowledge-enhanced spatio-temporal diffusion model
(KSTDiff) to generate urban flow for each region. Specifically, to accurately
generate urban flow for regions with different flow volumes, we design a novel
diffusion process guided by a volume estimator, which is learnable and
customized for each region. Moreover, we propose a knowledge-enhanced denoising
network to capture the spatio-temporal dependencies of urban flow as well as
the impact of urban environment in the denoising process. Extensive experiments
on four real-world datasets validate the superiority of our model over
state-of-the-art baselines in urban flow generation. Further in-depth studies
demonstrate the utility of generated urban flow data and the ability of our
model for long-term flow generation and urban flow prediction. Our code is
released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.",None,-1
Efficient OCR for Building a Diverse Digital History,0.91978,"Thousands of users consult digital archives daily, but the information they
can access is unrepresentative of the diversity of documentary history. The
sequence-to-sequence architecture typically used for optical character
recognition (OCR) - which jointly learns a vision and language model - is
poorly extensible to low-resource document collections, as learning a
language-vision model requires extensive labeled sequences and compute. This
study models OCR as a character level image retrieval problem, using a
contrastively trained vision encoder. Because the model only learns characters'
visual features, it is more sample efficient and extensible than existing
architectures, enabling accurate OCR in settings where existing solutions fail.
Crucially, the model opens new avenues for community engagement in making
digital history more representative of documentary history.",None,-1
Towards Multi-Layered 3D Garments Animation,0.491815,"Mimicking realistic dynamics in 3D garment animations is a challenging task
due to the complex nature of multi-layered garments and the variety of outer
forces involved. Existing approaches mostly focus on single-layered garments
driven by only human bodies and struggle to handle general scenarios. In this
paper, we propose a novel data-driven method, called LayersNet, to model
garment-level animations as particle-wise interactions in a micro physics
system. We improve simulation efficiency by representing garments as
patch-level particles in a two-level structural hierarchy. Moreover, we
introduce a novel Rotation Equivalent Transformation that leverages the
rotation invariance and additivity of physics systems to better model outer
forces. To verify the effectiveness of our approach and bridge the gap between
experimental environments and real-world scenarios, we introduce a new
challenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900
different combinations of multi-layered garments driven by both human bodies
and randomly sampled wind. Our experiments show that LayersNet achieves
superior performance both quantitatively and qualitatively. We will make the
dataset and code publicly available at
https://mmlab-ntu.github.io/project/layersnet/index.html .",None,-1
The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward,0.0738586,"Artificial intelligence (AI) promises immense benefits across sectors, yet
also poses risks from dual-use potentials, biases, and unintended behaviors.
This paper reviews emerging issues with opaque and uncontrollable AI systems
and proposes an integrative framework called violet teaming to develop reliable
and responsible AI. Violet teaming combines adversarial vulnerability probing
(red teaming) with solutions for safety and security (blue teaming) while
prioritizing ethics and social benefit. It emerged from AI safety research to
manage risks proactively by design. The paper traces the evolution of red,
blue, and purple teaming toward violet teaming, and then discusses applying
violet techniques to address biosecurity risks of AI in biotechnology.
Additional sections review key perspectives across law, ethics, cybersecurity,
macrostrategy, and industry best practices essential for operationalizing
responsible AI through holistic technical and social considerations. Violet
teaming provides both philosophy and method for steering AI trajectories toward
societal good. With conscience and wisdom, the extraordinary capabilities of AI
can enrich humanity. But without adequate precaution, the risks could prove
catastrophic. Violet teaming aims to empower moral technology for the common
welfare.",None,-1
Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table,0.269592,"Despite recent interest in open domain question answering (ODQA) over tables,
many studies still rely on datasets that are not truly optimal for the task
with respect to utilizing structural nature of table. These datasets assume
answers reside as a single cell value and do not necessitate exploring over
multiple cells such as aggregation, comparison, and sorting. Thus, we release
Open-WikiTable, the first ODQA dataset that requires complex reasoning over
tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be
applicable in the open-domain setting. As each question is coupled with both
textual answers and SQL queries, Open-WikiTable opens up a wide range of
possibilities for future research, as both reader and parser methods can be
applied. The dataset and code are publicly available.",None,-1
Asymptotic Convergence and Performance of Multi-Agent Q-Learning Dynamics,0.806154,"Achieving convergence of multiple learning agents in general $N$-player games
is imperative for the development of safe and reliable machine learning (ML)
algorithms and their application to autonomous systems. Yet it is known that,
outside the bounds of simple two-player games, convergence cannot be taken for
granted.
  To make progress in resolving this problem, we study the dynamics of smooth
Q-Learning, a popular reinforcement learning algorithm which quantifies the
tendency for learning agents to explore their state space or exploit their
payoffs. We show a sufficient condition on the rate of exploration such that
the Q-Learning dynamics is guaranteed to converge to a unique equilibrium in
any game. We connect this result to games for which Q-Learning is known to
converge with arbitrary exploration rates, including weighted Potential games
and weighted zero sum polymatrix games.
  Finally, we examine the performance of the Q-Learning dynamic as measured by
the Time Averaged Social Welfare, and comparing this with the Social Welfare
achieved by the equilibrium. We provide a sufficient condition whereby the
Q-Learning dynamic will outperform the equilibrium even if the dynamics do not
converge.",None,-1
Estimating Distances Between People using a Single Overhead Fisheye Camera with Application to Social-Distancing Oversight,0.182092,"Unobtrusive monitoring of distances between people indoors is a useful tool
in the fight against pandemics. A natural resource to accomplish this are
surveillance cameras. Unlike previous distance estimation methods, we use a
single, overhead, fisheye camera with wide area coverage and propose two
approaches. One method leverages a geometric model of the fisheye lens, whereas
the other method uses a neural network to predict the 3D-world distance from
people-locations in a fisheye image. To evaluate our algorithms, we collected a
first-of-its-kind dataset using single fisheye camera, that comprises a wide
range of distances between people (1-58 ft) and will be made publicly
available. The algorithms achieve 1-2 ft distance error and over 95% accuracy
in detecting social-distance violations.",None,-1
MMVP: Motion-Matrix-based Video Prediction,0.727268,"A central challenge of video prediction lies where the system has to reason
the objects' future motions from image frames while simultaneously maintaining
the consistency of their appearances across frames. This work introduces an
end-to-end trainable two-stream video prediction framework, Motion-Matrix-based
Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that
usually handle motion prediction and appearance maintenance within the same set
of modules, MMVP decouples motion and appearance information by constructing
appearance-agnostic motion matrices. The motion matrices represent the temporal
similarity of each and every pair of feature patches in the input frames, and
are the sole input of the motion prediction module in MMVP. This design
improves video prediction in both accuracy and efficiency, and reduces the
model size. Results of extensive experiments demonstrate that MMVP outperforms
state-of-the-art systems on public data sets by non-negligible large margins
(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the
size or smaller).",None,-1
RECLIP: Resource-efficient CLIP by Training with Small Images,0.267802,"We present RECLIP (Resource-efficient CLIP), a simple method that minimizes
computational resource footprint for CLIP (Contrastive Language Image
Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we
leverage small images to learn from large-scale language supervision
efficiently, and finetune the model with high-resolution data in the end. Since
the complexity of the vision transformer heavily depends on input image size,
our approach significantly reduces the training resource requirements both in
theory and in practice. Using the same batch size and training epoch, RECLIP
achieves highly competitive zero-shot classification and image-text retrieval
accuracy with 6 to 8x less computational resources and 7 to 9x fewer FLOPs than
the baseline. Compared to the state-of-the-art contrastive learning methods,
RECLIP demonstrates 5 to 59x training resource savings while maintaining highly
competitive zero-shot classification and retrieval performance. Finally, RECLIP
matches the state of the art in transfer learning to open-vocabulary detection
tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the
broader research community to explore language supervised pretraining in
resource-friendly settings.",None,-1
Anytime Approximate Formal Feature Attribution,0.631019,"Widespread use of artificial intelligence (AI) algorithms and machine
learning (ML) models on the one hand and a number of crucial issues pertaining
to them warrant the need for explainable artificial intelligence (XAI). A key
explainability question is: given this decision was made, what are the input
features which contributed to the decision? Although a range of XAI approaches
exist to tackle this problem, most of them have significant limitations.
Heuristic XAI approaches suffer from the lack of quality guarantees, and often
try to approximate Shapley values, which is not the same as explaining which
features contribute to a decision. A recent alternative is so-called formal
feature attribution (FFA), which defines feature importance as the fraction of
formal abductive explanations (AXp's) containing the given feature. This
measures feature importance from the view of formally reasoning about the
model's behavior. It is challenging to compute FFA using its definition because
that involves counting AXp's, although one can approximate it. Based on these
results, this paper makes several contributions. First, it gives compelling
evidence that computing FFA is intractable, even if the set of contrastive
formal explanations (CXp's) is provided, by proving that the problem is
#P-hard. Second, by using the duality between AXp's and CXp's, it proposes an
efficient heuristic to switch from CXp enumeration to AXp enumeration
on-the-fly resulting in an adaptive explanation enumeration algorithm
effectively approximating FFA in an anytime fashion. Finally, experimental
results obtained on a range of widely used datasets demonstrate the
effectiveness of the proposed FFA approximation approach in terms of the error
of FFA approximation as well as the number of explanations computed and their
diversity given a fixed time limit.",None,-1
BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion Tracking,0.395527,"Robust, fast, and accurate human state - 6D pose and posture - estimation
remains a challenging problem. For real-world applications, the ability to
estimate the human state in real-time is highly desirable. In this paper, we
present BodySLAM++, a fast, efficient, and accurate human and camera state
estimation framework relying on visual-inertial data. BodySLAM++ extends an
existing visual-inertial state estimation framework, OKVIS2, to solve the dual
task of estimating camera and human states simultaneously. Our system improves
the accuracy of both human and camera state estimation with respect to baseline
methods by 26% and 12%, respectively, and achieves real-time performance at 15+
frames per second on an Intel i7-model CPU. Experiments were conducted on a
custom dataset containing both ground truth human and camera poses collected
with an indoor motion tracking system.",None,-1
A Pseudo-Boolean Polynomials Approach for Image Edge Detection,0.28323,"We introduce a novel approach for image edge detection based on
pseudo-Boolean polynomials for image patches. We show that patches covering
edge regions in the image result in pseudo-Boolean polynomials with higher
degrees compared to patches that cover blob regions. The proposed approach is
based on reduction of polynomial degree and equivalence properties of
penalty-based pseudo-Boolean polynomials.",None,-1
On Modeling Network Slicing Communication Resources with SARSA Optimization,0.318434,"Network slicing is a crucial enabler to support the composition and
deployment of virtual network infrastructures required by the dynamic behavior
of networks like 5G/6G mobile networks, IoT-aware networks, e-health systems,
and industry verticals like the internet of vehicles (IoV) and industry 4.0.
The communication slices and their allocated communication resources are
essential in slicing architectures for resource orchestration and allocation,
virtual network function (VNF) deployment, and slice operation functionalities.
The communication slices provide the communications capabilities required to
support slice operation, SLA guarantees, and QoS/ QoE application requirements.
Therefore, this contribution proposes a networking slicing conceptual model to
formulate the optimization problem related to the sharing of communication
resources among communication slices. First, we present a conceptual model of
network slicing, we then formulate analytically some aspects of the model and
the optimization problem to address. Next, we proposed to use a SARSA agent to
solve the problem and implement a proof of concept prototype. Finally, we
present the obtained results and discuss them.",None,-1
Generalized Universal Domain Adaptation with Generative Flow Networks,0.483344,"We introduce a new problem in unsupervised domain adaptation, termed as
Generalized Universal Domain Adaptation (GUDA), which aims to achieve precise
prediction of all target labels including unknown categories. GUDA bridges the
gap between label distribution shift-based and label space mismatch-based
variants, essentially categorizing them as a unified problem, guiding to a
comprehensive framework for thoroughly solving all the variants. The key
challenge of GUDA is developing and identifying novel target categories while
estimating the target label distribution. To address this problem, we take
advantage of the powerful exploration capability of generative flow networks
and propose an active domain adaptation algorithm named GFlowDA, which selects
diverse samples with probabilities proportional to a reward function. To
enhance the exploration capability and effectively perceive the target label
distribution, we tailor the states and rewards, and introduce an efficient
solution for parent exploration and state transition. We also propose a
training paradigm for GUDA called Generalized Universal Adversarial Network
(GUAN), which involves collaborative optimization between GUAN and GFlowNet.
Theoretical analysis highlights the importance of exploration, and extensive
experiments on benchmark datasets demonstrate the superiority of GFlowDA.",None,-1
SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised Point Cloud Shape Correspondence,0.662247,"Unsupervised point cloud shape correspondence aims to obtain dense
point-to-point correspondences between point clouds without manually annotated
pairs. However, humans and some animals have bilateral symmetry and various
orientations, which lead to severe mispredictions of symmetrical parts.
Besides, point cloud noise disrupts consistent representations for point cloud
and thus degrades the shape correspondence accuracy. To address the above
issues, we propose a Self-Ensembling ORientation-aware Network termed SE-ORNet.
The key of our approach is to exploit an orientation estimation module with a
domain adaptive discriminator to align the orientations of point cloud pairs,
which significantly alleviates the mispredictions of symmetrical parts.
Additionally, we design a selfensembling framework for unsupervised point cloud
shape correspondence. In this framework, the disturbances of point cloud noise
are overcome by perturbing the inputs of the student and teacher networks with
different data augmentations and constraining the consistency of predictions.
Extensive experiments on both human and animal datasets show that our SE-ORNet
can surpass state-of-the-art unsupervised point cloud shape correspondence
methods.",None,-1
Unsupervised Image Denoising with Score Function,0.0704568,"Though achieving excellent performance in some cases, current unsupervised
learning methods for single image denoising usually have constraints in
applications. In this paper, we propose a new approach which is more general
and applicable to complicated noise models. Utilizing the property of score
function, the gradient of logarithmic probability, we define a solving system
for denoising. Once the score function of noisy images has been estimated, the
denoised result can be obtained through the solving system. Our approach can be
applied to multiple noise models, such as the mixture of multiplicative and
additive noise combined with structured correlation. Experimental results show
that our method is comparable when the noise model is simple, and has good
performance in complicated cases where other methods are not applicable or
perform poorly.",None,-1
UOR: Universal Backdoor Attacks on Pre-trained Language Models,0.362869,"Backdoors implanted in pre-trained language models (PLMs) can be transferred
to various downstream tasks, which exposes a severe security threat. However,
most existing backdoor attacks against PLMs are un-targeted and task-specific.
Few targeted and task-agnostic methods use manually pre-defined triggers and
output representations, which prevent the attacks from being more effective and
general. In this paper, we first summarize the requirements that a more
threatening backdoor attack against PLMs should satisfy, and then propose a new
backdoor attack method called UOR, which breaks the bottleneck of the previous
approach by turning manual selection into automatic optimization. Specifically,
we define poisoned supervised contrastive learning which can automatically
learn the more uniform and universal output representations of triggers for
various PLMs. Moreover, we use gradient search to select appropriate trigger
words which can be adaptive to different PLMs and vocabularies. Experiments
show that our method can achieve better attack performance on various text
classification tasks compared to manual methods. Further, we tested our method
on PLMs with different architectures, different usage paradigms, and more
difficult tasks, which demonstrated the universality of our method.",None,-1
Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring,0.775477,"Automated essay scoring (AES) aims to score essays written for a given
prompt, which defines the writing topic. Most existing AES systems assume to
grade essays of the same prompt as used in training and assign only a holistic
score. However, such settings conflict with real-education situations;
pre-graded essays for a particular prompt are lacking, and detailed trait
scores of sub-rubrics are required. Thus, predicting various trait scores of
unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining
challenge of AES. In this paper, we propose a robust model: prompt- and trait
relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay
representation by essay-prompt attention and utilizing the topic-coherence
feature extracted by the topic-modeling mechanism without access to labeled
data; therefore, our model considers the prompt adherence of an essay, even in
a cross-prompt setting. To facilitate multi-trait scoring, we design
trait-similarity loss that encapsulates the correlations of traits. Experiments
prove the efficacy of our model, showing state-of-the-art results for all
prompts and traits. Significant improvements in low-resource-prompt and
inferior traits further indicate our model's strength.",None,-1
Effective Real Image Editing with Accelerated Iterative Diffusion Inversion,0.737315,"Despite all recent progress, it is still challenging to edit and manipulate
natural images with modern generative models. When using Generative Adversarial
Network (GAN), one major hurdle is in the inversion process mapping a real
image to its corresponding noise vector in the latent space, since its
necessary to be able to reconstruct an image to edit its contents. Likewise for
Denoising Diffusion Implicit Models (DDIM), the linearization assumption in
each inversion step makes the whole deterministic inversion process unreliable.
Existing approaches that have tackled the problem of inversion stability often
incur in significant trade-offs in computational efficiency. In this work we
propose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that
significantly improves reconstruction accuracy with minimal additional overhead
in space and time complexity. By using a novel blended guidance technique, we
show that effective results can be obtained on a large range of image editing
tasks without large classifier-free guidance in inversion. Furthermore, when
compared with other diffusion inversion based works, our proposed process is
shown to be more robust for fast image editing in the 10 and 20 diffusion
steps' regimes.",None,-1
MagicEdit: High-Fidelity and Temporally Coherent Video Editing,0.810554,"In this report, we present MagicEdit, a surprisingly simple yet effective
solution to the text-guided video editing task. We found that high-fidelity and
temporally coherent video-to-video translation can be achieved by explicitly
disentangling the learning of content, structure and motion signals during
training. This is in contradict to most existing methods which attempt to
jointly model both the appearance and temporal representation within a single
framework, which we argue, would lead to degradation in per-frame quality.
Despite its simplicity, we show that MagicEdit supports various downstream
video editing tasks, including video stylization, local editing, video-MagicMix
and video outpainting.",None,-1
Scale-Equivariant UNet for Histopathology Image Segmentation,0.451235,"Digital histopathology slides are scanned and viewed under different
magnifications and stored as images at different resolutions. Convolutional
Neural Networks (CNNs) trained on such images at a given scale fail to
generalise to those at different scales. This inability is often addressed by
augmenting training data with re-scaled images, allowing a model with
sufficient capacity to learn the requisite patterns. Alternatively, designing
CNN filters to be scale-equivariant frees up model capacity to learn
discriminative features. In this paper, we propose the Scale-Equivariant UNet
(SEUNet) for image segmentation by building on scale-space theory. The SEUNet
contains groups of filters that are linear combinations of Gaussian basis
filters, whose scale parameters are trainable but constrained to span disjoint
scales through the layers of the network. Extensive experiments on a nuclei
segmentation dataset and a tissue type segmentation dataset demonstrate that
our method outperforms other approaches, with much fewer trainable parameters.",None,-1
The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings,0.5704,"Numerous works use word embedding-based metrics to quantify societal biases
and stereotypes in texts. Recent studies have found that word embeddings can
capture semantic similarity but may be affected by word frequency. In this work
we study the effect of frequency when measuring female vs. male gender bias
with word embedding-based bias quantification methods. We find that Skip-gram
with negative sampling and GloVe tend to detect male bias in high frequency
words, while GloVe tends to return female bias in low frequency words. We show
these behaviors still exist when words are randomly shuffled. This proves that
the frequency-based effect observed in unshuffled corpora stems from properties
of the metric rather than from word associations. The effect is spurious and
problematic since bias metrics should depend exclusively on word co-occurrences
and not individual word frequencies. Finally, we compare these results with the
ones obtained with an alternative metric based on Pointwise Mutual Information.
We find that this metric does not show a clear dependence on frequency, even
though it is slightly skewed towards male bias across all frequencies.",None,-1
Zero-shot Visual Question Answering with Language Model Feedback,0.236799,"In this paper, we propose a novel language model guided captioning approach,
LAMOC, for knowledge-based visual question answering (VQA). Our approach
employs the generated captions by a captioning model as the context of an
answer prediction model, which is a Pre-trained Language model (PLM). As the
major contribution, we leverage the guidance and feedback of the prediction
model to improve the capability of the captioning model. In this way, the
captioning model can become aware of the task goal and information need from
the PLM. To develop our approach, we design two specific training stages, where
the first stage adapts the captioning model to the prediction model (selecting
more suitable caption propositions for training) and the second stage tunes the
captioning model according to the task goal (learning from feedback of the
PLM). Extensive experiments demonstrate the effectiveness of the proposed
approach on the knowledge-based VQA task. Specifically, on the challenging
A-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and
even achieves comparable results to a fine-tuned VLP model. Our code is
publicly available at https://github.com/RUCAIBox/LAMOC.",None,-1
Learning Logic Specifications for Soft Policy Guidance in POMCP,0.851188,"Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for
Partially Observable Markov Decision Processes (POMDPs). It allows scaling to
large state spaces by computing an approximation of the optimal policy locally
and online, using a Monte Carlo Tree Search based strategy. However, POMCP
suffers from sparse reward function, namely, rewards achieved only when the
final goal is reached, particularly in environments with large state spaces and
long horizons. Recently, logic specifications have been integrated into POMCP
to guide exploration and to satisfy safety requirements. However, such
policy-related rules require manual definition by domain experts, especially in
real-world scenarios. In this paper, we use inductive logic programming to
learn logic specifications from traces of POMCP executions, i.e., sets of
belief-action pairs generated by the planner. Specifically, we learn rules
expressed in the paradigm of answer set programming. We then integrate them
inside POMCP to provide soft policy bias toward promising actions. In the
context of two benchmark scenarios, rocksample and battery, we show that the
integration of learned rules from small task instances can improve performance
with fewer Monte Carlo simulations and in larger task instances. We make our
modified version of POMCP publicly available at
https://github.com/GiuMaz/pomcp_clingo.git.",None,-1
Benchmarking fixed-length Fingerprint Representations across different Embedding Sizes and Sensor Types,0.319943,"Traditional minutiae-based fingerprint representations consist of a
variable-length set of minutiae. This necessitates a more complex comparison
causing the drawback of high computational cost in one-to-many comparison.
Recently, deep neural networks have been proposed to extract fixed-length
embeddings from fingerprints. In this paper, we explore to what extent
fingerprint texture information contained in such embeddings can be reduced in
terms of dimension while preserving high biometric performance. This is of
particular interest since it would allow to reduce the number of operations
incurred at comparisons. We also study the impact in terms of recognition
performance of the fingerprint textural information for two sensor types, i.e.
optical and capacitive. Furthermore, the impact of rotation and translation of
fingerprint images on the extraction of fingerprint embeddings is analysed.
Experimental results conducted on a publicly available database reveal an
optimal embedding size of 512 feature elements for the texture-based embedding
part of fixed-length fingerprint representations. In addition, differences in
performance between sensor types can be perceived.",None,-1
Provable Data Subset Selection For Efficient Neural Network Training,0.166961,"Radial basis function neural networks (\emph{RBFNN}) are {well-known} for
their capability to approximate any continuous function on a closed bounded set
with arbitrary precision given enough hidden neurons. In this paper, we
introduce the first algorithm to construct coresets for \emph{RBFNNs}, i.e.,
small weighted subsets that approximate the loss of the input data on any
radial basis function network and thus approximate any function defined by an
\emph{RBFNN} on the larger input data. In particular, we construct coresets for
radial basis and Laplacian loss functions. We then use our coresets to obtain a
provable data subset selection algorithm for training deep neural networks.
Since our coresets approximate every function, they also approximate the
gradient of each weight in a neural network, which is a particular function on
the input. We then perform empirical evaluations on function approximation and
dataset subset selection on popular network architectures and data sets,
demonstrating the efficacy and accuracy of our coreset construction.",None,-1
Spatiotemporally Consistent HDR Indoor Lighting Estimation,0.758518,"We propose a physically-motivated deep learning framework to solve a general
version of the challenging indoor lighting estimation problem. Given a single
LDR image with a depth map, our method predicts spatially consistent lighting
at any given image position. Particularly, when the input is an LDR video
sequence, our framework not only progressively refines the lighting prediction
as it sees more regions, but also preserves temporal consistency by keeping the
refinement smooth. Our framework reconstructs a spherical Gaussian lighting
volume (SGLV) through a tailored 3D encoder-decoder, which enables spatially
consistent lighting prediction through volume ray tracing, a hybrid blending
network for detailed environment maps, an in-network Monte-Carlo rendering
layer to enhance photorealism for virtual object insertion, and recurrent
neural networks (RNN) to achieve temporally consistent lighting prediction with
a video sequence as the input. For training, we significantly enhance the
OpenRooms public dataset of photorealistic synthetic indoor scenes with around
360K HDR environment maps of much higher resolution and 38K video sequences,
rendered with GPU-based path tracing. Experiments show that our framework
achieves lighting prediction with higher quality compared to state-of-the-art
single-image or video-based methods, leading to photorealistic AR applications
such as object insertion.",None,-1
Multimodal Industrial Anomaly Detection via Hybrid Fusion,0.953207,"2D-based Industrial Anomaly Detection has been widely discussed, however,
multimodal industrial anomaly detection based on 3D point clouds and RGB images
still has many untouched fields. Existing multimodal industrial anomaly
detection methods directly concatenate the multimodal features, which leads to
a strong disturbance between features and harms the detection performance. In
this paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly
detection method with hybrid fusion scheme: firstly, we design an unsupervised
feature fusion with patch-wise contrastive learning to encourage the
interaction of different modal features; secondly, we use a decision layer
fusion with multiple memory banks to avoid loss of information and additional
novelty classifiers to make the final decision. We further propose a point
feature alignment operation to better align the point cloud and RGB features.
Extensive experiments show that our multimodal industrial anomaly detection
model outperforms the state-of-the-art (SOTA) methods on both detection and
segmentation precision on MVTec-3D AD dataset. Code is available at
https://github.com/nomewang/M3DM.",None,-1
Iterative Deepening Hyperband,0.100869,"Hyperparameter optimization (HPO) is concerned with the automated search for
the most appropriate hyperparameter configuration (HPC) of a parameterized
machine learning algorithm. A state-of-the-art HPO method is Hyperband, which,
however, has its own parameters that influence its performance. One of these
parameters, the maximal budget, is especially problematic: If chosen too small,
the budget needs to be increased in hindsight and, as Hyperband is not
incremental by design, the entire algorithm must be re-run. This is not only
costly but also comes with a loss of valuable knowledge already accumulated. In
this paper, we propose incremental variants of Hyperband that eliminate these
drawbacks, and show that these variants satisfy theoretical guarantees
qualitatively similar to those for the original Hyperband with the ""right""
budget. Moreover, we demonstrate their practical utility in experiments with
benchmark data sets.",None,-1
Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches,0.2822,"Anomaly detection in command shell sessions is a critical aspect of computer
security. Recent advances in deep learning and natural language processing,
particularly transformer-based models, have shown great promise for addressing
complex security challenges. In this paper, we implement a comprehensive
approach to detect anomalies in Unix shell sessions using a pretrained
DistilBERT model, leveraging both unsupervised and supervised learning
techniques to identify anomalous activity while minimizing data labeling. The
unsupervised method captures the underlying structure and syntax of Unix shell
commands, enabling the detection of session deviations from normal behavior.
Experiments on a large-scale enterprise dataset collected from production
systems demonstrate the effectiveness of our approach in detecting anomalous
behavior in Unix shell sessions. This work highlights the potential of
leveraging recent advances in transformers to address important computer
security challenges.",None,-1
From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models,0.949464,"Language models (LMs) are pretrained on diverse data sources, including news,
discussion forums, books, and online encyclopedias. A significant portion of
this data includes opinions and perspectives which, on one hand, celebrate
democracy and diversity of ideas, and on the other hand are inherently socially
biased. Our work develops new methods to (1) measure political biases in LMs
trained on such corpora, along social and economic axes, and (2) measure the
fairness of downstream NLP models trained on top of politically biased LMs. We
focus on hate speech and misinformation detection, aiming to empirically
quantify the effects of political (social, economic) biases in pretraining data
on the fairness of high-stakes social-oriented tasks. Our findings reveal that
pretrained LMs do have political leanings that reinforce the polarization
present in pretraining corpora, propagating social biases into hate speech
predictions and misinformation detectors. We discuss the implications of our
findings for NLP research and propose future directions to mitigate unfairness.",None,-1
TomatoDIFF: On-plant Tomato Segmentation with Denoising Diffusion Models,0.078704,"Artificial intelligence applications enable farmers to optimize crop growth
and production while reducing costs and environmental impact. Computer
vision-based algorithms in particular, are commonly used for fruit
segmentation, enabling in-depth analysis of the harvest quality and accurate
yield estimation. In this paper, we propose TomatoDIFF, a novel diffusion-based
model for semantic segmentation of on-plant tomatoes. When evaluated against
other competitive methods, our model demonstrates state-of-the-art (SOTA)
performance, even in challenging environments with highly occluded fruits.
Additionally, we introduce Tomatopia, a new, large and challenging dataset of
greenhouse tomatoes. The dataset comprises high-resolution RGB-D images and
pixel-level annotations of the fruits.",None,-1
Large Language Models Can Be Good Privacy Protection Learners,0.810937,"The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.",None,-1
Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models,0.275378,"We present a novel method, the Chain of Empathy (CoE) prompting, that
utilizes insights from psychotherapy to induce Large Language Models (LLMs) to
reason about human emotional states. This method is inspired by various
psychotherapy approaches including Cognitive Behavioral Therapy (CBT),
Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality
Therapy (RT), each leading to different patterns of interpreting clients'
mental states. LLMs without reasoning generated predominantly exploratory
responses. However, when LLMs used CoE reasoning, we found a more comprehensive
range of empathetic responses aligned with the different reasoning patterns of
each psychotherapy model. The CBT based CoE resulted in the most balanced
generation of empathetic responses. The findings underscore the importance of
understanding the emotional context and how it affects human and AI
communication. Our research contributes to understanding how psychotherapeutic
models can be incorporated into LLMs, facilitating the development of
context-specific, safer, and empathetic AI.",None,-1
DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation,0.436157,"We present DeblurSR, a novel motion deblurring approach that converts a
blurry image into a sharp video. DeblurSR utilizes event data to compensate for
motion ambiguities and exploits the spiking representation to parameterize the
sharp output video as a mapping from time to intensity. Our key contribution,
the Spiking Representation (SR), is inspired by the neuromorphic principles
determining how biological neurons communicate with each other in living
organisms. We discuss why the spikes can represent sharp edges and how the
spiking parameters are interpreted from the neuromorphic perspective. DeblurSR
has higher output quality and requires fewer computing resources than
state-of-the-art event-based motion deblurring methods. We additionally show
that our approach easily extends to video super-resolution when combined with
recent advances in implicit neural representation. The implementation and
animated visualization of DeblurSR are available at
https://github.com/chensong1995/DeblurSR.",None,-1
EdgeFace: Efficient Face Recognition Model for Edge Devices,0.722207,"In this paper, we present EdgeFace, a lightweight and efficient face
recognition network inspired by the hybrid architecture of EdgeNeXt. By
effectively combining the strengths of both CNN and Transformer models, and a
low rank linear layer, EdgeFace achieves excellent face recognition performance
optimized for edge devices. The proposed EdgeFace network not only maintains
low computational costs and compact storage, but also achieves high face
recognition accuracy, making it suitable for deployment on edge devices.
Extensive experiments on challenging benchmark face datasets demonstrate the
effectiveness and efficiency of EdgeFace in comparison to state-of-the-art
lightweight models and deep face recognition models. Our EdgeFace model with
1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B
(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger
computational complexities. The code to replicate the experiments will be made
available publicly.",None,-1
Cut and Learn for Unsupervised Object Detection and Instance Segmentation,0.964317,"We propose Cut-and-LEaRn (CutLER), a simple approach for training
unsupervised object detection and segmentation models. We leverage the property
of self-supervised models to 'discover' objects without supervision and amplify
it to train a state-of-the-art localization model without any human labels.
CutLER first uses our proposed MaskCut approach to generate coarse masks for
multiple objects in an image and then learns a detector on these masks using
our robust loss function. We further improve the performance by self-training
the model on its predictions. Compared to prior work, CutLER is simpler,
compatible with different detection architectures, and detects multiple
objects. CutLER is also a zero-shot unsupervised detector and improves
detection performance AP50 by over 2.7 times on 11 benchmarks across domains
like video frames, paintings, sketches, etc. With finetuning, CutLER serves as
a low-shot detector surpassing MoCo-v2 by 7.3% APbox and 6.6% APmask on COCO
when training with 5% labels.",None,-1
LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain,0.599472,"Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.",None,-1
Hierarchically Gated Recurrent Neural Network for Sequence Modeling,0.821246,"Transformers have surpassed RNNs in popularity due to their superior
abilities in parallel training and long-term dependency modeling. Recently,
there has been a renewed interest in using linear RNNs for efficient sequence
modeling. These linear RNNs often employ gating mechanisms in the output of the
linear recurrence layer while ignoring the significance of using forget gates
within the recurrence. In this paper, we propose a gated linear RNN model
dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes
forget gates that are lower bounded by a learnable value. The lower bound
increases monotonically when moving up layers. This allows the upper layers to
model long-term dependencies and the lower layers to model more local,
short-term dependencies. Experiments on language modeling, image
classification, and long-range arena benchmarks showcase the efficiency and
effectiveness of our proposed model. The source code is available at
https://github.com/OpenNLPLab/HGRN.",None,-1
Harnessing Large Language Models' Empathetic Response Generation Capabilities for Online Mental Health Counselling Support,0.978513,"Large Language Models (LLMs) have demonstrated remarkable performance across
various information-seeking and reasoning tasks. These computational systems
drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also
carry substantial promise in meeting the growing demands of mental health care,
albeit relatively unexplored. As such, this study sought to examine LLMs'
capability to generate empathetic responses in conversations that emulate those
in a mental health counselling setting. We selected five LLMs: version 3.5 and
version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways
Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple
instructional prompt, these models responded to utterances derived from the
EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we
compared their responses to those from traditional response generation dialogue
systems, which were fine-tuned on the ED dataset, along with human-generated
responses. Notably, we discovered that responses from the LLMs were remarkably
more empathetic in most scenarios. We position our findings in light of
catapulting advancements in creating empathetic conversational systems.",None,-1
SCoDA: Domain Adaptive Shape Completion for Real Scans,0.166089,"3D shape completion from point clouds is a challenging task, especially from
scans of real-world objects. Considering the paucity of 3D shape ground truths
for real scans, existing works mainly focus on benchmarking this task on
synthetic data, e.g. 3D computer-aided design models. However, the domain gap
between synthetic and real data limits the generalizability of these methods.
Thus, we propose a new task, SCoDA, for the domain adaptation of real scan
shape completion from synthetic data. A new dataset, ScanSalon, is contributed
with a bunch of elaborate 3D models created by skillful artists according to
scans. To address this new task, we propose a novel cross-domain feature fusion
method for knowledge transfer and a novel volume-consistent self-training
framework for robust learning from real data. Extensive experiments prove our
method is effective to bring an improvement of 6%~7% mIoU.",None,-1
Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems,0.809944,"We present Soda (Symbolic Objective Descriptive Analysis), a language that
helps to treat qualities and quantities in a natural way and greatly simplifies
the task of checking their correctness. We present key properties for the
language motivated by the design of a descriptive language to encode complex
requirements on computer systems, and we explain how these key properties must
be addressed to model these requirements with simple definitions. We give an
overview of a tool that helps to describe problems in an easy way that we
consider more transparent and less error-prone.",None,-1
REFinD: Relation Extraction Financial Dataset,0.722532,"A number of datasets for Relation Extraction (RE) have been created to aide
downstream tasks such as information retrieval, semantic search, question
answering and textual entailment. However, these datasets fail to capture
financial-domain specific challenges since most of these datasets are compiled
using general knowledge sources such as Wikipedia, web-based text and news
articles, hindering real-life progress and adoption within the financial world.
To address this limitation, we propose REFinD, the first large-scale annotated
dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types
of entity pairs, generated entirely over financial documents. We also provide
an empirical evaluation with various state-of-the-art models as benchmarks for
the RE task and highlight the challenges posed by our dataset. We observed that
various state-of-the-art deep learning models struggle with numeric inference,
relational and directional ambiguity.",None,-1
Active Neural Mapping,0.582136,"We address the problem of active mapping with a continually-learned neural
scene representation, namely Active Neural Mapping. The key lies in actively
finding the target space to be explored with efficient agent movement, thus
minimizing the map uncertainty on-the-fly within a previously unseen
environment. In this paper, we examine the weight space of the
continually-learned neural field, and show empirically that the neural
variability, the prediction robustness against random weight perturbation, can
be directly utilized to measure the instant uncertainty of the neural map.
Together with the continuous geometric information inherited in the neural map,
the agent can be guided to find a traversable path to gradually gain knowledge
of the environment. We present for the first time an active mapping system with
a coordinate-based implicit neural representation for online scene
reconstruction. Experiments in the visually-realistic Gibson and Matterport3D
environment demonstrate the efficacy of the proposed method.",None,-1
ESCL: Equivariant Self-Contrastive Learning for Sentence Representations,0.0545415,"Previous contrastive learning methods for sentence representations often
focus on insensitive transformations to produce positive pairs, but neglect the
role of sensitive transformations that are harmful to semantic representations.
Therefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to
make full use of sensitive transformations, which encourages the learned
representations to be sensitive to certain types of transformations with an
additional equivariant learning task. Meanwhile, in order to improve
practicability and generality, ESCL simplifies the implementations of
traditional equivariant contrastive methods to share model parameters from the
perspective of multi-task learning. We evaluate our ESCL on semantic textual
similarity tasks. The proposed method achieves better results while using fewer
learning parameters compared to previous methods.",None,-1
CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction,0.656728,"Recent advances in neural reconstruction using posed image sequences have
made remarkable progress. However, due to the lack of depth information,
existing volumetric-based techniques simply duplicate 2D image features of the
object surface along the entire camera ray. We contend this duplication
introduces noise in empty and occluded spaces, posing challenges for producing
high-quality 3D geometry. Drawing inspiration from traditional multi-view
stereo methods, we propose an end-to-end 3D neural reconstruction framework
CVRecon, designed to exploit the rich geometric embedding in the cost volumes
to facilitate 3D geometric feature learning. Furthermore, we present
Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature
representation that encodes view-dependent information with improved integrity
and robustness. Through comprehensive experiments, we demonstrate that our
approach significantly improves the reconstruction quality in various metrics
and recovers clear fine details of the 3D geometries. Our extensive ablation
studies provide insights into the development of effective 3D geometric feature
learning schemes. Project page: https://cvrecon.ziyue.cool/",None,-1
INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback,0.221501,"Automatically evaluating the quality of language generation is critical.
Although recent learned metrics show high correlation with human judgement,
these metrics can not explain their verdict or associate the scores with
defects in generated text. To address this limitation, we present
InstructScore, an explainable evaluation metric for text generation. By
harnessing both explicit human instruction and the implicit knowledge of GPT-4,
we fine-tune a text evaluation metric based on LLaMA, producing both a score
for generated text and a human readable diagnostic report. We evaluate
InstructScore on a variety of generation tasks, including translation,
captioning, data-to-text and commonsense generation. Experiments show that our
7B model surpasses all other unsupervised metrics, including those based on
175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct
supervision from human-rated data, achieves performance levels on par with
state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.",None,-1
Preference-conditioned Pixel-based AI Agent For Game Testing,0.544551,"The game industry is challenged to cope with increasing growth in demand and
game complexity while maintaining acceptable quality standards for released
games. Classic approaches solely depending on human efforts for quality
assurance and game testing do not scale effectively in terms of time and cost.
Game-testing AI agents that learn by interaction with the environment have the
potential to mitigate these challenges with good scalability properties on time
and costs. However, most recent work in this direction depends on game state
information for the agent's state representation, which limits generalization
across different game scenarios. Moreover, game test engineers usually prefer
exploring a game in a specific style, such as exploring the golden path.
However, current game testing AI agents do not provide an explicit way to
satisfy such a preference. This paper addresses these limitations by proposing
an agent design that mainly depends on pixel-based state observations while
exploring the environment conditioned on a user's preference specified by
demonstration trajectories. In addition, we propose an imitation learning
method that couples self-supervised and supervised learning objectives to
enhance the quality of imitation behaviors. Our agent significantly outperforms
state-of-the-art pixel-based game testing agents over exploration coverage and
test execution quality when evaluated on a complex open-world environment
resembling many aspects of real AAA games.",None,-1
An interpretability framework for Similar case matching,0.101782,"Similar Case Matching (SCM) plays a pivotal role in the legal system by
facilitating the efficient identification of similar cases for legal
professionals. While previous research has primarily concentrated on enhancing
the performance of SCM models, the aspect of interpretability has been
neglected. To bridge the gap, this study proposes an integrated pipeline
framework for interpretable SCM. The framework comprises four modules: judicial
feature sentence identification, case matching, feature sentence alignment, and
conflict resolution. In contrast to current SCM methods, our framework first
extracts feature sentences within a legal case that contain essential
information. Then it conducts case matching based on these extracted features.
Subsequently, our framework aligns the corresponding sentences in two legal
cases to provide evidence of similarity. In instances where the results of case
matching and feature sentence alignment exhibit conflicts, the conflict
resolution module resolves these inconsistencies. The experimental results show
the effectiveness of our proposed framework, establishing a new benchmark for
interpretable SCM.",None,-1
Guideline Learning for In-context Information Extraction,0.665379,"Large language models (LLMs) can perform a new task by merely conditioning on
task instructions and a few input-output examples, without optimizing any
parameters. This is called In-Context Learning (ICL). In-context Information
Extraction (IE) has recently garnered attention in the research community.
However, the performance of In-context IE generally lags behind the
state-of-the-art supervised expert models. We highlight a key reason for this
shortfall: underspecified task description. The limited-length context
struggles to thoroughly express the intricate IE task instructions and various
edge cases, leading to misalignment in task comprehension with humans. In this
paper, we propose a Guideline Learning (GL) framework for In-context IE which
reflectively learns and follows guidelines. During the learning phrase, GL
automatically synthesizes a set of guidelines based on a few error cases, and
during inference, GL retrieves helpful guidelines for better ICL. Moreover, we
propose a self-consistency-based active learning method to enhance the
efficiency of GL. Experiments on event extraction and relation extraction show
that GL can significantly improve the performance of in-context IE.",None,-1
DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents,0.954769,"Vision-language pretraining models have achieved great success in supporting
multimedia applications by understanding the alignments between images and
text. While existing vision-language pretraining models primarily focus on
understanding single image associated with a single piece of text, they often
ignore the alignment at the intra-document level, consisting of multiple
sentences with multiple images. In this work, we propose DocumentCLIP, a
salience-aware contrastive learning framework to enforce vision-language
pretraining models to comprehend the interaction between images and longer text
within documents. Our model is beneficial for the real-world multimodal
document understanding like news article, magazines, product descriptions,
which contain linguistically and visually richer content. To the best of our
knowledge, we are the first to explore multimodal intra-document links by
contrastive learning. In addition, we collect a large Wikipedia dataset for
pretraining, which provides various topics and structures. Experiments show
DocumentCLIP not only outperforms the state-of-the-art baselines in the
supervised setting, but also achieves the best zero-shot performance in the
wild after human evaluation. Our code is available at
https://github.com/FuxiaoLiu/DocumentCLIP.",None,-1
An Efficient Approximate Method for Online Convolutional Dictionary Learning,0.0655538,"Most existing convolutional dictionary learning (CDL) algorithms are based on
batch learning, where the dictionary filters and the convolutional sparse
representations are optimized in an alternating manner using a training
dataset. When large training datasets are used, batch CDL algorithms become
prohibitively memory-intensive. An online-learning technique is used to reduce
the memory requirements of CDL by optimizing the dictionary incrementally after
finding the sparse representations of each training sample. Nevertheless,
learning large dictionaries using the existing online CDL (OCDL) algorithms
remains highly computationally expensive. In this paper, we present a novel
approximate OCDL method that incorporates sparse decomposition of the training
samples. The resulting optimization problems are addressed using the
alternating direction method of multipliers. Extensive experimental evaluations
using several image datasets show that the proposed method substantially
reduces computational costs while preserving the effectiveness of the
state-of-the-art OCDL algorithms.",None,-1
Narrative as a Dynamical System,0.0940448,"There is increasing evidence that human activity in general, and narrative in
particular, can be treated as a dynamical system in the physics sense; a system
whose evolution is described by an action integral, such that the average of
all possible paths from point A to point B is given by the extremum of the
action. We create by construction three such paths by averaging about 500
different narratives, and we show that the average path is consistent with an
action principle.",None,-1
Controllable Path of Destruction,0.0505502,"Path of Destruction (PoD) is a self-supervised method for learning iterative
generators. The core idea is to produce a training set by destroying a set of
artifacts, and for each destructive step create a training instance based on
the corresponding repair action. A generator trained on this dataset can then
generate new artifacts by repairing from arbitrary states. The PoD method is
very data-efficient in terms of original training examples and well-suited to
functional artifacts composed of categorical data, such as game levels and
discrete 3D structures. In this paper, we extend the Path of Destruction method
to allow designer control over aspects of the generated artifacts.
Controllability is introduced by adding conditional inputs to the state-action
pairs that make up the repair trajectories. We test the controllable PoD method
in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",None,-1
Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs,0.710906,"Knowledge graph embeddings (KGE) have been extensively studied to embed
large-scale relational data for many real-world applications. Existing methods
have long ignored the fact many KGs contain two fundamentally different views:
high-level ontology-view concepts and fine-grained instance-view entities. They
usually embed all nodes as vectors in one latent space. However, a single
geometric representation fails to capture the structural differences between
two views and lacks probabilistic semantics towards concepts' granularity. We
propose Concept2Box, a novel approach that jointly embeds the two views of a KG
using dual geometric representations. We model concepts with box embeddings,
which learn the hierarchy structure and complex relations such as overlap and
disjoint among them. Box volumes can be interpreted as concepts' granularity.
Different from concepts, we model entities as vectors. To bridge the gap
between concept box embeddings and entity vector embeddings, we propose a novel
vector-to-box distance metric and learn both embeddings jointly. Experiments on
both the public DBpedia KG and a newly-created industrial KG showed the
effectiveness of Concept2Box.",None,-1
JSEEGraph: Joint Structured Event Extraction as Graph Parsing,0.659386,"We propose a graph-based event extraction framework JSEEGraph that approaches
the task of event extraction as general graph parsing in the tradition of
Meaning Representation Parsing. It explicitly encodes entities and events in a
single semantic graph, and further has the flexibility to encode a wider range
of additional IE relations and jointly infer individual tasks. JSEEGraph
performs in an end-to-end manner via general graph parsing: (1) instead of flat
sequence labelling, nested structures between entities/triggers are efficiently
encoded as separate nodes in the graph, allowing for nested and overlapping
entities and triggers; (2) both entities, relations, and events can be encoded
in the same graph, where entities and event triggers are represented as nodes
and entity relations and event arguments are constructed via edges; (3) joint
inference avoids error propagation and enhances the interpolation of different
IE tasks. We experiment on two benchmark datasets of varying structural
complexities; ACE05 and Rich ERE, covering three languages: English, Chinese,
and Spanish. Experimental results show that JSEEGraph can handle nested event
structures, that it is beneficial to solve different IE tasks jointly, and that
event argument extraction in particular benefits from entity extraction. Our
code and models are released as open-source.",None,-1
Exploiting Diverse Feature for Multimodal Sentiment Analysis,0.102323,"In this paper, we present our solution to the MuSe-Personalisation
sub-challenge in the MuSe 2023 Multimodal Sentiment Analysis Challenge. The
task of MuSe-Personalisation aims to predict the continuous arousal and valence
values of a participant based on their audio-visual, language, and
physiological signal modalities data. Considering different people have
personal characteristics, the main challenge of this task is how to build
robustness feature presentation for sentiment prediction. To address this
issue, we propose exploiting diverse features. Specifically, we proposed a
series of feature extraction methods to build a robust representation and model
ensemble. We empirically evaluate the performance of the utilized method on the
officially provided dataset. \textbf{As a result, we achieved 3rd place in the
MuSe-Personalisation sub-challenge.} Specifically, we achieve the results of
0.8492 and 0.8439 for MuSe-Personalisation in terms of arousal and valence CCC.",None,-1
Computer Assisted Proofs and Automated Methods in Mathematics Education,0.155785,"This survey paper is an expanded version of an invited keynote at the
ThEdu'22 workshop, August 2022, in Haifa (Israel). After a short introduction
on the developments of CAS, DGS and other useful technologies, we show
implications in Mathematics Education, and in the broader frame of STEAM
Education. In particular, we discuss the transformation of Mathematics
Education into exploration-discovery-conjecture-proof scheme, avoiding usage as
a black box . This scheme fits well into the so-called 4 C's of 21st Century
Education. Communication and Collaboration are emphasized not only between
humans, but also between machines, and between man and machine. Specific
characteristics of the outputs enhance the need of Critical Thinking. The usage
of automated commands for exploration and discovery is discussed, with mention
of limitations where they exist. We illustrate the topic with examples from
parametric integrals (describing a ""cognitive neighborhood"" of a mathematical
notion), plane geometry, and the study of plane curves (envelopes, isoptic
curves). Some of the examples are fully worked out, others are explained and
references are given.",None,-1
Exploring Effective Mask Sampling Modeling for Neural Image Compression,0.451188,"Image compression aims to reduce the information redundancy in images. Most
existing neural image compression methods rely on side information from
hyperprior or context models to eliminate spatial redundancy, but rarely
address the channel redundancy. Inspired by the mask sampling modeling in
recent self-supervised learning methods for natural language processing and
high-level vision, we propose a novel pretraining strategy for neural image
compression. Specifically, Cube Mask Sampling Module (CMSM) is proposed to
apply both spatial and channel mask sampling modeling to image compression in
the pre-training stage. Moreover, to further reduce channel redundancy, we
propose the Learnable Channel Mask Module (LCMM) and the Learnable Channel
Completion Module (LCCM). Our plug-and-play CMSM, LCMM, LCCM modules can apply
to both CNN-based and Transformer-based architectures, significantly reduce the
computational cost, and improve the quality of images. Experiments on the
public Kodak and Tecnick datasets demonstrate that our method achieves
competitive performance with lower computational complexity compared to
state-of-the-art image compression methods.",None,-1
Data Driven Reward Initialization for Preference based Reinforcement Learning,0.103422,"Preference-based Reinforcement Learning (PbRL) methods utilize binary
feedback from the human in the loop (HiL) over queried trajectory pairs to
learn a reward model in an attempt to approximate the human's underlying reward
function capturing their preferences. In this work, we investigate the issue of
a high degree of variability in the initialized reward models which are
sensitive to random seeds of the experiment. This further compounds the issue
of degenerate reward functions PbRL methods already suffer from. We propose a
data-driven reward initialization method that does not add any additional cost
to the human in the loop and negligible cost to the PbRL agent and show that
doing so ensures that the predicted rewards of the initialized reward model are
uniform in the state space and this reduces the variability in the performance
of the method across multiple runs and is shown to improve the overall
performance compared to other initialization methods.",None,-1
E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition,0.507972,"Most named entity recognition (NER) systems focus on improving model
performance, ignoring the need to quantify model uncertainty, which is critical
to the reliability of NER systems in open environments. Evidential deep
learning (EDL) has recently been proposed as a promising solution to explicitly
model predictive uncertainty for classification tasks. However, directly
applying EDL to NER applications faces two challenges, i.e., the problems of
sparse entities and OOV/OOD entities in NER tasks. To address these challenges,
we propose a trustworthy NER framework named E-NER by introducing two
uncertainty-guided loss terms to the conventional EDL, along with a series of
uncertainty-guided training strategies. Experiments show that E-NER can be
applied to multiple NER paradigms to obtain accurate uncertainty estimation.
Furthermore, compared to state-of-the-art baselines, the proposed method
achieves a better OOV/OOD detection performance and better generalization
ability on OOV entities.",None,-1
Multilingual Speech-to-Speech Translation into Multiple Target Languages,0.367332,"Speech-to-speech translation (S2ST) enables spoken communication between
people talking in different languages. Despite a few studies on multilingual
S2ST, their focus is the multilinguality on the source side, i.e., the
translation from multiple source languages to one target language. We present
the first work on multilingual S2ST supporting multiple target languages.
Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we
equip these key components with multilingual capability. Speech-to-masked-unit
(S2MU) is the multilingual extension of S2U, which applies masking to units
which don't belong to the given target language to reduce the language
interference. We also propose multilingual vocoder which is trained with
language embedding and the auxiliary loss of language identification. On
benchmark translation testsets, our proposed multilingual model shows superior
performance than bilingual models in the translation from English into $16$
target languages.",None,-1
"Language Models Hallucinate, but May Excel at Fact Verification",0.35762,"Recent progress in natural language processing (NLP) owes much to remarkable
advances in large language models (LLMs). Nevertheless, LLMs frequently
""hallucinate,"" resulting in non-factual outputs. Our carefully-designed human
evaluation substantiates the serious hallucination issue, revealing that even
GPT-3.5 produces factual outputs less than 25% of the time. This underscores
the importance of fact verifiers in order to measure and incentivize progress.
Our systematic investigation affirms that LLMs can be repurposed as effective
fact verifiers with strong correlations with human judgments. Surprisingly,
FLAN-T5-11B, the least factual generator in our study, performs the best as a
fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.
Delving deeper, we analyze the reliance of these LLMs on high-quality evidence,
as well as their deficiencies in robustness and generalization ability. Our
study presents insights for developing trustworthy generation models.",None,-1
THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment,0.98139,"This paper describes the approach of the THUIR team at the COLIEE 2023 Legal
Case Entailment task. This task requires the participant to identify a specific
paragraph from a given supporting case that entails the decision for the query
case. We try traditional lexical matching methods and pre-trained language
models with different sizes. Furthermore, learning-to-rank methods are employed
to further improve performance. However, learning-to-rank is not very robust on
this task. which suggests that answer passages cannot simply be determined with
information retrieval techniques. Experimental results show that more
parameters and legal knowledge contribute to the legal case entailment task.
Finally, we get the third place in COLIEE 2023. The implementation of our
method can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.",None,-1
Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting,0.772749,"Automatically generated reports from medical images promise to improve the
workflow of radiologists. Existing methods consider an image-to-report modeling
task by directly generating a fully-fledged report from an image. However, this
conflates the content of the report (e.g., findings and their attributes) with
its style (e.g., format and choice of words), which can lead to clinically
inaccurate reports. To address this, we propose a two-step approach for
radiology report generation. First, we extract the content from an image; then,
we verbalize the extracted content into a report that matches the style of a
specific radiologist. For this, we leverage RadGraph -- a graph representation
of reports -- together with large language models (LLMs). In our quantitative
evaluations, we find that our approach leads to beneficial performance. Our
human evaluation with clinical raters highlights that the AI-generated reports
are indistinguishably tailored to the style of individual radiologist despite
leveraging only a few examples as context.",None,-1
The Making and Breaking of Camouflage,0.0794919,"Not all camouflages are equally effective, as even a partially visible
contour or a slight color difference can make the animal stand out and break
its camouflage. In this paper, we address the question of what makes a
camouflage successful, by proposing three scores for automatically assessing
its effectiveness. In particular, we show that camouflage can be measured by
the similarity between background and foreground features and boundary
visibility. We use these camouflage scores to assess and compare all available
camouflage datasets. We also incorporate the proposed camouflage score into a
generative model as an auxiliary loss and show that effective camouflage images
or videos can be synthesised in a scalable manner. The generated synthetic
dataset is used to train a transformer-based model for segmenting camouflaged
animals in videos. Experimentally, we demonstrate state-of-the-art camouflage
breaking performance on the public MoCA-Mask benchmark.",None,-1
Few-shots Portrait Generation with Style Enhancement and Identity Preservation,0.0302141,"Nowadays, the wide application of virtual digital human promotes the
comprehensive prosperity and development of digital culture supported by
digital economy. The personalized portrait automatically generated by AI
technology needs both the natural artistic style and human sentiment. In this
paper, we propose a novel StyleIdentityGAN model, which can ensure the identity
and artistry of the generated portrait at the same time. Specifically, the
style-enhanced module focuses on artistic style features decoupling and
transferring to improve the artistry of generated virtual face images.
Meanwhile, the identity-enhanced module preserves the significant features
extracted from the input photo. Furthermore, the proposed method requires a
small number of reference style data. Experiments demonstrate the superiority
of StyleIdentityGAN over state-of-art methods in artistry and identity effects,
with comparisons done qualitatively, quantitatively and through a perceptual
user study. Code has been released on Github3.",None,-1
Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark,0.994129,"Recent studies have demonstrated promising potential of ChatGPT for various
text annotation and classification tasks. However, ChatGPT is non-deterministic
which means that, as with human coders, identical input can lead to different
outputs. Given this, it seems appropriate to test the reliability of ChatGPT.
Therefore, this study investigates the consistency of ChatGPT's zero-shot
capabilities for text annotation and classification, focusing on different
model parameters, prompt variations, and repetitions of identical inputs. Based
on the real-world classification task of differentiating website texts into
news and not news, results show that consistency in ChatGPT's classification
output can fall short of scientific thresholds for reliability. For example,
even minor wording alterations in prompts or repeating the identical input can
lead to varying outputs. Although pooling outputs from multiple repetitions can
improve reliability, this study advises caution when using ChatGPT for
zero-shot text annotation and underscores the need for thorough validation,
such as comparison against human-annotated data. The unsupervised application
of ChatGPT for text annotation and classification is not recommended.",None,-1
"Towards Conceptualization of ""Fair Explanation"": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators",0.172302,"Recent research at the intersection of AI explainability and fairness has
focused on how explanations can improve human-plus-AI task performance as
assessed by fairness measures. We propose to characterize what constitutes an
explanation that is itself ""fair"" -- an explanation that does not adversely
impact specific populations. We formulate a novel evaluation method of ""fair
explanations"" using not just accuracy and label time, but also psychological
impact of explanations on different user groups across many metrics (mental
discomfort, stereotype activation, and perceived workload). We apply this
method in the context of content moderation of potential hate speech, and its
differential impact on Asian vs. non-Asian proxy moderators, across explanation
approaches (saliency map and counterfactual explanation). We find that saliency
maps generally perform better and show less evidence of disparate impact
(group) and individual unfairness than counterfactual explanations.
  Content warning: This paper contains examples of hate speech and racially
discriminatory language. The authors do not support such content. Please
consider your risk of discomfort carefully before continuing reading!",None,-1
Fast Region of Interest Proposals on Maritime UAVs,0.124823,"Unmanned aerial vehicles assist in maritime search and rescue missions by
flying over large search areas to autonomously search for objects or people.
Reliably detecting objects of interest requires fast models to employ on
embedded hardware. Moreover, with increasing distance to the ground station
only part of the video data can be transmitted. In this work, we consider the
problem of finding meaningful region of interest proposals in a video stream on
an embedded GPU. Current object or anomaly detectors are not suitable due to
their slow speed, especially on limited hardware and for large image
resolutions. Lastly, objects of interest, such as pieces of wreckage, are often
not known a priori. Therefore, we propose an end-to-end future frame prediction
model running in real-time on embedded GPUs to generate region proposals. We
analyze its performance on large-scale maritime data sets and demonstrate its
benefits over traditional and modern methods.",None,-1
Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems,0.361499,"This paper aims to serve as a comprehensive guide for researchers and
practitioners, offering insights into the current state, potential
applications, and future research directions for generative artificial
intelligence and foundation models within the context of intelligent vehicles.
As the automotive industry progressively integrates AI, generative artificial
intelligence technologies hold the potential to revolutionize user
interactions, delivering more immersive, intuitive, and personalised in-car
experiences. We provide an overview of current applications of generative
artificial intelligence in the automotive domain, emphasizing speech, audio,
vision, and multimodal interactions. We subsequently outline critical future
research areas, including domain adaptability, alignment, multimodal
integration and others, as well as, address the challenges and risks associated
with ethics. By fostering collaboration and addressing these research areas,
generative artificial intelligence can unlock its full potential, transforming
the driving experience and shaping the future of intelligent vehicles.",None,-1
Non-autoregressive Machine Translation with Probabilistic Context-free Grammar,0.420422,"Non-autoregressive Transformer(NAT) significantly accelerates the inference
of neural machine translation. However, conventional NAT models suffer from
limited expression power and performance degradation compared to autoregressive
(AT) models due to the assumption of conditional independence among target
tokens. To address these limitations, we propose a novel approach called
PCFG-NAT, which leverages a specially designed Probabilistic Context-Free
Grammar (PCFG) to enhance the ability of NAT models to capture complex
dependencies among output tokens. Experimental results on major machine
translation benchmarks demonstrate that PCFG-NAT further narrows the gap in
translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a
deeper understanding of the generated sentences, addressing the lack of
satisfactory explainability in neural machine translation.Code is publicly
available at https://github.com/ictnlp/PCFG-NAT.",None,-1
Identity-Preserving Aging of Face Images via Latent Diffusion Models,0.331116,"The performance of automated face recognition systems is inevitably impacted
by the facial aging process. However, high quality datasets of individuals
collected over several years are typically small in scale. In this work, we
propose, train, and validate the use of latent text-to-image diffusion models
for synthetically aging and de-aging face images. Our models succeed with
few-shot training, and have the added benefit of being controllable via
intuitive textual prompting. We observe high degrees of visual realism in the
generated images while maintaining biometric fidelity measured by commonly used
metrics. We evaluate our method on two benchmark datasets (CelebA and AgeDB)
and observe significant reduction (~44%) in the False Non-Match Rate compared
to existing state-of the-art baselines.",None,-1
Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints,0.636655,"Local feature extraction is a standard approach in computer vision for
tackling important tasks such as image matching and retrieval. The core
assumption of most methods is that images undergo affine transformations,
disregarding more complicated effects such as non-rigid deformations.
Furthermore, incipient works tailored for non-rigid correspondence still rely
on keypoint detectors designed for rigid transformations, hindering performance
due to the limitations of the detector. We propose DALF (Deformation-Aware
Local Features), a novel deformation-aware network for jointly detecting and
describing keypoints, to handle the challenging problem of matching deformable
surfaces. All network components work cooperatively through a feature fusion
approach that enforces the descriptors' distinctiveness and invariance.
Experiments using real deforming objects showcase the superiority of our
method, where it delivers 8% improvement in matching scores compared to the
previous best results. Our approach also enhances the performance of two
real-world applications: deformable object retrieval and non-rigid 3D surface
registration. Code for training, inference, and applications are publicly
available at https://verlab.dcc.ufmg.br/descriptors/dalf_cvpr23.",None,-1
Computer Vision Estimation of Emotion Reaction Intensity in the Wild,0.518931,"Emotions play an essential role in human communication. Developing computer
vision models for automatic recognition of emotion expression can aid in a
variety of domains, including robotics, digital behavioral healthcare, and
media analytics. There are three types of emotional representations which are
traditionally modeled in affective computing research: Action Units, Valence
Arousal (VA), and Categorical Emotions. As part of an effort to move beyond
these representations towards more fine-grained labels, we describe our
submission to the newly introduced Emotional Reaction Intensity (ERI)
Estimation challenge in the 5th competition for Affective Behavior Analysis
in-the-Wild (ABAW). We developed four deep neural networks trained in the
visual domain and a multimodal model trained with both visual and audio
features to predict emotion reaction intensity. Our best performing model on
the Hume-Reaction dataset achieved an average Pearson correlation coefficient
of 0.4080 on the test set using a pre-trained ResNet50 model. This work
provides a first step towards the development of production-grade models which
predict emotion reaction intensities rather than discrete emotion categories.",None,-1
TACO: Topics in Algorithmic COde generation dataset,0.291125,"We introduce TACO, an open-source, large-scale code generation dataset, with
a focus on the optics of algorithms, designed to provide a more challenging
training dataset and evaluation benchmark in the field of code generation
models. TACO includes competition-level programming questions that are more
challenging, to enhance or evaluate problem understanding and reasoning
abilities in real-world programming scenarios. There are 25433 and 1000 coding
problems in training and test set, as well as up to 1.55 million diverse
solution answers. Moreover, each TACO problem includes several fine-grained
labels such as task topics, algorithms, programming skills, and difficulty
levels, providing a more precise reference for the training and evaluation of
code generation models. The dataset and evaluation scripts are available on
Hugging Face Hub (https://huggingface.co/datasets/BAAI/TACO) and Github
(https://github.com/FlagOpen/TACO).",None,-1
Traveling Words: A Geometric Interpretation of Transformers,0.122851,"Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.",None,-1
arXiv4TGC: Large-Scale Datasets for Temporal Graph Clustering,0.330936,"Temporal graph clustering (TGC) is a crucial task in temporal graph learning.
Its focus is on node clustering on temporal graphs, and it offers greater
flexibility for large-scale graph structures due to the mechanism of temporal
graph methods. However, the development of TGC is currently constrained by a
significant problem: the lack of suitable and reliable large-scale temporal
graph datasets to evaluate clustering performance. In other words, most
existing temporal graph datasets are in small sizes, and even large-scale
datasets contain only a limited number of available node labels. It makes
evaluating models for large-scale temporal graph clustering challenging. To
address this challenge, we build arXiv4TGC, a set of novel academic datasets
(including arXivAI, arXivCS, arXivMath, arXivPhy, and arXivLarge) for
large-scale temporal graph clustering. In particular, the largest dataset,
arXivLarge, contains 1.3 million labeled available nodes and 10 million
temporal edges. We further compare the clustering performance with typical
temporal graph learning models on both previous classic temporal graph datasets
and the new datasets proposed in this paper. The clustering performance on
arXiv4TGC can be more apparent for evaluating different models, resulting in
higher clustering confidence and more suitable for large-scale temporal graph
clustering. The arXiv4TGC datasets are publicly available at:
https://github.com/MGitHubL/arXiv4TGC.",None,-1
Efficient Parametric Approximations of Neural Network Function Space Distance,0.314159,"It is often useful to compactly summarize important properties of model
parameters and training data so that they can be used later without storing
and/or iterating over the entire dataset. As a specific case, we consider
estimating the Function Space Distance (FSD) over a training set, i.e. the
average discrepancy between the outputs of two neural networks. We propose a
Linearized Activation Function TRick (LAFTR) and derive an efficient
approximation to FSD for ReLU neural networks. The key idea is to approximate
the architecture as a linear network with stochastic gating. Despite requiring
only one parameter per unit of the network, our approach outcompetes other
parametric approximations with larger memory requirements. Applied to continual
learning, our parametric approximation is competitive with state-of-the-art
nonparametric approximations, which require storing many training examples.
Furthermore, we show its efficacy in estimating influence functions accurately
and detecting mislabeled examples without expensive iterations over the entire
dataset.",None,-1
Interaction-Aware Decision-Making for Autonomous Vehicles in Forced Merging Scenario Leveraging Social Psychology Factors,0.275522,"Understanding the intention of vehicles in the surrounding traffic is crucial
for an autonomous vehicle to successfully accomplish its driving tasks in
complex traffic scenarios such as highway forced merging. In this paper, we
consider a behavioral model that incorporates both social behaviors and
personal objectives of the interacting drivers. Leveraging this model, we
develop a receding-horizon control-based decision-making strategy, that
estimates online the other drivers' intentions using Bayesian filtering and
incorporates predictions of nearby vehicles' behaviors under uncertain
intentions. The effectiveness of the proposed decision-making strategy is
demonstrated and evaluated based on simulation studies in comparison with a
game theoretic controller and a real-world traffic dataset.",None,-1
AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities,0.703748,"Accident detection and traffic analysis is a critical component of smart city
and autonomous transportation systems that can reduce accident frequency,
severity and improve overall traffic management. This paper presents a
comprehensive analysis of traffic accidents in different regions across the
United States using data from the National Highway Traffic Safety
Administration (NHTSA) Crash Report Sampling System (CRSS). To address the
challenges of accident detection and traffic analysis, this paper proposes a
framework that uses traffic surveillance cameras and action recognition systems
to detect and respond to traffic accidents spontaneously. Integrating the
proposed framework with emergency services will harness the power of traffic
cameras and machine learning algorithms to create an efficient solution for
responding to traffic accidents and reducing human errors. Advanced
intelligence technologies, such as the proposed accident detection systems in
smart cities, will improve traffic management and traffic accident severity.
Overall, this study provides valuable insights into traffic accidents in the US
and presents a practical solution to enhance the safety and efficiency of
transportation systems.",None,-1
Large Language Models Can Be Easily Distracted by Irrelevant Context,0.998703,"Large language models have achieved impressive performance on various natural
language processing tasks. However, so far they have been evaluated primarily
on benchmarks where all information in the input context is relevant for
solving the task. In this work, we investigate the distractibility of large
language models, i.e., how the model problem-solving accuracy can be influenced
by irrelevant context. In particular, we introduce Grade-School Math with
Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant
information in the problem description. We use this benchmark to measure the
distractibility of cutting-edge prompting techniques for large language models,
and find that the model performance is dramatically decreased when irrelevant
information is included. We also identify several approaches for mitigating
this deficiency, such as decoding with self-consistency and adding to the
prompt an instruction that tells the language model to ignore the irrelevant
information.",None,-1
An adaptive large neighborhood search heuristic for the multi-port continuous berth allocation problem,0.0772219,"In this paper, we study a problem that integrates the vessel scheduling
problem with the berth allocation into a collaborative problem denoted as the
multi-port continuous berth allocation problem (MCBAP). This problem optimizes
the berth allocation of a set of ships simultaneously in multiple ports while
also considering the sailing speed of ships between ports. Due to the highly
combinatorial character of the problem, exact methods struggle to scale to
large-size instances, which points to exploring heuristic methods. We present a
mixed-integer problem formulation for the MCBAP and introduce an adaptive large
neighborhood search (ALNS) algorithm enhanced with a local search procedure to
solve it. The computational results highlight the method's suitability for
larger instances by providing high-quality solutions in short computational
times. Practical insights indicate that the carriers' and terminal operators'
operational costs are impacted in different ways by fuel prices, external ships
at port, and the modeling of a continuous quay.",None,-1
Parallelizing Optical Flow Estimation on an Ultra-Low Power RISC-V Cluster for Nano-UAV Navigation,0.150126,"Optical flow estimation is crucial for autonomous navigation and localization
of unmanned aerial vehicles (UAV). On micro and nano UAVs, real-time
calculation of the optical flow is run on low power and resource-constrained
microcontroller units (MCUs). Thus, lightweight algorithms for optical flow
have been proposed targeting real-time execution on traditional single-core
MCUs. This paper introduces an efficient parallelization strategy for optical
flow computation targeting new-generation multicore low power RISC-V based
microcontroller units. Our approach enables higher frame rates at lower clock
speeds. It has been implemented and evaluated on the eight-core cluster of a
commercial octa-core MCU (GAP8) reaching a parallelization speedup factor of
7.21 allowing for a frame rate of 500 frames per second when running on a 50
MHz clock frequency. The proposed parallel algorithm significantly boosts the
camera frame rate on micro unmanned aerial vehicles, which enables higher
flight speeds: the maximum flight speed can be doubled, while using less than a
third of the clock frequency of previous single-core implementations.",None,-1
ARB: Advanced Reasoning Benchmark for Large Language Models,0.592191,"Large Language Models (LLMs) have demonstrated remarkable performance on
various quantitative reasoning and knowledge benchmarks. However, many of these
benchmarks are losing utility as LLMs get increasingly high scores, despite not
yet reaching expert performance in these domains. We introduce ARB, a novel
benchmark composed of advanced reasoning problems in multiple fields. ARB
presents a more challenging test than prior benchmarks, featuring problems in
mathematics, physics, biology, chemistry, and law. As a subset of ARB, we
introduce a challenging set of math and physics problems which require advanced
symbolic reasoning and domain knowledge. We evaluate recent models such as
GPT-4 and Claude on ARB and demonstrate that current models score well below
50% on more demanding tasks. In order to improve both automatic and assisted
evaluation capabilities, we introduce a rubric-based evaluation approach,
allowing GPT-4 to score its own intermediate reasoning steps. Further, we
conduct a human evaluation of the symbolic subset of ARB, finding promising
agreement between annotators and GPT-4 rubric evaluation scores.",None,-1
GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond,0.120102,"This study proposes the GOOSE algorithm as a novel metaheuristic algorithm
based on the goose's behavior during rest and foraging. The goose stands on one
leg and keeps his balance to guard and protect other individuals in the flock.
The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions,
and the results are verified by a comparative study with genetic algorithm
(GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitness
dependent optimizer (FDO). In addition, the proposed algorithm is tested on 10
modern benchmark functions, and the gained results are compared with three
recent algorithms, such as the dragonfly algorithm, whale optimization
algorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithm
is tested on 5 classical benchmark functions, and the obtained results are
evaluated with six algorithms, such as fitness dependent optimizer (FDO), FOX
optimizer, butterfly optimization algorithm (BOA), whale optimization
algorithm, dragonfly algorithm, and chimp optimization algorithm (ChOA). The
achieved findings attest to the proposed algorithm's superior performance
compared to the other algorithms that were utilized in the current study. The
technique is then used to optimize Welded beam design and Economic Load
Dispatch Problem, three renowned real-world engineering challenges, and the
Pathological IgG Fraction in the Nervous System. The outcomes of the
engineering case studies illustrate how well the suggested approach can
optimize issues that arise in the real-world.",None,-1
Point2Vec for Self-Supervised Representation Learning on Point Clouds,0.554849,"Recently, the self-supervised learning framework data2vec has shown inspiring
performance for various modalities using a masked student-teacher approach.
However, it remains open whether such a framework generalizes to the unique
challenges of 3D point clouds. To answer this question, we extend data2vec to
the point cloud domain and report encouraging results on several downstream
tasks. In an in-depth analysis, we discover that the leakage of positional
information reveals the overall object shape to the student even under heavy
masking and thus hampers data2vec to learn strong representations for point
clouds. We address this 3D-specific shortcoming by proposing point2vec, which
unleashes the full potential of data2vec-like pre-training on point clouds. Our
experiments show that point2vec outperforms other self-supervised methods on
shape classification and few-shot learning on ModelNet40 and ScanObjectNN,
while achieving competitive results on part segmentation on ShapeNetParts.
These results suggest that the learned representations are strong and
transferable, highlighting point2vec as a promising direction for
self-supervised learning of point cloud representations.",None,-1
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models,0.992221,"Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating textual inputs into numerical representations,
capturing the semantics of the text. These models excel in applications like
dense retrieval and semantic textual similarity. This paper details the
development of Jina Embeddings, starting with the creation of high-quality
pairwise and triplet datasets. It underlines the crucial role of data cleaning
in dataset preparation, offers in-depth insights into the model training
process, and concludes with a comprehensive performance evaluation using the
Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model's
awareness of grammatical negation, we construct a novel training and evaluation
dataset of negated and non-negated statements, which we make publicly available
to the community.",None,-1
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,0.0767834,"We present a novel methodology that combines graph and dense segmentation
techniques by jointly learning both point and pixel contour representations,
thereby leveraging the benefits of each approach. This addresses deficiencies
in typical graph segmentation methods where misaligned objectives restrict the
network from learning discriminative vertex and contour features. Our joint
learning strategy allows for rich and diverse semantic features to be encoded,
while alleviating common contour stability issues in dense-based approaches,
where pixel-level objectives can lead to anatomically implausible topologies.
In addition, we identify scenarios where correct predictions that fall on the
contour boundary are penalised and address this with a novel hybrid contour
distance loss. Our approach is validated on several Chest X-ray datasets,
demonstrating clear improvements in segmentation stability and accuracy against
a variety of dense- and point-based methods. Our source code is freely
available at: www.github.com/kitbransby/Joint_Graph_Segmentation",None,-1
Robust Detection Outcome: A Metric for Pathology Detection in Medical Images,0.190247,"Detection of pathologies is a fundamental task in medical imaging and the
evaluation of algorithms that can perform this task automatically is crucial.
However, current object detection metrics for natural images do not reflect the
specific clinical requirements in pathology detection sufficiently. To tackle
this problem, we propose Robust Detection Outcome (RoDeO); a novel metric for
evaluating algorithms for pathology detection in medical images, especially in
chest X-rays. RoDeO evaluates different errors directly and individually, and
reflects clinical needs better than current metrics. Extensive evaluation on
the ChestX-ray8 dataset shows the superiority of our metrics compared to
existing ones. We released the code at https://github.com/FeliMe/RoDeO and
published RoDeO as pip package (rodeometric).",None,-1
EgoBlur: Responsible Innovation in Aria,0.0691899,"Project Aria pushes the frontiers of Egocentric AI with large-scale
real-world data collection using purposely designed glasses with privacy first
approach. To protect the privacy of bystanders being recorded by the glasses,
our research protocols are designed to ensure recorded video is processed by an
AI anonymization model that removes bystander faces and vehicle license plates.
Detected face and license plate regions are processed with a Gaussian blur such
that these personal identification information (PII) regions are obscured. This
process helps to ensure that anonymized versions of the video is retained for
research purposes. In Project Aria, we have developed a state-of-the-art
anonymization system EgoBlur. In this paper, we present extensive analysis of
EgoBlur on challenging datasets comparing its performance with other
state-of-the-art systems from industry and academia including extensive
Responsible AI analysis on recently released Casual Conversations V2 dataset.",None,-1
MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition,0.521802,"We present a novel approach for action recognition in UAV videos. Our
formulation is designed to handle occlusion and viewpoint changes caused by the
movement of a UAV. We use the concept of mutual information to compute and
align the regions corresponding to human action or motion in the temporal
domain. This enables our recognition model to learn from the key features
associated with the motion. We also propose a novel frame sampling method that
uses joint mutual information to acquire the most informative frame sequence in
UAV videos. We have integrated our approach with X3D and evaluated the
performance on multiple datasets. In practice, we achieve 18.9% improvement in
Top-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al.,
2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16%
improvement on NEC Drones(Choi et al., 2020).",None,-1
ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding,0.837592,"We introduce ZeroSCROLLS, a zero-shot benchmark for natural language
understanding over long texts, which contains only test and small validation
sets, without training data. We adapt six tasks from the SCROLLS benchmark, and
add four new datasets, including two novel information fusing tasks, such as
aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a
comprehensive evaluation of both open-source and closed large language models,
finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest
average score. However, there is still room for improvement on multiple open
challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to
pass the naive baseline. As the state of the art is a moving target, we invite
researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",None,-1
Class-Specific Distribution Alignment for Semi-Supervised Medical Image Classification,0.30621,"Despite the success of deep neural networks in medical image classification,
the problem remains challenging as data annotation is time-consuming, and the
class distribution is imbalanced due to the relative scarcity of diseases. To
address this problem, we propose Class-Specific Distribution Alignment (CSDA),
a semi-supervised learning framework based on self-training that is suitable to
learn from highly imbalanced datasets. Specifically, we first provide a new
perspective to distribution alignment by considering the process as a change of
basis in the vector space spanned by marginal predictions, and then derive CSDA
to capture class-dependent marginal predictions on both labeled and unlabeled
data, in order to avoid the bias towards majority classes. Furthermore, we
propose a Variable Condition Queue (VCQ) module to maintain a proportionately
balanced number of unlabeled samples for each class. Experiments on three
public datasets HAM10000, CheXpert and Kvasir show that our method provides
competitive performance on semi-supervised skin disease, thoracic disease, and
endoscopic image classification tasks.",None,-1
Human Pose Estimation in Extremely Low-Light Conditions,0.670756,"We study human pose estimation in extremely low-light images. This task is
challenging due to the difficulty of collecting real low-light images with
accurate labels, and severely corrupted inputs that degrade prediction quality
significantly. To address the first issue, we develop a dedicated camera system
and build a new dataset of real low-light images with accurate pose labels.
Thanks to our camera system, each low-light image in our dataset is coupled
with an aligned well-lit image, which enables accurate pose labeling and is
used as privileged information during training. We also propose a new model and
a new training strategy that fully exploit the privileged information to learn
representation insensitive to lighting conditions. Our method demonstrates
outstanding performance on real extremely low light images, and extensive
analyses validate that both of our model and dataset contribute to the success.",None,-1
Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge,0.487077,"In this paper, we study the problem of knowledge-intensive text-to-SQL, in
which domain knowledge is necessary to parse expert questions into SQL queries
over domain-specific tables. We formalize this scenario by building a new
Chinese benchmark KnowSQL consisting of domain-specific questions covering
various domains. We then address this problem by presenting formulaic
knowledge, rather than by annotating additional data examples. More concretely,
we construct a formulaic knowledge bank as a domain knowledge base and propose
a framework (ReGrouP) to leverage this formulaic knowledge during parsing.
Experiments using ReGrouP demonstrate a significant 28.2% improvement overall
on KnowSQL.",None,-1
Specification of MiniDemographicABM.jl: A simplified agent-based demographic model of the UK,0.115318,"This documentation specifies a simplified non-calibrated demographic
agent-based model of the UK, a largely simplified version of the Lone Parent
Model presented in [Gostolil and Silverman 2020]. In the presented model,
individuals of an initial population are subject to ageing, deaths, births,
divorces and marriages throughout a simplified map of towns of the UK. The
specification employs the formal terminology presented in [Elsheikh 2023a]. The
main purpose of the model is to explore and exploit capabilities of the
state-of-the-art Agents.jl Julia package [Datseris2022] in the context of
demographic modeling applications. Implementation is provided via the Julia
package MiniDemographicABM.jl [Elsheikh 2023b]. A specific simulation is
progressed with a user-defined simulation fixed step size on a hourly, daily,
weekly, monthly basis or even an arbitrary user-defined clock rate. The model
can serve for comparative studies if implemented in other agent-based modelling
frameworks and programming languages. Moreover, the model serves as a base
implementation to be adjusted to realistic large-scale socio-economics,
pandemics or immigration studies mainly within a demographic context.",None,-1
Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents,0.988383,"Human-like chatbots necessitate the use of commonsense reasoning in order to
effectively comprehend and respond to implicit information present within
conversations. Achieving such coherence and informativeness in responses,
however, is a non-trivial task. Even for large language models (LLMs), the task
of identifying and aggregating key evidence within a single hop presents a
substantial challenge. This complexity arises because such evidence is
scattered across multiple turns in a conversation, thus necessitating
integration over multiple hops. Hence, our focus is to facilitate such
multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought
(CoT) reasoning. To this end, we propose a knowledge distillation framework
that leverages LLMs as unreliable teachers and selectively distills consistent
and helpful rationales via alignment filters. We further present DOCTOR, a
DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for
response generation. We conduct extensive experiments to show that enhancing
dialogue agents with high-quality rationales from DOCTOR significantly improves
the quality of their responses.",None,-1
Deceptive Reinforcement Learning in Model-Free Domains,0.0852194,"This paper investigates deceptive reinforcement learning for privacy
preservation in model-free and continuous action space domains. In
reinforcement learning, the reward function defines the agent's objective. In
adversarial scenarios, an agent may need to both maximise rewards and keep its
reward function private from observers. Recent research presented the ambiguity
model (AM), which selects actions that are ambiguous over a set of possible
reward functions, via pre-trained $Q$-functions. Despite promising results in
model-based domains, our investigation shows that AM is ineffective in
model-free domains due to misdirected state space exploration. It is also
inefficient to train and inapplicable in continuous action space domains. We
propose the deceptive exploration ambiguity model (DEAM), which learns using
the deceptive policy during training, leading to targeted exploration of the
state space. DEAM is also applicable in continuous action spaces. We evaluate
DEAM in discrete and continuous action space path planning environments. DEAM
achieves similar performance to an optimal model-based version of AM and
outperforms a model-free version of AM in terms of path cost, deceptiveness and
training efficiency. These results extend to the continuous domain.",None,-1
CARLA-BSP: a simulated dataset with pedestrians,0.33011,"We present a sample dataset featuring pedestrians generated using the ARCANE
framework, a new framework for generating datasets in CARLA (0.9.13). We
provide use cases for pedestrian detection, autoencoding, pose estimation, and
pose lifting. We also showcase baseline results. For more information, visit
https://project-arcane.eu/.",None,-1
On the Possibilities of AI-Generated Text Detection,0.985796,"Our work addresses the critical issue of distinguishing text generated by
Large Language Models (LLMs) from human-produced text, a task essential for
numerous applications. Despite ongoing debate about the feasibility of such
differentiation, we present evidence supporting its consistent achievability,
except when human and machine text distributions are indistinguishable across
their entire support. Drawing from information theory, we argue that as
machine-generated text approximates human-like quality, the sample size needed
for detection increases. We establish precise sample complexity bounds for
detecting AI-generated text, laying groundwork for future research aimed at
developing advanced, multi-sample detectors. Our empirical evaluations across
multiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the
viability of enhanced detection methods. We test various state-of-the-art text
generators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and
Llama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector,
GPTZero. Our findings align with OpenAI's empirical data related to sequence
length, marking the first theoretical substantiation for these observations.",None,-1
Towards General Text Embeddings with Multi-stage Contrastive Learning,0.996546,"We present GTE, a general-purpose text embedding model trained with
multi-stage contrastive learning. In line with recent advancements in unifying
various NLP tasks into a single format, we train a unified text embedding model
by employing contrastive learning over a diverse mixture of datasets from
multiple sources. By significantly increasing the number of training data
during both unsupervised pre-training and supervised fine-tuning stages, we
achieve substantial performance gains over existing embedding models. Notably,
even with a relatively modest parameter count of 110M, GTE$_\text{base}$
outperforms the black-box embedding API provided by OpenAI and even surpasses
10x larger text embedding models on the massive text embedding benchmark.
Furthermore, without additional fine-tuning on each programming language
individually, our model outperforms previous best code retrievers of similar
size by treating code as text. In summary, our model achieves impressive
results by effectively harnessing multi-stage contrastive learning, offering a
powerful and efficient text embedding model with broad applicability across
various NLP and code-related tasks.",None,-1
Classification of Primitive Manufacturing Tasks from Filtered Event Data,0.573349,"Collaborative robots are increasingly present in industry to support human
activities. However, to make the human-robot collaborative process more
effective, there are several challenges to be addressed. Collaborative robotic
systems need to be aware of the human activities to (1) anticipate
collaborative/assistive actions, (2) learn by demonstration, and (3) activate
safety procedures in shared workspace. This study proposes an action
classification system to recognize primitive assembly tasks from human motion
events data captured by a Dynamic and Active-pixel Vision Sensor (DAVIS).
Several filters are compared and combined to remove event data noise. Task
patterns are classified from a continuous stream of event data using advanced
deep learning and recurrent networks to classify spatial and temporal features.
Experiments were conducted on a novel dataset, the dataset of manufacturing
tasks (DMT22), featuring 5 classes of representative manufacturing primitives
(PickUp, Place, Screw, Hold, Idle) from 5 participants. Results show that the
proposed filters remove about 65\% of all events (noise) per recording,
conducting to a classification accuracy up to 99,37\% for subjects that trained
the system and 97.08\% for new subjects. Data from a left-handed subject were
successfully classified using only right-handed training data. These results
are object independent.",None,-1
Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks,0.882324,"Arguments often do not make explicit how a conclusion follows from its
premises. To compensate for this lack, we enrich arguments with structured
background knowledge to support knowledge-intense argumentation tasks. We
present a new unsupervised method for constructing Contextualized Commonsense
Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from
large knowledge graphs (KGs) efficiently and at high quality. Our work goes
beyond context-insensitive knowledge extraction heuristics by computing
semantic similarity between KG triplets and textual arguments. Using these
triplet similarities as weights, we extract contextualized knowledge paths that
connect a conclusion to its premise, while maximizing similarity to the
argument. We combine multiple paths into a CCKG that we optionally prune to
reduce noise and raise precision. Intrinsic evaluation of the quality of our
graphs shows that our method is effective for (re)constructing human
explanation graphs. Manual evaluations in a large-scale knowledge selection
setup confirm high recall and precision of implicit CSK in the CCKGs. Finally,
we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument
quality rating task, outperforming strong baselines and rivaling a GPT-3 based
system.",None,-1
AI-assisted coding: Experiments with GPT-4,0.715421,"Artificial intelligence (AI) tools based on large language models have
acheived human-level performance on some computer programming tasks. We report
several experiments using GPT-4 to generate computer code. These experiments
demonstrate that AI code generation using the current generation of tools,
while powerful, requires substantial human validation to ensure accurate
performance. We also demonstrate that GPT-4 refactoring of existing code can
significantly improve that code along several established metrics for code
quality, and we show that GPT-4 can generate tests with substantial coverage,
but that many of the tests fail when applied to the associated code. These
findings suggest that while AI coding tools are very powerful, they still
require humans in the loop to ensure validity and accuracy of the results.",None,-1
GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice,0.257787,"We assess the ability of GPT -- a large language model -- to serve as a
financial robo-advisor for the masses, by using a financial literacy test.
Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial
literacy test, respectively, compared to a baseline of 33%. However, ChatGPT
based on GPT-4 achieves a near-perfect 99% score, pointing to financial
literacy becoming an emergent ability of state-of-the-art models. We use the
Judge-Advisor System and a savings dilemma to illustrate how researchers might
assess advice-utilization from large language models. We also present a number
of directions for future research.",None,-1
Evaluation of ChatGPT for NLP-based Mental Health Applications,0.999113,"Large language models (LLM) have been successful in several natural language
understanding tasks and could be relevant for natural language processing
(NLP)-based mental health application research. In this work, we report the
performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three
text-based mental health classification tasks: stress detection (2-class
classification), depression detection (2-class classification), and suicidality
detection (5-class classification). We obtained annotated social media posts
for the three classification tasks from public datasets. Then ChatGPT API
classified the social media posts with an input prompt for classification. We
obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression
detection, and suicidality detection, respectively. A baseline model that
always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and
0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a
potential use of language models for mental health classification tasks.",None,-1
VicTR: Video-conditioned Text Representations for Activity Recognition,0.569299,"Vision-Language models (VLMs) have excelled in the image-domain -- especially
in zero-shot settings -- thanks to the availability of vast pretraining data
(i.e., paired image-text samples). However for videos, such paired data is not
as abundant. Therefore, video-VLMs are usually designed by adapting pretrained
image-VLMs to the video-domain, instead of training from scratch. All such
recipes rely on augmenting visual embeddings with temporal information (i.e.,
image $\rightarrow$ video), often keeping text embeddings unchanged or even
being discarded. In this paper, we argue the contrary, that better video-VLMs
can be designed by focusing more on augmenting text, rather than visual
information. More specifically, we introduce Video-conditioned Text
Representations (VicTR): a form of text embeddings optimized w.r.t. visual
embeddings, creating a more-flexible contrastive latent space. Our model can
further make use of freely-available semantic information, in the form of
visually-grounded auxiliary text (e.g. object or scene information). We
evaluate our model on few-shot, zero-shot (HMDB-51, UCF-101), short-form
(Kinetics-400) and long-form (Charades) activity recognition benchmarks,
showing strong performance among video-VLMs.",None,-1
ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning,0.352635,"Noisy partial label learning (noisy PLL) is an important branch of weakly
supervised learning. Unlike PLL where the ground-truth label must conceal in
the candidate label set, noisy PLL relaxes this constraint and allows the
ground-truth label may not be in the candidate label set. To address this
challenging problem, most of the existing works attempt to detect noisy samples
and estimate the ground-truth label for each noisy sample. However, detection
errors are unavoidable. These errors can accumulate during training and
continuously affect model optimization. To this end, we propose a novel
framework for noisy PLL with theoretical guarantees, called ``Adjusting Label
Importance Mechanism (ALIM)''. It aims to reduce the negative impact of
detection errors by trading off the initial candidate set and model outputs.
ALIM is a plug-in strategy that can be integrated with existing PLL approaches.
Experimental results on benchmark datasets demonstrate that our method can
achieve state-of-the-art performance on noisy PLL.
\textcolor[rgb]{0.93,0.0,0.47}{Our code can be found in Supplementary
Material}.",None,-1
ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics,0.844938,"We introduce ProofNet, a benchmark for autoformalization and formal proving
of undergraduate-level mathematics. The ProofNet benchmarks consists of 371
examples, each consisting of a formal theorem statement in Lean 3, a natural
language theorem statement, and a natural language proof. The problems are
primarily drawn from popular undergraduate pure mathematics textbooks and cover
topics such as real and complex analysis, linear algebra, abstract algebra, and
topology. We intend for ProofNet to be a challenging benchmark that will drive
progress in autoformalization and automatic theorem proving. We report baseline
results on statement autoformalization via in-context learning. Moreover, we
introduce two novel statement autoformalization methods: prompt retrieval and
distilled backtranslation.",None,-1
Roll-Drop: accounting for observation noise with a single parameter,0.138803,"This paper proposes a simple strategy for sim-to-real in Deep-Reinforcement
Learning (DRL) -- called Roll-Drop -- that uses dropout during simulation to
account for observation noise during deployment without explicitly modelling
its distribution for each state. DRL is a promising approach to control robots
for highly dynamic and feedback-based manoeuvres, and accurate simulators are
crucial to providing cheap and abundant data to learn the desired behaviour.
Nevertheless, the simulated data are noiseless and generally show a
distributional shift that challenges the deployment on real machines where
sensor readings are affected by noise. The standard solution is modelling the
latter and injecting it during training; while this requires a thorough system
identification, Roll-Drop enhances the robustness to sensor noise by tuning
only a single parameter. We demonstrate an 80% success rate when up to 25%
noise is injected in the observations, with twice higher robustness than the
baselines. We deploy the controller trained in simulation on a Unitree A1
platform and assess this improved robustness on the physical system.",None,-1
ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations,0.374525,"Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data
for model training. Empirical studies show that SSL can achieve promising
performance in distribution shift scenarios, where the downstream and training
distributions differ. However, the theoretical understanding of its
transferability remains limited. In this paper, we develop a theoretical
framework to analyze the transferability of self-supervised contrastive
learning, by investigating the impact of data augmentation on it. Our results
reveal that the downstream performance of contrastive learning depends largely
on the choice of data augmentation. Moreover, we show that contrastive learning
fails to learn domain-invariant features, which limits its transferability.
Based on these theoretical insights, we propose a novel method called
Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn
domain-invariant features and can be easily integrated with existing
contrastive learning algorithms. We conduct experiments on several datasets and
show that ArCL significantly improves the transferability of contrastive
learning.",None,-1
TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement,0.686279,"Speech enhancement models have greatly progressed in recent years, but still
show limits in perceptual quality of their speech outputs. We propose an
objective for perceptual quality based on temporal acoustic parameters. These
are fundamental speech features that play an essential role in various
applications, including speaker recognition and paralinguistic analysis. We
provide a differentiable estimator for four categories of low-level acoustic
descriptors involving: frequency-related parameters, energy or
amplitude-related parameters, spectral balance parameters, and temporal
features. Unlike prior work that looks at aggregated acoustic parameters or a
few categories of acoustic parameters, our temporal acoustic parameter (TAP)
loss enables auxiliary optimization and improvement of many fine-grain speech
characteristics in enhancement workflows. We show that adding TAPLoss as an
auxiliary objective in speech enhancement produces speech with improved
perceptual quality and intelligibility. We use data from the Deep Noise
Suppression 2020 Challenge to demonstrate that both time-domain models and
time-frequency domain models can benefit from our method.",None,-1
SceneCalib: Automatic Targetless Calibration of Cameras and Lidars in Autonomous Driving,0.439047,"Accurate camera-to-lidar calibration is a requirement for sensor data fusion
in many 3D perception tasks. In this paper, we present SceneCalib, a novel
method for simultaneous self-calibration of extrinsic and intrinsic parameters
in a system containing multiple cameras and a lidar sensor. Existing methods
typically require specially designed calibration targets and human operators,
or they only attempt to solve for a subset of calibration parameters. We
resolve these issues with a fully automatic method that requires no explicit
correspondences between camera images and lidar point clouds, allowing for
robustness to many outdoor environments. Furthermore, the full system is
jointly calibrated with explicit cross-camera constraints to ensure that
camera-to-camera and camera-to-lidar extrinsic parameters are consistent.",None,-1
Mixture-of-Expert Conformer for Streaming Multilingual ASR,0.663956,"End-to-end models with large capacity have significantly improved
multilingual automatic speech recognition, but their computation cost poses
challenges for on-device applications. We propose a streaming truly
multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn
to only activate a subset of parameters in training and inference. The MoE
layer consists of a softmax gate which chooses the best two experts among many
in forward propagation. The proposed MoE layer offers efficient inference by
activating a fixed number of parameters as the number of experts increases. We
evaluate the proposed model on a set of 12 languages, and achieve an average
11.9% relative improvement in WER over the baseline. Compared to an adapter
model using ground truth information, our MoE model achieves similar WER and
activates similar number of parameters but without any language information. We
further show around 3% relative WER improvement by multilingual shallow fusion.",None,-1
Context-Aware Change Detection With Semi-Supervised Learning,0.0996934,"Change detection using earth observation data plays a vital role in
quantifying the impact of disasters in affected areas. While data sources like
Sentinel-2 provide rich optical information, they are often hindered by cloud
cover, limiting their usage in disaster scenarios. However, leveraging
pre-disaster optical data can offer valuable contextual information about the
area such as landcover type, vegetation cover, soil types, enabling a better
understanding of the disaster's impact. In this study, we develop a model to
assess the contribution of pre-disaster Sentinel-2 data in change detection
tasks, focusing on disaster-affected areas. The proposed Context-Aware Change
Detection Network (CACDN) utilizes a combination of pre-disaster Sentinel-2
data, pre and post-disaster Sentinel-1 data and ancillary Digital Elevation
Models (DEM) data. The model is validated on flood and landslide detection and
evaluated using three metrics: Area Under the Precision-Recall Curve (AUPRC),
Intersection over Union (IoU), and mean IoU. The preliminary results show
significant improvement (4\%, AUPRC, 3-7\% IoU, 3-6\% mean IoU) in model's
change detection capabilities when incorporated with pre-disaster optical data
reflecting the effectiveness of using contextual information for accurate flood
and landslide detection.",None,-1
GAM : Gradient Attention Module of Optimization for Point Clouds Analysis,0.646464,"In point cloud analysis tasks, the existing local feature aggregation
descriptors (LFAD) are unable to fully utilize information in the neighborhood
of central points. Previous methods rely solely on Euclidean distance to
constrain the local aggregation process, which can be easily affected by
abnormal points and cannot adequately fit with the original geometry of the
point cloud. We believe that fine-grained geometric information (FGGI) is
significant for the aggregation of local features. Therefore, we propose a
gradient-based local attention module, termed as Gradient Attention Module
(GAM), to address the aforementioned problem. Our proposed GAM simplifies the
process that extracts gradient information in the neighborhood and uses the
Zenith Angle matrix and Azimuth Angle matrix as explicit representation, which
accelerates the module by 35X. Comprehensive experiments were conducted on five
benchmark datasets to demonstrate the effectiveness and generalization
capability of the proposed GAM for 3D point cloud analysis. Especially on S3DIS
dataset, GAM achieves the best performance among current point-based models
with mIoU/OA/mAcc of 74.4%/90.6%/83.2%, respectively.",None,-1
LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa,0.880884,"Named Entity Recognition(NER) is a task of recognizing entities at a token
level in a sentence. This paper focuses on solving NER tasks in a multilingual
setting for complex named entities. Our team, LLM-RM participated in the
recently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual
Complex Named Entity Recognition. We approach the problem by leveraging
cross-lingual representation provided by fine-tuning XLM-Roberta base model on
datasets of all of the 12 languages provided -- Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and
Ukrainian",None,-1
Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation,0.682554,"The CLIP model has been recently proven to be very effective for a variety of
cross-modal tasks, including the evaluation of captions generated from
vision-and-language architectures. In this paper, we propose a new recipe for a
contrastive-based evaluation metric for image captioning, namely
Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way
unifies the learning of a contrastive visual-semantic space with the addition
of generated images and text on curated data. Experiments spanning several
datasets demonstrate that our new metric achieves the highest correlation with
human judgments on both images and videos, outperforming existing
reference-based metrics like CIDEr and SPICE and reference-free metrics like
CLIP-Score. Finally, we test the system-level correlation of the proposed
metric when considering popular image captioning approaches, and assess the
impact of employing different cross-modal features. Our source code and trained
models are publicly available at: https://github.com/aimagelab/pacscore.",None,-1
Understanding and Mitigating Spurious Correlations in Text Classification with Neighborhood Analysis,0.0595305,"Recent research has revealed that machine learning models have a tendency to
leverage spurious correlations that exist in the training set but may not hold
true in general circumstances. For instance, a sentiment classifier may
erroneously learn that the token ""performances"" is commonly associated with
positive movie reviews. Relying on these spurious correlations degrades the
classifiers performance when it deploys on out-of-distribution data. In this
paper, we examine the implications of spurious correlations through a novel
perspective called neighborhood analysis. The analysis uncovers how spurious
correlations lead unrelated words to erroneously cluster together in the
embedding space. Driven by the analysis, we design a metric to detect spurious
tokens and also propose a family of regularization methods, NFL (doN't Forget
your Language) to mitigate spurious correlations in text classification.
Experiments show that NFL can effectively prevent erroneous clusters and
significantly improve the robustness of classifiers without auxiliary data. The
code is publicly available at
https://github.com/oscarchew/doNt-Forget-your-Language.",None,-1
Multi-View Azimuth Stereo via Tangent Space Consistency,0.640484,"We present a method for 3D reconstruction only using calibrated multi-view
surface azimuth maps. Our method, multi-view azimuth stereo, is effective for
textureless or specular surfaces, which are difficult for conventional
multi-view stereo methods. We introduce the concept of tangent space
consistency: Multi-view azimuth observations of a surface point should be
lifted to the same tangent space. Leveraging this consistency, we recover the
shape by optimizing a neural implicit surface representation. Our method
harnesses the robust azimuth estimation capabilities of photometric stereo
methods or polarization imaging while bypassing potentially complex zenith
angle estimation. Experiments using azimuth maps from various sources validate
the accurate shape recovery with our method, even without zenith angles.",None,-1
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning,0.0989542,"Federated learning (FL) is a machine learning paradigm, which enables
multiple and decentralized clients to collaboratively train a model under the
orchestration of a central aggregator. FL can be a scalable machine learning
solution in big data scenarios. Traditional FL relies on the trust assumption
of the central aggregator, which forms cohorts of clients honestly. However, a
malicious aggregator, in reality, could abandon and replace the client's
training models, or insert fake clients, to manipulate the final training
results. In this work, we introduce zkFL, which leverages zero-knowledge proofs
to tackle the issue of a malicious aggregator during the training model
aggregation process. To guarantee the correct aggregation results, the
aggregator provides a proof per round, demonstrating to the clients that the
aggregator executes the intended behavior faithfully. To further reduce the
verification cost of clients, we use blockchain to handle the proof in a
zero-knowledge way, where miners (i.e., the participants validating and
maintaining the blockchain data) can verify the proof without knowing the
clients' local and aggregated models. The theoretical analysis and empirical
results show that zkFL achieves better security and privacy than traditional
FL, without modifying the underlying FL network structure or heavily
compromising the training speed.",None,-1
Orca 2: Teaching Small Language Models How to Reason,0.846302,"Orca 1 learns from rich signals, such as explanation traces, allowing it to
outperform conventional instruction-tuned models on benchmarks like BigBench
Hard and AGIEval. In Orca 2, we continue exploring how improved training
signals can enhance smaller LMs' reasoning abilities. Research on training
small LMs has often relied on imitation learning to replicate the output of
more capable models. We contend that excessive emphasis on imitation may
restrict the potential of smaller models. We seek to teach small LMs to employ
different solution strategies for different tasks, potentially different from
the one used by the larger model. For example, while larger models might
provide a direct answer to a complex task, smaller models may not have the same
capacity. In Orca 2, we teach the model various reasoning techniques
(step-by-step, recall then generate, recall-reason-generate, direct answer,
etc.). More crucially, we aim to help the model learn to determine the most
effective solution strategy for each task. We evaluate Orca 2 using a
comprehensive set of 15 diverse benchmarks (corresponding to approximately 100
tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of
similar size and attains performance levels similar or better to those of
models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings. make Orca 2 weights publicly available at
aka.ms/orca-lm to support research on the development, evaluation, and
alignment of smaller LMs",None,-1
Inseq: An Interpretability Toolkit for Sequence Generation Models,0.927388,"Past work in natural language processing interpretability focused mainly on
popular classification tasks while largely overlooking generation settings,
partly due to a lack of dedicated tools. In this work, we introduce Inseq, a
Python library to democratize access to interpretability analyses of sequence
generation models. Inseq enables intuitive and optimized extraction of models'
internal information and feature importance scores for popular decoder-only and
encoder-decoder Transformers architectures. We showcase its potential by
adopting it to highlight gender biases in machine translation models and locate
factual knowledge inside GPT-2. Thanks to its extensible interface supporting
cutting-edge techniques such as contrastive feature attribution, Inseq can
drive future advances in explainable natural language generation, centralizing
good practices and enabling fair and reproducible model evaluations.",None,-1
DPPD: Deformable Polar Polygon Object Detection,0.039385,"Regular object detection methods output rectangle bounding boxes, which are
unable to accurately describe the actual object shapes. Instance segmentation
methods output pixel-level labels, which are computationally expensive for
real-time applications. Therefore, a polygon representation is needed to
achieve precise shape alignment, while retaining low computation cost. We
develop a novel Deformable Polar Polygon Object Detection method (DPPD) to
detect objects in polygon shapes. In particular, our network predicts, for each
object, a sparse set of flexible vertices to construct the polygon, where each
vertex is represented by a pair of angle and distance in the Polar coordinate
system. To enable training, both ground truth and predicted polygons are
densely resampled to have the same number of vertices with equal-spaced
raypoints. The resampling operation is fully differentable, allowing gradient
back-propagation. Sparse polygon predicton ensures high-speed runtime inference
while dense resampling allows the network to learn object shapes with high
precision. The polygon detection head is established on top of an anchor-free
and NMS-free network architecture. DPPD has been demonstrated successfully in
various object detection tasks for autonomous driving such as traffic-sign,
crosswalk, vehicle and pedestrian objects.",None,-1
deep learning of segment-level feature representation for speech emotion recognition in conversations,0.341275,"Accurately detecting emotions in conversation is a necessary yet challenging
task due to the complexity of emotions and dynamics in dialogues. The emotional
state of a speaker can be influenced by many different factors, such as
interlocutor stimulus, dialogue scene, and topic. In this work, we propose a
conversational speech emotion recognition method to deal with capturing
attentive contextual dependency and speaker-sensitive interactions. First, we
use a pretrained VGGish model to extract segment-based audio representation in
individual utterances. Second, an attentive bi-directional gated recurrent unit
(GRU) models contextual-sensitive information and explores intra- and
inter-speaker dependencies jointly in a dynamic manner. The experiments
conducted on the standard conversational dataset MELD demonstrate the
effectiveness of the proposed method when compared against state-of the-art
methods.",None,-1
An Asymmetric Loss with Anomaly Detection LSTM Framework for Power Consumption Prediction,0.138386,"Building an accurate load forecasting model with minimal underpredictions is
vital to prevent any undesired power outages due to underproduction of
electricity. However, the power consumption patterns of the residential sector
contain fluctuations and anomalies making them challenging to predict. In this
paper, we propose multiple Long Short-Term Memory (LSTM) frameworks with
different asymmetric loss functions to impose a higher penalty on
underpredictions. We also apply a density-based spatial clustering of
applications with noise (DBSCAN) anomaly detection approach, prior to the load
forecasting task, to remove any present oultiers. Considering the effect of
weather and social factors, seasonality splitting is performed on the three
considered datasets from France, Germany, and Hungary containing hourly power
consumption, weather, and calendar features. Root-mean-square error (RMSE)
results show that removing the anomalies efficiently reduces the
underestimation and overestimation errors in all the seasonal datasets.
Additionally, asymmetric loss functions and seasonality splitting effectively
minimize underestimations despite increasing the overestimation error to some
degree. Reducing underpredictions of electricity consumption is essential to
prevent power outages that can be damaging to the community.",None,-1
High-Fidelity Zero-Shot Texture Anomaly Localization Using Feature Correspondence Analysis,0.432781,"We propose a novel method for Zero-Shot Anomaly Localization on textures. The
task refers to identifying abnormal regions in an otherwise homogeneous image.
To obtain a high-fidelity localization, we leverage a bijective mapping derived
from the 1-dimensional Wasserstein Distance. As opposed to using holistic
distances between distributions, the proposed approach allows pinpointing the
non-conformity of a pixel in a local context with increased precision. By
aggregating the contribution of the pixel to the errors of all nearby patches
we obtain a reliable anomaly score estimate. We validate our solution on
several datasets and obtain more than a 40% reduction in error over the
previous state of the art on the MVTec AD dataset in a zero-shot setting. Also
see https://reality.tf.fau.de/pub/ardelean2024highfidelity.html.",None,-1
Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions,0.997889,"As systems based on opaque Artificial Intelligence (AI) continue to flourish
in diverse real-world applications, understanding these black box models has
become paramount. In response, Explainable AI (XAI) has emerged as a field of
research with practical and ethical benefits across various domains. This paper
not only highlights the advancements in XAI and its application in real-world
scenarios but also addresses the ongoing challenges within XAI, emphasizing the
need for broader perspectives and collaborative efforts. We bring together
experts from diverse fields to identify open problems, striving to synchronize
research agendas and accelerate XAI in practical applications. By fostering
collaborative discussion and interdisciplinary cooperation, we aim to propel
XAI forward, contributing to its continued success. Our goal is to put forward
a comprehensive proposal for advancing XAI. To achieve this goal, we present a
manifesto of 27 open problems categorized into nine categories. These
challenges encapsulate the complexities and nuances of XAI and offer a road map
for future research. For each problem, we provide promising research directions
in the hope of harnessing the collective intelligence of interested
stakeholders.",None,-1
An Empirical Evaluation of Federated Contextual Bandit Algorithms,0.470348,"As the adoption of federated learning increases for learning from sensitive
data local to user devices, it is natural to ask if the learning can be done
using implicit signals generated as users interact with the applications of
interest, rather than requiring access to explicit labels which can be
difficult to acquire in many tasks. We approach such problems with the
framework of federated contextual bandits, and develop variants of prominent
contextual bandit algorithms from the centralized seting for the federated
setting. We carefully evaluate these algorithms in a range of scenarios
simulated using publicly available datasets. Our simulations model typical
setups encountered in the real-world, such as various misalignments between an
initial pre-trained model and the subsequent user interactions due to
non-stationarity in the data and/or heterogeneity across clients. Our
experiments reveal the surprising effectiveness of the simple and commonly used
softmax heuristic in balancing the well-know exploration-exploitation tradeoff
across the breadth of our settings.",None,-1
Generative Plug and Play: Posterior Sampling for Inverse Problems,0.747906,"Over the past decade, Plug-and-Play (PnP) has become a popular method for
reconstructing images using a modular framework consisting of a forward and
prior model. The great strength of PnP is that an image denoiser can be used as
a prior model while the forward model can be implemented using more traditional
physics-based approaches. However, a limitation of PnP is that it reconstructs
only a single deterministic image.
  In this paper, we introduce Generative Plug-and-Play (GPnP), a generalization
of PnP to sample from the posterior distribution. As with PnP, GPnP has a
modular framework using a physics-based forward model and an image denoising
prior model. However, in GPnP these models are extended to become proximal
generators, which sample from associated distributions. GPnP applies these
proximal generators in alternation to produce samples from the posterior. We
present experimental simulations using the well-known BM3D denoiser. Our
results demonstrate that the GPnP method is robust, easy to implement, and
produces intuitively reasonable samples from the posterior for sparse
interpolation and tomographic reconstruction. Code to accompany this paper is
available at https://github.com/gbuzzard/generative-pnp-allerton .",None,-1
Fake News Detectors are Biased against Texts Generated by Large Language Models,0.878794,"The spread of fake news has emerged as a critical challenge, undermining
trust and posing threats to society. In the era of Large Language Models
(LLMs), the capability to generate believable fake content has intensified
these concerns. In this study, we present a novel paradigm to evaluate fake
news detectors in scenarios involving both human-written and LLM-generated
misinformation. Intriguingly, our findings reveal a significant bias in many
existing detectors: they are more prone to flagging LLM-generated content as
fake news while often misclassifying human-written fake news as genuine. This
unexpected bias appears to arise from distinct linguistic patterns inherent to
LLM outputs. To address this, we introduce a mitigation strategy that leverages
adversarial training with LLM-paraphrased genuine news. The resulting model
yielded marked improvements in detection accuracy for both human and
LLM-generated news. To further catalyze research in this domain, we release two
comprehensive datasets, \texttt{GossipCop++} and \texttt{PolitiFact++}, thus
amalgamating human-validated articles with LLM-generated fake and real news.",None,-1
Face0: Instantaneously Conditioning a Text-to-Image Model on a Face,0.60605,"We present Face0, a novel way to instantaneously condition a text-to-image
generation model on a face, in sample time, without any optimization procedures
such as fine-tuning or inversions. We augment a dataset of annotated images
with embeddings of the included faces and train an image generation model, on
the augmented dataset. Once trained, our system is practically identical at
inference time to the underlying base model, and is therefore able to generate
images, given a user-supplied face image and a prompt, in just a couple of
seconds. Our method achieves pleasing results, is remarkably simple, extremely
fast, and equips the underlying model with new capabilities, like controlling
the generated images both via text or via direct manipulation of the input face
embeddings. In addition, when using a fixed random vector instead of a face
embedding from a user supplied image, our method essentially solves the problem
of consistent character generation across images. Finally, while requiring
further research, we hope that our method, which decouples the model's textual
biases from its biases on faces, might be a step towards some mitigation of
biases in future text-to-image models.",None,-1
DynamicDet: A Unified Dynamic Architecture for Object Detection,0.406882,"Dynamic neural network is an emerging research topic in deep learning. With
adaptive inference, dynamic models can achieve remarkable accuracy and
computational efficiency. However, it is challenging to design a powerful
dynamic detector, because of no suitable dynamic architecture and exiting
criterion for object detection. To tackle these difficulties, we propose a
dynamic framework for object detection, named DynamicDet. Firstly, we carefully
design a dynamic architecture based on the nature of the object detection task.
Then, we propose an adaptive router to analyze the multi-scale information and
to decide the inference route automatically. We also present a novel
optimization strategy with an exiting criterion based on the detection losses
for our dynamic detectors. Last, we present a variable-speed inference
strategy, which helps to realize a wide range of accuracy-speed trade-offs with
only one dynamic detector. Extensive experiments conducted on the COCO
benchmark demonstrate that the proposed DynamicDet achieves new
state-of-the-art accuracy-speed trade-offs. For instance, with comparable
accuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpasses
YOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code is
available at https://github.com/VDIGPKU/DynamicDet.",None,-1
Automatically Summarizing Evidence from Clinical Trials: A Prototype Highlighting Current Challenges,0.716113,"We present TrialsSummarizer, a system that aims to automatically summarize
evidence presented in the set of randomized controlled trials most relevant to
a given query. Building on prior work, the system retrieves trial publications
matching a query specifying a combination of condition, intervention(s), and
outcome(s), and ranks these according to sample size and estimated study
quality. The top-k such studies are passed through a neural multi-document
summarization system, yielding a synopsis of these trials. We consider two
architectures: A standard sequence-to-sequence model based on BART and a
multi-headed architecture intended to provide greater transparency to
end-users. Both models produce fluent and relevant summaries of evidence
retrieved for queries, but their tendency to introduce unsupported statements
render them inappropriate for use in this domain at present. The proposed
architecture may help users verify outputs allowing users to trace generated
tokens back to inputs.",None,-1
Modular Visual Question Answering via Code Generation,0.743643,"We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.",None,-1
IDA: Informed Domain Adaptive Semantic Segmentation,0.449823,"Mixup-based data augmentation has been validated to be a critical stage in
the self-training framework for unsupervised domain adaptive semantic
segmentation (UDA-SS), which aims to transfer knowledge from a well-annotated
(source) domain to an unlabeled (target) domain. Existing self-training methods
usually adopt the popular region-based mixup techniques with a random sampling
strategy, which unfortunately ignores the dynamic evolution of different
semantics across various domains as training proceeds. To improve the UDA-SS
performance, we propose an Informed Domain Adaptation (IDA) model, a
self-training framework that mixes the data based on class-level segmentation
performance, which aims to emphasize small-region semantics during mixup. In
our IDA model, the class-level performance is tracked by an expected confidence
score (ECS). We then use a dynamic schedule to determine the mixing ratio for
data in different domains. Extensive experimental results reveal that our
proposed method is able to outperform the state-of-the-art UDA-SS method by a
margin of 1.1 mIoU in the adaptation of GTA-V to Cityscapes and of 0.9 mIoU in
the adaptation of SYNTHIA to Cityscapes.",None,-1
Patched Line Segment Learning for Vector Road Mapping,0.454504,"This paper presents a novel approach to computing vector road maps from
satellite remotely sensed images, building upon a well-defined Patched Line
Segment (PaLiS) representation for road graphs that holds geometric
significance. Unlike prevailing methods that derive road vector representations
from satellite images using binary masks or keypoints, our method employs line
segments. These segments not only convey road locations but also capture their
orientations, making them a robust choice for representation. More precisely,
given an input image, we divide it into non-overlapping patches and predict a
suitable line segment within each patch. This strategy enables us to capture
spatial and structural cues from these patch-based line segments, simplifying
the process of constructing the road network graph without the necessity of
additional neural networks for connectivity. In our experiments, we demonstrate
how an effective representation of a road graph significantly enhances the
performance of vector road mapping on established benchmarks, without requiring
extensive modifications to the neural network architecture. Furthermore, our
method achieves state-of-the-art performance with just 6 GPU hours of training,
leading to a substantial 32-fold reduction in training costs in terms of GPU
hours.",None,-1
CC3D: Layout-Conditioned Generation of Compositional 3D Scenes,0.543213,"In this work, we introduce CC3D, a conditional generative model that
synthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained
using single-view images. Different from most existing 3D GANs that limit their
applicability to aligned single objects, we focus on generating complex scenes
with multiple objects, by modeling the compositional nature of 3D scenes. By
devising a 2D layout-based approach for 3D synthesis and implementing a new 3D
field representation with a stronger geometric inductive bias, we have created
a 3D GAN that is both efficient and of high quality, while allowing for a more
controllable generation process. Our evaluations on synthetic 3D-FRONT and
real-world KITTI-360 datasets demonstrate that our model generates scenes of
improved visual and geometric quality in comparison to previous works.",None,-1
Likelihood-Based Generative Radiance Field with Latent Space Energy-Based Model for 3D-Aware Disentangled Image Representation,0.216564,"We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.",None,-1
HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird's Eye View,0.514041,"Vision-based Bird's Eye View (BEV) representation is an emerging perception
formulation for autonomous driving. The core challenge is to construct BEV
space with multi-camera features, which is a one-to-many ill-posed problem.
Diving into all previous BEV representation generation methods, we found that
most of them fall into two types: modeling depths in image views or modeling
heights in the BEV space, mostly in an implicit way. In this work, we propose
to explicitly model heights in the BEV space, which needs no extra data like
LiDAR and can fit arbitrary camera rigs and types compared to modeling depths.
Theoretically, we give proof of the equivalence between height-based methods
and depth-based methods. Considering the equivalence and some advantages of
modeling heights, we propose HeightFormer, which models heights and
uncertainties in a self-recursive way. Without any extra data, the proposed
HeightFormer could estimate heights in BEV accurately. Benchmark results show
that the performance of HeightFormer achieves SOTA compared with those
camera-only methods.",None,-1
Manipulating Embeddings of Stable Diffusion Prompts,0.037178,"Prompt engineering is still the primary way for users of generative
text-to-image models to manipulate generated images in a targeted way. Based on
treating the model as a continuous function and by passing gradients between
the image space and the prompt embedding space, we propose and analyze a new
method to directly manipulate the embedding of a prompt instead of the prompt
text. We then derive three practical interaction tools to support users with
image generation: (1) Optimization of a metric defined in the image space that
measures, for example, the image style. (2) Supporting a user in creative tasks
by allowing them to navigate in the image space along a selection of directions
of ""near"" prompt embeddings. (3) Changing the embedding of the prompt to
include information that a user has seen in a particular seed but has
difficulty describing in the prompt. Compared to prompt engineering,
user-driven prompt embedding manipulation enables a more fine-grained, targeted
control that integrates a user's intentions. Our user study shows that our
methods are considered less tedious and that the resulting images are often
preferred.",None,-1
The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers,0.698124,"Interpretable part-prototype models are computer vision models that are
explainable by design. The models learn prototypical parts and recognise these
components in an image, thereby combining classification and explanation.
Despite the recent attention for intrinsically interpretable models, there is
no comprehensive overview on evaluating the explanation quality of
interpretable part-prototype models. Based on the Co-12 properties for
explanation quality as introduced in arXiv:2201.08164 (e.g., correctness,
completeness, compactness), we review existing work that evaluates
part-prototype models, reveal research gaps and outline future approaches for
evaluation of the explanation quality of part-prototype models. This paper,
therefore, contributes to the progression and maturity of this relatively new
research field on interpretable part-prototype models. We additionally provide
a ``Co-12 cheat sheet'' that acts as a concise summary of our findings on
evaluating part-prototype models.",None,-1
Simplified Continuous High Dimensional Belief Space Planning with Adaptive Probabilistic Belief-dependent Constraints,0.346499,"Online decision making under uncertainty in partially observable domains,
also known as Belief Space Planning, is a fundamental problem in robotics and
Artificial Intelligence. Due to an abundance of plausible future unravelings,
calculating an optimal course of action inflicts an enormous computational
burden on the agent. Moreover, in many scenarios, e.g., information gathering,
it is required to introduce a belief-dependent constraint. Prompted by this
demand, in this paper, we consider a recently introduced probabilistic
belief-dependent constrained POMDP. We present a technique to adaptively accept
or discard a candidate action sequence with respect to a probabilistic
belief-dependent constraint, before expanding a complete set of future
observations samples and without any loss in accuracy. Moreover, using our
proposed framework, we contribute an adaptive method to find a maximal feasible
return (e.g., information gain) in terms of Value at Risk for the candidate
action sequence with substantial acceleration. On top of that, we introduce an
adaptive simplification technique for a probabilistically constrained setting.
Such an approach provably returns an identical-quality solution while
dramatically accelerating online decision making. Our universal framework
applies to any belief-dependent constrained continuous POMDP with parametric
beliefs, as well as nonparametric beliefs represented by particles. In the
context of an information-theoretic constraint, our presented framework
stochastically quantifies if a cumulative information gain along the planning
horizon is sufficiently significant (e.g. for, information gathering, active
SLAM). We apply our method to active SLAM, a highly challenging problem of high
dimensional Belief Space Planning. Extensive realistic simulations corroborate
the superiority of our proposed ideas.",None,-1
Deep Image Harmonization in Dual Color Spaces,0.695278,"Image harmonization is an essential step in image composition that adjusts
the appearance of composite foreground to address the inconsistency between
foreground and background. Existing methods primarily operate in correlated
$RGB$ color space, leading to entangled features and limited representation
ability. In contrast, decorrelated color space (e.g., $Lab$) has decorrelated
channels that provide disentangled color and illumination statistics. In this
paper, we explore image harmonization in dual color spaces, which supplements
entangled $RGB$ features with disentangled $L$, $a$, $b$ features to alleviate
the workload in harmonization process. The network comprises a $RGB$
harmonization backbone, an $Lab$ encoding module, and an $Lab$ control module.
The backbone is a U-Net network translating composite image to harmonized
image. Three encoders in $Lab$ encoding module extract three control codes
independently from $L$, $a$, $b$ channels, which are used to manipulate the
decoder features in harmonization backbone via $Lab$ control module. Our code
and model are available at
\href{https://github.com/bcmi/DucoNet-Image-Harmonization}{https://github.com/bcmi/DucoNet-Image-Harmonization}.",None,-1
Shape-aware Text-driven Layered Video Editing,0.631658,"Temporal consistency is essential for video editing applications. Existing
work on layered representation of videos allows propagating edits consistently
to each frame. These methods, however, can only edit object appearance rather
than object shape changes due to the limitation of using a fixed UV mapping
field for texture atlas. We present a shape-aware, text-driven video editing
method to tackle this challenge. To handle shape changes in video editing, we
first propagate the deformation field between the input and edited keyframe to
all frames. We then leverage a pre-trained text-conditioned diffusion model as
guidance for refining shape distortion and completing unseen regions. The
experimental results demonstrate that our method can achieve shape-aware
consistent video editing and compare favorably with the state-of-the-art.",None,-1
Revisiting Deformable Convolution for Depth Completion,0.108424,"Depth completion, which aims to generate high-quality dense depth maps from
sparse depth maps, has attracted increasing attention in recent years. Previous
work usually employs RGB images as guidance, and introduces iterative spatial
propagation to refine estimated coarse depth maps. However, most of the
propagation refinement methods require several iterations and suffer from a
fixed receptive field, which may contain irrelevant and useless information
with very sparse input. In this paper, we address these two challenges
simultaneously by revisiting the idea of deformable convolution. We propose an
effective architecture that leverages deformable kernel convolution as a
single-pass refinement module, and empirically demonstrate its superiority. To
better understand the function of deformable convolution and exploit it for
depth completion, we further systematically investigate a variety of
representative strategies. Our study reveals that, different from prior work,
deformable convolution needs to be applied on an estimated depth map with a
relatively high density for better performance. We evaluate our model on the
large-scale KITTI dataset and achieve state-of-the-art level performance in
both accuracy and inference speed. Our code is available at
https://github.com/AlexSunNik/ReDC.",None,-1
Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations,0.941993,"Large pre-trained language models have exhibited unprecedented capabilities
in producing high-quality text via prompting techniques. This fact introduces
new possibilities for data collection and annotation, particularly in
situations where such data is scarce, complex to gather, expensive, or even
sensitive. In this paper, we explore the potential of these models to generate
and annotate goal-oriented dialogues, and conduct an in-depth analysis to
evaluate their quality. Our experiments employ ChatGPT, and encompass three
categories of goal-oriented dialogues (task-oriented, collaborative, and
explanatory), two generation modes (interactive and one-shot), and two
languages (English and Italian). Based on extensive human-based evaluations, we
demonstrate that the quality of generated dialogues and annotations is on par
with those generated by humans.",None,-1
SGLang: Efficient Execution of Structured Language Model Programs,0.880764,"Large language models (LLMs) are increasingly used for complex tasks that
require multiple generation calls, advanced prompting techniques, control flow,
and structured inputs/outputs. However, efficient systems are lacking for
programming and executing these applications. We introduce SGLang, a system for
efficient execution of complex language model programs. SGLang consists of a
frontend language and a runtime. The frontend simplifies programming with
primitives for generation and parallelism control. The runtime accelerates
execution with novel optimizations like RadixAttention for KV cache reuse and
compressed finite state machines for faster structured output decoding.
Experiments show that SGLang achieves up to 6.4x higher throughput compared to
state-of-the-art inference systems on various large language and multi-modal
models on tasks including agent control, logical reasoning, few-shot learning
benchmarks, JSON decoding, retrieval-augmented generation pipelines, and
multi-turn chat. The code is publicly available at
https://github.com/sgl-project/sglang",None,-1
DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion,0.770817,"Denosing diffusion model, as a generative model, has received a lot of
attention in the field of image generation recently, thanks to its powerful
generation capability. However, diffusion models have not yet received
sufficient research in the field of image fusion. In this article, we introduce
diffusion model to the image fusion field, treating the image fusion task as
image-to-image translation and designing two different conditional injection
modulation modules (i.e., style transfer modulation and wavelet modulation) to
inject coarse-grained style information and fine-grained high-frequency and
low-frequency information into the diffusion UNet, thereby generating fused
images. In addition, we also discussed the residual learning and the selection
of training objectives of the diffusion model in the image fusion task.
Extensive experimental results based on quantitative and qualitative
assessments compared with benchmarks demonstrates state-of-the-art results and
good generalization performance in image fusion tasks. Finally, it is hoped
that our method can inspire other works and gain insight into this field to
better apply the diffusion model to image fusion tasks. Code shall be released
for better reproducibility.",None,-1
Blockchain-based Federated Learning with Secure Aggregation in Trusted Execution Environment for Internet-of-Things,0.947796,"This paper proposes a blockchain-based Federated Learning (FL) framework with
Intel Software Guard Extension (SGX)-based Trusted Execution Environment (TEE)
to securely aggregate local models in Industrial Internet-of-Things (IIoTs). In
FL, local models can be tampered with by attackers. Hence, a global model
generated from the tampered local models can be erroneous. Therefore, the
proposed framework leverages a blockchain network for secure model aggregation.
Each blockchain node hosts an SGX-enabled processor that securely performs the
FL-based aggregation tasks to generate a global model. Blockchain nodes can
verify the authenticity of the aggregated model, run a blockchain consensus
mechanism to ensure the integrity of the model, and add it to the distributed
ledger for tamper-proof storage. Each cluster can obtain the aggregated model
from the blockchain and verify its integrity before using it. We conducted
several experiments with different CNN models and datasets to evaluate the
performance of the proposed framework.",None,-1
Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation,0.436762,"Using multiple spatial modalities has been proven helpful in improving
semantic segmentation performance. However, there are several real-world
challenges that have yet to be addressed: (a) improving label efficiency and
(b) enhancing robustness in realistic scenarios where modalities are missing at
the test time. To address these challenges, we first propose a simple yet
efficient multi-modal fusion mechanism Linear Fusion, that performs better than
the state-of-the-art multi-modal models even with limited supervision. Second,
we propose M3L: Multi-modal Teacher for Masked Modality Learning, a
semi-supervised framework that not only improves the multi-modal performance
but also makes the model robust to the realistic missing modality scenario
using unlabeled data. We create the first benchmark for semi-supervised
multi-modal semantic segmentation and also report the robustness to missing
modalities. Our proposal shows an absolute improvement of up to 10% on robust
mIoU above the most competitive baselines. Our code is available at
https://github.com/harshm121/M3L",None,-1
Compensation Learning in Semantic Segmentation,0.103225,"Label noise and ambiguities between similar classes are challenging problems
in developing new models and annotating new data for semantic segmentation. In
this paper, we propose Compensation Learning in Semantic Segmentation, a
framework to identify and compensate ambiguities as well as label noise. More
specifically, we add a ground truth depending and globally learned bias to the
classification logits and introduce a novel uncertainty branch for neural
networks to induce the compensation bias only to relevant regions. Our method
is employed into state-of-the-art segmentation frameworks and several
experiments demonstrate that our proposed compensation learns inter-class
relations that allow global identification of challenging ambiguities as well
as the exact localization of subsequent label noise. Additionally, it enlarges
robustness against label noise during training and allows target-oriented
manipulation during inference. We evaluate the proposed method on %the widely
used datasets Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.",None,-1
Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding,0.494303,"Negative sampling (NS) is widely used in knowledge graph embedding (KGE),
which aims to generate negative triples to make a positive-negative contrast
during training. However, existing NS methods are unsuitable when multi-modal
information is considered in KGE models. They are also inefficient due to their
complex design. In this paper, we propose Modality-Aware Negative Sampling
(MANS) for multi-modal knowledge graph embedding (MMKGE) to address the
mentioned problems. MANS could align structural and visual embeddings for
entities in KGs and learn meaningful embeddings to perform better in
multi-modal KGE while keeping lightweight and efficient. Empirical results on
two benchmarks demonstrate that MANS outperforms existing NS methods.
Meanwhile, we make further explorations about MANS to confirm its
effectiveness.",None,-1
"Multi3WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",0.912754,"Creating high-quality annotated data for task-oriented dialog (ToD) is known
to be notoriously difficult, and the challenges are amplified when the goal is
to create equitable, culturally adapted, and large-scale ToD datasets for
multiple languages. Therefore, the current datasets are still very scarce and
suffer from limitations such as translation-based non-native dialogs with
translation artefacts, small scale, or lack of cultural adaptation, among
others. In this work, we first take stock of the current landscape of
multilingual ToD datasets, offering a systematic overview of their properties
and limitations. Aiming to reduce all the detected limitations, we then
introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD
dataset. It is large-scale and offers culturally adapted dialogs in 4 languages
to enable training and evaluation of multilingual and cross-lingual ToD
systems. We describe a complex bottom-up data collection process that yielded
the final dataset, and offer the first sets of baseline scores across different
ToD-related tasks for future reference, also highlighting its challenging
nature.",None,-1
Age-Friendly Route Planner: Calculating Comfortable Routes for Senior Citizens,0.535536,"The application of routing algorithms to real-world situations is a widely
studied research topic. Despite this, routing algorithms and applications are
usually developed for a general purpose, meaning that certain groups, such as
ageing people, are often marginalized due to the broad approach of the designed
algorithms. This situation may pose a problem in cities which are suffering a
slow but progressive ageing of their populations. With this motivation in mind,
this paper focuses on describing our implemented Age-Friendly Route Planner,
whose goal is to improve the experience in the city for senior citizens. In
order to measure the age-friendliness of a route, several variables have been
deemed, such as the number of amenities along the route, the amount of
comfortable elements found, or the avoidance of sloppy sections. In this paper,
we describe one of the main features of the Age-Friendly Route Planner: the
preference-based routes, and we also demonstrate how it can contribute to the
creation of adapted friendly routes.",None,-1
Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models,0.233794,"Few-shot or zero-shot fact verification only relies on a few or no labeled
training examples. In this paper, we propose a novel method called ProToCo, to
\underline{Pro}mpt pre-trained language models (PLMs) \underline{To} be
\underline{Co}nsistent, for improving the factuality assessment capability of
PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair,
ProToCo generates multiple variants of the claim with different relations and
frames a simple consistency mechanism as constraints for making compatible
predictions across these variants. We update PLMs by using parameter-efficient
fine-tuning (PEFT), leading to more accurate predictions in few-shot and
zero-shot fact verification tasks. Our experiments on three public verification
datasets show that ProToCo significantly outperforms state-of-the-art few-shot
fact verification baselines. With a small number of unlabeled instances,
ProToCo also outperforms the strong zero-shot learner T0 on zero-shot
verification. Compared to large PLMs using in-context learning (ICL) method,
ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in
both few- and zero-shot settings.",None,-1
Implicit Neural Head Synthesis via Controllable Local Deformation Fields,0.524946,"High-quality reconstruction of controllable 3D head avatars from 2D videos is
highly desirable for virtual human applications in movies, games, and
telepresence. Neural implicit fields provide a powerful representation to model
3D head avatars with personalized shape, expressions, and facial parts, e.g.,
hair and mouth interior, that go beyond the linear 3D morphable model (3DMM).
However, existing methods do not model faces with fine-scale facial features,
or local control of facial parts that extrapolate asymmetric expressions from
monocular videos. Further, most condition only on 3DMM parameters with poor(er)
locality, and resolve local features with a global neural field. We build on
part-based implicit shape models that decompose a global deformation field into
local ones. Our novel formulation models multiple implicit deformation fields
with local semantic rig-like control via 3DMM-based parameters, and
representative facial landmarks. Further, we propose a local control loss and
attention mask mechanism that promote sparsity of each learned deformation
field. Our formulation renders sharper locally controllable nonlinear
deformations than previous implicit monocular approaches, especially mouth
interior, asymmetric expressions, and facial details.",None,-1
RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing,0.823442,"Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at https://github.com/thomas0809/RxnScribe.",None,-1
Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,0.789293,"Large-scale pre-trained language models have shown outstanding performance in
a variety of NLP tasks. However, they are also known to be significantly
brittle against specifically crafted adversarial examples, leading to
increasing interest in probing the adversarial robustness of NLP systems. We
introduce RSMI, a novel two-stage framework that combines randomized smoothing
(RS) with masked inference (MI) to improve the adversarial robustness of NLP
systems. RS transforms a classifier into a smoothed classifier to obtain robust
representations, whereas MI forces a model to exploit the surrounding context
of a masked token in an input sequence. RSMI improves adversarial robustness by
2 to 3 times over existing state-of-the-art methods on benchmark datasets. We
also perform in-depth qualitative analysis to validate the effectiveness of the
different stages of RSMI and probe the impact of its components through
extensive ablations. By empirically proving the stability of RSMI, we put it
forward as a practical method to robustly train large-scale NLP models. Our
code and datasets are available at https://github.com/Han8931/rsmi_nlp",None,-1
Parameterized Decision-making with Multi-modal Perception for Autonomous Driving,0.346105,"Autonomous driving is an emerging technology that has advanced rapidly over
the last decade. Modern transportation is expected to benefit greatly from a
wise decision-making framework of autonomous vehicles, including the
improvement of mobility and the minimization of risks and travel time. However,
existing methods either ignore the complexity of environments only fitting
straight roads, or ignore the impact on surrounding vehicles during
optimization phases, leading to weak environmental adaptability and incomplete
optimization objectives. To address these limitations, we propose a
parameterized decision-making framework with multi-modal perception based on
deep reinforcement learning, called AUTO. We conduct a comprehensive perception
to capture the state features of various traffic participants around the
autonomous vehicle, based on which we design a graph-based model to learn a
state representation of the multi-modal semantic features. To distinguish
between lane-following and lane-changing, we decompose an action of the
autonomous vehicle into a parameterized action structure that first decides
whether to change lanes and then computes an exact action to execute. A hybrid
reward function takes into account aspects of safety, traffic efficiency,
passenger comfort, and impact to guide the framework to generate optimal
actions. In addition, we design a regularization term and a multi-worker
paradigm to enhance the training. Extensive experiments offer evidence that
AUTO can advance state-of-the-art in terms of both macroscopic and microscopic
effectiveness.",None,-1
Japanese Lexical Complexity for Non-Native Readers: A New Dataset,0.694203,"Lexical complexity prediction (LCP) is the task of predicting the complexity
of words in a text on a continuous scale. It plays a vital role in simplifying
or annotating complex words to assist readers. To study lexical complexity in
Japanese, we construct the first Japanese LCP dataset. Our dataset provides
separate complexity scores for Chinese/Korean annotators and others to address
the readers' L1-specific needs. In the baseline experiment, we demonstrate the
effectiveness of a BERT-based system for Japanese LCP.",None,-1
TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training,0.321341,"Sparsely gated Mixture-of-Expert (MoE) has demonstrated its effectiveness in
scaling up deep neural networks to an extreme scale. Despite that numerous
efforts have been made to improve the performance of MoE from the model design
or system optimization perspective, existing MoE dispatch patterns are still
not able to fully exploit the underlying heterogeneous network environments. In
this paper, we propose TA-MoE, a topology-aware routing strategy for
large-scale MoE trainging, from a model-system co-design perspective, which can
dynamically adjust the MoE dispatch pattern according to the network topology.
Based on communication modeling, we abstract the dispatch problem into an
optimization objective and obtain the approximate dispatch pattern under
different topologies. On top of that, we design a topology-aware auxiliary
loss, which can adaptively route the data to fit in the underlying topology
without sacrificing the model accuracy. Experiments show that TA-MoE can
substantially outperform its counterparts on various hardware and model
configurations, with roughly 1.01x-1.61x, 1.01x-4.77x, 1.25x-1.54x improvements
over the popular DeepSpeed-MoE, FastMoE and FasterMoE.",None,-1
From Compass and Ruler to Convolution and Nonlinearity: On the Surprising Difficulty of Understanding a Simple CNN Solving a Simple Geometric Estimation Task,0.0419453,"Neural networks are omnipresent, but remain poorly understood. Their
increasing complexity and use in critical systems raises the important
challenge to full interpretability. We propose to address a simple well-posed
learning problem: estimating the radius of a centred pulse in a one-dimensional
signal or of a centred disk in two-dimensional images using a simple
convolutional neural network. Surprisingly, understanding what trained networks
have learned is difficult and, to some extent, counter-intuitive. However, an
in-depth theoretical analysis in the one-dimensional case allows us to
comprehend constraints due to the chosen architecture, the role of each filter
and of the nonlinear activation function, and every single value taken by the
weights of the model. Two fundamental concepts of neural networks arise: the
importance of invariance and of the shape of the nonlinear activation
functions.",None,-1
Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings,0.44943,"This study investigates the consistency of feedback ratings generated by
OpenAI's GPT-4, a state-of-the-art artificial intelligence language model,
across multiple iterations, time spans and stylistic variations. The model
rated responses to tasks within the Higher Education (HE) subject domain of
macroeconomics in terms of their content and style. Statistical analysis was
conducted in order to learn more about the interrater reliability, consistency
of the ratings across iterations and the correlation between ratings in terms
of content and style. The results revealed a high interrater reliability with
ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting
that GPT-4 is capable of generating consistent ratings across repetitions with
a clear prompt. Style and content ratings show a high correlation of 0.87. When
applying a non-adequate style the average content ratings remained constant,
while style ratings decreased, which indicates that the large language model
(LLM) effectively distinguishes between these two criteria during evaluation.
The prompt used in this study is furthermore presented and explained. Further
research is necessary to assess the robustness and reliability of AI models in
various use cases.",None,-1
Navigating to Objects Specified by Images,0.968765,"Images are a convenient way to specify which particular object instance an
embodied agent should navigate to. Solving this task requires semantic visual
reasoning and exploration of unknown environments. We present a system that can
perform this task in both simulation and the real world. Our modular method
solves sub-tasks of exploration, goal instance re-identification, goal
localization, and local navigation. We re-identify the goal instance in
egocentric vision using feature-matching and localize the goal instance by
projecting matched features to a map. Each sub-task is solved using
off-the-shelf components requiring zero fine-tuning. On the HM3D
InstanceImageNav benchmark, this system outperforms a baseline end-to-end RL
policy 7x and a state-of-the-art ImageNav model 2.3x (56% vs 25% success). We
deploy this system to a mobile robot platform and demonstrate effective
real-world performance, achieving an 88% success rate across a home and an
office environment.",None,-1
Jigsaw: Learning to Assemble Multiple Fractured Objects,0.869065,"Automated assembly of 3D fractures is essential in orthopedics, archaeology,
and our daily life. This paper presents Jigsaw, a novel framework for
assembling physically broken 3D objects from multiple pieces. Our approach
leverages hierarchical features of global and local geometry to match and align
the fracture surfaces. Our framework consists of four components: (1) front-end
point feature extractor with attention layers, (2) surface segmentation to
separate fracture and original parts, (3) multi-parts matching to find
correspondences among fracture surface points, and (4) robust global alignment
to recover the global poses of the pieces. We show how to jointly learn
segmentation and matching and seamlessly integrate feature matching and
rigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and
achieve superior performance compared to state-of-the-art methods. Our method
also generalizes well to diverse fracture modes, objects, and unseen instances.
To the best of our knowledge, this is the first learning-based method designed
specifically for 3D fracture assembly over multiple pieces. Our code is
available at https://jiaxin-lu.github.io/Jigsaw/.",None,-1
Geolocation Predicting of Tweets Using BERT-Based Models,0.0779462,"This research is aimed to solve the tweet/user geolocation prediction task
and provide a flexible methodology for the geotagging of textual big data. The
suggested approach implements neural networks for natural language processing
(NLP) to estimate the location as coordinate pairs (longitude, latitude) and
two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models
has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder
Representations from Transformers (BERT) as base models. Performance metrics
show a median error of fewer than 30 km on a worldwide-level, and fewer than 15
km on the US-level datasets for the models trained and evaluated on text
features of tweets' content and metadata context. Our source code and data are
available at https://github.com/K4TEL/geo-twitter.git",None,-1
Weakly Supervised 3D Instance Segmentation without Instance-level Annotations,0.208335,"3D semantic scene understanding tasks have achieved great success with the
emergence of deep learning, but often require a huge amount of manually
annotated training data. To alleviate the annotation cost, we propose the first
weakly-supervised 3D instance segmentation method that only requires
categorical semantic labels as supervision, and we do not need instance-level
labels. The required semantic annotations can be either dense or extreme sparse
(e.g. 0.02% of total points). Even without having any instance-related
ground-truth, we design an approach to break point clouds into raw fragments
and find the most confident samples for learning instance centroids.
Furthermore, we construct a recomposed dataset using pseudo instances, which is
used to learn our defined multilevel shape-aware objectness signal. An
asymmetrical object inference algorithm is followed to process core points and
boundary points with different strategies, and generate high-quality pseudo
instance labels to guide iterative training. Experiments demonstrate that our
method can achieve comparable results with recent fully supervised methods. By
generating pseudo instance labels from categorical semantic labels, our
designed approach can also assist existing methods for learning 3D instance
segmentation at reduced annotation cost.",None,-1
GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,0.693178,"We present a novel framework to regularize Neural Radiance Field (NeRF) in a
few-shot setting with a geometry-aware consistency regularization. The proposed
approach leverages a rendered depth map at unobserved viewpoint to warp sparse
input images to the unobserved viewpoint and impose them as pseudo ground
truths to facilitate learning of NeRF. By encouraging such geometry-aware
consistency at a feature-level instead of using pixel-level reconstruction
loss, we regularize the NeRF at semantic and structural levels while allowing
for modeling view dependent radiance to account for color variations across
viewpoints. We also propose an effective method to filter out erroneous warped
solutions, along with training strategies to stabilize training during
optimization. We show that our model achieves competitive results compared to
state-of-the-art few-shot NeRF models. Project page is available at
https://ku-cvlab.github.io/GeCoNeRF/.",None,-1
SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training with Adversarial Remarks,0.0375026,"Large Language Models (LLMs) can justify or critique their predictions
through discussions with other models or humans, thereby enriching their
intrinsic understanding of instances. While proactive discussions in the
inference phase have been shown to boost performance, such interactions have
not been extensively explored during the training phase. We hypothesize that
incorporating interactive discussions into the training process can enhance the
models' understanding and improve their reasoning and verbal expression
abilities during inference. This work introduces the SAIE framework, which
facilitates supportive and adversarial discussions between learner and partner
models. The learner model receives responses from the partner, and its
parameters are then updated based on this discussion. This dynamic adjustment
process continues throughout the training phase, responding to the evolving
outputs of the learner model. Our empirical evaluation across various tasks,
including math problems, commonsense reasoning, and multi-domain knowledge,
demonstrates that models fine-tuned with the SAIE framework outperform those
trained with conventional fine-tuning approaches. Furthermore, our method
enhances the models' reasoning capabilities, improving both individual and
multi-agent inference performance.",None,-1
Analysis over vision-based models for pedestrian action anticipation,0.527633,"Anticipating human actions in front of autonomous vehicles is a challenging
task. Several papers have recently proposed model architectures to address this
problem by combining multiple input features to predict pedestrian crossing
actions. This paper focuses specifically on using images of the pedestrian's
context as an input feature. We present several spatio-temporal model
architectures that utilize standard CNN and Transformer modules to serve as a
backbone for pedestrian anticipation. However, the objective of this paper is
not to surpass state-of-the-art benchmarks but rather to analyze the positive
and negative predictions of these models. Therefore, we provide insights on the
explainability of vision-based Transformer models in the context of pedestrian
action prediction. We will highlight cases where the model can achieve correct
quantitative results but falls short in providing human-like explanations
qualitatively, emphasizing the importance of investing in explainability for
pedestrian action anticipation problems.",None,-1
Approximate spectral clustering with eigenvector selection and self-tuned $k$,0.184638,"The recently emerged spectral clustering surpasses conventional clustering
methods by detecting clusters of any shape without the convexity assumption.
Unfortunately, with a computational complexity of $O(n^3)$, it was infeasible
for multiple real applications, where $n$ could be large. This stimulates
researchers to propose the approximate spectral clustering (ASC). However, most
of ASC methods assumed that the number of clusters $k$ was known. In practice,
manual setting of $k$ could be subjective or time consuming. The proposed
algorithm has two relevance metrics for estimating $k$ in two vital steps of
ASC. One for selecting the eigenvectors spanning the embedding space, and the
other to discover the number of clusters in that space. The algorithm used a
growing neural gas (GNG) approximation, GNG is superior in preserving input
data topology. The experimental setup demonstrates the efficiency of the
proposed algorithm and its ability to compete with similar methods where $k$
was set manually.",None,-1
Trajectory-User Linking via Hierarchical Spatio-Temporal Attention Networks,0.182059,"Trajectory-User Linking (TUL) is crucial for human mobility modeling by
linking diferent trajectories to users with the exploration of complex mobility
patterns. Existing works mainly rely on the recurrent neural framework to
encode the temporal dependencies in trajectories, have fall short in capturing
spatial-temporal global context for TUL prediction. To ill this gap, this work
presents a new hierarchical spatio-temporal attention neural network, called
AttnTUL, to jointly encode the local trajectory transitional patterns and
global spatial dependencies for TUL. Speciically, our irst model component is
built over the graph neural architecture to preserve the local and global
context and enhance the representation paradigm of geographical regions and
user trajectories. Additionally, a hierarchically structured attention network
is designed to simultaneously encode the intra-trajectory and inter-trajectory
dependencies, with the integration of the temporal attention mechanism and
global elastic attentional encoder. Extensive experiments demonstrate the
superiority of our AttnTUL method as compared to state-of-the-art baselines on
various trajectory datasets. The source code of our model is available at
https://github.com/Onedean/AttnTUL.",None,-1
Guided Image Synthesis via Initial Image Editing in Diffusion Model,0.660078,"Diffusion models have the ability to generate high quality images by
denoising pure Gaussian noise images. While previous research has primarily
focused on improving the control of image generation through adjusting the
denoising process, we propose a novel direction of manipulating the initial
noise to control the generated image. Through experiments on stable diffusion,
we show that blocks of pixels in the initial latent images have a preference
for generating specific content, and that modifying these blocks can
significantly influence the generated image. In particular, we show that
modifying a part of the initial image affects the corresponding region of the
generated image while leaving other regions unaffected, which is useful for
repainting tasks. Furthermore, we find that the generation preferences of pixel
blocks are primarily determined by their values, rather than their position. By
moving pixel blocks with a tendency to generate user-desired content to
user-specified regions, our approach achieves state-of-the-art performance in
layout-to-image generation. Our results highlight the flexibility and power of
initial image manipulation in controlling the generated image.",None,-1
Less than One-shot: Named Entity Recognition via Extremely Weak Supervision,0.731505,"We study the named entity recognition (NER) problem under the extremely weak
supervision (XWS) setting, where only one example entity per type is given in a
context-free way. While one can see that XWS is lighter than one-shot in terms
of the amount of supervision, we propose a novel method X-NER that can
outperform the state-of-the-art one-shot NER methods. We first mine entity
spans that are similar to the example entities from an unlabelled training
corpus. Instead of utilizing entity span representations from language models,
we find it more effective to compare the context distributions before and after
the span is replaced by the entity example. We then leverage the top-ranked
spans as pseudo-labels to train an NER tagger. Extensive experiments and
analyses on 4 NER datasets show the superior end-to-end NER performance of
X-NER, outperforming the state-of-the-art few-shot methods with 1-shot
supervision and ChatGPT annotations significantly. Finally, our X-NER possesses
several notable properties, such as inheriting the cross-lingual abilities of
the underlying language models.",None,-1
Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data,0.611651,"This study proposes a novel heterogeneous graph convolutional neural network
(HGCNN) to handle complex brain fMRI data at regional and across-region levels.
We introduce a generic formulation of spectral filters on heterogeneous graphs
by introducing the $k-th$ Hodge-Laplacian (HL) operator. In particular, we
propose Laguerre polynomial approximations of HL spectral filters and prove
that their spatial localization on graphs is related to the polynomial order.
Furthermore, based on the bijection property of boundary operators on simplex
graphs, we introduce a generic topological graph pooling (TGPool) method that
can be used at any dimensional simplices. This study designs HL-node, HL-edge,
and HL-HGCNN neural networks to learn signal representation at a graph node,
edge levels, and both, respectively. Our experiments employ fMRI from the
Adolescent Brain Cognitive Development (ABCD; n=7693) to predict general
intelligence. Our results demonstrate the advantage of the HL-edge network over
the HL-node network when functional brain connectivity is considered as
features. The HL-HGCNN outperforms the state-of-the-art graph neural networks
(GNNs) approaches, such as GAT, BrainGNN, dGCN, BrainNetCNN, and Hypergraph NN.
The functional connectivity features learned from the HL-HGCNN are meaningful
in interpreting neural circuits related to general intelligence.",None,-1
Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment,0.548843,"Urban environments, characterized by their complex, multi-layered networks
encompassing physical, social, economic, and environmental dimensions, face
significant challenges in the face of rapid urbanization. These challenges,
ranging from traffic congestion and pollution to social inequality, call for
advanced technological interventions. Recent developments in big data,
artificial intelligence, urban computing, and digital twins have laid the
groundwork for sophisticated city modeling and simulation. However, a gap
persists between these technological capabilities and their practical
implementation in addressing urban challenges in an systemic-intelligent way.
This paper proposes Urban Generative Intelligence (UGI), a novel foundational
platform integrating Large Language Models (LLMs) into urban systems to foster
a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model
trained on city-specific multi-source data, to create embodied agents for
various urban tasks. These agents, operating within a textual urban environment
emulated by city simulator and urban knowledge graph, interact through a
natural language interface, offering an open platform for diverse intelligent
and embodied agent development. This platform not only addresses specific urban
issues but also simulates complex urban systems, providing a multidisciplinary
approach to understand and manage urban complexity. This work signifies a
transformative step in city science and urban intelligence, harnessing the
power of LLMs to unravel and address the intricate dynamics of urban systems.
The code repository with demonstrations will soon be released here
https://github.com/tsinghua-fib-lab/UGI.",None,-1
"An End-to-End Framework of Road User Detection, Tracking, and Prediction from Monocular Images",0.151706,"Perception that involves multi-object detection and tracking, and trajectory
prediction are two major tasks of autonomous driving. However, they are
currently mostly studied separately, which results in most trajectory
prediction modules being developed based on ground truth trajectories without
taking into account that trajectories extracted from the detection and tracking
modules in real-world scenarios are noisy. These noisy trajectories can have a
significant impact on the performance of the trajectory predictor and can lead
to serious prediction errors. In this paper, we build an end-to-end framework
for detection, tracking, and trajectory prediction called ODTP (Online
Detection, Tracking and Prediction). It adopts the state-of-the-art online
multi-object tracking model, QD-3DT, for perception and trains the trajectory
predictor, DCENet++, directly based on the detection results without purely
relying on ground truth trajectories. We evaluate the performance of ODTP on
the widely used nuScenes dataset for autonomous driving. Extensive experiments
show that ODPT achieves high performance end-to-end trajectory prediction.
DCENet++, with the enhanced dynamic maps, predicts more accurate trajectories
than its base model. It is also more robust when compared with other generative
and deterministic trajectory prediction models trained on noisy detection
results.",None,-1
Weight-Inherited Distillation for Task-Agnostic BERT Compression,0.476911,"Knowledge Distillation (KD) is a predominant approach for BERT compression.
Previous KD-based methods focus on designing extra alignment losses for the
student model to mimic the behavior of the teacher model. These methods
transfer the knowledge in an indirect way. In this paper, we propose a novel
Weight-Inherited Distillation (WID), which directly transfers knowledge from
the teacher. WID does not require any additional alignment loss and trains a
compact student by inheriting the weights, showing a new perspective of
knowledge distillation. Specifically, we design the row compactors and column
compactors as mappings and then compress the weights via structural
re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show
that WID outperforms previous state-of-the-art KD-based baselines. Further
analysis indicates that WID can also learn the attention patterns from the
teacher model without any alignment loss on attention distributions. The code
is available at https://github.com/wutaiqiang/WID-NAACL2024.",None,-1
Parameter Sharing with Network Pruning for Scalable Multi-Agent Deep Reinforcement Learning,0.346992,"Handling the problem of scalability is one of the essential issues for
multi-agent reinforcement learning (MARL) algorithms to be applied to
real-world problems typically involving massively many agents. For this,
parameter sharing across multiple agents has widely been used since it reduces
the training time by decreasing the number of parameters and increasing the
sample efficiency. However, using the same parameters across agents limits the
representational capacity of the joint policy and consequently, the performance
can be degraded in multi-agent tasks that require different behaviors for
different agents. In this paper, we propose a simple method that adopts
structured pruning for a deep neural network to increase the representational
capacity of the joint policy without introducing additional parameters. We
evaluate the proposed method on several benchmark tasks, and numerical results
show that the proposed method significantly outperforms other parameter-sharing
methods.",None,-1
TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models,0.65292,"Data augmentation has been established as an efficacious approach to
supplement useful information for low-resource datasets. Traditional
augmentation techniques such as noise injection and image transformations have
been widely used. In addition, generative data augmentation (GDA) has been
shown to produce more diverse and flexible data. While generative adversarial
networks (GANs) have been frequently used for GDA, they lack diversity and
controllability compared to text-to-image diffusion models. In this paper, we
propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the
capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image
(T2I) generative models for data augmentation. By conditioning the T2I model on
detailed descriptions produced by T2T models, we are able to generate
photo-realistic labeled images in a flexible and controllable manner.
Experiments on in-domain classification, cross-domain classification, and image
captioning tasks show consistent improvements over other data augmentation
baselines. Analytical studies in varied settings, including few-shot,
long-tail, and adversarial, further reinforce the effectiveness of TTIDA in
enhancing performance and increasing robustness.",None,-1
Dual Learning for Large Vocabulary On-Device ASR,0.0783221,"Dual learning is a paradigm for semi-supervised machine learning that seeks
to leverage unsupervised data by solving two opposite tasks at once. In this
scheme, each model is used to generate pseudo-labels for unlabeled examples
that are used to train the other model. Dual learning has seen some use in
speech processing by pairing ASR and TTS as dual tasks. However, these results
mostly address only the case of using unpaired examples to compensate for very
small supervised datasets, and mostly on large, non-streaming models. Dual
learning has not yet been proven effective for using unsupervised data to
improve realistic on-device streaming models that are already trained on large
supervised corpora. We provide this missing piece though an analysis of an
on-device-sized streaming conformer trained on the entirety of Librispeech,
showing relative WER improvements of 10.7%/5.2% without an LM and 11.7%/16.4%
with an LM.",None,-1
RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction,0.676627,"Recently, neural implicit surfaces have become popular for multi-view
reconstruction. To facilitate practical applications like scene editing and
manipulation, some works extend the framework with semantic masks input for the
object-compositional reconstruction rather than the holistic perspective.
Though achieving plausible disentanglement, the performance drops significantly
when processing the indoor scenes where objects are usually partially observed.
We propose RICO to address this by regularizing the unobservable regions for
indoor compositional reconstruction. Our key idea is to first regularize the
smoothness of the occluded background, which then in turn guides the foreground
object reconstruction in unobservable regions based on the object-background
relationship. Particularly, we regularize the geometry smoothness of occluded
background patches. With the improved background surface, the signed distance
function and the reversedly rendered depth of objects can be optimized to bound
them within the background range. Extensive experiments show our method
outperforms other methods on synthetic and real-world indoor scenes and prove
the effectiveness of proposed regularizations. The code is available at
https://github.com/kyleleey/RICO.",None,-1
"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",0.973256,"Humor is a central aspect of human communication that has not been solved for
artificial agents so far. Large language models (LLMs) are increasingly able to
capture implicit and contextual information. Especially, OpenAI's ChatGPT
recently gained immense public attention. The GPT3-based model almost seems to
communicate on a human level and can even tell jokes. Humor is an essential
component of human communication. But is ChatGPT really funny? We put ChatGPT's
sense of humor to the test. In a series of exploratory experiments around
jokes, i.e., generation, explanation, and detection, we seek to understand
ChatGPT's capability to grasp and reproduce human humor. Since the model itself
is not accessible, we applied prompt-based experiments. Our empirical evidence
indicates that jokes are not hard-coded but mostly also not newly generated by
the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system
accurately explains valid jokes but also comes up with fictional explanations
for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the
classification of jokes. ChatGPT has not solved computational humor yet but it
can be a big leap toward ""funny"" machines.",None,-1
Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation,0.521157,"This paper investigates Cross-Domain Sequential Recommendation (CDSR), a
promising method that uses information from multiple domains (more than three)
to generate accurate and diverse recommendations, and takes into account the
sequential nature of user interactions. The effectiveness of these systems
often depends on the complex interplay among the multiple domains. In this
dynamic landscape, the problem of negative transfer arises, where heterogeneous
knowledge between dissimilar domains leads to performance degradation due to
differences in user preferences across these domains. As a remedy, we propose a
new CDSR framework that addresses the problem of negative transfer by assessing
the extent of negative transfer from one domain to another and adaptively
assigning low weight values to the corresponding prediction losses. To this
end, the amount of negative transfer is estimated by measuring the marginal
contribution of each domain to model performance based on a cooperative game
theory. In addition, a hierarchical contrastive learning approach that
incorporates information from the sequence of coarse-level categories into that
of fine-level categories (e.g., item level) when implementing contrastive
learning was developed to mitigate negative transfer. Despite the potentially
low relevance between domains at the fine-level, there may be higher relevance
at the category level due to its generalised and broader preferences. We show
that our model is superior to prior works in terms of model performance on two
real-world datasets across ten different domains.",None,-1
DomainAdaptor: A Novel Approach to Test-time Adaptation,0.661406,"To deal with the domain shift between training and test samples, current
methods have primarily focused on learning generalizable features during
training and ignore the specificity of unseen samples that are also critical
during the test. In this paper, we investigate a more challenging task that
aims to adapt a trained CNN model to unseen domains during the test. To
maximumly mine the information in the test data, we propose a unified method
called DomainAdaptor for the test-time adaptation, which consists of an
AdaMixBN module and a Generalized Entropy Minimization (GEM) loss.
Specifically, AdaMixBN addresses the domain shift by adaptively fusing training
and test statistics in the normalization layer via a dynamic mixture
coefficient and a statistic transformation operation. To further enhance the
adaptation ability of AdaMixBN, we design a GEM loss that extends the Entropy
Minimization loss to better exploit the information in the test data. Extensive
experiments show that DomainAdaptor consistently outperforms the
state-of-the-art methods on four benchmarks. Furthermore, our method brings
more remarkable improvement against existing methods on the few-data unseen
domain. The code is available at https://github.com/koncle/DomainAdaptor.",None,-1
Agent-based Collaborative Random Search for Hyper-parameter Tuning and Global Function Optimization,0.580747,"Hyper-parameter optimization is one of the most tedious yet crucial steps in
training machine learning models. There are numerous methods for this vital
model-building stage, ranging from domain-specific manual tuning guidelines
suggested by the oracles to the utilization of general-purpose black-box
optimization techniques. This paper proposes an agent-based collaborative
technique for finding near-optimal values for any arbitrary set of
hyper-parameters (or decision variables) in a machine learning model (or
general function optimization problem). The developed method forms a
hierarchical agent-based architecture for the distribution of the searching
operations at different dimensions and employs a cooperative searching
procedure based on an adaptive width-based random sampling technique to locate
the optima. The behavior of the presented model, specifically against the
changes in its design parameters, is investigated in both machine learning and
global function optimization applications, and its performance is compared with
that of two randomized tuning strategies that are commonly used in practice.
According to the empirical results, the proposed model outperformed the
compared methods in the experimented classification, regression, and
multi-dimensional function optimization tasks, notably in a higher number of
dimensions and in the presence of limited on-device computational resources.",None,-1
Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning,0.548094,"Patients undergoing chest X-rays (CXR) often endure multiple lung diseases.
When evaluating a patient's condition, due to the complex pathologies, subtle
texture changes of different lung lesions in images, and patient condition
differences, radiologists may make uncertain even when they have experienced
long-term clinical training and professional guidance, which makes much noise
in extracting disease labels based on CXR reports. In this paper, we re-extract
disease labels from CXR reports to make them more realistic by considering
disease severity and uncertainty in classification. Our contributions are as
follows: 1. We re-extracted the disease labels with severity and uncertainty by
a rule-based approach with keywords discussed with clinical experts. 2. To
further improve the explainability of chest X-ray diagnosis, we designed a
multi-relationship graph learning method with an expert uncertainty-aware loss
function. 3. Our multi-relationship graph learning method can also interpret
the disease classification results. Our experimental results show that models
considering disease severity and uncertainty outperform previous
state-of-the-art methods.",None,-1
SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF),0.151706,"The accurate reconstruction of surgical scenes from surgical videos is
critical for various applications, including intraoperative navigation and
image-guided robotic surgery automation. However, previous approaches, mainly
relying on depth estimation, have limited effectiveness in reconstructing
surgical scenes with moving surgical tools. To address this limitation and
provide accurate 3D position prediction for surgical tools in all frames, we
propose a novel approach called SAMSNeRF that combines Segment Anything Model
(SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates
accurate segmentation masks of surgical tools using SAM, which guides the
refinement of the dynamic surgical scene reconstruction by NeRF. Our
experimental results on public endoscopy surgical videos demonstrate that our
approach successfully reconstructs high-fidelity dynamic surgical scenes and
accurately reflects the spatial information of surgical tools. Our proposed
approach can significantly enhance surgical navigation and automation by
providing surgeons with accurate 3D position information of surgical tools
during surgery.The source code will be released soon.",None,-1
LCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance,0.985602,"Automatic crowd counting using density estimation has gained significant
attention in computer vision research. As a result, a large number of crowd
counting and density estimation models using convolution neural networks (CNN)
have been published in the last few years. These models have achieved good
accuracy over benchmark datasets. However, attempts to improve the accuracy
often lead to higher complexity in these models. In real-time video
surveillance applications using drones with limited computing resources, deep
models incur intolerable higher inference delay. In this paper, we propose (i)
a Lightweight Crowd Density estimation model (LCDnet) for real-time video
surveillance, and (ii) an improved training method using curriculum learning
(CL). LCDnet is trained using CL and evaluated over two benchmark datasets
i.e., DroneRGBT and CARPK. Results are compared with existing crowd models. Our
evaluation shows that the LCDnet achieves a reasonably good accuracy while
significantly reducing the inference time and memory requirement and thus can
be deployed over edge devices with very limited computing resources.",None,-1
SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models,0.417822,"Stereotype benchmark datasets are crucial to detect and mitigate social
stereotypes about groups of people in NLP models. However, existing datasets
are limited in size and coverage, and are largely restricted to stereotypes
prevalent in the Western society. This is especially problematic as language
technologies gain hold across the globe. To address this gap, we present
SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative
capabilities of large language models such as PaLM, and GPT-3, and leveraging a
globally diverse rater pool to validate the prevalence of those stereotypes in
society. SeeGULL is in English, and contains stereotypes about identity groups
spanning 178 countries across 8 different geo-political regions across 6
continents, as well as state-level identities within the US and India. We also
include fine-grained offensiveness scores for different stereotypes and
demonstrate their global disparities. Furthermore, we include comparative
annotations about the same groups by annotators living in the region vs. those
that are based in North America, and demonstrate that within-region stereotypes
about groups differ from those prevalent in North America. CONTENT WARNING:
This paper contains stereotype examples that may be offensive.",None,-1
A Theory of Emergent In-Context Learning as Implicit Structure Induction,0.301364,"Scaling large language models (LLMs) leads to an emergent capacity to learn
in-context from example demonstrations. Despite progress, theoretical
understanding of this phenomenon remains limited. We argue that in-context
learning relies on recombination of compositional operations found in natural
language data. We derive an information-theoretic bound showing how in-context
learning abilities arise from generic next-token prediction when the
pretraining distribution has sufficient amounts of compositional structure,
under linguistically motivated assumptions. A second bound provides a
theoretical justification for the empirical success of prompting LLMs to output
intermediate steps towards an answer. To validate theoretical predictions, we
introduce a controlled setup for inducing in-context learning; unlike previous
approaches, it accounts for the compositional nature of language. Trained
transformers can perform in-context learning for a range of tasks, in a manner
consistent with the theoretical results. Mirroring real-world LLMs in a
miniature setup, in-context learning emerges when scaling parameters and data,
and models perform better when prompted to output intermediate steps. Probing
shows that in-context learning is supported by a representation of the input's
compositional structure. Taken together, these results provide a step towards
theoretical understanding of emergent behavior in large language models.",None,-1
Multi-Task End-to-End Training Improves Conversational Recommendation,0.0949504,"In this paper, we analyze the performance of a multitask end-to-end
transformer model on the task of conversational recommendations, which aim to
provide recommendations based on a user's explicit preferences expressed in
dialogue. While previous works in this area adopt complex multi-component
approaches where the dialogue management and entity recommendation tasks are
handled by separate components, we show that a unified transformer model, based
on the T5 text-to-text transformer model, can perform competitively in both
recommending relevant items and generating conversation dialogue. We fine-tune
our model on the ReDIAL conversational movie recommendation dataset, and create
additional training tasks derived from MovieLens (such as the prediction of
movie attributes and related movies based on an input movie), in a multitask
learning setting. Using a series of probe studies, we demonstrate that the
learned knowledge in the additional tasks is transferred to the conversational
setting, where each task leads to a 9%-52% increase in its related probe score.",None,-1
Can GPT-3 Perform Statutory Reasoning?,0.655884,"Statutory reasoning is the task of reasoning with facts and statutes, which
are rules written in natural language by a legislature. It is a basic legal
skill. In this paper we explore the capabilities of the most capable GPT-3
model, text-davinci-003, on an established statutory-reasoning dataset called
SARA. We consider a variety of approaches, including dynamic few-shot
prompting, chain-of-thought prompting, and zero-shot prompting. While we
achieve results with GPT-3 that are better than the previous best published
results, we also identify several types of clear errors it makes. We
investigate why these errors happen. We discover that GPT-3 has imperfect prior
knowledge of the actual U.S. statutes on which SARA is based. More importantly,
we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen
during training. We find GPT-3 performs poorly at answering straightforward
questions about these simple synthetic statutes.",None,-1
Enhancing Video Super-Resolution via Implicit Resampling-based Alignment,0.0789836,"In video super-resolution, it is common to use a frame-wise alignment to
support the propagation of information over time. The role of alignment is
well-studied for low-level enhancement in video, but existing works overlook a
critical step -- resampling. We show through extensive experiments that for
alignment to be effective, the resampling should preserve the reference
frequency spectrum while minimizing spatial distortions. However, most existing
works simply use a default choice of bilinear interpolation for resampling even
though bilinear interpolation has a smoothing effect and hinders
super-resolution. From these observations, we propose an implicit
resampling-based alignment. The sampling positions are encoded by a sinusoidal
positional encoding, while the value is estimated with a coordinate network and
a window-based cross-attention. We show that bilinear interpolation inherently
attenuates high-frequency information while an MLP-based coordinate network can
approximate more frequencies. Experiments on synthetic and real-world datasets
show that alignment with our proposed implicit resampling enhances the
performance of state-of-the-art frameworks with minimal impact on both compute
and parameters.",None,-1
Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,0.571189,"In this work, we propose a novel complementary learning approach to enhance
test-time adaptation (TTA), which has been proven to exhibit good performance
on testing data with distribution shifts such as corruptions. In test-time
adaptation tasks, information from the source domain is typically unavailable
and the model has to be optimized without supervision for test-time samples.
Hence, usual methods assign labels for unannotated data with the prediction by
a well-trained source model in an unsupervised learning framework. Previous
studies have employed unsupervised objectives, such as the entropy of model
predictions, as optimization targets to effectively learn features for
test-time samples. However, the performance of the model is easily compromised
by the quality of pseudo-labels, since inaccuracies in pseudo-labels introduce
noise to the model. Therefore, we propose to leverage the ""less probable
categories"" to decrease the risk of incorrect pseudo-labeling. The
complementary label is introduced to designate these categories. We highlight
that the risk function of complementary labels agrees with their Vanilla loss
formula under the conventional true label distribution. Experiments show that
the proposed learning algorithm achieves state-of-the-art performance on
different datasets and experiment settings.",None,-1
To be Robust and to be Fair: Aligning Fairness with Robustness,0.136645,"Adversarial training has been shown to be reliable in improving robustness
against adversarial samples. However, the problem of adversarial training in
terms of fairness has not yet been properly studied, and the relationship
between fairness and accuracy attack still remains unclear. Can we
simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle
this topic, in this paper, we study the problem of adversarial training and
adversarial attack w.r.t. both metrics. We propose a unified structure for
fairness attack which brings together common notions in group fairness, and we
theoretically prove the equivalence of fairness attack against different
notions. Moreover, we show the alignment of fairness and accuracy attack, and
theoretically demonstrate that robustness w.r.t. one metric benefits from
robustness w.r.t. the other metric. Our study suggests a novel way to unify
adversarial training and attack w.r.t. fairness and accuracy, and experimental
results show that our proposed method achieves better performance in terms of
robustness w.r.t. both metrics.",None,-1
Does Federated Learning Really Need Backpropagation?,0.510565,"Federated learning (FL) is a general principle for decentralized clients to
train a server model collectively without sharing local data. FL is a promising
framework with practical applications, but its standard training paradigm
requires the clients to backpropagate through the model to compute gradients.
Since these clients are typically edge devices and not fully trusted, executing
backpropagation on them incurs computational and storage overhead as well as
white-box vulnerability. In light of this, we develop backpropagation-free
federated learning, dubbed BAFFLE, in which backpropagation is replaced by
multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient
and easily fits uploading bandwidth; 2) compatible with inference-only hardware
optimization and model quantization or pruning; and 3) well-suited to trusted
execution environments, because the clients in BAFFLE only execute forward
propagation and return a set of scalars to the server. Empirically we use
BAFFLE to train deep models from scratch or to finetune pretrained models,
achieving acceptable results. Code is available in
https://github.com/FengHZ/BAFFLE.",None,-1
PointFlowHop: Green and Interpretable Scene Flow Estimation from Consecutive Point Clouds,0.217843,"An efficient 3D scene flow estimation method called PointFlowHop is proposed
in this work. PointFlowHop takes two consecutive point clouds and determines
the 3D flow vectors for every point in the first point cloud. PointFlowHop
decomposes the scene flow estimation task into a set of subtasks, including
ego-motion compensation, object association and object-wise motion estimation.
It follows the green learning (GL) pipeline and adopts the feedforward data
processing path. As a result, its underlying mechanism is more transparent than
deep-learning (DL) solutions based on end-to-end optimization of network
parameters. We conduct experiments on the stereoKITTI and the Argoverse LiDAR
point cloud datasets and demonstrate that PointFlowHop outperforms
deep-learning methods with a small model size and less training time.
Furthermore, we compare the Floating Point Operations (FLOPs) required by
PointFlowHop and other learning-based methods in inference, and show its big
savings in computational complexity.",None,-1
Rethinking Voice-Face Correlation: A Geometry View,0.835534,"Previous works on voice-face matching and voice-guided face synthesis
demonstrate strong correlations between voice and face, but mainly rely on
coarse semantic cues such as gender, age, and emotion. In this paper, we aim to
investigate the capability of reconstructing the 3D facial shape from voice
from a geometry perspective without any semantic information. We propose a
voice-anthropometric measurement (AM)-face paradigm, which identifies
predictable facial AMs from the voice and uses them to guide 3D face
reconstruction. By leveraging AMs as a proxy to link the voice and face
geometry, we can eliminate the influence of unpredictable AMs and make the face
geometry tractable. Our approach is evaluated on our proposed dataset with
ground-truth 3D face scans and corresponding voice recordings, and we find
significant correlations between voice and specific parts of the face geometry,
such as the nasal cavity and cranium. Our work offers a new perspective on
voice-face correlation and can serve as a good empirical study for
anthropometry science.",None,-1
AU-aware graph convolutional network for Macro- and Micro-expression spotting,0.766672,"Automatic Micro-Expression (ME) spotting in long videos is a crucial step in
ME analysis but also a challenging task due to the short duration and low
intensity of MEs. When solving this problem, previous works generally lack in
considering the structures of human faces and the correspondence between
expressions and relevant facial muscles. To address this issue for better
performance of ME spotting, this paper seeks to extract finer spatial features
by modeling the relationships between facial Regions of Interest (ROIs).
Specifically, we propose a graph convolutional-based network, called
Action-Unit-aWare Graph Convolutional Network (AUW-GCN). Furthermore, to inject
prior information and to cope with the problem of small datasets, AU-related
statistics are encoded into the network. Comprehensive experiments show that
our results outperform baseline methods consistently and achieve new SOTA
performance in two benchmark datasets,CAS(ME)^2 and SAMM-LV. Our code is
available at https://github.com/xjtupanda/AUW-GCN.",None,-1
Audio-Visual Class-Incremental Learning,0.999428,"In this paper, we introduce audio-visual class-incremental learning, a
class-incremental learning scenario for audio-visual video recognition. We
demonstrate that joint audio-visual modeling can improve class-incremental
learning, but current methods fail to preserve semantic similarity between
audio and visual features as incremental step grows. Furthermore, we observe
that audio-visual correlations learned in previous tasks can be forgotten as
incremental steps progress, leading to poor performance. To overcome these
challenges, we propose AV-CIL, which incorporates Dual-Audio-Visual Similarity
Constraint (D-AVSC) to maintain both instance-aware and class-aware semantic
similarity between audio-visual modalities and Visual Attention Distillation
(VAD) to retain previously learned audio-guided visual attentive ability. We
create three audio-visual class-incremental datasets, AVE-Class-Incremental
(AVE-CI), Kinetics-Sounds-Class-Incremental (K-S-CI), and
VGGSound100-Class-Incremental (VS100-CI) based on the AVE, Kinetics-Sounds, and
VGGSound datasets, respectively. Our experiments on AVE-CI, K-S-CI, and
VS100-CI demonstrate that AV-CIL significantly outperforms existing
class-incremental learning methods in audio-visual class-incremental learning.
Code and data are available at: https://github.com/weiguoPian/AV-CIL_ICCV2023.",None,-1
Multi-view Self-supervised Disentanglement for General Image Denoising,0.431914,"With its significant performance improvements, the deep learning paradigm has
become a standard tool for modern image denoisers. While promising performance
has been shown on seen noise distributions, existing approaches often suffer
from generalisation to unseen noise types or general and real noise. It is
understandable as the model is designed to learn paired mapping (e.g. from a
noisy image to its clean version). In this paper, we instead propose to learn
to disentangle the noisy image, under the intuitive assumption that different
corrupted versions of the same clean image share a common latent space. A
self-supervised learning framework is proposed to achieve the goal, without
looking at the latent clean image. By taking two different corrupted versions
of the same image as input, the proposed Multi-view Self-supervised
Disentanglement (MeD) approach learns to disentangle the latent clean features
from the corruptions and recover the clean image consequently. Extensive
experimental analysis on both synthetic and real noise shows the superiority of
the proposed method over prior self-supervised approaches, especially on unseen
novel noise types. On real noise, the proposed method even outperforms its
supervised counterparts by over 3 dB.",None,-1
Scaling Sentence Embeddings with Large Language Models,0.982438,"Large language models (LLMs) have recently garnered significant interest.
With in-context learning, LLMs achieve impressive results in various natural
language tasks. However, the application of LLMs to sentence embeddings remains
an area of ongoing research. In this work, we propose an in-context
learning-based method aimed at improving sentence embeddings performance. Our
approach involves adapting the previous prompt-based representation method for
autoregressive models, constructing a demonstration set that enables LLMs to
perform in-context learning, and scaling up the LLMs to different model sizes.
Through extensive experiments, in-context learning enables LLMs to generate
high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve
performance comparable to current contrastive learning methods. By scaling
model size, we find scaling to more than tens of billion parameters harms the
performance on semantic textual similarity (STS) tasks. However, the largest
model outperforms other counterparts and achieves the new state-of-the-art
result on transfer tasks. We also fine-tune LLMs with current contrastive
learning approach, and the 2.7B OPT model, incorporating our prompt-based
method, surpasses the performance of 4.8B ST5, achieving the new
state-of-the-art results on STS tasks. Our code is available at
https://github.com/kongds/scaling_sentemb.",None,-1
Just Tell Me: Prompt Engineering in Business Process Management,0.586599,"GPT-3 and several other language models (LMs) can effectively address various
natural language processing (NLP) tasks, including machine translation and text
summarization. Recently, they have also been successfully employed in the
business process management (BPM) domain, e.g., for predictive process
monitoring and process extraction from text. This, however, typically requires
fine-tuning the employed LM, which, among others, necessitates large amounts of
suitable training data. A possible solution to this problem is the use of
prompt engineering, which leverages pre-trained LMs without fine-tuning them.
Recognizing this, we argue that prompt engineering can help bring the
capabilities of LMs to BPM research. We use this position paper to develop a
research agenda for the use of prompt engineering for BPM research by
identifying the associated potentials and challenges.",None,-1
Graph Decision Transformer,0.495718,"Offline reinforcement learning (RL) is a challenging task, whose objective is
to learn policies from static trajectory data without interacting with the
environment. Recently, offline RL has been viewed as a sequence modeling
problem, where an agent generates a sequence of subsequent actions based on a
set of static transition experiences. However, existing approaches that use
transformers to attend to all tokens naively can overlook the dependencies
between different tokens and limit long-term dependency learning. In this
paper, we propose the Graph Decision Transformer (GDT), a novel offline RL
approach that models the input sequence into a causal graph to capture
potential dependencies between fundamentally different concepts and facilitate
temporal and causal relationship learning. GDT uses a graph transformer to
process the graph inputs with relation-enhanced mechanisms, and an optional
sequence transformer to handle fine-grained spatial information in visual
tasks. Our experiments show that GDT matches or surpasses the performance of
state-of-the-art offline RL methods on image-based Atari and OpenAI Gym.",None,-1
Story Visualization by Online Text Augmentation with Context Memory,0.139489,"Story visualization (SV) is a challenging text-to-image generation task for
the difficulty of not only rendering visual details from the text descriptions
but also encoding a long-term context across multiple sentences. While prior
efforts mostly focus on generating a semantically relevant image for each
sentence, encoding a context spread across the given paragraph to generate
contextually convincing images (e.g., with a correct character or with a proper
background of the scene) remains a challenge. To this end, we propose a novel
memory architecture for the Bi-directional Transformer framework with an online
text augmentation that generates multiple pseudo-descriptions as supplementary
supervision during training for better generalization to the language variation
at inference. In extensive experiments on the two popular SV benchmarks, i.e.,
the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms
the state of the arts in various metrics including FID, character F1, frame
accuracy, BLEU-2/3, and R-precision with similar or less computational
complexity.",None,-1
Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence,0.777899,"Sentence-level representations are beneficial for various natural language
processing tasks. It is commonly believed that vector representations can
capture rich linguistic properties. Currently, large language models (LMs)
achieve state-of-the-art performance on sentence embedding. However, some
recent works suggest that vector representations from LMs can cause information
leakage. In this work, we further investigate the information leakage issue and
propose a generative embedding inversion attack (GEIA) that aims to reconstruct
input sequences based only on their sentence embeddings. Given the black-box
access to a language model, we treat sentence embeddings as initial tokens'
representations and train or fine-tune a powerful decoder model to decode the
whole sequences directly. We conduct extensive experiments to demonstrate that
our generative inversion attack outperforms previous embedding inversion
attacks in classification metrics and generates coherent and contextually
similar sentences as the original inputs.",None,-1
Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science,0.986305,"Large Language Models (LLMs) have democratized synthetic data generation,
which in turn has the potential to simplify and broaden a wide gamut of NLP
tasks. Here, we tackle a pervasive problem in synthetic data generation: its
generative distribution often differs from the distribution of real-world data
researchers care about (in other words, it is unfaithful). In a case study on
sarcasm detection, we study three strategies to increase the faithfulness of
synthetic data: grounding, filtering, and taxonomy-based generation. We
evaluate these strategies using the performance of classifiers trained with
generated synthetic data on real-world data. While all three strategies improve
the performance of classifiers, we find that grounding works best for the task
at hand. As synthetic data generation plays an ever-increasing role in NLP
research, we expect this work to be a stepping stone in improving its utility.
We conclude this paper with some recommendations on how to generate
high(er)-fidelity synthetic data for specific tasks.",None,-1
Instance-based Explanations for Gradient Boosting Machine Predictions with AXIL Weights,0.096829,"We show that regression predictions from linear and tree-based models can be
represented as linear combinations of target instances in the training data.
This also holds for models constructed as ensembles of trees, including Random
Forests and Gradient Boosting Machines. The weights used in these linear
combinations are measures of instance importance, complementing existing
measures of feature importance, such as SHAP and LIME. We refer to these
measures as AXIL weights (Additive eXplanations with Instance Loadings). Since
AXIL weights are additive across instances, they offer both local and global
explanations. Our work contributes to the broader effort to make machine
learning predictions more interpretable and explainable.",None,-1
Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4,0.999682,"Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.",None,-1
ViTs for SITS: Vision Transformers for Satellite Image Time Series,0.943036,"In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a
fully-attentional model for general Satellite Image Time Series (SITS)
processing based on the Vision Transformer (ViT). TSViT splits a SITS record
into non-overlapping patches in space and time which are tokenized and
subsequently processed by a factorized temporo-spatial encoder. We argue, that
in contrast to natural images, a temporal-then-spatial factorization is more
intuitive for SITS processing and present experimental evidence for this claim.
Additionally, we enhance the model's discriminative power by introducing two
novel mechanisms for acquisition-time-specific temporal positional encodings
and multiple learnable class tokens. The effect of all novel design choices is
evaluated through an extensive ablation study. Our proposed architecture
achieves state-of-the-art performance, surpassing previous approaches by a
significant margin in three publicly available SITS semantic segmentation and
classification datasets. All model, training and evaluation codes are made
publicly available to facilitate further research.",None,-1
Learning Optimal Policy for Simultaneous Machine Translation via Binary Search,0.699964,"Simultaneous machine translation (SiMT) starts to output translation while
reading the source sentence and needs a precise policy to decide when to output
the generated translation. Therefore, the policy determines the number of
source tokens read during the translation of each target token. However, it is
difficult to learn a precise translation policy to achieve good latency-quality
trade-offs, because there is no golden policy corresponding to parallel
sentences as explicit supervision. In this paper, we present a new method for
constructing the optimal policy online via binary search. By employing explicit
supervision, our approach enables the SiMT model to learn the optimal policy,
which can guide the model in completing the translation during inference.
Experiments on four translation tasks show that our method can exceed strong
baselines across all latency scenarios.",None,-1
Image To Tree with Recursive Prompting,0.0770791,"Extracting complex structures from grid-based data is a common key step in
automated medical image analysis. The conventional solution to recovering
tree-structured geometries typically involves computing the minimal cost path
through intermediate representations derived from segmentation masks. However,
this methodology has significant limitations in the context of projective
imaging of tree-structured 3D anatomical data such as coronary arteries, since
there are often overlapping branches in the 2D projection. In this work, we
propose a novel approach to predicting tree connectivity structure which
reformulates the task as an optimization problem over individual steps of a
recursive process. We design and train a two-stage model which leverages the
UNet and Transformer architectures and introduces an image-based prompting
technique. Our proposed method achieves compelling results on a pair of
synthetic datasets, and outperforms a shortest-path baseline.",None,-1
Open-Set Domain Adaptation with Visual-Language Foundation Models,0.28677,"Unsupervised domain adaptation (UDA) has proven to be very effective in
transferring knowledge obtained from a source domain with labeled data to a
target domain with unlabeled data. Owing to the lack of labeled data in the
target domain and the possible presence of unknown classes, open-set domain
adaptation (ODA) has emerged as a potential solution to identify these classes
during the training phase. Although existing ODA approaches aim to solve the
distribution shifts between the source and target domains, most methods
fine-tuned ImageNet pre-trained models on the source domain with the adaptation
on the target domain. Recent visual-language foundation models (VLFM), such as
Contrastive Language-Image Pre-Training (CLIP), are robust to many distribution
shifts and, therefore, should substantially improve the performance of ODA. In
this work, we explore generic ways to adopt CLIP, a popular VLFM, for ODA. We
investigate the performance of zero-shot prediction using CLIP, and then
propose an entropy optimization strategy to assist the ODA models with the
outputs of CLIP. The proposed approach achieves state-of-the-art results on
various benchmarks, demonstrating its effectiveness in addressing the ODA
problem.",None,-1
Model-Based Simulation for Optimising Smart Reply,0.425716,"Smart Reply (SR) systems present a user with a set of replies, of which one
can be selected in place of having to type out a response. To perform well at
this task, a system should be able to effectively present the user with a
diverse set of options, to maximise the chance that at least one of them
conveys the user's desired response. This is a significant challenge, due to
the lack of datasets containing sets of responses to learn from. Resultantly,
previous work has focused largely on post-hoc diversification, rather than
explicitly learning to predict sets of responses. Motivated by this problem, we
present a novel method SimSR, that employs model-based simulation to discover
high-value response sets, through simulating possible user responses with a
learned world model. Unlike previous approaches, this allows our method to
directly optimise the end-goal of SR--maximising the relevance of at least one
of the predicted replies. Empirically on two public datasets, when compared to
SoTA baselines, our method achieves up to 21% and 18% improvement in ROUGE
score and Self-ROUGE score respectively.",None,-1
An Extensible Multimodal Multi-task Object Dataset with Materials,0.0397434,"We present EMMa, an Extensible, Multimodal dataset of Amazon product listings
that contains rich Material annotations. It contains more than 2.8 million
objects, each with image(s), listing text, mass, price, product ratings, and
position in Amazon's product-category taxonomy. We also design a comprehensive
taxonomy of 182 physical materials (e.g., Plastic $\rightarrow$ Thermoplastic
$\rightarrow$ Acrylic). Objects are annotated with one or more materials from
this taxonomy. With the numerous attributes available for each object, we
develop a Smart Labeling framework to quickly add new binary labels to all
objects with very little manual labeling effort, making the dataset extensible.
Each object attribute in our dataset can be included in either the model inputs
or outputs, leading to combinatorial possibilities in task configurations. For
example, we can train a model to predict the object category from the listing
text, or the mass and price from the product listing image. EMMa offers a new
benchmark for multi-task learning in computer vision and NLP, and allows
practitioners to efficiently add new tasks and object attributes at scale.",None,-1
The Role of Interactive Visualization in Explaining (Large) NLP Models: from Data to Inference,0.656826,"With a constant increase of learned parameters, modern neural language models
become increasingly more powerful. Yet, explaining these complex model's
behavior remains a widely unsolved problem. In this paper, we discuss the role
interactive visualization can play in explaining NLP models (XNLP). We motivate
the use of visualization in relation to target users and common NLP pipelines.
We also present several use cases to provide concrete examples on XNLP with
visualization. Finally, we point out an extensive list of research
opportunities in this field.",None,-1
Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition,0.780015,"Human emotion recognition plays an important role in human-computer
interaction. In this paper, we present our approach to the Valence-Arousal (VA)
Estimation Challenge, Expression (Expr) Classification Challenge, and Action
Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective
Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel
multi-modal fusion model that leverages Temporal Convolutional Networks (TCN)
and Transformer to enhance the performance of continuous emotion recognition.
Our model aims to effectively integrate visual and audio information for
improved accuracy in recognizing emotions. Our model outperforms the baseline
and ranks 3 in the Expression Classification challenge.",None,-1
Comparing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication,0.687926,"Traditionally, writing assistance systems have focused on short or even
single-word suggestions. Recently, large language models like GPT-3 have made
it possible to generate significantly longer natural-sounding suggestions,
offering more advanced assistance opportunities. This study explores the
trade-offs between sentence- vs. message-level suggestions for AI-mediated
communication. We recruited 120 participants to act as staffers from
legislators' offices who often need to respond to large volumes of constituent
concerns. Participants were asked to reply to emails with different types of
assistance. The results show that participants receiving message-level
suggestions responded faster and were more satisfied with the experience, as
they mainly edited the suggested drafts. In addition, the texts they wrote were
evaluated as more helpful by others. In comparison, participants receiving
sentence-level assistance retained a higher sense of agency, but took longer
for the task as they needed to plan the flow of their responses and decide when
to use suggestions. Our findings have implications for designing
task-appropriate communication assistance systems.",None,-1
NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,0.974691,"Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.",None,-1
I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction,0.78707,"Multimodal information extraction is attracting research attention nowadays,
which requires aggregating representations from different modalities. In this
paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM)
method for this task, which contains two modules. Firstly, the intra-sample
relationship modeling module operates on a single sample and aims to learn
effective representations. Embeddings from textual and visual modalities are
shifted to bridge the modality gap caused by distinct pre-trained language and
image models. Secondly, the inter-sample relationship modeling module considers
relationships among multiple samples and focuses on capturing the interactions.
An AttnMixup strategy is proposed, which not only enables collaboration among
samples but also augments data to improve generalization. We conduct extensive
experiments on the multimodal named entity recognition datasets Twitter-2015
and Twitter-2017, and the multimodal relation extraction dataset MNRE. Our
proposed method I2SRM achieves competitive results, 77.12% F1-score on
Twitter-2015, 88.40% F1-score on Twitter-2017, and 84.12% F1-score on MNRE.",None,-1
ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification,0.781883,"Few-shot text classification has recently been promoted by the meta-learning
paradigm which aims to identify target classes with knowledge transferred from
source classes with sets of small tasks named episodes. Despite their success,
existing works building their meta-learner based on Prototypical Networks are
unsatisfactory in learning discriminative text representations between similar
classes, which may lead to contradictions during label prediction. In addition,
the tasklevel and instance-level overfitting problems in few-shot text
classification caused by a few training examples are not sufficiently tackled.
In this work, we propose a contrastive learning framework named ContrastNet to
tackle both discriminative representation and overfitting problems in few-shot
text classification. ContrastNet learns to pull closer text representations
belonging to the same class and push away text representations belonging to
different classes, while simultaneously introducing unsupervised contrastive
regularization at both task-level and instance-level to prevent overfitting.
Experiments on 8 few-shot text classification datasets show that ContrastNet
outperforms the current state-of-the-art models.",None,-1
Towards Integration of Discriminability and Robustness for Document-Level Relation Extraction,0.775232,"Document-level relation extraction (DocRE) predicts relations for entity
pairs that rely on long-range context-dependent reasoning in a document. As a
typical multi-label classification problem, DocRE faces the challenge of
effectively distinguishing a small set of positive relations from the majority
of negative ones. This challenge becomes even more difficult to overcome when
there exists a significant number of annotation errors in the dataset. In this
work, we aim to achieve better integration of both the discriminability and
robustness for the DocRE problem. Specifically, we first design an effective
loss function to endow high discriminability to both probabilistic outputs and
internal representations. We innovatively customize entropy minimization and
supervised contrastive learning for the challenging multi-label and long-tailed
learning problems. To ameliorate the impact of label errors, we equipped our
method with a novel negative label sampling strategy to strengthen the model
robustness. In addition, we introduce two new data regimes to mimic more
realistic scenarios with annotation errors and evaluate our sampling strategy.
Experimental results verify the effectiveness of each component and show that
our method achieves new state-of-the-art results on the DocRED dataset, its
recently cleaned version, Re-DocRED, and the proposed data regimes.",None,-1
GaitFormer: Revisiting Intrinsic Periodicity for Gait Recognition,0.324766,"Gait recognition aims to distinguish different walking patterns by analyzing
video-level human silhouettes, rather than relying on appearance information.
Previous research on gait recognition has primarily focused on extracting local
or global spatial-temporal representations, while overlooking the intrinsic
periodic features of gait sequences, which, when fully utilized, can
significantly enhance performance. In this work, we propose a plug-and-play
strategy, called Temporal Periodic Alignment (TPA), which leverages the
periodic nature and fine-grained temporal dependencies of gait patterns. The
TPA strategy comprises two key components. The first component is Adaptive
Fourier-transform Position Encoding (AFPE), which adaptively converts features
and discrete-time signals into embeddings that are sensitive to periodic
walking patterns. The second component is the Temporal Aggregation Module
(TAM), which separates embeddings into trend and seasonal components, and
extracts meaningful temporal correlations to identify primary components, while
filtering out random noise. We present a simple and effective baseline method
for gait recognition, based on the TPA strategy. Extensive experiments
conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW)
demonstrate that our proposed method achieves state-of-the-art performance on
multiple benchmark tests.",None,-1
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations,0.996819,"Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations in a supervised manner. SACL applies
contrast-aware adversarial training to generate worst-case samples and uses
joint class-spread contrastive learning to extract structured representations.
It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training (CAT) strategy to learn more diverse features from context and enhance
the model's context robustness. Under the framework with CAT, we develop a
sequence-based SACL-LSTM to learn label-consistent and context-robust features
for ERC. Experiments on three datasets show that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of SACL and CAT.",None,-1
Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision,0.242535,"This paper introduces a new learning-based approach for approximately solving
the Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problem
consists of, given a pool of kidney donors and patients waiting for kidney
donations, optimally selecting a set of donations to optimize the quantity and
quality of transplants performed while respecting a set of constraints about
the arrangement of these donations. The proposed technique consists of two main
steps: the first is a Graph Neural Network (GNN) trained without supervision;
the second is a deterministic non-learned search heuristic that uses the output
of the GNN to find paths and cycles. To allow for comparisons, we also
implemented and tested an exact solution method using integer programming, two
greedy search heuristics without the machine learning module, and the GNN alone
without a heuristic. We analyze and compare the methods and conclude that the
learning-based two-stage approach is the best solution quality, outputting
approximate solutions on average 1.1 times more valuable than the ones from the
deterministic heuristic alone.",None,-1
CLIPER: A Unified Vision-Language Framework for In-the-Wild Facial Expression Recognition,0.874825,"Facial expression recognition (FER) is an essential task for understanding
human behaviors. As one of the most informative behaviors of humans, facial
expressions are often compound and variable, which is manifested by the fact
that different people may express the same expression in very different ways.
However, most FER methods still use one-hot or soft labels as the supervision,
which lack sufficient semantic descriptions of facial expressions and are less
interpretable. Recently, contrastive vision-language pre-training (VLP) models
(e.g., CLIP) use text as supervision and have injected new vitality into
various computer vision tasks, benefiting from the rich semantics in text.
Therefore, in this work, we propose CLIPER, a unified framework for both static
and dynamic facial Expression Recognition based on CLIP. Besides, we introduce
multiple expression text descriptors (METD) to learn fine-grained expression
representations that make CLIPER more interpretable. We conduct extensive
experiments on several popular FER benchmarks and achieve state-of-the-art
performance, which demonstrates the effectiveness of CLIPER.",None,-1
Spot-the-Camel: Computer Vision for Safer Roads,0.0921796,"As the population grows and more land is being used for urbanization,
ecosystems are disrupted by our roads and cars. This expansion of
infrastructure cuts through wildlife territories, leading to many instances of
Wildlife-Vehicle Collision (WVC). These instances of WVC are a global issue
that is having a global socio-economic impact, resulting in billions of dollars
in property damage and, at times, fatalities for vehicle occupants. In Saudi
Arabia, this issue is similar, with instances of Camel-Vehicle Collision (CVC)
being particularly deadly due to the large size of camels, which results in a
25% fatality rate [1]. The focus of this work is to test different object
detection models on the task of detecting camels on the road. The Deep Learning
(DL) object detection models used in the experiments are: Center Net, Efficient
Det, Faster R-CNN, SSD, and YOLOv8. Results of the experiments show that YOLOv8
performed the best in terms of accuracy and was the most efficient in training.
In the future, the plan is to expand on this work by developing a system to
make countryside roads safer.",None,-1
MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors,0.470477,"3D single object tracking has been a crucial problem for decades with
numerous applications such as autonomous driving. Despite its wide-ranging use,
this task remains challenging due to the significant appearance variation
caused by occlusion and size differences among tracked targets. To address
these issues, we present MBPTrack, which adopts a Memory mechanism to utilize
past information and formulates localization in a coarse-to-fine scheme using
Box Priors given in the first frame. Specifically, past frames with targetness
masks serve as an external memory, and a transformer-based module propagates
tracked target cues from the memory to the current frame. To precisely localize
objects of all sizes, MBPTrack first predicts the target center via Hough
voting. By leveraging box priors given in the first frame, we adaptively sample
reference points around the target center that roughly cover the target of
different sizes. Then, we obtain dense feature maps by aggregating point
features into the reference points, where localization can be performed more
effectively. Extensive experiments demonstrate that MBPTrack achieves
state-of-the-art performance on KITTI, nuScenes and Waymo Open Dataset, while
running at 50 FPS on a single RTX3090 GPU.",None,-1
Trusted Source Alignment in Large Language Models,0.01269,"Large language models (LLMs) are trained on web-scale corpora that inevitably
include contradictory factual information from sources of varying reliability.
In this paper, we propose measuring an LLM property called trusted source
alignment (TSA): the model's propensity to align with content produced by
trusted publishers in the face of uncertainty or controversy. We present
FactCheckQA, a TSA evaluation dataset based on a corpus of fact checking
articles. We describe a simple protocol for evaluating TSA and offer a detailed
analysis of design considerations including response extraction, claim
contextualization, and bias in prompt formulation. Applying the protocol to
PaLM-2, we find that as we scale up the model size, the model performance on
FactCheckQA improves from near-random to up to 80% balanced accuracy in
aligning with trusted sources.",None,-1
Revisiting Machine Translation for Cross-lingual Classification,0.917145,"Machine Translation (MT) has been widely used for cross-lingual
classification, either by translating the test set into English and running
inference with a monolingual model (translate-test), or translating the
training set into the target languages and finetuning a multilingual model
(translate-train). However, most research in the area focuses on the
multilingual models rather than the MT component. We show that, by using a
stronger MT system and mitigating the mismatch between training on original
text and running inference on machine translated text, translate-test can do
substantially better than previously assumed. The optimal approach, however, is
highly task dependent, as we identify various sources of cross-lingual transfer
gap that affect different tasks and approaches differently. Our work calls into
question the dominance of multilingual models for cross-lingual classification,
and prompts to pay more attention to MT-based baselines.",None,-1
Applying HCAI in developing effective human-AI teaming: A perspective from human-AI joint cognitive systems,0.282023,"Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the ""teaming"" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming",None,-1
Focus on Your Target: A Dual Teacher-Student Framework for Domain-adaptive Semantic Segmentation,0.2125,"We study unsupervised domain adaptation (UDA) for semantic segmentation.
Currently, a popular UDA framework lies in self-training which endows the model
with two-fold abilities: (i) learning reliable semantics from the labeled
images in the source domain, and (ii) adapting to the target domain via
generating pseudo labels on the unlabeled images. We find that, by
decreasing/increasing the proportion of training samples from the target
domain, the 'learning ability' is strengthened/weakened while the 'adapting
ability' goes in the opposite direction, implying a conflict between these two
abilities, especially for a single model. To alleviate the issue, we propose a
novel dual teacher-student (DTS) framework and equip it with a bidirectional
learning strategy. By increasing the proportion of target-domain data, the
second teacher-student model learns to 'Focus on Your Target' while the first
model is not affected. DTS is easily plugged into existing self-training
approaches. In a standard UDA scenario (training on synthetic, labeled data and
real, unlabeled data), DTS shows consistent gains over the baselines and sets
new state-of-the-art results of 76.5\% and 75.1\% mIoUs on
GTAv$\rightarrow$Cityscapes and SYNTHIA$\rightarrow$Cityscapes, respectively.",None,-1
Cooperative Open-ended Learning Framework for Zero-shot Coordination,0.735513,"Zero-shot coordination in cooperative artificial intelligence (AI) remains a
significant challenge, which means effectively coordinating with a wide range
of unseen partners. Previous algorithms have attempted to address this
challenge by optimizing fixed objectives within a population to improve
strategy or behaviour diversity. However, these approaches can result in a loss
of learning and an inability to cooperate with certain strategies within the
population, known as cooperative incompatibility. To address this issue, we
propose the Cooperative Open-ended LEarning (COLE) framework, which constructs
open-ended objectives in cooperative games with two players from the
perspective of graph theory to assess and identify the cooperative ability of
each strategy. We further specify the framework and propose a practical
algorithm that leverages knowledge from game theory and graph theory.
Furthermore, an analysis of the learning process of the algorithm shows that it
can efficiently overcome cooperative incompatibility. The experimental results
in the Overcooked game environment demonstrate that our method outperforms
current state-of-the-art methods when coordinating with different-level
partners. Our demo is available at https://sites.google.com/view/cole-2023.",None,-1
"Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges",0.577713,"Multi-agent reinforcement learning (MARL) is a widely used Artificial
Intelligence (AI) technique. However, current studies and applications need to
address its scalability, non-stationarity, and trustworthiness. This paper aims
to review methods and applications and point out research trends and visionary
prospects for the next decade. First, this paper summarizes the basic methods
and application scenarios of MARL. Second, this paper outlines the
corresponding research methods and their limitations on safety, robustness,
generalization, and ethical constraints that need to be addressed in the
practical applications of MARL. In particular, we believe that trustworthy MARL
will become a hot research topic in the next decade. In addition, we suggest
that considering human interaction is essential for the practical application
of MARL in various societies. Therefore, this paper also analyzes the
challenges while MARL is applied to human-machine interaction.",None,-1
Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO),0.592809,"This paper introduces the Hamilton-Jacobi-Bellman Proximal Policy
Optimization (HJBPPO) algorithm into reinforcement learning. The
Hamilton-Jacobi-Bellman (HJB) equation is used in control theory to evaluate
the optimality of the value function. Our work combines the HJB equation with
reinforcement learning in continuous state and action spaces to improve the
training of the value network. We treat the value network as a Physics-Informed
Neural Network (PINN) to solve for the HJB equation by computing its
derivatives with respect to its inputs exactly. The Proximal Policy
Optimization (PPO)-Clipped algorithm is improvised with this implementation as
it uses a value network to compute the objective function for its policy
network. The HJBPPO algorithm shows an improved performance compared to PPO on
the MuJoCo environments.",None,-1
NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images,0.896954,"We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.",None,-1
Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages,0.495302,"One of the challenges with finetuning pretrained language models (PLMs) is
that their tokenizer is optimized for the language(s) it was pretrained on, but
brittle when it comes to previously unseen variations in the data. This can for
instance be observed when finetuning PLMs on one language and evaluating them
on data in a closely related language variety with no standardized orthography.
Despite the high linguistic similarity, tokenization no longer corresponds to
meaningful representations of the target data, leading to low performance in,
e.g., part-of-speech tagging.
  In this work, we finetune PLMs on seven languages from three different
families and analyze their zero-shot performance on closely related,
non-standardized varieties. We consider different measures for the divergence
in the tokenization of the source and target data, and the way they can be
adjusted by manipulating the tokenization during the finetuning step. Overall,
we find that the similarity between the percentage of words that get split into
subwords in the source and target data (the split word ratio difference) is the
strongest predictor for model performance on target data.",None,-1
SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,0.858947,"In this study, we applied the PointRend (Point-based Rendering) method to
semiconductor defect segmentation. PointRend is an iterative segmentation
algorithm inspired by image rendering in computer graphics, a new image
segmentation method that can generate high-resolution segmentation masks. It
can also be flexibly integrated into common instance segmentation
meta-architecture such as Mask-RCNN and semantic meta-architecture such as FCN.
We implemented a model, termed as SEMI-PointRend, to generate precise
segmentation masks by applying the PointRend neural network module. In this
paper, we focus on comparing the defect segmentation predictions of
SEMI-PointRend and Mask-RCNN for various defect types (line-collapse, single
bridge, thin bridge, multi bridge non-horizontal). We show that SEMI-PointRend
can outperforms Mask R-CNN by up to 18.8% in terms of segmentation mean average
precision.",None,-1
Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation,0.051823,"Added toxicity in the context of translation refers to the fact of producing
a translation output with more toxicity than there exists in the input. In this
paper, we present MinTox which is a novel pipeline to identify added toxicity
and mitigate this issue which works at inference time. MinTox uses a toxicity
detection classifier which is multimodal (speech and text) and works in
languages at scale. The mitigation method is applied to languages at scale and
directly in text outputs. MinTox is applied to SEAMLESSM4T, which is the latest
multimodal and massively multilingual machine translation system. For this
system, MinTox achieves significant added toxicity mitigation across domains,
modalities and language directions. MinTox manages to approximately filter out
from 25% to 95% of added toxicity (depending on the modality and domain) while
keeping translation quality.",None,-1
Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting,0.999317,"We explore the ability of large language models (LLMs) to act as speech
recognition post-processors that perform rescoring and error correction. Our
first focus is on instruction prompting to let LLMs perform these task without
fine-tuning, for which we evaluate different prompting schemes, both zero- and
few-shot in-context learning, and a novel task activation prompting method that
combines causal instructions and demonstration to increase its context windows.
Next, we show that rescoring only by in-context learning with frozen LLMs
achieves results that are competitive with rescoring by domain-tuned LMs, using
a pretrained first-pass recognition system and rescoring output on two
out-of-domain tasks (ATIS and WSJ). By combining prompting techniques with
fine-tuning we achieve error rates below the N-best oracle level, showcasing
the generalization power of the LLMs.",None,-1
DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation,0.198201,"While Diffusion Generative Models have achieved great success on image
generation tasks, how to efficiently and effectively incorporate them into
speech generation especially translation tasks remains a non-trivial problem.
Specifically, due to the low information density of speech data, the
transformed discrete speech unit sequence is much longer than the corresponding
text transcription, posing significant challenges to existing auto-regressive
models. Furthermore, it is not optimal to brutally apply discrete diffusion on
the speech unit sequence while disregarding the continuous space structure,
which will degrade the generation performance significantly. In this paper, we
propose a novel diffusion model by applying the diffusion forward process in
the \textit{continuous} speech representation space, while employing the
diffusion backward process in the \textit{discrete} speech unit space. In this
way, we preserve the semantic structure of the continuous speech representation
space in the diffusion process and integrate the continuous and discrete
diffusion models. We conduct extensive experiments on the textless direct
speech-to-speech translation task, where the proposed method achieves
comparable results to the computationally intensive auto-regressive baselines
(500 steps on average) with significantly fewer decoding steps (50 steps).",None,-1
IASCAR: Incremental Answer Set Counting by Anytime Refinement,0.102532,"Answer set programming (ASP) is a popular declarative programming paradigm
with various applications. Programs can easily have many answer sets that
cannot be enumerated in practice, but counting still allows quantifying
solution spaces. If one counts under assumptions on literals, one obtains a
tool to comprehend parts of the solution space, so-called answer set
navigation. However, navigating through parts of the solution space requires
counting many times, which is expensive in theory. Knowledge compilation
compiles instances into representations on which counting works in polynomial
time. However, these techniques exist only for CNF formulas, and compiling ASP
programs into CNF formulas can introduce an exponential overhead. This paper
introduces a technique to iteratively count answer sets under assumptions on
knowledge compilations of CNFs that encode supported models. Our anytime
technique uses the inclusion-exclusion principle to improve bounds by over- and
undercounting systematically. In a preliminary empirical analysis, we
demonstrate promising results. After compiling the input (offline phase), our
approach quickly (re)counts.",None,-1
Dynamic Planning with a LLM,0.989497,"While Large Language Models (LLMs) can solve many NLP tasks in zero-shot
settings, applications involving embodied agents remain problematic. In
particular, complex plans that require multi-step reasoning become difficult
and too costly as the context window grows. Planning requires understanding the
likely effects of one's actions and identifying whether the current environment
satisfies the goal state. While symbolic planners find optimal solutions
quickly, they require a complete and accurate representation of the planning
problem, severely limiting their use in practical scenarios. In contrast,
modern LLMs cope with noisy observations and high levels of uncertainty when
reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a
neuro-symbolic framework where an LLM works hand-in-hand with a traditional
planner to solve an embodied task. Given action-descriptions, LLM-DP solves
Alfworld faster and more efficiently than a naive LLM ReAct baseline.",None,-1
Revisiting Large Language Models as Zero-shot Relation Extractors,0.882441,"Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.",None,-1
DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models,0.196606,"Hand mesh reconstruction from the monocular image is a challenging task due
to its depth ambiguity and severe occlusion, there remains a non-unique mapping
between the monocular image and hand mesh. To address this, we develop
DiffHand, the first diffusion-based framework that approaches hand mesh
reconstruction as a denoising diffusion process. Our one-stage pipeline
utilizes noise to model the uncertainty distribution of the intermediate hand
mesh in a forward process. We reformulate the denoising diffusion process to
gradually refine noisy hand mesh and then select mesh with the highest
probability of being correct based on the image itself, rather than relying on
2D joints extracted beforehand. To better model the connectivity of hand
vertices, we design a novel network module called the cross-modality decoder.
Extensive experiments on the popular benchmarks demonstrate that our method
outperforms the state-of-the-art hand mesh reconstruction approaches by
achieving 5.8mm PA-MPJPE on the Freihand test set, 4.98mm PA-MPJPE on the
DexYCB test set.",None,-1
StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding,0.999778,"Analogy-making between narratives is crucial for human reasoning. In this
paper, we evaluate the ability to identify and generate analogies by
constructing a first-of-its-kind large-scale story-level analogy corpus,
\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with
human annotations on two similarities from the extended Structure-Mapping
Theory. We design a set of tests on \textsc{StoryAnalogy}, presenting the first
evaluation of story-level analogy identification and generation. Interestingly,
we find that the analogy identification tasks are incredibly difficult not only
for sentence embedding models but also for the recent large language models
(LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around
30% accuracy in multiple-choice questions (compared to over 85% accuracy for
humans). Furthermore, we observe that the data in \textsc{StoryAnalogy} can
improve the quality of analogy generation in LLMs, where a fine-tuned
FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",None,-1
Mobile User Interface Element Detection Via Adaptively Prompt Tuning,0.908859,"Recent object detection approaches rely on pretrained vision-language models
for image-text alignment. However, they fail to detect the Mobile User
Interface (MUI) element since it contains additional OCR information, which
describes its content and function but is often ignored. In this paper, we
develop a new MUI element detection dataset named MUI-zh and propose an
Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR
information. APT is a lightweight and effective module to jointly optimize
category prompts across different modalities. For every element, APT uniformly
encodes its visual features and OCR descriptions to dynamically adjust the
representation of frozen category prompts. We evaluate the effectiveness of our
plug-and-play APT upon several existing CLIP-based detectors for both standard
and open-vocabulary MUI element detection. Extensive experiments show that our
method achieves considerable improvements on two datasets. The datasets is
available at \url{github.com/antmachineintelligence/MUI-zh}.",None,-1
Long Range Pooling for 3D Large-Scale Scene Understanding,0.083368,"Inspired by the success of recent vision transformers and large kernel design
in convolutional neural networks (CNNs), in this paper, we analyze and explore
essential reasons for their success. We claim two factors that are critical for
3D large-scale scene understanding: a larger receptive field and operations
with greater non-linearity. The former is responsible for providing long range
contexts and the latter can enhance the capacity of the network. To achieve the
above properties, we propose a simple yet effective long range pooling (LRP)
module using dilation max pooling, which provides a network with a large
adaptive receptive field. LRP has few parameters, and can be readily added to
current CNNs. Also, based on LRP, we present an entire network architecture,
LRPNet, for 3D understanding. Ablation studies are presented to support our
claims, and show that the LRP module achieves better results than large kernel
convolution yet with reduced computation, due to its nonlinearity. We also
demonstrate the superiority of LRPNet on various benchmarks: LRPNet performs
the best on ScanNet and surpasses other CNN-based methods on S3DIS and
Matterport3D. Code will be made publicly available.",None,-1
AutoSTL: Automated Spatio-Temporal Multi-Task Learning,0.785725,"Spatio-Temporal prediction plays a critical role in smart city construction.
Jointly modeling multiple spatio-temporal tasks can further promote an
intelligent city life by integrating their inseparable relationship. However,
existing studies fail to address this joint learning problem well, which
generally solve tasks individually or a fixed task combination. The challenges
lie in the tangled relation between different properties, the demand for
supporting flexible combinations of tasks and the complex spatio-temporal
dependency. To cope with the problems above, we propose an Automated
Spatio-Temporal multi-task Learning (AutoSTL) method to handle multiple
spatio-temporal tasks jointly. Firstly, we propose a scalable architecture
consisting of advanced spatio-temporal operations to exploit the complicated
dependency. Shared modules and feature fusion mechanism are incorporated to
further capture the intrinsic relationship between tasks. Furthermore, our
model automatically allocates the operations and fusion weight. Extensive
experiments on benchmark datasets verified that our model achieves
state-of-the-art performance. As we can know, AutoSTL is the first automated
spatio-temporal multi-task learning method.",None,-1
"Transcending the ""Male Code"": Implicit Masculine Biases in NLP Contexts",0.523526,"Critical scholarship has elevated the problem of gender bias in data sets
used to train virtual assistants (VAs). Most work has focused on explicit
biases in language, especially against women, girls, femme-identifying people,
and genderqueer folk; implicit associations through word embeddings; and
limited models of gender and masculinities, especially toxic masculinities,
conflation of sex and gender, and a sex/gender binary framing of the masculine
as diametric to the feminine. Yet, we must also interrogate how masculinities
are ""coded"" into language and the assumption of ""male"" as the linguistic
default: implicit masculine biases. To this end, we examined two natural
language processing (NLP) data sets. We found that when gendered language was
present, so were gender biases and especially masculine biases. Moreover, these
biases related in nuanced ways to the NLP context. We offer a new dictionary
called AVA that covers ambiguous associations between gendered language and the
language of VAs.",None,-1
Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling,0.655846,"The popularity of on-demand ride pooling is owing to the benefits offered to
customers (lower prices), taxi drivers (higher revenue), environment (lower
carbon footprint due to fewer vehicles) and aggregation companies like Uber
(higher revenue). To achieve these benefits, two key interlinked challenges
have to be solved effectively: (a) pricing -- setting prices to customer
requests for taxis; and (b) matching -- assignment of customers (that accepted
the prices) to taxis/cars. Traditionally, both these challenges have been
studied individually and using myopic approaches (considering only current
requests), without considering the impact of current matching on addressing
future requests. In this paper, we develop a novel framework that handles the
pricing and matching problems together, while also considering the future
impact of the pricing and matching decisions. In our experimental results on a
real-world taxi dataset, we demonstrate that our framework can significantly
improve revenue (up to 17% and on average 6.4%) in a sustainable manner by
reducing the number of vehicles (up to 14% and on average 10.6%) required to
obtain a given fixed revenue and the overall distance travelled by vehicles (up
to 11.1% and on average 3.7%). That is to say, we are able to provide an ideal
win-win scenario for all stakeholders (customers, drivers, aggregator,
environment) involved by obtaining higher revenue for customers, drivers,
aggregator (ride pooling company) while being good for the environment (due to
fewer number of vehicles on the road and lesser fuel consumed).",None,-1
Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series,0.564197,"Learning accurate predictive models of real-world dynamic phenomena (e.g.,
climate, biological) remains a challenging task. One key issue is that the data
generated by both natural and artificial processes often comprise time series
that are irregularly sampled and/or contain missing observations. In this work,
we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for
continuous-time modeling of time series through discrete-time observations.
NCDSSM employs auxiliary variables to disentangle recognition from dynamics,
thus requiring amortized inference only for the auxiliary variables. Leveraging
techniques from continuous-discrete filtering theory, we demonstrate how to
perform accurate Bayesian inference for the dynamic states. We propose three
flexible parameterizations of the latent dynamics and an efficient training
objective that marginalizes the dynamic states during inference. Empirical
results on multiple benchmark datasets across various domains show improved
imputation and forecasting performance of NCDSSM over existing models.",None,-1
Online Gesture Recognition using Transformer and Natural Language Processing,0.261288,"The Transformer architecture is shown to provide a powerful machine
transduction framework for online handwritten gestures corresponding to glyph
strokes of natural language sentences. The attention mechanism is successfully
used to create latent representations of an end-to-end encoder-decoder model,
solving multi-level segmentation while also learning some language features and
syntax rules. The additional use of a large decoding space with some learned
Byte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and
syntax rules. The encoder stack was directly fed with spatio-temporal data
tokens potentially forming an infinitely large input vocabulary, an approach
that finds applications beyond that of this work. Encoder transfer learning
capabilities is also demonstrated on several languages resulting in faster
optimisation and shared parameters. A new supervised dataset of online
handwriting gestures suitable for generic handwriting recognition tasks was
used to successfully train a small transformer model to an average normalised
Levenshtein accuracy of 96% on English or German sentences and 94% in French.",None,-1
Leveraging GPT-4 for Automatic Translation Post-Editing,0.947425,"While Neural Machine Translation (NMT) represents the leading approach to
Machine Translation (MT), the outputs of NMT models still require translation
post-editing to rectify errors and enhance quality under critical settings. In
this work, we formalize the task of direct translation post-editing with Large
Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit
NMT outputs across several language pairs. Our results demonstrate that GPT-4
is adept at translation post-editing, producing meaningful and trustworthy
edits to translations that help improve its general quality as well as remove
different classes of major errors in translations. In particular, human
evaluations on assessing edit trustworthiness show that GPT-4 exhibits a large
improvement over the prior state-of-the-art LLM. Notably, we improve upon
state-of-the-art performance on WMT-22 English-Chinese, English-German,
Chinese-English and German-English language pairs using GPT-4 based
post-editing, as evaluated by state-of-the-art MT quality metrics. However, we
also show that GPT-4 could produce hallucinated edits, thereby urging caution
in its use as an expert translation post-editor.",None,-1
Spatial Knowledge-Infused Hierarchical Learning: An Application in Flood Mapping on Earth Imagery,0.522586,"Deep learning for Earth imagery plays an increasingly important role in
geoscience applications such as agriculture, ecology, and natural disaster
management. Still, progress is often hindered by the limited training labels.
Given Earth imagery with limited training labels, a base deep neural network
model, and a spatial knowledge base with label constraints, our problem is to
infer the full labels while training the neural network. The problem is
challenging due to the sparse and noisy input labels, spatial uncertainty
within the label inference process, and high computational costs associated
with a large number of sample locations. Existing works on neuro-symbolic
models focus on integrating symbolic logic into neural networks (e.g., loss
function, model architecture, and training label augmentation), but these
methods do not fully address the challenges of spatial data (e.g., spatial
uncertainty, the trade-off between spatial granularity and computational
costs). To bridge this gap, we propose a novel Spatial Knowledge-Infused
Hierarchical Learning (SKI-HL) framework that iteratively infers sample labels
within a multi-resolution hierarchy. Our framework consists of a module to
selectively infer labels in different resolutions based on spatial uncertainty
and a module to train neural network parameters with uncertainty-aware
multi-instance learning. Extensive experiments on real-world flood mapping
datasets show that the proposed model outperforms several baseline methods. The
code is available at \url{https://github.com/ZelinXu2000/SKI-HL}.",None,-1
Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning,0.637571,"To advance argumentative stance prediction as a multimodal problem, the First
Shared Task in Multimodal Argument Mining hosted stance prediction in crucial
social topics of gun control and abortion. Our exploratory study attempts to
evaluate the necessity of images for stance prediction in tweets and compare
out-of-the-box text-based large-language models (LLM) in few-shot settings
against fine-tuned unimodal and multimodal models. Our work suggests an
ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms
both the multimodal (0.677 F1-score) and text-based few-shot prediction using a
recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in
performance, our findings suggest that the multimodal models tend to perform
better when image content is summarized as natural language over their native
pixel structure and, using in-context examples improves few-shot performance of
LLMs.",None,-1
Less is More for Long Document Summary Evaluation by LLMs,0.804249,"Large Language Models (LLMs) have shown promising performance in summary
evaluation tasks, yet they face challenges such as high computational costs and
the Lost-in-the-Middle problem where important information in the middle of
long documents is often overlooked. To address these issues, this paper
introduces a novel approach, Extract-then-Evaluate, which involves extracting
key sentences from a long source document and then evaluating the summary by
prompting LLMs. The results reveal that the proposed method not only
significantly reduces evaluation costs but also exhibits a higher correlation
with human evaluations. Furthermore, we provide practical recommendations for
optimal document length and sentence extraction methods, contributing to the
development of cost-effective yet more accurate methods for LLM-based text
generation evaluation.",None,-1
Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet,0.737485,"The rapid adoption of generative Artificial Intelligence (AI) tools that can
generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have
put the societal impacts of these technologies at the center of public debate.
These tools are possible due to the massive amount of data (text and images)
that is publicly available through the Internet. At the same time, these
generative AI tools become content creators that are already contributing to
the data that is available to train future models. Therefore, future versions
of generative AI tools will be trained with a mix of human-created and
AI-generated content, causing a potential feedback loop between generative AI
and public data repositories. This interaction raises many questions: how will
future versions of generative AI tools behave when trained on a mixture of real
and AI generated data? Will they evolve and improve with the new data sets or
on the contrary will they degrade? Will evolution introduce biases or reduce
diversity in subsequent generations of generative AI tools? What are the
societal implications of the possible degradation of these models? Can we
mitigate the effects of this feedback loop? In this document, we explore the
effect of this interaction and report some initial results using simple
diffusion models trained with various image datasets. Our results show that the
quality and diversity of the generated images can degrade over time suggesting
that incorporating AI-created data can have undesired effects on future
versions of generative models.",None,-1
Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,0.410679,"Societal biases present in pre-trained large language models are a critical
issue as these models have been shown to propagate biases in countless
downstream applications, rendering them unfair towards specific groups of
people. Since large-scale retraining of these models from scratch is both time
and compute-expensive, a variety of approaches have been previously proposed
that de-bias a pre-trained model. While the majority of current
state-of-the-art debiasing methods focus on changes to the training regime, in
this paper, we propose data intervention strategies as a powerful yet simple
technique to reduce gender bias in pre-trained models. Specifically, we
empirically show that by fine-tuning a pre-trained model on only 10 de-biased
(intervened) training examples, the tendency to favor any gender is
significantly reduced. Since our proposed method only needs a few training
examples, our few-shot debiasing approach is highly feasible and practical.
Through extensive experimentation, we show that our debiasing technique
performs better than competitive state-of-the-art baselines with minimal loss
in language modeling ability.",None,-1
"HealthEdge: A Machine Learning-Based Smart Healthcare Framework for Prediction of Type 2 Diabetes in an Integrated IoT, Edge, and Cloud Computing System",0.725276,"Diabetes Mellitus has no permanent cure to date and is one of the leading
causes of death globally. The alarming increase in diabetes calls for the need
to take precautionary measures to avoid/predict the occurrence of diabetes.
This paper proposes HealthEdge, a machine learning-based smart healthcare
framework for type 2 diabetes prediction in an integrated IoT-edge-cloud
computing system. Numerical experiments and comparative analysis were carried
out between the two most used machine learning algorithms in the literature,
Random Forest (RF) and Logistic Regression (LR), using two real-life diabetes
datasets. The results show that RF predicts diabetes with 6% more accuracy on
average compared to LR.",None,-1
Two-stage Pipeline for Multilingual Dialect Detection,0.0497358,"Dialect Identification is a crucial task for localizing various Large
Language Models. This paper outlines our approach to the VarDial 2023 shared
task. Here we have to identify three or two dialects from three languages each
which results in a 9-way classification for Track-1 and 6-way classification
for Track-2 respectively. Our proposed approach consists of a two-stage system
and outperforms other participants' systems and previous works in this domain.
We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase
is available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).",None,-1
Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,0.275121,"In this paper, we focus on mean-field variational Bayesian Neural Networks
(BNNs) and explore the representation capacity of such BNNs by investigating
which types of concepts are less likely to be encoded by the BNN. It has been
observed and studied that a relatively small set of interactive concepts
usually emerge in the knowledge representation of a sufficiently-trained neural
network, and such concepts can faithfully explain the network output. Based on
this, our study proves that compared to standard deep neural networks (DNNs),
it is less likely for BNNs to encode complex concepts. Experiments verify our
theoretical proofs. Note that the tendency to encode less complex concepts does
not necessarily imply weak representation power, considering that complex
concepts exhibit low generalization power and high adversarial vulnerability.
The code is available at https://github.com/sjtu-xai-lab/BNN-concepts.",None,-1
MoStGAN-V: Video Generation with Temporal Motion Styles,0.700254,"Video generation remains a challenging task due to spatiotemporal complexity
and the requirement of synthesizing diverse motions with temporal consistency.
Previous works attempt to generate videos in arbitrary lengths either in an
autoregressive manner or regarding time as a continuous signal. However, they
struggle to synthesize detailed and diverse motions with temporal coherence and
tend to generate repetitive scenes after a few time steps. In this work, we
argue that a single time-agnostic latent vector of style-based generator is
insufficient to model various and temporally-consistent motions. Hence, we
introduce additional time-dependent motion styles to model diverse motion
patterns. In addition, a Motion Style Attention modulation mechanism, dubbed as
MoStAtt, is proposed to augment frames with vivid dynamics for each specific
scale (i.e., layer), which assigns attention score for each motion style w.r.t
deconvolution filter weights in the target synthesis layer and softly attends
different motion styles for weight modulation. Experimental results show our
model achieves state-of-the-art performance on four unconditional $256^2$ video
synthesis benchmarks trained with only 3 frames per clip and produces better
qualitative results with respect to dynamic motions. Code and videos have been
made available at https://github.com/xiaoqian-shen/MoStGAN-V.",None,-1
Atmospheric Turbulence Correction via Variational Deep Diffusion,0.612773,"Atmospheric Turbulence (AT) correction is a challenging restoration task as
it consists of two distortions: geometric distortion and spatially variant
blur. Diffusion models have shown impressive accomplishments in photo-realistic
image synthesis and beyond. In this paper, we propose a novel deep conditional
diffusion model under a variational inference framework to solve the AT
correction problem. We use this framework to improve performance by learning
latent prior information from the input and degradation processes. We use the
learned information to further condition the diffusion model. Experiments are
conducted in a comprehensive synthetic AT dataset. We show that the proposed
framework achieves good quantitative and qualitative results.",None,-1
Exploring Large Language Models for Human Mobility Prediction under Public Events,0.773698,"Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.",None,-1
OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios,0.308986,"Modern approaches for vision-centric environment perception for autonomous
navigation make extensive use of self-supervised monocular depth estimation
algorithms that output disparity maps. However, when this disparity map is
projected onto 3D space, the errors in disparity are magnified, resulting in a
depth estimation error that increases quadratically as the distance from the
camera increases. Though Light Detection and Ranging (LiDAR) can solve this
issue, it is expensive and not feasible for many applications. To address the
challenge of accurate ranging with low-cost sensors, we propose, OCTraN, a
transformer architecture that uses iterative-attention to convert 2D image
features into 3D occupancy features and makes use of convolution and transpose
convolution to efficiently operate on spatial information. We also develop a
self-supervised training pipeline to generalize the model to any scene by
eliminating the need for LiDAR ground truth by substituting it with
pseudo-ground truth labels obtained from boosted monocular depth estimation.",None,-1
Have it your way: Individualized Privacy Assignment for DP-SGD,0.714111,"When training a machine learning model with differential privacy, one sets a
privacy budget. This budget represents a maximal privacy violation that any
user is willing to face by contributing their data to the training set. We
argue that this approach is limited because different users may have different
privacy expectations. Thus, setting a uniform privacy budget across all points
may be overly conservative for some users or, conversely, not sufficiently
protective for others. In this paper, we capture these preferences through
individualized privacy budgets. To demonstrate their practicality, we introduce
a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which
supports such individualized budgets. DP-SGD is the canonical approach to
training models with differential privacy. We modify its data sampling and
gradient noising mechanisms to arrive at our approach, which we call
Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees
tailored to the preferences of individual users and their data points, we find
it empirically improves privacy-utility trade-offs.",None,-1
Eye Disease Classification Using Deep Learning Techniques,0.5359,"Eye is the essential sense organ for vision function. Due to the fact that
certain eye disorders might result in vision loss, it is essential to diagnose
and treat eye diseases early on. By identifying common eye illnesses and
performing an eye check, eye care providers can safeguard patients against
vision loss or blindness. Convolutional neural networks (CNN) and transfer
learning were employed in this study to discriminate between a normal eye and
one with diabetic retinopathy, cataract, or glaucoma disease. Using transfer
learning for multi-class classification, high accuracy was achieved at 94%
while the traditional CNN achieved 84% rate.",None,-1
Assessing the potential of AI-assisted pragmatic annotation: The case of apologies,0.882681,"Certain forms of linguistic annotation, like part of speech and semantic
tagging, can be automated with high accuracy. However, manual annotation is
still necessary for complex pragmatic and discursive features that lack a
direct mapping to lexical forms. This manual process is time-consuming and
error-prone, limiting the scalability of function-to-form approaches in corpus
linguistics. To address this, our study explores automating pragma-discursive
corpus annotation using large language models (LLMs). We compare ChatGPT, the
Bing chatbot, and a human coder in annotating apology components in English
based on the local grammar framework. We find that the Bing chatbot
outperformed ChatGPT, with accuracy approaching that of a human coder. These
results suggest that AI can be successfully deployed to aid pragma-discursive
corpus annotation, making the process more efficient and scalable. Keywords:
linguistic annotation, function-to-form approaches, large language models,
local grammar analysis, Bing chatbot, ChatGPT",None,-1
Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty,0.888327,"Open Information Extraction (OIE) task aims at extracting structured facts
from unstructured text, typically in the form of (subject, relation, object)
triples. Despite the potential of large language models (LLMs) like ChatGPT as
a general task solver, they lag behind state-of-the-art (supervised) methods in
OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant
context from relevant relations and generate structured output due to the
restrictions on fine-tuning the model. Second, LLMs generates responses
autoregressively based on probability, which makes the predicted relations lack
confidence. In this paper, we assess the capabilities of LLMs in improving the
OIE task. Particularly, we propose various in-context learning strategies to
enhance LLM's instruction-following ability and a demonstration uncertainty
quantification module to enhance the confidence of the generated relations. Our
experiments on three OIE benchmark datasets show that our approach holds its
own against established supervised methods, both quantitatively and
qualitatively.",None,-1
C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT,0.100268,"Large language models (LLMs), such as ChatGPT, have demonstrated outstanding
performance in various fields, particularly in natural language understanding
and generation tasks. In complex application scenarios, users tend to engage in
multi-turn conversations with ChatGPT to keep contextual information and obtain
comprehensive responses. However, human forgetting and model contextual
forgetting remain prominent issues in multi-turn conversation scenarios, which
challenge the users' conversation comprehension and contextual continuity for
ChatGPT. To address these challenges, we propose an interactive conversation
visualization system called C5, which includes Global View, Topic View, and
Context-associated Q\&A View. The Global View uses the GitLog diagram metaphor
to represent the conversation structure, presenting the trend of conversation
evolution and supporting the exploration of locally salient features. The Topic
View is designed to display all the question and answer nodes and their
relationships within a topic using the structure of a knowledge graph, thereby
display the relevance and evolution of conversations. The Context-associated
Q\&A View consists of three linked views, which allow users to explore
individual conversations deeply while providing specific contextual information
when posing questions. The usefulness and effectiveness of C5 were evaluated
through a case study and a user study.",None,-1
Unproportional mosaicing,0.0632094,"Data shift is a gap between data distribution used for training and data
distribution encountered in the real-world. Data augmentations help narrow the
gap by generating new data samples, increasing data variability, and data space
coverage. We present a new data augmentation: Unproportional mosaicing
(Unprop). Our augmentation randomly splits an image into various-sized blocks
and swaps its content (pixels) while maintaining block sizes. Our method
achieves a lower error rate when combined with other state-of-the-art
augmentations.",None,-1
Exploiting Language Models as a Source of Knowledge for Cognitive Agents,0.694346,"Large language models (LLMs) provide capabilities far beyond sentence
completion, including question answering, summarization, and natural-language
inference. While many of these capabilities have potential application to
cognitive systems, our research is exploiting language models as a source of
task knowledge for cognitive agents, that is, agents realized via a cognitive
architecture. We identify challenges and opportunities for using language
models as an external knowledge source for cognitive systems and possible ways
to improve the effectiveness of knowledge extraction by integrating extraction
with cognitive architecture capabilities, highlighting with examples from our
recent work in this area.",None,-1
Dual-level Interaction for Domain Adaptive Semantic Segmentation,0.146989,"Self-training approach recently secures its position in domain adaptive
semantic segmentation, where a model is trained with target domain
pseudo-labels. Current advances have mitigated noisy pseudo-labels resulting
from the domain gap. However, they still struggle with erroneous pseudo-labels
near the boundaries of the semantic classifier. In this paper, we tackle this
issue by proposing a dual-level interaction for domain adaptation (DIDA) in
semantic segmentation. Explicitly, we encourage the different augmented views
of the same pixel to have not only similar class prediction (semantic-level)
but also akin similarity relationship with respect to other pixels
(instance-level). As it's impossible to keep features of all pixel instances
for a dataset, we, therefore, maintain a labeled instance bank with dynamic
updating strategies to selectively store the informative features of instances.
Further, DIDA performs cross-level interaction with scattering and gathering
techniques to regenerate more reliable pseudo-labels. Our method outperforms
the state-of-the-art by a notable margin, especially on confusing and
long-tailed classes. Code is available at
\href{https://github.com/RainJamesY/DIDA}",None,-1
Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity,0.586106,"Amidst the sharp rise in the evaluation of large language models (LLMs) on
various tasks, we find that semantic textual similarity (STS) has been
under-explored. In this study, we show that STS can be cast as a text
generation problem while maintaining strong performance on multiple STS
benchmarks. Additionally, we show generative LLMs significantly outperform
existing encoder-based STS models when characterizing the semantic similarity
between two texts with complex semantic relationships dependent on world
knowledge. We validate this claim by evaluating both generative LLMs and
existing encoder-based STS models on three newly collected STS challenge sets
which require world knowledge in the domains of Health, Politics, and Sports.
All newly collected data is sourced from social media content posted after May
2023 to ensure the performance of closed-source models like ChatGPT cannot be
credited to memorization. Our results show that, on average, generative LLMs
outperform the best encoder-only baselines by an average of 22.3% on STS tasks
requiring world knowledge. Our results suggest generative language models with
STS-specific prompting strategies achieve state-of-the-art performance in
complex, domain-specific STS tasks.",None,-1
Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems,0.748593,"In this article, a benchmark for real-world bin packing problems is proposed.
This dataset consists of 12 instances of varying levels of complexity regarding
size (with the number of packages ranging from 38 to 53) and user-defined
requirements. In fact, several real-world-oriented restrictions were taken into
account to build these instances: i) item and bin dimensions, ii) weight
restrictions, iii) affinities among package categories iv) preferences for
package ordering and v) load balancing. Besides the data, we also offer an own
developed Python script for the dataset generation, coined Q4RealBPP-DataGen.
The benchmark was initially proposed to evaluate the performance of quantum
solvers. Therefore, the characteristics of this set of instances were designed
according to the current limitations of quantum devices. Additionally, the
dataset generator is included to allow the construction of general-purpose
benchmarks. The data introduced in this article provides a baseline that will
encourage quantum computing researchers to work on real-world bin packing
problems.",None,-1
Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,0.422977,"In many applications of advanced robotic manipulation, six degrees of freedom
(6DoF) object pose estimates are continuously required. In this work, we
develop a multi-modality tracker that fuses information from visual appearance
and geometry to estimate object poses. The algorithm extends our previous
method ICG, which uses geometry, to additionally consider surface appearance.
In general, object surfaces contain local characteristics from text, graphics,
and patterns, as well as global differences from distinct materials and colors.
To incorporate this visual information, two modalities are developed. For local
characteristics, keypoint features are used to minimize distances between
points from keyframes and the current image. For global differences, a novel
region approach is developed that considers multiple regions on the object
surface. In addition, it allows the modeling of external geometries.
Experiments on the YCB-Video and OPT datasets demonstrate that our approach
ICG+ performs best on both datasets, outperforming both conventional and deep
learning-based methods. At the same time, the algorithm is highly efficient and
runs at more than 300 Hz. The source code of our tracker is publicly available.",None,-1
PersonalTailor: Personalizing 2D Pattern Design from 3D Garment Point Clouds,0.413855,"Garment pattern design aims to convert a 3D garment to the corresponding 2D
panels and their sewing structure. Existing methods rely either on template
fitting with heuristics and prior assumptions, or on model learning with
complicated shape parameterization. Importantly, both approaches do not allow
for personalization of the output garment, which today has increasing demands.
To fill this demand, we introduce PersonalTailor: a personalized 2D pattern
design method, where the user can input specific constraints or demands (in
language or sketch) for personal 2D panel fabrication from 3D point clouds.
PersonalTailor first learns a multi-modal panel embeddings based on
unsupervised cross-modal association and attentive fusion. It then predicts a
binary panel masks individually using a transformer encoder-decoder framework.
Extensive experiments show that our PersonalTailor excels on both personalized
and standard pattern fabrication tasks.",None,-1
The Capability of Large Language Models to Measure Psychiatric Functioning,0.994716,"The current work investigates the capability of Large language models (LLMs)
that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)
to predict psychiatric functioning from patient interviews and clinical
descriptions without being trained to do so. To assess this, n = 145 depression
and n =115 PTSD assessments and n = 46 clinical case studies across high
prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma
and stress, Addictive disorders) were analyzed using prompts to extract
estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is
capable of assessing psychiatric functioning across a range of psychiatric
conditions with the strongest performance being the prediction of depression
scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which
were statistically indistinguishable from human clinical raters t(1,144) =
1.20; p = 0.23. Results show the potential for general clinical language models
to flexibly predict psychiatric risk based on free descriptions of functioning
from both patients and clinicians.",None,-1
Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs,0.593364,"The ever-increasing large language models (LLMs), though opening a potential
path for the upcoming artificial general intelligence, sadly drops a daunting
obstacle on the way towards their on-device deployment. As one of the most
well-established pre-LLMs approaches in reducing model complexity, network
pruning appears to lag behind in the era of LLMs, due mostly to its costly
fine-tuning (or re-training) necessity under the massive volumes of model
parameter and training data. To close this industry-academia gap, we introduce
Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that
slightly updates sparse LLMs without the expensive backpropagation and any
weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the
reconstruction error between the dense and sparse LLMs, in the fashion of
performing iterative weight pruning-and-growing on top of sparse LLMs. To
accomplish this purpose, DSnoT particularly takes into account the anticipated
reduction in reconstruction error for pruning and growing, as well as the
variance w.r.t. different input data for growing each weight. This practice can
be executed efficiently in linear time since its obviates the need of
backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,
Vicuna, and OPT across various benchmarks demonstrate the effectiveness of
DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity
levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by
26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights
into how to fine-tune sparse LLMs in an efficient training-free manner and open
new venues to scale the great potential of sparsity to LLMs. Codes are
available at https://github.com/zyxxmu/DSnoT.",None,-1
LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models,0.626541,"Traditional automatic speech recognition (ASR) models output lower-cased
words without punctuation marks, which reduces readability and necessitates a
subsequent text processing model to convert ASR transcripts into a proper
format. Simultaneously, the development of end-to-end ASR models capable of
predicting punctuation and capitalization presents several challenges,
primarily due to limited data availability and shortcomings in the existing
evaluation methods, such as inadequate assessment of punctuation prediction. In
this paper, we introduce a LibriSpeech-PC benchmark designed to assess the
punctuation and capitalization prediction capabilities of end-to-end ASR
models. The benchmark includes a LibriSpeech-PC dataset with restored
punctuation and capitalization, a novel evaluation metric called Punctuation
Error Rate (PER) that focuses on punctuation marks, and initial baseline
models. All code, data, and models are publicly available.",None,-1
Robust Domain Misinformation Detection via Multi-modal Feature Alignment,0.738281,"Social media misinformation harms individuals and societies and is
potentialized by fast-growing multi-modal content (i.e., texts and images),
which accounts for higher ""credibility"" than text-only news pieces. Although
existing supervised misinformation detection methods have obtained acceptable
performances in key setups, they may require large amounts of labeled data from
various events, which can be time-consuming and tedious. In turn, directly
training a model by leveraging a publicly available dataset may fail to
generalize due to domain shifts between the training data (a.k.a. source
domains) and the data from target domains. Most prior work on domain shift
focuses on a single modality (e.g., text modality) and ignores the scenario
where sufficient unlabeled target domain data may not be readily available in
an early stage. The lack of data often happens due to the dynamic propagation
trend (i.e., the number of posts related to fake news increases slowly before
catching the public attention). We propose a novel robust domain and
cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection.
It reduces the domain shift by aligning the joint distribution of textual and
visual modalities through an inter-domain alignment module and bridges the
semantic gap between both modalities through a cross-modality alignment module.
We also propose a framework that simultaneously considers application scenarios
of domain generalization (in which the target domain data is unavailable) and
domain adaptation (in which unlabeled target domain data is available).
Evaluation results on two public multi-modal misinformation detection datasets
(Pheme and Twitter Datasets) evince the superiority of the proposed model. The
formal implementation of this paper can be found in this link:
https://github.com/less-and-less-bugs/RDCM",None,-1
Emptying the Ocean with a Spoon: Should We Edit Models?,0.6024,"We call into question the recently popularized method of direct model editing
as a means of correcting factual errors in LLM generations. We contrast model
editing with three similar but distinct approaches that pursue better defined
objectives: (1) retrieval-based architectures, which decouple factual memory
from inference and linguistic capabilities embodied in LLMs; (2) concept
erasure methods, which aim at preventing systemic bias in generated text; and
(3) attribution methods, which aim at grounding generations into identified
textual sources. We argue that direct model editing cannot be trusted as a
systematic remedy for the disadvantages inherent to LLMs, and while it has
proven potential in improving model explainability, it opens risks by
reinforcing the notion that models can be trusted for factuality. We call for
cautious promotion and application of model editing as part of the LLM
deployment process, and for responsibly limiting the use cases of LLMs to those
not relying on editing as a critical component.",None,-1
A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,0.0973849,"Cracks play a crucial role in assessing the safety and durability of
manufactured buildings. However, the long and sharp topological features and
complex background of cracks make the task of crack segmentation extremely
challenging. In this paper, we propose a novel convolutional-transformer
network based on encoder-decoder architecture to solve this challenge.
Particularly, we designed a Dilated Residual Block (DRB) and a Boundary
Awareness Module (BAM). The DRB pays attention to the local detail of cracks
and adjusts the feature dimension for other blocks as needed. And the BAM
learns the boundary features from the dilated crack label. Furthermore, the DRB
is combined with a lightweight transformer that captures global information to
serve as an effective encoder. Experimental results show that the proposed
network performs better than state-of-the-art algorithms on two typical
datasets. Datasets, code, and trained models are available for research at
https://github.com/HqiTao/CT-crackseg.",None,-1
Semantic-Preserving Augmentation for Robust Image-Text Retrieval,0.0232218,"Image text retrieval is a task to search for the proper textual descriptions
of the visual world and vice versa. One challenge of this task is the
vulnerability to input image and text corruptions. Such corruptions are often
unobserved during the training, and degrade the retrieval model decision
quality substantially. In this paper, we propose a novel image text retrieval
technique, referred to as robust visual semantic embedding (RVSE), which
consists of novel image-based and text-based augmentation techniques called
semantic preserving augmentation for image (SPAugI) and text (SPAugT). Since
SPAugI and SPAugT change the original data in a way that its semantic
information is preserved, we enforce the feature extractors to generate
semantic aware embedding vectors regardless of the corruption, improving the
model robustness significantly. From extensive experiments using benchmark
datasets, we show that RVSE outperforms conventional retrieval schemes in terms
of image-text retrieval performance.",None,-1
Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations,0.91337,"Causal abstraction is a promising theoretical framework for explainable
artificial intelligence that defines when an interpretable high-level causal
model is a faithful simplification of a low-level deep learning system.
However, existing causal abstraction methods have two major limitations: they
require a brute-force search over alignments between the high-level model and
the low-level one, and they presuppose that variables in the high-level model
will align with disjoint sets of neurons in the low-level one. In this paper,
we present distributed alignment search (DAS), which overcomes these
limitations. In DAS, we find the alignment between high-level and low-level
models using gradient descent rather than conducting a brute-force search, and
we allow individual neurons to play multiple distinct roles by analyzing
representations in non-standard bases-distributed representations. Our
experiments show that DAS can discover internal structure that prior approaches
miss. Overall, DAS removes previous obstacles to conducting causal abstraction
analyses and allows us to find conceptual structure in trained neural nets.",None,-1
Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation,0.198025,"NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS)
models flounder when sufficient amounts of parallel data is not available for
fine-tuning. This specifically holds for languages missing/under-represented in
these models. The problem gets aggravated when the data comes from different
domains. In this paper, we show that intermediate-task fine-tuning (ITFT) of
PMSS models is extremely beneficial for domain-specific NMT, especially when
target domain data is limited/unavailable and the considered languages are
missing or under-represented in the PMSS model. We quantify the domain-specific
results variations using a domain-divergence test, and show that ITFT can
mitigate the impact of domain divergence to some extent.",None,-1
Semi-Oblivious Chase Termination for Linear Existential Rules: An Experimental Study,0.0389661,"The chase procedure is a fundamental algorithmic tool in databases that
allows us to reason with constraints, such as existential rules, with a
plethora of applications. It takes as input a database and a set of
constraints, and iteratively completes the database as dictated by the
constraints. A key challenge, though, is the fact that it may not terminate,
which leads to the problem of checking whether it terminates given a database
and a set of constraints. In this work, we focus on the semi-oblivious version
of the chase, which is well-suited for practical implementations, and linear
existential rules, a central class of constraints with several applications. In
this setting, there is a mature body of theoretical work that provides
syntactic characterizations of when the chase terminates, algorithms for
checking chase termination, precise complexity results, and worst-case optimal
bounds on the size of the result of the chase (whenever is finite). Our main
objective is to experimentally evaluate the existing chase termination
algorithms with the aim of understanding which input parameters affect their
performance, clarifying whether they can be used in practice, and revealing
their performance limitations.",None,-1
DarkBERT: A Language Model for the Dark Side of the Internet,0.726194,"Recent research has suggested that there are clear differences in the
language used in the Dark Web compared to that of the Surface Web. As studies
on the Dark Web commonly require textual analysis of the domain, language
models specific to the Dark Web may provide valuable insights to researchers.
In this work, we introduce DarkBERT, a language model pretrained on Dark Web
data. We describe the steps taken to filter and compile the text data used to
train DarkBERT to combat the extreme lexical and structural diversity of the
Dark Web that may be detrimental to building a proper representation of the
domain. We evaluate DarkBERT and its vanilla counterpart along with other
widely used language models to validate the benefits that a Dark Web domain
specific model offers in various use cases. Our evaluations show that DarkBERT
outperforms current language models and may serve as a valuable resource for
future research on the Dark Web.",None,-1
"The logic behind desirable sets of things, and its filter representation",0.116113,"We identify the (filter representation of the) logic behind the recent theory
of coherent sets of desirable (sets of) things, which generalise coherent sets
of desirable (sets of) gambles as well as coherent choice functions, and show
that this identification allows us to establish various representation results
for such coherent models in terms of simpler ones.",None,-1
IMF: Interactive Multimodal Fusion Model for Link Prediction,0.8842,"Link prediction aims to identify potential missing triples in knowledge
graphs. To get better results, some recent studies have introduced multimodal
information to link prediction. However, these methods utilize multimodal
information separately and neglect the complicated interaction between
different modalities. In this paper, we aim at better modeling the
inter-modality information and thus introduce a novel Interactive Multimodal
Fusion (IMF) model to integrate knowledge from different modalities. To this
end, we propose a two-stage multimodal fusion framework to preserve
modality-specific knowledge as well as take advantage of the complementarity
between different modalities. Instead of directly projecting different
modalities into a unified space, our multimodal fusion module limits the
representations of different modalities independent while leverages bilinear
pooling for fusion and incorporates contrastive learning as additional
constraints. Furthermore, the decision fusion module delivers the learned
weighted average over the predictions of all modalities to better incorporate
the complementarity of different modalities. Our approach has been demonstrated
to be effective through empirical evaluations on several real-world datasets.
The implementation code is available online at
https://github.com/HestiaSky/IMF-Pytorch.",None,-1
Zero-Shot Co-salient Object Detection Framework,0.777138,"Co-salient Object Detection (CoSOD) endeavors to replicate the human visual
system's capacity to recognize common and salient objects within a collection
of images. Despite recent advancements in deep learning models, these models
still rely on training with well-annotated CoSOD datasets. The exploration of
training-free zero-shot CoSOD frameworks has been limited. In this paper,
taking inspiration from the zero-shot transfer capabilities of foundational
computer vision models, we introduce the first zero-shot CoSOD framework that
harnesses these models without any training process. To achieve this, we
introduce two novel components in our proposed framework: the group prompt
generation (GPG) module and the co-saliency map generation (CMP) module. We
evaluate the framework's performance on widely-used datasets and observe
impressive results. Our approach surpasses existing unsupervised methods and
even outperforms fully supervised methods developed before 2020, while
remaining competitive with some fully supervised methods developed before 2022.",None,-1
PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow,0.807914,"During industrial processing, unforeseen defects may arise in products due to
uncontrollable factors. Although unsupervised methods have been successful in
defect localization, the usual use of pre-trained models results in
low-resolution outputs, which damages visual performance. To address this
issue, we propose PyramidFlow, the first fully normalizing flow method without
pre-trained models that enables high-resolution defect localization.
Specifically, we propose a latent template-based defect contrastive
localization paradigm to reduce intra-class variance, as the pre-trained models
do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for
multi-scale fusing and volume normalization to help generalization. Our
comprehensive studies on MVTecAD demonstrate the proposed method outperforms
the comparable algorithms that do not use external priors, even achieving
state-of-the-art performance in more challenging BTAD scenarios.",None,-1
Tracr: Compiled Transformers as a Laboratory for Interpretability,0.746284,"We show how to ""compile"" human-readable programs into standard decoder-only
transformer models. Our compiler, Tracr, generates models with known structure.
This structure can be used to design experiments. For example, we use it to
study ""superposition"" in transformers that execute multi-step algorithms.
Additionally, the known structure of Tracr-compiled models can serve as
ground-truth for evaluating interpretability methods. Commonly, because the
""programs"" learned by transformers are unknown it is unclear whether an
interpretation succeeded. We demonstrate our approach by implementing and
examining programs including computing token frequencies, sorting, and
parenthesis checking. We provide an open-source implementation of Tracr at
https://github.com/google-deepmind/tracr.",None,-1
Channelformer: Attention based Neural Solution for Wireless Channel Estimation and Effective Online Training,0.635372,"In this paper, we propose an encoder-decoder neural architecture (called
Channelformer) to achieve improved channel estimation for orthogonal
frequency-division multiplexing (OFDM) waveforms in downlink scenarios. The
self-attention mechanism is employed to achieve input precoding for the input
features before processing them in the decoder. In particular, we implement
multi-head attention in the encoder and a residual convolutional neural
architecture as the decoder, respectively. We also employ a customized
weight-level pruning to slim the trained neural network with a fine-tuning
process, which reduces the computational complexity significantly to realize a
low complexity and low latency solution. This enables reductions of up to 70\%
in the parameters, while maintaining an almost identical performance compared
with the complete Channelformer. We also propose an effective online training
method based on the fifth generation (5G) new radio (NR) configuration for the
modern communication systems, which only needs the available information at the
receiver for online training. Using industrial standard channel models, the
simulations of attention-based solutions show superior estimation performance
compared with other candidate neural network methods for channel estimation.",None,-1
Intelligence-Endogenous Management Platform for Computing and Network Convergence,0.324818,"Massive emerging applications are driving demand for the ubiquitous
deployment of computing power today. This trend not only spurs the recent
popularity of the \emph{Computing and Network Convergence} (CNC), but also
introduces an urgent need for the intelligentization of a management platform
to coordinate changing resources and tasks in the CNC. Therefore, in this
article, we present the concept of an intelligence-endogenous management
platform for CNCs called \emph{CNC brain} based on artificial intelligence
technologies. It aims at efficiently and automatically matching the supply and
demand with high heterogeneity in a CNC via four key building blocks, i.e.,
perception, scheduling, adaptation, and governance, throughout the CNC's life
cycle. Their functionalities, goals, and challenges are presented. To examine
the effectiveness of the proposed concept and framework, we also implement a
prototype for the CNC brain based on a deep reinforcement learning technology.
Also, it is evaluated on a CNC testbed that integrates two open-source and
popular frameworks (OpenFaas and Kubernetes) and a real-world business dataset
provided by Microsoft Azure. The evaluation results prove the proposed method's
effectiveness in terms of resource utilization and performance. Finally, we
highlight the future research directions of the CNC brain.",None,-1
"An Optimal, Universal and Agnostic Decoding Method for Message Reconstruction, Bio and Technosignature Detection",0.11958,"We present a signal reconstruction method for zero-knowledge one-way
communication channels in which a receiver aims to interpret a message sent by
an unknown source about which no prior knowledge is available and to which no
return message can be sent. Our reconstruction method is agnostic vis-\`a-vis
the arbitrarily chosen encoding-decoding scheme and other observer-dependent
characteristics, such as the arbitrarily chosen computation model or underlying
mathematical theory. We investigate how non-random messages may encode
information about the physical properties, such as dimension and length scales
of the space in which a signal or message may have been originally encoded,
embedded, or generated. We argue that our results have applications to life and
technosignature detection and to coding theory in general.",None,-1
I3D: Transformer architectures with input-dependent dynamic depth for speech recognition,0.539909,"Transformer-based end-to-end speech recognition has achieved great success.
However, the large footprint and computational overhead make it difficult to
deploy these models in some real-world applications. Model compression
techniques can reduce the model size and speed up inference, but the compressed
model has a fixed architecture which might be suboptimal. We propose a novel
Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong
performance-efficiency trade-offs. With a similar number of layers at inference
time, I3D-based models outperform the vanilla Transformer and the static pruned
model via iterative layer pruning. We also present interesting analysis on the
gate probabilities and the input-dependency, which helps us better understand
deep encoders.",None,-1
Adapting the adapters for code-switching in multilingual ASR,0.24436,"Recently, large pre-trained multilingual speech models have shown potential
in scaling Automatic Speech Recognition (ASR) to many low-resource languages.
Some of these models employ language adapters in their formulation, which helps
to improve monolingual performance and avoids some of the drawbacks of
multi-lingual modeling on resource-rich languages. However, this formulation
restricts the usability of these models on code-switched speech, where two
languages are mixed together in the same utterance. In this work, we propose
ways to effectively fine-tune such models on code-switched speech, by
assimilating information from both language adapters at each language
adaptation point in the network. We also model code-switching as a sequence of
latent binary sequences that can be used to guide the flow of information from
each language adapter at the frame level. The proposed approaches are evaluated
on three code-switched datasets encompassing Arabic, Mandarin, and Hindi
languages paired with English, showing consistent improvements in
code-switching performance with at least 10\% absolute reduction in CER across
all test sets.",None,-1
Unpaired Image-to-Image Translation via Neural Schrdinger Bridge,0.860262,"Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. While
diffusion models have achieved remarkable progress, they have limitations in
unpaired image-to-image (I2I) translation tasks due to the Gaussian prior
assumption. Schr\""{o}dinger Bridge (SB), which learns an SDE to translate
between two arbitrary distributions, have risen as an attractive solution to
this problem. Yet, to our best knowledge, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose Unpaired Neural Schr\""{o}dinger Bridge (UNSB), which expresses
the SB problem as a sequence of adversarial learning problems. This allows us
to incorporate advanced discriminators and regularization to learn a SB between
unpaired data. We show that UNSB is scalable and successfully solves various
unpaired I2I translation tasks. Code: \url{https://github.com/cyclomon/UNSB}",None,-1
Automatic coral reef fish identification and 3D measurement in the wild,0.930517,"In this paper we present a pipeline using stereo images in order to
automatically identify, track in 3D fish, and measure fish population.",None,-1
GlyphDiffusion: Text Generation as Image Generation,0.0350853,"Diffusion models have become a new generative paradigm for text generation.
Considering the discrete categorical nature of text, in this paper, we propose
GlyphDiffusion, a novel diffusion approach for text generation via text-guided
image generation. Our key idea is to render the target text as a glyph image
containing visual language content. In this way, conditional text generation
can be cast as a glyph image generation task, and it is then natural to apply
continuous diffusion models to discrete texts. Specially, we utilize a cascaded
architecture (ie a base and a super-resolution diffusion model) to generate
high-fidelity glyph images, conditioned on the input text. Furthermore, we
design a text grounding module to transform and refine the visual language
content from generated glyph images into the final texts. In experiments over
four conditional text generation tasks and two classes of metrics (ie quality
and diversity), GlyphDiffusion can achieve comparable or even better results
than several baselines, including pretrained language models. Our model also
makes significant improvements compared to the recent diffusion model.",None,-1
ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints,0.147032,"Reinforcement Learning (RL) for constrained MDPs (CMDPs) is an increasingly
important problem for various applications. Often, the average criterion is
more suitable than the discounted criterion. Yet, RL for average-CMDPs (ACMDPs)
remains a challenging problem. Algorithms designed for discounted constrained
RL problems often do not perform well for the average CMDP setting. In this
paper, we introduce a new policy optimization with function approximation
algorithm for constrained MDPs with the average criterion. The
Average-Constrained Policy Optimization (ACPO) algorithm is inspired by trust
region-based policy optimization algorithms. We develop basic sensitivity
theory for average CMDPs, and then use the corresponding bounds in the design
of the algorithm. We provide theoretical guarantees on its performance, and
through extensive experimental work in various challenging OpenAI Gym
environments, show its superior empirical performance when compared to other
state-of-the-art algorithms adapted for the ACMDPs.",None,-1
Decomposing Complex Queries for Tip-of-the-tongue Retrieval,0.418106,"When re-finding items, users who forget or are uncertain about identifying
details often rely on creative strategies for expressing their information
needs -- complex queries that describe content elements (e.g., book characters
or events), information beyond the document text (e.g., descriptions of book
covers), or personal context (e.g., when they read a book). This retrieval
setting, called tip of the tongue (TOT), is especially challenging for models
heavily reliant on lexical and semantic overlap between query and document
text. In this work, we introduce a simple yet effective framework for handling
such complex queries by decomposing the query into individual clues, routing
those as sub-queries to specialized retrievers, and ensembling the results.
This approach allows us to take advantage of off-the-shelf retrievers (e.g.,
CLIP for retrieving images of book covers) or incorporate retriever-specific
logic (e.g., date constraints). We show that our framework incorportating query
decompositions into retrievers can improve gold book recall up to 7% relative
again for Recall@5 on a new collection of 14,441 real-world query-book pairs
from an online community for resolving TOT inquiries.",None,-1
CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion,0.825314,"This paper proposes a novel diffusion-based model, CompoDiff, for solving
zero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper
also introduces a new synthetic dataset, named SynthTriplets18M, with 18.8
million reference images, conditions, and corresponding target image triplets
to train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the
previous CIR approaches, such as poor generalizability due to the small dataset
scale and the limited types of conditions. CompoDiff not only achieves a new
state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,
and GeneCIS, but also enables a more versatile and controllable CIR by
accepting various conditions, such as negative text, and image mask conditions.
CompoDiff also shows the controllability of the condition strength between text
and image queries and the trade-off between inference speed and performance,
which are unavailable with existing CIR methods. The code and dataset are
available at https://github.com/navervision/CompoDiff",None,-1
Leveraging Visemes for Better Visual Speech Representation and Lip Reading,0.185354,"Lip reading is a challenging task that has many potential applications in
speech recognition, human-computer interaction, and security systems. However,
existing lip reading systems often suffer from low accuracy due to the
limitations of video features. In this paper, we propose a novel approach that
leverages visemes, which are groups of phonetically similar lip shapes, to
extract more discriminative and robust video features for lip reading. We
evaluate our approach on various tasks, including word-level and sentence-level
lip reading, and audiovisual speech recognition using the Arman-AV dataset, a
largescale Persian corpus. Our experimental results show that our viseme based
approach consistently outperforms the state-of-theart methods in all these
tasks. The proposed method reduces the lip-reading word error rate (WER) by
9.1% relative to the best previous method.",None,-1
HGDNet: A Height-Hierarchy Guided Dual-Decoder Network for Single View Building Extraction and Height Estimation,0.684299,"Unifying the correlative single-view satellite image building extraction and
height estimation tasks indicates a promising way to share representations and
acquire generalist model for large-scale urban 3D reconstruction. However, the
common spatial misalignment between building footprints and
stereo-reconstructed nDSM height labels incurs degraded performance on both
tasks. To address this issue, we propose a Height-hierarchy Guided Dual-decoder
Network (HGDNet) to estimate building height. Under the guidance of synthesized
discrete height-hierarchy nDSM, auxiliary height-hierarchical building
extraction branch enhance the height estimation branch with implicit
constraints, yielding an accuracy improvement of more than 6% on the DFC 2023
track2 dataset. Additional two-stage cascade architecture is adopted to achieve
more accurate building extraction. Experiments on the DFC 2023 Track 2 dataset
shows the superiority of the proposed method in building height estimation
({\delta}1:0.8012), instance extraction (AP50:0.7730), and the final average
score 0.7871 ranks in the first place in test phase.",None,-1
"""What do others think?"": Task-Oriented Conversational Modeling with Subjective Knowledge",0.401227,"Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that
assist users in accomplishing specific goals, such as booking a hotel or a
restaurant. Traditional TODs rely on domain-specific APIs/DBs or external
factual knowledge to generate responses, which cannot accommodate subjective
user requests (e.g., ""Is the WIFI reliable?"" or ""Does the restaurant have a
good atmosphere?""). To address this issue, we propose a novel task of
subjective-knowledge-based TOD (SK-TOD). We also propose the first
corresponding dataset, which contains subjective knowledge-seeking dialogue
contexts and manually annotated responses grounded in subjective knowledge
sources. When evaluated with existing TOD approaches, we find that this task
poses new challenges such as aggregating diverse opinions from multiple
knowledge snippets. We hope this task and dataset can promote further research
on TOD and subjective content understanding. The code and the dataset are
available at https://github.com/alexa/dstc11-track5.",None,-1
"Tell, don't show: Declarative facts influence how LLMs generalize",0.0316925,"We examine how large language models (LLMs) generalize from abstract
declarative statements in their training data. As an illustration, consider an
LLM that is prompted to generate weather reports for London in 2050. One
possibility is that the temperatures in the reports match the mean and variance
of reports from 2023 (i.e. matching the statistics of pretraining). Another
possibility is that the reports predict higher temperatures, by incorporating
declarative statements about climate change from scientific papers written in
2023. An example of such a declarative statement is ""global temperatures will
increase by $1^{\circ} \mathrm{C}$ by 2050"".
  To test the influence of abstract declarative statements, we construct tasks
in which LLMs are finetuned on both declarative and procedural information. We
find that declarative statements influence model predictions, even when they
conflict with procedural information. In particular, finetuning on a
declarative statement $S$ increases the model likelihood for logical
consequences of $S$. The effect of declarative statements is consistent across
three domains: aligning an AI assistant, predicting weather, and predicting
demographic features. Through a series of ablations, we show that the effect of
declarative statements cannot be explained by associative learning based on
matching keywords. Nevertheless, the effect of declarative statements on model
likelihoods is small in absolute terms and increases surprisingly little with
model size (i.e. from 330 million to 175 billion parameters). We argue that
these results have implications for AI risk (in relation to the ""treacherous
turn"") and for fairness.",None,-1
A Boosted Model Ensembling Approach to Ball Action Spotting in Videos: The Runner-Up Solution to CVPR'23 SoccerNet Challenge,0.761796,"This technical report presents our solution to Ball Action Spotting in
videos. Our method reached second place in the CVPR'23 SoccerNet Challenge.
Details of this challenge can be found at
https://www.soccer-net.org/tasks/ball-action-spotting. Our approach is
developed based on a baseline model termed E2E-Spot, which was provided by the
organizer of this competition. We first generated several variants of the
E2E-Spot model, resulting in a candidate model set. We then proposed a strategy
for selecting appropriate model members from this set and assigning an
appropriate weight to each model. The aim of this strategy is to boost the
performance of the resulting model ensemble. Therefore, we call our approach
Boosted Model Ensembling (BME). Our code is available at
https://github.com/ZJLAB-AMMI/E2E-Spot-MBS.",None,-1
CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities,0.609976,"Detecting norm violations in online communities is critical to maintaining
healthy and safe spaces for online discussions. Existing machine learning
approaches often struggle to adapt to the diverse rules and interpretations
across different communities due to the inherent challenges of fine-tuning
models for such context-specific tasks. In this paper, we introduce
Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a
novel method that employs prompt-based learning to detect norm violations
across various types of rules. CPL-NoViD outperforms the baseline by
incorporating context through natural language prompts and demonstrates
improved performance across different rule types. Significantly, it not only
excels in cross-rule-type and cross-community norm violation detection but also
exhibits adaptability in few-shot learning scenarios. Most notably, it
establishes a new state-of-the-art in norm violation detection, surpassing
existing benchmarks. Our work highlights the potential of prompt-based learning
for context-sensitive norm violation detection and paves the way for future
research on more adaptable, context-aware models to better support online
community moderators.",None,-1
ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots,0.650692,"The birth of ChatGPT, a cutting-edge language model-based chatbot developed
by OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its
role in rigorous scientific research is not clear yet. This paper vividly
showcases its innovative application within the field of drug discovery.
Focused specifically on developing anti-cocaine addiction drugs, the study
employs GPT-4 as a virtual guide, offering strategic and methodological
insights to researchers working on generative models for drug candidates. The
primary objective is to generate optimal drug-like molecules with desired
properties. By leveraging the capabilities of ChatGPT, the study introduces a
novel approach to the drug discovery process. This symbiotic partnership
between AI and researchers transforms how drug development is approached.
Chatbots become facilitators, steering researchers towards innovative
methodologies and productive paths for creating effective drug candidates. This
research sheds light on the collaborative synergy between human expertise and
AI assistance, wherein ChatGPT's cognitive abilities enhance the design and
development of potential pharmaceutical solutions. This paper not only explores
the integration of advanced AI in drug discovery but also reimagines the
landscape by advocating for AI-powered chatbots as trailblazers in
revolutionizing therapeutic innovation.",None,-1
Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition,0.697305,"Named Entity Recognition (NER) is a well and widely studied task in natural
language processing. Recently, the nested NER has attracted more attention
since its practicality and difficulty. Existing works for nested NER ignore the
recognition order and boundary position relation of nested entities. To address
these issues, we propose a novel seq2seq model named GPRL, which formulates the
nested NER task as an entity triplet sequence generation process. GPRL adopts
the reinforcement learning method to generate entity triplets decoupling the
entity order in gold labels and expects to learn a reasonable recognition order
of entities via trial and error. Based on statistics of boundary distance for
nested entities, GPRL designs a Gaussian prior to represent the boundary
distance distribution between nested entities and adjust the output probability
distribution of nested boundary tokens. Experiments on three nested NER
datasets demonstrate that GPRL outperforms previous nested NER models.",None,-1
Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios,0.328185,"With the continuous growth in communication network complexity and traffic
volume, communication load balancing solutions are receiving increasing
attention. Specifically, reinforcement learning (RL)-based methods have shown
impressive performance compared with traditional rule-based methods. However,
standard RL methods generally require an enormous amount of data to train, and
generalize poorly to scenarios that are not encountered during training. We
propose a policy reuse framework in which a policy selector chooses the most
suitable pre-trained RL policy to execute based on the current traffic
condition. Our method hinges on a policy bank composed of policies trained on a
diverse set of traffic scenarios. When deploying to an unknown traffic
scenario, we select a policy from the policy bank based on the similarity
between the previous-day traffic of the current scenario and the traffic
observed during training. Experiments demonstrate that this framework can
outperform classical and adaptive rule-based methods by a large margin.",None,-1
Backpack Language Models,0.840257,"We present Backpacks: a new neural architecture that marries strong modeling
performance with an interface for interpretability and control. Backpacks learn
multiple non-contextual sense vectors for each word in a vocabulary, and
represent a word in a sequence as a context-dependent, non-negative linear
combination of sense vectors in this sequence. We find that, after training,
sense vectors specialize, each encoding a different aspect of a word. We can
interpret a sense vector by inspecting its (non-contextual, linear) projection
onto the output space, and intervene on these interpretable hooks to change the
model's behavior in predictable ways. We train a 170M-parameter Backpack
language model on OpenWebText, matching the loss of a GPT-2 small
(124Mparameter) Transformer. On lexical similarity evaluations, we find that
Backpack sense vectors outperform even a 6B-parameter Transformer LM's word
embeddings. Finally, we present simple algorithms that intervene on sense
vectors to perform controllable text generation and debiasing. For example, we
can edit the sense vocabulary to tend more towards a topic, or localize a
source of gender bias to a sense vector and globally suppress that sense.",None,-1
Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play,0.999662,"Teaching dexterity to multi-fingered robots has been a longstanding challenge
in robotics. Most prominent work in this area focuses on learning controllers
or policies that either operate on visual observations or state estimates
derived from vision. However, such methods perform poorly on fine-grained
manipulation tasks that require reasoning about contact forces or about objects
occluded by the hand itself. In this work, we present T-Dex, a new approach for
tactile-based dexterity, that operates in two phases. In the first phase, we
collect 2.5 hours of play data, which is used to train self-supervised tactile
encoders. This is necessary to bring high-dimensional tactile readings to a
lower-dimensional embedding. In the second phase, given a handful of
demonstrations for a dexterous task, we learn non-parametric policies that
combine the tactile observations with visual ones. Across five challenging
dexterous tasks, we show that our tactile-based dexterity models outperform
purely vision and torque-based models by an average of 1.7X. Finally, we
provide a detailed analysis on factors critical to T-Dex including the
importance of play data, architectures, and representation learning.",None,-1
Solving Travelling Thief Problems using Coordination Based Methods,0.517159,"A travelling thief problem (TTP) is a proxy to real-life problems such as
postal collection. TTP comprises an entanglement of a travelling salesman
problem (TSP) and a knapsack problem (KP) since items of KP are scattered over
cities of TSP, and a thief has to visit cities to collect items. In TTP, city
selection and item selection decisions need close coordination since the
thief's travelling speed depends on the knapsack's weight and the order of
visiting cities affects the order of item collection. Existing TTP solvers deal
with city selection and item selection separately, keeping decisions for one
type unchanged while dealing with the other type. This separation essentially
means very poor coordination between two types of decision. In this paper, we
first show that a simple local search based coordination approach does not work
in TTP. Then, to address the aforementioned problems, we propose a human
designed coordination heuristic that makes changes to collection plans during
exploration of cyclic tours. We further propose another human designed
coordination heuristic that explicitly exploits the cyclic tours in item
selections during collection plan exploration. Lastly, we propose a machine
learning based coordination heuristic that captures characteristics of the two
human designed coordination heuristics. Our proposed coordination based
approaches help our TTP solver significantly outperform existing
state-of-the-art TTP solvers on a set of benchmark problems. Our solver is
named Cooperation Coordination (CoCo) and its source code is available from
https://github.com/majid75/CoCo",None,-1
The Gradient of Generative AI Release: Methods and Considerations,0.871335,"As increasingly powerful generative AI systems are developed, the release
method greatly varies. We propose a framework to assess six levels of access to
generative AI systems: fully closed; gradual or staged access; hosted access;
cloud-based or API access; downloadable access; and fully open. Each level,
from fully closed to fully open, can be viewed as an option along a gradient.
We outline key considerations across this gradient: release methods come with
tradeoffs, especially around the tension between concentrating power and
mitigating risks. Diverse and multidisciplinary perspectives are needed to
examine and mitigate risk in generative AI systems from conception to
deployment. We show trends in generative system release over time, noting
closedness among large companies for powerful systems and openness among
organizations founded on principles of openness. We also enumerate safety
controls and guardrails for generative systems and necessary investments to
improve future releases.",None,-1
RQAT-INR: Improved Implicit Neural Image Compression,0.620297,"Deep variational autoencoders for image and video compression have gained
significant attraction in the recent years, due to their potential to offer
competitive or better compression rates compared to the decades long
traditional codecs such as AVC, HEVC or VVC. However, because of complexity and
energy consumption, these approaches are still far away from practical usage in
industry. More recently, implicit neural representation (INR) based codecs have
emerged, and have lower complexity and energy usage to classical approaches at
decoding. However, their performances are not in par at the moment with
state-of-the-art methods. In this research, we first show that INR based image
codec has a lower complexity than VAE based approaches, then we propose several
improvements for INR-based image codec and outperformed baseline model by a
large margin.",None,-1
FaceLit: Neural 3D Relightable Faces,0.356079,"We propose a generative framework, FaceLit, capable of generating a 3D face
that can be rendered at various user-defined lighting conditions and views,
learned purely from 2D images in-the-wild without any manual annotation. Unlike
existing works that require careful capture setup or human labor, we rely on
off-the-shelf pose and illumination estimators. With these estimates, we
incorporate the Phong reflectance model in the neural volume rendering
framework. Our model learns to generate shape and material properties of a face
such that, when rendered according to the natural statistics of pose and
illumination, produces photorealistic face images with multiview 3D and
illumination consistency. Our method enables photorealistic generation of faces
with explicit illumination and view controls on multiple datasets - FFHQ,
MetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware
GANs on FFHQ dataset achieving an FID score of 3.5.",None,-1
Multi-view knowledge distillation transformer for human action recognition,0.187712,"Recently, Transformer-based methods have been utilized to improve the
performance of human action recognition. However, most of these studies assume
that multi-view data is complete, which may not always be the case in
real-world scenarios. Therefore, this paper presents a novel Multi-view
Knowledge Distillation Transformer (MKDT) framework that consists of a teacher
network and a student network. This framework aims to handle incomplete human
action problems in real-world applications. Specifically, the multi-view
knowledge distillation transformer uses a hierarchical vision transformer with
shifted windows to capture more spatial-temporal information. Experimental
results demonstrate that our framework outperforms the CNN-based method on
three public datasets.",None,-1
Data Contamination Through the Lens of Time,0.17065,"Recent claims about the impressive abilities of large language models (LLMs)
are often supported by evaluating publicly available benchmarks. Since LLMs
train on wide swaths of the internet, this practice raises concerns of data
contamination, i.e., evaluating on examples that are explicitly or implicitly
included in the training data. Data contamination remains notoriously
challenging to measure and mitigate, even with partial attempts like controlled
experimentation of training data, canary strings, or embedding similarities. In
this work, we conduct the first thorough longitudinal analysis of data
contamination in LLMs by using the natural experiment of training cutoffs in
GPT models to look at benchmarks released over time. Specifically, we consider
two code/mathematical problem-solving datasets, Codeforces and Project Euler,
and find statistically significant trends among LLM pass rate vs. GitHub
popularity and release date that provide strong evidence of contamination. By
open-sourcing our dataset, raw results, and evaluation framework, our work
paves the way for rigorous analyses of data contamination in modern models. We
conclude with a discussion of best practices and future steps for publicly
releasing benchmarks in the age of LLMs that train on webscale data.",None,-1
Prompt-Based Tuning of Transformer Models for Multi-Center Medical Image Segmentation of Head and Neck Cancer,0.180877,"Medical image segmentation is a vital healthcare endeavor requiring precise
and efficient models for appropriate diagnosis and treatment. Vision
transformer (ViT)-based segmentation models have shown great performance in
accomplishing this task. However, to build a powerful backbone, the
self-attention block of ViT requires large-scale pre-training data. The present
method of modifying pre-trained models entails updating all or some of the
backbone parameters. This paper proposes a novel fine-tuning strategy for
adapting a pretrained transformer-based segmentation model on data from a new
medical center. This method introduces a small number of learnable parameters,
termed prompts, into the input space (less than 1\% of model parameters) while
keeping the rest of the model parameters frozen. Extensive studies employing
data from new unseen medical centers show that the prompt-based fine-tuning of
medical segmentation models provides excellent performance regarding the
new-center data with a negligible drop regarding the old centers. Additionally,
our strategy delivers great accuracy with minimum re-training on new-center
data, significantly decreasing the computational and time costs of fine-tuning
pre-trained models.",None,-1
SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation,0.941997,"We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.",None,-1
On the Generalization of Training-based ChatGPT Detection Methods,0.157527,"ChatGPT is one of the most popular language models which achieve amazing
performance on various natural language tasks. Consequently, there is also an
urgent need to detect the texts generated ChatGPT from human written. One of
the extensively studied methods trains classification models to distinguish
both. However, existing studies also demonstrate that the trained models may
suffer from distribution shifts (during test), i.e., they are ineffective to
predict the generated texts from unseen language tasks or topics. In this work,
we aim to have a comprehensive investigation on these methods' generalization
behaviors under distribution shift caused by a wide range of factors, including
prompts, text lengths, topics, and language tasks. To achieve this goal, we
first collect a new dataset with human and ChatGPT texts, and then we conduct
extensive studies on the collected dataset. Our studies unveil insightful
findings which provide guidance for developing future methodologies or data
collection strategies for ChatGPT detection.",None,-1
Analytical reconstructions of full-scan multiple source-translation computed tomography under large field of views,0.293778,"This paper is to investigate the high-quality analytical reconstructions of
multiple source-translation computed tomography (mSTCT) under an extended field
of view (FOV). Under the larger FOVs, the previously proposed backprojection
filtration (BPF) algorithms for mSTCT, including D-BPF and S-BPF (their
differences are different derivate directions along the detector and source,
respectively), make some errors and artifacts in the reconstructed images due
to a backprojection weighting factor and the half-scan mode, which deviates
from the intention of mSTCT imaging. In this paper, to achieve reconstruction
with as little error as possible under the extremely extended FOV, we combine
the full-scan mSTCT (F-mSTCT) geometry with the previous BPF algorithms to
study the performance and derive a suitable redundancy-weighted function for
F-mSTCT. The experimental results indicate FS-BPF can get high-quality, stable
images under the extremely extended FOV of imaging a large object, though it
requires more projections than FD-BPF. Finally, for different practical
requirements in extending FOV imaging, we give suggestions on algorithm
selection.",None,-1
Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior,0.524051,"We propose a surrogate function for efficient use of score-based priors for
Bayesian inverse imaging. Recent work turned score-based diffusion models into
probabilistic priors for solving ill-posed imaging problems by appealing to an
ODE-based log-probability function. However, evaluating this function is
computationally inefficient and inhibits posterior estimation of
high-dimensional images. Our proposed surrogate prior is based on the evidence
lower-bound of a score-based diffusion model. We demonstrate the surrogate
prior on variational inference for efficient approximate posterior sampling of
large images. Compared to the exact prior in previous work, our surrogate prior
accelerates optimization of the variational image distribution by at least two
orders of magnitude. We also find that our principled approach achieves
higher-fidelity images than non-Bayesian baselines that involve
hyperparameter-tuning at inference. Our work establishes a practical path
forward for using score-based diffusion models as general-purpose priors for
imaging.",None,-1
Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge,0.629679,"Pre-trained language models (LMs) are used for knowledge intensive tasks like
question answering, but their knowledge gets continuously outdated as the world
changes. Prior work has studied targeted updates to LMs, injecting individual
facts and evaluating whether the model learns these facts while not changing
predictions on other contexts. We take a step forward and study LMs' abilities
to make inferences based on injected facts (or propagate those facts): for
example, after learning that something is a TV show, does an LM predict that
you can watch it? We study this with two cloze-style tasks: an existing dataset
of real-world sentences about novel entities (ECBD) as well as a new controlled
benchmark with manually designed templates requiring varying levels of
inference about injected knowledge. Surprisingly, we find that existing methods
for updating knowledge (gradient-based fine-tuning and modifications of this
approach) show little propagation of injected knowledge. These methods improve
performance on cloze instances only when there is lexical overlap between
injected facts and target inferences. Yet, prepending entity definitions in an
LM's context improves performance across all settings, suggesting that there is
substantial headroom for parameter-updating approaches for knowledge injection.",None,-1
PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs,0.800289,"Large language models (LLMs) have shown great abilities of solving various
natural language tasks in different domains. Due to the training objective of
LLMs and their pre-training data, LLMs are not very well equipped for tasks
involving structured data generation. We propose a framework, Prompting with
Iterative Verification (PiVe), to improve graph-based generative capability of
LLMs. We show how a small language model could be trained to act as a verifier
module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively
improve its performance via fine-grained corrective instructions. We also show
how the verifier module could apply iterative corrections offline for a more
cost-effective solution to the text-to-graph generation task. Experiments on
three graph-based datasets show consistent improvement gained via PiVe.
Additionally, we create GenWiki-HIQ and highlight that the verifier module can
be used as a data augmentation tool to help improve the quality of
automatically generated parallel text-graph datasets.",None,-1
RLTF: Reinforcement Learning from Unit Test Feedback,0.96901,"The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.",None,-1
SelfClean: A Self-Supervised Data Cleaning Strategy,0.189759,"Most benchmark datasets for computer vision contain irrelevant images, near
duplicates, and label errors. Consequently, model performance on these
benchmarks may not be an accurate estimate of generalization capabilities. This
is a particularly acute concern in computer vision for medicine where datasets
are typically small, stakes are high, and annotation processes are expensive
and error-prone. In this paper we propose SelfClean, a general procedure to
clean up image datasets exploiting a latent space learned with
self-supervision. By relying on self-supervised learning, our approach focuses
on intrinsic properties of the data and avoids annotation biases. We formulate
dataset cleaning as either a set of ranking problems, which significantly
reduce human annotation effort, or a set of scoring problems, which enable
fully automated decisions based on score distributions. We demonstrate that
SelfClean achieves state-of-the-art performance in detecting irrelevant images,
near duplicates, and label errors within popular computer vision benchmarks,
retrieving both injected synthetic noise and natural contamination. In
addition, we apply our method to multiple image datasets and confirm an
improvement in evaluation reliability.",None,-1
Mixture Encoder for Joint Speech Separation and Recognition,0.38205,"Multi-speaker automatic speech recognition (ASR) is crucial for many
real-world applications, but it requires dedicated modeling techniques.
Existing approaches can be divided into modular and end-to-end methods. Modular
approaches separate speakers and recognize each of them with a single-speaker
ASR system. End-to-end models process overlapped speech directly in a single,
powerful neural network. This work proposes a middle-ground approach that
leverages explicit speech separation similarly to the modular approach but also
incorporates mixture speech information directly into the ASR module in order
to mitigate the propagation of errors made by the speech separator. We also
explore a way to exchange cross-speaker context information through a layer
that combines information of the individual speakers. Our system is optimized
through separate and joint training stages and achieves a relative improvement
of 7% in word error rate over a purely modular setup on the SMS-WSJ task.",None,-1
Intelligent System for Assessing University Student Personality Development and Career Readiness,0.0593644,"While academic metrics such as transcripts and GPA are commonly used to
evaluate students' knowledge acquisition, there is a lack of comprehensive
metrics to measure their preparedness for the challenges of post-graduation
life. This research paper explores the impact of various factors on university
students' readiness for change and transition, with a focus on their
preparedness for careers. The methodology employed in this study involves
designing a survey based on Paul J. Mayer's ""The Balance Wheel"" to capture
students' sentiments on various life aspects, including satisfaction with the
educational process and expectations of salary. The collected data from a KBTU
student survey (n=47) were processed through machine learning models: Linear
Regression, Support Vector Regression (SVR), Random Forest Regression.
Subsequently, an intelligent system was built using these models and fuzzy
sets. The system is capable of evaluating graduates' readiness for their future
careers and demonstrates a high predictive power. The findings of this research
have practical implications for educational institutions. Such an intelligent
system can serve as a valuable tool for universities to assess and enhance
students' preparedness for post-graduation challenges. By recognizing the
factors contributing to students' readiness for change, universities can refine
curricula and processes to better prepare students for their career journeys.",None,-1
ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning,0.977107,"This paper assesses the accuracy, reliability and bias of the Large Language
Model (LLM) ChatGPT-4 on the text analysis task of classifying the political
affiliation of a Twitter poster based on the content of a tweet. The LLM is
compared to manual annotation by both expert classifiers and crowd workers,
generally considered the gold standard for such tasks. We use Twitter messages
from United States politicians during the 2020 election, providing a ground
truth against which to measure accuracy. The paper finds that ChatGPT-4 has
achieves higher accuracy, higher reliability, and equal or lower bias than the
human classifiers. The LLM is able to correctly annotate messages that require
reasoning on the basis of contextual knowledge, and inferences around the
author's intentions - traditionally seen as uniquely human abilities. These
findings suggest that LLM will have substantial impact on the use of textual
data in the social sciences, by enabling interpretive research at a scale.",None,-1
Facial Expression Recognition at the Edge: CPU vs GPU vs VPU vs TPU,0.688763,"Facial Expression Recognition (FER) plays an important role in human-computer
interactions and is used in a wide range of applications. Convolutional Neural
Networks (CNN) have shown promise in their ability to classify human facial
expressions, however, large CNNs are not well-suited to be implemented on
resource- and energy-constrained IoT devices. In this work, we present a
hierarchical framework for developing and optimizing hardware-aware CNNs tuned
for deployment at the edge. We perform a comprehensive analysis across various
edge AI accelerators including NVIDIA Jetson Nano, Intel Neural Compute Stick,
and Coral TPU. Using the proposed strategy, we achieved a peak accuracy of
99.49% when testing on the CK+ facial expression recognition dataset.
Additionally, we achieved a minimum inference latency of 0.39 milliseconds and
a minimum power consumption of 0.52 Watts.",None,-1
Do Differences in Values Influence Disagreements in Online Discussions?,0.896703,"Disagreements are common in online discussions. Disagreement may foster
collaboration and improve the quality of a discussion under some conditions.
Although there exist methods for recognizing disagreement, a deeper
understanding of factors that influence disagreement is lacking in the
literature. We investigate a hypothesis that differences in personal values are
indicative of disagreement in online discussions. We show how state-of-the-art
models can be used for estimating values in online discussions and how the
estimated values can be aggregated into value profiles. We evaluate the
estimated value profiles based on human-annotated agreement labels. We find
that the dissimilarity of value profiles correlates with disagreement in
specific cases. We also find that including value information in agreement
prediction improves performance.",None,-1
Uncertainty-Aware Natural Language Inference with Stochastic Weight Averaging,0.166018,"This paper introduces Bayesian uncertainty modeling using Stochastic Weight
Averaging-Gaussian (SWAG) in Natural Language Understanding (NLU) tasks. We
apply the approach to standard tasks in natural language inference (NLI) and
demonstrate the effectiveness of the method in terms of prediction accuracy and
correlation with human annotation disagreements. We argue that the uncertainty
representations in SWAG better reflect subjective interpretation and the
natural variation that is also present in human language understanding. The
results reveal the importance of uncertainty modeling, an often neglected
aspect of neural language modeling, in NLU tasks.",None,-1
Full Parameter Fine-tuning for Large Language Models with Limited Resources,0.75408,"Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.Code and data are available at
https://github.com/OpenLMLab/LOMO.",None,-1
Neural Amortized Inference for Nested Multi-agent Reasoning,0.0392152,"Multi-agent interactions, such as communication, teaching, and bluffing,
often rely on higher-order social inference, i.e., understanding how others
infer oneself. Such intricate reasoning can be effectively modeled through
nested multi-agent reasoning. Nonetheless, the computational complexity
escalates exponentially with each level of reasoning, posing a significant
challenge. However, humans effortlessly perform complex social inferences as
part of their daily lives. To bridge the gap between human-like inference
capabilities and computational limitations, we propose a novel approach:
leveraging neural networks to amortize high-order social inference, thereby
expediting nested multi-agent reasoning. We evaluate our method in two
challenging multi-agent interaction domains. The experimental results
demonstrate that our method is computationally efficient while exhibiting
minimal degradation in accuracy.",None,-1
AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR Parsing,0.598511,"In this paper, we examine the current state-of-the-art in AMR parsing, which
relies on ensemble strategies by merging multiple graph predictions. Our
analysis reveals that the present models often violate AMR structural
constraints. To address this issue, we develop a validation method, and show
how ensemble models can exploit SMATCH metric weaknesses to obtain higher
scores, but sometimes result in corrupted graphs. Additionally, we highlight
the demanding need to compute the SMATCH score among all possible predictions.
To overcome these challenges, we propose two novel ensemble strategies based on
Transformer models, improving robustness to structural constraints, while also
reducing the computational time. Our methods provide new insights for enhancing
AMR parsers and metrics. Our code is available at
\href{https://www.github.com/babelscape/AMRs-Assemble}{github.com/babelscape/AMRs-Assemble}.",None,-1
Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri,0.718671,"Recent advancements in Digital Document Restoration (DDR) have led to
significant breakthroughs in analyzing highly damaged written artifacts. Among
those, there has been an increasing interest in applying Artificial
Intelligence techniques for virtually unwrapping and automatically detecting
ink on the Herculaneum papyri collection. This collection consists of
carbonized scrolls and fragments of documents, which have been digitized via
X-ray tomography to allow the development of ad-hoc deep learning-based DDR
solutions. In this work, we propose a modification of the Fast Fourier
Convolution operator for volumetric data and apply it in a segmentation
architecture for ink detection on the challenging Herculaneum papyri,
demonstrating its suitability via deep experimental analysis. To encourage the
research on this task and the application of the proposed operator to other
tasks involving volumetric data, we will release our implementation
(https://github.com/aimagelab/vffc)",None,-1
From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation,0.0934233,"Stroke is the second leading cause of death worldwide. Machine learning
classification algorithms have been widely adopted for stroke prediction.
However, these algorithms were evaluated using different datasets and
evaluation metrics. Moreover, there is no comprehensive framework for stroke
data analytics. This paper proposes an intelligent stroke prediction framework
based on a critical examination of machine learning prediction algorithms in
the literature. The five most used machine learning algorithms for stroke
prediction are evaluated using a unified setup for objective comparison.
Comparative analysis and numerical results reveal that the Random Forest
algorithm is best suited for stroke prediction.",None,-1
Active hypothesis testing in unknown environments using recurrent neural networks and model free reinforcement learning,0.143515,"A combination of deep reinforcement learning and supervised learning is
proposed for the problem of active sequential hypothesis testing in completely
unknown environments. We make no assumptions about the prior probability, the
action and observation sets, and the observation generating process. Our method
can be used in any environment even if it has continuous observations or
actions, and performs competitively and sometimes better than the Chernoff
test, in both finite and infinite horizon problems, despite not having access
to the environment dynamics.",None,-1
Deep Metric Multi-View Hashing for Multimedia Retrieval,0.795111,"Learning the hash representation of multi-view heterogeneous data is an
important task in multimedia retrieval. However, existing methods fail to
effectively fuse the multi-view features and utilize the metric information
provided by the dissimilar samples, leading to limited retrieval precision.
Current methods utilize weighted sum or concatenation to fuse the multi-view
features. We argue that these fusion methods cannot capture the interaction
among different views. Furthermore, these methods ignored the information
provided by the dissimilar samples. We propose a novel deep metric multi-view
hashing (DMMVH) method to address the mentioned problems. Extensive empirical
evidence is presented to show that gate-based fusion is better than typical
methods. We introduce deep metric learning to the multi-view hashing problems,
which can utilize metric information of dissimilar samples. On the
MIR-Flickr25K, MS COCO, and NUS-WIDE, our method outperforms the current
state-of-the-art methods by a large margin (up to 15.28 mean Average Precision
(mAP) improvement).",None,-1
MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,0.667286,"Monocular 3D object detection reveals an economical but challenging task in
autonomous driving. Recently center-based monocular methods have developed
rapidly with a great trade-off between speed and accuracy, where they usually
depend on the object center's depth estimation via 2D features. However, the
visual semantic features without sufficient pixel geometry information, may
affect the performance of clues for spatial 3D detection tasks. To alleviate
this, we propose MonoPGC, a novel end-to-end Monocular 3D object detection
framework with rich Pixel Geometry Contexts. We introduce the pixel depth
estimation as our auxiliary task and design depth cross-attention pyramid
module (DCPM) to inject local and global depth geometry knowledge into visual
features. In addition, we present the depth-space-aware transformer (DSAT) to
integrate 3D space position and depth-aware features efficiently. Besides, we
design a novel depth-gradient positional encoding (DGPE) to bring more distinct
pixel geometry contexts into the transformer for better object detection.
Extensive experiments demonstrate that our method achieves the state-of-the-art
performance on the KITTI dataset.",None,-1
TUVF: Learning Generalizable Texture UV Radiance Fields,0.0632253,"Textures are a vital aspect of creating visually appealing and realistic 3D
models. In this paper, we study the problem of generating high-fidelity texture
given shapes of 3D assets, which has been relatively less explored compared
with generic 3D shape modeling. Our goal is to facilitate a controllable
texture generation process, such that one texture code can correspond to a
particular appearance style independent of any input shapes from a category. We
introduce Texture UV Radiance Fields (TUVF) that generate textures in a
learnable UV sphere space rather than directly on the 3D shape. This allows the
texture to be disentangled from the underlying shape and transferable to other
shapes that share the same UV space, i.e., from the same category. We integrate
the UV sphere space with the radiance field, which provides a more efficient
and accurate representation of textures than traditional texture maps. We
perform our experiments on synthetic and real-world object datasets where we
achieve not only realistic synthesis but also substantial improvements over
state-of-the-arts on texture controlling and editing. Project Page:
https://www.anjiecheng.me/TUVF",None,-1
Explicit Neural Surfaces: Learning Continuous Geometry With Deformation Fields,0.264789,"We introduce Explicit Neural Surfaces (ENS), an efficient smooth surface
representation that directly encodes topology with a deformation field from a
known base domain. We apply this representation to reconstruct explicit
surfaces from multiple views, where we use a series of neural deformation
fields to progressively transform the base domain into a target shape. By using
meshes as discrete surface proxies, we train the deformation fields through
efficient differentiable rasterization. Using a fixed base domain allows us to
have Laplace-Beltrami eigenfunctions as an intrinsic positional encoding
alongside standard extrinsic Fourier features, with which our approach can
capture fine surface details. Compared to implicit surfaces, ENS trains faster
and has several orders of magnitude faster inference times. The explicit nature
of our approach also allows higher-quality mesh extraction whilst maintaining
competitive surface reconstruction performance and real-time capabilities.",None,-1
DeltaEdit: Exploring Text-free Training for Text-Driven Image Manipulation,0.525493,"Text-driven image manipulation remains challenging in training or inference
flexibility. Conditional generative models depend heavily on expensive
annotated training data. Meanwhile, recent frameworks, which leverage
pre-trained vision-language models, are limited by either per text-prompt
optimization or inference-time hyper-parameters tuning. In this work, we
propose a novel framework named \textit{DeltaEdit} to address these problems.
Our key idea is to investigate and identify a space, namely delta image and
text space that has well-aligned distribution between CLIP visual feature
differences of two images and CLIP textual embedding differences of source and
target texts. Based on the CLIP delta space, the DeltaEdit network is designed
to map the CLIP visual features differences to the editing directions of
StyleGAN at training phase. Then, in inference phase, DeltaEdit predicts the
StyleGAN's editing directions from the differences of the CLIP textual
features. In this way, DeltaEdit is trained in a text-free manner. Once
trained, it can well generalize to various text prompts for zero-shot inference
without bells and whistles. Code is available at
https://github.com/Yueming6568/DeltaEdit.",None,-1
"Can I say, now machines can think?",0.0552321,"Generative AI techniques have opened the path for new generations of machines
in diverse domains. These machines have various capabilities for example, they
can produce images, generate answers or stories, and write codes based on the
""prompts"" only provided by users. These machines are considered 'thinking
minds' because they have the ability to generate human-like responses. In this
study, we have analyzed and explored the capabilities of artificial
intelligence-enabled machines. We have revisited on Turing's concept of
thinking machines and compared it with recent technological advancements. The
objections and consequences of the thinking machines are also discussed in this
study, along with available techniques to evaluate machines' cognitive
capabilities. We have concluded that Turing Test is a critical aspect of
evaluating machines' ability. However, there are other aspects of intelligence
too, and AI machines exhibit most of these aspects.",None,-1
Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL,0.345541,"We study the design of sample-efficient algorithms for reinforcement learning
in the presence of rich, high-dimensional observations, formalized via the
Block MDP problem. Existing algorithms suffer from either 1) computational
intractability, 2) strong statistical assumptions that are not necessarily
satisfied in practice, or 3) suboptimal sample complexity. We address these
issues by providing the first computationally efficient algorithm that attains
rate-optimal sample complexity with respect to the desired accuracy level, with
minimal statistical assumptions. Our algorithm, MusIK, combines systematic
exploration with representation learning based on multi-step inverse
kinematics, a learning objective in which the aim is to predict the learner's
own action from the current observation and observations in the (potentially
distant) future. MusIK is simple and flexible, and can efficiently take
advantage of general-purpose function approximation. Our analysis leverages
several new techniques tailored to non-optimistic exploration algorithms, which
we anticipate will find broader use.",None,-1
AI model GPT-3 (dis)informs us better than humans,0.831456,"Artificial intelligence is changing the way we create and evaluate
information, and this is happening during an infodemic, which has been having
dramatic effects on global health. In this paper we evaluate whether recruited
individuals can distinguish disinformation from accurate information,
structured in the form of tweets, and determine whether a tweet is organic or
synthetic, i.e., whether it has been written by a Twitter user or by the AI
model GPT-3. Our results show that GPT-3 is a double-edge sword, which, in
comparison with humans, can produce accurate information that is easier to
understand, but can also produce more compelling disinformation. We also show
that humans cannot distinguish tweets generated by GPT-3 from tweets written by
human users. Starting from our results, we reflect on the dangers of AI for
disinformation, and on how we can improve information campaigns to benefit
global health.",None,-1
Adaptive Sparse Pairwise Loss for Object Re-Identification,0.874729,"Object re-identification (ReID) aims to find instances with the same identity
as the given probe from a large gallery. Pairwise losses play an important role
in training a strong ReID network. Existing pairwise losses densely exploit
each instance as an anchor and sample its triplets in a mini-batch. This dense
sampling mechanism inevitably introduces positive pairs that share few visual
similarities, which can be harmful to the training. To address this problem, we
propose a novel loss paradigm termed Sparse Pairwise (SP) loss that only
leverages few appropriate pairs for each class in a mini-batch, and empirically
demonstrate that it is sufficient for the ReID tasks. Based on the proposed
loss framework, we propose an adaptive positive mining strategy that can
dynamically adapt to diverse intra-class variations. Extensive experiments show
that SP loss and its adaptive variant AdaSP loss outperform other pairwise
losses, and achieve state-of-the-art performance across several ReID
benchmarks. Code is available at https://github.com/Astaxanthin/AdaSP.",None,-1
CLIP-KD: An Empirical Study of CLIP Model Distillation,0.483363,"Contrastive Language-Image Pre-training (CLIP) has become a promising
language-supervised visual pre-training framework. This paper aims to distill
small CLIP models supervised by a large teacher CLIP model. We propose several
distillation strategies, including relation, feature, gradient and contrastive
paradigms, to examine the effectiveness of CLIP-Knowledge Distillation (KD). We
show that a simple feature mimicry with Mean Squared Error loss works
surprisingly well. Moreover, interactive contrastive learning across teacher
and student encoders is also effective in performance improvement. We explain
that the success of CLIP-KD can be attributed to maximizing the feature
similarity between teacher and student. The unified method is applied to
distill several student models trained on CC3M+12M. CLIP-KD improves student
CLIP models consistently over zero-shot ImageNet classification and cross-modal
retrieval benchmarks. When using ViT-L/14 pretrained on Laion-400M as the
teacher, CLIP-KD achieves 57.5\% and 55.4\% zero-shot top-1 ImageNet accuracy
over ViT-B/16 and ResNet-50, surpassing the original CLIP without KD by 20.5\%
and 20.1\% margins, respectively. Our code is released on
https://github.com/winycg/CLIP-KD.",None,-1
DisCLIP: Open-Vocabulary Referring Expression Generation,0.45875,"Referring Expressions Generation (REG) aims to produce textual descriptions
that unambiguously identifies specific objects within a visual scene.
Traditionally, this has been achieved through supervised learning methods,
which perform well on specific data distributions but often struggle to
generalize to new images and concepts. To address this issue, we present a
novel approach for REG, named DisCLIP, short for discriminative CLIP. We build
on CLIP, a large-scale visual-semantic model, to guide an LLM to generate a
contextual description of a target concept in an image while avoiding other
distracting concepts. Notably, this optimization happens at inference time and
does not require additional training or tuning of learned parameters. We
measure the quality of the generated text by evaluating the capability of a
receiver model to accurately identify the described object within the scene. To
achieve this, we use a frozen zero-shot comprehension module as a critique of
our generated referring expressions. We evaluate DisCLIP on multiple referring
expression benchmarks through human evaluation and show that it significantly
outperforms previous methods on out-of-domain datasets. Our results highlight
the potential of using pre-trained visual-semantic models for generating
high-quality contextual descriptions.",None,-1
Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration,0.439626,"Recent text-to-image generation models have shown promising results in
generating high-fidelity photo-realistic images. In parallel, the problem of
data scarcity has brought a growing interest in employing AIGC technology for
high-quality data expansion. However, this paradigm requires well-designed
prompt engineering that cost-less data expansion and labeling remain
under-explored. Inspired by LLM's powerful capability in task guidance, we
propose a new paradigm of annotated data expansion named as ChatGenImage. The
core idea behind it is to leverage the complementary strengths of diverse
models to establish a highly effective and user-friendly pipeline for
interactive data augmentation. In this work, we extensively study how LLMs
communicate with AIGC model to achieve more controllable image generation and
make the first attempt to collaborate them for automatic data augmentation for
a variety of downstream tasks. Finally, we present fascinating results obtained
from our ChatGenImage framework and demonstrate the powerful potential of our
synthetic data for systematic vision adaptation. Our codes are available at
https://github.com/Yuqifan1117/Labal-Anything-Pipeline.",None,-1
Precise Benchmarking of Explainable AI Attribution Methods,0.102041,"The rationale behind a deep learning model's output is often difficult to
understand by humans. EXplainable AI (XAI) aims at solving this by developing
methods that improve interpretability and explainability of machine learning
models. Reliable evaluation metrics are needed to assess and compare different
XAI methods. We propose a novel evaluation approach for benchmarking
state-of-the-art XAI attribution methods. Our proposal consists of a synthetic
classification model accompanied by its derived ground truth explanations
allowing high precision representation of input nodes contributions. We also
propose new high-fidelity metrics to quantify the difference between
explanations of the investigated XAI method and those derived from the
synthetic model. Our metrics allow assessment of explanations in terms of
precision and recall separately. Also, we propose metrics to independently
evaluate negative or positive contributions of inputs. Our proposal provides
deeper insights into XAI methods output. We investigate our proposal by
constructing a synthetic convolutional image classification model and
benchmarking several widely used XAI attribution methods using our evaluation
approach. We compare our results with established prior XAI evaluation metrics.
By deriving the ground truth directly from the constructed model in our method,
we ensure the absence of bias, e.g., subjective either based on the training
set. Our experimental results provide novel insights into the performance of
Guided-Backprop and Smoothgrad XAI methods that are widely in use. Both have
good precision and recall scores among positively contributing pixels (0.7,
0.76 and 0.7, 0.77, respectively), but poor precision scores among negatively
contributing pixels (0.44, 0.61 and 0.47, 0.75, resp.). The recall scores in
the latter case remain close. We show that our metrics are among the fastest in
terms of execution time.",None,-1
Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders,0.580866,"The injection of syntactic information in Variational AutoEncoders (VAEs) has
been shown to result in an overall improvement of performances and
generalisation. An effective strategy to achieve such a goal is to separate the
encoding of distributional semantic features and syntactic structures into
heterogeneous latent spaces via multi-task learning or dual encoder
architectures. However, existing works employing such techniques are limited to
LSTM-based VAEs. In this paper, we investigate latent space separation methods
for structural syntactic injection in Transformer-based VAE architectures
(i.e., Optimus). Specifically, we explore how syntactic structures can be
leveraged in the encoding stage through the integration of graph-based and
sequential models, and how multiple, specialised latent representations can be
injected into the decoder's attention mechanism via low-rank operators. Our
empirical evaluation, carried out on natural language sentences and
mathematical expressions, reveals that the proposed end-to-end VAE architecture
can result in a better overall organisation of the latent space, alleviating
the information loss occurring in standard VAE setups, resulting in enhanced
performances on language modelling and downstream generation tasks.",None,-1
Predictive Heterogeneity: Measures and Applications,0.0716784,"As an intrinsic and fundamental property of big data, data heterogeneity
exists in a variety of real-world applications, such as precision medicine,
autonomous driving, financial applications, etc. For machine learning
algorithms, the ignorance of data heterogeneity will greatly hurt the
generalization performance and the algorithmic fairness, since the prediction
mechanisms among different sub-populations are likely to differ from each
other. In this work, we focus on the data heterogeneity that affects the
prediction of machine learning models, and firstly propose the \emph{usable
predictive heterogeneity}, which takes into account the model capacity and
computational constraints. We prove that it can be reliably estimated from
finite data with probably approximately correct (PAC) bounds. Additionally, we
design a bi-level optimization algorithm to explore the usable predictive
heterogeneity from data. Empirically, the explored heterogeneity provides
insights for sub-population divisions in income prediction, crop yield
prediction and image classification tasks, and leveraging such heterogeneity
benefits the out-of-distribution generalization performance.",None,-1
Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs,0.620706,"Dynamic scene graphs generated from video clips could help enhance the
semantic visual understanding in a wide range of challenging tasks such as
environmental perception, autonomous navigation, and task planning of
self-driving vehicles and mobile robots. In the process of temporal and spatial
modeling during dynamic scene graph generation, it is particularly intractable
to learn time-variant relations in dynamic scene graphs among frames. In this
paper, we propose a Time-variant Relation-aware TRansformer (TR$^2$), which
aims to model the temporal change of relations in dynamic scene graphs.
Explicitly, we leverage the difference of text embeddings of prompted sentences
about relation labels as the supervision signal for relations. In this way,
cross-modality feature guidance is realized for the learning of time-variant
relations. Implicitly, we design a relation feature fusion module with a
transformer and an additional message token that describes the difference
between adjacent frames. Extensive experiments on the Action Genome dataset
prove that our TR$^2$ can effectively model the time-variant relations. TR$^2$
significantly outperforms previous state-of-the-art methods under two different
settings by 2.1% and 2.6% respectively.",None,-1
Semantic Channel Equalizer: Modelling Language Mismatch in Multi-User Semantic Communications,0.700326,"We consider a multi-user semantic communications system in which agents
(transmitters and receivers) interact through the exchange of semantic messages
to convey meanings. In this context, languages are instrumental in structuring
the construction and consolidation of knowledge, influencing conceptual
representation and semantic extraction and interpretation. Yet, the crucial
role of languages in semantic communications is often overlooked. When this is
not the case, agent languages are assumed compatible and unambiguously
interoperable, ignoring practical limitations that may arise due to language
mismatching. This is the focus of this work. When agents use distinct
languages, message interpretation is prone to semantic noise resulting from
critical distortion introduced by semantic channels. To address this problem,
this paper proposes a new semantic channel equalizer to counteract and limit
the critical ambiguity in message interpretation. Our proposed solution models
the mismatch of languages with measurable transformations over semantic
representation spaces. We achieve this using optimal transport theory, where we
model such transformations as transportation maps. Then, to recover at the
receiver the meaning intended by the teacher we operate semantic equalization
to compensate for the transformation introduced by the semantic channel, either
before transmission and/or after the reception of semantic messages. We
implement the proposed approach as an operation over a codebook of
transformations specifically designed for successful communication. Numerical
results show that the proposed semantic channel equalizer outperforms
traditional approaches in terms of operational complexity and transmission
accuracy.",None,-1
"Wireless Channel Charting: Theory, Practice, and Applications",0.623191,"Channel charting is a recently proposed framework that applies dimensionality
reduction to channel state information (CSI) in wireless systems with the goal
of associating a pseudo-position to each mobile user in a low-dimensional
space: the channel chart. Channel charting summarizes the entire CSI dataset in
a self-supervised manner, which opens up a range of applications that are tied
to user location. In this article, we introduce the theoretical underpinnings
of channel charting and present an overview of recent algorithmic developments
and experimental results obtained in the field. We furthermore discuss concrete
application examples of channel charting to network- and user-related
applications, and we provide a perspective on future developments and
challenges as well as the role of channel charting in next-generation wireless
networks.",None,-1
NerfAcc: Efficient Sampling Accelerates NeRFs,0.962804,"Optimizing and rendering Neural Radiance Fields is computationally expensive
due to the vast number of samples required by volume rendering. Recent works
have included alternative sampling approaches to help accelerate their methods,
however, they are often not the focus of the work. In this paper, we
investigate and compare multiple sampling approaches and demonstrate that
improved sampling is generally applicable across NeRF variants under an unified
concept of transmittance estimator. To facilitate future experiments, we
develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating
advanced sampling methods into NeRF related methods. We demonstrate its
flexibility by showing that it can reduce the training time of several recent
NeRF methods by 1.5x to 20x with minimal modifications to the existing
codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be
implemented in native PyTorch using NerfAcc.",None,-1
Active Visual Exploration Based on Attention-Map Entropy,0.41033,"Active visual exploration addresses the issue of limited sensor capabilities
in real-world scenarios, where successive observations are actively chosen
based on the environment. To tackle this problem, we introduce a new technique
called Attention-Map Entropy (AME). It leverages the internal uncertainty of
the transformer-based model to determine the most informative observations. In
contrast to existing solutions, it does not require additional loss components,
which simplifies the training. Through experiments, which also mimic
retina-like sensors, we show that such simplified training significantly
improves the performance of reconstruction, segmentation and classification on
publicly available datasets.",None,-1
Chasing Consistency in Text-to-3D Generation from a Single Image,0.500005,"Text-to-3D generation from a single-view image is a popular but challenging
task in 3D vision. Although numerous methods have been proposed, existing works
still suffer from the inconsistency issues, including 1) semantic
inconsistency, 2) geometric inconsistency, and 3) saturation inconsistency,
resulting in distorted, overfitted, and over-saturated generations. In light of
the above issues, we present Consist3D, a three-stage framework Chasing for
semantic-, geometric-, and saturation-Consistent Text-to-3D generation from a
single image, in which the first two stages aim to learn parameterized
consistency tokens, and the last stage is for optimization. Specifically, the
semantic encoding stage learns a token independent of views and estimations,
promoting semantic consistency and robustness. Meanwhile, the geometric
encoding stage learns another token with comprehensive geometry and
reconstruction constraints under novel-view estimations, reducing overfitting
and encouraging geometric consistency. Finally, the optimization stage benefits
from the semantic and geometric tokens, allowing a low classifier-free guidance
scale and therefore preventing oversaturation. Experimental results demonstrate
that Consist3D produces more consistent, faithful, and photo-realistic 3D
assets compared to previous state-of-the-art methods. Furthermore, Consist3D
also allows background and object editing through text prompts.",None,-1
NLP Workbench: Efficient and Extensible Integration of State-of-the-art Text Mining Tools,0.120631,"NLP Workbench is a web-based platform for text mining that allows non-expert
users to obtain semantic understanding of large-scale corpora using
state-of-the-art text mining models. The platform is built upon latest
pre-trained models and open source systems from academia that provide semantic
analysis functionalities, including but not limited to entity linking,
sentiment analysis, semantic parsing, and relation extraction. Its extensible
design enables researchers and developers to smoothly replace an existing model
or integrate a new one. To improve efficiency, we employ a microservice
architecture that facilitates allocation of acceleration hardware and
parallelization of computation. This paper presents the architecture of NLP
Workbench and discusses the challenges we faced in designing it. We also
discuss diverse use cases of NLP Workbench and the benefits of using it over
other approaches. The platform is under active development, with its source
code released under the MIT license. A website and a short video demonstrating
our platform are also available.",None,-1
Message Ritual: A Posthuman Account of Living with Lamp,0.534128,"As we become increasingly entangled with digital technologies, the boundary
between human and machine is progressively blurring. Adopting a performative,
posthumanist perspective resolves this ambiguity by proposing that such
boundaries are not predetermined, rather they are enacted within a certain
material configuration. Using this approach, dubbed `Entanglement HCI', this
paper presents \emph{Message Ritual} -- a novel, integrated AI system that
encourages the re-framing of memory through machine generated poetics. Embodied
within a domestic table lamp, the system listens in on conversations occurring
within the home, drawing out key topics and phrases of the day and
reconstituting them through machine generated poetry, delivered to household
members via SMS upon waking each morning. Participants across four households
were asked to live with the lamp over a two week period. We present a
diffractive analysis exploring how the lamp \emph{becomes with} participants
and discuss the implications of this method for future HCI research.",None,-1
Overwriting Pretrained Bias with Finetuning Data,0.406082,"Transfer learning is beneficial by allowing the expressive features of models
pretrained on large-scale datasets to be finetuned for the target task of
smaller, more domain-specific datasets. However, there is a concern that these
pretrained models may come with their own biases which would propagate into the
finetuned model. In this work, we investigate bias when conceptualized as both
spurious correlations between the target task and a sensitive attribute as well
as underrepresentation of a particular group in the dataset. Under both notions
of bias, we find that (1) models finetuned on top of pretrained models can
indeed inherit their biases, but (2) this bias can be corrected for through
relatively minor interventions to the finetuning dataset, and often with a
negligible impact to performance. Our findings imply that careful curation of
the finetuning dataset is important for reducing biases on a downstream task,
and doing so can even compensate for bias in the pretrained model.",None,-1
The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing,0.200765,"In this work, we analyse the role of output vocabulary for text-to-text (T2T)
models on the task of SPARQL semantic parsing. We perform experiments within
the the context of knowledge graph question answering (KGQA), where the task is
to convert questions in natural language to the SPARQL query language. We
observe that the query vocabulary is distinct from human vocabulary. Language
Models (LMs) are pre-dominantly trained for human language tasks, and hence, if
the query vocabulary is replaced with a vocabulary more attuned to the LM
tokenizer, the performance of models may improve. We carry out carefully
selected vocabulary substitutions on the queries and find absolute gains in the
range of 17% on the GrailQA dataset.",None,-1
NeutrEx: A 3D Quality Component Measure on Facial Expression Neutrality,0.241487,"Accurate face recognition systems are increasingly important in sensitive
applications like border control or migration management. Therefore, it becomes
crucial to quantify the quality of facial images to ensure that low-quality
images are not affecting recognition accuracy. In this context, the current
draft of ISO/IEC 29794-5 introduces the concept of component quality to
estimate how single factors of variation affect recognition outcomes. In this
study, we propose a quality measure (NeutrEx) based on the accumulated
distances of a 3D face reconstruction to a neutral expression anchor. Our
evaluations demonstrate the superiority of our proposed method compared to
baseline approaches obtained by training Support Vector Machines on face
embeddings extracted from a pre-trained Convolutional Neural Network for facial
expression classification. Furthermore, we highlight the explainable nature of
our NeutrEx measures by computing per-vertex distances to unveil the most
impactful face regions and allow operators to give actionable feedback to
subjects.",None,-1
DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting,0.523711,"Multi-class cell detection and counting is an essential task for many
pathological diagnoses. Manual counting is tedious and often leads to
inter-observer variations among pathologists. While there exist multiple,
general-purpose, deep learning-based object detection and counting methods,
they may not readily transfer to detecting and counting cells in medical
images, due to the limited data, presence of tiny overlapping objects, multiple
cell types, severe class-imbalance, minute differences in size/shape of cells,
etc. In response, we propose guided posterior regularization (DeGPR), which
assists an object detector by guiding it to exploit discriminative features
among cells. The features may be pathologist-provided or inferred directly from
visual data. We validate our model on two publicly available datasets (CoNSeP
and MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists
of 55 biopsy images of the human duodenum for predicting celiac disease. We
perform extensive experimentation with three object detection baselines on
three datasets to show that DeGPR is model-agnostic, and consistently improves
baselines obtaining up to 9% (absolute) mAP gains.",None,-1
Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers,0.970653,"Abstracts derived from biomedical literature possess distinct domain-specific
characteristics, including specialised writing styles and biomedical
terminologies, which necessitate a deep understanding of the related
literature. As a result, existing language models struggle to generate
technical summaries that are on par with those produced by biomedical experts,
given the absence of domain-specific background knowledge. This paper aims to
enhance the performance of language models in biomedical abstractive
summarisation by aggregating knowledge from external papers cited within the
source article. We propose a novel attention-based citation aggregation model
that integrates domain-specific knowledge from citation papers, allowing neural
networks to generate summaries by leveraging both the paper content and
relevant knowledge from citation papers. Furthermore, we construct and release
a large-scale biomedical summarisation dataset that serves as a foundation for
our research. Extensive experiments demonstrate that our model outperforms
state-of-the-art approaches and achieves substantial improvements in
abstractive biomedical text summarisation.",None,-1
A Novel Approach for Auto-Formulation of Optimization Problems,0.622167,"In the Natural Language for Optimization (NL4Opt) NeurIPS 2022 competition,
competitors focus on improving the accessibility and usability of optimization
solvers, with the aim of subtask 1: recognizing the semantic entities that
correspond to the components of the optimization problem; subtask 2: generating
formulations for the optimization problem. In this paper, we present the
solution of our team. First, we treat subtask 1 as a named entity recognition
(NER) problem with the solution pipeline including pre-processing methods,
adversarial training, post-processing methods and ensemble learning. Besides,
we treat subtask 2 as a generation problem with the solution pipeline including
specially designed prompts, adversarial training, post-processing methods and
ensemble learning. Our proposed methods have achieved the F1-score of 0.931 in
subtask 1 and the accuracy of 0.867 in subtask 2, which won the fourth and
third places respectively in this competition. Our code is available at
https://github.com/bigdata-ustc/nl4opt.",None,-1
Rumor Detection with Diverse Counterfactual Evidence,0.162872,"The growth in social media has exacerbated the threat of fake news to
individuals and communities. This draws increasing attention to developing
efficient and timely rumor detection methods. The prevailing approaches resort
to graph neural networks (GNNs) to exploit the post-propagation patterns of the
rumor-spreading process. However, these methods lack inherent interpretation of
rumor detection due to the black-box nature of GNNs. Moreover, these methods
suffer from less robust results as they employ all the propagation patterns for
rumor detection. In this paper, we address the above issues with the proposed
Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our
intuition is to exploit the diverse counterfactual evidence of an event graph
to serve as multi-view interpretations, which are further aggregated for robust
rumor detection results. Specifically, our method first designs a subgraph
generation strategy to efficiently generate different subgraphs of the event
graph. We constrain the removal of these subgraphs to cause the change in rumor
detection results. Thus, these subgraphs naturally serve as counterfactual
evidence for rumor detection. To achieve multi-view interpretation, we design a
diversity loss inspired by Determinantal Point Processes (DPP) to encourage
diversity among the counterfactual evidence. A GNN-based rumor detection model
further aggregates the diverse counterfactual evidence discovered by the
proposed DCE-RD to achieve interpretable and robust rumor detection results.
Extensive experiments on two real-world datasets show the superior performance
of our method. Our code is available at https://github.com/Vicinity111/DCE-RD.",None,-1
MegaWika: Millions of reports and their sources across 50 diverse languages,0.277874,"To foster the development of new models for collaborative AI-assisted report
generation, we introduce MegaWika, consisting of 13 million Wikipedia articles
in 50 diverse languages, along with their 71 million referenced source
materials. We process this dataset for a myriad of applications, going beyond
the initial Wikipedia citation extraction and web scraping of content,
including translating non-English articles for cross-lingual applications and
providing FrameNet parses for automated semantic analysis. MegaWika is the
largest resource for sentence-level report generation and the only report
generation dataset that is multilingual. We manually analyze the quality of
this resource through a semantically stratified sample. Finally, we provide
baseline results and trained models for crucial steps in automated report
generation: cross-lingual question answering and citation retrieval.",None,-1
Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations,0.933434,"Unlike empathetic dialogues, the system in emotional support conversations
(ESC) is expected to not only convey empathy for comforting the help-seeker,
but also proactively assist in exploring and addressing their problems during
the conversation. In this work, we study the problem of mixed-initiative ESC
where the user and system can both take the initiative in leading the
conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC
systems with a tailor-designed schema that divides utterances into different
types with speaker roles and initiative types. Four emotional support metrics
are proposed to evaluate the mixed-initiative interactions. The analysis
reveals the necessity and challenges of building mixed-initiative ESC systems.
In the light of this, we propose a knowledge-enhanced mixed-initiative
framework (KEMI) for ESC, which retrieves actual case knowledge from a
large-scale mental health knowledge graph for generating mixed-initiative
responses. Experimental results on two ESC datasets show the superiority of
KEMI in both content-preserving evaluation and mixed initiative related
analyses.",None,-1
Generative Models for 3D Point Clouds,0.0737171,"Point clouds are rich geometric data structures, where their three
dimensional structure offers an excellent domain for understanding the
representation learning and generative modeling in 3D space. In this work, we
aim to improve the performance of point cloud latent-space generative models by
experimenting with transformer encoders, latent-space flow models, and
autoregressive decoders. We analyze and compare both generation and
reconstruction performance of these models on various object types.",None,-1
Zero-shot Causal Graph Extrapolation from Text via LLMs,0.808854,"We evaluate the ability of large language models (LLMs) to infer causal
relations from natural language. Compared to traditional natural language
processing and deep learning techniques, LLMs show competitive performance in a
benchmark of pairwise relations without needing (explicit) training samples.
This motivates us to extend our approach to extrapolating causal graphs through
iterated pairwise queries. We perform a preliminary analysis on a benchmark of
biomedical abstracts with ground-truth causal graphs validated by experts. The
results are promising and support the adoption of LLMs for such a crucial step
in causal inference, especially in medical domains, where the amount of
scientific text to analyse might be huge, and the causal statements are often
implicit.",None,-1
Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A Case Study in Taiwanese Hokkien,0.0687832,"In natural language processing (NLP), code-mixing (CM) is a challenging task,
especially when the mixed languages include dialects. In Southeast Asian
countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the
most widespread code-mixed language pair among Chinese immigrants, and it is
also common in Taiwan. However, dialects such as Hokkien often have a scarcity
of resources and the lack of an official writing system, limiting the
development of dialect CM research. In this paper, we propose a method to
construct a Hokkien-Mandarin CM dataset to mitigate the limitation, overcome
the morphological issue under the Sino-Tibetan language family, and offer an
efficient Hokkien word segmentation method through a linguistics-based toolkit.
Furthermore, we use our proposed dataset and employ transfer learning to train
the XLM (cross-lingual language model) for translation tasks. To fit the
code-mixing scenario, we adapt XLM slightly. We found that by using linguistic
knowledge, rules, and language tags, the model produces good results on CM data
translation while maintaining monolingual translation quality.",None,-1
In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making,0.933393,"The current literature on AI-advised decision making -- involving explainable
AI systems advising human decision makers -- presents a series of inconclusive
and confounding results. To synthesize these findings, we propose a simple
theory that elucidates the frequent failure of AI explanations to engender
appropriate reliance and complementary decision making performance. We argue
explanations are only useful to the extent that they allow a human decision
maker to verify the correctness of an AI's prediction, in contrast to other
desiderata, e.g., interpretability or spelling out the AI's reasoning process.
Prior studies find in many decision making contexts AI explanations do not
facilitate such verification. Moreover, most tasks fundamentally do not allow
easy verification, regardless of explanation method, limiting the potential
benefit of any type of explanation. We also compare the objective of
complementary performance with that of appropriate reliance, decomposing the
latter into the notions of outcome-graded and strategy-graded reliance.",None,-1
Contrast with Major Classifier Vectors for Federated Medical Relation Extraction with Heterogeneous Label Distribution,0.125436,"Federated medical relation extraction enables multiple clients to train a
deep network collaboratively without sharing their raw medical data. In order
to handle the heterogeneous label distribution across clients, most of the
existing works only involve enforcing regularization between local and global
models during optimization. In this paper, we fully utilize the models of all
clients and propose a novel concept of \textit{major classifier vectors}, where
a group of class vectors is obtained in an ensemble rather than the weighted
average method on the server. The major classifier vectors are then distributed
to all clients and the local training of each client is Contrasted with Major
Classifier vectors (FedCMC), so the local model is not prone to overfitting to
the local label distribution. FedCMC requires only a small amount of additional
transfer of classifier parameters without any leakage of raw data, extracted
representations, and label distributions. Our extensive experiments show that
FedCMC outperforms the other state-of-the-art FL algorithms on three medical
relation extraction datasets.",None,-1
Emotion-Cause Pair Extraction as Question Answering,0.716379,"The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all
potential emotion-cause pairs of a document without any annotation of emotion
or cause clauses. Previous approaches on ECPE have tried to improve
conventional two-step processing schemes by using complex architectures for
modeling emotion-cause interaction. In this paper, we cast the ECPE task to the
question answering (QA) problem and propose simple yet effective BERT-based
solutions to tackle it. Given a document, our Guided-QA model first predicts
the best emotion clause using a fixed question. Then the predicted emotion is
used as a question to predict the most potential cause for the emotion. We
evaluate our model on a standard ECPE corpus. The experimental results show
that despite its simplicity, our Guided-QA achieves promising results and is
easy to reproduce. The code of Guided-QA is also provided.",None,-1
Feature Tracks are not Zero-Mean Gaussian,0.0525362,"In state estimation algorithms that use feature tracks as input, it is
customary to assume that the errors in feature track positions are zero-mean
Gaussian. Using a combination of calibrated camera intrinsics, ground-truth
camera pose, and depth images, it is possible to compute ground-truth positions
for feature tracks extracted using an image processing algorithm. We find that
feature track errors are not zero-mean Gaussian and that the distribution of
errors is conditional on the type of motion, the speed of motion, and the image
processing algorithm used to extract the tracks.",None,-1
Point2Tree(P2T) -- framework for parameter tuning of semantic and instance segmentation used with mobile laser scanning data in coniferous forest,0.696624,"This article introduces Point2Tree, a novel framework that incorporates a
three-stage process involving semantic segmentation, instance segmentation,
optimization analysis of hyperparemeters importance. It introduces a
comprehensive and modular approach to processing laser points clouds in
Forestry. We tested it on two independent datasets. The first area was located
in an actively managed boreal coniferous dominated forest in V{\aa}ler, Norway,
16 circular plots of 400 square meters were selected to cover a range of forest
conditions in terms of species composition and stand density. We trained a
model based on Pointnet++ architecture which achieves 0.92 F1-score in semantic
segmentation. As a second step in our pipeline we used graph-based approach for
instance segmentation which reached F1-score approx. 0.6. The optimization
allowed to further boost the performance of the pipeline by approx. 4 \%
points.",None,-1
RelPose++: Recovering 6D Poses from Sparse-view Observations,0.982951,"We address the task of estimating 6D camera poses from sparse-view image sets
(2-8 images). This task is a vital pre-processing stage for nearly all
contemporary (neural) reconstruction algorithms but remains challenging given
sparse views, especially for objects with visual symmetries and texture-less
surfaces. We build on the recent RelPose framework which learns a network that
infers distributions over relative rotations over image pairs. We extend this
approach in two key ways; first, we use attentional transformer layers to
process multiple images jointly, since additional views of an object may
resolve ambiguous symmetries in any given image pair (such as the handle of a
mug that becomes visible in a third view). Second, we augment this network to
also report camera translations by defining an appropriate coordinate system
that decouples the ambiguity in rotation estimation from translation
prediction. Our final system results in large improvements in 6D pose
prediction over prior art on both seen and unseen object categories and also
enables pose estimation and 3D reconstruction for in-the-wild objects.",None,-1
SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,0.753719,"Visual question answering on document images that contain textual, visual,
and layout information, called document VQA, has received much attention
recently. Although many datasets have been proposed for developing document VQA
systems, most of the existing datasets focus on understanding the content
relationships within a single image and not across multiple images. In this
study, we propose a new multi-image document VQA dataset, SlideVQA, containing
2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a
slide deck. SlideVQA requires complex reasoning, including single-hop,
multi-hop, and numerical reasoning, and also provides annotated arithmetic
expressions of numerical answers for enhancing the ability of numerical
reasoning. Moreover, we developed a new end-to-end document VQA model that
treats evidence selection and question answering in a unified
sequence-to-sequence format. Experiments on SlideVQA show that our model
outperformed existing state-of-the-art QA models, but that it still has a large
gap behind human performance. We believe that our dataset will facilitate
research on document VQA.",None,-1
A Unified Conditional Framework for Diffusion-based Image Restoration,0.726608,"Diffusion Probabilistic Models (DPMs) have recently shown remarkable
performance in image generation tasks, which are capable of generating highly
realistic images. When adopting DPMs for image restoration tasks, the crucial
aspect lies in how to integrate the conditional information to guide the DPMs
to generate accurate and natural output, which has been largely overlooked in
existing works. In this paper, we present a unified conditional framework based
on diffusion models for image restoration. We leverage a lightweight UNet to
predict initial guidance and the diffusion model to learn the residual of the
guidance. By carefully designing the basic module and integration module for
the diffusion model block, we integrate the guidance and other auxiliary
conditional information into every block of the diffusion model to achieve
spatially-adaptive generation conditioning. To handle high-resolution images,
we propose a simple yet effective inter-step patch-splitting strategy to
produce arbitrary-resolution images without grid artifacts. We evaluate our
conditional framework on three challenging tasks: extreme low-light denoising,
deblurring, and JPEG restoration, demonstrating its significant improvements in
perceptual quality and the generalization to restoration tasks.",None,-1
Art or Artifice? Large Language Models and the False Promise of Creativity,0.274,"Researchers have argued that large language models (LLMs) exhibit
high-quality writing capabilities from blogs to stories. However, evaluating
objectively the creativity of a piece of writing is challenging. Inspired by
the Torrance Test of Creative Thinking (TTCT), which measures creativity as a
process, we use the Consensual Assessment Technique [3] and propose the
Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product.
TTCW consists of 14 binary tests organized into the original dimensions of
Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative
writers and implement a human assessment of 48 stories written either by
professional authors or LLMs using TTCW. Our analysis shows that LLM-generated
stories pass 3-10X less TTCW tests than stories written by professionals. In
addition, we explore the use of LLMs as assessors to automate the TTCW
evaluation, revealing that none of the LLMs positively correlate with the
expert assessments.",None,-1
SAT-Based PAC Learning of Description Logic Concepts,0.368482,"We propose bounded fitting as a scheme for learning description logic
concepts in the presence of ontologies. A main advantage is that the resulting
learning algorithms come with theoretical guarantees regarding their
generalization to unseen examples in the sense of PAC learning. We prove that,
in contrast, several other natural learning algorithms fail to provide such
guarantees. As a further contribution, we present the system SPELL which
efficiently implements bounded fitting for the description logic
$\mathcal{ELH}^r$ based on a SAT solver, and compare its performance to a
state-of-the-art learner.",None,-1
SLiC-HF: Sequence Likelihood Calibration with Human Feedback,1.0,"Learning from human feedback has been shown to be effective at aligning
language models with human preferences. Past work has often relied on
Reinforcement Learning from Human Feedback (RLHF), which optimizes the language
model using reward scores assigned from a reward model trained on human
preference data. In this work we show how the recently introduced Sequence
Likelihood Calibration (SLiC), can also be used to effectively learn from human
preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human
feedback data collected for a different model, similar to off-policy, offline
RL data. Automatic and human evaluation experiments on the TL;DR summarization
task show that SLiC-HF significantly improves supervised fine-tuning baselines.
Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF
implementation used in past work while being much simpler to implement, easier
to tune and more computationally efficient in practice.",None,-1
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs,0.864471,"The performance of large language models (LLMs) has recently improved to the
point where the models can perform well on many language tasks. We show here
that for the first time, the models can also generate coherent and valid formal
analyses of linguistic data and illustrate the vast potential of large language
models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their
metalinguistic abilities improves our understanding of their general
capabilities and sheds new light on theoretical models in linguistics. In this
paper, we probe into GPT-4's metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a
research program for metalinguistic analyses of large language models, propose
experimental designs, provide general guidelines, discuss limitations, and
offer future directions for this line of research. This line of inquiry also
exemplifies behavioral interpretability of deep learning, where models'
representations are accessed by explicit prompting rather than internal
representations.",None,-1
GDBN: a Graph Neural Network Approach to Dynamic Bayesian Network,0.180357,"Identifying causal relations among multi-variate time series is one of the
most important elements towards understanding the complex mechanisms underlying
the dynamic system. It provides critical tools for forecasting, simulations and
interventions in science and business analytics. In this paper, we proposed a
graph neural network approach with score-based method aiming at learning a
sparse DAG that captures the causal dependencies in a discretized time temporal
graph. We demonstrate methods with graph neural network significantly
outperformed other state-of-the-art methods with dynamic bayesian networking
inference. In addition, from the experiments, the structural causal model can
be more accurate than a linear SCM discovered by the methods such as Notears.",None,-1
Controllable Mixed-Initiative Dialogue Generation through Prompting,0.247264,"Mixed-initiative dialogue tasks involve repeated exchanges of information and
conversational control. Conversational agents gain control by generating
responses that follow particular dialogue intents or strategies, prescribed by
a policy planner. The standard approach has been fine-tuning pre-trained
language models to perform generation conditioned on these intents. However,
these supervised generation models are limited by the cost and quality of data
annotation. We instead prompt large language models as a drop-in replacement to
fine-tuning on conditional generation. We formalize prompt construction for
controllable mixed-initiative dialogue. Our findings show improvements over
fine-tuning and ground truth responses according to human evaluation and
automatic metrics for two tasks: PersuasionForGood and Emotional Support
Conversations.",None,-1
HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation,0.423082,"The Shapley value is widely regarded as a trustworthy attribution metric.
However, when people use Shapley values to explain the attribution of input
variables of a deep neural network (DNN), it usually requires a very high
computational cost to approximate relatively accurate Shapley values in
real-world applications. Therefore, we propose a novel network architecture,
the HarsanyiNet, which makes inferences on the input sample and simultaneously
computes the exact Shapley values of the input variables in a single forward
propagation. The HarsanyiNet is designed on the theoretical foundation that the
Shapley value can be reformulated as the redistribution of Harsanyi
interactions encoded by the network.",None,-1
$\varepsilon$ K <MASK>: Integrating Yorb cultural greetings into machine translation,0.338169,"This paper investigates the performance of massively multilingual neural
machine translation (NMT) systems in translating Yor\`ub\'a greetings
($\varepsilon$ k\'u [MASK]), which are a big part of Yor\`ub\'a language and
culture, into English. To evaluate these models, we present IkiniYor\`ub\'a, a
Yor\`ub\'a-English translation dataset containing some Yor\`ub\'a greetings,
and sample use cases. We analysed the performance of different multilingual NMT
systems including Google and NLLB and show that these models struggle to
accurately translate Yor\`ub\'a greetings into English. In addition, we trained
a Yor\`ub\'a-English model by finetuning an existing NMT model on the training
split of IkiniYor\`ub\'a and this achieved better performance when compared to
the pre-trained multilingual NMT models, although they were trained on a large
volume of data.",None,-1
BiLMa: Bidirectional Local-Matching for Text-based Person Re-identification,0.826398,"Text-based person re-identification (TBPReID) aims to retrieve person images
represented by a given textual query. In this task, how to effectively align
images and texts globally and locally is a crucial challenge. Recent works have
obtained high performances by solving Masked Language Modeling (MLM) to align
image/text parts. However, they only performed uni-directional (i.e., from
image to text) local-matching, leaving room for improvement by introducing
opposite-directional (i.e., from text to image) local-matching. In this work,
we introduce Bidirectional Local-Matching (BiLMa) framework that jointly
optimize MLM and Masked Image Modeling (MIM) in TBPReID model training. With
this framework, our model is trained so as the labels of randomly masked both
image and text tokens are predicted by unmasked tokens. In addition, to narrow
the semantic gap between image and text in MIM, we propose Semantic MIM
(SemMIM), in which the labels of masked image tokens are automatically given by
a state-of-the-art human parser. Experimental results demonstrate that our
BiLMa framework with SemMIM achieves state-of-the-art Rank@1 and mAP scores on
three benchmarks.",None,-1
Stable Yaw Estimation of Boats from the Viewpoint of UAVs and USVs,0.0658819,"Yaw estimation of boats from the viewpoint of unmanned aerial vehicles (UAVs)
and unmanned surface vehicles (USVs) or boats is a crucial task in various
applications such as 3D scene rendering, trajectory prediction, and navigation.
However, the lack of literature on yaw estimation of objects from the viewpoint
of UAVs has motivated us to address this domain. In this paper, we propose a
method based on HyperPosePDF for predicting the orientation of boats in the 6D
space. For that, we use existing datasets, such as PASCAL3D+ and our own
datasets, SeaDronesSee-3D and BOArienT, which we annotated manually. We extend
HyperPosePDF to work in video-based scenarios, such that it yields robust
orientation predictions across time. Naively applying HyperPosePDF on video
data yields single-point predictions, resulting in far-off predictions and
often incorrect symmetric orientations due to unseen or visually different
data. To alleviate this issue, we propose aggregating the probability
distributions of pose predictions, resulting in significantly improved
performance, as shown in our experimental evaluation. Our proposed method could
significantly benefit downstream tasks in marine robotics.",None,-1
Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?,0.487268,"To decipher the algorithm underlying the human brain's language
representation, previous work probed brain responses to language input with
pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks.
However, full fine-tuning generally updates the entire parametric space and
distorts pre-trained features, cognitively inconsistent with the brain's robust
multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained
weights and learns task-specific embeddings to fit a task. Could prompt-tuning
generate representations that better account for the brain's language
representations than fine-tuning? If so, what kind of NLU task leads a
pre-trained model to better decode the information represented in the human
brain? We investigate these questions by comparing prompt-tuned and fine-tuned
representations in neural decoding, that is predicting the linguistic stimulus
from the brain activities evoked by the stimulus. We find that on none of the
10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning in
neural decoding, implicating that a more brain-consistent tuning method yields
representations that better correlate with brain data. Moreover, we identify
that tasks dealing with fine-grained concept meaning yield representations that
better decode brain activation patterns than other tasks, especially the
syntactic chunking task. This indicates that our brain encodes more
fine-grained concept information than shallow syntactic information when
representing languages.",None,-1
Enhancing Embedding Representations of Biomedical Data using Logic Knowledge,0.0823712,"Knowledge Graph Embeddings (KGE) have become a quite popular class of models
specifically devised to deal with ontologies and graph structure data, as they
can implicitly encode statistical dependencies between entities and relations
in a latent space. KGE techniques are particularly effective for the biomedical
domain, where it is quite common to deal with large knowledge graphs underlying
complex interactions between biological and chemical objects. Recently in the
literature, the PharmKG dataset has been proposed as one of the most
challenging knowledge graph biomedical benchmark, with hundreds of thousands of
relational facts between genes, diseases and chemicals. Despite KGEs can scale
to very large relational domains, they generally fail at representing more
complex relational dependencies between facts, like logic rules, which may be
fundamental in complex experimental settings. In this paper, we exploit logic
rules to enhance the embedding representations of KGEs on the PharmKG dataset.
To this end, we adopt Relational Reasoning Network (R2N), a recently proposed
neural-symbolic approach showing promising results on knowledge graph
completion tasks. An R2N uses the available logic rules to build a neural
architecture that reasons over KGE latent representations. In the experiments,
we show that our approach is able to significantly improve the current
state-of-the-art on the PharmKG dataset. Finally, we provide an ablation study
to experimentally compare the effect of alternative sets of rules according to
different selection criteria and varying the number of considered rules.",None,-1
Knowledge Engineering using Large Language Models,0.684334,"Knowledge engineering is a discipline that focuses on the creation and
maintenance of processes that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on knowledge expressed in formal
languages. The emergence of large language models and their capabilities to
effectively work with natural language, in its broadest sense, raises questions
about the foundations and practice of knowledge engineering. Here, we outline
the potential role of LLMs in knowledge engineering, identifying two central
directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2)
enabling knowledge engineering in natural language. Additionally, we formulate
key open research questions to tackle these directions.",None,-1
How to prepare your task head for finetuning,0.233644,"In deep learning, transferring information from a pretrained network to a
downstream task by finetuning has many benefits. The choice of task head plays
an important role in fine-tuning, as the pretrained and downstream tasks are
usually different. Although there exist many different designs for finetuning,
a full understanding of when and why these algorithms work has been elusive. We
analyze how the choice of task head controls feature adaptation and hence
influences the downstream performance. By decomposing the learning dynamics of
adaptation, we find that the key aspect is the training accuracy and loss at
the beginning of finetuning, which determines the ""energy"" available for the
feature's adaptation. We identify a significant trend in the effect of changes
in this initial energy on the resulting features after fine-tuning.
Specifically, as the energy increases, the Euclidean and cosine distances
between the resulting and original features increase, while their dot products
(and the resulting features' norm) first increase and then decrease. Inspired
by this, we give several practical principles that lead to better downstream
performance. We analytically prove this trend in an overparamterized linear
setting and verify its applicability to different experimental settings.",None,-1
Parkinson gait modelling from an anomaly deep representation,0.217655,"Parkinson's Disease (PD) is associated with gait movement disorders, such as
bradykinesia, stiffness, tremors and postural instability, caused by
progressive dopamine deficiency. Today, some approaches have implemented
learning representations to quantify kinematic patterns during locomotion,
supporting clinical procedures such as diagnosis and treatment planning. These
approaches assumes a large amount of stratified and labeled data to optimize
discriminative representations. Nonetheless these considerations may restrict
the approaches to be operable in real scenarios during clinical practice. This
work introduces a self-supervised generative representation to learn
gait-motion-related patterns, under the pretext of video reconstruction and an
anomaly detection framework. This architecture is trained following a one-class
weakly supervised learning to avoid inter-class variance and approach the
multiple relationships that represent locomotion. The proposed approach was
validated with 14 PD patients and 23 control subjects, and trained with the
control population only, achieving an AUC of 95%, homocedasticity level of 70%
and shapeness level of 70% in the classification task considering its
generalization.",None,-1
Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations,0.877148,"In this paper, we advocate for using large pre-trained monolingual language
models in cross lingual zero-shot word sense disambiguation (WSD) coupled with
a contextualized mapping mechanism. We also report rigorous experiments that
illustrate the effectiveness of employing sparse contextualized word
representations obtained via a dictionary learning procedure. Our experimental
results demonstrate that the above modifications yield a significant
improvement of nearly 6.5 points of increase in the average F-score (from 62.0
to 68.5) over a collection of 17 typologically diverse set of target languages.
We release our source code for replicating our experiments at
https://github.com/begab/sparsity_makes_sense.",None,-1
PointWavelet: Learning in Spectral Domain for 3D Point Cloud Analysis,0.154608,"With recent success of deep learning in 2D visual recognition, deep
learning-based 3D point cloud analysis has received increasing attention from
the community, especially due to the rapid development of autonomous driving
technologies. However, most existing methods directly learn point features in
the spatial domain, leaving the local structures in the spectral domain poorly
investigated. In this paper, we introduce a new method, PointWavelet, to
explore local graphs in the spectral domain via a learnable graph wavelet
transform. Specifically, we first introduce the graph wavelet transform to form
multi-scale spectral graph convolution to learn effective local structural
representations. To avoid the time-consuming spectral decomposition, we then
devise a learnable graph wavelet transform, which significantly accelerates the
overall training process. Extensive experiments on four popular point cloud
datasets, ModelNet40, ScanObjectNN, ShapeNet-Part, and S3DIS, demonstrate the
effectiveness of the proposed method on point cloud classification and
segmentation.",None,-1
In-Context Impersonation Reveals Large Language Models' Strengths and Biases,0.424797,"In everyday conversations, humans can take on different roles and adapt their
vocabulary to their chosen roles. We explore whether LLMs can take on, that is
impersonate, different roles when they generate text in-context. We ask LLMs to
assume different personas before solving vision and language tasks. We do this
by prefixing the prompt with a persona that is associated either with a social
identity or domain expertise. In a multi-armed bandit task, we find that LLMs
pretending to be children of different ages recover human-like developmental
stages of exploration. In a language-based reasoning task, we find that LLMs
impersonating domain experts perform better than LLMs impersonating non-domain
experts. Finally, we test whether LLMs' impersonations are complementary to
visual information when describing different categories. We find that
impersonation can improve performance: an LLM prompted to be a bird expert
describes birds better than one prompted to be a car expert. However,
impersonation can also uncover LLMs' biases: an LLM prompted to be a man
describes cars better than one prompted to be a woman. These findings
demonstrate that LLMs are capable of taking on diverse roles and that this
in-context impersonation can be used to uncover their hidden strengths and
biases.",None,-1
Language Models are Few-shot Learners for Prognostic Prediction,0.967815,"Clinical prediction is an essential task in the healthcare industry. However,
the recent success of transformers, on which large language models are built,
has not been extended to this domain. In this research, we explore the use of
transformers and language models in prognostic prediction for immunotherapy
using real-world patients' clinical data and molecular profiles. This paper
investigates the potential of transformers to improve clinical prediction
compared to conventional machine learning approaches and addresses the
challenge of few-shot learning in predicting rare disease areas. The study
benchmarks the efficacy of baselines and language models on prognostic
prediction across multiple cancer types and investigates the impact of
different pretrained language models under few-shot regimes. The results
demonstrate significant improvements in accuracy and highlight the potential of
NLP in clinical research to improve early detection and intervention for
different diseases.",None,-1
Not Only Generative Art: Stable Diffusion for Content-Style Disentanglement in Art Analysis,0.371777,"The duality of content and style is inherent to the nature of art. For
humans, these two elements are clearly different: content refers to the objects
and concepts in the piece of art, and style to the way it is expressed. This
duality poses an important challenge for computer vision. The visual appearance
of objects and concepts is modulated by the style that may reflect the author's
emotions, social trends, artistic movement, etc., and their deep comprehension
undoubtfully requires to handle both. A promising step towards a general
paradigm for art analysis is to disentangle content and style, whereas relying
on human annotations to cull a single aspect of artworks has limitations in
learning semantic concepts and the visual appearance of paintings. We thus
present GOYA, a method that distills the artistic knowledge captured in a
recent generative model to disentangle content and style. Experiments show that
synthetically generated images sufficiently serve as a proxy of the real
distribution of artworks, allowing GOYA to separately represent the two
elements of art while keeping more information than existing methods.",None,-1
Accelerated Coordinate Encoding: Learning to Relocalize in Minutes using RGB and Poses,0.282319,"Learning-based visual relocalizers exhibit leading pose accuracy, but require
hours or days of training. Since training needs to happen on each new scene
again, long training times make learning-based relocalization impractical for
most applications, despite its promise of high accuracy. In this paper we show
how such a system can actually achieve the same accuracy in less than 5
minutes. We start from the obvious: a relocalization network can be split in a
scene-agnostic feature backbone, and a scene-specific prediction head. Less
obvious: using an MLP prediction head allows us to optimize across thousands of
view points simultaneously in each single training iteration. This leads to
stable and extremely fast convergence. Furthermore, we substitute effective but
slow end-to-end training using a robust pose solver with a curriculum over a
reprojection loss. Our approach does not require privileged knowledge, such a
depth maps or a 3D model, for speedy training. Overall, our approach is up to
300x faster in mapping than state-of-the-art scene coordinate regression, while
keeping accuracy on par.",None,-1
ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection,0.464425,"The task of multi-author writing style detection aims at finding any
positions of writing style change in a given text document. We formulate the
task as a natural language inference problem where two consecutive paragraphs
are paired. Our approach focuses on transitions between paragraphs while
truncating input tokens for the task. As backbone models, we employ different
Transformer-based encoders with warmup phase during training. We submit the
model version that outperforms baselines and other proposed model versions in
our experiments. For the easy and medium setups, we submit transition-focused
natural language inference based on DeBERTa with warmup training, and the same
model without transition for the hard setup.",None,-1
Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,0.348089,"Recently, large pretrained language models have demonstrated strong language
understanding capabilities. This is particularly reflected in their zero-shot
and in-context learning abilities on downstream tasks through prompting. To
assess their impact on spoken language understanding (SLU), we evaluate several
such models like ChatGPT and OPT of different sizes on multiple benchmarks. We
verify the emergent ability unique to the largest models as they can reach
intent classification accuracy close to that of supervised models with zero or
few shots on various languages given oracle transcripts. By contrast, the
results for smaller models fitting a single GPU fall far behind. We note that
the error cases often arise from the annotation scheme of the dataset;
responses from ChatGPT are still reasonable. We show, however, that the model
is worse at slot filling, and its performance is sensitive to ASR errors,
suggesting serious challenges for the application of those textual models on
SLU.",None,-1
NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes,0.857539,"With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.",None,-1
2nd Place Winning Solution for the CVPR2023 Visual Anomaly and Novelty Detection Challenge: Multimodal Prompting for Data-centric Anomaly Detection,0.097928,"This technical report introduces the winning solution of the team Segment Any
Anomaly for the CVPR2023 Visual Anomaly and Novelty Detection (VAND) challenge.
Going beyond uni-modal prompt, e.g., language prompt, we present a novel
framework, i.e., Segment Any Anomaly + (SAA$+$), for zero-shot anomaly
segmentation with multi-modal prompts for the regularization of cascaded modern
foundation models. Inspired by the great zero-shot generalization ability of
foundation models like Segment Anything, we first explore their assembly (SAA)
to leverage diverse multi-modal prior knowledge for anomaly localization.
Subsequently, we further introduce multimodal prompts (SAA$+$) derived from
domain expert knowledge and target image context to enable the non-parameter
adaptation of foundation models to anomaly segmentation. The proposed SAA$+$
model achieves state-of-the-art performance on several anomaly segmentation
benchmarks, including VisA and MVTec-AD, in the zero-shot setting. We will
release the code of our winning solution for the CVPR2023 VAN.",None,-1
Detecting and Mitigating Hallucinations in Multilingual Summarisation,0.859355,"Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.",None,-1
Enhancing Translation for Indigenous Languages: Experiments with Multilingual Models,0.239799,"This paper describes CIC NLP's submission to the AmericasNLP 2023 Shared Task
on machine translation systems for indigenous languages of the Americas. We
present the system descriptions for three methods. We used two multilingual
models, namely M2M-100 and mBART50, and one bilingual (one-to-one) -- Helsinki
NLP Spanish-English translation model, and experimented with different transfer
learning setups. We experimented with 11 languages from America and report the
setups we used as well as the results we achieved. Overall, the mBART setup was
able to improve upon the baseline for three out of the eleven languages.",None,-1
Robust Multi-Agent Pickup and Delivery with Delays,0.620802,"Multi-Agent Pickup and Delivery (MAPD) is the problem of computing
collision-free paths for a group of agents such that they can safely reach
delivery locations from pickup ones. These locations are provided at runtime,
making MAPD a combination between classical Multi-Agent Path Finding (MAPF) and
online task assignment. Current algorithms for MAPD do not consider many of the
practical issues encountered in real applications: real agents often do not
follow the planned paths perfectly, and may be subject to delays and failures.
In this paper, we study the problem of MAPD with delays, and we present two
solution approaches that provide robustness guarantees by planning paths that
limit the effects of imperfect execution. In particular, we introduce two
algorithms, k-TP and p-TP, both based on a decentralized algorithm typically
used to solve MAPD, Token Passing (TP), which offer deterministic and
probabilistic guarantees, respectively. Experimentally, we compare our
algorithms against a version of TP enriched with online replanning. k-TP and
p-TP provide robust solutions, significantly reducing the number of replans
caused by delays, with little or no increase in solution cost and running time.",None,-1
Learning from Noisy Crowd Labels with Logics,0.32128,"This paper explores the integration of symbolic logic knowledge into deep
neural networks for learning from noisy crowd labels. We introduce Logic-guided
Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic
knowledge distillation framework that learns from both noisy labeled data and
logic rules of interest. Unlike traditional EM methods, our framework contains
a ``pseudo-E-step'' that distills from the logic rules a new type of learning
target, which is then used in the ``pseudo-M-step'' for training the
classifier. Extensive evaluations on two real-world datasets for text sentiment
classification and named entity recognition demonstrate that the proposed
framework improves the state-of-the-art and provides a new solution to learning
from noisy crowd labels.",None,-1
Towards Explainable TOPSIS: Visual Insights into the Effects of Weights and Aggregations on Rankings,0.545257,"Multi-Criteria Decision Analysis (MCDA) is extensively used across diverse
industries to assess and rank alternatives. Among numerous MCDA methods
developed to solve real-world ranking problems, TOPSIS remains one of the most
popular choices in many application areas. TOPSIS calculates distances between
the considered alternatives and two predefined ones, namely the ideal and the
anti-ideal, and creates a ranking of the alternatives according to a chosen
aggregation of these distances. However, the interpretation of the inner
workings of TOPSIS is difficult, especially when the number of criteria is
large. To this end, recent research has shown that TOPSIS aggregations can be
expressed using the means (M) and standard deviations (SD) of alternatives,
creating MSD-space, a tool for visualizing and explaining aggregations. Even
though MSD-space is highly useful, it assumes equally important criteria,
making it less applicable to real-world ranking problems. In this paper, we
generalize the concept of MSD-space to weighted criteria by introducing the
concept of WMSD-space defined by what is referred to as weight-scaled means and
standard deviations. We demonstrate that TOPSIS and similar distance-based
aggregation methods can be successfully illustrated in a plane and interpreted
even when the criteria are weighted, regardless of their number. The proposed
WMSD-space offers a practical method for explaining TOPSIS rankings in
real-world decision problems.",None,-1
Multi-modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models,0.817643,"Chain-of-thought (CoT) reasoning has exhibited impressive performance in
language models for solving complex tasks and answering questions. However,
many real-world questions require multi-modal information, such as text and
images. Previous research on multi-modal CoT has primarily focused on
extracting fixed image features from off-the-shelf vision models and then
fusing them with text using attention mechanisms. This approach has limitations
because these vision models were not designed for complex reasoning tasks and
do not align well with language thoughts. To overcome this limitation, we
introduce a novel approach for multi-modal CoT reasoning that utilizes latent
space learning via diffusion processes to generate effective image features
that align with language thoughts. Our method fuses image features and text
representations at a deep level and improves the complex reasoning ability of
multi-modal CoT. We demonstrate the efficacy of our proposed method on
multi-modal ScienceQA and machine translation benchmarks, achieving
state-of-the-art performance on ScienceQA. Overall, our approach offers a more
robust and effective solution for multi-modal reasoning in language models,
enhancing their ability to tackle complex real-world problems.",None,-1
"Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution",0.991115,"Geocoding is the task of converting location mentions in text into structured
data that encodes the geospatial semantics. We propose a new architecture for
geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to
generate a list of candidate entries from the geospatial ontology. Then it
reranks the candidate entries using a transformer-based neural network that
incorporates information from the ontology such as the entry's population. This
generate-and-rerank process is applied twice: first to resolve the less
ambiguous countries, states, and counties, and second to resolve the remaining
location mentions, using the identified countries, states, and counties as
context. Our proposed toponym resolution framework achieves state-of-the-art
performance on multiple datasets. Code and models are available at
\url{https://github.com/clulab/geonorm}.",None,-1
Referring Multi-Object Tracking,0.970872,"Existing referring understanding tasks tend to involve the detection of a
single text-referred object. In this paper, we propose a new and general
referring understanding task, termed referring multi-object tracking (RMOT).
Its core idea is to employ a language expression as a semantic cue to guide the
prediction of multi-object tracking. To the best of our knowledge, it is the
first work to achieve an arbitrary number of referent object predictions in
videos. To push forward RMOT, we construct one benchmark with scalable
expressions based on KITTI, named Refer-KITTI. Specifically, it provides 18
videos with 818 expressions, and each expression in a video is annotated with
an average of 10.7 objects. Further, we develop a transformer-based
architecture TransRMOT to tackle the new task in an online manner, which
achieves impressive detection performance and outperforms other counterparts.
The dataset and code will be available at https://github.com/wudongming97/RMOT.",None,-1
Chain of Thought Prompting Elicits Knowledge Augmentation,0.754477,"The knowledge-augmented deep learning paradigm refers to a paradigm in which
domain knowledge is identified and integrated into deep models. Conventional
methods typically employ task-specific approaches to gather external knowledge
from various sources. In contrast, large language models are extensively
pre-trained and can serve as a comprehensive source of external knowledge. In
this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments
knowledge for deep learning. CoT-KA avoids the need for additional knowledge
retrieval or knowledge reasoning models, as required in conventional
augmentation methods. Our results demonstrate that CoT-KA outperforms both pure
CoT-based methods and the non-augmented method across the majority of eleven
publicly available benchmarks for various reasoning tasks.",None,-1
Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models,0.55578,"In this study, we present an efficient and effective approach for achieving
temporally consistent synthetic-to-real video translation in videos of varying
lengths. Our method leverages off-the-shelf conditional image diffusion models,
allowing us to perform multiple synthetic-to-real image generations in
parallel. By utilizing the available optical flow information from the
synthetic videos, our approach seamlessly enforces temporal consistency among
corresponding pixels across frames. This is achieved through joint noise
optimization, effectively minimizing spatial and temporal discrepancies. To the
best of our knowledge, our proposed method is the first to accomplish diverse
and temporally consistent synthetic-to-real video translation using conditional
image diffusion models. Furthermore, our approach does not require any training
or fine-tuning of the diffusion models. Extensive experiments conducted on
various benchmarks for synthetic-to-real video translation demonstrate the
effectiveness of our approach, both quantitatively and qualitatively. Finally,
we show that our method outperforms other baseline methods in terms of both
temporal consistency and visual quality.",None,-1
NLLB-CLIP -- train performant multilingual image retrieval model on a budget,0.556889,"Today, the exponential rise of large models developed by academic and
industrial institutions with the help of massive computing resources raises the
question of whether someone without access to such resources can make a
valuable scientific contribution. To explore this, we tried to solve the
challenging task of multilingual image retrieval having a limited budget of
$1,000. As a result, we present NLLB-CLIP - CLIP model with a text encoder from
the NLLB model. To train the model, we used an automatically created dataset of
106,246 good-quality images with captions in 201 languages derived from the
LAION COCO dataset. We trained multiple models using image and text encoders of
various sizes and kept different parts of the model frozen during the training.
We thoroughly analyzed the trained models using existing evaluation datasets
and newly created XTD200 and Flickr30k-200 datasets. We show that NLLB-CLIP is
comparable in quality to state-of-the-art models and significantly outperforms
them on low-resource languages.",None,-1
Uni-NLX: Unifying Textual Explanations for Vision and Vision-Language Tasks,0.0652204,"Natural Language Explanations (NLE) aim at supplementing the prediction of a
model with human-friendly natural text. Existing NLE approaches involve
training separate models for each downstream task. In this work, we propose
Uni-NLX, a unified framework that consolidates all NLE tasks into a single and
compact multi-task model using a unified training objective of text generation.
Additionally, we introduce two new NLE datasets: 1) ImageNetX, a dataset of
144K samples for explaining ImageNet categories, and 2) VQA-ParaX, a dataset of
123K samples for explaining the task of Visual Question Answering (VQA). Both
datasets are derived leveraging large language models (LLMs). By training on
the 1M combined NLE samples, our single unified framework is capable of
simultaneously performing seven NLE tasks including VQA, visual recognition and
visual reasoning tasks with 7X fewer parameters, demonstrating comparable
performance to the independent task-specific models in previous approaches, and
in certain tasks even outperforming them. Code is at
https://github.com/fawazsammani/uni-nlx",None,-1
GarmentTracking: Category-Level Garment Pose Tracking,0.741504,"Garments are important to humans. A visual system that can estimate and track
the complete garment pose can be useful for many downstream tasks and
real-world applications. In this work, we present a complete package to address
the category-level garment pose tracking task: (1) A recording system
VR-Garment, with which users can manipulate virtual garment models in
simulation through a VR interface. (2) A large-scale dataset VR-Folding, with
complex garment pose configurations in manipulation like flattening and
folding. (3) An end-to-end online tracking framework GarmentTracking, which
predicts complete garment pose both in canonical space and task space given a
point cloud sequence. Extensive experiments demonstrate that the proposed
GarmentTracking achieves great performance even when the garment has large
non-rigid deformation. It outperforms the baseline approach on both speed and
accuracy. We hope our proposed solution can serve as a platform for future
research. Codes and datasets are available in
https://garment-tracking.robotflow.ai.",None,-1
SAOR: Single-View Articulated Object Reconstruction,0.457083,"We introduce SAOR, a novel approach for estimating the 3D shape, texture, and
viewpoint of an articulated object from a single image captured in the wild.
Unlike prior approaches that rely on pre-defined category-specific 3D templates
or tailored 3D skeletons, SAOR learns to articulate shapes from single-view
image collections with a skeleton-free part-based model without requiring any
3D object shape priors. To prevent ill-posed solutions, we propose a
cross-instance consistency loss that exploits disentangled object shape
deformation and articulation. This is helped by a new silhouette-based sampling
mechanism to enhance viewpoint diversity during training. Our method only
requires estimated object silhouettes and relative depth maps from
off-the-shelf pre-trained networks during training. At inference time, given a
single-view image, it efficiently outputs an explicit mesh representation. We
obtain improved qualitative and quantitative results on challenging quadruped
animals compared to relevant existing work.",None,-1
Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks,0.128665,"Large language models (LLMs) are proficient at generating fluent text with
minimal task-specific supervision. Yet, their ability to provide well-grounded
rationalizations for knowledge-intensive tasks remains under-explored. Such
tasks, like commonsense multiple-choice questions, require rationales based on
world knowledge to support predictions and refute alternate options. We
consider the task of generating knowledge-guided rationalization in natural
language by using expert-written examples in a few-shot manner. Surprisingly,
crowd-workers preferred knowledge-grounded rationales over crowdsourced
rationalizations, citing their factuality, sufficiency, and comprehensive
refutations. Although LLMs-generated rationales were preferable, further
improvements in conciseness and novelty are required. In another study, we show
how rationalization of incorrect model predictions erodes humans' trust in
LLM-generated rationales. Motivated by these observations, we create a
two-stage pipeline to review task predictions and eliminate potential incorrect
decisions before rationalization, enabling trustworthy rationale generation.",None,-1
Improving Online Lane Graph Extraction by Object-Lane Clustering,0.506368,"Autonomous driving requires accurate local scene understanding information.
To this end, autonomous agents deploy object detection and online BEV lane
graph extraction methods as a part of their perception stack. In this work, we
propose an architecture and loss formulation to improve the accuracy of local
lane graph estimates by using 3D object detection outputs. The proposed method
learns to assign the objects to centerlines by considering the centerlines as
cluster centers and the objects as data points to be assigned a probability
distribution over the cluster centers. This training scheme ensures direct
supervision on the relationship between lanes and objects, thus leading to
better performance. The proposed method improves lane graph estimation
substantially over state-of-the-art methods. The extensive ablations show that
our method can achieve significant performance improvements by using the
outputs of existing 3D object detection methods. Since our method uses the
detection outputs rather than detection method intermediate representations, a
single model of our method can use any detection method at test time.",None,-1
Beyond Prompts: Exploring the Design Space of Mixed-Initiative Co-Creativity Systems,0.917915,"Generative Artificial Intelligence systems have been developed for image,
code, story, and game generation with the goal of facilitating human
creativity. Recent work on neural generative systems has emphasized one
particular means of interacting with AI systems: the user provides a
specification, usually in the form of prompts, and the AI system generates the
content. However, there are other configurations of human and AI coordination,
such as co-creativity (CC) in which both human and AI systems can contribute to
content creation, and mixed-initiative (MI) in which both human and AI systems
can initiate content changes. In this paper, we define a hypothetical human-AI
configuration design space consisting of different means for humans and AI
systems to communicate creative intent to each other. We conduct a human
participant study with 185 participants to understand how users want to
interact with differently configured MI-CC systems. We find out that MI-CC
systems with more extensive coverage of the design space are rated higher or on
par on a variety of creative and goal-completion metrics, demonstrating that
wider coverage of the design space can improve user experience and achievement
when using the system; Preference varies greatly between expertise groups,
suggesting the development of adaptive, personalized MI-CC systems;
Participants identified new design space dimensions including scrutability --
the ability to poke and prod at models -- and explainability.",None,-1
Distillation of encoder-decoder transformers for sequence labelling,0.311763,"Driven by encouraging results on a wide range of tasks, the field of NLP is
experiencing an accelerated race to develop bigger language models. This race
for bigger models has also underscored the need to continue the pursuit of
practical distillation approaches that can leverage the knowledge acquired by
these big models in a compute-efficient manner. Having this goal in mind, we
build on recent work to propose a hallucination-free framework for sequence
tagging that is especially suited for distillation. We show empirical results
of new state-of-the-art performance across multiple sequence labelling datasets
and validate the usefulness of this framework for distilling a large model in a
few-shot learning scenario.",None,-1
Energy Efficiency of Training Neural Network Architectures: An Empirical Study,0.524001,"The evaluation of Deep Learning models has traditionally focused on criteria
such as accuracy, F1 score, and related measures. The increasing availability
of high computational power environments allows the creation of deeper and more
complex models. However, the computations needed to train such models entail a
large carbon footprint. In this work, we study the relations between DL model
architectures and their environmental impact in terms of energy consumed and
CO$_2$ emissions produced during training by means of an empirical study using
Deep Convolutional Neural Networks. Concretely, we study: (i) the impact of the
architecture and the location where the computations are hosted on the energy
consumption and emissions produced; (ii) the trade-off between accuracy and
energy efficiency; and (iii) the difference on the method of measurement of the
energy consumed using software-based and hardware-based tools.",None,-1
KGEx: Explaining Knowledge Graph Embeddings via Subgraph Sampling and Knowledge Distillation,0.280163,"Despite being the go-to choice for link prediction on knowledge graphs,
research on interpretability of knowledge graph embeddings (KGE) has been
relatively unexplored. We present KGEx, a novel post-hoc method that explains
individual link predictions by drawing inspiration from surrogate models
research. Given a target triple to predict, KGEx trains surrogate KGE models
that we use to identify important training triples. To gauge the impact of a
training triple, we sample random portions of the target triple neighborhood
and we train multiple surrogate KGE models on each of them. To ensure
faithfulness, each surrogate is trained by distilling knowledge from the
original KGE model. We then assess how well surrogates predict the target
triple being explained, the intuition being that those leading to faithful
predictions have been trained on impactful neighborhood samples. Under this
assumption, we then harvest triples that appear frequently across impactful
neighborhoods. We conduct extensive experiments on two publicly available
datasets, to demonstrate that KGEx is capable of providing explanations
faithful to the black-box model.",None,-1
Large Language Models for Propaganda Detection,0.166213,"The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models' performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.",None,-1
Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin,0.498387,"Developing effective spoken language processing systems for low-resource
languages poses several challenges due to the lack of parallel data and limited
resources for fine-tuning models. In this work, we target on improving upon
both text classification and translation of Nigerian Pidgin (Naija) by
collecting a large-scale parallel English-Pidgin corpus and further propose a
framework of cross-lingual adaptive training that includes both continual and
task adaptive training so as to adapt a base pre-trained model to low-resource
languages. Our studies show that English pre-trained language models serve as a
stronger prior than multilingual language models on English-Pidgin tasks with
up to 2.38 BLEU improvements; and demonstrate that augmenting orthographic data
and using task adaptive training with back-translation can have a significant
impact on model performance.",None,-1
A Multiagent CyberBattleSim for RL Cyber Operation Agents,0.687528,"Hardening cyber physical assets is both crucial and labor-intensive.
Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more
specifically has shown great promise to automate tasks that otherwise would
require significant human insight/intelligence. The development of autonomous
RL agents requires a suitable training environment that allows us to quickly
evaluate various alternatives, in particular how to arrange training scenarios
that pit attackers and defenders against each other. CyberBattleSim is a
training environment that supports the training of red agents, i.e., attackers.
We added the capability to train blue agents, i.e., defenders. The paper
describes our changes and reports on the results we obtained when training blue
agents, either in isolation or jointly with red agents. Our results show that
training a blue agent does lead to stronger defenses against attacks. In
particular, training a blue agent jointly with a red agent increases the blue
agent's capability to thwart sophisticated red agents.",None,-1
Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach,0.310662,"Named Entity Recognition (NER) aims to extract and classify entity mentions
in the text into pre-defined types (e.g., organization or person name).
Recently, many works have been proposed to shape the NER as a machine reading
comprehension problem (also termed MRC-based NER), in which entity recognition
is achieved by answering the formulated questions related to pre-defined entity
types through MRC, based on the contexts. However, these works ignore the label
dependencies among entity types, which are critical for precisely recognizing
named entities. In this paper, we propose to incorporate the label dependencies
among entity types into a multi-task learning framework for better MRC-based
NER. We decompose MRC-based NER into multiple tasks and use a self-attention
module to capture label dependencies. Comprehensive experiments on both nested
NER and flat NER datasets are conducted to validate the effectiveness of the
proposed Multi-NER. Experimental results show that Multi-NER can achieve better
performance on all datasets.",None,-1
Framing Relevance for Safety-Critical Autonomous Systems,0.0766716,"We are in the process of building complex highly autonomous systems that have
build-in beliefs, perceive their environment and exchange information. These
systems construct their respective world view and based on it they plan their
future manoeuvres, i.e., they choose their actions in order to establish their
goals based on their prediction of the possible futures. Usually these systems
face an overwhelming flood of information provided by a variety of sources
where by far not everything is relevant. The goal of our work is to develop a
formal approach to determine what is relevant for a safety critical autonomous
system at its current mission, i.e., what information suffices to build an
appropriate world view to accomplish its mission goals.",None,-1
The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models,0.415661,"Despite the notable accomplishments of deep object detection models, a major
challenge that persists is the requirement for extensive amounts of training
data. The process of procuring such real-world data is a laborious undertaking,
which has prompted researchers to explore new avenues of research, such as
synthetic data generation techniques. This study presents a framework for the
generation of synthetic datasets by fine-tuning pretrained stable diffusion
models. The synthetic datasets are then manually annotated and employed for
training various object detection models. These detectors are evaluated on a
real-world test set of 331 images and compared against a baseline model that
was trained on real-world images. The results of this study reveal that the
object detection models trained on synthetic data perform similarly to the
baseline model. In the context of apple detection in orchards, the average
precision deviation with the baseline ranges from 0.09 to 0.12. This study
illustrates the potential of synthetic data generation techniques as a viable
alternative to the collection of extensive training data for the training of
deep models.",None,-1
"Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis",0.999831,"Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a
major driver of financial market returns. We construct the largest tokenized
and annotated dataset of FOMC speeches, meeting minutes, and press conference
transcripts in order to understand how monetary policy influences financial
markets. In this study, we develop a novel task of hawkish-dovish
classification and benchmark various pre-trained language models on the
proposed dataset. Using the best-performing model (RoBERTa-large), we construct
a measure of monetary policy stance for the FOMC document release days. To
evaluate the constructed measure, we study its impact on the treasury market,
stock market, and macroeconomic indicators. Our dataset, models, and code are
publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.",None,-1
FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding,0.726271,"Although Domain Adaptation in Semantic Scene Segmentation has shown
impressive improvement in recent years, the fairness concerns in the domain
adaptation have yet to be well defined and addressed. In addition, fairness is
one of the most critical aspects when deploying the segmentation models into
human-related real-world applications, e.g., autonomous driving, as any unfair
predictions could influence human safety. In this paper, we propose a novel
Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In
particular, from the proposed formulated fairness objective, a new adaptation
framework will be introduced based on the fair treatment of class
distributions. Moreover, to generally model the context of structural
dependency, a new conditional structural constraint is introduced to impose the
consistency of predicted segmentation. Thanks to the proposed Conditional
Structure Network, the self-attention mechanism has sufficiently modeled the
structural information of segmentation. Through the ablation studies, the
proposed method has shown the performance improvement of the segmentation
models and promoted fairness in the model predictions. The experimental results
on the two standard benchmarks, i.e., SYNTHIA $\to$ Cityscapes and GTA5 $\to$
Cityscapes, have shown that our method achieved State-of-the-Art (SOTA)
performance.",None,-1
Moral Responsibility for AI Systems,0.213086,"As more and more decisions that have a significant ethical dimension are
being outsourced to AI systems, it is important to have a definition of moral
responsibility that can be applied to AI systems. Moral responsibility for an
outcome of an agent who performs some action is commonly taken to involve both
a causal condition and an epistemic condition: the action should cause the
outcome, and the agent should have been aware -- in some form or other -- of
the possible moral consequences of their action. This paper presents a formal
definition of both conditions within the framework of causal models. I compare
my approach to the existing approaches of Braham and van Hees (BvH) and of
Halpern and Kleiman-Weiner (HK). I then generalize my definition into a degree
of responsibility.",None,-1
Recurrent Attention Networks for Long-text Modeling,0.398139,"Self-attention-based models have achieved remarkable progress in short-text
mining. However, the quadratic computational complexities restrict their
application in long text processing. Prior works have adopted the chunking
strategy to divide long documents into chunks and stack a self-attention
backbone with the recurrent structure to extract semantic representation. Such
an approach disables parallelization of the attention mechanism, significantly
increasing the training cost and raising hardware requirements. Revisiting the
self-attention mechanism and the recurrent structure, this paper proposes a
novel long-document encoding model, Recurrent Attention Network (RAN), to
enable the recurrent operation of self-attention. Combining the advantages from
both sides, the well-designed RAN is capable of extracting global semantics in
both token-level and document-level representations, making it inherently
compatible with both sequential and classification tasks, respectively.
Furthermore, RAN is computationally scalable as it supports parallelization on
long document processing. Extensive experiments demonstrate the long-text
encoding ability of the proposed RAN model on both classification and
sequential tasks, showing its potential for a wide range of applications.",None,-1
Tied-Augment: Controlling Representation Similarity Improves Data Augmentation,0.194263,"Data augmentation methods have played an important role in the recent advance
of deep learning models, and have become an indispensable component of
state-of-the-art models in semi-supervised, self-supervised, and supervised
training for vision. Despite incurring no additional latency at test time, data
augmentation often requires more epochs of training to be effective. For
example, even the simple flips-and-crops augmentation requires training for
more than 5 epochs to improve performance, whereas RandAugment requires more
than 90 epochs. We propose a general framework called Tied-Augment, which
improves the efficacy of data augmentation in a wide range of applications by
adding a simple term to the loss that can control the similarity of
representations under distortions. Tied-Augment can improve state-of-the-art
methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g.
SAM), and semi-supervised learning (e.g. FixMatch). For example,
Tied-RandAugment can outperform RandAugment by 2.0% on ImageNet. Notably, using
Tied-Augment, data augmentation can be made to improve generalization even when
training for a few epochs and when fine-tuning. We open source our code at
https://github.com/ekurtulus/tied-augment/tree/main.",None,-1
"OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",0.517995,"Large language models (LLMs) often struggle with maintaining accuracy
throughout multiple multiple reasoning steps, especially in mathematical
reasoning where an error in earlier steps can propagate to subsequent ones and
it ultimately leading to an incorrect answer. To reduce error propagation,
guided decoding is employed to direct the LM decoding on a step-by-step basis.
We argue that in guided decoding, assessing the potential of an incomplete
reasoning path can be more advantageous than simply ensuring per-step
correctness, as the former approach leads towards a correct final answer. This
transforms the task into a $\textit{value estimation}$ problem in planning.
  Inspired by the findings that $\textit{outcome supervision for guided
decoding essentially acts as a value model}$, we propose Outcome-supervised
Value Model (OVM) that employs outcome supervision for training a value model,
which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM
eliminates the need for labor-intensive annotations of step-level correctness,
thereby significantly enhancing its scalability. Our experiments on two
multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate
the superior performance of the OVM model. Notably, in GSM8K, our
$\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B
parameters}$; especially it does not utilize GPT-4 or code execution. These
findings offer a novel perspective on the role of outcome supervision in
training value models for multi-step reasoning tasks and provide theoretical
justification for its advantage in value estimation for guided decoding.",None,-1
Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques,0.242769,"This paper investigates the transferability of debiasing techniques across
different languages within multilingual models. We examine the applicability of
these techniques in English, French, German, and Dutch. Using multilingual BERT
(mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is
not only feasible but also yields promising results. Surprisingly, our findings
reveal no performance disadvantages when applying these techniques to
non-English languages. Using translations of the CrowS-Pairs dataset, our
analysis identifies SentenceDebias as the best technique across different
languages, reducing bias in mBERT by an average of 13%. We also find that
debiasing techniques with additional pretraining exhibit enhanced cross-lingual
effectiveness for the languages included in the analyses, particularly in
lower-resource languages. These novel insights contribute to a deeper
understanding of bias mitigation in multilingual language models and provide
practical guidance for debiasing techniques in different language contexts.",None,-1
VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer,0.783639,"Current talking face generation methods mainly focus on speech-lip
synchronization. However, insufficient investigation on the facial talking
style leads to a lifeless and monotonous avatar. Most previous works fail to
imitate expressive styles from arbitrary video prompts and ensure the
authenticity of the generated video. This paper proposes an unsupervised
variational style transfer model (VAST) to vivify the neutral photo-realistic
avatars. Our model consists of three key components: a style encoder that
extracts facial style representations from the given video prompts; a hybrid
facial expression decoder to model accurate speech-related movements; a
variational style enhancer that enhances the style space to be highly
expressive and meaningful. With our essential designs on facial style learning,
our model is able to flexibly capture the expressive facial style from
arbitrary video prompts and transfer it onto a personalized image renderer in a
zero-shot manner. Experimental results demonstrate the proposed approach
contributes to a more vivid talking avatar with higher authenticity and richer
expressiveness.",None,-1
Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework,0.167541,"Factuality is important to dialogue summarization. Factual error correction
(FEC) of model-generated summaries is one way to improve factuality. Current
FEC evaluation that relies on factuality metrics is not reliable and detailed
enough. To address this problem, we are the first to manually annotate a FEC
dataset for dialogue summarization containing 4000 items and propose FERRANTI,
a fine-grained evaluation framework based on reference correction that
automatically evaluates the performance of FEC models on different error
categories. Using this evaluation framework, we conduct sufficient experiments
with FEC approaches under a variety of settings and find the best training
modes and significant differences in the performance of the existing approaches
on different factual error categories.",None,-1
CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality,0.241372,"There are three problems existing in the popular data-to-text datasets.
First, the large-scale datasets either contain noise or lack real application
scenarios. Second, the datasets close to real applications are relatively small
in size. Last, current datasets bias in the English language while leaving
other languages underexplored. To alleviate these limitations, in this paper,
we present CATS, a pragmatic Chinese answer-to-sequence dataset with large
scale and high quality. The dataset aims to generate textual descriptions for
the answer in the practical TableQA system. Further, to bridge the structural
gap between the input SQL and table and establish better semantic alignments,
we propose a Unified Graph Transformation approach to establish a joint
encoding space for the two hybrid knowledge resources and convert this task to
a graph-to-text problem. The experiment results demonstrate the effectiveness
of our proposed method. Further analysis on CATS attests to both the high
quality and challenges of the dataset.",None,-1
Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers,0.573969,"Recent applications of LLMs in Machine Reading Comprehension (MRC) systems
have shown impressive results, but the use of shortcuts, mechanisms triggered
by features spuriously correlated to the true label, has emerged as a potential
threat to their reliability. We analyze the problem from two angles: LLMs as
editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer
questions based on the edited text. We introduce a framework that guides an
editor to add potential shortcuts-triggers to samples. Using GPT4 as the
editor, we find it can successfully edit trigger shortcut in samples that fool
LLMs. Analysing LLMs as readers, we observe that even capable LLMs can be
deceived using shortcut knowledge. Strikingly, we discover that GPT4 can be
deceived by its own edits (15% drop in F1). Our findings highlight inherent
vulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a
curated dataset generated by our framework for future research.",None,-1
X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation,0.482172,"An important component of human analysis of medical images and their context
is the ability to relate newly seen things to related instances in our memory.
In this paper we mimic this ability by using multi-modal retrieval augmentation
and apply it to several tasks in chest X-ray analysis. By retrieving similar
images and/or radiology reports we expand and regularize the case at hand with
additional knowledge, while maintaining factual knowledge consistency. The
method consists of two components. First, vision and language modalities are
aligned using a pre-trained CLIP model. To enforce that the retrieval focus
will be on detailed disease-related content instead of global visual appearance
it is fine-tuned using disease class information. Subsequently, we construct a
non-parametric retrieval index, which reaches state-of-the-art retrieval
levels. We use this index in our downstream tasks to augment image
representations through multi-head attention for disease classification and
report retrieval. We show that retrieval augmentation gives considerable
improvements on these tasks. Our downstream report retrieval even shows to be
competitive with dedicated report generation methods, paving the path for this
method in medical imaging.",None,-1
Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition,0.787181,"Existing pedestrian attribute recognition (PAR) algorithms are mainly
developed based on a static image. However, the performance is not reliable for
images with challenging factors, such as heavy occlusion, motion blur, etc. In
this work, we propose to understand human attributes using video frames that
can make full use of temporal information. Specifically, we formulate the
video-based PAR as a vision-language fusion problem and adopt pre-trained big
models CLIP to extract the feature embeddings of given video frames. To better
utilize the semantic information, we take the attribute list as another input
and transform the attribute words/phrase into the corresponding sentence via
split, expand, and prompt. Then, the text encoder of CLIP is utilized for
language embedding. The averaged visual tokens and text tokens are concatenated
and fed into a fusion Transformer for multi-modal interactive learning. The
enhanced tokens will be fed into a classification head for pedestrian attribute
prediction. Extensive experiments on a large-scale video-based PAR dataset
fully validated the effectiveness of our proposed framework.",None,-1
Learning Neural Eigenfunctions for Unsupervised Semantic Segmentation,0.113173,"Unsupervised semantic segmentation is a long-standing challenge in computer
vision with great significance. Spectral clustering is a theoretically grounded
solution to it where the spectral embeddings for pixels are computed to
construct distinct clusters. Despite recent progress in enhancing spectral
clustering with powerful pre-trained models, current approaches still suffer
from inefficiencies in spectral decomposition and inflexibility in applying
them to the test data. This work addresses these issues by casting spectral
clustering as a parametric approach that employs neural network-based
eigenfunctions to produce spectral embeddings. The outputs of the neural
eigenfunctions are further restricted to discrete vectors that indicate
clustering assignments directly. As a result, an end-to-end NN-based paradigm
of spectral clustering emerges. In practice, the neural eigenfunctions are
lightweight and take the features from pre-trained models as inputs, improving
training efficiency and unleashing the potential of pre-trained models for
dense prediction. We conduct extensive empirical studies to validate the
effectiveness of our approach and observe significant performance gains over
competitive baselines on Pascal Context, Cityscapes, and ADE20K benchmarks.",None,-1
Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature,0.824431,"Large language models (LLMs) have shown the ability to produce fluent and
cogent content, presenting both productivity opportunities and societal risks.
To build trustworthy AI systems, it is imperative to distinguish between
machine-generated and human-authored content. The leading zero-shot detector,
DetectGPT, showcases commendable performance but is marred by its intensive
computational costs. In this paper, we introduce the concept of conditional
probability curvature to elucidate discrepancies in word choices between LLMs
and humans within a given context. Utilizing this curvature as a foundational
metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which
substitutes DetectGPT's perturbation step with a more efficient sampling step.
Our evaluations on various datasets, source models, and test conditions
indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around
75% in both the white-box and black-box settings but also accelerates the
detection process by a factor of 340, as detailed in Table 1. See
\url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and
results.",None,-1
Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation,0.804045,"Target-oriented dialogue systems, designed to proactively steer conversations
toward predefined targets or accomplish specific system-side goals, are an
exciting area in conversational AI. In this work, by formulating a <dialogue
act, topic> pair as the conversation target, we explore a novel problem of
personalized target-oriented dialogue by considering personalization during the
target accomplishment process. However, there remains an emergent need for
high-quality datasets, and building one from scratch requires tremendous human
effort. To address this, we propose an automatic dataset curation framework
using a role-playing approach. Based on this framework, we construct a
large-scale personalized target-oriented dialogue dataset, TopDial, which
comprises about 18K multi-turn dialogues. The experimental results show that
this dataset is of high quality and could contribute to exploring personalized
target-oriented dialogue.",None,-1
SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep Models for Kidney Stone Classification,0.572601,"Recently, deep learning has produced encouraging results for kidney stone
classification using endoscope images. However, the shortage of annotated
training data poses a severe problem in improving the performance and
generalization ability of the trained model. It is thus crucial to fully
exploit the limited data at hand. In this paper, we propose SegPrompt to
alleviate the data shortage problems by exploiting segmentation maps from two
aspects. First, SegPrompt integrates segmentation maps to facilitate
classification training so that the classification model is aware of the
regions of interest. The proposed method allows the image and segmentation
tokens to interact with each other to fully utilize the segmentation map
information. Second, we use the segmentation maps as prompts to tune the
pretrained deep model, resulting in much fewer trainable parameters than
vanilla finetuning. We perform extensive experiments on the collected kidney
stone dataset. The results show that SegPrompt can achieve an advantageous
balance between the model fitting ability and the generalization ability,
eventually leading to an effective model with limited training data.",None,-1
Explainable Goal Recognition: A Framework Based on Weight of Evidence,0.231949,"We introduce and evaluate an eXplainable Goal Recognition (XGR) model that
uses the Weight of Evidence (WoE) framework to explain goal recognition
problems. Our model provides human-centered explanations that answer why? and
why not? questions. We computationally evaluate the performance of our system
over eight different domains. Using a human behavioral study to obtain the
ground truth from human annotators, we further show that the XGR model can
successfully generate human-like explanations. We then report on a study with
60 participants who observe agents playing Sokoban game and then receive
explanations of the goal recognition output. We investigate participants'
understanding obtained by explanations through task prediction, explanation
satisfaction, and trust.",None,-1
Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges,0.213675,"Large Language Models (LLMs) have demonstrated impressive zero shot
performance on a wide range of NLP tasks, demonstrating the ability to reason
and apply commonsense. A relevant application is to use them for creating high
quality synthetic datasets for downstream tasks. In this work, we probe whether
GPT-4 can be used to augment existing extractive reading comprehension
datasets. Automating data annotation processes has the potential to save large
amounts of time, money and effort that goes into manually labelling datasets.
In this paper, we evaluate the performance of GPT-4 as a replacement for human
annotators for low resource reading comprehension tasks, by comparing
performance after fine tuning, and the cost associated with annotation. This
work serves to be the first analysis of LLMs as synthetic data augmenters for
QA systems, highlighting the unique opportunities and challenges. Additionally,
we release augmented versions of low resource datasets, that will allow the
research community to create further benchmarks for evaluation of generated
datasets.",None,-1
ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint,0.534718,"Large-scale online recommender system spreads all over the Internet being in
charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion
Rate (CVR) estimations. However, traditional CVR estimators suffer from
well-known Sample Selection Bias and Data Sparsity issues. Entire space models
were proposed to address the two issues via tracing the decision-making path of
""exposure_click_purchase"". Further, some researchers observed that there are
purchase-related behaviors between click and purchase, which can better draw
the user's decision-making intention and improve the recommendation
performance. Thus, the decision-making path has been extended to
""exposure_click_in-shop action_purchase"" and can be modeled with conditional
probability approach. Nevertheless, we observe that the chain rule of
conditional probability does not always hold. We report Probability Space
Confusion (PSC) issue and give a derivation of difference between ground-truth
and estimation mathematically. We propose a novel Entire Space Multi-Task Model
for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two
alternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and
Entire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.
Specifically, we handle ""exposure_click_in-shop action"" and ""in-shop
action_purchase"" separately in the light of characteristics of in-shop action.
The first path is still treated with conditional probability while the second
one is treated with parameter constraint strategy. Experiments on both offline
and online environments in a large-scale recommendation system illustrate the
superiority of our proposed methods over state-of-the-art models. The
real-world datasets will be released.",None,-1
DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception,0.418501,"Closing the domain gap between training and deployment and incorporating
multiple sensor modalities are two challenging yet critical topics for
self-driving. Existing work only focuses on single one of the above topics,
overlooking the simultaneous domain and modality shift which pervasively exists
in real-world scenarios. A model trained with multi-sensor data collected in
Europe may need to run in Asia with a subset of input sensors available. In
this work, we propose DualCross, a cross-modality cross-domain adaptation
framework to facilitate the learning of a more robust monocular bird's-eye-view
(BEV) perception model, which transfers the point cloud knowledge from a LiDAR
sensor in one domain during the training phase to the camera-only testing
scenario in a different domain. This work results in the first open analysis of
cross-domain cross-sensor perception and adaptation for monocular 3D tasks in
the wild. We benchmark our approach on large-scale datasets under a wide range
of domain shifts and show state-of-the-art results against various baselines.",None,-1
The Expressive Power of Tuning Only the Normalization Layers,0.362832,"Feature normalization transforms such as Batch and Layer-Normalization have
become indispensable ingredients of state-of-the-art deep neural networks.
Recent studies on fine-tuning large pretrained models indicate that just tuning
the parameters of these affine transforms can achieve high accuracy for
downstream tasks. These findings open the questions about the expressive power
of tuning the normalization layers of frozen networks. In this work, we take
the first step towards this question and show that for random ReLU networks,
fine-tuning only its normalization layers can reconstruct any target network
that is $O(\sqrt{\text{width}})$ times smaller. We show that this holds even
for randomly sparsified networks, under sufficient overparameterization, in
agreement with prior empirical work.",None,-1
Graph Agent: Explicit Reasoning Agent for Graphs,0.471165,"Graph embedding methods such as Graph Neural Networks (GNNs) and Graph
Transformers have contributed to the development of graph reasoning algorithms
for various tasks on knowledge graphs. However, the lack of interpretability
and explainability of graph embedding methods has limited their applicability
in scenarios requiring explicit reasoning. In this paper, we introduce the
Graph Agent (GA), an intelligent agent methodology of leveraging large language
models (LLMs), inductive-deductive reasoning modules, and long-term memory for
knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning
and existing graph embedding methods to provide an innovative approach for
complex graph reasoning tasks. By converting graph structures into textual
data, GA enables LLMs to process, reason, and provide predictions alongside
human-interpretable explanations. The effectiveness of the GA was evaluated on
node classification and link prediction tasks. Results showed that GA reached
state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and
89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to
existing GNN and transformer models, GA offered advantages of explicit
reasoning ability, free-of-training, easy adaption to various graph reasoning
tasks",None,-1
A novel approach to generate datasets with XAI ground truth to evaluate image models,0.200777,"With the increased usage of artificial intelligence (AI), it is imperative to
understand how these models work internally. These needs have led to the
development of a new field called eXplainable artificial intelligence (XAI).
This field consists of on a set of techniques that allows us to theoretically
determine the cause of the AI decisions. One main issue of XAI is how to verify
the works on this field, taking into consideration the lack of ground truth
(GT). In this study, we propose a new method to generate datasets with GT. We
conducted a set of experiments that compared our GT with real model
explanations and obtained excellent results confirming that our proposed method
is correct.",None,-1
On The Convergence Of Policy Iteration-Based Reinforcement Learning With Monte Carlo Policy Evaluation,0.136966,"A common technique in reinforcement learning is to evaluate the value
function from Monte Carlo simulations of a given policy, and use the estimated
value function to obtain a new policy which is greedy with respect to the
estimated value function. A well-known longstanding open problem in this
context is to prove the convergence of such a scheme when the value function of
a policy is estimated from data collected from a single sample path obtained
from implementing the policy (see page 99 of [Sutton and Barto, 2018], page 8
of [Tsitsiklis, 2002]). We present a solution to the open problem by showing
that a first-visit version of such a policy iteration scheme indeed converges
to the optimal policy provided that the policy improvement step uses lookahead
[Silver et al., 2016, Mnih et al., 2016, Silver et al., 2017b] rather than a
simple greedy policy improvement. We provide results both for the original open
problem in the tabular setting and also present extensions to the function
approximation setting, where we show that the policy resulting from the
algorithm performs close to the optimal policy within a function approximation
error.",None,-1
F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks,0.387991,"Computational complexity and overthinking problems have become the
bottlenecks for pre-training language models (PLMs) with millions or even
trillions of parameters. A Flexible-Patience-Based Early Exiting method
(F-PABEE) has been proposed to alleviate the problems mentioned above for
single-label classification (SLC) and multi-label classification (MLC) tasks.
F-PABEE makes predictions at the classifier and will exit early if predicted
distributions of cross-layer are consecutively similar. It is more flexible
than the previous state-of-the-art (SOTA) early exiting method PABEE because it
can simultaneously adjust the similarity score thresholds and the patience
parameters. Extensive experiments show that: (1) F-PABEE makes a better
speedup-accuracy balance than existing early exiting strategies on both SLC and
MLC tasks. (2) F-PABEE achieves faster inference and better performances on
different PLMs such as BERT and ALBERT. (3) F-PABEE-JSKD performs best for
F-PABEE with different similarity measures.",None,-1
Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping,0.317187,"Clinical practice in psychiatry is burdened with the increased demand for
healthcare services and the scarce resources available. New paradigms of health
data powered with machine learning techniques could open the possibility to
improve clinical workflow in critical stages of clinical assessment and
treatment in psychiatry. In this work, we propose a machine learning system
capable of predicting, detecting, and explaining individual changes in symptoms
of patients with Schizophrenia by using behavioral digital phenotyping data. We
forecast symptoms of patients with an error rate below 10%. The system detects
decreases in symptoms using changepoint algorithms and uses counterfactual
explanations as a recourse in a simulated continuous monitoring scenario in
healthcare. Overall, this study offers valuable insights into the performance
and potential of counterfactual explanations, predictive models, and
change-point detection within a simulated clinical workflow. These findings lay
the foundation for further research to explore additional facets of the
workflow, aiming to enhance its effectiveness and applicability in real-world
healthcare settings. By leveraging these components, the goal is to develop an
actionable, interpretable, and trustworthy integrative decision support system
that combines real-time clinical assessments with sensor-based inputs.",None,-1
Dr.ICL: Demonstration-Retrieved In-context Learning,0.647619,"In-context learning (ICL), teaching a large language model (LLM) to perform a
task with few-shot demonstrations rather than adjusting the model parameters,
has emerged as a strong paradigm for using LLMs. While early studies primarily
used a fixed or random set of demonstrations for all test queries, recent
research suggests that retrieving semantically similar demonstrations to the
input from a pool of available demonstrations results in better performance.
This work expands the applicability of retrieval-based ICL approaches by
demonstrating that even simple word-overlap similarity measures such as BM25
outperform randomly selected demonstrations. Furthermore, we extend the success
of retrieval-based ICL to instruction-finetuned LLMs as well as
Chain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that
although a model has already seen the training data at training time,
retrieving demonstrations from the training data at test time yields better
results compared to using no demonstrations or random demonstrations. Last but
not least, we train a task-specific demonstration retriever that outperforms
off-the-shelf retrievers.",None,-1
Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding,0.522717,"Masked signal modeling has greatly advanced self-supervised pre-training for
language and 2D images. However, it is still not fully explored in 3D scene
understanding. Thus, this paper introduces Masked Shape Prediction (MSP), a new
framework to conduct masked signal modeling in 3D scenes. MSP uses the
essential 3D semantic cue, i.e., geometric shape, as the prediction target for
masked points. The context-enhanced shape target consisting of explicit shape
context and implicit deep shape feature is proposed to facilitate exploiting
contextual cues in shape prediction. Meanwhile, the pre-training architecture
in MSP is carefully designed to alleviate the masked shape leakage from point
coordinates. Experiments on multiple 3D understanding tasks on both indoor and
outdoor datasets demonstrate the effectiveness of MSP in learning good feature
representations to consistently boost downstream performance.",None,-1
Coherent Concept-based Explanations in Medical Image and Its Application to Skin Lesion Diagnosis,0.733773,"Early detection of melanoma is crucial for preventing severe complications
and increasing the chances of successful treatment. Existing deep learning
approaches for melanoma skin lesion diagnosis are deemed black-box models, as
they omit the rationale behind the model prediction, compromising the
trustworthiness and acceptability of these diagnostic methods. Attempts to
provide concept-based explanations are based on post-hoc approaches, which
depend on an additional model to derive interpretations. In this paper, we
propose an inherently interpretable framework to improve the interpretability
of concept-based models by incorporating a hard attention mechanism and a
coherence loss term to assure the visual coherence of concept activations by
the concept encoder, without requiring the supervision of additional
annotations. The proposed framework explains its decision in terms of
human-interpretable concepts and their respective contribution to the final
prediction, as well as a visual interpretation of the locations where the
concept is present in the image. Experiments on skin image datasets demonstrate
that our method outperforms existing black-box and concept-based models for
skin lesion classification.",None,-1
Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks,0.450334,"Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.",None,-1
Center Contrastive Loss for Metric Learning,0.127683,"Contrastive learning is a major studied topic in metric learning. However,
sampling effective contrastive pairs remains a challenge due to factors such as
limited batch size, imbalanced data distribution, and the risk of overfitting.
In this paper, we propose a novel metric learning function called Center
Contrastive Loss, which maintains a class-wise center bank and compares the
category centers with the query data points using a contrastive loss. The
center bank is updated in real-time to boost model convergence without the need
for well-designed sample mining. The category centers are well-optimized
classification proxies to re-balance the supervisory signal of each class.
Furthermore, the proposed loss combines the advantages of both contrastive and
classification methods by reducing intra-class variations and enhancing
inter-class differences to improve the discriminative power of embeddings. Our
experimental results, as shown in Figure 1, demonstrate that a standard network
(ResNet50) trained with our loss achieves state-of-the-art performance and
faster convergence.",None,-1
Cos R-CNN for Online Few-shot Object Detection,0.111746,"We propose Cos R-CNN, a simple exemplar-based R-CNN formulation that is
designed for online few-shot object detection. That is, it is able to localise
and classify novel object categories in images with few examples without
fine-tuning. Cos R-CNN frames detection as a learning-to-compare task: unseen
classes are represented as exemplar images, and objects are detected based on
their similarity to these exemplars. The cosine-based classification head
allows for dynamic adaptation of classification parameters to the exemplar
embedding, and encourages the clustering of similar classes in embedding space
without the need for manual tuning of distance-metric hyperparameters. This
simple formulation achieves best results on the recently proposed 5-way
ImageNet few-shot detection benchmark, beating the online 1/5/10-shot scenarios
by more than 8/3/1%, as well as performing up to 20% better in online 20-way
few-shot VOC across all shots on novel classes.",None,-1
"LARG, Language-based Automatic Reward and Goal Generation",0.200136,"Goal-conditioned and Multi-Task Reinforcement Learning (GCRL and MTRL)
address numerous problems related to robot learning, including locomotion,
navigation, and manipulation scenarios. Recent works focusing on
language-defined robotic manipulation tasks have led to the tedious production
of massive human annotations to create dataset of textual descriptions
associated with trajectories. To leverage reinforcement learning with
text-based task descriptions, we need to produce reward functions associated
with individual tasks in a scalable manner. In this paper, we leverage recent
capabilities of Large Language Models (LLMs) and introduce \larg,
Language-based Automatic Reward and Goal Generation, an approach that converts
a text-based task description into its corresponding reward and goal-generation
functions We evaluate our approach for robotic manipulation and demonstrate its
ability to train and execute policies in a scalable manner, without the need
for handcrafted reward functions.",None,-1
Agglomerative Transformer for Human-Object Interaction Detection,0.119252,"We propose an agglomerative Transformer (AGER) that enables Transformer-based
human-object interaction (HOI) detectors to flexibly exploit extra
instance-level cues in a single-stage and end-to-end manner for the first time.
AGER acquires instance tokens by dynamically clustering patch tokens and
aligning cluster centers to instances with textual guidance, thus enjoying two
benefits: 1) Integrality: each instance token is encouraged to contain all
discriminative feature regions of an instance, which demonstrates a significant
improvement in the extraction of different instance-level cues and subsequently
leads to a new state-of-the-art performance of HOI detection with 36.75 mAP on
HICO-Det. 2) Efficiency: the dynamical clustering mechanism allows AGER to
generate instance tokens jointly with the feature learning of the Transformer
encoder, eliminating the need of an additional object detector or instance
decoder in prior methods, thus allowing the extraction of desirable extra cues
for HOI detection in a single-stage and end-to-end pipeline. Concretely, AGER
reduces GFLOPs by 8.5% and improves FPS by 36%, even compared to a vanilla
DETR-like pipeline without extra cue extraction.",None,-1
T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing,0.13947,"Translating natural language queries into SQLs in a seq2seq manner has
attracted much attention recently. However, compared with
abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face
much more challenges, including poor quality on schematical information
prediction and poor semantic coherence between natural language queries and
SQLs. This paper analyses the above difficulties and proposes a
seq2seq-oriented decoding strategy called SR, which includes a new intermediate
representation SSQL and a reranking method with score re-estimator to solve the
above obstacles respectively. Experimental results demonstrate the
effectiveness of our proposed techniques and T5-SR-3b achieves new
state-of-the-art results on the Spider dataset.",None,-1
Discourse Structures Guided Fine-grained Propaganda Identification,0.375043,"Propaganda is a form of deceptive narratives that instigate or mislead the
public, usually with a political purpose. In this paper, we aim to identify
propaganda in political news at two fine-grained levels: sentence-level and
token-level. We observe that propaganda content is more likely to be embedded
in sentences that attribute causality or assert contrast to nearby sentences,
as well as seen in opinionated evaluation, speculation and discussions of
future expectation. Hence, we propose to incorporate both local and global
discourse structures for propaganda discovery and construct two teacher models
for identifying PDTB-style discourse relations between nearby sentences and
common discourse roles of sentences in a news article respectively. We further
devise two methods to incorporate the two types of discourse structures for
propaganda identification by either using teacher predicted probabilities as
additional features or soliciting guidance in a knowledge distillation
framework. Experiments on the benchmark dataset demonstrate that leveraging
guidance from discourse structures can significantly improve both precision and
recall of propaganda content identification.",None,-1
Unified Open-Vocabulary Dense Visual Prediction,0.226108,"In recent years, open-vocabulary (OV) dense visual prediction (such as OV
object detection, semantic, instance and panoptic segmentations) has attracted
increasing research attention. However, most of existing approaches are
task-specific and individually tackle each task. In this paper, we propose a
Unified Open-Vocabulary Network (UOVN) to jointly address four common dense
prediction tasks. Compared with separate models, a unified network is more
desirable for diverse industrial applications. Moreover, OV dense prediction
training data is relatively less. Separate networks can only leverage
task-relevant training data, while a unified approach can integrate diverse
training data to boost individual tasks. We address two major challenges in
unified OV prediction. Firstly, unlike unified methods for fixed-set
predictions, OV networks are usually trained with multi-modal data. Therefore,
we propose a multi-modal, multi-scale and multi-task (MMM) decoding mechanism
to better leverage multi-modal data. Secondly, because UOVN uses data from
different tasks for training, there are significant domain and task gaps. We
present a UOVN training mechanism to reduce such gaps. Experiments on four
datasets demonstrate the effectiveness of our UOVN.",None,-1
Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model,0.051095,"Model inversion attacks (MIAs) aim to recover private data from inaccessible
training sets of deep learning models, posing a privacy threat. MIAs primarily
focus on the white-box scenario where attackers have full access to the model's
structure and parameters. However, practical applications are usually in
black-box scenarios or label-only scenarios, i.e., the attackers can only
obtain the output confidence vectors or labels by accessing the model.
Therefore, the attack models in existing MIAs are difficult to effectively
train with the knowledge of the target model, resulting in sub-optimal attacks.
To the best of our knowledge, we pioneer the research of a powerful and
practical attack model in the label-only scenario.
  In this paper, we develop a novel MIA method, leveraging a conditional
diffusion model (CDM) to recover representative samples under the target label
from the training set. Two techniques are introduced: selecting an auxiliary
dataset relevant to the target model task and using predicted labels as
conditions to guide training CDM; and inputting target label, pre-defined
guidance strength, and random noise into the trained attack model to generate
and correct multiple results for final selection. This method is evaluated
using Learned Perceptual Image Patch Similarity as a new metric and as a
judgment basis for deciding the values of hyper-parameters. Experimental
results show that this method can generate similar and accurate samples to the
target label, outperforming generators of previous approaches.",None,-1
Getting too personal(ized): The importance of feature choice in online adaptive algorithms,0.214496,"Digital educational technologies offer the potential to customize students'
experiences and learn what works for which students, enhancing the technology
as more students interact with it. We consider whether and when attempting to
discover how to personalize has a cost, such as if the adaptation to personal
information can delay the adoption of policies that benefit all students. We
explore these issues in the context of using multi-armed bandit (MAB)
algorithms to learn a policy for what version of an educational technology to
present to each student, varying the relation between student characteristics
and outcomes and also whether the algorithm is aware of these characteristics.
Through simulations, we demonstrate that the inclusion of student
characteristics for personalization can be beneficial when those
characteristics are needed to learn the optimal action. In other scenarios,
this inclusion decreases performance of the bandit algorithm. Moreover,
including unneeded student characteristics can systematically disadvantage
students with less common values for these characteristics. Our simulations do
however suggest that real-time personalization will be helpful in particular
real-world scenarios, and we illustrate this through case studies using
existing experimental results in ASSISTments. Overall, our simulations show
that adaptive personalization in educational technologies can be a double-edged
sword: real-time adaptation improves student experiences in some contexts, but
the slower adaptation and potentially discriminatory results mean that a more
personalized model is not always beneficial.",None,-1
USA-Net: Unified Semantic and Affordance Representations for Robot Memory,0.597776,"In order for robots to follow open-ended instructions like ""go open the brown
cabinet over the sink"", they require an understanding of both the scene
geometry and the semantics of their environment. Robotic systems often handle
these through separate pipelines, sometimes using very different representation
spaces, which can be suboptimal when the two objectives conflict. In this work,
we present USA-Net, a simple method for constructing a world representation
that encodes both the semantics and spatial affordances of a scene in a
differentiable map. This allows us to build a gradient-based planner which can
navigate to locations in the scene specified using open-ended vocabulary. We
use this planner to consistently generate trajectories which are both shorter
5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than
paths from comparable grid-based planners which don't leverage gradient
information. To our knowledge, this is the first end-to-end differentiable
planner optimizes for both semantics and affordance in a single implicit map.
Code and visuals are available at our website: https://usa.bolte.cc/",None,-1
New Perspectives on Regularization and Computation in Optimal Transport-Based Distributionally Robust Optimization,0.975029,"We study optimal transport-based distributionally robust optimization
problems where a fictitious adversary, often envisioned as nature, can choose
the distribution of the uncertain problem parameters by reshaping a prescribed
reference distribution at a finite transportation cost. In this framework, we
show that robustification is intimately related to various forms of variation
and Lipschitz regularization even if the transportation cost function fails to
be (some power of) a metric. We also derive conditions for the existence and
the computability of a Nash equilibrium between the decision-maker and nature,
and we demonstrate numerically that nature's Nash strategy can be viewed as a
distribution that is supported on remarkably deceptive adversarial samples.
Finally, we identify practically relevant classes of optimal transport-based
distributionally robust optimization problems that can be addressed with
efficient gradient descent algorithms even if the loss function or the
transportation cost function are nonconvex (but not both at the same time).",None,-1
Test Time Adaptation for Blind Image Quality Assessment,0.86683,"While the design of blind image quality assessment (IQA) algorithms has
improved significantly, the distribution shift between the training and testing
scenarios often leads to a poor performance of these methods at inference time.
This motivates the study of test time adaptation (TTA) techniques to improve
their performance at inference time. Existing auxiliary tasks and loss
functions used for TTA may not be relevant for quality-aware adaptation of the
pre-trained model. In this work, we introduce two novel quality-relevant
auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In
particular, we introduce a group contrastive loss at the batch level and a
relative rank loss at the sample level to make the model quality aware and
adapt to the target data. Our experiments reveal that even using a small batch
of images from the test distribution helps achieve significant improvement in
performance by updating the batch normalization statistics of the source model.",None,-1
The Study of Highway for Lifelong Multi-Agent Path Finding,0.29588,"In modern fulfillment warehouses, agents traverse the map to complete endless
tasks that arrive on the fly, which is formulated as a lifelong Multi-Agent
Path Finding (lifelong MAPF) problem. The goal of tackling this challenging
problem is to find the path for each agent in a finite runtime while maximizing
the throughput. However, existing methods encounter exponential growth of
runtime and undesirable phenomena of deadlocks and rerouting as the map size or
agent density grows. To address these challenges in lifelong MAPF, we explore
the idea of highways mainly studied for one-shot MAPF (i.e., finding paths at
once beforehand), which reduces the complexity of the problem by encouraging
agents to move in the same direction. We utilize two methods to incorporate the
highway idea into the lifelong MAPF framework and discuss the properties that
minimize the existing problems of deadlocks and rerouting. The experimental
results demonstrate that the runtime is considerably reduced and the decay of
throughput is gradually insignificant as the map size enlarges under the
settings of the highway. Furthermore, when the density of agents increases, the
phenomena of deadlocks and rerouting are significantly reduced by leveraging
the highway.",None,-1
FormalGeo: An Extensible Formalized Framework for Olympiad Geometric Problem Solving,0.535811,"This is the first paper in a series of work we have accomplished over the
past three years. In this paper, we have constructed a consistent formal plane
geometry system. This will serve as a crucial bridge between IMO-level plane
geometry challenges and readable AI automated reasoning. Within this formal
framework, we have been able to seamlessly integrate modern AI models with our
formal system. AI is now capable of providing deductive reasoning solutions to
IMO-level plane geometry problems, just like handling other natural languages,
and these proofs are readable, traceable, and verifiable. We propose the
geometry formalization theory (GFT) to guide the development of the geometry
formal system. Based on the GFT, we have established the FormalGeo, which
consists of 88 geometric predicates and 196 theorems. It can represent,
validate, and solve IMO-level geometry problems. we also have crafted the FGPS
(formal geometry problem solver) in Python. It serves as both an interactive
assistant for verifying problem-solving processes and an automated problem
solver. We've annotated the formalgeo7k and formalgeo-imo datasets. The former
contains 6,981 (expand to 133,818 through data augmentation) geometry problems,
while the latter includes 18 (expand to 2,627 and continuously increasing)
IMO-level challenging geometry problems. All annotated problems include
detailed formal language descriptions and solutions. Implementation of the
formal system and experiments validate the correctness and utility of the GFT.
The backward depth-first search method only yields a 2.42% problem-solving
failure rate, and we can incorporate deep learning techniques to achieve lower
one. The source code of FGPS and datasets are available at
https://github.com/BitSecret/FGPS.",None,-1
CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis,0.641148,"While recent text-to-speech (TTS) systems have made remarkable strides toward
human-level quality, the performance of cross-lingual TTS lags behind that of
intra-lingual TTS. This gap is mainly rooted from the speaker-language
entanglement problem in cross-lingual TTS. In this paper, we propose
CrossSpeech which improves the quality of cross-lingual speech by effectively
disentangling speaker and language information in the level of acoustic feature
space. Specifically, CrossSpeech decomposes the speech generation pipeline into
the speaker-independent generator (SIG) and speaker-dependent generator (SDG).
The SIG produces the speaker-independent acoustic representation which is not
biased to specific speaker distributions. On the other hand, the SDG models
speaker-dependent speech variation that characterizes speaker attributes. By
handling each information separately, CrossSpeech can obtain disentangled
speaker and language representations. From the experiments, we verify that
CrossSpeech achieves significant improvements in cross-lingual TTS, especially
in terms of speaker similarity to the target speaker.",None,-1
From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models,0.0286899,"Instruction tuning is essential for large language models (LLMs) to become
interactive. While many instruction tuning datasets exist in English, there is
a noticeable lack in other languages. Also, their effectiveness has not been
well verified in non-English languages. We construct a Japanese instruction
dataset by expanding and filtering existing datasets and apply the dataset to a
Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning
on both Japanese and English existing models using our instruction dataset. We
evaluated these models from both quantitative and qualitative perspectives. As
a result, the effectiveness of Japanese instruction datasets is confirmed. The
results also indicate that even with relatively small LLMs, performances in
downstream tasks would be improved through instruction tuning. Our instruction
dataset, tuned models, and implementation are publicly available online.",None,-1
Document-level Relation Extraction with Cross-sentence Reasoning Graph,0.844489,"Relation extraction (RE) has recently moved from the sentence-level to
document-level, which requires aggregating document information and using
entities and mentions for reasoning. Existing works put entity nodes and
mention nodes with similar representations in a document-level graph, whose
complex edges may incur redundant information. Furthermore, existing studies
only focus on entity-level reasoning paths without considering global
interactions among entities cross-sentence. To these ends, we propose a novel
document-level RE model with a GRaph information Aggregation and Cross-sentence
Reasoning network (GRACR). Specifically, a simplified document-level graph is
constructed to model the semantic information of all mentions and sentences in
a document, and an entity-level graph is designed to explore relations of
long-distance cross-sentence entity pairs. Experimental results show that GRACR
achieves excellent performance on two public datasets of document-level RE. It
is especially effective in extracting potential relations of cross-sentence
entity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.",None,-1
A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding,0.741395,"Zero-shot dialogue understanding aims to enable dialogue to track the user's
needs without any training data, which has gained increasing attention. In this
work, we investigate the understanding ability of ChatGPT for zero-shot
dialogue understanding tasks including spoken language understanding (SLU) and
dialogue state tracking (DST). Experimental results on four popular benchmarks
reveal the great potential of ChatGPT for zero-shot dialogue understanding. In
addition, extensive analysis shows that ChatGPT benefits from the multi-turn
interactive prompt in the DST task but struggles to perform slot filling for
SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue
understanding tasks, hoping to provide some insights for future research on
building zero-shot dialogue understanding systems with Large Language Models
(LLMs).",None,-1
NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models,0.983609,"Automatically generating high-quality real world 3D scenes is of enormous
interest for applications such as virtual reality and robotics simulation.
Towards this goal, we introduce NeuralField-LDM, a generative model capable of
synthesizing complex 3D environments. We leverage Latent Diffusion Models that
have been successfully utilized for efficient high-quality 2D content creation.
We first train a scene auto-encoder to express a set of image and pose pairs as
a neural field, represented as density and feature voxel grids that can be
projected to produce novel views of the scene. To further compress this
representation, we train a latent-autoencoder that maps the voxel grids to a
set of latent representations. A hierarchical diffusion model is then fit to
the latents to complete the scene generation pipeline. We achieve a substantial
improvement over existing state-of-the-art scene generation models.
Additionally, we show how NeuralField-LDM can be used for a variety of 3D
content creation applications, including conditional scene generation, scene
inpainting and scene style manipulation.",None,-1
LLM Cognitive Judgements Differ From Human,0.0780877,"Large Language Models (LLMs) have lately been on the spotlight of
researchers, businesses, and consumers alike. While the linguistic capabilities
of such models have been studied extensively, there is growing interest in
investigating them as cognitive subjects. In the present work I examine GPT-3
and ChatGPT capabilities on an limited-data inductive reasoning task from the
cognitive science literature. The results suggest that these models' cognitive
judgements are not human-like.",None,-1
Parmesan: mathematical concept extraction for education,0.507478,"Mathematics is a highly specialized domain with its own unique set of
challenges that has seen limited study in natural language processing. However,
mathematics is used in a wide variety of fields and multidisciplinary research
in many different domains often relies on an understanding of mathematical
concepts. To aid researchers coming from other fields, we develop a prototype
system for searching for and defining mathematical concepts in context,
focusing on the field of category theory. This system, Parmesan, depends on
natural language processing components including concept extraction, relation
extraction, definition extraction, and entity linking. In developing this
system, we show that existing techniques cannot be applied directly to the
category theory domain, and suggest hybrid techniques that do perform well,
though we expect the system to evolve over time. We also provide two cleaned
mathematical corpora that power the prototype system, which are based on
journal articles and wiki pages, respectively. The corpora have been annotated
with dependency trees, lemmas, and part-of-speech tags.",None,-1
Open-TI: Open Traffic Intelligence with Augmented Language Model,0.791498,"Transportation has greatly benefited the cities' development in the modern
civilization process. Intelligent transportation, leveraging advanced computer
algorithms, could further increase people's daily commuting efficiency.
However, intelligent transportation, as a cross-discipline, often requires
practitioners to comprehend complicated algorithms and obscure neural networks,
bringing a challenge for the advanced techniques to be trusted and deployed in
practical industries. Recognizing the expressiveness of the pre-trained large
language models, especially the potential of being augmented with abilities to
understand and execute intricate commands, we introduce Open-TI. Serving as a
bridge to mitigate the industry-academic gap, Open-TI is an innovative model
targeting the goal of Turing Indistinguishable Traffic Intelligence, it is
augmented with the capability to harness external traffic analysis packages
based on existing conversations. Marking its distinction, Open-TI is the first
method capable of conducting exhaustive traffic analysis from scratch -
spanning from map data acquisition to the eventual execution in complex
simulations. Besides, Open-TI is able to conduct task-specific embodiment like
training and adapting the traffic signal control policies (TSC), explore demand
optimizations, etc. Furthermore, we explored the viability of LLMs directly
serving as control agents, by understanding the expected intentions from
Open-TI, we designed an agent-to-agent communication mode to support Open-TI
conveying messages to ChatZero (control agent), and then the control agent
would choose from the action space to proceed the execution. We eventually
provide the formal implementation structure, and the open-ended design invites
further community-driven enhancements.",None,-1
Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception,0.827748,"Multi-agent collaborative perception as a potential application for
vehicle-to-everything communication could significantly improve the perception
performance of autonomous vehicles over single-agent perception. However,
several challenges remain in achieving pragmatic information sharing in this
emerging research. In this paper, we propose SCOPE, a novel collaborative
perception framework that aggregates the spatio-temporal awareness
characteristics across on-road agents in an end-to-end manner. Specifically,
SCOPE has three distinct strengths: i) it considers effective semantic cues of
the temporal context to enhance current representations of the target agent;
ii) it aggregates perceptually critical spatial information from heterogeneous
agents and overcomes localization errors via multi-scale feature interactions;
iii) it integrates multi-source representations of the target agent based on
their complementary contributions by an adaptive fusion paradigm. To thoroughly
evaluate SCOPE, we consider both real-world and simulated scenarios of
collaborative 3D object detection tasks on three datasets. Extensive
experiments demonstrate the superiority of our approach and the necessity of
the proposed components.",None,-1
"Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques",0.516021,"This paper describes the participation of team QUST in the SemEval2023 task
3. The monolingual models are first evaluated with the under-sampling of the
majority classes in the early stage of the task. Then, the pre-trained
multilingual model is fine-tuned with a combination of the class weights and
the sample weights. Two different fine-tuning strategies, the task-agnostic and
the task-dependent, are further investigated. All experiments are conducted
under the 10-fold cross-validation, the multilingual approaches are superior to
the monolingual ones. The submitted system achieves the second best in Italian
and Spanish (zero-shot) in subtask-1.",None,-1
Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues,0.475984,"In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.",None,-1
Revisiting Recommendation Loss Functions through Contrastive Learning (Technical Report),0.180195,"Inspired by the success of contrastive learning, we systematically examine
recommendation losses, including listwise (softmax), pairwise (BPR), and
pointwise (MSE and CCL) losses. In this endeavor, we introduce InfoNCE+, an
optimized generalization of InfoNCE with balance coefficients, and highlight
its performance advantages, particularly when aligned with our new decoupled
contrastive loss, MINE+. We also leverage debiased InfoNCE to debias pointwise
recommendation loss (CCL) as Debiased CCL. Interestingly, our analysis reveals
that linear models like iALS and EASE are inherently debiased. Empirical
results demonstrates the effectiveness of MINE+ and Debiased-CCL.",None,-1
Your Day in Your Pocket: Complex Activity Recognition from Smartphone Accelerometers,0.468659,"Human Activity Recognition (HAR) enables context-aware user experiences where
mobile apps can alter content and interactions depending on user activities.
Hence, smartphones have become valuable for HAR as they allow large, and
diversified data collection. Although previous work in HAR managed to detect
simple activities (i.e., sitting, walking, running) with good accuracy using
inertial sensors (i.e., accelerometer), the recognition of complex daily
activities remains an open problem, specially in remote work/study settings
when people are more sedentary. Moreover, understanding the everyday activities
of a person can support the creation of applications that aim to support their
well-being. This paper investigates the recognition of complex activities
exclusively using smartphone accelerometer data. We used a large smartphone
sensing dataset collected from over 600 users in five countries during the
pandemic and showed that deep learning-based, binary classification of eight
complex activities (sleeping, eating, watching videos, online communication,
attending a lecture, sports, shopping, studying) can be achieved with AUROC
scores up to 0.76 with partially personalized models. This shows encouraging
signs toward assessing complex activities only using phone accelerometer data
in the post-pandemic world.",None,-1
DKT-STDRL: Spatial and Temporal Representation Learning Enhanced Deep Knowledge Tracing for Learning Performance Prediction,0.133905,"Knowledge tracing (KT) serves as a primary part of intelligent education
systems. Most current KTs either rely on expert judgments or only exploit a
single network structure, which affects the full expression of learning
features. To adequately mine features of students' learning process, Deep
Knowledge Tracing Based on Spatial and Temporal Deep Representation Learning
for Learning Performance Prediction (DKT-STDRL) is proposed in this paper.
DKT-STDRL extracts spatial features from students' learning history sequence,
and then further extracts temporal features to extract deeper hidden
information. Specifically, firstly, the DKT-STDRL model uses CNN to extract the
spatial feature information of students' exercise sequences. Then, the spatial
features are connected with the original students' exercise features as joint
learning features. Then, the joint features are input into the BiLSTM part.
Finally, the BiLSTM part extracts the temporal features from the joint learning
features to obtain the prediction information of whether the students answer
correctly at the next time step. Experiments on the public education datasets
ASSISTment2009, ASSISTment2015, Synthetic-5, ASSISTchall, and Statics2011 prove
that DKT-STDRL can achieve better prediction effects than DKT and CKT.",None,-1
KGTrust: Evaluating Trustworthiness of SIoT via Knowledge Enhanced Graph Neural Networks,0.506885,"Social Internet of Things (SIoT), a promising and emerging paradigm that
injects the notion of social networking into smart objects (i.e., things),
paving the way for the next generation of Internet of Things. However, due to
the risks and uncertainty, a crucial and urgent problem to be settled is
establishing reliable relationships within SIoT, that is, trust evaluation.
Graph neural networks for trust evaluation typically adopt a straightforward
way such as one-hot or node2vec to comprehend node characteristics, which
ignores the valuable semantic knowledge attached to nodes. Moreover, the
underlying structure of SIoT is usually complex, including both the
heterogeneous graph structure and pairwise trust relationships, which renders
hard to preserve the properties of SIoT trust during information propagation.
To address these aforementioned problems, we propose a novel knowledge-enhanced
graph neural network (KGTrust) for better trust evaluation in SIoT.
Specifically, we first extract useful knowledge from users' comment behaviors
and external structured triples related to object descriptions, in order to
gain a deeper insight into the semantics of users and objects. Furthermore, we
introduce a discriminative convolutional layer that utilizes heterogeneous
graph structure, node semantics, and augmented trust relationships to learn
node embeddings from the perspective of a user as a trustor or a trustee,
effectively capturing multi-aspect properties of SIoT trust during information
propagation. Finally, a trust prediction layer is developed to estimate the
trust relationships between pairwise nodes. Extensive experiments on three
public datasets illustrate the superior performance of KGTrust over
state-of-the-art methods.",None,-1
Analysis of Recent Trends in Face Recognition Systems,0.406701,"With the tremendous advancements in face recognition technology, face
modality has been widely recognized as a significant biometric identifier in
establishing a person's identity rather than any other biometric trait like
fingerprints that require contact sensors. However, due to inter-class
similarities and intra-class variations, face recognition systems generate
false match and false non-match errors respectively. Recent research focuses on
improving the robustness of extracted features and the pre-processing
algorithms to enhance recognition accuracy. Since face recognition has been
extensively used for several applications ranging from law enforcement to
surveillance systems, the accuracy and performance of face recognition must be
the finest. In this paper various face recognition systems are discussed and
analysed like RPRV, LWKPCA, SVM Model, LTrP based SPM and a deep learning
framework for recognising images from CCTV. All these face recognition methods,
their implementations and performance evaluations are compared to derive the
best outcome for future developmental works.",None,-1
Multilingual k-Nearest-Neighbor Machine Translation,0.269713,"k-nearest-neighbor machine translation has demonstrated remarkable
improvements in machine translation quality by creating a datastore of cached
examples. However, these improvements have been limited to high-resource
language pairs, with large datastores, and remain a challenge for low-resource
languages. In this paper, we address this issue by combining representations
from multiple languages into a single datastore. Our results consistently
demonstrate substantial improvements not only in low-resource translation
quality (up to +3.6 BLEU), but also for high-resource translation quality (up
to +0.5 BLEU). Our experiments show that it is possible to create multilingual
datastores that are a quarter of the size, achieving a 5.3x speed improvement,
by using linguistic similarities for datastore creation.",None,-1
UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction,0.625422,"Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the
development and operation of the smart city. As an emerging building block,
multi-sourced urban data are usually integrated as urban knowledge graphs
(UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction
models. However, existing UrbanKGs are often tailored for specific downstream
prediction tasks and are not publicly available, which limits the potential
advancement. This paper presents UUKG, the unified urban knowledge graph
dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically,
we first construct UrbanKGs consisting of millions of triplets for two
metropolises by connecting heterogeneous urban entities such as administrative
boroughs, POIs, and road segments. Moreover, we conduct qualitative and
quantitative analysis on constructed UrbanKGs and uncover diverse high-order
structural patterns, such as hierarchies and cycles, that can be leveraged to
benefit downstream USTP tasks. To validate and facilitate the use of UrbanKGs,
we implement and evaluate 15 KG embedding methods on the KG completion task and
integrate the learned KG embeddings into 9 spatiotemporal models for five
different USTP tasks. The extensive experimental results not only provide
benchmarks of knowledge-enhanced USTP models under different task settings but
also highlight the potential of state-of-the-art high-order structure-aware
UrbanKG embedding methods. We hope the proposed UUKG fosters research on urban
knowledge graphs and broad smart city applications. The dataset and source code
are available at https://github.com/usail-hkust/UUKG/.",None,-1
DDT: Dual-branch Deformable Transformer for Image Denoising,0.196828,"Transformer is beneficial for image denoising tasks since it can model
long-range dependencies to overcome the limitations presented by inductive
convolutional biases. However, directly applying the transformer structure to
remove noise is challenging because its complexity grows quadratically with the
spatial resolution. In this paper, we propose an efficient Dual-branch
Deformable Transformer (DDT) denoising network which captures both local and
global interactions in parallel. We divide features with a fixed patch size and
a fixed number of patches in local and global branches, respectively. In
addition, we apply deformable attention operation in both branches, which helps
the network focus on more important regions and further reduces computational
complexity. We conduct extensive experiments on real-world and synthetic
denoising tasks, and the proposed DDT achieves state-of-the-art performance
with significantly fewer computational costs.",None,-1
Large-scale Ridesharing DARP Instances Based on Real Travel Demand,0.632121,"Accurately predicting the real-life performance of algorithms solving the
Dial-a-Ride Problem (DARP) in the context of Mobility on Demand (MoD) systems
with ridesharing requires evaluating them on representative instances. However,
the benchmarking of state-of-the-art DARP solution methods has been limited to
small, artificial instances or outdated non-public instances, hindering direct
comparisons. With the rise of large MoD systems and the availability of open
travel demand datasets for many US cities, there is now an opportunity to
evaluate these algorithms on standardized, realistic, and representative
instances. Despite the significant challenges involved in processing obfuscated
and diverse datasets, we have developed a methodology using which we have
created a comprehensive set of large-scale demand instances based on real-world
data. These instances cover diverse use cases, one of which is demonstrated in
an evaluation of two established DARP methods: the insertion heuristic and
optimal vehicle-group assignment method. We publish the full results of both
methods in a standardized format. The results show significant differences
between areas in all measured quantities, emphasizing the importance of
evaluating methods across different cities.",None,-1
NOPE: Novel Object Pose Estimation from a Single Image,0.633687,"The practicality of 3D object pose estimation remains limited for many
applications due to the need for prior knowledge of a 3D model and a training
period for new objects. To address this limitation, we propose an approach that
takes a single image of a new object as input and predicts the relative pose of
this object in new images without prior knowledge of the object's 3D model and
without requiring training time for new objects and categories. We achieve this
by training a model to directly predict discriminative embeddings for
viewpoints surrounding the object. This prediction is done using a simple U-Net
architecture with attention and conditioned on the desired pose, which yields
extremely fast inference. We compare our approach to state-of-the-art methods
and show it outperforms them both in terms of accuracy and robustness. Our
source code is publicly available at https://github.com/nv-nguyen/nope",None,-1
CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models,0.00840268,"Parameter-efficient tuning (PET) has been widely explored in recent years
because it tunes much fewer parameters (PET modules) than full-parameter
fine-tuning (FT) while still stimulating sufficient knowledge from large
language models (LLMs) for downstream tasks. Moreover, when PET is employed to
serve multiple tasks, different task-specific PET modules can be built on a
frozen LLM, avoiding redundant LLM deployments. Although PET significantly
reduces the cost of tuning and deploying LLMs, its inference still suffers from
the computational bottleneck of LLMs. To address the above issue, we propose an
effective PET framework based on compressed LLMs, named ""CPET"". In CPET, we
evaluate the impact of mainstream LLM compression techniques on PET performance
and then introduce knowledge inheritance and recovery strategies to restore the
knowledge loss caused by these compression techniques. Our experimental results
demonstrate that, owing to the restoring strategies of CPET, collaborating
task-specific PET modules with a compressed LLM can achieve comparable
performance to collaborating PET modules with the original version of the
compressed LLM and outperform directly applying vanilla PET methods to the
compressed LLM.",None,-1
Complex Claim Verification with Evidence Retrieved in the Wild,0.901963,"Evidence retrieval is a core part of automatic fact-checking. Prior work
makes simplifying assumptions in retrieval that depart from real-world use
cases: either no access to evidence, access to evidence curated by a human
fact-checker, or access to evidence available long after the claim has been
made. In this work, we present the first fully automated pipeline to check
real-world claims by retrieving raw evidence from the web. We restrict our
retriever to only search documents available prior to the claim's making,
modeling the realistic scenario where an emerging claim needs to be checked.
Our pipeline includes five components: claim decomposition, raw document
retrieval, fine-grained evidence retrieval, claim-focused summarization, and
veracity judgment. We conduct experiments on complex political claims in the
ClaimDecomp dataset and show that the aggregated evidence produced by our
pipeline improves veracity judgments. Human evaluation finds the evidence
summary produced by our system is reliable (it does not hallucinate
information) and relevant to answering key questions about a claim, suggesting
that it can assist fact-checkers even when it cannot surface a complete
evidence set.",None,-1
Independent Component Alignment for Multi-Task Learning,0.591255,"In a multi-task learning (MTL) setting, a single model is trained to tackle a
diverse set of tasks jointly. Despite rapid progress in the field, MTL remains
challenging due to optimization issues such as conflicting and dominating
gradients. In this work, we propose using a condition number of a linear system
of gradients as a stability criterion of an MTL optimization. We theoretically
demonstrate that a condition number reflects the aforementioned optimization
issues. Accordingly, we present Aligned-MTL, a novel MTL optimization approach
based on the proposed criterion, that eliminates instability in the training
process by aligning the orthogonal components of the linear system of
gradients. While many recent MTL approaches guarantee convergence to a minimum,
task trade-offs cannot be specified in advance. In contrast, Aligned-MTL
provably converges to an optimal point with pre-defined task-specific weights,
which provides more control over the optimization result. Through experiments,
we show that the proposed approach consistently improves performance on a
diverse set of MTL benchmarks, including semantic and instance segmentation,
depth estimation, surface normal estimation, and reinforcement learning. The
source code is publicly available at https://github.com/SamsungLabs/MTL .",None,-1
SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,0.726047,"In the last year alone, a surge of new benchmarks to measure compositional
understanding of vision-language models have permeated the machine learning
ecosystem. Given an image, these benchmarks probe a model's ability to identify
its associated caption amongst a set of compositional distractors.
Surprisingly, we find significant biases in all these benchmarks rendering them
hackable. This hackability is so dire that blind models with no access to the
image outperform state-of-the-art vision-language models. To remedy this
rampant vulnerability, we introduce SugarCrepe, a new benchmark for
vision-language compositionality evaluation. We employ large language models,
instead of rule-based templates used in previous benchmarks, to generate fluent
and sensical hard negatives, and utilize an adversarial refinement mechanism to
maximally reduce biases. We re-evaluate state-of-the-art models and recently
proposed compositionality inducing strategies, and find that their improvements
were hugely overestimated, suggesting that more innovation is needed in this
important direction. We release SugarCrepe and the code for evaluation at:
https://github.com/RAIVNLab/sugar-crepe.",None,-1
Can Language Models Employ the Socratic Method? Experiments with Code Debugging,0.802069,"When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark",None,-1
With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector,0.628339,"This work presents our efforts to reproduce the results of the human
evaluation experiment presented in the paper of Vamvas and Sennrich (2022),
which evaluated an automatic system detecting over- and undertranslations
(translations containing more or less information than the original) in machine
translation (MT) outputs. Despite the high quality of the documentation and
code provided by the authors, we discuss some problems we found in reproducing
the exact experimental setup and offer recommendations for improving
reproducibility. Our replicated results generally confirm the conclusions of
the original study, but in some cases, statistically significant differences
were observed, suggesting a high variability of human annotation.",None,-1
Pre-training Contextual Location Embeddings in Personal Trajectories via Efficient Hierarchical Location Representations,0.208703,"Pre-training the embedding of a location generated from human mobility data
has become a popular method for location based services. In practice, modeling
the location embedding is too expensive, due to the large number of locations
to be trained in situations with fine-grained resolution or extensive target
regions. Previous studies have handled less than ten thousand distinct
locations, which is insufficient in the real-world applications. To tackle this
problem, we propose a Geo-Tokenizer, designed to efficiently reduce the number
of locations to be trained by representing a location as a combination of
several grids at different scales. In the Geo-Tokenizer, a grid at a larger
scale shares the common set of grids at smaller scales, which is a key factor
in reducing the size of the location vocabulary. The sequences of locations
preprocessed with the Geo-Tokenizer are utilized by a causal location embedding
model to capture the temporal dependencies of locations. This model dynamically
calculates the embedding vector of a target location, which varies depending on
its trajectory. In addition, to efficiently pre-train the location embedding
model, we propose the Hierarchical Auto-regressive Location Model objective to
effectively train decomposed locations in the Geo-Tokenizer. We conducted
experiments on two real-world user trajectory datasets using our pre-trained
location model. The experimental results show that our model significantly
improves the performance of downstream tasks with fewer model parameters
compared to existing location embedding methods.",None,-1
MathChat: Converse to Tackle Challenging Math Problems with LLM Agents,0.999455,"Employing Large Language Models (LLMs) to address mathematical problems is an
intriguing research endeavor, considering the abundance of math problems
expressed in natural language across numerous science and engineering fields.
LLMs, with their generalized ability, are used as a foundation model to build
AI agents for different tasks. In this paper, we study the effectiveness of
utilizing LLM agents to solve math problems through conversations. We propose
MathChat, a conversational problem-solving framework designed for math
problems. MathChat consists of an LLM agent and a user proxy agent which is
responsible for tool execution and additional guidance. This synergy
facilitates a collaborative problem-solving process, where the agents engage in
a dialogue to solve the problems. We perform evaluation on difficult high
school competition problems from the MATH dataset. Utilizing Python, we show
that MathChat can further improve previous tool-using prompting methods by 6%.",None,-1
Elliptic PDE learning is provably data-efficient,0.17317,"PDE learning is an emerging field that combines physics and machine learning
to recover unknown physical systems from experimental data. While deep learning
models traditionally require copious amounts of training data, recent PDE
learning techniques achieve spectacular results with limited data availability.
Still, these results are empirical. Our work provides theoretical guarantees on
the number of input-output training pairs required in PDE learning.
Specifically, we exploit randomized numerical linear algebra and PDE theory to
derive a provably data-efficient algorithm that recovers solution operators of
3D uniformly elliptic PDEs from input-output data and achieves an exponential
convergence rate of the error with respect to the size of the training dataset
with an exceptionally high probability of success.",None,-1
Clickbait Detection via Large Language Models,0.0572167,"Clickbait, which aims to induce users with some surprising and even thrilling
headlines for increasing click-through rates, permeates almost all online
content publishers, such as news portals and social media. Recently, Large
Language Models (LLMs) have emerged as a powerful instrument and achieved
tremendous success in a series of NLP downstream tasks. However, it is not yet
known whether LLMs can be served as a high-quality clickbait detection system.
In this paper, we analyze the performance of LLMs in the few-shot and zero-shot
scenarios on several English and Chinese benchmark datasets. Experimental
results show that LLMs cannot achieve the best results compared to the
state-of-the-art deep and fine-tuning PLMs methods. Different from human
intuition, the experiments demonstrated that LLMs cannot make satisfied
clickbait detection just by the headlines.",None,-1
Inferring Fluid Dynamics via Inverse Rendering,0.414274,"Humans have a strong intuitive understanding of physical processes such as
fluid falling by just a glimpse of such a scene picture, i.e., quickly derived
from our immersive visual experiences in memory. This work achieves such a
photo-to-fluid-dynamics reconstruction functionality learned from unannotated
videos, without any supervision of ground-truth fluid dynamics. In a nutshell,
a differentiable Euler simulator modeled with a ConvNet-based pressure
projection solver, is integrated with a volumetric renderer, supporting
end-to-end/coherent differentiable dynamic simulation and rendering. By
endowing each sampled point with a fluid volume value, we derive a NeRF-like
differentiable renderer dedicated from fluid data; and thanks to this
volume-augmented representation, fluid dynamics could be inversely inferred
from the error signal between the rendered result and ground-truth video frame
(i.e., inverse rendering). Experiments on our generated Fluid Fall datasets and
DPI Dam Break dataset are conducted to demonstrate both effectiveness and
generalization ability of our method.",None,-1
