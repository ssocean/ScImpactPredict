id,title,TNCSI,abstract,OA,authors_title
a43e0a62-d59e-4aaa-ab3c-902d6431b48d,Graph Agent: Explicit Reasoning Agent for Graphs,0.471165,"Graph embedding methods such as Graph Neural Networks (GNNs) and Graph
Transformers have contributed to the development of graph reasoning algorithms
for various tasks on knowledge graphs. However, the lack of interpretability
and explainability of graph embedding methods has limited their applicability
in scenarios requiring explicit reasoning. In this paper, we introduce the
Graph Agent (GA), an intelligent agent methodology of leveraging large language
models (LLMs), inductive-deductive reasoning modules, and long-term memory for
knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning
and existing graph embedding methods to provide an innovative approach for
complex graph reasoning tasks. By converting graph structures into textual
data, GA enables LLMs to process, reason, and provide predictions alongside
human-interpretable explanations. The effectiveness of the GA was evaluated on
node classification and link prediction tasks. Results showed that GA reached
state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and
89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to
existing GNN and transformer models, GA offered advantages of explicit
reasoning ability, free-of-training, easy adaption to various graph reasoning
tasks",None,-1
cd14f10a-4cd9-448a-955c-6f2f42204062,"Does the ""most sinfully decadent cake ever"" taste good? Answering Yes/No Questions from Figurative Contexts",0.0290516,"Figurative language is commonplace in natural language, and while making
communication memorable and creative, can be difficult to understand. In this
work, we investigate the robustness of Question Answering (QA) models on
figurative text. Yes/no questions, in particular, are a useful probe of
figurative language understanding capabilities of large language models. We
propose FigurativeQA, a set of 1000 yes/no questions with figurative and
non-figurative contexts, extracted from the domains of restaurant and product
reviews. We show that state-of-the-art BERT-based QA models exhibit an average
performance drop of up to 15\% points when answering questions from figurative
contexts, as compared to non-figurative ones. While models like GPT-3 and
ChatGPT are better at handling figurative texts, we show that further
performance gains can be achieved by automatically simplifying the figurative
contexts into their non-figurative (literal) counterparts. We find that the
best overall model is ChatGPT with chain-of-thought prompting to generate
non-figurative contexts. Our work provides a promising direction for building
more robust QA models with figurative language understanding capabilities.",None,-1
b1a86dd0-aa79-4153-a72e-cebeda6adfe9,Specialist or Generalist? Instruction Tuning for Specific NLP Tasks,0.282464,"The potential of large language models (LLMs) to simultaneously perform a
wide range of natural language processing (NLP) tasks has been the subject of
extensive research. Although instruction tuning has proven to be a
data-efficient method for transforming LLMs into such generalist models, their
performance still lags behind specialist models trained exclusively for
specific tasks. In this paper, we investigate whether incorporating
broad-coverage generalist instruction tuning can contribute to building a
specialist model. We hypothesize that its efficacy depends on task specificity
and skill requirements. Our experiments assess four target tasks with distinct
coverage levels, revealing that integrating generalist instruction tuning
consistently enhances model performance when the task coverage is broad. The
effect is particularly pronounced when the amount of task-specific training
data is limited. Further investigation into three target tasks focusing on
different capabilities demonstrates that generalist instruction tuning improves
understanding and reasoning abilities. However, for tasks requiring factual
knowledge, generalist data containing hallucinatory information may negatively
affect the model's performance. Overall, our work provides a systematic guide
for developing specialist models with general instruction tuning. Our code and
other related resources can be found at
https://github.com/DavidFanzz/Generalist_or_Specialist.",None,-1
79556151-37ab-4718-a0e1-79f9b74a0236,Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation,0.0831274,"We introduce a deterministic approach to edge detection and image
segmentation by formulating pseudo-Boolean polynomials on image patches. The
approach works by applying a binary classification of blob and edge regions in
an image based on the degrees of pseudo-Boolean polynomials calculated on
patches extracted from the provided image. We test our method on simple images
containing primitive shapes of constant and contrasting colour and establish
the feasibility before applying it to complex instances like aerial landscape
images. The proposed method is based on the exploitation of the reduction,
polynomial degree, and equivalence properties of penalty-based pseudo-Boolean
polynomials.",None,-1
5f7660ba-c5d0-4362-8cdb-5b77e2397e2a,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,0.776024,"A robot operating in a household environment will see a wide range of unique
and unfamiliar objects. While a system could train on many of these, it is
infeasible to predict all the objects a robot will see. In this paper, we
present a method to generalize object manipulation skills acquired from a
limited number of demonstrations, to novel objects from unseen shape
categories. Our approach, Local Neural Descriptor Fields (L-NDF), utilizes
neural descriptors defined on the local geometry of the object to effectively
transfer manipulation demonstrations to novel objects at test time. In doing
so, we leverage the local geometry shared between objects to produce a more
general manipulation framework. We illustrate the efficacy of our approach in
manipulating novel objects in novel poses -- both in simulation and in the real
world.",None,-1
2771e076-3217-47ff-a0f5-9308d935726f,Mark My Words: Dangers of Watermarked Images in ImageNet,0.0686769,"The utilization of pre-trained networks, especially those trained on
ImageNet, has become a common practice in Computer Vision. However, prior
research has indicated that a significant number of images in the ImageNet
dataset contain watermarks, making pre-trained networks susceptible to learning
artifacts such as watermark patterns within their latent spaces. In this paper,
we aim to assess the extent to which popular pre-trained architectures display
such behavior and to determine which classes are most affected. Additionally,
we examine the impact of watermarks on the extracted features. Contrary to the
popular belief that the Chinese logographic watermarks impact the ""carton""
class only, our analysis reveals that a variety of ImageNet classes, such as
""monitor"", ""broom"", ""apron"" and ""safe"" rely on spurious correlations. Finally,
we propose a simple approach to mitigate this issue in fine-tuned networks by
ignoring the encodings from the feature-extractor layer of ImageNet pre-trained
networks that are most susceptible to watermark imprints.",None,-1
2a0585fb-936e-4255-9296-c971d7807052,Pushing the Limits of Machine Design: Automated CPU Design with AI,0.3609,"Design activity -- constructing an artifact description satisfying given
goals and constraints -- distinguishes humanity from other animals and
traditional machines, and endowing machines with design abilities at the human
level or beyond has been a long-term pursuit. Though machines have already
demonstrated their abilities in designing new materials, proteins, and computer
programs with advanced artificial intelligence (AI) techniques, the search
space for designing such objects is relatively small, and thus, ""Can machines
design like humans?"" remains an open question. To explore the boundary of
machine design, here we present a new AI approach to automatically design a
central processing unit (CPU), the brain of a computer, and one of the world's
most intricate devices humanity have ever designed. This approach generates the
circuit logic, which is represented by a graph structure called Binary
Speculation Diagram (BSD), of the CPU design from only external input-output
observations instead of formal program code. During the generation of BSD,
Monte Carlo-based expansion and the distance of Boolean functions are used to
guarantee accuracy and efficiency, respectively. By efficiently exploring a
search space of unprecedented size 10^{10^{540}}, which is the largest one of
all machine-designed objects to our best knowledge, and thus pushing the limits
of machine design, our approach generates an industrial-scale RISC-V CPU within
only 5 hours. The taped-out CPU successfully runs the Linux operating system
and performs comparably against the human-designed Intel 80486SX CPU. In
addition to learning the world's first CPU only from input-output observations,
which may reform the semiconductor industry by significantly reducing the
design cycle, our approach even autonomously discovers human knowledge of the
von Neumann architecture.",None,-1
a0a7b6ed-93e3-40c6-97e6-f218b6265eab,An Investigation of Noise in Morphological Inflection,0.265067,"With a growing focus on morphological inflection systems for languages where
high-quality data is scarce, training data noise is a serious but so far
largely ignored concern. We aim at closing this gap by investigating the types
of noise encountered within a pipeline for truly unsupervised morphological
paradigm completion and its impact on morphological inflection systems: First,
we propose an error taxonomy and annotation pipeline for inflection training
data. Then, we compare the effect of different types of noise on multiple
state-of-the-art inflection models. Finally, we propose a novel character-level
masked language modeling (CMLM) pretraining objective and explore its impact on
the models' resistance to noise. Our experiments show that various
architectures are impacted differently by separate types of noise, but
encoder-decoders tend to be more robust to noise than models trained with a
copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.",None,-1
aeb13161-2dea-4a44-9c4c-45d29d8f3260,Robustness and Generalizability of Deepfake Detection: A Study with Diffusion Models,0.454549,"The rise of deepfake images, especially of well-known personalities, poses a
serious threat to the dissemination of authentic information. To tackle this,
we present a thorough investigation into how deepfakes are produced and how
they can be identified. The cornerstone of our research is a rich collection of
artificial celebrity faces, titled DeepFakeFace (DFF). We crafted the DFF
dataset using advanced diffusion models and have shared it with the community
through online platforms. This data serves as a robust foundation to train and
test algorithms designed to spot deepfakes. We carried out a thorough review of
the DFF dataset and suggest two evaluation methods to gauge the strength and
adaptability of deepfake recognition tools. The first method tests whether an
algorithm trained on one type of fake images can recognize those produced by
other methods. The second evaluates the algorithm's performance with imperfect
images, like those that are blurry, of low quality, or compressed. Given varied
results across deepfake methods and image changes, our findings stress the need
for better deepfake detectors. Our DFF dataset and tests aim to boost the
development of more effective tools against deepfakes.",None,-1
d76ebaee-c001-42d9-9b4c-d14bb61385fd,MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,0.979906,"We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of
vision experts to achieve multimodal reasoning and action. In this paper, we
define and explore a comprehensive list of advanced vision tasks that are
intriguing to solve, but may exceed the capabilities of existing vision and
vision-language models. To achieve such advanced visual intelligence, MM-REACT
introduces a textual prompt design that can represent text descriptions,
textualized spatial coordinates, and aligned file names for dense visual
signals such as images and videos. MM-REACT's prompt design allows language
models to accept, associate, and process multimodal information, thereby
facilitating the synergetic combination of ChatGPT and various vision experts.
Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the
specified capabilities of interests and its wide application in different
scenarios that require advanced visual understanding. Furthermore, we discuss
and compare MM-REACT's system paradigm with an alternative approach that
extends language models for multimodal scenarios through joint finetuning.
Code, demo, video, and visualization are available at
https://multimodal-react.github.io/",None,-1
a0c43641-5ca8-4f58-887f-858e0a13a0c2,Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors,0.66934,"It is vital to infer signed distance functions (SDFs) from 3D point clouds.
The latest methods rely on generalizing the priors learned from large scale
supervision. However, the learned priors do not generalize well to various
geometric variations that are unseen during training, especially for extremely
sparse point clouds. To resolve this issue, we present a neural network to
directly infer SDFs from single sparse point clouds without using signed
distance supervision, learned priors or even normals. Our insight here is to
learn surface parameterization and SDFs inference in an end-to-end manner. To
make up the sparsity, we leverage parameterized surfaces as a coarse surface
sampler to provide many coarse surface estimations in training iterations,
according to which we mine supervision and our thin plate splines (TPS) based
network infers SDFs as smooth functions in a statistical way. Our method
significantly improves the generalization ability and accuracy in unseen point
clouds. Our experimental results show our advantages over the state-of-the-art
methods in surface reconstruction for sparse point clouds under synthetic
datasets and real scans.The code is available at
\url{https://github.com/chenchao15/NeuralTPS}.",None,-1
348f8ebc-6fc5-449b-9e67-409c8eedb424,"""Im not Racist but..."": Discovering Bias in the Internal Knowledge of Large Language Models",0.105028,"Large language models (LLMs) have garnered significant attention for their
remarkable performance in a continuously expanding set of natural language
processing tasks. However, these models have been shown to harbor inherent
societal biases, or stereotypes, which can adversely affect their performance
in their many downstream applications. In this paper, we introduce a novel,
purely prompt-based approach to uncover hidden stereotypes within any arbitrary
LLM. Our approach dynamically generates a knowledge representation of internal
stereotypes, enabling the identification of biases encoded within the LLM's
internal knowledge. By illuminating the biases present in LLMs and offering a
systematic methodology for their analysis, our work contributes to advancing
transparency and promoting fairness in natural language processing systems.",None,-1
6cf2f0af-1ef4-4ac1-b98a-36f52c559119,Tell Model Where to Attend: Improving Interpretability of Aspect-Based Sentiment Classification via Small Explanation Annotations,0.24788,"Gradient-based explanation methods play an important role in the field of
interpreting complex deep neural networks for NLP models. However, the existing
work has shown that the gradients of a model are unstable and easily
manipulable, which impacts the model's reliability largely. According to our
preliminary analyses, we also find the interpretability of gradient-based
methods is limited for complex tasks, such as aspect-based sentiment
classification (ABSC). In this paper, we propose an
\textbf{I}nterpretation-\textbf{E}nhanced \textbf{G}radient-based framework for
\textbf{A}BSC via a small number of explanation annotations, namely
\texttt{{IEGA}}. Particularly, we first calculate the word-level saliency map
based on gradients to measure the importance of the words in the sentence
towards the given aspect. Then, we design a gradient correction module to
enhance the model's attention on the correct parts (e.g., opinion words). Our
model is model agnostic and task agnostic so that it can be integrated into the
existing ABSC methods or other tasks. Comprehensive experimental results on
four benchmark datasets show that our \texttt{IEGA} can improve not only the
interpretability of the model but also the performance and robustness.",None,-1
5347baec-794a-4c12-8281-93a545ac4485,Holistic Network Virtualization and Pervasive Network Intelligence for 6G,0.993416,"In this tutorial paper, we look into the evolution and prospect of network
architecture and propose a novel conceptual architecture for the 6th generation
(6G) networks. The proposed architecture has two key elements, i.e., holistic
network virtualization and pervasive artificial intelligence (AI). The holistic
network virtualization consists of network slicing and digital twin, from the
aspects of service provision and service demand, respectively, to incorporate
service-centric and user-centric networking. The pervasive network intelligence
integrates AI into future networks from the perspectives of networking for AI
and AI for networking, respectively. Building on holistic network
virtualization and pervasive network intelligence, the proposed architecture
can facilitate three types of interplay, i.e., the interplay between digital
twin and network slicing paradigms, between model-driven and data-driven
methods for network management, and between virtualization and AI, to maximize
the flexibility, scalability, adaptivity, and intelligence for 6G networks. We
also identify challenges and open issues related to the proposed architecture.
By providing our vision, we aim to inspire further discussions and developments
on the potential architecture of 6G.",None,-1
1b4e78ce-0cc0-411b-bc05-db60f67a0c26,MemGPT: Towards LLMs as Operating Systems,0.397283,"Large language models (LLMs) have revolutionized AI, but are constrained by
limited context windows, hindering their utility in tasks like extended
conversations and document analysis. To enable using context beyond limited
context windows, we propose virtual context management, a technique drawing
inspiration from hierarchical memory systems in traditional operating systems
that provide the appearance of large memory resources through data movement
between fast and slow memory. Using this technique, we introduce MemGPT
(Memory-GPT), a system that intelligently manages different memory tiers in
order to effectively provide extended context within the LLM's limited context
window, and utilizes interrupts to manage control flow between itself and the
user. We evaluate our OS-inspired design in two domains where the limited
context windows of modern LLMs severely handicaps their performance: document
analysis, where MemGPT is able to analyze large documents that far exceed the
underlying LLM's context window, and multi-session chat, where MemGPT can
create conversational agents that remember, reflect, and evolve dynamically
through long-term interactions with their users. We release MemGPT code and
data for our experiments at https://memgpt.ai.",None,-1
96155284-1232-4e1b-828e-a4823c284314,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,0.49269,"Retaining walls are often built to prevent excessive lateral movements of the
ground surrounding an excavation site. During an excavation, failure of
retaining walls could cause catastrophic accidents and hence their lateral
deformations are monitored regularly. Laser scanning can rapidly acquire the
spatial data of a relatively large area at fine spatial resolutions, which is
ideal for monitoring retaining walls' deformations. This paper attempts to
apply laser scanning to measurements of the lateral deformations of a soil
mixing retaining wall at an ongoing excavation site. Reference measurements by
total station and inclinometer were also conducted to verify those from the
laser scanning. The deformations derived using laser scanning data were
consistent with the reference measurements at the top part of the retaining
wall (i.e., mainly the ring beam of the wall). This research also shows that
the multi-scale-model-to-model method was the most accurate deformation
estimation method on the research data.",None,-1
6050d5a2-cbd9-46cd-b58e-1b20303e77d3,AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling,0.699167,"Business optimisation refers to the process of finding and implementing
efficient and cost-effective means of operation to bring a competitive
advantage for businesses. Synthesizing problem formulations is an integral part
of business optimisation, which relies on human expertise to construct problem
formulations using optimisation languages. Interestingly, with advancements in
Large Language Models (LLMs), the human expertise needed in problem formulation
can be minimized. However, developing an LLM for problem formulation is
challenging, due to training data, token limitations, and lack of appropriate
performance metrics. For the requirement of training data, recent attention has
been directed towards fine-tuning pre-trained LLMs for downstream tasks rather
than training an LLM from scratch for a specific task. In this paper, we adopt
an LLM fine-tuning approach and propose an AI-Copilot for business optimisation
problem formulation. For token limitations, we introduce modularization and
prompt engineering techniques to synthesize complex problem formulations as
modules that fit into the token limits of LLMs. Additionally, we design
performance evaluation metrics that are better suited for assessing the
accuracy and quality of problem formulations. The experiment results
demonstrate that with this approach we can synthesize complex and large problem
formulations for a typical business optimisation problem in production
scheduling.",None,-1
03ab1645-a9d7-4a81-8700-db5c88c7f77b,Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models,0.532965,"Pre-trained and frozen large language models (LLMs) can effectively map
simple scene rearrangement instructions to programs over a robot's visuomotor
functions through appropriate few-shot example prompting. To parse open-domain
natural language and adapt to a user's idiosyncratic procedures, not known
during prompt engineering time, fixed prompts fall short. In this paper, we
introduce HELPER, an embodied agent equipped with an external memory of
language-program pairs that parses free-form human-robot dialogue into action
programs through retrieval-augmented LLM prompting: relevant memories are
retrieved based on the current dialogue, instruction, correction, or VLM
description, and used as in-context prompt examples for LLM querying. The
memory is expanded during deployment to include pairs of user's language and
action plans, to assist future inferences and personalize them to the user's
language and routines. HELPER sets a new state-of-the-art in the TEACh
benchmark in both Execution from Dialog History (EDH) and Trajectory from
Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for
TfD. Our models, code, and video results can be found in our project's website:
https://helper-agent-llm.github.io.",None,-1
f681a533-1b4e-408c-813b-60995b2e9110,Eye Disease Classification Using Deep Learning Techniques,0.5359,"Eye is the essential sense organ for vision function. Due to the fact that
certain eye disorders might result in vision loss, it is essential to diagnose
and treat eye diseases early on. By identifying common eye illnesses and
performing an eye check, eye care providers can safeguard patients against
vision loss or blindness. Convolutional neural networks (CNN) and transfer
learning were employed in this study to discriminate between a normal eye and
one with diabetic retinopathy, cataract, or glaucoma disease. Using transfer
learning for multi-class classification, high accuracy was achieved at 94%
while the traditional CNN achieved 84% rate.",None,-1
e92d41a9-4aa2-4ad3-8fc9-23ae5290b3ad,Ambiguous Medical Image Segmentation using Diffusion Models,0.976718,"Collective insights from a group of experts have always proven to outperform
an individual's best diagnostic for clinical tasks. For the task of medical
image segmentation, existing research on AI-based alternatives focuses more on
developing models that can imitate the best individual rather than harnessing
the power of expert groups. In this paper, we introduce a single diffusion
model-based approach that produces multiple plausible outputs by learning a
distribution over group insights. Our proposed model generates a distribution
of segmentation masks by leveraging the inherent stochastic sampling process of
diffusion using only minimal additional learning. We demonstrate on three
different medical image modalities- CT, ultrasound, and MRI that our model is
capable of producing several possible variants while capturing the frequencies
of their occurrences. Comprehensive results show that our proposed approach
outperforms existing state-of-the-art ambiguous segmentation networks in terms
of accuracy while preserving naturally occurring variation. We also propose a
new metric to evaluate the diversity as well as the accuracy of segmentation
predictions that aligns with the interest of clinical practice of collective
insights.",None,-1
0a67bb3b-4679-42e8-ba8d-8cbcad62c7aa,Hierarchical Neural Memory Network for Low Latency Event Processing,0.85635,"This paper proposes a low latency neural network architecture for event-based
dense prediction tasks. Conventional architectures encode entire scene contents
at a fixed rate regardless of their temporal characteristics. Instead, the
proposed network encodes contents at a proper temporal scale depending on its
movement speed. We achieve this by constructing temporal hierarchy using
stacked latent memories that operate at different rates. Given low latency
event steams, the multi-level memories gradually extract dynamic to static
scene contents by propagating information from the fast to the slow memory
modules. The architecture not only reduces the redundancy of conventional
architectures but also exploits long-term dependencies. Furthermore, an
attention-based event representation efficiently encodes sparse event streams
into the memory cells. We conduct extensive evaluations on three event-based
dense prediction tasks, where the proposed approach outperforms the existing
methods on accuracy and latency, while demonstrating effective event and image
fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",None,-1
415b358b-bea8-4ad6-8f0f-f60ca8ea80b3,Knowledge Enhanced Semantic Communication Receiver,0.796956,"In recent years, with the rapid development of deep learning and natural
language processing technologies, semantic communication has become a topic of
great interest in the field of communication. Although existing deep
learning-based semantic communication approaches have shown many advantages,
they still do not make sufficient use of prior knowledge. Moreover, most
existing semantic communication methods focus on the semantic encoding at the
transmitter side, while we believe that the semantic decoding capability of the
receiver should also be concerned. In this paper, we propose a knowledge
enhanced semantic communication framework in which the receiver can more
actively utilize the facts in the knowledge base for semantic reasoning and
decoding, on the basis of only affecting the parameters rather than the
structure of the neural networks at the transmitter side. Specifically, we
design a transformer-based knowledge extractor to find relevant factual triples
for the received noisy signal. Extensive simulation results on the WebNLG
dataset demonstrate that the proposed receiver yields superior performance on
top of the knowledge graph enhanced decoding.",None,-1
ccfcdd86-ddb2-45a6-ad02-d63a80a9307c,A Diffusion Model for Event Skeleton Generation,0.522847,"Event skeleton generation, aiming to induce an event schema skeleton graph
with abstracted event nodes and their temporal relations from a set of event
instance graphs, is a critical step in the temporal complex event schema
induction task. Existing methods effectively address this task from a graph
generation perspective but suffer from noise-sensitive and error accumulation,
e.g., the inability to correct errors while generating schema. We, therefore,
propose a novel Diffusion Event Graph Model~(DEGM) to address these issues. Our
DEGM is the first workable diffusion model for event skeleton generation, where
the embedding and rounding techniques with a custom edge-based loss are
introduced to transform a discrete event graph into learnable latent
representation. Furthermore, we propose a denoising training process to
maintain the model's robustness. Consequently, DEGM derives the final schema,
where error correction is guaranteed by iteratively refining the latent
representation during the schema generation process. Experimental results on
three IED bombing datasets demonstrate that our DEGM achieves better results
than other state-of-the-art baselines. Our code and data are available at
https://github.com/zhufq00/EventSkeletonGeneration.",None,-1
e89da94b-99fb-4038-a2f3-ec1213dc19d8,Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty,0.819467,"We present our submission to the BabyLM challenge, whose goal was to improve
the sample efficiency of language models. We trained an ensemble consisting of
a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word
BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,
which exceeds in performance both of its teachers as well as a similar model
trained without distillation. This suggests that distillation can not only
retain the full performance of the teacher model when the latter is trained on
a sufficiently small dataset; it can exceed it, and lead to significantly
better performance than direct training.",None,-1
73a86bf5-b6f9-4e39-81a8-9e17cbf33179,Salient Span Masking for Temporal Understanding,0.589817,"Salient Span Masking (SSM) has shown itself to be an effective strategy to
improve closed-book question answering performance. SSM extends general masked
language model pretraining by creating additional unsupervised training
sentences that mask a single entity or date span, thus oversampling factual
information. Despite the success of this paradigm, the span types and sampling
strategies are relatively arbitrary and not widely studied for other tasks.
Thus, we investigate SSM from the perspective of temporal tasks, where learning
a good representation of various temporal expressions is important. To that
end, we introduce Temporal Span Masking (TSM) intermediate training. First, we
find that SSM alone improves the downstream performance on three temporal tasks
by an avg. +5.8 points. Further, we are able to achieve additional improvements
(avg. +0.29 points) by adding the TSM task. These comprise the new best
reported results on the targeted tasks. Our analysis suggests that the
effectiveness of SSM stems from the sentences chosen in the training data
rather than the mask choice: sentences with entities frequently also contain
temporal expressions. Nonetheless, the additional targeted spans of TSM can
still improve performance, especially in a zero-shot context.",None,-1
d036d2a1-f729-4062-9a0c-bf3af50429a4,Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation,0.33404,"Large pre-trained language models achieve impressive results across many
tasks. However, recent works point out that pre-trained language models may
memorize a considerable fraction of their training data, leading to the privacy
risk of information leakage. In this paper, we propose a method named Ethicist
for targeted training data extraction through loss smoothed soft prompting and
calibrated confidence estimation, investigating how to recover the suffix in
the training data when given a prefix. To elicit memorization in the attacked
model, we tune soft prompt embeddings while keeping the model fixed. We further
propose a smoothing loss that smooths the loss distribution of the suffix
tokens to make it easier to sample the correct suffix. In order to select the
most probable suffix from a collection of sampled suffixes and estimate the
prediction confidence, we propose a calibrated confidence estimation method,
which normalizes the confidence of the generated suffixes with a local
estimation. We show that Ethicist significantly improves the extraction
performance on a recently proposed public benchmark. We also investigate
several factors influencing the data extraction performance, including decoding
strategy, model scale, prefix length, and suffix length. Our code is available
at https://github.com/thu-coai/Targeted-Data-Extraction.",None,-1
6ec36090-0c69-4b1e-9fbe-80567beb9340,Can we trust the evaluation on ChatGPT?,0.796242,"ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.",None,-1
d325f4fa-5a1d-4473-bef8-363c307561d5,Poisson Conjugate Prior for PHD Filtering based Track-Before-Detect Strategies in Radar Systems,0.0510169,"A variety of filters with track-before-detect (TBD) strategies have been
developed and applied to low signal-to-noise ratio (SNR) scenarios, including
the probability hypothesis density (PHD) filter. Assumptions of the standard
point measurement model based on detect-before-track (DBT) strategies are not
suitable for the amplitude echo model based on TBD strategies. However, based
on different models and unmatched assumptions, the measurement update formulas
for DBT-PHD filter are just mechanically applied to existing TBD-PHD filters.
In this paper, based on the Kullback-Leibler divergence minimization criterion,
finite set statistics theory and rigorous Bayes rule, a principled closed-form
solution of TBD-PHD filter is derived. Furthermore, we emphasize that PHD
filter is conjugated to the Poisson prior based on TBD strategies. Next, a
capping operation is devised to handle the divergence of target number
estimation as SNR increases. Moreover, the sequential Monte Carlo
implementations of dynamic and amplitude echo models are proposed for the radar
system. Finally, Monte Carlo experiments exhibit good performance in Rayleigh
noise and low SNR scenarios.",None,-1
8e7802d3-b430-43f7-90b4-8e0e666bd5ed,STEVE-1: A Generative Model for Text-to-Behavior in Minecraft,0.920397,"Constructing AI models that respond to text instructions is challenging,
especially for sequential decision-making tasks. This work introduces a
methodology, inspired by unCLIP, for instruction-tuning generative models of
behavior without relying on a large dataset of instruction-labeled
trajectories. Using this methodology, we create an instruction-tuned Video
Pretraining (VPT) model called STEVE-1, which can follow short-horizon
open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two
steps: adapting the pretrained VPT model to follow commands in MineCLIP's
latent space, then training a prior to predict latent codes from text. This
allows us to finetune VPT through self-supervised behavioral cloning and
hindsight relabeling, reducing the need for costly human text annotations, and
all for only $60 of compute. By leveraging pretrained models like VPT and
MineCLIP and employing best practices from text-conditioned image generation,
STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with
low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming
previous baselines and robustly completing 12 of 13 tasks in our early-game
evaluation suite. We provide experimental evidence highlighting key factors for
downstream performance, including pretraining, classifier-free guidance, and
data scaling. All resources, including our model weights, training scripts, and
evaluation tools are made available for further research.",None,-1
a852bcbc-98f4-426e-8020-9926a1e86a32,"Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models",0.782911,"The introduction of diffusion models in anomaly detection has paved the way
for more effective and accurate image reconstruction in pathologies. However,
the current limitations in controlling noise granularity hinder diffusion
models' ability to generalize across diverse anomaly types and compromise the
restoration of healthy tissues. To overcome these challenges, we propose
AutoDDPM, a novel approach that enhances the robustness of diffusion models.
AutoDDPM utilizes diffusion models to generate initial likelihood maps of
potential anomalies and seamlessly integrates them with the original image.
Through joint noised distribution re-sampling, AutoDDPM achieves harmonization
and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in
replacing anomalous regions while preserving healthy tissues, considerably
surpassing diffusion models' limitations. It also contributes valuable insights
and analysis on the limitations of current diffusion models, promoting robust
and interpretable anomaly detection in medical imaging - an essential aspect of
building autonomous clinical decision systems with higher interpretability.",None,-1
1bbc667e-8625-4b8b-bbbd-bb29501c6aab,EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras,0.559424,"Falls are significant and often fatal for vulnerable populations such as the
elderly. Previous works have addressed the detection of falls by relying on
data capture by a single sensor, images or accelerometers. In this work, we
rely on multimodal descriptors extracted from videos captured by egocentric
cameras. Our proposed method includes a late decision fusion layer that builds
on top of the extracted descriptors. Furthermore, we collect a new dataset on
which we assess our proposed approach. We believe this is the first public
dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects.
We conducted ablation experiments to assess the performance of individual
feature extractors, fusion of visual information, and fusion of both visual and
audio information. Moreover, we experimented with internal and external
cross-validation. Our results demonstrate that the fusion of audio and visual
information through late decision fusion improves detection performance, making
it a promising tool for fall prevention and mitigation.",None,-1
173b3bcc-625f-4426-acf6-7b2465fbedf3,Effect of Lossy Compression Algorithms on Face Image Quality and Recognition,0.0741267,"Lossy face image compression can degrade the image quality and the utility
for the purpose of face recognition. This work investigates the effect of lossy
image compression on a state-of-the-art face recognition model, and on multiple
face image quality assessment models. The analysis is conducted over a range of
specific image target sizes. Four compression types are considered, namely
JPEG, JPEG 2000, downscaled PNG, and notably the new JPEG XL format. Frontal
color images from the ColorFERET database were used in a Region Of Interest
(ROI) variant and a portrait variant. We primarily conclude that JPEG XL allows
for superior mean and worst case face recognition performance especially at
lower target sizes, below approximately 5kB for the ROI variant, while there
appears to be no critical advantage among the compression types at higher
target sizes. Quality assessments from modern models correlate well overall
with the compression effect on face recognition performance.",None,-1
521bb9e2-4f66-4791-ab28-cbc6c6d47a4e,"More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies",0.523632,"Collecting personally identifiable information (PII) on data subjects has
become big business. Data brokers and data processors are part of a
multi-billion-dollar industry that profits from collecting, buying, and selling
consumer data. Yet there is little transparency in the data collection industry
which makes it difficult to understand what types of data are being collected,
used, and sold, and thus the risk to individual data subjects. In this study,
we examine a large textual dataset of privacy policies from 1997-2019 in order
to investigate the data collection activities of data brokers and data
processors. We also develop an original lexicon of PII-related terms
representing PII data types curated from legislative texts. This mesoscale
analysis looks at privacy policies overtime on the word, topic, and network
levels to understand the stability, complexity, and sensitivity of privacy
policies over time. We find that (1) privacy legislation correlates with
changes in stability and turbulence of PII data types in privacy policies; (2)
the complexity of privacy policies decreases over time and becomes more
regularized; (3) sensitivity rises over time and shows spikes that are
correlated with events when new privacy legislation is introduced.",None,-1
17bea033-fbc9-47d9-93c8-4b59c3318576,SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification,0.337472,"Extreme classification (XC) involves predicting over large numbers of classes
(thousands to millions), with real-world applications like news article
classification and e-commerce product tagging. The zero-shot version of this
task requires generalization to novel classes without additional supervision.
In this paper, we develop SemSup-XC, a model that achieves state-of-the-art
zero-shot and few-shot performance on three XC datasets derived from legal,
e-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically
collected semantic class descriptions to represent classes and facilitate
generalization through a novel hybrid matching module that matches input
instances to class descriptions using a combination of semantic and lexical
similarity. Trained with contrastive learning, SemSup-XC significantly
outperforms baselines and establishes state-of-the-art performance on all three
datasets considered, gaining up to 12 precision points on zero-shot and more
than 10 precision points on one-shot tests, with similar gains for recall@10.
Our ablation studies highlight the relative importance of our hybrid matching
module and automatically collected class descriptions.",None,-1
e816942a-6c8a-41e4-a66c-c8d6e614e95f,A Dynamic Multi-Scale Voxel Flow Network for Video Prediction,0.99586,"The performance of video prediction has been greatly boosted by advanced deep
neural networks. However, most of the current methods suffer from large model
sizes and require extra inputs, e.g., semantic/depth maps, for promising
performance. For efficiency consideration, in this paper, we propose a Dynamic
Multi-scale Voxel Flow Network (DMVFN) to achieve better video prediction
performance at lower computational costs with only RGB images, than previous
methods. The core of our DMVFN is a differentiable routing module that can
effectively perceive the motion scales of video frames. Once trained, our DMVFN
selects adaptive sub-networks for different inputs at the inference stage.
Experiments on several benchmarks demonstrate that our DMVFN is an order of
magnitude faster than Deep Voxel Flow and surpasses the state-of-the-art
iterative-based OPT on generated image quality. Our code and demo are available
at https://huxiaotaostasy.github.io/DMVFN/.",None,-1
0a3d3c0e-3ecb-462f-b49c-d15d7b8cfbcb,Mastering Diverse Domains through World Models,0.999757,"Developing a general algorithm that learns to solve tasks across a wide range
of applications has been a fundamental challenge in artificial intelligence.
Although current reinforcement learning algorithms can be readily applied to
tasks similar to what they have been developed for, configuring them for new
application domains requires significant human expertise and experimentation.
We present DreamerV3, a general algorithm that outperforms specialized methods
across over 150 diverse tasks, with a single configuration. Dreamer learns a
model of the environment and improves its behavior by imagining future
scenarios. Robustness techniques based on normalization, balancing, and
transformations enable stable learning across domains. Applied out of the box,
Dreamer is the first algorithm to collect diamonds in Minecraft from scratch
without human data or curricula. This achievement has been posed as a
significant challenge in artificial intelligence that requires exploring
farsighted strategies from pixels and sparse rewards in an open world. Our work
allows solving challenging control problems without extensive experimentation,
making reinforcement learning broadly applicable.",None,-1
525ebe2b-8395-40f4-8237-209a9f79f0ca,Symbolic Planning and Code Generation for Grounded Dialogue,0.135426,"Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code's output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system's performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.",None,-1
3ee626dd-efcb-4422-be39-da0d68561f40,LayoutDM: Discrete Diffusion Model for Controllable Layout Generation,0.489803,"Controllable layout generation aims at synthesizing plausible arrangement of
element bounding boxes with optional constraints, such as type or position of a
specific element. In this work, we try to solve a broad range of layout
generation tasks in a single model that is based on discrete state-space
diffusion models. Our model, named LayoutDM, naturally handles the structured
layout data in the discrete representation and learns to progressively infer a
noiseless layout from the initial input, where we model the layout corruption
process by modality-wise discrete diffusion. For conditional generation, we
propose to inject layout constraints in the form of masking or logit adjustment
during inference. We show in the experiments that our LayoutDM successfully
generates high-quality layouts and outperforms both task-specific and
task-agnostic baselines on several layout tasks.",None,-1
38854433-579c-422f-b512-61f3b6bd3004,The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default,0.922064,"In recent years fairness in machine learning (ML) has emerged as a highly
active area of research and development. Most define fairness in simple terms,
where fairness means reducing gaps in performance or outcomes between
demographic groups while preserving as much of the accuracy of the original
system as possible. This oversimplification of equality through fairness
measures is troubling. Many current fairness measures suffer from both fairness
and performance degradation, or ""levelling down,"" where fairness is achieved by
making every group worse off, or by bringing better performing groups down to
the level of the worst off. When fairness can only be achieved by making
everyone worse off in material or relational terms through injuries of stigma,
loss of solidarity, unequal concern, and missed opportunities for substantive
equality, something would appear to have gone wrong in translating the vague
concept of 'fairness' into practice. This paper examines the causes and
prevalence of levelling down across fairML, and explore possible justifications
and criticisms based on philosophical and legal theories of equality and
distributive justice, as well as equality law jurisprudence. We find that
fairML does not currently engage in the type of measurement, reporting, or
analysis necessary to justify levelling down in practice. We propose a first
step towards substantive equality in fairML: ""levelling up"" systems by design
through enforcement of minimum acceptable harm thresholds, or ""minimum rate
constraints,"" as fairness constraints. We likewise propose an alternative
harms-based framework to counter the oversimplified egalitarian framing
currently dominant in the field and push future discussion more towards
substantive equality opportunities and away from strict egalitarianism by
default. N.B. Shortened abstract, see paper for full abstract.",None,-1
5f2ab749-e778-4ad8-99df-cba8d163a145,Assessing the Impact of Noise on Quantum Neural Networks: An Experimental Analysis,0.401511,"In the race towards quantum computing, the potential benefits of quantum
neural networks (QNNs) have become increasingly apparent. However, Noisy
Intermediate-Scale Quantum (NISQ) processors are prone to errors, which poses a
significant challenge for the execution of complex algorithms or quantum
machine learning. To ensure the quality and security of QNNs, it is crucial to
explore the impact of noise on their performance. This paper provides a
comprehensive analysis of the impact of noise on QNNs, examining the Mottonen
state preparation algorithm under various noise models and studying the
degradation of quantum states as they pass through multiple layers of QNNs.
Additionally, the paper evaluates the effect of noise on the performance of
pre-trained QNNs and highlights the challenges posed by noise models in quantum
computing. The findings of this study have significant implications for the
development of quantum software, emphasizing the importance of prioritizing
stability and noise-correction measures when developing QNNs to ensure reliable
and trustworthy results. This paper contributes to the growing body of
literature on quantum computing and quantum machine learning, providing new
insights into the impact of noise on QNNs and paving the way towards the
development of more robust and efficient quantum algorithms.",None,-1
5d0c4e62-ca3a-4736-a288-3be45feb84f6,AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation,0.313764,"Both indoor and outdoor environments are inherently structured and
repetitive. Traditional modeling pipelines keep an asset library storing unique
object templates, which is both versatile and memory efficient in practice.
Inspired by this observation, we propose AssetField, a novel neural scene
representation that learns a set of object-aware ground feature planes to
represent the scene, where an asset library storing template feature patches
can be constructed in an unsupervised manner. Unlike existing methods which
require object masks to query spatial points for object editing, our ground
feature plane representation offers a natural visualization of the scene in the
bird-eye view, allowing a variety of operations (e.g. translation, duplication,
deformation) on objects to configure a new scene. With the template feature
patches, group editing is enabled for scenes with many recurring items to avoid
repetitive work on object individuals. We show that AssetField not only
achieves competitive performance for novel-view synthesis but also generates
realistic renderings for new scene configurations.",None,-1
35137f42-dc42-432d-9b15-71e0e862f090,UNICORN: A Unified Backdoor Trigger Inversion Framework,0.765972,"The backdoor attack, where the adversary uses inputs stamped with triggers
(e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat
to Deep Neural Network (DNN) models. Trigger inversion is an effective way of
identifying backdoor models and understanding embedded adversarial behaviors. A
challenge of trigger inversion is that there are many ways of constructing the
trigger. Existing methods cannot generalize to various types of triggers by
making certain assumptions or attack-specific constraints. The fundamental
reason is that existing work does not consider the trigger's design space in
their formulation of the inversion problem. This work formally defines and
analyzes the triggers injected in different spaces and the inversion problem.
Then, it proposes a unified framework to invert backdoor triggers based on the
formalization of triggers and the identified inner behaviors of backdoor models
from our analysis. Our prototype UNICORN is general and effective in inverting
backdoor triggers in DNNs. The code can be found at
https://github.com/RU-System-Software-and-Security/UNICORN.",None,-1
cf619139-04e4-4bd1-98dc-a194dbd5106f,SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models,0.417822,"Stereotype benchmark datasets are crucial to detect and mitigate social
stereotypes about groups of people in NLP models. However, existing datasets
are limited in size and coverage, and are largely restricted to stereotypes
prevalent in the Western society. This is especially problematic as language
technologies gain hold across the globe. To address this gap, we present
SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative
capabilities of large language models such as PaLM, and GPT-3, and leveraging a
globally diverse rater pool to validate the prevalence of those stereotypes in
society. SeeGULL is in English, and contains stereotypes about identity groups
spanning 178 countries across 8 different geo-political regions across 6
continents, as well as state-level identities within the US and India. We also
include fine-grained offensiveness scores for different stereotypes and
demonstrate their global disparities. Furthermore, we include comparative
annotations about the same groups by annotators living in the region vs. those
that are based in North America, and demonstrate that within-region stereotypes
about groups differ from those prevalent in North America. CONTENT WARNING:
This paper contains stereotype examples that may be offensive.",None,-1
1ee507a7-7aa6-4d39-a9d9-572f0e3c6f64,AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism,0.482918,"Generating 3D human motion based on textual descriptions has been a research
focus in recent years. It requires the generated motion to be diverse, natural,
and conform to the textual description. Due to the complex spatio-temporal
nature of human motion and the difficulty in learning the cross-modal
relationship between text and motion, text-driven motion generation is still a
challenging problem. To address these issues, we propose \textbf{AttT2M}, a
two-stage method with multi-perspective attention mechanism: \textbf{body-part
attention} and \textbf{global-local motion-text attention}. The former focuses
on the motion embedding perspective, which means introducing a body-part
spatio-temporal encoder into VQ-VAE to learn a more expressive discrete latent
space. The latter is from the cross-modal perspective, which is used to learn
the sentence-level and word-level motion-text cross-modal relationship. The
text-driven motion is finally generated with a generative transformer.
Extensive experiments conducted on HumanML3D and KIT-ML demonstrate that our
method outperforms the current state-of-the-art works in terms of qualitative
and quantitative evaluation, and achieve fine-grained synthesis and
action2motion. Our code is in https://github.com/ZcyMonkey/AttT2M",None,-1
275a6b6e-01db-49ee-a57c-9248ae9a4ec7,Learning Language-Specific Layers for Multilingual Machine Translation,0.721236,"Multilingual Machine Translation promises to improve translation quality
between non-English languages. This is advantageous for several reasons, namely
lower latency (no need to translate twice), and reduced error cascades (e.g.,
avoiding losing gender and formality information when translating through
English). On the downside, adding more languages reduces model capacity per
language, which is usually countered by increasing the overall model size,
making training harder and inference slower. In this work, we introduce
Language-Specific Transformer Layers (LSLs), which allow us to increase model
capacity, while keeping the amount of computation and the number of parameters
used in the forward pass constant. The key idea is to have some layers of the
encoder be source or target language-specific, while keeping the remaining
layers shared. We study the best way to place these layers using a neural
architecture search inspired approach, and achieve an improvement of 1.3 chrF
(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and
1.9 chrF (2.2 spBLEU) on a shared decoder one.",None,-1
214b959f-7f72-4899-945a-12cc8fa977d9,Knowledge-Guided Short-Context Action Anticipation in Human-Centric Videos,0.137915,"This work focuses on anticipating long-term human actions, particularly using
short video segments, which can speed up editing workflows through improved
suggestions while fostering creativity by suggesting narratives. To this end,
we imbue a transformer network with a symbolic knowledge graph for action
anticipation in video segments by boosting certain aspects of the transformer's
attention mechanism at run-time. Demonstrated on two benchmark datasets,
Breakfast and 50Salads, our approach outperforms current state-of-the-art
methods for long-term action anticipation using short video context by up to
9%.",None,-1
7adc7092-2486-4582-8781-caaaaab56a1d,PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction,0.73799,"In the era of information explosion, spatio-temporal data mining serves as a
critical part of urban management. Considering the various fields demanding
attention, e.g., traffic state, human activity, and social event, predicting
multiple spatio-temporal attributes simultaneously can alleviate regulatory
pressure and foster smart city construction. However, current research can not
handle the spatio-temporal multi-attribute prediction well due to the complex
relationships between diverse attributes. The key challenge lies in how to
address the common spatio-temporal patterns while tackling their distinctions.
In this paper, we propose an effective solution for spatio-temporal
multi-attribute prediction, PromptST. We devise a spatio-temporal transformer
and a parameter-sharing training scheme to address the common knowledge among
different spatio-temporal attributes. Then, we elaborate a spatio-temporal
prompt tuning strategy to fit the specific attributes in a lightweight manner.
Through the pretrain and prompt tuning phases, our PromptST is able to enhance
the specific spatio-temoral characteristic capture by prompting the backbone
model to fit the specific target attribute while maintaining the learned common
knowledge. Extensive experiments on real-world datasets verify that our
PromptST attains state-of-the-art performance. Furthermore, we also prove
PromptST owns good transferability on unseen spatio-temporal attributes, which
brings promising application potential in urban computing. The implementation
code is available to ease reproducibility.",None,-1
fda1c487-b865-44a4-9e5d-971230caa275,Hierarchical Interactive Reconstruction Network For Video Compressive Sensing,0.178846,"Deep network-based image and video Compressive Sensing(CS) has attracted
increasing attentions in recent years. However, in the existing deep
network-based CS methods, a simple stacked convolutional network is usually
adopted, which not only weakens the perception of rich contextual prior
knowledge, but also limits the exploration of the correlations between temporal
video frames. In this paper, we propose a novel Hierarchical InTeractive Video
CS Reconstruction Network(HIT-VCSNet), which can cooperatively exploit the deep
priors in both spatial and temporal domains to improve the reconstruction
quality. Specifically, in the spatial domain, a novel hierarchical structure is
designed, which can hierarchically extract deep features from keyframes and
non-keyframes. In the temporal domain, a novel hierarchical interaction
mechanism is proposed, which can cooperatively learn the correlations among
different frames in the multiscale space. Extensive experiments manifest that
the proposed HIT-VCSNet outperforms the existing state-of-the-art video and
image CS methods in a large margin.",None,-1
bbcc4835-ed16-45b7-bbb4-fefa2047910e,AVSegFormer: Audio-Visual Segmentation with Transformer,0.772234,"The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.",None,-1
14e8bb98-910c-4a62-80c9-b76dbdba0dd9,Pushdown Layers: Encoding Recursive Structure in Transformer Language Models,0.329091,"Recursion is a prominent feature of human language, and fundamentally
challenging for self-attention due to the lack of an explicit recursive-state
tracking mechanism. Consequently, Transformer language models poorly capture
long-tail recursive structure and exhibit sample-inefficient syntactic
generalization. This work introduces Pushdown Layers, a new self-attention
layer that models recursive state via a stack tape that tracks estimated depths
of every token in an incremental parse of the observed prefix. Transformer LMs
with Pushdown Layers are syntactic language models that autoregressively and
synchronously update this stack tape as they predict new tokens, in turn using
the stack tape to softly modulate attention over tokens -- for instance,
learning to ""skip"" over closed constituents. When trained on a corpus of
strings annotated with silver constituency parses, Transformers equipped with
Pushdown Layers achieve dramatically better and 3-5x more sample-efficient
syntactic generalization, while maintaining similar perplexities. Pushdown
Layers are a drop-in replacement for standard self-attention. We illustrate
this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed
WikiText-103, leading to improvements on several GLUE text classification
tasks.",None,-1
7a07e8d8-4cde-44e6-9022-40a1b4898445,MobileVidFactory: Automatic Diffusion-Based Social Media Video Generation for Mobile Devices from Text,0.865314,"Videos for mobile devices become the most popular access to share and acquire
information recently. For the convenience of users' creation, in this paper, we
present a system, namely MobileVidFactory, to automatically generate vertical
mobile videos where users only need to give simple texts mainly. Our system
consists of two parts: basic and customized generation. In the basic
generation, we take advantage of the pretrained image diffusion model, and
adapt it to a high-quality open-domain vertical video generator for mobile
devices. As for the audio, by retrieving from our big database, our system
matches a suitable background sound for the video. Additionally to produce
customized content, our system allows users to add specified screen texts to
the video for enriching visual expression, and specify texts for automatic
reading with optional voices as they like.",None,-1
6e41f41e-d741-430e-8346-30f1edc06798,HoloDiffusion: Training a 3D Diffusion Model using 2D Images,0.821449,"Diffusion models have emerged as the best approach for generative modeling of
2D images. Part of their success is due to the possibility of training them on
millions if not billions of images with a stable learning objective. However,
extending these models to 3D remains difficult for two reasons. First, finding
a large quantity of 3D training data is much more complex than for 2D images.
Second, while it is conceptually trivial to extend the models to operate on 3D
rather than 2D grids, the associated cubic growth in memory and compute
complexity makes this infeasible. We address the first challenge by introducing
a new diffusion setup that can be trained, end-to-end, with only posed 2D
images for supervision; and the second challenge by proposing an image
formation model that decouples model memory from spatial memory. We evaluate
our method on real-world data, using the CO3D dataset which has not been used
to train 3D generative models before. We show that our diffusion models are
scalable, train robustly, and are competitive in terms of sample quality and
fidelity to existing approaches for 3D generative modeling.",None,-1
105c463f-5be5-42e5-8edd-a67dd45c6da9,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,0.851785,"The ability to accurately locate and navigate to a specific object is a
crucial capability for embodied agents that operate in the real world and
interact with objects to complete tasks. Such object navigation tasks usually
require large-scale training in visual environments with labeled objects, which
generalizes poorly to novel objects in unknown environments. In this work, we
present a novel zero-shot object navigation method, Exploration with Soft
Commonsense constraints (ESC), that transfers commonsense knowledge in
pre-trained models to open-world object navigation without any navigation
experience nor any other training on the visual environments. First, ESC
leverages a pre-trained vision and language model for open-world prompt-based
grounding and a pre-trained commonsense language model for room and object
reasoning. Then ESC converts commonsense knowledge into navigation actions by
modeling it as soft logic predicates for efficient exploration. Extensive
experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method
improves significantly over baselines, and achieves new state-of-the-art
results for zero-shot object navigation (e.g., 288% relative Success Rate
improvement than CoW on MP3D).",None,-1
b453c70b-4baa-4328-8924-ce624b75f985,Time out of Mind: Generating Rate of Speech conditioned on emotion and speaker,0.119054,"Voice synthesis has seen significant improvements in the past decade
resulting in highly intelligible voices. Further investigations have resulted
in models that can produce variable speech, including conditional emotional
expression. The problem lies, however, in a focus on phrase-level modifications
and prosodic vocal features. Using the CREMA-D dataset we have trained a GAN
conditioned on emotion to generate worth lengths for a given input text. These
word lengths are relative to neutral speech and can be provided, through speech
synthesis markup language (SSML) to a text-to-speech (TTS) system to generate
more expressive speech. Additionally, a generative model is also trained using
implicit maximum likelihood estimation (IMLE) and a comparative analysis with
GANs is included. We were able to achieve better performances on objective
measures for neutral speech, and better time alignment for happy speech when
compared to an out-of-box model. However, further investigation of subjective
evaluation is required.",None,-1
d4c735bc-1130-4cdd-be7d-3105d8bbc6b9,Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues,0.475984,"In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.",None,-1
e0e43930-ade5-4e05-a4e2-d9895b3f762d,SGLang: Efficient Execution of Structured Language Model Programs,0.880764,"Large language models (LLMs) are increasingly used for complex tasks that
require multiple generation calls, advanced prompting techniques, control flow,
and structured inputs/outputs. However, efficient systems are lacking for
programming and executing these applications. We introduce SGLang, a system for
efficient execution of complex language model programs. SGLang consists of a
frontend language and a runtime. The frontend simplifies programming with
primitives for generation and parallelism control. The runtime accelerates
execution with novel optimizations like RadixAttention for KV cache reuse and
compressed finite state machines for faster structured output decoding.
Experiments show that SGLang achieves up to 6.4x higher throughput compared to
state-of-the-art inference systems on various large language and multi-modal
models on tasks including agent control, logical reasoning, few-shot learning
benchmarks, JSON decoding, retrieval-augmented generation pipelines, and
multi-turn chat. The code is publicly available at
https://github.com/sgl-project/sglang",None,-1
cfe03d8b-5651-49a0-b173-8ed0a146aa8b,Class Attention Transfer Based Knowledge Distillation,0.846371,"Previous knowledge distillation methods have shown their impressive
performance on model compression tasks, however, it is hard to explain how the
knowledge they transferred helps to improve the performance of the student
network. In this work, we focus on proposing a knowledge distillation method
that has both high interpretability and competitive performance. We first
revisit the structure of mainstream CNN models and reveal that possessing the
capacity of identifying class discriminative regions of input is critical for
CNN to perform classification. Furthermore, we demonstrate that this capacity
can be obtained and enhanced by transferring class activation maps. Based on
our findings, we propose class attention transfer based knowledge distillation
(CAT-KD). Different from previous KD methods, we explore and present several
properties of the knowledge transferred by our method, which not only improve
the interpretability of CAT-KD but also contribute to a better understanding of
CNN. While having high interpretability, CAT-KD achieves state-of-the-art
performance on multiple benchmarks. Code is available at:
https://github.com/GzyAftermath/CAT-KD.",None,-1
b4e00046-0645-4c9e-8d2a-bc032c5de30d,Robust face anti-spoofing framework with Convolutional Vision Transformer,0.85237,"Owing to the advances in image processing technology and large-scale
datasets, companies have implemented facial authentication processes, thereby
stimulating increased focus on face anti-spoofing (FAS) against realistic
presentation attacks. Recently, various attempts have been made to improve face
recognition performance using both global and local learning on face images;
however, to the best of our knowledge, this is the first study to investigate
whether the robustness of FAS against domain shifts is improved by considering
global information and local cues in face images captured using self-attention
and convolutional layers. This study proposes a convolutional vision
transformer-based framework that achieves robust performance for various unseen
domain data. Our model resulted in 7.3%$p$ and 12.9%$p$ increases in FAS
performance compared to models using only a convolutional neural network or
vision transformer, respectively. It also shows the highest average rank in
sub-protocols of cross-dataset setting over the other nine benchmark models for
domain generalization.",None,-1
4f023c38-9c91-4c6f-8335-8326c970efb4,Interpretable Unified Language Checking,0.33644,"Despite recent concerns about undesirable behaviors generated by large
language models (LLMs), including non-factual, biased, and hateful language, we
find LLMs are inherent multi-task language checkers based on their latent
representations of natural and social knowledge. We present an interpretable,
unified, language checking (UniLC) method for both human and machine-generated
language that aims to check if language input is factual and fair. While
fairness and fact-checking tasks have been handled separately with dedicated
models, we find that LLMs can achieve high performance on a combination of
fact-checking, stereotype detection, and hate speech detection tasks with a
simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task
language checking method proposed in this work, the GPT3.5-turbo model
outperforms fully supervised baselines on several language tasks. The simple
approach and results suggest that based on strong latent knowledge
representations, an LLM can be an adaptive and explainable tool for detecting
misinformation, stereotypes, and hate speech.",None,-1
20c9f264-747f-4444-93db-4c1b5856b01e,Organizational Governance of Emerging Technologies: AI Adoption in Healthcare,0.890001,"Private and public sector structures and norms refine how emerging technology
is used in practice. In healthcare, despite a proliferation of AI adoption, the
organizational governance surrounding its use and integration is often poorly
understood. What the Health AI Partnership (HAIP) aims to do in this research
is to better define the requirements for adequate organizational governance of
AI systems in healthcare settings and support health system leaders to make
more informed decisions around AI adoption. To work towards this understanding,
we first identify how the standards for the AI adoption in healthcare may be
designed to be used easily and efficiently. Then, we map out the precise
decision points involved in the practical institutional adoption of AI
technology within specific health systems. Practically, we achieve this through
a multi-organizational collaboration with leaders from major health systems
across the United States and key informants from related fields. Working with
the consultancy IDEO [dot] org, we were able to conduct usability-testing
sessions with healthcare and AI ethics professionals. Usability analysis
revealed a prototype structured around mock key decision points that align with
how organizational leaders approach technology adoption. Concurrently, we
conducted semi-structured interviews with 89 professionals in healthcare and
other relevant fields. Using a modified grounded theory approach, we were able
to identify 8 key decision points and comprehensive procedures throughout the
AI adoption lifecycle. This is one of the most detailed qualitative analyses to
date of the current governance structures and processes involved in AI adoption
by health systems in the United States. We hope these findings can inform
future efforts to build capabilities to promote the safe, effective, and
responsible adoption of emerging technologies in healthcare.",None,-1
8e6086be-606e-4c25-b7ff-4481f4d933ed,"Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",0.659174,"We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.",None,-1
8ced093b-c360-4524-a04b-0930c4ae4125,SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding,0.237694,"Modern speech processing systems rely on self-attention. Unfortunately, token
mixing with self-attention takes quadratic time in the length of the speech
utterance, slowing down inference as well as training and increasing memory
consumption. Cheaper alternatives to self-attention for ASR have been
developed, but they fail to consistently reach the same level of accuracy. This
paper, therefore, proposes a novel linear-time alternative to self-attention.
It summarises an utterance with the mean over vectors for all time steps. This
single summary is then combined with time-specific information. We call this
method ""SummaryMixing"". Introducing SummaryMixing in state-of-the-art ASR
models makes it feasible to preserve or exceed previous speech recognition
performance while lowering the training and inference times by up to 28$\%$ and
reducing the memory budget by a factor of two. The benefits of SummaryMixing
can also be generalized to other speech-processing tasks, such as speech
understanding.",None,-1
4431f244-011a-466e-9b8e-2338bfd1dffa,The Devil in the Details: Simple and Effective Optical Flow Synthetic Data Generation,0.246635,"Recent work on dense optical flow has shown significant progress, primarily
in a supervised learning manner requiring a large amount of labeled data. Due
to the expensiveness of obtaining large scale real-world data, computer
graphics are typically leveraged for constructing datasets. However, there is a
common belief that synthetic-to-real domain gaps limit generalization to real
scenes. In this paper, we show that the required characteristics in an optical
flow dataset are rather simple and present a simpler synthetic data generation
method that achieves a certain level of realism with compositions of elementary
operations. With 2D motion-based datasets, we systematically analyze the
simplest yet critical factors for generating synthetic datasets. Furthermore,
we propose a novel method of utilizing occlusion masks in a supervised method
and observe that suppressing gradients on occluded regions serves as a powerful
initial state in the curriculum learning sense. The RAFT network initially
trained on our dataset outperforms the original RAFT on the two most
challenging online benchmarks, MPI Sintel and KITTI 2015.",None,-1
3c38df13-184a-481f-ab75-37ab88fcbb11,DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation,0.877061,"While large-scale pre-trained text-to-image models can synthesize diverse and
high-quality human-centric images, an intractable problem is how to preserve
the face identity for conditioned face images. Existing methods either require
time-consuming optimization for each face-identity or learning an efficient
encoder at the cost of harming the editability of models. In this work, we
present an optimization-free method for each face identity, meanwhile keeping
the editability for text-to-image models. Specifically, we propose a novel
face-identity encoder to learn an accurate representation of human faces, which
applies multi-scale face features followed by a multi-embedding projector to
directly generate the pseudo words in the text embedding space. Besides, we
propose self-augmented editability learning to enhance the editability of
models, which is achieved by constructing paired generated face and edited face
images using celebrity names, aiming at transferring mature ability of
off-the-shelf text-to-image models in celebrity faces to unseen faces.
Extensive experiments show that our methods can generate identity-preserved
images under different scenes at a much faster speed.",None,-1
4406a9fd-99cf-43c6-8182-e2905b15e0e7,Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering,0.112645,"Commonsense question answering (QA) research requires machines to answer
questions based on commonsense knowledge. However, this research requires
expensive labor costs to annotate data as the basis of research, and models
that rely on fine-tuning paradigms only apply to specific tasks, rather than
learn a general commonsense reasoning ability. As a more robust method,
zero-shot commonsense question answering shows a good prospect. The current
zero-shot framework tries to convert triples in commonsense knowledge graphs
(KGs) into QA-form samples as the pre-trained data source to incorporate
commonsense knowledge into the model. However, this method ignores the
multi-hop relationship in the KG, which is also an important central problem in
commonsense reasoning. In this paper, we propose a novel multi-hop commonsense
knowledge injection framework. Specifically, it explores multi-hop reasoning
paradigm in KGs that conform to linguistic logic, and we further propose two
multi-hop QA generation methods based on KGs. Then, we utilize contrastive
learning to pre-train the model with the synthetic QA dataset to inject
multi-hop commonsense knowledge. Extensive experiments on five commonsense
question answering benchmarks demonstrate that our framework achieves
state-of-art performance.",None,-1
122c958a-ec55-49d1-981f-a1d1acdfd065,Neural Probabilistic Logic Programming in Discrete-Continuous Domains,0.953736,"Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic
background knowledge in the form of logic. It has been shown to aid learning in
the limited data regime and to facilitate inference on out-of-distribution
data. Probabilistic NeSy focuses on integrating neural networks with both logic
and probability theory, which additionally allows learning under uncertainty. A
major limitation of current probabilistic NeSy systems, such as DeepProbLog, is
their restriction to finite probability distributions, i.e., discrete random
variables. In contrast, deep probabilistic programming (DPP) excels in
modelling and optimising continuous probability distributions. Hence, we
introduce DeepSeaProbLog, a neural probabilistic logic programming language
that incorporates DPP techniques into NeSy. Doing so results in the support of
inference and learning of both discrete and continuous probability
distributions under logical constraints. Our main contributions are 1) the
semantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a
proven asymptotically unbiased learning algorithm, and 3) a series of
experiments that illustrate the versatility of our approach.",None,-1
43227d23-fa0b-4ec9-b3c3-76c35d741a6a,Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs,0.754654,"Large Language Models (LLMs) have demonstrated exceptional proficiency in
language-related tasks, but their deployment poses significant challenges due
to substantial memory and storage requirements. Weight-only quantization has
emerged as a promising solution to address these challenges. Previous research
suggests that fine-tuning through up and down rounding can enhance performance.
In this study, we introduce SignRound, a method that utilizes signed gradient
descent (SignSGD) to optimize rounding values and weight clipping within just
200 steps. SignRound integrates the advantages of Quantization-Aware Training
(QAT) and Post-Training Quantization (PTQ), achieving exceptional results
across 2 to 4 bits while maintaining low tuning costs and avoiding additional
inference overhead. For example, SignRound achieves absolute average accuracy
improvements ranging from 6.91\% to 33.22\% at 2 bits. It also demonstrates
robust generalization to recent models and achieves near-lossless quantization
in most scenarios at 4 bits. The source code is publicly available at
\url{https://github.com/intel/auto-round}.",None,-1
0ca28bf2-ffa6-4f37-bb18-fa1fd59b411e,Augmenting Large Language Model Translators via Translation Memories,0.668645,"Using translation memories (TMs) as prompts is a promising approach to
in-context learning of machine translation models. In this work, we take a step
towards prompting large language models (LLMs) with TMs and making them better
translators. We find that the ability of LLMs to ``understand'' prompts is
indeed helpful for making better use of TMs. Experiments show that the results
of a pre-trained LLM translator can be greatly improved by using high-quality
TM-based prompts. These results are even comparable to those of the
state-of-the-art NMT systems which have access to large-scale in-domain
bilingual data and are well tuned on the downstream tasks.",None,-1
389363d5-ec1b-4de6-a04a-f0c735ec3c1e,X-VoE: Measuring eXplanatory Violation of Expectation in Physical Events,0.117063,"Intuitive physics is pivotal for human understanding of the physical world,
enabling prediction and interpretation of events even in infancy. Nonetheless,
replicating this level of intuitive physics in artificial intelligence (AI)
remains a formidable challenge. This study introduces X-VoE, a comprehensive
benchmark dataset, to assess AI agents' grasp of intuitive physics. Built on
the developmental psychology-rooted Violation of Expectation (VoE) paradigm,
X-VoE establishes a higher bar for the explanatory capacities of intuitive
physics models. Each VoE scenario within X-VoE encompasses three distinct
settings, probing models' comprehension of events and their underlying
explanations. Beyond model evaluation, we present an explanation-based learning
system that captures physics dynamics and infers occluded object states solely
from visual sequences, without explicit occlusion labels. Experimental outcomes
highlight our model's alignment with human commonsense when tested against
X-VoE. A remarkable feature is our model's ability to visually expound VoE
events by reconstructing concealed scenes. Concluding, we discuss the findings'
implications and outline future research directions. Through X-VoE, we catalyze
the advancement of AI endowed with human-like intuitive physics capabilities.",None,-1
dfba119c-ec54-43d3-b48a-e135647638dc,PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs,0.800289,"Large language models (LLMs) have shown great abilities of solving various
natural language tasks in different domains. Due to the training objective of
LLMs and their pre-training data, LLMs are not very well equipped for tasks
involving structured data generation. We propose a framework, Prompting with
Iterative Verification (PiVe), to improve graph-based generative capability of
LLMs. We show how a small language model could be trained to act as a verifier
module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively
improve its performance via fine-grained corrective instructions. We also show
how the verifier module could apply iterative corrections offline for a more
cost-effective solution to the text-to-graph generation task. Experiments on
three graph-based datasets show consistent improvement gained via PiVe.
Additionally, we create GenWiki-HIQ and highlight that the verifier module can
be used as a data augmentation tool to help improve the quality of
automatically generated parallel text-graph datasets.",None,-1
513a1f9a-f3fd-4218-ba83-2bee450a58e3,Proving Conjectures Acquired by Composing Multiple Biases,0.552112,"We present the proofs of the conjectures mentioned in the paper published in
the proceedings of the 2024 AAAI conference [1], and discovered by the
decomposition methods presented in the same paper.",None,-1
f90cca98-7304-49e4-b9e6-de51a0d9a841,Deep Learning Models to Study Sentence Comprehension in the Human Brain,0.097609,"Recent artificial neural networks that process natural language achieve
unprecedented performance in tasks requiring sentence-level understanding. As
such, they could be interesting models of the integration of linguistic
information in the human brain. We review works that compare these artificial
language models with human brain activity and we assess the extent to which
this approach has improved our understanding of the neural processes involved
in natural language comprehension. Two main results emerge. First, the neural
representation of word meaning aligns with the context-dependent, dense word
vectors used by the artificial neural networks. Second, the processing
hierarchy that emerges within artificial neural networks broadly matches the
brain, but is surprisingly inconsistent across studies. We discuss current
challenges in establishing artificial neural networks as process models of
natural language comprehension. We suggest exploiting the highly structured
representational geometry of artificial neural networks when mapping
representations to brain data.",None,-1
8b4f2c2b-6f50-45be-8127-02dcd78a6305,Maskomaly:Zero-Shot Mask Anomaly Segmentation,0.245672,"We present a simple and practical framework for anomaly segmentation called
Maskomaly. It builds upon mask-based standard semantic segmentation networks by
adding a simple inference-time post-processing step which leverages the raw
mask outputs of such networks. Maskomaly does not require additional training
and only adds a small computational overhead to inference. Most importantly, it
does not require anomalous data at training. We show top results for our method
on SMIYC, RoadAnomaly, and StreetHazards. On the most central benchmark, SMIYC,
Maskomaly outperforms all directly comparable approaches. Further, we introduce
a novel metric that benefits the development of robust anomaly segmentation
methods and demonstrate its informativeness on RoadAnomaly.",None,-1
4a537d76-3f2e-4b02-8f3e-f356ea6c18ee,Understanding the Distillation Process from Deep Generative Models to Tractable Probabilistic Circuits,0.806473,"Probabilistic Circuits (PCs) are a general and unified computational
framework for tractable probabilistic models that support efficient computation
of various inference tasks (e.g., computing marginal probabilities). Towards
enabling such reasoning capabilities in complex real-world tasks, Liu et al.
(2022) propose to distill knowledge (through latent variable assignments) from
less tractable but more expressive deep generative models. However, it is still
unclear what factors make this distillation work well. In this paper, we
theoretically and empirically discover that the performance of a PC can exceed
that of its teacher model. Therefore, instead of performing distillation from
the most expressive deep generative model, we study what properties the teacher
model and the PC should have in order to achieve good distillation performance.
This leads to a generic algorithmic improvement as well as other
data-type-specific ones over the existing latent variable distillation
pipeline. Empirically, we outperform SoTA TPMs by a large margin on challenging
image modeling benchmarks. In particular, on ImageNet32, PCs achieve 4.06
bits-per-dimension, which is only 0.34 behind variational diffusion models
(Kingma et al., 2021).",None,-1
9d8f4805-6ac9-4a05-9bf4-419c478c22cb,Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation,0.333623,"The images and sounds that we perceive undergo subtle but geometrically
consistent changes as we rotate our heads. In this paper, we use these cues to
solve a problem we call Sound Localization from Motion (SLfM): jointly
estimating camera rotation and localizing sound sources. We learn to solve
these tasks solely through self-supervision. A visual model predicts camera
rotation from a pair of images, while an audio model predicts the direction of
sound sources from binaural sounds. We train these models to generate
predictions that agree with one another. At test time, the models can be
deployed independently. To obtain a feature representation that is well-suited
to solving this challenging problem, we also propose a method for learning an
audio-visual representation through cross-view binauralization: estimating
binaural sound from one view, given images and sound from another. Our model
can successfully estimate accurate rotations on both real and synthetic scenes,
and localize sound sources with accuracy competitive with state-of-the-art
self-supervised approaches. Project site: https://ificl.github.io/SLfM/",None,-1
30ccdffc-1e30-4c9a-ba67-8c21cdcb5406,UKnow: A Unified Knowledge Protocol for Common-Sense Reasoning and Vision-Language Pre-training,0.113752,"This work presents a unified knowledge protocol, called UKnow, which
facilitates knowledge-based studies from the perspective of data. Particularly
focusing on visual and linguistic modalities, we categorize data knowledge into
five unit types, namely, in-image, in-text, cross-image, cross-text, and
image-text, and set up an efficient pipeline to help construct the multimodal
knowledge graph from any data collection. Thanks to the logical information
naturally contained in knowledge graph, organizing datasets under UKnow format
opens up more possibilities of data usage compared to the commonly used
image-text pairs. Following UKnow protocol, we collect, from public
international news, a large-scale multimodal knowledge graph dataset that
consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817
triplets. The dataset is also annotated with rich event tags, including 11
coarse labels and 9,185 fine labels. Experiments on four benchmarks demonstrate
the potential of UKnow in supporting common-sense reasoning and boosting
vision-language pre-training with a single dataset, benefiting from its unified
form of knowledge organization. Code, dataset, and models will be made publicly
available.",None,-1
06b74fd5-d244-4495-a93f-ec267c93c7b6,BLEU Meets COMET: Combining Lexical and Neural Metrics Towards Robust Machine Translation Evaluation,0.227146,"Although neural-based machine translation evaluation metrics, such as COMET
or BLEURT, have achieved strong correlations with human judgements, they are
sometimes unreliable in detecting certain phenomena that can be considered as
critical errors, such as deviations in entities and numbers. In contrast,
traditional evaluation metrics, such as BLEU or chrF, which measure lexical or
character overlap between translation hypotheses and human references, have
lower correlations with human judgements but are sensitive to such deviations.
In this paper, we investigate several ways of combining the two approaches in
order to increase robustness of state-of-the-art evaluation methods to
translations with critical errors. We show that by using additional information
during training, such as sentence-level features and word-level tags, the
trained metrics improve their capability to penalize translations with specific
troublesome phenomena, which leads to gains in correlation with human judgments
and on recent challenge sets on several language pairs.",None,-1
4de59377-53b8-4138-8c07-78065d3d2a1e,End-to-End Spatio-Temporal Action Localisation with Video Transformers,0.636293,"The most performant spatio-temporal action localisation models use external
person proposals and complex external memory banks. We propose a fully
end-to-end, purely-transformer based model that directly ingests an input
video, and outputs tubelets -- a sequence of bounding boxes and the action
classes at each frame. Our flexible model can be trained with either sparse
bounding-box supervision on individual frames, or full tubelet annotations. And
in both cases, it predicts coherent tubelets as the output. Moreover, our
end-to-end model requires no additional pre-processing in the form of
proposals, or post-processing in terms of non-maximal suppression. We perform
extensive ablation experiments, and significantly advance the state-of-the-art
results on four different spatio-temporal action localisation benchmarks with
both sparse keyframes and full tubelet annotations.",None,-1
1956c161-f482-4c2f-9ffa-be3b3fc13740,TransFool: An Adversarial Attack against Neural Machine Translation Models,0.687018,"Deep neural networks have been shown to be vulnerable to small perturbations
of their inputs, known as adversarial attacks. In this paper, we investigate
the vulnerability of Neural Machine Translation (NMT) models to adversarial
attacks and propose a new attack algorithm called TransFool. To fool NMT
models, TransFool builds on a multi-term optimization problem and a gradient
projection step. By integrating the embedding representation of a language
model, we generate fluent adversarial examples in the source language that
maintain a high level of semantic similarity with the clean samples.
Experimental results demonstrate that, for different translation tasks and NMT
architectures, our white-box attack can severely degrade the translation
quality while the semantic similarity between the original and the adversarial
sentences stays high. Moreover, we show that TransFool is transferable to
unknown target models. Finally, based on automatic and human evaluations,
TransFool leads to improvement in terms of success rate, semantic similarity,
and fluency compared to the existing attacks both in white-box and black-box
settings. Thus, TransFool permits us to better characterize the vulnerability
of NMT models and outlines the necessity to design strong defense mechanisms
and more robust NMT systems for real-life applications.",None,-1
d4fd22fc-c9db-4a1b-b557-d4932138e9b2,Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization,0.902196,"Weakly-supervised temporal action localization aims to localize and recognize
actions in untrimmed videos with only video-level category labels during
training. Without instance-level annotations, most existing methods follow the
Segment-based Multiple Instance Learning (S-MIL) framework, where the
predictions of segments are supervised by the labels of videos. However, the
objective for acquiring segment-level scores during training is not consistent
with the target for acquiring proposal-level scores during testing, leading to
suboptimal results. To deal with this problem, we propose a novel
Proposal-based Multiple Instance Learning (P-MIL) framework that directly
classifies the candidate proposals in both the training and testing stages,
which includes three key designs: 1) a surrounding contrastive feature
extraction module to suppress the discriminative short proposals by considering
the surrounding contrastive information, 2) a proposal completeness evaluation
module to inhibit the low-quality proposals with the guidance of the
completeness pseudo labels, and 3) an instance-level rank consistency loss to
achieve robust detection by leveraging the complementarity of RGB and FLOW
modalities. Extensive experimental results on two challenging benchmarks
including THUMOS14 and ActivityNet demonstrate the superior performance of our
method.",None,-1
d77f1678-21fc-401e-92e7-cb0ea3d34089,InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules,0.173946,"Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant
challenge that existing approaches struggle to address without extensive
modifications to vanilla NeRF framework. We introduce InsertNeRF, a method for
INStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play
HyperNet modules, InsertNeRF dynamically tailors NeRF's weights to specific
reference scenes, transforming multi-scale sampling-aware features into
scene-specific representations. This novel design allows for more accurate and
efficient representations of complex appearances and geometries. Experiments
show that this method not only achieves superior generalization performance but
also provides a flexible pathway for integration with other NeRF-like systems,
even in sparse input settings. Code will be available
https://github.com/bbbbby-99/InsertNeRF.",None,-1
1e5c0c05-beb8-4227-9c1b-d16ee85259ed,Epic-Sounds: A Large-scale Dataset of Actions That Sound,0.957615,"We introduce EPIC-SOUNDS, a large-scale dataset of audio annotations
capturing temporal extents and class labels within the audio stream of the
egocentric videos. We propose an annotation pipeline where annotators
temporally label distinguishable audio segments and describe the action that
could have caused this sound. We identify actions that can be discriminated
purely from audio, through grouping these free-form descriptions of audio into
classes. For actions that involve objects colliding, we collect human
annotations of the materials of these objects (e.g. a glass object being placed
on a wooden surface), which we verify from visual labels, discarding
ambiguities. Overall, EPIC-SOUNDS includes 78.4k categorised segments of
audible events and actions, distributed across 44 classes as well as 39.2k
non-categorised segments. We train and evaluate two state-of-the-art audio
recognition models on our dataset, highlighting the importance of audio-only
labels and the limitations of current models to recognise actions that sound.",None,-1
0e3a5f25-69c7-4f8d-ae8e-c7ff5c4bb31f,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",0.621143,"Human feedback is increasingly used to steer the behaviours of Large Language
Models (LLMs). However, it is unclear how to collect and incorporate feedback
in a way that is efficient, effective and unbiased, especially for highly
subjective human preferences and values. In this paper, we survey existing
approaches for learning from human feedback, drawing on 95 papers primarily
from the ACL and arXiv repositories.First, we summarise the past, pre-LLM
trends for integrating human feedback into language models. Second, we give an
overview of present techniques and practices, as well as the motivations for
using feedback; conceptual frameworks for defining values and preferences; and
how feedback is collected and from whom. Finally, we encourage a better future
of feedback learning in LLMs by raising five unresolved conceptual and
practical challenges.",None,-1
d812d48c-750a-4573-a0e4-c1b5ee1e734d,Learning Universal Policies via Text-Guided Video Generation,0.975186,"A goal of artificial intelligence is to construct an agent that can solve a
wide variety of tasks. Recent progress in text-guided image synthesis has
yielded models with an impressive ability to generate complex novel images,
exhibiting combinatorial generalization across domains. Motivated by this
success, we investigate whether such tools can be used to construct more
general-purpose agents. Specifically, we cast the sequential decision making
problem as a text-conditioned video generation problem, where, given a
text-encoded specification of a desired goal, a planner synthesizes a set of
future frames depicting its planned actions in the future, after which control
actions are extracted from the generated video. By leveraging text as the
underlying goal specification, we are able to naturally and combinatorially
generalize to novel goals. The proposed policy-as-video formulation can further
represent environments with different state and action spaces in a unified
space of images, which, for example, enables learning and generalization across
a variety of robot manipulation tasks. Finally, by leveraging pretrained
language embeddings and widely available videos from the internet, the approach
enables knowledge transfer through predicting highly realistic video plans for
real robots.",None,-1
47c670f3-b791-48c0-a166-8b162ff41393,Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks,0.632121,"In this paper, a new deep-learning architecture for solving the non-linear
Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks,
this approach shows how orthogonal polynomials can be used in neural networks
to increase the approximation capability of artificial neural networks. In
addition, utilizing the mathematical properties of these functions, we overcome
the computational complexity of the backpropagation algorithm by using the
operational matrices of the derivative. The efficiency of the proposed method
is carried out by simulating various configurations of the Falkner-Skan
equation.",None,-1
dffef0e1-cf71-4837-81c8-4c93be3b6de8,A Computational Approach to Style in American Poetry,0.29714,"We develop a quantitative method to assess the style of American poems and to
visualize a collection of poems in relation to one another. Qualitative poetry
criticism helped guide our development of metrics that analyze various
orthographic, syntactic, and phonemic features. These features are used to
discover comprehensive stylistic information from a poem's multi-layered latent
structure, and to compute distances between poems in this space. Visualizations
provide ready access to the analytical components. We demonstrate our method on
several collections of poetry, showing that it better delineates poetry style
than the traditional word-occurrence features that are used in typical text
analysis algorithms. Our method has potential applications to academic research
of texts, to research of the intuitive personal response to poetry, and to
making recommendations to readers based on their favorite poems.",None,-1
4873741b-618e-496d-939c-9be51eafd938,Semantic Strengthening of Neuro-Symbolic Learning,0.735066,"Numerous neuro-symbolic approaches have recently been proposed typically with
the goal of adding symbolic knowledge to the output layer of a neural network.
Ideally, such losses maximize the probability that the neural network's
predictions satisfy the underlying domain. Unfortunately, this type of
probabilistic inference is often computationally infeasible. Neuro-symbolic
approaches therefore commonly resort to fuzzy approximations of this
probabilistic objective, sacrificing sound probabilistic semantics, or to
sampling which is very seldom feasible. We approach the problem by first
assuming the constraint decomposes conditioned on the features learned by the
network. We iteratively strengthen our approximation, restoring the dependence
between the constraints most responsible for degrading the quality of the
approximation. This corresponds to computing the mutual information between
pairs of constraints conditioned on the network's learned features, and may be
construed as a measure of how well aligned the gradients of two distributions
are. We show how to compute this efficiently for tractable circuits. We test
our approach on three tasks: predicting a minimum-cost path in Warcraft,
predicting a minimum-cost perfect matching, and solving Sudoku puzzles,
observing that it improves upon the baselines while sidestepping
intractability.",None,-1
602d636e-1c9a-4570-85f7-707379b369b9,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,0.709024,"In this paper, we introduce the BeaverTails dataset, aimed at fostering
research on safety alignment in large language models (LLMs). This dataset
uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have gathered safety meta-labels for 333,963
question-answer (QA) pairs and 361,903 pairs of expert comparison data for both
the helpfulness and harmlessness metrics. We further showcase applications of
BeaverTails in content moderation and reinforcement learning with human
feedback (RLHF), emphasizing its potential for practical safety measures in
LLMs. We believe this dataset provides vital resources for the community,
contributing towards the safe development and deployment of LLMs. Our project
page is available at the following URL:
https://sites.google.com/view/pku-beavertails.",None,-1
3dcaaee7-b18c-423c-8c32-4477da693c9b,FaceLit: Neural 3D Relightable Faces,0.356079,"We propose a generative framework, FaceLit, capable of generating a 3D face
that can be rendered at various user-defined lighting conditions and views,
learned purely from 2D images in-the-wild without any manual annotation. Unlike
existing works that require careful capture setup or human labor, we rely on
off-the-shelf pose and illumination estimators. With these estimates, we
incorporate the Phong reflectance model in the neural volume rendering
framework. Our model learns to generate shape and material properties of a face
such that, when rendered according to the natural statistics of pose and
illumination, produces photorealistic face images with multiview 3D and
illumination consistency. Our method enables photorealistic generation of faces
with explicit illumination and view controls on multiple datasets - FFHQ,
MetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware
GANs on FFHQ dataset achieving an FID score of 3.5.",None,-1
1d9ccf00-e9a9-456f-b809-32c38d3c1854,Monocular Visual-Inertial Depth Estimation,0.836452,"We present a visual-inertial depth estimation pipeline that integrates
monocular depth estimation and visual-inertial odometry to produce dense depth
estimates with metric scale. Our approach performs global scale and shift
alignment against sparse metric depth, followed by learning-based dense
alignment. We evaluate on the TartanAir and VOID datasets, observing up to 30%
reduction in inverse RMSE with dense scale alignment relative to performing
just global alignment alone. Our approach is especially competitive at low
density; with just 150 sparse metric depth points, our dense-to-dense depth
alignment method achieves over 50% lower iRMSE over sparse-to-dense depth
completion by KBNet, currently the state of the art on VOID. We demonstrate
successful zero-shot transfer from synthetic TartanAir to real-world VOID data
and perform generalization tests on NYUv2 and VCU-RVI. Our approach is modular
and is compatible with a variety of monocular depth estimation models. Video:
https://youtu.be/IMwiKwSpshQ Code: https://github.com/isl-org/VI-Depth",None,-1
c2cd6d76-fcda-4845-9906-23fc04d014b7,The Confidence-Competence Gap in Large Language Models: A Cognitive Study,0.0380692,"Large Language Models (LLMs) have acquired ubiquitous attention for their
performances across diverse domains. Our study here searches through LLMs'
cognitive abilities and confidence dynamics. We dive deep into understanding
the alignment between their self-assessed confidence and actual performance. We
exploit these models with diverse sets of questionnaires and real-world
scenarios and extract how LLMs exhibit confidence in their responses. Our
findings reveal intriguing instances where models demonstrate high confidence
even when they answer incorrectly. This is reminiscent of the Dunning-Kruger
effect observed in human psychology. In contrast, there are cases where models
exhibit low confidence with correct answers revealing potential underestimation
biases. Our results underscore the need for a deeper understanding of their
cognitive processes. By examining the nuances of LLMs' self-assessment
mechanism, this investigation provides noteworthy revelations that serve to
advance the functionalities and broaden the potential applications of these
formidable language models.",None,-1
3e2c6986-adb7-48ca-b48d-bf284fd23f27,RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer,0.60886,"Non-autoregressive neural machine translation (NAT) models are proposed to
accelerate the inference process while maintaining relatively high performance.
However, existing NAT models are difficult to achieve the desired
efficiency-quality trade-off. For one thing, fully NAT models with efficient
inference perform inferior to their autoregressive counterparts. For another,
iterative NAT models can, though, achieve comparable performance while
diminishing the advantage of speed. In this paper, we propose RenewNAT, a
flexible framework with high efficiency and effectiveness, to incorporate the
merits of fully and iterative NAT models. RenewNAT first generates the
potential translation results and then renews them in a single pass. It can
achieve significant performance improvements at the same expense as traditional
NAT models (without introducing additional model parameters and decoding
latency). Experimental results on various translation benchmarks (e.g.,
\textbf{4} WMT) show that our framework consistently improves the performance
of strong fully NAT methods (e.g., GLAT and DSLP) without additional speed
overhead.",None,-1
e5e1b435-f4ce-452e-a052-94ed6cd55607,Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding,0.959829,"3D GAN inversion aims to achieve high reconstruction fidelity and reasonable
3D geometry simultaneously from a single image input. However, existing 3D GAN
inversion methods rely on time-consuming optimization for each individual case.
In this work, we introduce a novel encoder-based inversion framework based on
EG3D, one of the most widely-used 3D GAN models. We leverage the inherent
properties of EG3D's latent space to design a discriminator and a background
depth regularization. This enables us to train a geometry-aware encoder capable
of converting the input image into corresponding latent code. Additionally, we
explore the feature space of EG3D and develop an adaptive refinement stage that
improves the representation ability of features in EG3D to enhance the recovery
of fine-grained textural details. Finally, we propose an occlusion-aware fusion
operation to prevent distortion in unobserved regions. Our method achieves
impressive results comparable to optimization-based methods while operating up
to 500 times faster. Our framework is well-suited for applications such as
semantic editing.",None,-1
28033602-1634-4430-864a-2f730c15d2b7,Digital Twin Applications in Urban Logistics: An Overview,0.650761,"Urban traffic attributed to commercial and industrial transportation is
observed to largely affect living standards in cities due to external effects
pertaining to pollution and congestion. In order to counter this, smart cities
deploy technological tools to achieve sustainability. Such tools include
Digital Twins (DT)s which are virtual replicas of real-life physical systems.
Research suggests that DTs can be very beneficial in how they control a
physical system by constantly optimizing its performance. The concept has been
extensively studied in other technology-driven industries like manufacturing.
However, little work has been done with regards to their application in urban
logistics. In this paper, we seek to provide a framework by which DTs could be
easily adapted to urban logistics networks. To do this, we provide a
characterization of key factors in urban logistics for dynamic decision-making.
We also survey previous research on DT applications in urban logistics as we
found that a holistic overview is lacking. Using this knowledge in combination
with the characterization, we produce a conceptual model that describes the
ontology, learning capabilities and optimization prowess of an urban logistics
digital twin through its quantitative models. We finish off with a discussion
on potential research benefits and limitations based on previous research and
our practical experience.",None,-1
5d84e3fb-c74b-4e85-b83c-ca28a7c8f229,The Quo Vadis of the Relationship between Language and Large Language Models,0.0711078,"In the field of Artificial (General) Intelligence (AI), the several recent
advancements in Natural language processing (NLP) activities relying on Large
Language Models (LLMs) have come to encourage the adoption of LLMs as
scientific models of language. While the terminology employed for the
characterization of LLMs favors their embracing as such, it is not clear that
they are in a place to offer insights into the target system they seek to
represent. After identifying the most important theoretical and empirical risks
brought about by the adoption of scientific models that lack transparency, we
discuss LLMs relating them to every scientific model's fundamental components:
the object, the medium, the meaning and the user. We conclude that, at their
current stage of development, LLMs hardly offer any explanations for language,
and then we provide an outlook for more informative future research directions
on this topic.",None,-1
1757cf8e-1e1b-40f6-9777-d5ced37a41b0,Collective Privacy Recovery: Data-sharing Coordination via Decentralized Artificial Intelligence,0.66407,"Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving >27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers.",None,-1
aee436bc-7ac9-46e2-84d9-2e0dd8a95b14,MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis,0.0796629,"Existing multimodal conditional image synthesis (MCIS) methods generate
images conditioned on any combinations of various modalities that require all
of them must be exactly conformed, hindering the synthesis controllability and
leaving the potential of cross-modality under-exploited. To this end, we
propose to generate images conditioned on the compositions of multimodal
control signals, where modalities are imperfectly complementary, i.e., composed
multimodal conditional image synthesis (CMCIS). Specifically, we observe two
challenging issues of the proposed CMCIS task, i.e., the modality coordination
problem and the modality imbalance problem. To tackle these issues, we
introduce a Mixture-of-Modality-Tokens Transformer (MMoT) that adaptively fuses
fine-grained multimodal control signals, a multimodal balanced training loss to
stabilize the optimization of each modality, and a multimodal sampling guidance
to balance the strength of each modality control signal. Comprehensive
experimental results demonstrate that MMoT achieves superior performance on
both unimodal conditional image synthesis (UCIS) and MCIS tasks with
high-quality and faithful image synthesis on complex multimodal conditions. The
project website is available at https://jabir-zheng.github.io/MMoT.",None,-1
9e0bb62e-d71f-417b-b1db-d930cb165578,MixedTeacher : Knowledge Distillation for fast inference textural anomaly detection,0.304856,"For a very long time, unsupervised learning for anomaly detection has been at
the heart of image processing research and a stepping stone for high
performance industrial automation process. With the emergence of CNN, several
methods have been proposed such as Autoencoders, GAN, deep feature extraction,
etc. In this paper, we propose a new method based on the promising concept of
knowledge distillation which consists of training a network (the student) on
normal samples while considering the output of a larger pretrained network (the
teacher). The main contributions of this paper are twofold: First, a reduced
student architecture with optimal layer selection is proposed, then a new
Student-Teacher architecture with network bias reduction combining two teachers
is proposed in order to jointly enhance the performance of anomaly detection
and its localization accuracy. The proposed texture anomaly detector has an
outstanding capability to detect defects in any texture and a fast inference
time compared to the SOTA methods.",None,-1
36e4e50a-1127-4f00-8f69-0e044756255c,Chatbots as Problem Solvers: Playing Twenty Questions with Role Reversals,0.138944,"New chat AI applications like ChatGPT offer an advanced understanding of
question context and memory across multi-step tasks, such that experiments can
test its deductive reasoning. This paper proposes a multi-role and multi-step
challenge, where ChatGPT plays the classic twenty-questions game but
innovatively switches roles from the questioner to the answerer. The main
empirical result establishes that this generation of chat applications can
guess random object names in fewer than twenty questions (average, 12) and
correctly guess 94% of the time across sixteen different experimental setups.
The research introduces four novel cases where the chatbot fields the
questions, asks the questions, both question-answer roles, and finally tries to
guess appropriate contextual emotions. One task that humans typically fail but
trained chat applications complete involves playing bilingual games of twenty
questions (English answers to Spanish questions). Future variations address
direct problem-solving using a similar inquisitive format to arrive at novel
outcomes deductively, such as patentable inventions or combination thinking.
Featured applications of this dialogue format include complex protein designs,
neuroscience metadata, and child development educational materials.",None,-1
6955368a-067d-48b2-840c-a3624efabaf5,Quantifying the perceptual value of lexical and non-lexical channels in speech,0.202477,"Speech is a fundamental means of communication that can be seen to provide
two channels for transmitting information: the lexical channel of which words
are said, and the non-lexical channel of how they are spoken. Both channels
shape listener expectations of upcoming communication; however, directly
quantifying their relative effect on expectations is challenging. Previous
attempts require spoken variations of lexically-equivalent dialogue turns or
conspicuous acoustic manipulations. This paper introduces a generalised
paradigm to study the value of non-lexical information in dialogue across
unconstrained lexical content. By quantifying the perceptual value of the
non-lexical channel with both accuracy and entropy reduction, we show that
non-lexical information produces a consistent effect on expectations of
upcoming dialogue: even when it leads to poorer discriminative turn judgements
than lexical content alone, it yields higher consensus among participants.",None,-1
4f3577c9-11a6-4ab2-a7c5-dbf65bc29050,Multi-view Cross-Modality MR Image Translation for Vestibular Schwannoma and Cochlea Segmentation,0.552344,"In this work, we propose a multi-view image translation framework, which can
translate contrast-enhanced T1 (ceT1) MR imaging to high-resolution T2 (hrT2)
MR imaging for unsupervised vestibular schwannoma and cochlea segmentation. We
adopt two image translation models in parallel that use a pixel-level
consistent constraint and a patch-level contrastive constraint, respectively.
Thereby, we can augment pseudo-hrT2 images reflecting different perspectives,
which eventually lead to a high-performing segmentation model. Our experimental
results on the CrossMoDA challenge show that the proposed method achieved
enhanced performance on the vestibular schwannoma and cochlea segmentation.",None,-1
42f13c6a-70c6-4dc5-b9a9-63882d7e4066,Large Language Models Cannot Self-Correct Reasoning Yet,0.86057,"Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance even
degrades after self-correction. Drawing from these insights, we offer
suggestions for future research and practical applications in this field.",None,-1
f93c2ab3-1c3b-4d6f-a4f7-1941aaed1d83,Event Temporal Relation Extraction with Bayesian Translational Model,0.425748,"Existing models to extract temporal relations between events lack a
principled method to incorporate external knowledge. In this study, we
introduce Bayesian-Trans, a Bayesian learning-based method that models the
temporal relation representations as latent variables and infers their values
via Bayesian inference and translational functions. Compared to conventional
neural approaches, instead of performing point estimation to find the best set
parameters, the proposed model infers the parameters' posterior distribution
directly, enhancing the model's capability to encode and express uncertainty
about the predictions. Experimental results on the three widely used datasets
show that Bayesian-Trans outperforms existing approaches for event temporal
relation extraction. We additionally present detailed analyses on uncertainty
quantification, comparison of priors, and ablation studies, illustrating the
benefits of the proposed approach.",None,-1
bff4edf7-35f1-4601-9516-7137963ae1d3,BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives,0.457191,"Implicit neural representations have become pivotal in robotic perception,
enabling robots to comprehend 3D environments from 2D images. Given a set of
camera poses and associated images, the models can be trained to synthesize
novel, unseen views. To successfully navigate and interact in dynamic settings,
robots require the understanding of their spatial surroundings driven by
unassisted reconstruction of 3D scenes and camera poses from real-time video
footage. Existing approaches like COLMAP and bundle-adjusting neural radiance
field methods take hours to days to process due to the high computational
demands of feature matching, dense point sampling, and training of a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP) which leverages accelerated sampling and
hash encoding to expedite automatic pose refinement/estimation and 3D scene
reconstruction. Experimental results demonstrate 10 to 20 x speed improvement
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation. The github repository can be found
here https://github.com/IntelLabs/baa-ngp.",None,-1
56ed26e3-623c-49da-bd03-d2f4faca19bf,TADA! Text to Animatable Digital Avatars,1.0,"We introduce TADA, a simple-yet-effective approach that takes textual
descriptions and produces expressive 3D avatars with high-quality geometry and
lifelike textures, that can be animated and rendered with traditional graphics
pipelines. Existing text-based character generation methods are limited in
terms of geometry and texture quality, and cannot be realistically animated due
to inconsistent alignment between the geometry and the texture, particularly in
the face region. To overcome these limitations, TADA leverages the synergy of a
2D diffusion model and an animatable parametric body model. Specifically, we
derive an optimizable high-resolution body model from SMPL-X with 3D
displacements and a texture map, and use hierarchical rendering with score
distillation sampling (SDS) to create high-quality, detailed, holistic 3D
avatars from text. To ensure alignment between the geometry and texture, we
render normals and RGB images of the generated character and exploit their
latent embeddings in the SDS training process. We further introduce various
expression parameters to deform the generated character during training,
ensuring that the semantics of our generated character remain consistent with
the original SMPL-X model, resulting in an animatable character. Comprehensive
evaluations demonstrate that TADA significantly surpasses existing approaches
on both qualitative and quantitative measures. TADA enables creation of
large-scale digital character assets that are ready for animation and
rendering, while also being easily editable through natural language. The code
will be public for research purposes.",None,-1
5b5f7ccc-81d5-446f-bad1-363e0ed962b0,Generalized Planning in PDDL Domains with Pretrained Large Language Models,0.999973,"Recent work has considered whether large language models (LLMs) can function
as planners: given a task, generate a plan. We investigate whether LLMs can
serve as generalized planners: given a domain and training tasks, generate a
program that efficiently produces plans for other tasks in the domain. In
particular, we consider PDDL domains and use GPT-4 to synthesize Python
programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the
LLM is prompted to summarize the domain and propose a strategy in words before
synthesizing the program; and (2) automated debugging, where the program is
validated with respect to the training tasks, and in case of errors, the LLM is
re-prompted with four types of feedback. We evaluate this approach in seven
PDDL domains and compare it to four ablations and four baselines. Overall, we
find that GPT-4 is a surprisingly powerful generalized planner. We also
conclude that automated debugging is very important, that CoT summarization has
non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two
training tasks are often sufficient for strong generalization.",None,-1
ab19b95c-14e2-4a73-8580-5715ca94122e,Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten,0.47037,"The Right to Explanation and the Right to be Forgotten are two important
principles outlined to regulate algorithmic decision making and data usage in
real-world applications. While the right to explanation allows individuals to
request an actionable explanation for an algorithmic decision, the right to be
forgotten grants them the right to ask for their data to be deleted from all
the databases and models of an organization. Intuitively, enforcing the right
to be forgotten may trigger model updates which in turn invalidate previously
provided explanations, thus violating the right to explanation. In this work,
we investigate the technical implications arising due to the interference
between the two aforementioned regulatory principles, and propose the first
algorithmic framework to resolve the tension between them. To this end, we
formulate a novel optimization problem to generate explanations that are robust
to model updates due to the removal of training data instances by data deletion
requests. We then derive an efficient approximation algorithm to handle the
combinatorial complexity of this optimization problem. We theoretically
demonstrate that our method generates explanations that are provably robust to
worst-case data deletion requests with bounded costs in case of linear models
and certain classes of non-linear models. Extensive experimentation with
real-world datasets demonstrates the efficacy of the proposed framework.",None,-1
e997a552-8238-4cb2-8bee-687910ae8586,Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data,0.188949,"Multivariate time series (MTS) data collected from multiple sensors provide
the potential for accurate abnormal activity detection in smart healthcare
scenarios. However, anomalies exhibit diverse patterns and become unnoticeable
in MTS data. Consequently, achieving accurate anomaly detection is challenging
since we have to capture both temporal dependencies of time series and
inter-relationships among variables. To address this problem, we propose a
Residual-based Anomaly Detection approach, Rs-AD, for effective representation
learning and abnormal activity detection. We evaluate our scheme on a
real-world gait dataset and the experimental results demonstrate an F1 score of
0.839.",None,-1
4e97e66a-57c1-4f9b-9eca-139a2bf5aae7,Practical PCG Through Large Language Models,0.306927,"Large Language Models (LLMs) have proven to be useful tools in various
domains outside of the field of their inception, which was natural language
processing. In this study, we provide practical directions on how to use LLMs
to generate 2D-game rooms for an under-development game, named Metavoidal. Our
technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which
allows our method to create 37% Playable-Novel levels from as scarce data as
only 60 hand-designed rooms under a scenario of the non-trivial game, with
respect to (Procedural Content Generation) PCG, that has a good amount of local
and global constraints.",None,-1
bfef7069-4df2-4cbd-abab-4030751f6770,The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers,0.698124,"Interpretable part-prototype models are computer vision models that are
explainable by design. The models learn prototypical parts and recognise these
components in an image, thereby combining classification and explanation.
Despite the recent attention for intrinsically interpretable models, there is
no comprehensive overview on evaluating the explanation quality of
interpretable part-prototype models. Based on the Co-12 properties for
explanation quality as introduced in arXiv:2201.08164 (e.g., correctness,
completeness, compactness), we review existing work that evaluates
part-prototype models, reveal research gaps and outline future approaches for
evaluation of the explanation quality of part-prototype models. This paper,
therefore, contributes to the progression and maturity of this relatively new
research field on interpretable part-prototype models. We additionally provide
a ``Co-12 cheat sheet'' that acts as a concise summary of our findings on
evaluating part-prototype models.",None,-1
82732f7d-07e9-49d8-b19d-96256379d33a,Deep Deformable Models: Learning 3D Shape Abstractions with Part Consistency,0.268402,"The task of shape abstraction with semantic part consistency is challenging
due to the complex geometries of natural objects. Recent methods learn to
represent an object shape using a set of simple primitives to fit the target.
\textcolor{black}{However, in these methods, the primitives used do not always
correspond to real parts or lack geometric flexibility for semantic
interpretation.} In this paper, we investigate salient and efficient primitive
descriptors for accurate shape abstractions, and propose \textit{Deep
Deformable Models (DDMs)}. DDM employs global deformations and diffeomorphic
local deformations. These properties enable DDM to abstract complex object
shapes with significantly fewer primitives that offer broader geometry coverage
and finer details. DDM is also capable of learning part-level semantic
correspondences due to the differentiable and invertible properties of our
primitive deformation. Moreover, DDM learning formulation is based on dynamic
and kinematic modeling, which enables joint regularization of each
sub-transformation during primitive fitting. Extensive experiments on
\textit{ShapeNet} demonstrate that DDM outperforms the state-of-the-art in
terms of reconstruction and part consistency by a notable margin.",None,-1
8f625610-1897-46b8-ad3f-c14e3caa6d43,Lightweight Adaptation of Neural Language Models via Subspace Embedding,0.0301973,"Traditional neural word embeddings are usually dependent on a richer
diversity of vocabulary. However, the language models recline to cover major
vocabularies via the word embedding parameters, in particular, for multilingual
language models that generally cover a significant part of their overall
learning parameters. In this work, we present a new compact embedding structure
to reduce the memory footprint of the pre-trained language models with a
sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction
follows a set of subspace embeddings and an assignment procedure via the
contextual relationship among tokens from pre-trained language models. The
subspace embedding structure calibrates to masked language models, to evaluate
our compact embedding structure on similarity and textual entailment tasks,
sentence and paraphrase tasks. Our experimental evaluation shows that the
subspace embeddings achieve compression rates beyond 99.8% in comparison with
the original embeddings for the language models on XNLI and GLUE benchmark
suites.",None,-1
29aab42d-df17-4a83-869b-7547ae66c8f9,Polynomial Neural Fields for Subband Decomposition and Manipulation,0.346051,"Neural fields have emerged as a new paradigm for representing signals, thanks
to their ability to do it compactly while being easy to optimize. In most
applications, however, neural fields are treated like black boxes, which
precludes many signal manipulation tasks. In this paper, we propose a new class
of neural fields called polynomial neural fields (PNFs). The key advantage of a
PNF is that it can represent a signal as a composition of a number of
manipulable and interpretable components without losing the merits of neural
fields representation. We develop a general theoretical framework to analyze
and design PNFs. We use this framework to design Fourier PNFs, which match
state-of-the-art performance in signal representation tasks that use neural
fields. In addition, we empirically demonstrate that Fourier PNFs enable signal
manipulation applications such as texture transfer and scale-space
interpolation. Code is available at https://github.com/stevenygd/PNF.",None,-1
27f1923f-4583-453d-9364-eca23ea6715d,Context-faithful Prompting for Large Language Models,0.210835,"Large language models (LLMs) encode parametric knowledge about world facts
and have shown remarkable performance in knowledge-driven NLP tasks. However,
their reliance on parametric knowledge may cause them to overlook contextual
cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,
knowledge acquisition tasks). In this paper, we seek to assess and enhance
LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction
with abstention. We demonstrate that LLMs' faithfulness can be significantly
improved using carefully designed prompting strategies. In particular, we
identify opinion-based prompts and counterfactual demonstrations as the most
effective methods. Opinion-based prompts reframe the context as a narrator's
statement and inquire about the narrator's opinions, while counterfactual
demonstrations use instances containing false facts to improve faithfulness in
knowledge conflict situations. Neither technique requires additional training.
We conduct experiments on three datasets of two standard NLP tasks, machine
reading comprehension and relation extraction, and the results demonstrate
significant improvement in faithfulness to contexts. Code and data are released
at https://github.com/wzhouad/context-faithful-llm.",None,-1
72a38d30-16f8-4bc1-a3c1-77c2c1048b22,Revisiting Rotation Averaging: Uncertainties and Robust Losses,0.284427,"In this paper, we revisit the rotation averaging problem applied in global
Structure-from-Motion pipelines. We argue that the main problem of current
methods is the minimized cost function that is only weakly connected with the
input data via the estimated epipolar geometries.We propose to better model the
underlying noise distributions by directly propagating the uncertainty from the
point correspondences into the rotation averaging. Such uncertainties are
obtained for free by considering the Jacobians of two-view refinements.
Moreover, we explore integrating a variant of the MAGSAC loss into the rotation
averaging problem, instead of using classical robust losses employed in current
frameworks. The proposed method leads to results superior to baselines, in
terms of accuracy, on large-scale public benchmarks. The code is public.
https://github.com/zhangganlin/GlobalSfMpy",None,-1
b8ef8437-451c-43f5-9776-658cd2e5b3f3,Probabilistic Circuits That Know What They Don't Know,0.665424,"Probabilistic circuits (PCs) are models that allow exact and tractable
probabilistic inference. In contrast to neural networks, they are often assumed
to be well-calibrated and robust to out-of-distribution (OOD) data. In this
paper, we show that PCs are in fact not robust to OOD data, i.e., they don't
know what they don't know. We then show how this challenge can be overcome by
model uncertainty quantification. To this end, we propose tractable dropout
inference (TDI), an inference procedure to estimate uncertainty by deriving an
analytical solution to Monte Carlo dropout (MCD) through variance propagation.
Unlike MCD in neural networks, which comes at the cost of multiple network
evaluations, TDI provides tractable sampling-free uncertainty estimates in a
single forward pass. TDI improves the robustness of PCs to distribution shift
and OOD data, demonstrated through a series of experiments evaluating the
classification confidence and uncertainty estimates on real-world data.",None,-1
20c9d980-61c4-4520-b3ce-8b49866d171e,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,0.323726,"We propose a novel framework for the task of object-centric video prediction,
i.e., extracting the compositional structure of a video sequence, as well as
modeling objects dynamics and interactions from visual observations in order to
predict the future object states, from which we can then generate subsequent
video frames. With the goal of learning meaningful spatio-temporal object
representations and accurately forecasting object states, we propose two novel
object-centric video predictor (OCVP) transformer modules, which decouple the
processing of temporal dynamics and object interactions, thus presenting an
improved prediction performance. In our experiments, we show how our
object-centric prediction framework utilizing our OCVP predictors outperforms
object-agnostic video prediction models on two different datasets, while
maintaining consistent and accurate object representations.",None,-1
8c62d505-f8db-488b-8587-241f89bd7276,SwinFSR: Stereo Image Super-Resolution using SwinIR and Frequency Domain Knowledge,0.542926,"Stereo Image Super-Resolution (stereoSR) has attracted significant attention
in recent years due to the extensive deployment of dual cameras in mobile
phones, autonomous vehicles and robots. In this work, we propose a new StereoSR
method, named SwinFSR, based on an extension of SwinIR, originally designed for
single image restoration, and the frequency domain knowledge obtained by the
Fast Fourier Convolution (FFC). Specifically, to effectively gather global
information, we modify the Residual Swin Transformer blocks (RSTBs) in SwinIR
by explicitly incorporating the frequency domain knowledge using the FFC and
employing the resulting residual Swin Fourier Transformer blocks (RSFTBs) for
feature extraction. Besides, for the efficient and accurate fusion of stereo
views, we propose a new cross-attention module referred to as RCAM, which
achieves highly competitive performance while requiring less computational cost
than the state-of-the-art cross-attention modules. Extensive experimental
results and ablation studies demonstrate the effectiveness and efficiency of
our proposed SwinFSR.",None,-1
f90dda6f-4c90-4f4a-9d7c-1a0df85ddc27,Partial Network Cloning,0.83801,"In this paper, we study a novel task that enables partial knowledge transfer
from pre-trained models, which we term as Partial Network Cloning (PNC). Unlike
prior methods that update all or at least part of the parameters in the target
network throughout the knowledge transfer process, PNC conducts partial
parametric ""cloning"" from a source network and then injects the cloned module
to the target, without modifying its parameters. Thanks to the transferred
module, the target network is expected to gain additional functionality, such
as inference on new classes; whenever needed, the cloned module can be readily
removed from the target, with its original parameters and competence kept
intact. Specifically, we introduce an innovative learning scheme that allows us
to identify simultaneously the component to be cloned from the source and the
position to be inserted within the target network, so as to ensure the optimal
performance. Experimental results on several datasets demonstrate that, our
method yields a significant improvement of 5% in accuracy and 50% in locality
when compared with parameter-tuning based methods. Our code is available at
https://github.com/JngwenYe/PNCloning.",None,-1
76bc10b4-81e6-4e8f-9ae1-90991de2d565,P+: Extended Textual Conditioning in Text-to-Image Generation,0.918145,"We introduce an Extended Textual Conditioning space in text-to-image models,
referred to as $P+$. This space consists of multiple textual conditions,
derived from per-layer prompts, each corresponding to a layer of the denoising
U-net of the diffusion model.
  We show that the extended space provides greater disentangling and control
over image synthesis. We further introduce Extended Textual Inversion (XTI),
where the images are inverted into $P+$, and represented by per-layer tokens.
  We show that XTI is more expressive and precise, and converges faster than
the original Textual Inversion (TI) space. The extended inversion method does
not involve any noticeable trade-off between reconstruction and editability and
induces more regular inversions.
  We conduct a series of extensive experiments to analyze and understand the
properties of the new space, and to showcase the effectiveness of our method
for personalizing text-to-image models. Furthermore, we utilize the unique
properties of this space to achieve previously unattainable results in
object-style mixing using text-to-image models. Project page:
https://prompt-plus.github.io",None,-1
5a94c8ac-a5d5-40bc-bf57-ba2043814f07,Stress Testing Chain-of-Thought Prompting for Large Language Models,0.00965626,"This report examines the effectiveness of Chain-of-Thought (CoT) prompting in
improving the multi-step reasoning abilities of large language models (LLMs).
Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the
impact of three types of CoT prompt perturbations, namely CoT order, CoT
values, and CoT operators on the performance of GPT-3 on various tasks. Our
findings show that incorrect CoT prompting leads to poor performance on
accuracy metrics. Correct values in the CoT is crucial for predicting correct
answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT
order are wrong, do not affect the performance as drastically when compared to
the value based perturbations. This research deepens our understanding of CoT
prompting and opens some new questions regarding the capability of LLMs to
learn reasoning in context.",None,-1
c194530d-0e99-440c-9621-51f96e8960b3,Reusable Slotwise Mechanisms,0.101032,"Agents with the ability to comprehend and reason about the dynamics of
objects would be expected to exhibit improved robustness and generalization in
novel scenarios. However, achieving this capability necessitates not only an
effective scene representation but also an understanding of the mechanisms
governing interactions among object subsets. Recent studies have made
significant progress in representing scenes using object slots. In this work,
we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models
object dynamics by leveraging communication among slots along with a modular
architecture capable of dynamically selecting reusable mechanisms for
predicting the future states of each object slot. Crucially, RSM leverages the
Central Contextual Information (CCI), enabling selected mechanisms to access
the remaining slots through a bottleneck, effectively allowing for modeling of
higher order and complex interactions that might require a sparse subset of
objects. Experimental results demonstrate the superior performance of RSM
compared to state-of-the-art methods across various future prediction and
related downstream tasks, including Visual Question Answering and action
planning. Furthermore, we showcase RSM's Out-of-Distribution generalization
ability to handle scenes in intricate scenarios.",None,-1
740d9031-1297-4b8a-9baf-a039aa842aad,Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems,0.748593,"In this article, a benchmark for real-world bin packing problems is proposed.
This dataset consists of 12 instances of varying levels of complexity regarding
size (with the number of packages ranging from 38 to 53) and user-defined
requirements. In fact, several real-world-oriented restrictions were taken into
account to build these instances: i) item and bin dimensions, ii) weight
restrictions, iii) affinities among package categories iv) preferences for
package ordering and v) load balancing. Besides the data, we also offer an own
developed Python script for the dataset generation, coined Q4RealBPP-DataGen.
The benchmark was initially proposed to evaluate the performance of quantum
solvers. Therefore, the characteristics of this set of instances were designed
according to the current limitations of quantum devices. Additionally, the
dataset generator is included to allow the construction of general-purpose
benchmarks. The data introduced in this article provides a baseline that will
encourage quantum computing researchers to work on real-world bin packing
problems.",None,-1
c876267a-6c45-48ab-a794-7d23dd8c0fda,An Empirical Comparison of LM-based Question and Answer Generation Methods,0.51331,"Question and answer generation (QAG) consists of generating a set of
question-answer pairs given a context (e.g. a paragraph). This task has a
variety of applications, such as data augmentation for question answering (QA)
models, information retrieval and education. In this paper, we establish
baselines with three different QAG methodologies that leverage
sequence-to-sequence language model (LM) fine-tuning. Experiments show that an
end-to-end QAG model, which is computationally light at both training and
inference times, is generally robust and outperforms other more convoluted
approaches. However, there are differences depending on the underlying
generative LM. Finally, our analysis shows that QA models fine-tuned solely on
generated question-answer pairs can be competitive when compared to supervised
QA models trained on human-labeled data.",None,-1
bfb9e354-2ee9-4ee7-b4c6-7952daae9da9,Hijacking Context in Large Multi-modal Models,0.244737,"Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to
understand the visual contents of images given the instructions regarding the
images. Built upon the Large Language Models (LLMs), LMMs also inherit their
abilities and characteristics such as in-context learning where a coherent
sequence of images and texts are given as the input prompt. However, we
identify a new limitation of off-the-shelf LMMs where a small fraction of
incoherent images or text descriptions mislead LMMs to only generate biased
output about the hijacked context, not the originally intended context. To
address this, we propose a pre-filtering method that removes irrelevant
contexts via GPT-4V, based on its robustness towards distribution shift within
the contexts. We further investigate whether replacing the hijacked visual and
textual contexts with the correlated ones via GPT-4V and text-to-image models
can help yield coherent responses.",None,-1
1d9fe077-4ccd-4bbc-8b51-5113854ebdbe,Revisiting Supertagging for HPSG,0.864665,"We present new supertaggers trained on HPSG-based treebanks. These treebanks
feature high-quality annotation based on a well-developed linguistic theory and
include diverse and challenging test datasets, beyond the usual WSJ section 23
and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based
models. We use SVM and neural CRF- and BERT-based methods and show that both
SVM and neural supertaggers achieve considerably higher accuracy compared to
the baseline. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 1000
sentences from WSJ23 and 93.88% on the completely out-of-domain The Cathedral
and the Bazaar (cb)). We conclude that it therefore makes sense to integrate
these new supertaggers into modern HPSG parsers, and we also hope that the
diverse and difficult datasets we used here will gain more popularity in the
field. We contribute the complete dataset reformatted for token classification.",None,-1
c0ef4644-350b-4e69-b2fc-8916169a4732,Aligning Language Models with Offline Learning from Human Feedback,0.0226404,"Learning from human preferences is crucial for language models (LMs) to
effectively cater to human needs and societal values. Previous research has
made notable progress by leveraging human feedback to follow instructions.
However, these approaches rely primarily on online learning techniques like
Proximal Policy Optimization (PPO), which have been proven unstable and
challenging to tune for language models. Moreover, PPO requires complex
distributed system implementation, hindering the efficiency of large-scale
distributed training. In this study, we propose an offline learning from human
feedback framework to align LMs without interacting with environments.
Specifically, we explore filtering alignment (FA), reward-weighted regression
(RWR), and conditional alignment (CA) to align language models to human
preferences. By employing a loss function similar to supervised fine-tuning,
our methods ensure more stable model training than PPO with a simple machine
learning system~(MLSys) and much fewer (around 9\%) computing resources.
Experimental results demonstrate that conditional alignment outperforms other
offline alignment methods and is comparable to PPO.",None,-1
c6fabe74-8a51-4270-addc-ebe0177629aa,Behavioral Cloning via Search in Embedded Demonstration Dataset,0.117969,"Behavioural cloning uses a dataset of demonstrations to learn a behavioural
policy. To overcome various learning and policy adaptation problems, we propose
to use latent space to index a demonstration dataset, instantly access similar
relevant experiences, and copy behavior from these situations. Actions from a
selected similar situation can be performed by the agent until representations
of the agent's current situation and the selected experience diverge in the
latent space. Thus, we formulate our control problem as a search problem over a
dataset of experts' demonstrations. We test our approach on BASALT
MineRL-dataset in the latent representation of a Video PreTraining model. We
compare our model to state-of-the-art Minecraft agents. Our approach can
effectively recover meaningful demonstrations and show human-like behavior of
an agent in the Minecraft environment in a wide variety of scenarios.
Experimental results reveal that performance of our search-based approach is
comparable to trained models, while allowing zero-shot task adaptation by
changing the demonstration examples.",None,-1
68c25482-f5b5-4437-9084-3a73e1720ae3,Improving Multitask Retrieval by Promoting Task Specialization,0.0473978,"In multitask retrieval, a single retriever is trained to retrieve relevant
contexts for multiple tasks. Despite its practical appeal, naive multitask
retrieval lags behind task-specific retrieval in which a separate retriever is
trained for each task. We show that it is possible to train a multitask
retriever that outperforms task-specific retrievers by promoting task
specialization. The main ingredients are: (1) a better choice of pretrained
model (one that is explicitly optimized for multitasking) along with compatible
prompting, and (2) a novel adaptive learning method that encourages each
parameter to specialize in a particular task. The resulting multitask retriever
is highly performant on the KILT benchmark. Upon analysis, we find that the
model indeed learns parameters that are more task-specialized compared to naive
multitasking without prompting or adaptive learning.",None,-1
c0f10a94-9f4a-49a1-91a8-d253079867e5,Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting,0.351132,"Detailed 3D reconstruction and photo-realistic relighting of digital humans
are essential for various applications. To this end, we propose a novel
sparse-view 3d human reconstruction framework that closely incorporates the
occupancy field and albedo field with an additional visibility field--it not
only resolves occlusion ambiguity in multiview feature aggregation, but can
also be used to evaluate light attenuation for self-shadowed relighting. To
enhance its training viability and efficiency, we discretize visibility onto a
fixed set of sample directions and supply it with coupled geometric 3D depth
feature and local 2D image feature. We further propose a novel
rendering-inspired loss, namely TransferLoss, to implicitly enforce the
alignment between visibility and occupancy field, enabling end-to-end joint
training. Results and extensive experiments demonstrate the effectiveness of
the proposed method, as it surpasses state-of-the-art in terms of
reconstruction accuracy while achieving comparably accurate relighting to
ray-traced ground truth.",None,-1
a4cb1b41-7c0e-4ffe-b635-8abbbcb43837,Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition,0.974076,"We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.",None,-1
f82c55e7-d68b-4699-a997-617ea917b165,High-Throughput and Accurate 3D Scanning of Cattle Using Time-of-Flight Sensors and Deep Learning,0.21116,"We introduce a high throughput 3D scanning solution specifically designed to
precisely measure cattle phenotypes. This scanner leverages an array of depth
sensors, i.e. time-of-flight (Tof) sensors, each governed by dedicated embedded
devices. The system excels at generating high-fidelity 3D point clouds, thus
facilitating an accurate mesh that faithfully reconstructs the cattle geometry
on the fly. In order to evaluate the performance of our system, we have
implemented a two-fold validation process. Initially, we test the scanner's
competency in determining volume and surface area measurements within a
controlled environment featuring known objects. Secondly, we explore the impact
and necessity of multi-device synchronization when operating a series of
time-of-flight sensors. Based on the experimental results, the proposed system
is capable of producing high-quality meshes of untamed cattle for livestock
studies.",None,-1
93fdc74c-4b6b-49b0-ade0-cce65a7a5e65,Zero-shot Referring Image Segmentation with Global-Local Context Features,0.695557,"Referring image segmentation (RIS) aims to find a segmentation mask given a
referring expression grounded to a region of the input image. Collecting
labelled datasets for this task, however, is notoriously costly and
labor-intensive. To overcome this issue, we propose a simple yet effective
zero-shot referring image segmentation method by leveraging the pre-trained
cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded
to the input text, we propose a mask-guided visual encoder that captures global
and local contextual information of an input image. By utilizing instance masks
obtained from off-the-shelf mask proposal techniques, our method is able to
segment fine-detailed Istance-level groundings. We also introduce a
global-local text encoder where the global feature captures complex
sentence-level semantics of the entire input expression while the local feature
focuses on the target noun phrase extracted by a dependency parser. In our
experiments, the proposed method outperforms several zero-shot baselines of the
task and even the weakly supervised referring expression segmentation method
with substantial margins. Our code is available at
https://github.com/Seonghoon-Yu/Zero-shot-RIS.",None,-1
2d5d7de8-ce1c-4d1a-a1fd-8968184f8816,ActionPrompt: Action-Guided 3D Human Pose Estimation With Text and Pose Prompting,0.282332,"Recent 2D-to-3D human pose estimation (HPE) utilizes temporal consistency
across sequences to alleviate the depth ambiguity problem but ignore the action
related prior knowledge hidden in the pose sequence. In this paper, we propose
a plug-and-play module named Action Prompt Module (APM) that effectively mines
different kinds of action clues for 3D HPE. The highlight is that, the mining
scheme of APM can be widely adapted to different frameworks and bring
consistent benefits. Specifically, we first present a novel Action-related Text
Prompt module (ATP) that directly embeds action labels and transfers the rich
language information in the label to the pose sequence. Besides, we further
introduce Action-specific Pose Prompt module (APP) to mine the position-aware
pose pattern of each action, and exploit the correlation between the mined
patterns and input pose sequence for further pose refinement. Experiments show
that APM can improve the performance of most video-based 2D-to-3D HPE
frameworks by a large margin.",None,-1
06df866c-28cc-4ae2-81e1-4eada1a50162,Enhancing Large Language Model with Self-Controlled Memory Framework,0.0772392,"Large Language Models (LLMs) are constrained by their inability to process
lengthy inputs, resulting in the loss of critical historical information. To
address this limitation, in this paper, we propose the Self-Controlled Memory
(SCM) framework to enhance the ability of LLMs to maintain long-term memory and
recall relevant information. Our SCM framework comprises three key components:
an LLM-based agent serving as the backbone of the framework, a memory stream
storing agent memories, and a memory controller updating memories and
determining when and how to utilize memories from memory stream. Additionally,
the proposed SCM is able to process ultra-long texts without any modification
or fine-tuning, which can integrate with any instruction following LLMs in a
plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the
effectiveness of SCM for handling lengthy inputs. The annotated dataset covers
three tasks: long-term dialogues, book summarization, and meeting
summarization. Experimental results demonstrate that our method achieves better
retrieval recall and generates more informative responses compared to
competitive baselines in long-term dialogues.
(https://github.com/wbbeyourself/SCM4LLMs)",None,-1
4b22f579-d633-40e8-86fc-ccf2a9c07329,GPT is becoming a Turing machine: Here are some ways to program it,0.3861,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",None,-1
d89fe4ac-a9b8-400c-a605-36a10e6c9f31,Unsupervised Out-of-Distribution Detection with Diffusion Inpainting,0.915165,"Unsupervised out-of-distribution detection (OOD) seeks to identify
out-of-domain data by learning only from unlabeled in-domain data. We present a
novel approach for this task - Lift, Map, Detect (LMD) - that leverages recent
advancement in diffusion models. Diffusion models are one type of generative
models. At their core, they learn an iterative denoising process that gradually
maps a noisy image closer to their training manifolds. LMD leverages this
intuition for OOD detection. Specifically, LMD lifts an image off its original
manifold by corrupting it, and maps it towards the in-domain manifold with a
diffusion model. For an out-of-domain image, the mapped image would have a
large distance away from its original manifold, and LMD would identify it as
OOD accordingly. We show through extensive experiments that LMD achieves
competitive performance across a broad variety of datasets. Code can be found
at https://github.com/zhenzhel/lift_map_detect.",None,-1
16bcd429-0b40-4c0c-a4fa-db16fc49b359,On the Importance of Noise Scheduling for Diffusion Models,0.69145,"We empirically study the effect of noise scheduling strategies for denoising
diffusion generative models. There are three findings: (1) the noise scheduling
is crucial for the performance, and the optimal one depends on the task (e.g.,
image sizes), (2) when increasing the image size, the optimal noise scheduling
shifts towards a noisier one (due to increased redundancy in pixels), and (3)
simply scaling the input data by a factor of $b$ while keeping the noise
schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a
good strategy across image sizes. This simple recipe, when combined with
recently proposed Recurrent Interface Network (RIN), yields state-of-the-art
pixel-based diffusion models for high-resolution images on ImageNet, enabling
single-stage, end-to-end generation of diverse and high-fidelity images at
1024$\times$1024 resolution (without upsampling/cascades).",None,-1
b034f7f9-5793-47d7-8ae7-15990bd4ec2e,Instant Continual Learning of Neural Radiance Fields,0.349591,"Neural radiance fields (NeRFs) have emerged as an effective method for
novel-view synthesis and 3D scene reconstruction. However, conventional
training methods require access to all training views during scene
optimization. This assumption may be prohibitive in continual learning
scenarios, where new data is acquired in a sequential manner and a continuous
update of the NeRF is desired, as in automotive or remote sensing applications.
When naively trained in such a continual setting, traditional scene
representation frameworks suffer from catastrophic forgetting, where previously
learned knowledge is corrupted after training on new data. Prior works in
alleviating forgetting with NeRFs suffer from low reconstruction quality and
high latency, making them impractical for real-world application. We propose a
continual learning framework for training NeRFs that leverages replay-based
methods combined with a hybrid explicit--implicit scene representation. Our
method outperforms previous methods in reconstruction quality when trained in a
continual setting, while having the additional benefit of being an order of
magnitude faster.",None,-1
7ebd676f-7359-498c-b348-0b4f152a6a89,Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction,0.686121,"Knowledge graph construction (KGC) is a multifaceted undertaking involving
the extraction of entities, relations, and events. Traditionally, large
language models (LLMs) have been viewed as solitary task-solving agents in this
complex landscape. However, this paper challenges this paradigm by introducing
a novel framework, CooperKGC. Departing from the conventional approach,
CooperKGC establishes a collaborative processing network, assembling a KGC
collaboration team capable of concurrently addressing entity, relation, and
event extraction tasks. Our experiments unequivocally demonstrate that
fostering collaboration and information interaction among diverse agents within
CooperKGC yields superior results compared to individual cognitive processes
operating in isolation. Importantly, our findings reveal that the collaboration
facilitated by CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.",None,-1
ed833205-0386-4f2f-86e2-d16232a6f192,The AI Revolution: Opportunities and Challenges for the Finance Sector,0.60828,"This report examines Artificial Intelligence (AI) in the financial sector,
outlining its potential to revolutionise the industry and identify its
challenges. It underscores the criticality of a well-rounded understanding of
AI, its capabilities, and its implications to effectively leverage its
potential while mitigating associated risks. The potential of AI potential
extends from augmenting existing operations to paving the way for novel
applications in the finance sector. The application of AI in the financial
sector is transforming the industry. Its use spans areas from customer service
enhancements, fraud detection, and risk management to credit assessments and
high-frequency trading. However, along with these benefits, AI also presents
several challenges. These include issues related to transparency,
interpretability, fairness, accountability, and trustworthiness. The use of AI
in the financial sector further raises critical questions about data privacy
and security. A further issue identified in this report is the systemic risk
that AI can introduce to the financial sector. Being prone to errors, AI can
exacerbate existing systemic risks, potentially leading to financial crises.
Regulation is crucial to harnessing the benefits of AI while mitigating its
potential risks. Despite the global recognition of this need, there remains a
lack of clear guidelines or legislation for AI use in finance. This report
discusses key principles that could guide the formation of effective AI
regulation in the financial sector, including the need for a risk-based
approach, the inclusion of ethical considerations, and the importance of
maintaining a balance between innovation and consumer protection. The report
provides recommendations for academia, the finance industry, and regulators.",None,-1
12597243-5fde-4188-82b9-8b116d51eb66,Design of JiuTian Intelligent Network Simulation Platform,0.517415,"This paper introduced the JiuTian Intelligent Network Simulation Platform,
which can provide wireless communication simulation data services for the Open
Innovation Platform. The platform contains a series of scalable simulator
functionalities, offering open services that enable users to use reinforcement
learning algorithms for model training and inference based on simulation
environments and data. Additionally, it allows users to address optimization
tasks in different scenarios by uploading and updating parameter
configurations. The platform and its open services were primarily introduced
from the perspectives of background, overall architecture, simulator, business
scenarios, and future directions.",None,-1
a5c0c8bf-93a8-4416-b1ee-371e8413b847,View Consistent Purification for Accurate Cross-View Localization,0.270002,"This paper proposes a fine-grained self-localization method for outdoor
robotics that utilizes a flexible number of onboard cameras and readily
accessible satellite images. The proposed method addresses limitations in
existing cross-view localization methods that struggle to handle noise sources
such as moving objects and seasonal variations. It is the first sparse
visual-only method that enhances perception in dynamic environments by
detecting view-consistent key points and their corresponding deep features from
ground and satellite views, while removing off-the-ground objects and
establishing homography transformation between the two views. Moreover, the
proposed method incorporates a spatial embedding approach that leverages camera
intrinsic and extrinsic information to reduce the ambiguity of purely visual
matching, leading to improved feature matching and overall pose estimation
accuracy. The method exhibits strong generalization and is robust to
environmental changes, requiring only geo-poses as ground truth. Extensive
experiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate that
our proposed method outperforms existing state-of-the-art methods, achieving
median spatial accuracy errors below $0.5$ meters along the lateral and
longitudinal directions, and a median orientation accuracy error below 2
degrees.",None,-1
38629e6a-15de-49be-8945-0f290bd7de48,Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification,0.98365,"Long-Term Person Re-Identification (LT-ReID) has become increasingly crucial
in computer vision and biometrics. In this work, we aim to extend LT-ReID
beyond pedestrian recognition to include a wider range of real-world human
activities while still accounting for cloth-changing scenarios over large time
gaps. This setting poses additional challenges due to the geometric
misalignment and appearance ambiguity caused by the diversity of human pose and
clothing. To address these challenges, we propose a new approach 3DInvarReID
for (i) disentangling identity from non-identity components (pose, clothing
shape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3D
clothed body shapes and learning discriminative features of naked body shapes
for person ReID in a joint manner. To better evaluate our study of LT-ReID, we
collect a real-world dataset called CCDA, which contains a wide variety of
human activities and clothing changes. Experimentally, we show the superior
performance of our approach for person ReID.",None,-1
4971d49d-bdd0-4f11-a581-f09db2c7d3de,Instruction Position Matters in Sequence Generation with Large Language Models,0.438551,"Large language models (LLMs) are capable of performing conditional sequence
generation tasks, such as translation or summarization, through instruction
fine-tuning. The fine-tuning data is generally sequentially concatenated from a
specific task instruction, an input sentence, and the corresponding response.
Considering the locality modeled by the self-attention mechanism of LLMs, these
models face the risk of instruction forgetting when generating responses for
long input sentences. To mitigate this issue, we propose enhancing the
instruction-following capability of LLMs by shifting the position of task
instructions after the input sentences. Theoretical analysis suggests that our
straightforward method can alter the model's learning focus, thereby
emphasizing the training of instruction-following capabilities. Concurrently,
experimental results demonstrate that our approach consistently outperforms
traditional settings across various model scales (1B / 7B / 13B) and different
sequence generation tasks (translation and summarization), without any
additional data or annotation costs. Notably, our method significantly improves
the zero-shot performance on conditional sequence generation, e.g., up to 9.7
BLEU points on WMT zero-shot translation tasks.",None,-1
2c772aa5-ed26-41e4-98b0-3f288f42a22f,Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation,0.742214,"Conversational Recommendation System (CRS) is a rapidly growing research area
that has gained significant attention alongside advancements in language
modelling techniques. However, the current state of conversational
recommendation faces numerous challenges due to its relative novelty and
limited existing contributions. In this study, we delve into benchmark datasets
for developing CRS models and address potential biases arising from the
feedback loop inherent in multi-turn interactions, including selection bias and
multiple popularity bias variants. Drawing inspiration from the success of
generative data via using language models and data augmentation techniques, we
present two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model
performance while mitigating biases. Through extensive experiments on ReDial
and TG-ReDial benchmark datasets, we show a consistent improvement of CRS
techniques with our data augmentation approaches and offer additional insights
on addressing multiple newly formulated biases.",None,-1
c0b5a234-071f-4710-b8f6-66d29f475ecf,RLLTE: Long-Term Evolution Project of Reinforcement Learning,0.0407192,"We present RLLTE: a long-term evolution, extremely modular, and open-source
framework for reinforcement learning (RL) research and application. Beyond
delivering top-notch algorithm implementations, RLLTE also serves as a toolkit
for developing algorithms. More specifically, RLLTE decouples the RL algorithms
completely from the exploitation-exploration perspective, providing a large
number of components to accelerate algorithm development and evolution. In
particular, RLLTE is the first RL framework to build a complete and luxuriant
ecosystem, which includes model training, evaluation, deployment, benchmark
hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set
standards for RL engineering practice and be highly stimulative for industry
and academia.",None,-1
193c6f97-2509-4a75-b689-9fb77c107347,Agent Lumos: Unified and Modular Training for Open-Source Language Agents,0.142137,"Closed-source agents suffer from several issues such as a lack of
affordability, transparency, and reproducibility, particularly on complex
interactive tasks. This motivates the development of open-source alternatives.
We introduce LUMOS, one of the first frameworks for training open-source
LLM-based agents. LUMOS features a learnable, unified, and modular architecture
with a planning module that learns high-level subgoal generation, and a
grounding module trained to translate these into actions using various tools in
the execution module. The design allows for modular upgrades and wider
applicability to diverse interactive tasks. To foster generalizable agent
learning, we collect large-scale, unified, and high-quality training
annotations derived from diverse ground-truth reasoning rationales across
various complex interactive tasks. On 9 datasets, LUMOS exhibits several key
advantages: (1) LUMOS excels multiple larger open-source agents on the held-out
datasets (unused for training) for each task type. LUMOS even surpasses GPT
agents on QA and web tasks; (2) LUMOS outperforms open-source agents produced
by chain-of-thoughts and unmodularized integrated training; and (3) LUMOS
effectively generalizes to unseen tasks, outperforming 33B-scale agents and
domain-specific agents.",None,-1
ae05bf0a-2c42-4c81-9377-1e165a77a9c8,Cultivated Wildness: Technodiversity and Wildness in Machines,0.329466,"This paper investigates the idea of cultivated wildness at the intersection
of landscape design and artificial intelligence. The paper posits that
contemporary landscape practices should overcome the potentially single
understanding on wilderness, and instead explore landscape strategies to
cultivate new forms of wild places via ideas and concerns in contemporary
Environmental Humanities, Science and Technology Studies, Ecological Sciences,
and Landscape Architecture. Drawing cases in environmental engineering,
computer science, and landscape architecture research, this paper explores a
framework to construct wild places with intelligent machines. In this
framework, machines are not understood as a layer of ""digital infrastructure""
that is used to extend localized human intelligence and agency. Rather machines
are conceptualized as active agents who can participate in the intelligence of
co-production. Recent developments in cybernetic technologies such as sensing
networks, artificial intelligence, and cyberphysical systems can also
contribute to establishing the framework. At the heart of this framework is
""technodiversity,"" in parallel with biodiversity, since a singular vision on
technological development driven by optimization and efficiency reinforces a
monocultural approach that eliminates other possible relationships to construct
with the environment. Thus, cultivated wildness is also about recognizing
""wildness"" in machines.",None,-1
7f6a27ac-c5b8-4831-86ea-c45fbda19a3c,Aligning Bag of Regions for Open-Vocabulary Object Detection,0.856991,"Pre-trained vision-language models (VLMs) learn to align vision and language
representations on large-scale datasets, where each image-text pair usually
contains a bag of semantic concepts. However, existing open-vocabulary object
detectors only align region embeddings individually with the corresponding
features extracted from the VLMs. Such a design leaves the compositional
structure of semantic concepts in a scene under-exploited, although the
structure may be implicitly learned by the VLMs. In this work, we propose to
align the embedding of bag of regions beyond individual regions. The proposed
method groups contextually interrelated regions as a bag. The embeddings of
regions in a bag are treated as embeddings of words in a sentence, and they are
sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which
is learned to be aligned to the corresponding features extracted by a frozen
VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the
previous best results by 4.6 box AP50 and 2.8 mask AP on novel categories of
open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are
available at https://github.com/wusize/ovdet.",None,-1
09dbd41b-cfd1-476b-aa02-f12f1dbf60e8,Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation,0.510616,"New Natural Langauge Process~(NLP) benchmarks are urgently needed to align
with the rapid development of large language models (LLMs). We present Xiezhi,
the most comprehensive evaluation suite designed to assess holistic domain
knowledge. Xiezhi comprises multiple-choice questions across 516 diverse
disciplines ranging from 13 different subjects with 249,587 questions and
accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k
questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results
indicate that LLMs exceed average performance of humans in science,
engineering, agronomy, medicine, and art, but fall short in economics,
jurisprudence, pedagogy, literature, history, and management. We anticipate
Xiezhi will help analyze important strengths and shortcomings of LLMs, and the
benchmark is released in~\url{https://github.com/MikeGu721/XiezhiBenchmark}.",None,-1
50c57b0f-297d-4a21-99fc-b8684a01cf6b,Weakly-supervised Representation Learning for Video Alignment and Analysis,0.105493,"Many tasks in video analysis and understanding boil down to the need for
frame-based feature learning, aiming to encapsulate the relevant visual content
so as to enable simpler and easier subsequent processing. While supervised
strategies for this learning task can be envisioned, self and weakly-supervised
alternatives are preferred due to the difficulties in getting labeled data.
This paper introduces LRProp -- a novel weakly-supervised representation
learning approach, with an emphasis on the application of temporal alignment
between pairs of videos of the same action category. The proposed approach uses
a transformer encoder for extracting frame-level features, and employs the DTW
algorithm within the training iterations in order to identify the alignment
path between video pairs. Through a process referred to as ``pair-wise position
propagation'', the probability distributions of these correspondences per
location are matched with the similarity of the frame-level features via
KL-divergence minimization. The proposed algorithm uses also a regularized
SoftDTW loss for better tuning the learned features. Our novel representation
learning paradigm consistently outperforms the state of the art on temporal
alignment tasks, establishing a new performance bar over several downstream
video analysis applications.",None,-1
355b1df0-1bcc-44ed-b221-62caaee5f813,FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead,1.0,"We present FengWu, an advanced data-driven global medium-range weather
forecast system based on Artificial Intelligence (AI). Different from existing
data-driven weather forecast methods, FengWu solves the medium-range forecast
problem from a multi-modal and multi-task perspective. Specifically, a deep
learning architecture equipped with model-specific encoder-decoders and
cross-modal fusion Transformer is elaborately designed, which is learned under
the supervision of an uncertainty loss to balance the optimization of different
predictors in a region-adaptive manner. Besides this, a replay buffer mechanism
is introduced to improve medium-range forecast performance. With 39-year data
training based on the ERA5 reanalysis, FengWu is able to accurately reproduce
the atmospheric dynamics and predict the future land and atmosphere states at
37 vertical levels on a 0.25{\deg} latitude-longitude resolution. Hindcasts of
6-hourly weather in 2018 based on ERA5 demonstrate that FengWu performs better
than GraphCast in predicting 80\% of the 880 reported predictands, e.g.,
reducing the root mean square error (RMSE) of 10-day lead global z500
prediction from 733 to 651 $m^{2}/s^2$. In addition, the inference cost of each
iteration is merely 600ms on NVIDIA Tesla A100 hardware. The results suggest
that FengWu can significantly improve the forecast skill and extend the
skillful global medium-range weather forecast out to 10.75 days lead (with ACC
of z500 > 0.6) for the first time.",None,-1
0ae98e27-03ad-471e-b27d-2bc5030d4b22,A Schedule of Duties in the Cloud Space Using a Modified Salp Swarm Algorithm,0.223362,"Cloud computing is a concept introduced in the information technology era,
with the main components being the grid, distributed, and valuable computing.
The cloud is being developed continuously and, naturally, comes up with many
challenges, one of which is scheduling. A schedule or timeline is a mechanism
used to optimize the time for performing a duty or set of duties. A scheduling
process is accountable for choosing the best resources for performing a duty.
The main goal of a scheduling algorithm is to improve the efficiency and
quality of the service while at the same time ensuring the acceptability and
effectiveness of the targets. The task scheduling problem is one of the most
important NP-hard issues in the cloud domain and, so far, many techniques have
been proposed as solutions, including using genetic algorithms (GAs), particle
swarm optimization, (PSO), and ant colony optimization (ACO). To address this
problem, in this paper, one of the collective intelligence algorithms, called
the Salp Swarm Algorithm (SSA), has been expanded, improved, and applied. The
performance of the proposed algorithm has been compared with that of GAs, PSO,
continuous ACO, and the basic SSA. The results show that our algorithm has
generally higher performance than the other algorithms. For example, compared
to the basic SSA, the proposed method has an average reduction of approximately
21% in makespan.",None,-1
fab0efc8-dced-4620-838f-4ce5faeeac31,Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery,0.496704,"Positive-unlabeled learning (PU learning) in hyperspectral remote sensing
imagery (HSI) is aimed at learning a binary classifier from positive and
unlabeled data, which has broad prospects in various earth vision applications.
However, when PU learning meets limited labeled HSI, the unlabeled data may
dominate the optimization process, which makes the neural networks overfit the
unlabeled data. In this paper, a Taylor variational loss is proposed for HSI PU
learning, which reduces the weight of the gradient of the unlabeled data by
Taylor series expansion to enable the network to find a balance between
overfitting and underfitting. In addition, the self-calibrated optimization
strategy is designed to stabilize the training process. Experiments on 7
benchmark datasets (21 tasks in total) validate the effectiveness of the
proposed method. Code is at: https://github.com/Hengwei-Zhao96/T-HOneCls.",None,-1
7c443f7f-c864-4770-96ca-f6763bc4accd,Edit Everything: A Text-Guided Generative System for Images Editing,0.603802,"We introduce a new generative system called Edit Everything, which can take
image and text inputs and produce image outputs. Edit Everything allows users
to edit images using simple text instructions. Our system designs prompts to
guide the visual module in generating requested images. Experiments demonstrate
that Edit Everything facilitates the implementation of the visual aspects of
Stable Diffusion with the use of Segment Anything model and CLIP. Our system is
publicly available at https://github.com/DefengXie/Edit_Everything.",None,-1
9a01329e-2619-4c48-84dc-3ddf5946d593,Cumulative Spatial Knowledge Distillation for Vision Transformers,0.410412,"Distilling knowledge from convolutional neural networks (CNNs) is a
double-edged sword for vision transformers (ViTs). It boosts the performance
since the image-friendly local-inductive bias of CNN helps ViT learn faster and
better, but leading to two problems: (1) Network designs of CNN and ViT are
completely different, which leads to different semantic levels of intermediate
features, making spatial-wise knowledge transfer methods (e.g., feature
mimicking) inefficient. (2) Distilling knowledge from CNN limits the network
convergence in the later training period since ViT's capability of integrating
global information is suppressed by CNN's local-inductive-bias supervision. To
this end, we present Cumulative Spatial Knowledge Distillation (CSKD). CSKD
distills spatial-wise knowledge to all patch tokens of ViT from the
corresponding spatial responses of CNN, without introducing intermediate
features. Furthermore, CSKD exploits a Cumulative Knowledge Fusion (CKF)
module, which introduces the global response of CNN and increasingly emphasizes
its importance during the training. Applying CKF leverages CNN's local
inductive bias in the early training period and gives full play to ViT's global
capability in the later one. Extensive experiments and analysis on ImageNet-1k
and downstream datasets demonstrate the superiority of our CSKD. Code will be
publicly available.",None,-1
0ab51909-5db7-425d-83a6-d518bffe86c7,"Transcending the ""Male Code"": Implicit Masculine Biases in NLP Contexts",0.523526,"Critical scholarship has elevated the problem of gender bias in data sets
used to train virtual assistants (VAs). Most work has focused on explicit
biases in language, especially against women, girls, femme-identifying people,
and genderqueer folk; implicit associations through word embeddings; and
limited models of gender and masculinities, especially toxic masculinities,
conflation of sex and gender, and a sex/gender binary framing of the masculine
as diametric to the feminine. Yet, we must also interrogate how masculinities
are ""coded"" into language and the assumption of ""male"" as the linguistic
default: implicit masculine biases. To this end, we examined two natural
language processing (NLP) data sets. We found that when gendered language was
present, so were gender biases and especially masculine biases. Moreover, these
biases related in nuanced ways to the NLP context. We offer a new dictionary
called AVA that covers ambiguous associations between gendered language and the
language of VAs.",None,-1
b7558925-ce2d-4674-aa21-6e25b5d15350,LEST: Large-scale LiDAR Semantic Segmentation with Transformer,0.0846284,"Large-scale LiDAR-based point cloud semantic segmentation is a critical task
in autonomous driving perception. Almost all of the previous state-of-the-art
LiDAR semantic segmentation methods are variants of sparse 3D convolution.
Although the Transformer architecture is becoming popular in the field of
natural language processing and 2D computer vision, its application to
large-scale point cloud semantic segmentation is still limited. In this paper,
we propose a LiDAR sEmantic Segmentation architecture with pure Transformer,
LEST. LEST comprises two novel components: a Space Filling Curve (SFC) Grouping
strategy and a Distance-based Cosine Linear Transformer, DISCO. On the public
nuScenes semantic segmentation validation set and SemanticKITTI test set, our
model outperforms all the other state-of-the-art methods.",None,-1
64fba723-e0fe-4224-8380-3f69e295c364,MATIS: Masked-Attention Transformers for Surgical Instrument Segmentation,0.92234,"We propose Masked-Attention Transformers for Surgical Instrument Segmentation
(MATIS), a two-stage, fully transformer-based method that leverages modern
pixel-wise attention mechanisms for instrument segmentation. MATIS exploits the
instance-level nature of the task by employing a masked attention module that
generates and classifies a set of fine instrument region proposals. Our method
incorporates long-term video-level information through video transformers to
improve temporal consistency and enhance mask classification. We validate our
approach in the two standard public benchmarks, Endovis 2017 and Endovis 2018.
Our experiments demonstrate that MATIS' per-frame baseline outperforms previous
state-of-the-art methods and that including our temporal consistency module
boosts our model's performance further.",None,-1
a328c93e-fd49-474f-9594-f05885fdf362,She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models,0.968864,"Implicit gender bias in software development is a well-documented issue, such
as the association of technical roles with men. To address this bias, it is
important to understand it in more detail. This study uses data mining
techniques to investigate the extent to which 56 tasks related to software
development, such as assigning GitHub issues and testing, are affected by
implicit gender bias embedded in large language models. We systematically
translated each task from English into a genderless language and back, and
investigated the pronouns associated with each task. Based on translating each
task 100 times in different permutations, we identify a significant disparity
in the gendered pronoun associations with different tasks. Specifically,
requirements elicitation was associated with the pronoun ""he"" in only 6% of
cases, while testing was associated with ""he"" in 100% of cases. Additionally,
tasks related to helping others had a 91% association with ""he"" while the same
association for tasks related to asking coworkers was only 52%. These findings
reveal a clear pattern of gender bias related to software development tasks and
have important implications for addressing this issue both in the training of
large language models and in broader society.",None,-1
2be0d596-faf1-44d6-994d-990d991d0b3b,Back Translation for Speech-to-text Translation Without Transcripts,0.896315,"The success of end-to-end speech-to-text translation (ST) is often achieved
by utilizing source transcripts, e.g., by pre-training with automatic speech
recognition (ASR) and machine translation (MT) tasks, or by introducing
additional ASR and MT data. Unfortunately, transcripts are only sometimes
available since numerous unwritten languages exist worldwide. In this paper, we
aim to utilize large amounts of target-side monolingual data to enhance ST
without transcripts. Motivated by the remarkable success of back translation in
MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo
ST data from monolingual target data. To ease the challenges posed by
short-to-long generation and one-to-many mapping, we introduce self-supervised
discrete units and achieve back translation by cascading a target-to-unit model
and a unit-to-speech model. With our synthetic ST data, we achieve an average
boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments
show that our method is especially effective in low-resource scenarios.",None,-1
a78b98b9-f242-47ef-92b3-bef4326c1f18,MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning,0.719917,"This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
Detection. We used a multi-label contrastive loss for fine-tuning large
pre-trained language models in a multi-lingual setting, achieving very
competitive results: our system was ranked first on the official test set and
on the official shared task leaderboard for five of the six languages for which
we had training data and for which we could perform fine-tuning. Here, we
describe our experimental setup, as well as various ablation studies. The code
of our system is available at https://github.com/QishengL/SemEval2023",None,-1
991c6476-8572-4a83-aafd-92cb7c7dbfa5,Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning,0.254425,"Neural MMO 2.0 is a massively multi-agent environment for reinforcement
learning research. The key feature of this new version is a flexible task
system that allows users to define a broad range of objectives and reward
signals. We challenge researchers to train agents capable of generalizing to
tasks, maps, and opponents never seen during training. Neural MMO features
procedurally generated maps with 128 agents in the standard setting and support
for up to. Version 2.0 is a complete rewrite of its predecessor with three-fold
improved performance and compatibility with CleanRL. We release the platform as
free and open-source software with comprehensive documentation available at
neuralmmo.github.io and an active community Discord. To spark initial research
on this new platform, we are concurrently running a competition at NeurIPS
2023.",None,-1
6be41e21-c646-47d5-bfb9-d0118f5c49ed,Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction,0.839892,"Accurate prediction of what types of patents that companies will apply for in
the next period of time can figure out their development strategies and help
them discover potential partners or competitors in advance. Although important,
this problem has been rarely studied in previous research due to the challenges
in modelling companies' continuously evolving preferences and capturing the
semantic correlations of classification codes. To fill in this gap, we propose
an event-based dynamic graph learning framework for patent application trend
prediction. In particular, our method is founded on the memorable
representations of both companies and patent classification codes. When a new
patent is observed, the representations of the related companies and
classification codes are updated according to the historical memories and the
currently encoded messages. Moreover, a hierarchical message passing mechanism
is provided to capture the semantic proximities of patent classification codes
by updating their representations along the hierarchical taxonomy. Finally, the
patent application trend is predicted by aggregating the representations of the
target company and classification codes from static, dynamic, and hierarchical
perspectives. Experiments on real-world data demonstrate the effectiveness of
our approach under various experimental conditions, and also reveal the
abilities of our method in learning semantics of classification codes and
tracking technology developing trajectories of companies.",None,-1
94c07cea-0d01-4b35-86c0-e1fb09666dcd,Mistral 7B,0.999375,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered
for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B
across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and
code generation. Our model leverages grouped-query attention (GQA) for faster
inference, coupled with sliding window attention (SWA) to effectively handle
sequences of arbitrary length with a reduced inference cost. We also provide a
model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses
the Llama 2 13B -- Chat model both on human and automated benchmarks. Our
models are released under the Apache 2.0 license.",None,-1
2c32e4c8-d4db-4036-b50d-7676f417a09b,Improving Joint Speech-Text Representations Without Alignment,0.434204,"The last year has seen astonishing progress in text-prompted image generation
premised on the idea of a cross-modal representation space in which the text
and image domains are represented jointly. In ASR, this idea has found
application as joint speech-text encoders that can scale to the capacities of
very large parameter models by being trained on both unpaired speech and text.
While these methods show promise, they have required special treatment of the
sequence-length mismatch inherent in speech and text, either by up-sampling
heuristics or an explicit alignment model. In this work, we offer evidence that
joint speech-text encoders naturally achieve consistent representations across
modalities by disregarding sequence length, and argue that consistency losses
could forgive length differences and simply assume the best alignment. We show
that such a loss improves downstream WER in both a large-parameter monolingual
and multilingual system.",None,-1
b83f21bb-e90f-45fe-b548-e7d055f45faf,Neural Refinement for Absolute Pose Regression with Feature Synthesis,0.42724,"Absolute Pose Regression (APR) methods use deep neural networks to directly
regress camera poses from RGB images. However, the predominant APR
architectures only rely on 2D operations during inference, resulting in limited
accuracy of pose estimation due to the lack of 3D geometry constraints or
priors. In this work, we propose a test-time refinement pipeline that leverages
implicit geometric constraints using a robust feature field to enhance the
ability of APR methods to use 3D information during inference. We also
introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D
geometric features during training and directly renders dense novel view
features at test time to refine APR methods. To enhance the robustness of our
model, we introduce a feature fusion module and a progressive training
strategy. Our proposed method achieves state-of-the-art single-image APR
accuracy on indoor and outdoor datasets.",None,-1
20a0c98c-1543-4fb1-a875-43dbe1ab0cf7,T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities,0.0643313,"Diffusion Probabilistic Field (DPF) models the distribution of continuous
functions defined over metric spaces. While DPF shows great potential for
unifying data generation of various modalities including images, videos, and 3D
geometry, it does not scale to a higher data resolution. This can be attributed
to the ``scaling property'', where it is difficult for the model to capture
local structures through uniform sampling. To this end, we propose a new model
comprising of a view-wise sampling algorithm to focus on local structure
learning, and incorporating additional guidance, e.g., text description, to
complement the global geometry. The model can be scaled to generate
high-resolution data while unifying multiple modalities. Experimental results
on data generation in various modalities demonstrate the effectiveness of our
model, as well as its potential as a foundation framework for scalable
modality-unified visual content generation.",None,-1
04e87ca6-1f9e-4c29-b641-8e2ded11f0ab,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,0.853552,"In this work, instead of directly predicting the pixel-level segmentation
masks, the problem of referring image segmentation is formulated as sequential
polygon generation, and the predicted polygons can be later converted into
segmentation masks. This is enabled by a new sequence-to-sequence framework,
Polygon Transformer (PolyFormer), which takes a sequence of image patches and
text query tokens as input, and outputs a sequence of polygon vertices
autoregressively. For more accurate geometric localization, we propose a
regression-based decoder, which predicts the precise floating-point coordinates
directly, without any coordinate quantization error. In the experiments,
PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52%
absolute improvements on the challenging RefCOCO+ and RefCOCOg datasets. It
also shows strong generalization ability when evaluated on the referring video
segmentation task without fine-tuning, e.g., achieving competitive 61.5% J&F on
the Ref-DAVIS17 dataset.",None,-1
47fa2231-4641-4816-be97-9a191e60088c,ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning,0.638877,"When personalized federated learning (FL) meets large foundation models, new
challenges arise from various limitations in resources. In addition to typical
limitations such as data, computation, and communication costs, access to the
models is also often limited. This paper endeavors to solve both the challenges
of limited resources and personalization. i.e., distribution shifts between
clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order
Optimization for Personalized Federated Learning. ZOOPFL avoids direct
interference with the foundation models and instead learns to adapt its inputs
through zeroth-order optimization. In addition, we employ simple yet effective
linear projections to remap its predictions for personalization. To reduce the
computation costs and enhance personalization, we propose input surgery to
incorporate an auto-encoder with low-dimensional and client-specific
embeddings. We provide theoretical support for ZOOPFL to analyze its
convergence. Extensive empirical experiments on computer vision and natural
language processing tasks using popular foundation models demonstrate its
effectiveness for FL on black-box foundation models.",None,-1
1c634899-5f95-4685-8cc2-763842d34053,Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,0.0318173,"The wide applicability and adaptability of generative large language models
(LLMs) has enabled their rapid adoption. While the pre-trained models can
perform many tasks, such models are often fine-tuned to improve their
performance on various downstream applications. However, this leads to issues
over violation of model licenses, model theft, and copyright infringement.
Moreover, recent advances show that generative technology is capable of
producing harmful content which exacerbates the problems of accountability
within model supply chains. Thus, we need a method to investigate how a model
was trained or a piece of text was generated and what their pre-trained base
model was. In this paper we take the first step to address this open problem by
tracing back the origin of a given fine-tuned LLM to its corresponding
pre-trained base model. We consider different knowledge levels and attribution
strategies, and find that we can correctly trace back 8 out of the 10 fine
tuned models with our best method.",None,-1
3030806c-03af-4e0a-8401-fe13cbd8ef35,BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection,0.971284,"While most recent autonomous driving system focuses on developing perception
methods on ego-vehicle sensors, people tend to overlook an alternative approach
to leverage intelligent roadside cameras to extend the perception ability
beyond the visual range. We discover that the state-of-the-art vision-centric
bird's eye view detection methods have inferior performances on roadside
cameras. This is because these methods mainly focus on recovering the depth
regarding the camera center, where the depth difference between the car and the
ground quickly shrinks while the distance increases. In this paper, we propose
a simple yet effective approach, dubbed BEVHeight, to address this issue. In
essence, instead of predicting the pixel-wise depth, we regress the height to
the ground to achieve a distance-agnostic formulation to ease the optimization
process of camera-only perception methods. On popular 3D detection benchmarks
of roadside cameras, our method surpasses all previous vision-centric methods
by a significant margin. The code is available at
{\url{https://github.com/ADLab-AutoDrive/BEVHeight}}.",None,-1
3ecb1b4c-464d-4598-b879-698f911163e2,"SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification",0.898299,"This paper describes our approach for SemEval-2023 Task 3: Detecting the
category, the framing, and the persuasion techniques in online news in a
multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of
fully trained and adapter mBERT models which was ranked joint-first for German,
and had the highest mean rank of multi-language teams. For Subtask 2 (Framing),
we achieved first place in 3 languages, and the best average rank across all
the languages, by using two separate ensembles: a monolingual
RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task
adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a
monolingual RoBERTa-Base model for English and a multilingual mBERT model for
the remaining languages, which achieved top 10 for all languages, including 2nd
for English. For each subtask, we compared monolingual and multilingual
approaches, and considered class imbalance techniques.",None,-1
2cbefb69-2f23-4021-b133-958c3969d940,Learning to Filter Context for Retrieval-Augmented Generation,0.981312,"On-the-fly retrieval of relevant knowledge has proven an essential element of
reliable systems for tasks such as open-domain question answering and fact
verification. However, because retrieval systems are not perfect, generation
models are required to generate outputs given partially or entirely irrelevant
passages. This can cause over- or under-reliance on context, and result in
problems in the generated output such as hallucinations. To alleviate these
problems, we propose FILCO, a method that improves the quality of the context
provided to the generator by (1) identifying useful context based on lexical
and information-theoretic approaches, and (2) training context filtering models
that can filter retrieved contexts at test time. We experiment on six
knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our
method outperforms existing approaches on extractive question answering (QA),
complex multi-hop and long-form QA, fact verification, and dialog generation
tasks. FILCO effectively improves the quality of context, whether or not it
supports the canonical output.",None,-1
26422591-6bc7-4590-8d9a-ff659581e572,Asymmetric double-winged multi-view clustering network for exploring Diverse and Consistent Information,0.0946049,"In unsupervised scenarios, deep contrastive multi-view clustering (DCMVC) is
becoming a hot research spot, which aims to mine the potential relationships
between different views. Most existing DCMVC algorithms focus on exploring the
consistency information for the deep semantic features, while ignoring the
diverse information on shallow features. To fill this gap, we propose a novel
multi-view clustering network termed CodingNet to explore the diverse and
consistent information simultaneously in this paper. Specifically, instead of
utilizing the conventional auto-encoder, we design an asymmetric structure
network to extract shallow and deep features separately. Then, by aligning the
similarity matrix on the shallow feature to the zero matrix, we ensure the
diversity for the shallow features, thus offering a better description of
multi-view data. Moreover, we propose a dual contrastive mechanism that
maintains consistency for deep features at both view-feature and pseudo-label
levels. Our framework's efficacy is validated through extensive experiments on
six widely used benchmark datasets, outperforming most state-of-the-art
multi-view clustering algorithms.",None,-1
1d8514b0-b1c8-44b4-9b46-1c7d06ae18a0,Unlearn What You Want to Forget: Efficient Unlearning for LLMs,0.901668,"Large language models (LLMs) have achieved significant progress from
pre-training on and memorizing a wide range of textual data, however, this
process might suffer from privacy issues and violations of data protection
regulations. As a result, the ability to easily remove data related to
individual users from such models while not deteriorating their predictive
quality after the removal becomes increasingly important. To address these
issues, in this work, we propose an efficient unlearning framework that could
efficiently update LLMs without having to retrain the whole model after data
removals, by introducing lightweight unlearning layers learned with a selective
teacher-student objective into the transformers. In addition, we introduce a
fusion mechanism to effectively combine different unlearning layers that learns
to forget different sets of data to handle a sequence of forgetting operations.
Experiments on classification and generation tasks demonstrate the
effectiveness of our proposed methods compared to the state-of-the-art
baselines.",None,-1
3b307d52-ff40-4b15-89b4-74d24f98c4ef,Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE,0.517372,"Knowledge graph embedding models (KGEMs) have gained considerable traction in
recent years. These models learn a vector representation of knowledge graph
entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning
versatile KGEs is desirable as it makes them useful for a broad range of tasks.
However, KGEMs are usually trained for a specific task, which makes their
embeddings task-dependent. In parallel, the widespread assumption that KGEMs
actually create a semantic representation of the underlying entities and
relations (e.g., project similar entities closer than dissimilar ones) has been
challenged. In this work, we design heuristics for generating protographs --
small, modified versions of a KG that leverage RDF/S information. The learnt
protograph-based embeddings are meant to encapsulate the semantics of a KG, and
can be leveraged in learning KGEs that, in turn, also better capture semantics.
Extensive experiments on various evaluation benchmarks demonstrate the
soundness of this approach, which we call Modular and Agnostic SCHema-based
Integration of protograph Embeddings (MASCHInE). In particular, MASCHInE helps
produce more versatile KGEs that yield substantially better performance for
entity clustering and node classification tasks. For link prediction, using
MASCHinE substantially increases the number of semantically valid predictions
with equivalent rank-based performance.",None,-1
5645309b-678c-4c36-ae6f-e2e0ebd8ab63,Data Augmentation Alone Can Improve Adversarial Training,0.916317,"Adversarial training suffers from the issue of robust overfitting, which
seriously impairs its generalization performance. Data augmentation, which is
effective at preventing overfitting in standard training, has been observed by
many previous works to be ineffective in mitigating overfitting in adversarial
training. This work proves that, contrary to previous findings, data
augmentation alone can significantly boost accuracy and robustness in
adversarial training. We find that the hardness and the diversity of data
augmentation are important factors in combating robust overfitting. In general,
diversity can improve both accuracy and robustness, while hardness can boost
robustness at the cost of accuracy within a certain limit and degrade them both
over that limit. To mitigate robust overfitting, we first propose a new crop
transformation, Cropshift, which has improved diversity compared to the
conventional one (Padcrop). We then propose a new data augmentation scheme,
based on Cropshift, with much improved diversity and well-balanced hardness.
Empirically, our augmentation method achieves the state-of-the-art accuracy and
robustness for data augmentations in adversarial training. Furthermore, when
combined with weight averaging it matches, or even exceeds, the performance of
the best contemporary regularization methods for alleviating robust
overfitting. Code is available at:
https://github.com/TreeLLi/DA-Alone-Improves-AT.",None,-1
2af44fda-5574-4701-9e90-6b5d8f4f1946,RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models,0.432753,"In this paper, we investigate the in-context learning ability of
retrieval-augmented encoder-decoder language models. We first conduct a
comprehensive analysis of existing models and identify their limitations in
in-context learning, primarily due to a mismatch between pretraining and
inference, as well as a restricted context length. To address these issues, we
propose RAVEN, a model that combines retrieval-augmented masked language
modeling and prefix language modeling. We further introduce Fusion-in-Context
Learning to enhance the few-shot performance by enabling the model to leverage
more in-context examples without requiring additional training. Through
extensive experiments, we demonstrate that our simple yet effective design
significantly improves performance, achieving results comparable to the most
advanced language models in certain scenarios, despite having substantially
fewer parameters. Our work underscores the potential of retrieval-augmented
encoder-decoder language models for in-context learning and encourages further
research in this direction.",None,-1
d7a5bfa2-ff23-4033-a6af-b49df8b32032,BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations,0.453804,"Real-world planning problems, including autonomous driving and sustainable
energy applications like carbon storage and resource exploration, have recently
been modeled as partially observable Markov decision processes (POMDPs) and
solved using approximate methods. To solve high-dimensional POMDPs in practice,
state-of-the-art methods use online planning with problem-specific heuristics
to reduce planning horizons and make the problems tractable. Algorithms that
learn approximations to replace heuristics have recently found success in
large-scale fully observable domains. The key insight is the combination of
online Monte Carlo tree search with offline neural network approximations of
the optimal policy and value function. In this work, we bring this insight to
partially observed domains and propose BetaZero, a belief-state planning
algorithm for high-dimensional POMDPs. BetaZero learns offline approximations
that replace heuristics to enable online decision making in long-horizon
problems. We address several challenges inherent in large-scale partially
observable domains; namely challenges of transitioning in stochastic
environments, prioritizing action branching with a limited search budget, and
representing beliefs as input to the network. To formalize the use of all
limited search information we train against a novel Q-weighted policy vector
target. We test BetaZero on various well-established benchmark POMDPs found in
the literature and a real-world, high-dimensional problem of critical mineral
exploration. Experiments show that BetaZero outperforms state-of-the-art POMDP
solvers on a variety of tasks.",None,-1
2884da29-c1c1-4dd8-9284-5d6c798d7d72,GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue,0.681286,"Pre-trained models have achieved success in Chinese Short Text Matching (STM)
tasks, but they often rely on superficial clues, leading to a lack of robust
predictions. To address this issue, it is crucial to analyze and mitigate the
influence of superficial clues on STM models. Our study aims to investigate
their over-reliance on the edit distance feature, commonly used to measure the
semantic similarity of Chinese text pairs, which can be considered a
superficial clue. To mitigate STM models' over-reliance on superficial clues,
we propose a novel resampling training strategy called Gradually Learn Samples
Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of
In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we
demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the
robustness and generalization of Chinese STM models. Moreover, we conduct a
detailed analysis of existing methods and reveal their commonality.",None,-1
081bb7c8-485c-4197-894d-569948dcea1b,Description-Based Text Similarity,0.397039,"Identifying texts with a given semantics is central for many information
seeking scenarios. Similarity search over vector embeddings appear to be
central to this ability, yet the similarity reflected in current text
embeddings is corpus-driven, and is inconsistent and sub-optimal for many use
cases. What, then, is a good notion of similarity for effective retrieval of
text?
  We identify the need to search for texts based on abstract descriptions of
their content, and the corresponding notion of \emph{description based
similarity}. We demonstrate the inadequacy of current text embeddings and
propose an alternative model that significantly improves when used in standard
nearest neighbor search. The model is trained using positive and negative pairs
sourced through prompting a LLM, demonstrating how data from LLMs can be used
for creating new capabilities not immediately possible using the original
model.",None,-1
0a3ddc7e-af74-4ff6-9776-043d0730c99b,Synthesizing a Progression of Subtasks for Block-Based Visual Programming Tasks,0.40238,"Block-based visual programming environments play an increasingly important
role in introducing computing concepts to K-12 students. In recent years, they
have also gained popularity in neuro-symbolic AI, serving as a benchmark to
evaluate general problem-solving and logical reasoning skills. The open-ended
and conceptual nature of these visual programming tasks make them challenging,
both for state-of-the-art AI agents as well as for novice programmers. A
natural approach to providing assistance for problem-solving is breaking down a
complex task into a progression of simpler subtasks; however, this is not
trivial given that the solution codes are typically nested and have non-linear
execution behavior. In this paper, we formalize the problem of synthesizing
such a progression for a given reference block-based visual programming task.
We propose a novel synthesis algorithm that generates a progression of subtasks
that are high-quality, well-spaced in terms of their complexity, and solving
this progression leads to solving the reference task. We show the utility of
our synthesis algorithm in improving the efficacy of AI agents (in this case,
neural program synthesizers) for solving tasks in the Karel programming
environment. Then, we conduct a user study to demonstrate that our synthesized
progression of subtasks can assist a novice programmer in solving tasks in the
Hour of Code: Maze Challenge by Code-dot-org.",None,-1
2303544e-b085-4d06-9cae-35408f21e026,Adaptive Discretization using Voronoi Trees for Continuous POMDPs,0.0572282,"Solving continuous Partially Observable Markov Decision Processes (POMDPs) is
challenging, particularly for high-dimensional continuous action spaces. To
alleviate this difficulty, we propose a new sampling-based online POMDP solver,
called Adaptive Discretization using Voronoi Trees (ADVT). It uses Monte Carlo
Tree Search in combination with an adaptive discretization of the action space
as well as optimistic optimization to efficiently sample high-dimensional
continuous action spaces and compute the best action to perform. Specifically,
we adaptively discretize the action space for each sampled belief using a
hierarchical partition called Voronoi tree, which is a Binary Space
Partitioning that implicitly maintains the partition of a cell as the Voronoi
diagram of two points sampled from the cell. ADVT uses the estimated diameters
of the cells to form an upper-confidence bound on the action value function
within the cell, guiding the Monte Carlo Tree Search expansion and further
discretization of the action space. This enables ADVT to better exploit local
information with respect to the action value function, allowing faster
identification of the most promising regions in the action space, compared to
existing solvers. Voronoi trees keep the cost of partitioning and estimating
the diameter of each cell low, even in high-dimensional spaces where many
sampled points are required to cover the space well. ADVT additionally handles
continuous observation spaces, by adopting an observation progressive widening
strategy, along with a weighted particle representation of beliefs.
Experimental results indicate that ADVT scales substantially better to
high-dimensional continuous action spaces, compared to state-of-the-art
methods.",None,-1
86237844-cbef-43bd-a76e-633595d89f63,Towards Mode Balancing of Generative Models via Diversity Weights,0.0518965,"Large data-driven image models are extensively used to support creative and
artistic work. Under the currently predominant distribution-fitting paradigm, a
dataset is treated as ground truth to be approximated as closely as possible.
Yet, many creative applications demand a diverse range of output, and creators
often strive to actively diverge from a given data distribution. We argue that
an adjustment of modelling objectives, from pure mode coverage towards mode
balancing, is necessary to accommodate the goal of higher output diversity. We
present diversity weights, a training scheme that increases a model's output
diversity by balancing the modes in the training dataset. First experiments in
a controlled setting demonstrate the potential of our method. We discuss
connections of our approach to diversity, equity, and inclusion in generative
machine learning more generally, and computational creativity specifically. An
implementation of our algorithm is available at
https://github.com/sebastianberns/diversity-weights",None,-1
0cb72cf6-6c5d-4d99-a7d4-aecc686c8d67,"Syntax and Semantics Meet in the ""Middle"": Probing the Syntax-Semantics Interface of LMs Through Agentivity",0.417362,"Recent advances in large language models have prompted researchers to examine
their abilities across a variety of linguistic tasks, but little has been done
to investigate how models handle the interactions in meaning across words and
larger syntactic forms -- i.e. phenomena at the intersection of syntax and
semantics. We present the semantic notion of agentivity as a case study for
probing such interactions. We created a novel evaluation dataset by utilitizing
the unique linguistic properties of a subset of optionally transitive English
verbs. This dataset was used to prompt varying sizes of three model classes to
see if they are sensitive to agentivity at the lexical level, and if they can
appropriately employ these word-level priors given a specific syntactic
context. Overall, GPT-3 text-davinci-003 performs extremely well across all
experiments, outperforming all other models tested by far. In fact, the results
are even better correlated with human judgements than both syntactic and
semantic corpus statistics. This suggests that LMs may potentially serve as
more useful tools for linguistic annotation, theory testing, and discovery than
select corpora for certain tasks. Code is available at
https://github.com/lindiatjuatja/lm_sem",None,-1
300cf31d-a6d9-4dde-a129-01271316b71a,Thread of Thought Unraveling Chaotic Contexts,0.474146,"Large Language Models (LLMs) have ushered in a transformative era in the
field of natural language processing, excelling in tasks related to text
comprehension and generation. Nevertheless, they encounter difficulties when
confronted with chaotic contexts (e.g., distractors rather than long irrelevant
context), leading to the inadvertent omission of certain details within the
chaotic context. In response to these challenges, we introduce the ""Thread of
Thought"" (ThoT) strategy, which draws inspiration from human cognitive
processes. ThoT systematically segments and analyzes extended contexts while
adeptly selecting pertinent information. This strategy serves as a versatile
""plug-and-play"" module, seamlessly integrating with various LLMs and prompting
techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as
well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to
illustrate that ThoT significantly improves reasoning performance compared to
other prompting techniques.",None,-1
8e3b57f5-d92f-46cc-ae21-276e641332ef,PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction,0.36636,"Speech-to-text errors made by automatic speech recognition (ASR) systems
negatively impact downstream models. Error correction models as a
post-processing text editing method have been recently developed for refining
the ASR outputs. However, efficient models that meet the low latency
requirements of industrial grade production systems have not been well studied.
We propose PATCorrect-a novel non-autoregressive (NAR) approach based on
multi-modal fusion leveraging representations from both text and phoneme
modalities, to reduce word error rate (WER) and perform robustly with varying
input transcription quality. We demonstrate that PATCorrect consistently
outperforms state-of-the-art NAR method on English corpus across different
upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to
9.46% WERR achieved by other methods using text only modality. Besides, its
inference latency is at tens of milliseconds, making it ideal for systems with
low latency requirements.",None,-1
ea6be990-ddd4-417e-8edb-1d1f45cd8728,From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for Conversational Exploratory Search,0.569733,"Exploratory search is an open-ended information retrieval process that aims
at discovering knowledge about a topic or domain rather than searching for a
specific answer or piece of information. Conversational interfaces are
particularly suitable for supporting exploratory search, allowing users to
refine queries and examine search results through interactive dialogues. In
addition to conversational search interfaces, knowledge graphs are also useful
in supporting information exploration due to their rich semantic representation
of data items. In this study, we demonstrate the synergistic effects of
combining knowledge graphs and conversational interfaces for exploratory
search, bridging the gap between structured and unstructured information
retrieval. To this end, we propose a knowledge-driven dialogue system for
exploring news articles by asking natural language questions and using the
graph structure to navigate between related topics. Based on a user study with
54 participants, we empirically evaluate the effectiveness of the graph-based
exploratory search and discuss design implications for developing such systems.",None,-1
0e03c476-46b0-4256-92f5-68d850c560ea,GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond,0.120102,"This study proposes the GOOSE algorithm as a novel metaheuristic algorithm
based on the goose's behavior during rest and foraging. The goose stands on one
leg and keeps his balance to guard and protect other individuals in the flock.
The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions,
and the results are verified by a comparative study with genetic algorithm
(GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitness
dependent optimizer (FDO). In addition, the proposed algorithm is tested on 10
modern benchmark functions, and the gained results are compared with three
recent algorithms, such as the dragonfly algorithm, whale optimization
algorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithm
is tested on 5 classical benchmark functions, and the obtained results are
evaluated with six algorithms, such as fitness dependent optimizer (FDO), FOX
optimizer, butterfly optimization algorithm (BOA), whale optimization
algorithm, dragonfly algorithm, and chimp optimization algorithm (ChOA). The
achieved findings attest to the proposed algorithm's superior performance
compared to the other algorithms that were utilized in the current study. The
technique is then used to optimize Welded beam design and Economic Load
Dispatch Problem, three renowned real-world engineering challenges, and the
Pathological IgG Fraction in the Nervous System. The outcomes of the
engineering case studies illustrate how well the suggested approach can
optimize issues that arise in the real-world.",None,-1
73c4a4a0-7ada-4bc7-8fc6-135d1d3ae9ce,Calibration-free BEV Representation for Infrastructure Perception,0.417321,"Effective BEV object detection on infrastructure can greatly improve traffic
scenes understanding and vehicle-toinfrastructure (V2I) cooperative perception.
However, cameras installed on infrastructure have various postures, and
previous BEV detection methods rely on accurate calibration, which is difficult
for practical applications due to inevitable natural factors (e.g., wind and
snow). In this paper, we propose a Calibration-free BEV Representation (CBR)
network, which achieves 3D detection based on BEV representation without
calibration parameters and additional depth supervision. Specifically, we
utilize two multi-layer perceptrons for decoupling the features from
perspective view to front view and birdeye view under boxes-induced foreground
supervision. Then, a cross-view feature fusion module matches features from
orthogonal views according to similarity and conducts BEV feature enhancement
with front view features. Experimental results on DAIR-V2X demonstrate that CBR
achieves acceptable performance without any camera parameters and is naturally
not affected by calibration noises. We hope CBR can serve as a baseline for
future research addressing practical challenges of infrastructure perception.",None,-1
19ee08af-a612-4f07-ad0d-8ac3ea3748a9,PM-DETR: Domain Adaptive Prompt Memory for Object Detection with Transformers,0.0463178,"The Transformer-based detectors (i.e., DETR) have demonstrated impressive
performance on end-to-end object detection. However, transferring DETR to
different data distributions may lead to a significant performance degradation.
Existing adaptation techniques focus on model-based approaches, which aim to
leverage feature alignment to narrow the distribution shift between different
domains. In this study, we propose a hierarchical Prompt Domain Memory (PDM)
for adapting detection transformers to different distributions. PDM
comprehensively leverages the prompt memory to extract domain-specific
knowledge and explicitly constructs a long-term memory space for the data
distribution, which represents better domain diversity compared to existing
methods. Specifically, each prompt and its corresponding distribution value are
paired in the memory space, and we inject top M distribution-similar prompts
into the input and multi-level embeddings of DETR. Additionally, we introduce
the Prompt Memory Alignment (PMA) to reduce the discrepancy between the source
and target domains by fully leveraging the domain-specific knowledge extracted
from the prompt domain memory. Extensive experiments demonstrate that our
method outperforms state-of-the-art domain adaptive object detection methods on
three benchmarks, including scene, synthetic to real, and weather adaptation.
Codes will be released.",None,-1
19ce1709-79b0-4f9f-9a06-a98e362d0c47,CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality,0.241372,"There are three problems existing in the popular data-to-text datasets.
First, the large-scale datasets either contain noise or lack real application
scenarios. Second, the datasets close to real applications are relatively small
in size. Last, current datasets bias in the English language while leaving
other languages underexplored. To alleviate these limitations, in this paper,
we present CATS, a pragmatic Chinese answer-to-sequence dataset with large
scale and high quality. The dataset aims to generate textual descriptions for
the answer in the practical TableQA system. Further, to bridge the structural
gap between the input SQL and table and establish better semantic alignments,
we propose a Unified Graph Transformation approach to establish a joint
encoding space for the two hybrid knowledge resources and convert this task to
a graph-to-text problem. The experiment results demonstrate the effectiveness
of our proposed method. Further analysis on CATS attests to both the high
quality and challenges of the dataset.",None,-1
227657e1-f4ba-4ab5-936d-e6cdb6e7f60c,"""You might think about slightly revising the title"": identifying hedges in peer-tutoring interactions",0.991125,"Hedges play an important role in the management of conversational
interaction. In peer tutoring, they are notably used by tutors in dyads (pairs
of interlocutors) experiencing low rapport to tone down the impact of
instructions and negative feedback. Pursuing the objective of building a
tutoring agent that manages rapport with students in order to improve learning,
we used a multimodal peer-tutoring dataset to construct a computational
framework for identifying hedges. We compared approaches relying on pre-trained
resources with others that integrate insights from the social science
literature. Our best performance involved a hybrid approach that outperforms
the existing baseline while being easier to interpret. We employ a model
explainability tool to explore the features that characterize hedges in
peer-tutoring conversations, and we identify some novel features, and the
benefits of such a hybrid model approach.",None,-1
c2b746a5-8d32-4b80-b5cb-17becb344dc6,Legal Extractive Summarization of U.S. Court Opinions,0.438252,"This paper tackles the task of legal extractive summarization using a dataset
of 430K U.S. court opinions with key passages annotated. According to automated
summary quality metrics, the reinforcement-learning-based MemSum model is best
and even out-performs transformer-based models. In turn, expert human
evaluation shows that MemSum summaries effectively capture the key points of
lengthy court opinions. Motivated by these results, we open-source our models
to the general public. This represents progress towards democratizing law and
making U.S. court opinions more accessible to the general public.",None,-1
eef1721b-7fb5-4842-8fe5-24e2252d84f6,L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models,0.101411,"The exploration of sentiment analysis in low-resource languages, such as
Marathi, has been limited due to the availability of suitable datasets. In this
work, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis
dataset, with four different domains - movie reviews, general tweets, TV show
subtitles, and political tweets. The dataset consists of around 60,000 manually
tagged samples covering 3 distinct sentiments - positive, negative, and
neutral. We create a sub-dataset for each domain comprising 15k samples. The
MahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset
within the Indic sentiment landscape. We fine-tune different monolingual and
multilingual BERT models on these datasets and report the best accuracy with
the MahaBERT model. We also present an extensive in-domain and cross-domain
analysis thus highlighting the need for low-resource multi-domain datasets. The
data and models are available at https://github.com/l3cube-pune/MarathiNLP .",None,-1
6be28591-b8db-4b32-b78c-823aeb99a4fc,Classification of retail products: From probabilistic ranking to neural networks,0.364721,"Food retailing is now on an accelerated path to a success penetration into
the digital market by new ways of value creation at all stages of the consumer
decision process. One of the most important imperatives in this path is the
availability of quality data to feed all the process in digital transformation.
But the quality of data is not so obvious if we consider the variety of
products and suppliers in the grocery market. Within this context of digital
transformation of grocery industry, \textit{Midiadia} is Spanish data provider
company that works on converting data from the retailers' products into
knowledge with attributes and insights from the product labels, that is,
maintaining quality data in a dynamic market with a high dispersion of
products. Currently, they manually categorize products (groceries) according to
the information extracted directly (text processing) from the product labelling
and packaging. This paper introduces a solution to automatically categorize the
constantly changing product catalogue into a 3-level food taxonomy. Our
proposal studies three different approaches: a score-based ranking method,
traditional machine learning algorithms, and deep neural networks. Thus, we
provide four different classifiers that support a more efficient and less
error-prone maintenance of groceries catalogues, the main asset of the company.
Finally, we have compared the performance of these three alternatives,
concluding that traditional machine learning algorithms perform better, but
closely followed by the score-based approach.",None,-1
54f51b3c-b3c8-4fdc-900c-5b3e04627e7f,TalkUp: Paving the Way for Understanding Empowering Language,0.0740225,"Empowering language is important in many real-world contexts, from education
to workplace dynamics to healthcare. Though language technologies are growing
more prevalent in these contexts, empowerment has seldom been studied in NLP,
and moreover, it is inherently challenging to operationalize because of its
implicit nature. This work builds from linguistic and social psychology
literature to explore what characterizes empowering language. We then
crowdsource a novel dataset of Reddit posts labeled for empowerment, reasons
why these posts are empowering to readers, and the social relationships between
posters and readers. Our preliminary analyses show that this dataset, which we
call TalkUp, can be used to train language models that capture empowering and
disempowering language. More broadly, TalkUp provides an avenue to explore
implication, presuppositions, and how social context influences the meaning of
language.",None,-1
711a440f-fc33-4598-ad80-0faa6346eb9e,Using Text-to-Image Generation for Architectural Design Ideation,0.354563,"The recent progress of text-to-image generation has been recognized in
architectural design. Our study is the first to investigate the potential of
text-to-image generators in supporting creativity during the early stages of
the architectural design process. We conducted a laboratory study with 17
architecture students, who developed a concept for a culture center using three
popular text-to-image generators: Midjourney, Stable Diffusion, and DALL-E.
Through standardized questionnaires and group interviews, we found that image
generation could be a meaningful part of the design process when design
constraints are carefully considered. Generative tools support serendipitous
discovery of ideas and an imaginative mindset, enriching the design process. We
identified several challenges of image generators and provided considerations
for software development and educators to support creativity and emphasize
designers' imaginative mindset. By understanding the limitations and potential
of text-to-image generators, architects and designers can leverage this
technology in their design process and education, facilitating innovation and
effective communication of concepts.",None,-1
59bc0a5f-ec15-46d8-bd6a-69176e84ba90,Robust Evaluation of Diffusion-Based Adversarial Purification,0.62877,"We question the current evaluation practice on diffusion-based purification
methods. Diffusion-based purification methods aim to remove adversarial effects
from an input data point at test time. The approach gains increasing attention
as an alternative to adversarial training due to the disentangling between
training and testing. Well-known white-box attacks are often employed to
measure the robustness of the purification. However, it is unknown whether
these attacks are the most effective for the diffusion-based purification since
the attacks are often tailored for adversarial training. We analyze the current
practices and provide a new guideline for measuring the robustness of
purification methods against adversarial attacks. Based on our analysis, we
further propose a new purification strategy improving robustness compared to
the current diffusion-based purification methods.",None,-1
209e75c9-1507-419a-8f1d-a37bebc759af,Where Would I Go Next? Large Language Models as Human Mobility Predictors,0.854218,"Accurate human mobility prediction underpins many important applications
across a variety of domains, including epidemic modelling, transport planning,
and emergency responses. Due to the sparsity of mobility data and the
stochastic nature of people's daily activities, achieving precise predictions
of people's locations remains a challenge. While recently developed large
language models (LLMs) have demonstrated superior performance across numerous
language-related tasks, their applicability to human mobility studies remains
unexplored. Addressing this gap, this article delves into the potential of LLMs
for human mobility prediction tasks. We introduce a novel method, LLM-Mob,
which leverages the language understanding and reasoning capabilities of LLMs
for analysing human mobility data. We present concepts of historical stays and
context stays to capture both long-term and short-term dependencies in human
movement and enable time-aware prediction by using time information of the
prediction target. Additionally, we design context-inclusive prompts that
enable LLMs to generate more accurate predictions. Comprehensive evaluations of
our method reveal that LLM-Mob excels in providing accurate and interpretable
predictions, highlighting the untapped potential of LLMs in advancing human
mobility prediction techniques. We posit that our research marks a significant
paradigm shift in human mobility modelling, transitioning from building complex
domain-specific models to harnessing general-purpose LLMs that yield accurate
predictions through language instructions. The code for this work is available
at https://github.com/xlwang233/LLM-Mob.",None,-1
86c2b650-3734-4d0d-a319-6e063ac9560f,On the Expressiveness and Generalization of Hypergraph Neural Networks,0.150498,"This extended abstract describes a framework for analyzing the
expressiveness, learning, and (structural) generalization of hypergraph neural
networks (HyperGNNs). Specifically, we focus on how HyperGNNs can learn from
finite datasets and generalize structurally to graph reasoning problems of
arbitrary input sizes. Our first contribution is a fine-grained analysis of the
expressiveness of HyperGNNs, that is, the set of functions that they can
realize. Our result is a hierarchy of problems they can solve, defined in terms
of various hyperparameters such as depths and edge arities. Next, we analyze
the learning properties of these neural networks, especially focusing on how
they can be trained on a finite set of small graphs and generalize to larger
graphs, which we term structural generalization. Our theoretical results are
further supported by the empirical results.",None,-1
51506249-a63b-4363-a6c2-27c2b73c3cae,Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean,0.0370753,"Accurate and up-to-date information on building characteristics is essential
for vulnerability assessment; however, the high costs and long timeframes
associated with conducting traditional field surveys can be an obstacle to
obtaining critical exposure datasets needed for disaster risk management. In
this work, we leverage deep learning techniques for the automated
classification of roof characteristics from very high-resolution orthophotos
and airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.
We demonstrate that the fusion of multimodal earth observation data performs
better than using any single data source alone. Using our proposed methods, we
achieve F1 scores of 0.93 and 0.92 for roof type and roof material
classification, respectively. This work is intended to help governments produce
more timely building information to improve resilience and disaster response in
the Caribbean.",None,-1
593308dd-42ed-4f04-b507-f902b17482c8,Quantifying the Dialect Gap and its Correlates Across Languages,0.456921,"Historically, researchers and consumers have noticed a decrease in quality
when applying NLP tools to minority variants of languages (i.e. Puerto Rican
Spanish or Swiss German), but studies exploring this have been limited to a
select few languages. Additionally, past studies have mainly been conducted in
a monolingual context, so cross-linguistic trends have not been identified and
tied to external factors. In this work, we conduct a comprehensive evaluation
of the most influential, state-of-the-art large language models (LLMs) across
two high-use applications, machine translation and automatic speech
recognition, to assess their functionality on the regional dialects of several
high- and low-resource languages. Additionally, we analyze how the regional
dialect gap is correlated with economic, social, and linguistic factors. The
impact of training data, including related factors like dataset size and its
construction procedure, is shown to be significant but not consistent across
models or languages, meaning a one-size-fits-all approach cannot be taken in
solving the dialect gap. This work will lay the foundation for furthering the
field of dialectal NLP by laying out evident disparities and identifying
possible pathways for addressing them through mindful data collection.",None,-1
fab827ac-597c-49dd-8c27-1c0e17bb65a3,ICDAR 2023 Competition on Hierarchical Text Detection and Recognition,0.270678,"We organize a competition on hierarchical text detection and recognition. The
competition is aimed to promote research into deep learning models and systems
that can jointly perform text detection and recognition and geometric layout
analysis. We present details of the proposed competition organization,
including tasks, datasets, evaluations, and schedule. During the competition
period (from January 2nd 2023 to April 1st 2023), at least 50 submissions from
more than 20 teams were made in the 2 proposed tasks. Considering the number of
teams and submissions, we conclude that the HierText competition has been
successfully held. In this report, we will also present the competition results
and insights from them.",None,-1
54424484-994a-47e2-88f3-3c53d59e367e,Hierarchical Graph Neural Networks for Causal Discovery and Root Cause Localization,0.432964,"In this paper, we propose REASON, a novel framework that enables the
automatic discovery of both intra-level (i.e., within-network) and inter-level
(i.e., across-network) causal relationships for root cause localization. REASON
consists of Topological Causal Discovery and Individual Causal Discovery. The
Topological Causal Discovery component aims to model the fault propagation in
order to trace back to the root causes. To achieve this, we propose novel
hierarchical graph neural networks to construct interdependent causal networks
by modeling both intra-level and inter-level non-linear causal relations. Based
on the learned interdependent causal networks, we then leverage random walks
with restarts to model the network propagation of a system fault. The
Individual Causal Discovery component focuses on capturing abrupt change
patterns of a single system entity. This component examines the temporal
patterns of each entity's metric data (i.e., time series), and estimates its
likelihood of being a root cause based on the Extreme Value theory. Combining
the topological and individual causal scores, the top K system entities are
identified as root causes. Extensive experiments on three real-world datasets
with case studies demonstrate the effectiveness and superiority of the proposed
framework.",None,-1
84811e69-c077-4351-bb28-d6687eebefea,ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks,1.0,"Many NLP applications require manual data annotations for a variety of tasks,
notably to train classifiers or evaluate the performance of unsupervised
models. Depending on the size and degree of complexity, the tasks may be
conducted by crowd-workers on platforms such as MTurk as well as trained
annotators, such as research assistants. Using a sample of 2,382 tweets, we
demonstrate that ChatGPT outperforms crowd-workers for several annotation
tasks, including relevance, stance, topics, and frames detection. Specifically,
the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of
five tasks, while ChatGPT's intercoder agreement exceeds that of both
crowd-workers and trained annotators for all tasks. Moreover, the
per-annotation cost of ChatGPT is less than $0.003 -- about twenty times
cheaper than MTurk. These results show the potential of large language models
to drastically increase the efficiency of text classification.",None,-1
39c2670e-626b-4e3d-9692-ce03a24304f5,Tailor: Altering Skip Connections for Resource-Efficient Inference,0.140661,"Deep neural networks use skip connections to improve training convergence.
However, these skip connections are costly in hardware, requiring extra buffers
and increasing on- and off-chip memory utilization and bandwidth requirements.
In this paper, we show that skip connections can be optimized for hardware when
tackled with a hardware-software codesign approach. We argue that while a
network's skip connections are needed for the network to learn, they can later
be removed or shortened to provide a more hardware efficient implementation
with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose
hardware-aware training algorithm gradually removes or shortens a fully trained
network's skip connections to lower their hardware cost. Tailor improves
resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs for
on-chip, dataflow-style architectures. Tailor increases performance by 30% and
reduces memory bandwidth by 45% for a 2D processing element array architecture.",None,-1
6e830458-6170-4d39-98fc-818a1f91f471,Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis,0.702697,"Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various
fine-grained sentiment analysis tasks on a target domain by transferring
knowledge from a source domain. Since labeled data only exists in the source
domain, a model is expected to bridge the domain gap for tackling cross-domain
ABSA. Though domain adaptation methods have proven to be effective, most of
them are based on a discriminative model, which needs to be specifically
designed for different ABSA tasks. To offer a more general solution, we propose
a unified bidirectional generative framework to tackle various cross-domain
ABSA tasks. Specifically, our framework trains a generative model in both
text-to-label and label-to-text directions. The former transforms each task
into a unified format to learn domain-agnostic features, and the latter
generates natural sentences from noisy labels for data augmentation, with which
a more accurate model can be trained. To investigate the effectiveness and
generality of our framework, we conduct extensive experiments on four
cross-domain ABSA tasks and present new state-of-the-art results on all tasks.
Our data and code are publicly available at
\url{https://github.com/DAMO-NLP-SG/BGCA}.",None,-1
c8ffd456-132a-4909-9938-808b198bc6aa,Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection,0.971434,"Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning,
yet constructing them through human annotations can be costly. As a result,
various automatic methods have been proposed to construct CSKG with larger
semantic coverage. However, these unsupervised approaches introduce spurious
noise that can lower the quality of the resulting CSKG, which cannot be tackled
easily by existing denoising algorithms due to the unique characteristics of
nodes and structures in CSKGs. To address this issue, we propose Gold (Global
and Local-aware Denoising), a denoising framework for CSKGs that incorporates
entity semantic information, global rules, and local structural information
from the CSKG. Experiment results demonstrate that Gold outperforms all
baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks.
Furthermore, we show that denoising a real-world CSKG is effective and even
benefits the downstream zero-shot commonsense question-answering task.",None,-1
4498fc06-f981-4aae-b0cd-bd5d4b4540d2,"Separability, Contextuality, and the Quantum Frame Problem",0.41659,"We study the relationship between assumptions of state separability and both
preparation and measurement contextuality, and the relationship of both of
these to the frame problem, the problem of predicting what does not change in
consequence of an action. We state a quantum analog of the latter and prove its
undecidability. We show how contextuality is generically induced in state
preparation and measurement by basis choice, thermodynamic exchange, and the
imposition of a priori causal models, and how fine-tuning assumptions appear
ubiquitously in settings characterized as non-contextual.",None,-1
1ead0706-9708-4b9c-904c-85db78b51dd1,Test Time Adaptation for Blind Image Quality Assessment,0.86683,"While the design of blind image quality assessment (IQA) algorithms has
improved significantly, the distribution shift between the training and testing
scenarios often leads to a poor performance of these methods at inference time.
This motivates the study of test time adaptation (TTA) techniques to improve
their performance at inference time. Existing auxiliary tasks and loss
functions used for TTA may not be relevant for quality-aware adaptation of the
pre-trained model. In this work, we introduce two novel quality-relevant
auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In
particular, we introduce a group contrastive loss at the batch level and a
relative rank loss at the sample level to make the model quality aware and
adapt to the target data. Our experiments reveal that even using a small batch
of images from the test distribution helps achieve significant improvement in
performance by updating the batch normalization statistics of the source model.",None,-1
6f7b5fcf-8c5d-4faf-b46c-04ce30413a3c,I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction,0.78707,"Multimodal information extraction is attracting research attention nowadays,
which requires aggregating representations from different modalities. In this
paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM)
method for this task, which contains two modules. Firstly, the intra-sample
relationship modeling module operates on a single sample and aims to learn
effective representations. Embeddings from textual and visual modalities are
shifted to bridge the modality gap caused by distinct pre-trained language and
image models. Secondly, the inter-sample relationship modeling module considers
relationships among multiple samples and focuses on capturing the interactions.
An AttnMixup strategy is proposed, which not only enables collaboration among
samples but also augments data to improve generalization. We conduct extensive
experiments on the multimodal named entity recognition datasets Twitter-2015
and Twitter-2017, and the multimodal relation extraction dataset MNRE. Our
proposed method I2SRM achieves competitive results, 77.12% F1-score on
Twitter-2015, 88.40% F1-score on Twitter-2017, and 84.12% F1-score on MNRE.",None,-1
1932c3b9-4fe8-4c57-b14a-0a06fb422ada,Max-min Learning of Approximate Weight Matrices from Fuzzy Data,0.4134,"In this article, we study the approximate solutions set $\Lambda_b$ of an
inconsistent system of $\max-\min$ fuzzy relational equations $(S): A
\Box_{\min}^{\max}x =b$. Using the $L_\infty$ norm, we compute by an explicit
analytical formula the Chebyshev distance $\Delta~=~\inf_{c \in \mathcal{C}}
\Vert b -c \Vert$, where $\mathcal{C}$ is the set of second members of the
consistent systems defined with the same matrix $A$. We study the set
$\mathcal{C}_b$ of Chebyshev approximations of the second member $b$ i.e.,
vectors $c \in \mathcal{C}$ such that $\Vert b -c \Vert = \Delta$, which is
associated to the approximate solutions set $\Lambda_b$ in the following sense:
an element of the set $\Lambda_b$ is a solution vector $x^\ast$ of a system $A
\Box_{\min}^{\max}x =c$ where $c \in \mathcal{C}_b$. As main results, we
describe both the structure of the set $\Lambda_b$ and that of the set
$\mathcal{C}_b$. We then introduce a paradigm for $\max-\min$ learning weight
matrices that relates input and output data from training data. The learning
error is expressed in terms of the $L_\infty$ norm. We compute by an explicit
formula the minimal value of the learning error according to the training data.
We give a method to construct weight matrices whose learning error is minimal,
that we call approximate weight matrices.
  Finally, as an application of our results, we show how to learn approximately
the rule parameters of a possibilistic rule-based system according to multiple
training data.",None,-1
08c73518-84dc-418f-a6c8-8da049d30e7d,MvCo-DoT:Multi-View Contrastive Domain Transfer Network for Medical Report Generation,0.0635709,"In clinical scenarios, multiple medical images with different views are
usually generated at the same time, and they have high semantic consistency.
However, the existing medical report generation methods cannot exploit the rich
multi-view mutual information of medical images. Therefore, in this work, we
propose the first multi-view medical report generation model, called MvCo-DoT.
Specifically, MvCo-DoT first propose a multi-view contrastive learning (MvCo)
strategy to help the deep reinforcement learning based model utilize the
consistency of multi-view inputs for better model learning. Then, to close the
performance gaps of using multi-view and single-view inputs, a domain transfer
network is further proposed to ensure MvCo-DoT achieve almost the same
performance as multi-view inputs using only single-view inputs.Extensive
experiments on the IU X-Ray public dataset show that MvCo-DoT outperforms the
SOTA medical report generation baselines in all metrics.",None,-1
d30eb8e0-59fa-406d-92cc-6ef2e8c717a9,Local Implicit Ray Function for Generalizable Radiance Field Representation,0.585239,"We propose LIRF (Local Implicit Ray Function), a generalizable neural
rendering approach for novel view rendering. Current generalizable neural
radiance fields (NeRF) methods sample a scene with a single ray per pixel and
may therefore render blurred or aliased views when the input views and rendered
views capture scene content with different resolutions. To solve this problem,
we propose LIRF to aggregate the information from conical frustums to construct
a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates
and the features of conical frustums as inputs and predicts a local volumetric
radiance field. Since the coordinates are continuous, LIRF renders high-quality
novel views at a continuously-valued scale via volume rendering. Besides, we
predict the visible weights for each input view via transformer-based feature
matching to improve the performance in occluded areas. Experimental results on
real-world scenes validate that our method outperforms state-of-the-art methods
on novel view rendering of unseen scenes at arbitrary scales.",None,-1
1fb645ae-7da9-45a0-9e4f-66b8942c6efb,CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval,0.194958,"Growing techniques have been emerging to improve the performance of passage
retrieval. As an effective representation bottleneck pretraining technique, the
contextual masked auto-encoder utilizes contextual embedding to assist in the
reconstruction of passages. However, it only uses a single auto-encoding
pre-task for dense representation pre-training. This study brings multi-view
modeling to the contextual masked auto-encoder. Firstly, multi-view
representation utilizes both dense and sparse vectors as multi-view
representations, aiming to capture sentence semantics from different aspects.
Moreover, multiview decoding paradigm utilizes both autoencoding and
auto-regressive decoders in representation bottleneck pre-training, aiming to
provide both reconstructive and generative signals for better contextual
representation pretraining. We refer to this multi-view pretraining method as
CoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective
and robust on large-scale passage retrieval benchmarks and out-of-domain
zero-shot benchmarks.",None,-1
2064917d-240b-40d1-9f04-2f30f8e29b0b,Recent Advancements In The Field Of Deepfake Detection,0.113636,"A deepfake is a photo or video of a person whose image has been digitally
altered or partially replaced with an image of someone else. Deepfakes have the
potential to cause a variety of problems and are often used maliciously. A
common usage is altering videos of prominent political figures and celebrities.
These deepfakes can portray them making offensive, problematic, and/or untrue
statements. Current deepfakes can be very realistic, and when used in this way,
can spread panic and even influence elections and political opinions. There are
many deepfake detection strategies currently in use but finding the most
comprehensive and universal method is critical. So, in this survey we will
address the problems of malicious deepfake creation and the lack of universal
deepfake detection methods. Our objective is to survey and analyze a variety of
current methods and advances in the field of deepfake detection.",None,-1
df6f9b6e-72d8-412d-9617-f982d3262cc8,VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON,0.957586,"Despite progress in vision-based inspection algorithms, real-world industrial
challenges -- specifically in data availability, quality, and complex
production requirements -- often remain under-addressed. We introduce the
VISION Datasets, a diverse collection of 14 industrial inspection datasets,
uniquely poised to meet these challenges. Unlike previous datasets, VISION
brings versatility to defect detection, offering annotation masks across all
splits and catering to various detection methodologies. Our datasets also
feature instance-segmentation annotation, enabling precise defect
identification. With a total of 18k images encompassing 44 defect types, VISION
strives to mirror a wide range of real-world production scenarios. By
supporting two ongoing challenge competitions on the VISION Datasets, we hope
to foster further advancements in vision-based industrial inspection.",None,-1
b25f1a29-3e74-4d31-8440-b771ca4291f2,Exploration with Principles for Diverse AI Supervision,0.0936138,"Training large transformers using next-token prediction has given rise to
groundbreaking advancements in AI. While this generative AI approach has
produced impressive results, it heavily leans on human supervision. Even
state-of-the-art AI models like ChatGPT depend on fine-tuning through human
demonstrations, demanding extensive human input and domain expertise. This
strong reliance on human oversight poses a significant hurdle to the
advancement of AI innovation. To address this limitation, we propose a novel
paradigm termed Exploratory AI (EAI) aimed at autonomously generating
high-quality training data. Drawing inspiration from unsupervised reinforcement
learning (RL) pretraining, EAI achieves exploration within the natural language
space. We accomplish this by harnessing large language models to assess the
novelty of generated content. Our approach employs two key components: an actor
that generates novel content following exploration principles and a critic that
evaluates the generated content, offering critiques to guide the actor.
Empirical evaluations demonstrate that EAI significantly boosts model
performance on complex reasoning tasks, addressing the limitations of
human-intensive supervision.",None,-1
7a78e528-bd7b-4e6e-aded-838fd808adb0,Orientation-Guided Contrastive Learning for UAV-View Geo-Localisation,0.640985,"Retrieving relevant multimedia content is one of the main problems in a world
that is increasingly data-driven. With the proliferation of drones, high
quality aerial footage is now available to a wide audience for the first time.
Integrating this footage into applications can enable GPS-less geo-localisation
or location correction.
  In this paper, we present an orientation-guided training framework for
UAV-view geo-localisation. Through hierarchical localisation orientations of
the UAV images are estimated in relation to the satellite imagery. We propose a
lightweight prediction module for these pseudo labels which predicts the
orientation between the different views based on the contrastive learned
embeddings. We experimentally demonstrate that this prediction supports the
training and outperforms previous approaches. The extracted pseudo-labels also
enable aligned rotation of the satellite image as augmentation to further
strengthen the generalisation. During inference, we no longer need this
orientation module, which means that no additional computations are required.
We achieve state-of-the-art results on both the University-1652 and
University-160k datasets.",None,-1
0ed7d332-8eb8-48e8-9bf0-625fa92d5d61,InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent,0.882771,"This research paper delves into the integration of OpenAI's ChatGPT into
embodied agent systems, evaluating its influence on interactive decision-making
benchmark. Drawing a parallel to the concept of people assuming roles according
to their unique strengths, we introduce InterAct. In this approach, we feed
ChatGPT with varied prompts, assigning it a numerous roles like a checker and a
sorter, then integrating them with the original language model. Our research
shows a remarkable success rate of 98% in AlfWorld, which consists of 6
different tasks in a simulated household environment, emphasizing the
significance of proficient prompt engineering. The results highlight ChatGPT's
competence in comprehending and performing intricate tasks effectively in
real-world settings, thus paving the way for further advancements in task
planning.",None,-1
328f3f24-eca9-4f3e-bdbb-58a3d536aa9f,Hybrid-Supervised Dual-Search: Leveraging Automatic Learning for Loss-free Multi-Exposure Image Fusion,0.170893,"Multi-exposure image fusion (MEF) has emerged as a prominent solution to
address the limitations of digital imaging in representing varied exposure
levels. Despite its advancements, the field grapples with challenges, notably
the reliance on manual designs for network structures and loss functions, and
the constraints of utilizing simulated reference images as ground truths.
Consequently, current methodologies often suffer from color distortions and
exposure artifacts, further complicating the quest for authentic image
representation. In addressing these challenges, this paper presents a
Hybrid-Supervised Dual-Search approach for MEF, dubbed HSDS-MEF, which
introduces a bi-level optimization search scheme for automatic design of both
network structures and loss functions. More specifically, we harnesses a unique
dual research mechanism rooted in a novel weighted structure refinement
architecture search. Besides, a hybrid supervised contrast constraint
seamlessly guides and integrates with searching process, facilitating a more
adaptive and comprehensive search for optimal loss functions. We realize the
state-of-the-art performance in comparison to various competitive schemes,
yielding a 10.61% and 4.38% improvement in Visual Information Fidelity (VIF)
for general and no-reference scenarios, respectively, while providing results
with high contrast, rich details and colors.",None,-1
3fd7bce7-04fd-4298-9150-e39e47b50e19,Explicit Syntactic Guidance for Neural Text Generation,0.116985,"Most existing text generation models follow the sequence-to-sequence
paradigm. Generative Grammar suggests that humans generate natural language
texts by learning language grammar. We propose a syntax-guided generation
schema, which generates the sequence guided by a constituency parse tree in a
top-down direction. The decoding process can be decomposed into two parts: (1)
predicting the infilling texts for each constituent in the lexicalized syntax
context given the source sentence; (2) mapping and expanding each constituent
to construct the next-level syntax context. Accordingly, we propose a
structural beam search method to find possible syntax structures
hierarchically. Experiments on paraphrase generation and machine translation
show that the proposed method outperforms autoregressive baselines, while also
demonstrating effectiveness in terms of interpretability, controllability, and
diversity.",None,-1
6e92c0c8-5029-487d-bbc6-3a3b00803cae,Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection,0.725526,"The expanding market for e-comics has spurred interest in the development of
automated methods to analyze comics. For further understanding of comics, an
automated approach is needed to link text in comics to characters speaking the
words. Comics speaker detection research has practical applications, such as
automatic character assignment for audiobooks, automatic translation according
to characters' personalities, and inference of character relationships and
stories.
  To deal with the problem of insufficient speaker-to-text annotations, we
created a new annotation dataset Manga109Dialog based on Manga109.
Manga109Dialog is the world's largest comics speaker annotation dataset,
containing 132,692 speaker-to-text pairs. We further divided our dataset into
different levels by prediction difficulties to evaluate speaker detection
methods more appropriately. Unlike existing methods mainly based on distances,
we propose a deep learning-based method using scene graph generation models.
Due to the unique features of comics, we enhance the performance of our
proposed model by considering the frame reading order. We conducted experiments
using Manga109Dialog and other datasets. Experimental results demonstrate that
our scene-graph-based approach outperforms existing methods, achieving a
prediction accuracy of over 75%.",None,-1
ee938269-918f-424b-b812-413cb7265cfb,iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling Perspectives?,0.169471,"There are two competing approaches for modelling annotator disagreement:
distributional soft-labelling approaches (which aim to capture the level of
disagreement) or modelling perspectives of individual annotators or groups
thereof. We adapt a multi-task architecture -- which has previously shown
success in modelling perspectives -- to evaluate its performance on the SEMEVAL
Task 11. We do so by combining both approaches, i.e. predicting individual
annotator perspectives as an interim step towards predicting annotator
disagreement. Despite its previous success, we found that a multi-task approach
performed poorly on datasets which contained distinct annotator opinions,
suggesting that this approach may not always be suitable when modelling
perspectives. Furthermore, our results explain that while strongly
perspectivist approaches might not achieve state-of-the-art performance
according to evaluation metrics used by distributional approaches, our approach
allows for a more nuanced understanding of individual perspectives present in
the data. We argue that perspectivist approaches are preferable because they
enable decision makers to amplify minority views, and that it is important to
re-evaluate metrics to reflect this goal.",None,-1
eaf18c06-503a-4a68-92d0-3d2a61231c8d,GDDS: Pulmonary Bronchioles Segmentation with Group Deep Dense Supervision,0.734749,"Airway segmentation, especially bronchioles segmentation, is an important but
challenging task because distal bronchus are sparsely distributed and of a fine
scale. Existing neural networks usually exploit sparse topology to learn the
connectivity of bronchioles and inefficient shallow features to capture such
high-frequency information, leading to the breakage or missed detection of
individual thin branches. To address these problems, we contribute a new
bronchial segmentation method based on Group Deep Dense Supervision (GDDS) that
emphasizes fine-scale bronchioles segmentation in a simple-but-effective
manner. First, Deep Dense Supervision (DDS) is proposed by constructing local
dense topology skillfully and implementing dense topological learning on a
specific shallow feature layer. GDDS further empowers the shallow features with
better perception ability to detect bronchioles, even the ones that are not
easily discernible to the naked eye. Extensive experiments on the BAS benchmark
dataset have shown that our method promotes the network to have a high
sensitivity in capturing fine-scale branches and outperforms state-of-the-art
methods by a large margin (+12.8 % in BD and +8.8 % in TD) while only
introducing a small number of extra parameters.",None,-1
eb334a6a-baaf-4777-b3be-a4bc21b9f834,Training on Foveated Images Improves Robustness to Adversarial Attacks,0.086711,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks -- subtle, perceptually indistinguishable perturbations of inputs that
change the response of the model. In the context of vision, we hypothesize that
an important contributor to the robustness of human visual perception is
constant exposure to low-fidelity visual stimuli in our peripheral vision. To
investigate this hypothesis, we develop \RBlur, an image transform that
simulates the loss in fidelity of peripheral vision by blurring the image and
reducing its color saturation based on the distance from a given fixation
point. We show that compared to DNNs trained on the original images, DNNs
trained on images transformed by \RBlur are substantially more robust to
adversarial attacks, as well as other, non-adversarial, corruptions, achieving
up to 25\% higher accuracy on perturbed data.",None,-1
1d921831-599b-462f-a61a-c1ab33e5790c,SkinSAM: Empowering Skin Cancer Segmentation with Segment Anything Model,0.963174,"Skin cancer is a prevalent and potentially fatal disease that requires
accurate and efficient diagnosis and treatment. Although manual tracing is the
current standard in clinics, automated tools are desired to reduce human labor
and improve accuracy. However, developing such tools is challenging due to the
highly variable appearance of skin cancers and complex objects in the
background. In this paper, we present SkinSAM, a fine-tuned model based on the
Segment Anything Model that showed outstanding segmentation performance. The
models are validated on HAM10000 dataset which includes 10015 dermatoscopic
images. While larger models (ViT_L, ViT_H) performed better than the smaller
one (ViT_b), the finetuned model (ViT_b_finetuned) exhibited the greatest
improvement, with a Mean pixel accuracy of 0.945, Mean dice score of 0.8879,
and Mean IoU score of 0.7843. Among the lesion types, vascular lesions showed
the best segmentation results. Our research demonstrates the great potential of
adapting SAM to medical image segmentation tasks.",None,-1
cd6b1a7a-7bfc-4cc8-a31f-d5cf3a1c2ca3,MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences,0.531145,"Occluded and long-range objects are ubiquitous and challenging for 3D object
detection. Point cloud sequence data provide unique opportunities to improve
such cases, as an occluded or distant object can be observed from different
viewpoints or gets better visibility over time. However, the efficiency and
effectiveness in encoding long-term sequence data can still be improved. In
this work, we propose MoDAR, using motion forecasting outputs as a type of
virtual modality, to augment LiDAR point clouds. The MoDAR modality propagates
object information from temporal contexts to a target frame, represented as a
set of virtual points, one for each object from a waypoint on a forecasted
trajectory. A fused point cloud of both raw sensor points and the virtual
points can then be fed to any off-the-shelf point-cloud based 3D object
detector. Evaluated on the Waymo Open Dataset, our method significantly
improves prior art detectors by using motion forecasting from extra-long
sequences (e.g. 18 seconds), achieving new state of the arts, while not adding
much computation overhead.",None,-1
596ad77d-cf23-437f-97dd-ca15ceaa7b47,ViTs for SITS: Vision Transformers for Satellite Image Time Series,0.943036,"In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a
fully-attentional model for general Satellite Image Time Series (SITS)
processing based on the Vision Transformer (ViT). TSViT splits a SITS record
into non-overlapping patches in space and time which are tokenized and
subsequently processed by a factorized temporo-spatial encoder. We argue, that
in contrast to natural images, a temporal-then-spatial factorization is more
intuitive for SITS processing and present experimental evidence for this claim.
Additionally, we enhance the model's discriminative power by introducing two
novel mechanisms for acquisition-time-specific temporal positional encodings
and multiple learnable class tokens. The effect of all novel design choices is
evaluated through an extensive ablation study. Our proposed architecture
achieves state-of-the-art performance, surpassing previous approaches by a
significant margin in three publicly available SITS semantic segmentation and
classification datasets. All model, training and evaluation codes are made
publicly available to facilitate further research.",None,-1
8b7a0069-e953-48f0-aaa7-9eed19197f17,Weight-based Mask for Domain Adaptation,0.331459,"In computer vision, unsupervised domain adaptation (UDA) is an approach to
transferring knowledge from a label-rich source domain to a fully-unlabeled
target domain. Conventional UDA approaches have two problems. The first problem
is that a class classifier can be biased to the source domain because it is
trained using only source samples. The second is that previous approaches align
image-level features regardless of foreground and background, although the
classifier requires foreground features. To solve these problems, we introduce
Weight-based Mask Network (WEMNet) composed of Domain Ignore Module (DIM) and
Semantic Enhancement Module (SEM). DIM obtains domain-agnostic feature
representations via the weight of the domain discriminator and predicts
categories. In addition, SEM obtains class-related feature representations
using the classifier weight and focuses on the foreground features for domain
adaptation. Extensive experimental results reveal that the proposed WEMNet
outperforms the competitive accuracy on representative UDA datasets.",None,-1
3d873082-08d0-41b8-b893-a889b01e7869,Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health,0.617625,"Amid ongoing health crisis, there is a growing necessity to discern possible
signs of Wellness Dimensions (WD) manifested in self-narrated text. As the
distribution of WD on social media data is intrinsically imbalanced, we
experiment the generative NLP models for data augmentation to enable further
improvement in the pre-screening task of classifying WD. To this end, we
propose a simple yet effective data augmentation approach through prompt-based
Generative NLP models, and evaluate the ROUGE scores and syntactic/semantic
similarity among existing interpretations and augmented data. Our approach with
ChatGPT model surpasses all the other methods and achieves improvement over
baselines such as Easy-Data Augmentation and Backtranslation. Introducing data
augmentation to generate more training samples and balanced dataset, results in
the improved F-score and the Matthew's Correlation Coefficient for upto 13.11%
and 15.95%, respectively.",None,-1
3a820d4b-fd4f-4d43-b382-68c3071270d4,EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities,0.89554,"The advent of artificial intelligence has led to a growing emphasis on
data-driven modeling in macroeconomics, with agent-based modeling (ABM)
emerging as a prominent bottom-up simulation paradigm. In ABM, agents (e.g.,
households, firms) interact within a macroeconomic environment, collectively
generating market dynamics. Existing agent modeling typically employs
predetermined rules or learning-based neural networks for decision-making.
However, customizing each agent presents significant challenges, complicating
the modeling of agent heterogeneity. Additionally, the influence of
multi-period market dynamics and multifaceted macroeconomic factors are often
overlooked in decision-making processes. In this work, we introduce EconAgent,
a large language model-empowered agent with human-like characteristics for
macroeconomic simulation. We first construct a simulation environment that
incorporates various market dynamics driven by agents' decisions regarding work
and consumption. Through the perception module, we create heterogeneous agents
with distinct decision-making mechanisms. Furthermore, we model the impact of
macroeconomic trends using a memory module, which allows agents to reflect on
past individual experiences and market dynamics. Simulation experiments show
that EconAgent can make realistic decisions, leading to more reasonable
macroeconomic phenomena compared to existing rule-based or learning-based
agents. Our codes are released at
https://github.com/tsinghua-fib-lab/ACL24-EconAgent.",None,-1
2f4cf78e-13b6-4202-8af8-b20856a1c725,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,0.885462,"Performant Convolutional Neural Network (CNN) architectures must be tailored
to specific tasks in order to consider the length, resolution, and
dimensionality of the input data. In this work, we tackle the need for
problem-specific CNN architectures. We present the Continuous Convolutional
Neural Network (CCNN): a single CNN able to process data of arbitrary
resolution, dimensionality and length without any structural changes. Its key
component are its continuous convolutional kernels which model long-range
dependencies at every layer, and thus remove the need of current CNN
architectures for task-dependent downsampling and depths. We showcase the
generality of our method by using the same architecture for tasks on sequential
($1{\rm D}$), visual ($2{\rm D}$) and point-cloud ($3{\rm D}$) data. Our CCNN
matches and often outperforms the current state-of-the-art across all tasks
considered.",None,-1
075f59b4-a1ce-4bf6-ab95-6f21b2695c34,Faithful and Robust Local Interpretability for Textual Predictions,0.317718,"Interpretability is essential for machine learning models to be trusted and
deployed in critical domains. However, existing methods for interpreting text
models are often complex, lack mathematical foundations, and their performance
is not guaranteed. In this paper, we propose FRED (Faithful and Robust
Explainer for textual Documents), a novel method for interpreting predictions
over text. FRED offers three key insights to explain a model prediction: (1) it
identifies the minimal set of words in a document whose removal has the
strongest influence on the prediction, (2) it assigns an importance score to
each token, reflecting its influence on the model's output, and (3) it provides
counterfactual explanations by generating examples similar to the original
document, but leading to a different prediction. We establish the reliability
of FRED through formal definitions and theoretical analyses on interpretable
classifiers. Additionally, our empirical evaluation against state-of-the-art
methods demonstrates the effectiveness of FRED in providing insights into text
models.",None,-1
94723557-65e9-44bf-9614-8b1e1f36400e,When SAM Meets Shadow Detection,0.308191,"As a promptable generic object segmentation model, segment anything model
(SAM) has recently attracted significant attention, and also demonstrates its
powerful performance. Nevertheless, it still meets its Waterloo when
encountering several tasks, e.g., medical image segmentation, camouflaged
object detection, etc. In this report, we try SAM on an unexplored popular
task: shadow detection. Specifically, four benchmarks were chosen and evaluated
with widely used metrics. The experimental results show that the performance
for shadow detection using SAM is not satisfactory, especially when comparing
with the elaborate models. Code is available at
https://github.com/LeipingJie/SAMSh.",None,-1
9f1584d1-9482-4ed4-8a8e-298fa528ea92,Balanced Energy Regularization Loss for Out-of-distribution Detection,0.746105,"In the field of out-of-distribution (OOD) detection, a previous method that
use auxiliary data as OOD data has shown promising performance. However, the
method provides an equal loss to all auxiliary data to differentiate them from
inliers. However, based on our observation, in various tasks, there is a
general imbalance in the distribution of the auxiliary OOD data across classes.
We propose a balanced energy regularization loss that is simple but generally
effective for a variety of tasks. Our balanced energy regularization loss
utilizes class-wise different prior probabilities for auxiliary data to address
the class imbalance in OOD data. The main concept is to regularize auxiliary
samples from majority classes, more heavily than those from minority classes.
Our approach performs better for OOD detection in semantic segmentation,
long-tailed image classification, and image classification than the prior
energy regularization loss. Furthermore, our approach achieves state-of-the-art
performance in two tasks: OOD detection in semantic segmentation and
long-tailed image classification. Code is available at
https://github.com/hyunjunChhoi/Balanced_Energy.",None,-1
83907ef1-2fa0-41da-a391-10746d3e7f5d,GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems,0.78489,"There has been considerable divergence of opinion on the reasoning abilities
of Large Language Models (LLMs). While the initial optimism that reasoning
might emerge automatically with scale has been tempered thanks to a slew of
counterexamples, a wide spread belief in their iterative self-critique
capabilities persists. In this paper, we set out to systematically investigate
the effectiveness of iterative prompting of LLMs in the context of Graph
Coloring, a canonical NP-complete reasoning problem that is related to
propositional satisfiability as well as practical problems like scheduling and
allocation. We present a principled empirical study of the performance of GPT4
in solving graph coloring instances or verifying the correctness of candidate
colorings. In iterative modes, we experiment with the model critiquing its own
answers and an external correct reasoner verifying proposed solutions. In both
cases, we analyze whether the content of the criticisms actually affects bottom
line performance. The study seems to indicate that (i) LLMs are bad at solving
graph coloring instances (ii) they are no better at verifying a solution--and
thus are not effective in iterative modes with LLMs critiquing LLM-generated
solutions (iii) the correctness and content of the criticisms--whether by LLMs
or external solvers--seems largely irrelevant to the performance of iterative
prompting. We show that the observed increase in effectiveness is largely due
to the correct solution being fortuitously present in the top-k completions of
the prompt (and being recognized as such by an external verifier). Our results
thus call into question claims about the self-critiquing capabilities of state
of the art LLMs.",None,-1
5f480d75-d0b0-49c0-b0f8-8e202590dc0d,Composing Task Knowledge with Modular Successor Feature Approximators,0.532457,"Recently, the Successor Features and Generalized Policy Improvement (SF&GPI)
framework has been proposed as a method for learning, composing, and
transferring predictive knowledge and behavior. SF&GPI works by having an agent
learn predictive representations (SFs) that can be combined for transfer to new
tasks with GPI. However, to be effective this approach requires state features
that are useful to predict, and these state-features are typically
hand-designed. In this work, we present a novel neural network architecture,
""Modular Successor Feature Approximators"" (MSFA), where modules both discover
what is useful to predict, and learn their own predictive representations. We
show that MSFA is able to better generalize compared to baseline architectures
for learning SFs and modular architectures",None,-1
0c2cb5bb-0bc9-49f1-a598-14f82b4edee9,Trustworthiness of Children Stories Generated by Large Language Models,0.0397229,"Large Language Models (LLMs) have shown a tremendous capacity for generating
literary text. However, their effectiveness in generating children's stories
has yet to be thoroughly examined. In this study, we evaluate the
trustworthiness of children's stories generated by LLMs using various measures,
and we compare and contrast our results with both old and new children's
stories to better assess their significance. Our findings suggest that LLMs
still struggle to generate children's stories at the level of quality and
nuance found in actual stories",None,-1
2d31df11-7eb7-4b97-b62c-4c9c21b445af,MTS-Mixers: Multivariate Time Series Forecasting via Factorized Temporal and Channel Mixing,0.8702,"Multivariate time series forecasting has been widely used in various
practical scenarios. Recently, Transformer-based models have shown significant
potential in forecasting tasks due to the capture of long-range dependencies.
However, recent studies in the vision and NLP fields show that the role of
attention modules is not clear, which can be replaced by other token
aggregation operations. This paper investigates the contributions and
deficiencies of attention mechanisms on the performance of time series
forecasting. Specifically, we find that (1) attention is not necessary for
capturing temporal dependencies, (2) the entanglement and redundancy in the
capture of temporal and channel interaction affect the forecasting performance,
and (3) it is important to model the mapping between the input and the
prediction sequence. To this end, we propose MTS-Mixers, which use two
factorized modules to capture temporal and channel dependencies. Experimental
results on several real-world datasets show that MTS-Mixers outperform existing
Transformer-based models with higher efficiency.",None,-1
8f087f93-0e59-4dda-85ba-a9810ad96ad4,Comparative study of Transformer and LSTM Network with attention mechanism on Image Captioning,0.272431,"In a globalized world at the present epoch of generative intelligence, most
of the manual labour tasks are automated with increased efficiency. This can
support businesses to save time and money. A crucial component of generative
intelligence is the integration of vision and language. Consequently, image
captioning become an intriguing area of research. There have been multiple
attempts by the researchers to solve this problem with different deep learning
architectures, although the accuracy has increased, but the results are still
not up to standard. This study buckles down to the comparison of Transformer
and LSTM with attention block model on MS-COCO dataset, which is a standard
dataset for image captioning. For both the models we have used pretrained
Inception-V3 CNN encoder for feature extraction of the images. The Bilingual
Evaluation Understudy score (BLEU) is used to checked the accuracy of caption
generated by both models. Along with the transformer and LSTM with attention
block models,CLIP-diffusion model, M2-Transformer model and the X-Linear
Attention model have been discussed with state of the art accuracy.",None,-1
36562f74-8171-4299-b050-80d639e2d9eb,Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence,0.777899,"Sentence-level representations are beneficial for various natural language
processing tasks. It is commonly believed that vector representations can
capture rich linguistic properties. Currently, large language models (LMs)
achieve state-of-the-art performance on sentence embedding. However, some
recent works suggest that vector representations from LMs can cause information
leakage. In this work, we further investigate the information leakage issue and
propose a generative embedding inversion attack (GEIA) that aims to reconstruct
input sequences based only on their sentence embeddings. Given the black-box
access to a language model, we treat sentence embeddings as initial tokens'
representations and train or fine-tune a powerful decoder model to decode the
whole sequences directly. We conduct extensive experiments to demonstrate that
our generative inversion attack outperforms previous embedding inversion
attacks in classification metrics and generates coherent and contextually
similar sentences as the original inputs.",None,-1
056800a5-aefc-4d81-be6a-b7a2de7bb8a4,The Flan Collection: Designing Data and Methods for Effective Instruction Tuning,0.985041,"We study the design decisions of publicly available instruction tuning
methods, and break down the development of Flan 2022 (Chung et al., 2022).
Through careful ablation studies on the Flan Collection of tasks and methods,
we tease apart the effect of design decisions which enable Flan-T5 to
outperform prior work by 3-17%+ across evaluation settings. We find task
balancing and enrichment techniques are overlooked but critical to effective
instruction tuning, and in particular, training with mixed prompt settings
(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)
performance in all settings. In further experiments, we show Flan-T5 requires
less finetuning to converge higher and faster than T5 on single downstream
tasks, motivating instruction-tuned models as more computationally-efficient
starting checkpoints for new tasks. Finally, to accelerate research on
instruction tuning, we make the Flan 2022 collection of datasets, templates,
and methods publicly available at
https://github.com/google-research/FLAN/tree/main/flan/v2.",None,-1
93c6684f-400d-4fb0-971f-9ff747094eee,Efficient Parametric Approximations of Neural Network Function Space Distance,0.314159,"It is often useful to compactly summarize important properties of model
parameters and training data so that they can be used later without storing
and/or iterating over the entire dataset. As a specific case, we consider
estimating the Function Space Distance (FSD) over a training set, i.e. the
average discrepancy between the outputs of two neural networks. We propose a
Linearized Activation Function TRick (LAFTR) and derive an efficient
approximation to FSD for ReLU neural networks. The key idea is to approximate
the architecture as a linear network with stochastic gating. Despite requiring
only one parameter per unit of the network, our approach outcompetes other
parametric approximations with larger memory requirements. Applied to continual
learning, our parametric approximation is competitive with state-of-the-art
nonparametric approximations, which require storing many training examples.
Furthermore, we show its efficacy in estimating influence functions accurately
and detecting mislabeled examples without expensive iterations over the entire
dataset.",None,-1
cf33deb6-1d43-4e32-a896-3e060b62ddcf,Safe Interval Path Planning With Kinodynamic Constraints,0.269101,"Safe Interval Path Planning (SIPP) is a powerful algorithm for solving
single-agent pathfinding problem when the agent is confined to a graph and
certain vertices/edges of this graph are blocked at certain time intervals due
to dynamic obstacles that populate the environment. Original SIPP algorithm
relies on the assumption that the agent is able to stop instantaneously.
However, this assumption often does not hold in practice, e.g. a mobile robot
moving with a cruising speed is not able to stop immediately but rather
requires gradual deceleration to a full stop that takes time. In other words,
the robot is subject to kinodynamic constraints. Unfortunately, as we show in
this work, in such a case original SIPP is incomplete. To this end, we
introduce a novel variant of SIPP that is provably complete and optimal for
planning with acceleration/deceleration. In the experimental evaluation we show
that the key property of the original SIPP still holds for the modified version
-- it performs much less expansions compared to A* and, as a result, is notably
faster.",None,-1
2db9584b-9209-4c7e-8eaf-7ce88076759a,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,0.811274,"Multi-view radar-camera fused 3D object detection provides a farther
detection range and more helpful features for autonomous driving, especially
under adverse weather. The current radar-camera fusion methods deliver kinds of
designs to fuse radar information with camera data. However, these fusion
approaches usually adopt the straightforward concatenation operation between
multi-modal features, which ignores the semantic alignment with radar features
and sufficient correlations across modals. In this paper, we present MVFusion,
a novel Multi-View radar-camera Fusion method to achieve semantic-aligned radar
features and enhance the cross-modal information interaction. To achieve so, we
inject the semantic alignment into the radar features via the semantic-aligned
radar encoder (SARE) to produce image-guided radar features. Then, we propose
the radar-guided fusion transformer (RGFT) to fuse our radar and image features
to strengthen the two modals' correlation from the global scope via the
cross-attention mechanism. Extensive experiments show that MVFusion achieves
state-of-the-art performance (51.7% NDS and 45.3% mAP) on the nuScenes dataset.
We shall release our code and trained networks upon publication.",None,-1
4bdae3bc-b59b-413f-a530-3e4c3f9aa515,AutoDroid: LLM-powered Task Automation in Android,0.87099,"Mobile task automation is an attractive technique that aims to enable
voice-based hands-free user interaction with smartphones. However, existing
approaches suffer from poor scalability due to the limited language
understanding ability and the non-trivial manual efforts required from
developers or end-users. The recent advance of large language models (LLMs) in
language understanding and reasoning inspires us to rethink the problem from a
model-centric perspective, where task preparation, comprehension, and execution
are handled by a unified language model. In this work, we introduce AutoDroid,
a mobile task automation system capable of handling arbitrary tasks on any
Android application without manual efforts. The key insight is to combine the
commonsense knowledge of LLMs and domain-specific knowledge of apps through
automated dynamic analysis. The main components include a functionality-aware
UI representation method that bridges the UI with the LLM, exploration-based
memory injection techniques that augment the app-specific domain knowledge of
LLM, and a multi-granularity query optimization module that reduces the cost of
model inference. We integrate AutoDroid with off-the-shelf LLMs including
online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a
new benchmark for memory-augmented Android task automation with 158 common
tasks. The results demonstrated that AutoDroid is able to precisely generate
actions with an accuracy of 90.9%, and complete tasks with a success rate of
71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,
benchmark suites, and source code of AutoDroid will be released at
url{https://autodroid-sys.github.io/}.",None,-1
296791c9-5d1f-4b92-ab20-e07005aea9f5,Revisiting non-English Text Simplification: A Unified Multilingual Benchmark,0.91152,"Recent advancements in high-quality, large-scale English resources have
pushed the frontier of English Automatic Text Simplification (ATS) research.
However, less work has been done on multilingual text simplification due to the
lack of a diverse evaluation benchmark that covers complex-simple sentence
pairs in many languages. This paper introduces the MultiSim benchmark, a
collection of 27 resources in 12 distinct languages containing over 1.7 million
complex-simple sentence pairs. This benchmark will encourage research in
developing more effective multilingual text simplification models and
evaluation metrics. Our experiments using MultiSim with pre-trained
multilingual language models reveal exciting performance improvements from
multilingual training in non-English settings. We observe strong performance
from Russian in zero-shot cross-lingual transfer to low-resource languages. We
further show that few-shot prompting with BLOOM-176b achieves comparable
quality to reference simplifications outperforming fine-tuned models in most
languages. We validate these findings through human evaluation.",None,-1
58292d41-9f15-4aa0-8b54-87cc32516eb6,Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,0.453849,"CT images corrupted by metal artifacts have serious negative effects on
clinical diagnosis. Considering the difficulty of collecting paired data with
ground truth in clinical settings, unsupervised methods for metal artifact
reduction are of high interest. However, it is difficult for previous
unsupervised methods to retain structural information from CT images while
handling the non-local characteristics of metal artifacts. To address these
challenges, we proposed a novel Dense Transformer based Enhanced Coding Network
(DTEC-Net) for unsupervised metal artifact reduction. Specifically, we
introduce a Hierarchical Disentangling Encoder, supported by the high-order
dense process, and transformer to obtain densely encoded sequences with
long-range correspondence. Then, we present a second-order disentanglement
method to improve the dense sequence's decoding process. Extensive experiments
and model discussions illustrate DTEC-Net's effectiveness, which outperforms
the previous state-of-the-art methods on a benchmark dataset, and greatly
reduces metal artifacts while restoring richer texture details.",None,-1
07e2d51f-bba6-4d65-9cfa-696e11e26947,Unleash the Potential of 3D Point Cloud Modeling with A Calibrated Local Geometry-driven Distance Metric,0.115577,"Quantifying the dissimilarity between two unstructured 3D point clouds is a
challenging task, with existing metrics often relying on measuring the distance
between corresponding points that can be either inefficient or ineffective. In
this paper, we propose a novel distance metric called Calibrated Local Geometry
Distance (CLGD), which computes the difference between the underlying 3D
surfaces calibrated and induced by a set of reference points. By associating
each reference point with two given point clouds through computing its
directional distances to them, the difference in directional distances of an
identical reference point characterizes the geometric difference between a
typical local region of the two point clouds. Finally, CLGD is obtained by
averaging the directional distance differences of all reference points. We
evaluate CLGD on various optimization and unsupervised learning-based tasks,
including shape reconstruction, rigid registration, scene flow estimation, and
feature representation. Extensive experiments show that CLGD achieves
significantly higher accuracy under all tasks in a memory and computationally
efficient manner, compared with existing metrics. As a generic metric, CLGD has
the potential to advance 3D point cloud modeling. The source code is publicly
available at https://github.com/rsy6318/CLGD.",None,-1
87cc90cc-17c2-47c3-9b0a-fa6953dcfaf4,Diffusion Action Segmentation,0.650794,"Temporal action segmentation is crucial for understanding long-form videos.
Previous works on this task commonly adopt an iterative refinement paradigm by
using multi-stage models. We propose a novel framework via denoising diffusion
models, which nonetheless shares the same inherent spirit of such iterative
refinement. In this framework, action predictions are iteratively generated
from random noise with input video features as conditions. To enhance the
modeling of three striking characteristics of human actions, including the
position prior, the boundary ambiguity, and the relational dependency, we
devise a unified masking strategy for the conditioning inputs in our framework.
Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and
Breakfast, are performed and the proposed method achieves superior or
comparable results to state-of-the-art methods, showing the effectiveness of a
generative approach for action segmentation.",None,-1
8702b39f-4de3-4d23-b4eb-97061ff2d1eb,Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification,0.345107,"Commonsense fact verification, as a challenging branch of commonsense
question-answering (QA), aims to verify through facts whether a given
commonsense claim is correct or not. Answering commonsense questions
necessitates a combination of knowledge from various levels. However, existing
studies primarily rest on grasping either unstructured evidence or potential
reasoning paths from structured knowledge bases, yet failing to exploit the
benefits of heterogeneous knowledge simultaneously. In light of this, we
propose Decker, a commonsense fact verification model that is capable of
bridging heterogeneous knowledge by uncovering latent relationships between
structured and unstructured knowledge. Experimental results on two commonsense
fact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the
effectiveness of our Decker and further analysis verifies its capability to
seize more precious information through reasoning.",None,-1
ef49f9e7-9240-4492-b75f-63c3d9cba22b,3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks,0.341965,"Medical image segmentation typically necessitates a large and precisely
annotated dataset. However, obtaining pixel-wise annotation is a
labor-intensive task that requires significant effort from domain experts,
making it challenging to obtain in practical clinical scenarios. In such
situations, reducing the amount of annotation required is a more practical
approach. One feasible direction is sparse annotation, which involves
annotating only a few slices, and has several advantages over traditional weak
annotation methods such as bounding boxes and scribbles, as it preserves exact
boundaries. However, learning from sparse annotation is challenging due to the
scarcity of supervision signals. To address this issue, we propose a framework
that can robustly learn from sparse annotation using the cross-teaching of both
3D and 2D networks. Considering the characteristic of these networks, we
develop two pseudo label selection strategies, which are hard-soft confidence
threshold and consistent label fusion. Our experimental results on the MMWHS
dataset demonstrate that our method outperforms the state-of-the-art (SOTA)
semi-supervised segmentation methods. Moreover, our approach achieves results
that are comparable to the fully-supervised upper bound result.",None,-1
551b811b-befe-4ecf-93fa-23e0247ae5c7,RealFusion: 360 Reconstruction of Any Object from a Single Image,1.0,"We consider the problem of reconstructing a full 360{\deg} photographic model
of an object from a single image of it. We do so by fitting a neural radiance
field to the image, but find this problem to be severely ill-posed. We thus
take an off-the-self conditional image generator based on diffusion and
engineer a prompt that encourages it to ""dream up"" novel views of the object.
Using an approach inspired by DreamFields and DreamFusion, we fuse the given
input view, the conditional prior, and other regularizers in a final,
consistent reconstruction. We demonstrate state-of-the-art reconstruction
results on benchmark images when compared to prior methods for monocular 3D
reconstruction of objects. Qualitatively, our reconstructions provide a
faithful match of the input view and a plausible extrapolation of its
appearance and 3D shape, including to the side of the object not visible in the
image.",None,-1
476d7bf2-b3b3-4313-870e-9e5148a66066,Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources,0.195585,"To address the data scarcity issue in Conversational question answering
(ConvQA), a dialog inpainting method, which utilizes documents to generate
ConvQA datasets, has been proposed. However, the original dialog inpainting
model is trained solely on the dialog reconstruction task, resulting in the
generation of questions with low contextual relevance due to insufficient
learning of question-answer alignment. To overcome this limitation, we propose
a novel framework called Dialogizer, which has the capability to automatically
generate ConvQA datasets with high contextual relevance from textual sources.
The framework incorporates two training tasks: question-answer matching (QAM)
and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted
during the inference phase based on the contextual relevance of the generated
questions. Using our framework, we produce four ConvQA datasets by utilizing
documents from multiple domains as the primary source. Through automatic
evaluation using diverse metrics, as well as human evaluation, we validate that
our proposed framework exhibits the ability to generate datasets of higher
quality compared to the baseline dialog inpainting model.",None,-1
b6b0f79d-4338-42b6-ac42-a7c751119096,Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification,0.504005,"Sentiment analysis, widely critiqued for capturing merely the overall tone of
a corpus, falls short in accurately reflecting the latent structures and
political stances within texts. This study introduces topic metrics, dummy
variables converted from extracted topics, as both an alternative and
complement to sentiment metrics in stance classification. By employing three
datasets identified by Bestvater and Monroe (2023), this study demonstrates
BERTopic's proficiency in extracting coherent topics and the effectiveness of
topic metrics in stance classification. The experiment results show that
BERTopic improves coherence scores by 17.07% to 54.20% when compared to
traditional approaches such as Dirichlet Allocation (LDA) and Non-negative
Matrix Factorization (NMF), prevalent in earlier political science research.
Additionally, our results indicate topic metrics outperform sentiment metrics
in stance classification, increasing performance by as much as 18.95%. Our
findings suggest topic metrics are especially effective for context-rich texts
and corpus where stance and sentiment correlations are weak. The combination of
sentiment and topic metrics achieve an optimal performance in most of the
scenarios and can further address the limitations of relying solely on
sentiment as well as the low coherence score of topic metrics.",None,-1
59b1deae-f159-4c70-b52a-40e73fccf621,Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer,0.240334,"Deep neural networks often suffer from poor generalization due to complex and
non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular
solution that smooths the loss landscape by minimizing the maximized change of
training loss when adding a perturbation to the weight. However, indiscriminate
perturbation of SAM on all parameters is suboptimal and results in excessive
computation, double the overhead of common optimizers like Stochastic Gradient
Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and
effective training scheme that achieves sparse perturbation by a binary mask.
To obtain the sparse mask, we provide two solutions based on Fisher information
and dynamic sparse training, respectively. We investigate the impact of
different masks, including unstructured, structured, and $N$:$M$ structured
patterns, as well as explicit and implicit forms of implementing sparse
perturbation. We theoretically prove that SSAM can converge at the same rate as
SAM, i.e., $O(\log T/\sqrt{T})$. Sparse SAM has the potential to accelerate
training and smooth the loss landscape effectively. Extensive experimental
results on CIFAR and ImageNet-1K confirm that our method is superior to SAM in
terms of efficiency, and the performance is preserved or even improved with a
perturbation of merely 50\% sparsity. Code is available at
https://github.com/Mi-Peng/Systematic-Investigation-of-Sparse-Perturbed-Sharpness-Aware-Minimization-Optimizer.",None,-1
4c872ea8-6e20-48b6-b363-83cbe823425e,Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs,0.464739,"Intent-based management will play a critical role in achieving customers'
expectations in the next-generation mobile networks. Traditional methods cannot
perform efficient resource management since they tend to handle each
expectation independently. Existing approaches, e.g., based on multi-agent
reinforcement learning (MARL) allocate resources in an efficient fashion when
there are conflicting expectations on the network slice. However, in reality,
systems are often far more complex to be addressed by a standalone MARL
formulation. Often there exists a hierarchical structure of intent fulfilment
where multiple pre-trained, self-interested agents may need to be further
orchestrated by a supervisor or controller agent. Such agents may arrive in the
system adhoc, which then needs to be orchestrated along with other available
agents. Retraining the whole system every time is often infeasible given the
associated time and cost. Given the challenges, such adhoc coordination of
pre-trained systems could be achieved through an intelligent supervisor agent
which incentivizes pre-trained RL/MARL agents through sets of dynamic contracts
(goals or bonuses) and encourages them to act as a cohesive unit towards
fulfilling a global expectation. Some approaches use a rule-based supervisor
agent and deploy the hierarchical constituent agents sequentially, based on
human-coded rules.
  In the current work, we propose a framework whereby pre-trained agents can be
orchestrated in parallel leveraging an AI-based supervisor agent. For this, we
propose to use Adhoc-Teaming approaches which assign optimal goals to the MARL
agents and incentivize them to exhibit certain desired behaviours. Results on
the network emulator show that the proposed approach results in faster and
improved fulfilment of expectations when compared to rule-based approaches and
even generalizes to changes in environments.",None,-1
58c70e8a-774d-445e-a0fa-ec6149a0fc2b,A real-time algorithm for human action recognition in RGB and thermal video,0.198632,"Monitoring the movement and actions of humans in video in real-time is an
important task. We present a deep learning based algorithm for human action
recognition for both RGB and thermal cameras. It is able to detect and track
humans and recognize four basic actions (standing, walking, running, lying) in
real-time on a notebook with a NVIDIA GPU. For this, it combines state of the
art components for object detection (Scaled YoloV4), optical flow (RAFT) and
pose estimation (EvoSkeleton). Qualitative experiments on a set of tunnel
videos show that the proposed algorithm works robustly for both RGB and thermal
video.",None,-1
f9b3fc5d-7444-47c2-b213-95641ffc8429,Level Generation Through Large Language Models,0.779704,"Large Language Models (LLMs) are powerful tools, capable of leveraging their
training on natural language to write stories, generate code, and answer
questions. But can they generate functional video game levels? Game levels,
with their complex functional constraints and spatial relationships in more
than one dimension, are very different from the kinds of data an LLM typically
sees during training. Datasets of game levels are also hard to come by,
potentially taxing the abilities of these data-hungry models. We investigate
the use of LLMs to generate levels for the game Sokoban, finding that LLMs are
indeed capable of doing so, and that their performance scales dramatically with
dataset size. We also perform preliminary experiments on controlling LLM level
generators and discuss promising areas for future work.",None,-1
c521f0e6-c5d6-4d78-8ceb-8eb4d1f82953,Channelformer: Attention based Neural Solution for Wireless Channel Estimation and Effective Online Training,0.635372,"In this paper, we propose an encoder-decoder neural architecture (called
Channelformer) to achieve improved channel estimation for orthogonal
frequency-division multiplexing (OFDM) waveforms in downlink scenarios. The
self-attention mechanism is employed to achieve input precoding for the input
features before processing them in the decoder. In particular, we implement
multi-head attention in the encoder and a residual convolutional neural
architecture as the decoder, respectively. We also employ a customized
weight-level pruning to slim the trained neural network with a fine-tuning
process, which reduces the computational complexity significantly to realize a
low complexity and low latency solution. This enables reductions of up to 70\%
in the parameters, while maintaining an almost identical performance compared
with the complete Channelformer. We also propose an effective online training
method based on the fifth generation (5G) new radio (NR) configuration for the
modern communication systems, which only needs the available information at the
receiver for online training. Using industrial standard channel models, the
simulations of attention-based solutions show superior estimation performance
compared with other candidate neural network methods for channel estimation.",None,-1
25682e1a-cbde-45ff-a98b-3bc368356fec,Knowledge Graphs: Opportunities and Challenges,0.994904,"With the explosive growth of artificial intelligence (AI) and big data, it
has become vitally important to organize and represent the enormous volume of
knowledge appropriately. As graph data, knowledge graphs accumulate and convey
knowledge of the real world. It has been well-recognized that knowledge graphs
effectively represent complex information; hence, they rapidly gain the
attention of academia and industry in recent years. Thus to develop a deeper
understanding of knowledge graphs, this paper presents a systematic overview of
this field. Specifically, we focus on the opportunities and challenges of
knowledge graphs. We first review the opportunities of knowledge graphs in
terms of two aspects: (1) AI systems built upon knowledge graphs; (2) potential
application fields of knowledge graphs. Then, we thoroughly discuss severe
technical challenges in this field, such as knowledge graph embeddings,
knowledge acquisition, knowledge graph completion, knowledge fusion, and
knowledge reasoning. We expect that this survey will shed new light on future
research and the development of knowledge graphs.",None,-1
43d42b9c-65a6-4a44-aa45-122eb7ec42ff,Automated multilingual detection of Pro-Kremlin propaganda in newspapers and Telegram posts,0.536058,"The full-scale conflict between the Russian Federation and Ukraine generated
an unprecedented amount of news articles and social media data reflecting
opposing ideologies and narratives. These polarized campaigns have led to
mutual accusations of misinformation and fake news, shaping an atmosphere of
confusion and mistrust for readers worldwide. This study analyses how the media
affected and mirrored public opinion during the first month of the war using
news articles and Telegram news channels in Ukrainian, Russian, Romanian and
English. We propose and compare two methods of multilingual automated
pro-Kremlin propaganda identification, based on Transformers and linguistic
features. We analyse the advantages and disadvantages of both methods, their
adaptability to new genres and languages, and ethical considerations of their
usage for content moderation. With this work, we aim to lay the foundation for
further development of moderation tools tailored to the current conflict.",None,-1
865056dc-a62b-4934-adb3-e93276f2f989,LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models,0.423243,"Quantization is an indispensable technique for serving Large Language Models
(LLMs) and has recently found its way into LoRA fine-tuning. In this work we
focus on the scenario where quantization and LoRA fine-tuning are applied
together on a pre-trained model. In such cases it is common to observe a
consistent gap in the performance on downstream tasks between full fine-tuning
and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ
(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that
simultaneously quantizes an LLM and finds a proper low-rank initialization for
LoRA fine-tuning. Such an initialization alleviates the discrepancy between the
quantized and full-precision model and significantly improves generalization in
downstream tasks. We evaluate our method on natural language understanding,
question answering, summarization, and natural language generation tasks.
Experiments show that our method is highly effective and outperforms existing
quantization methods, especially in the challenging 2-bit and 2/4-bit mixed
precision regimes. The code is available on https://github.com/yxli2123/LoftQ.",None,-1
ef0a9fc8-9b08-4cc6-8d46-5548fa8c34fc,A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset,0.801013,"Text Classification is the process of categorizing text into the relevant
categories and its algorithms are at the core of many Natural Language
Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP
are the most highly used information retrieval methods in text classification.
We have investigated and analyzed the feature weighting method for text
classification on unstructured data. The proposed model considered two features
N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset
for sentiment analysis. Then we have used the state-of-the-art classifier to
validate the method i.e., Support Vector Machine (SVM), Logistic Regression,
Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and
k-nearest neighbors (KNN). From those two feature extractions, a significant
increase in feature extraction with TF-IDF features rather than based on
N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall
(93.81%), and F1-score (91.99%) value in Random Forest classifier.",None,-1
c01175ef-b3f4-4d16-ab7d-8325abd430c3,MetaTKG: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning,0.665309,"Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts
based on given history. One of the key challenges for prediction is to learn
the evolution of facts. Most existing works focus on exploring evolutionary
information in history to obtain effective temporal embeddings for entities and
relations, but they ignore the variation in evolution patterns of facts, which
makes them struggle to adapt to future data with different evolution patterns.
Moreover, new entities continue to emerge along with the evolution of facts
over time. Since existing models highly rely on historical information to learn
embeddings for entities, they perform poorly on such entities with little
historical information. To tackle these issues, we propose a novel Temporal
Meta-learning framework for TKG reasoning, MetaTKG for brevity. Specifically,
our method regards TKG prediction as many temporal meta-tasks, and utilizes the
designed Temporal Meta-learner to learn evolutionary meta-knowledge from these
meta-tasks. The proposed method aims to guide the backbones to learn to adapt
quickly to future data and deal with entities with little historical
information by the learned meta-knowledge. Specially, in temporal meta-learner,
we design a Gating Integration module to adaptively establish temporal
correlations between meta-tasks. Extensive experiments on four widely-used
datasets and three backbones demonstrate that our method can greatly improve
the performance.",None,-1
ddb5bf0d-07cd-4041-89ae-cc5dbfd7442a,LM vs LM: Detecting Factual Errors via Cross Examination,0.834093,"A prominent weakness of modern language models (LMs) is their tendency to
generate factually incorrect text, which hinders their usability. A natural
question is whether such factual errors can be detected automatically. Inspired
by truth-seeking mechanisms in law, we propose a factuality evaluation
framework for LMs that is based on cross-examination. Our key idea is that an
incorrect claim is likely to result in inconsistency with other claims that the
model generates. To discover such inconsistencies, we facilitate a multi-turn
interaction between the LM that generated the claim and another LM (acting as
an examiner) which introduces questions to discover inconsistencies. We
empirically evaluate our method on factual claims made by multiple recent LMs
on four benchmarks, finding that it outperforms existing methods and baselines,
often by a large gap. Our results demonstrate the potential of using
interacting LMs for capturing factual errors.",None,-1
af6d9376-b69f-4dd3-97ef-cdcb89362cc9,Learning from Noisy Crowd Labels with Logics,0.32128,"This paper explores the integration of symbolic logic knowledge into deep
neural networks for learning from noisy crowd labels. We introduce Logic-guided
Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic
knowledge distillation framework that learns from both noisy labeled data and
logic rules of interest. Unlike traditional EM methods, our framework contains
a ``pseudo-E-step'' that distills from the logic rules a new type of learning
target, which is then used in the ``pseudo-M-step'' for training the
classifier. Extensive evaluations on two real-world datasets for text sentiment
classification and named entity recognition demonstrate that the proposed
framework improves the state-of-the-art and provides a new solution to learning
from noisy crowd labels.",None,-1
10064853-9c01-4cf0-8a50-e15e04bc1f70,ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning,0.977107,"This paper assesses the accuracy, reliability and bias of the Large Language
Model (LLM) ChatGPT-4 on the text analysis task of classifying the political
affiliation of a Twitter poster based on the content of a tweet. The LLM is
compared to manual annotation by both expert classifiers and crowd workers,
generally considered the gold standard for such tasks. We use Twitter messages
from United States politicians during the 2020 election, providing a ground
truth against which to measure accuracy. The paper finds that ChatGPT-4 has
achieves higher accuracy, higher reliability, and equal or lower bias than the
human classifiers. The LLM is able to correctly annotate messages that require
reasoning on the basis of contextual knowledge, and inferences around the
author's intentions - traditionally seen as uniquely human abilities. These
findings suggest that LLM will have substantial impact on the use of textual
data in the social sciences, by enabling interpretive research at a scale.",None,-1
5c793160-42ee-417c-b7d7-7acf81127392,VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions,0.378421,"Font design is of vital importance in the digital content design and modern
printing industry. Developing algorithms capable of automatically synthesizing
vector fonts can significantly facilitate the font design process. However,
existing methods mainly concentrate on raster image generation, and only a few
approaches can directly synthesize vector fonts. This paper proposes an
end-to-end trainable method, VecFontSDF, to reconstruct and synthesize
high-quality vector fonts using signed distance functions (SDFs). Specifically,
based on the proposed SDF-based implicit shape representation, VecFontSDF
learns to model each glyph as shape primitives enclosed by several parabolic
curves, which can be precisely converted to quadratic B\'ezier curves that are
widely used in vector font products. In this manner, most image generation
methods can be easily extended to synthesize vector fonts. Qualitative and
quantitative experiments conducted on a publicly-available dataset demonstrate
that our method obtains high-quality results on several tasks, including vector
font reconstruction, interpolation, and few-shot vector font synthesis,
markedly outperforming the state of the art.",None,-1
410e36f5-13af-48a6-9504-dc5d5d9efde9,Simple diffusion: End-to-end diffusion for high resolution images,0.999643,"Currently, applying diffusion models in pixel space of high resolution images
is difficult. Instead, existing approaches focus on diffusion in lower
dimensional spaces (latent diffusion), or have multiple super-resolution levels
of generation referred to as cascades. The downside is that these approaches
add additional complexity to the diffusion framework.
  This paper aims to improve denoising diffusion for high resolution images
while keeping the model as simple as possible. The paper is centered around the
research question: How can one train a standard denoising diffusion models on
high resolution images, and still obtain performance comparable to these
alternate approaches?
  The four main findings are: 1) the noise schedule should be adjusted for high
resolution images, 2) It is sufficient to scale only a particular part of the
architecture, 3) dropout should be added at specific locations in the
architecture, and 4) downsampling is an effective strategy to avoid high
resolution feature maps. Combining these simple yet effective techniques, we
achieve state-of-the-art on image generation among diffusion models without
sampling modifiers on ImageNet.",None,-1
98d763d3-0cc8-411c-bf32-058aad7bdbf2,Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon,0.452928,"Safety-critical cyber-physical systems require control strategies whose
worst-case performance is robust against adversarial disturbances and modeling
uncertainties. In this paper, we present a framework for approximate control
and learning in partially observed systems to minimize the worst-case
discounted cost over an infinite time horizon. We model disturbances to the
system as finite-valued uncertain variables with unknown probability
distributions. For problems with known system dynamics, we construct a dynamic
programming (DP) decomposition to compute the optimal control strategy. Our
first contribution is to define information states that improve the
computational tractability of this DP without loss of optimality. Then, we
describe a simplification for a class of problems where the incurred cost is
observable at each time instance. Our second contribution is defining an
approximate information state that can be constructed or learned directly from
observed data for problems with observable costs. We derive bounds on the
performance loss of the resulting approximate control strategy and illustrate
the effectiveness of our approach in partially observed decision-making
problems with a numerical example.",None,-1
45aa56f1-a0d7-4d5f-b0a8-40e167ad0a52,Approaches to Corpus Creation for Low-Resource Language Technology: the Case of Southern Kurdish and Laki,0.290177,"One of the major challenges that under-represented and endangered language
communities face in language technology is the lack or paucity of language
data. This is also the case of the Southern varieties of the Kurdish and Laki
languages for which very limited resources are available with insubstantial
progress in tools. To tackle this, we provide a few approaches that rely on the
content of local news websites, a local radio station that broadcasts content
in Southern Kurdish and fieldwork for Laki. In this paper, we describe some of
the challenges of such under-represented languages, particularly in writing and
standardization, and also, in retrieving sources of data and retro-digitizing
handwritten content to create a corpus for Southern Kurdish and Laki. In
addition, we study the task of language identification in light of the other
variants of Kurdish and Zaza-Gorani languages.",None,-1
674fd692-d933-47da-897b-35d099193013,Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models,0.706474,"The mission of open knowledge graph (KG) completion is to draw new findings
from known facts. Existing works that augment KG completion require either (1)
factual triples to enlarge the graph reasoning space or (2) manually designed
prompts to extract knowledge from a pre-trained language model (PLM),
exhibiting limited performance and requiring expensive efforts from experts. To
this end, we propose TAGREAL that automatically generates quality query prompts
and retrieves support information from large text corpora to probe knowledge
from PLM for KG completion. The results show that TAGREAL achieves
state-of-the-art performance on two benchmark datasets. We find that TAGREAL
has superb performance even with limited training data, outperforming existing
embedding-based, graph-based, and PLM-based methods.",None,-1
7dd16155-b22e-4ede-be01-c8560b6f2f07,GROOT: Learning to Follow Instructions by Watching Gameplay Videos,0.57947,"We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.",None,-1
46aff764-9b3e-47c8-93f8-da7e51b920a0,Foreground Object Search by Distilling Composite Image Feature,0.565502,"Foreground object search (FOS) aims to find compatible foreground objects for
a given background image, producing realistic composite image. We observe that
competitive retrieval performance could be achieved by using a discriminator to
predict the compatibility of composite image, but this approach has
unaffordable time cost. To this end, we propose a novel FOS method via
distilling composite feature (DiscoFOS). Specifically, the abovementioned
discriminator serves as teacher network. The student network employs two
encoders to extract foreground feature and background feature. Their
interaction output is enforced to match the composite image feature from the
teacher network. Additionally, previous works did not release their datasets,
so we contribute two datasets for FOS task: S-FOSD dataset with synthetic
composite images and R-FOSD dataset with real composite images. Extensive
experiments on our two datasets demonstrate the superiority of the proposed
method over previous approaches. The dataset and code are available at
https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD.",None,-1
1418e81b-2ddb-44a4-858a-18658f2e8a38,SPDER: Semiperiodic Damping-Enabled Object Representation,0.0797644,"We present a neural network architecture designed to naturally learn a
positional embedding and overcome the spectral bias towards lower frequencies
faced by conventional implicit neural representation networks. Our proposed
architecture, SPDER, is a simple MLP that uses an activation function composed
of a sinusoidal multiplied by a sublinear function, called the damping
function. The sinusoidal enables the network to automatically learn the
positional embedding of an input coordinate while the damping passes on the
actual coordinate value by preventing it from being projected down to within a
finite range of values. Our results indicate that SPDERs speed up training by
10x and converge to losses 1,500-50,000x lower than that of the
state-of-the-art for image representation. SPDER is also state-of-the-art in
audio representation. The superior representation capability allows SPDER to
also excel on multiple downstream tasks such as image super-resolution and
video frame interpolation. We provide intuition as to why SPDER significantly
improves fitting compared to that of other INR methods while requiring no
hyperparameter tuning or preprocessing.",None,-1
f7562af0-17e9-4bbe-b4eb-003b466fa4ec,External Language Model Integration for Factorized Neural Transducers,0.186699,"We propose an adaptation method for factorized neural transducers (FNT) with
external language models. We demonstrate that both neural and n-gram external
LMs add significantly more value when linearly interpolated with predictor
output compared to shallow fusion, thus confirming that FNT forces the
predictor to act like regular language models. Further, we propose a method to
integrate class-based n-gram language models into FNT framework resulting in
accuracy gains similar to a hybrid setup. We show average gains of 18% WERR
with lexical adaptation across various scenarios and additive gains of up to
60% WERR in one entity-rich scenario through a combination of class-based
n-gram and neural LMs.",None,-1
af32bf6b-ee42-49f7-89a6-7c30532ff01c,ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text,0.848816,"We present an overview of the ArAIEval shared task, organized as part of the
first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two
tasks over Arabic text: (i) persuasion technique detection, focusing on
identifying persuasion techniques in tweets and news articles, and (ii)
disinformation detection in binary and multiclass setups over tweets. A total
of 20 teams participated in the final evaluation phase, with 14 and 16 teams
participating in Tasks 1 and 2, respectively. Across both tasks, we observed
that fine-tuning transformer models such as AraBERT was at the core of the
majority of the participating systems. We provide a description of the task
setup, including a description of the dataset construction and the evaluation
setup. We further give a brief overview of the participating systems. All
datasets and evaluation scripts from the shared task are released to the
research community. (https://araieval.gitlab.io/) We hope this will enable
further research on these important tasks in Arabic.",None,-1
77157f50-e50f-4fdd-b2d9-53158d5b4194,Neural Airport Ground Handling,0.773387,"Airport ground handling (AGH) offers necessary operations to flights during
their turnarounds and is of great importance to the efficiency of airport
management and the economics of aviation. Such a problem involves the interplay
among the operations that leads to NP-hard problems with complex constraints.
Hence, existing methods for AGH are usually designed with massive domain
knowledge but still fail to yield high-quality solutions efficiently. In this
paper, we aim to enhance the solution quality and computation efficiency for
solving AGH. Particularly, we first model AGH as a multiple-fleet vehicle
routing problem (VRP) with miscellaneous constraints including precedence, time
windows, and capacity. Then we propose a construction framework that decomposes
AGH into sub-problems (i.e., VRPs) in fleets and present a neural method to
construct the routing solutions to these sub-problems. In specific, we resort
to deep learning and parameterize the construction heuristic policy with an
attention-based neural network trained with reinforcement learning, which is
shared across all sub-problems. Extensive experiments demonstrate that our
method significantly outperforms classic meta-heuristics, construction
heuristics and the specialized methods for AGH. Besides, we empirically verify
that our neural method generalizes well to instances with large numbers of
flights or varying parameters, and can be readily adapted to solve real-time
AGH with stochastic flight arrivals. Our code is publicly available at:
https://github.com/RoyalSkye/AGH.",None,-1
4825e777-1ee5-4d53-b5e8-a240bafad377,Masked Structural Growth for 2x Faster Language Model Pre-training,0.194552,"Accelerating large language model pre-training is a critical issue in present
research. In this paper, we focus on speeding up pre-training by progressively
growing from a small Transformer structure to a large one. There are two main
research problems associated with progressive growth: determining the optimal
growth schedule, and designing efficient growth operators. In terms of growth
schedule, the impact of each single dimension on a schedule's efficiency is
under-explored by existing work. Regarding the growth operators, existing
methods rely on the initialization of new weights to inherit knowledge, and
achieve only non-strict function preservation, limiting further improvements on
training dynamics. To address these issues, we propose Masked Structural Growth
(MSG), including (i) growth schedules involving all possible dimensions and
(ii) strictly function-preserving growth operators that is independent of the
initialization of new weights. Experiments show that MSG is significantly
faster than related work: we achieve up to 2.2x speedup in pre-training
different types of language models while maintaining comparable or better
downstream performances. Code is publicly available at
https://github.com/cofe-ai/MSG.",None,-1
c8c33474-d9f6-4487-a8e8-8b0689a7f8a3,Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration,0.757599,"Kendall's tau is frequently used to meta-evaluate how well machine
translation (MT) evaluation metrics score individual translations. Its focus on
pairwise score comparisons is intuitive but raises the question of how ties
should be handled, a gray area that has motivated different variants in the
literature. We demonstrate that, in settings like modern MT meta-evaluation,
existing variants have weaknesses arising from their handling of ties, and in
some situations can even be gamed. We propose instead to meta-evaluate metrics
with a version of pairwise accuracy that gives metrics credit for correctly
predicting ties, in combination with a tie calibration procedure that
automatically introduces ties into metric scores, enabling fair comparison
between metrics that do and do not predict ties. We argue and provide
experimental evidence that these modifications lead to fairer ranking-based
assessments of metric performance.",None,-1
143d49a6-ff65-4ce3-8709-32807a6d5b3c,Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT,0.980705,"Recently, ChatGPT has attracted great attention, as it can generate fluent
and high-quality responses to human inquiries. Several prior studies have shown
that ChatGPT attains remarkable generation ability compared with existing
models. However, the quantitative analysis of ChatGPT's understanding ability
has been given little attention. In this report, we explore the understanding
ability of ChatGPT by evaluating it on the most popular GLUE benchmark, and
comparing it with 4 representative fine-tuned BERT-style models. We find that:
1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT
outperforms all BERT models on inference tasks by a large margin; 3) ChatGPT
achieves comparable performance compared with BERT on sentiment analysis and
question-answering tasks. Additionally, by combining some advanced prompting
strategies, we show that the understanding ability of ChatGPT can be further
improved.",None,-1
fda009f2-bd1f-4fc3-9224-7ea5abf813ec,Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages,0.527058,"This work introduces Zambezi Voice, an open-source multilingual speech
resource for Zambian languages. It contains two collections of datasets:
unlabelled audio recordings of radio news and talk shows programs (160 hours)
and labelled data (over 80 hours) consisting of read speech recorded from text
sourced from publicly available literature books. The dataset is created for
speech recognition but can be extended to multilingual speech processing
research for both supervised and unsupervised learning approaches. To our
knowledge, this is the first multilingual speech dataset created for Zambian
languages. We exploit pretraining and cross-lingual transfer learning by
finetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build
end-to-end (E2E) speech recognition models for our baseline models. The dataset
is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be
accessed via https://github.com/unza-speech-lab/zambezi-voice .",None,-1
49aa567a-80f7-45a6-889d-cbbb7b7be718,S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction,0.637307,"In this work we introduce S-TREK, a novel local feature extractor that
combines a deep keypoint detector, which is both translation and rotation
equivariant by design, with a lightweight deep descriptor extractor. We train
the S-TREK keypoint detector within a framework inspired by reinforcement
learning, where we leverage a sequential procedure to maximize a reward
directly related to keypoint repeatability. Our descriptor network is trained
following a ""detect, then describe"" approach, where the descriptor loss is
evaluated only at those locations where keypoints have been selected by the
already trained detector. Extensive experiments on multiple benchmarks confirm
the effectiveness of our proposed method, with S-TREK often outperforming other
state-of-the-art methods in terms of repeatability and quality of the recovered
poses, especially when dealing with in-plane rotations.",None,-1
c93a8b67-735f-443c-8ed6-d6e3b33902b5,A Universal Question-Answering Platform for Knowledge Graphs,0.546348,"Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
  In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.",None,-1
676336e9-0231-498e-9024-d43241c9a384,Interpretable Visual Question Answering via Reasoning Supervision,0.0785989,"Transformer-based architectures have recently demonstrated remarkable
performance in the Visual Question Answering (VQA) task. However, such models
are likely to disregard crucial visual cues and often rely on multimodal
shortcuts and inherent biases of the language modality to predict the correct
answer, a phenomenon commonly referred to as lack of visual grounding. In this
work, we alleviate this shortcoming through a novel architecture for visual
question answering that leverages common sense reasoning as a supervisory
signal. Reasoning supervision takes the form of a textual justification of the
correct answer, with such annotations being already available on large-scale
Visual Common Sense Reasoning (VCR) datasets. The model's visual attention is
guided toward important elements of the scene through a similarity loss that
aligns the learned attention distributions guided by the question and the
correct reasoning. We demonstrate both quantitatively and qualitatively that
the proposed approach can boost the model's visual perception capability and
lead to performance increase, without requiring training on explicit grounding
annotations.",None,-1
91b84020-471f-4630-be51-79d592225579,Preference-conditioned Pixel-based AI Agent For Game Testing,0.544551,"The game industry is challenged to cope with increasing growth in demand and
game complexity while maintaining acceptable quality standards for released
games. Classic approaches solely depending on human efforts for quality
assurance and game testing do not scale effectively in terms of time and cost.
Game-testing AI agents that learn by interaction with the environment have the
potential to mitigate these challenges with good scalability properties on time
and costs. However, most recent work in this direction depends on game state
information for the agent's state representation, which limits generalization
across different game scenarios. Moreover, game test engineers usually prefer
exploring a game in a specific style, such as exploring the golden path.
However, current game testing AI agents do not provide an explicit way to
satisfy such a preference. This paper addresses these limitations by proposing
an agent design that mainly depends on pixel-based state observations while
exploring the environment conditioned on a user's preference specified by
demonstration trajectories. In addition, we propose an imitation learning
method that couples self-supervised and supervised learning objectives to
enhance the quality of imitation behaviors. Our agent significantly outperforms
state-of-the-art pixel-based game testing agents over exploration coverage and
test execution quality when evaluated on a complex open-world environment
resembling many aspects of real AAA games.",None,-1
69bc9c74-1fee-4eee-a3ee-140a4e5f6e05,Implicit Stacked Autoregressive Model for Video Prediction,0.656266,"Future frame prediction has been approached through two primary methods:
autoregressive and non-autoregressive. Autoregressive methods rely on the
Markov assumption and can achieve high accuracy in the early stages of
prediction when errors are not yet accumulated. However, their performance
tends to decline as the number of time steps increases. In contrast,
non-autoregressive methods can achieve relatively high performance but lack
correlation between predictions for each time step. In this paper, we propose
an Implicit Stacked Autoregressive Model for Video Prediction (IAM4VP), which
is an implicit video prediction model that applies a stacked autoregressive
method. Like non-autoregressive methods, stacked autoregressive methods use the
same observed frame to estimate all future frames. However, they use their own
predictions as input, similar to autoregressive methods. As the number of time
steps increases, predictions are sequentially stacked in the queue. To evaluate
the effectiveness of IAM4VP, we conducted experiments on three common future
frame prediction benchmark datasets and weather\&climate prediction benchmark
datasets. The results demonstrate that our proposed model achieves
state-of-the-art performance.",None,-1
24ad9c7b-a84a-437f-8076-c7a8b8c872ed,Generator-Retriever-Generator Approach for Open-Domain Question Answering,0.405064,"Open-domain question answering (QA) tasks usually require the retrieval of
relevant information from a large corpus to generate accurate answers. We
propose a novel approach called Generator-Retriever-Generator (GRG) that
combines document retrieval techniques with a large language model (LLM), by
first prompting the model to generate contextual documents based on a given
question. In parallel, a dual-encoder network retrieves documents that are
relevant to the question from an external corpus. The generated and retrieved
documents are then passed to the second LLM, which generates the final answer.
By combining document retrieval and LLM generation, our approach addresses the
challenges of open-domain QA, such as generating informative and contextually
relevant answers. GRG outperforms the state-of-the-art generate-then-read and
retrieve-then-read pipelines (GENREAD and RFiD) improving their performance by
at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,
respectively. We provide code, datasets, and checkpoints at
https://github.com/abdoelsayed2016/GRG.",None,-1
59233ae3-5399-4413-9667-0ef0cdf89426,Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste,0.223015,"Existing fashion datasets do not consider the multi-facts that cause a
consumer to like or dislike a fashion image. Even two consumers like a same
fashion image, they could like this image for total different reasons. In this
paper, we study the reason why a consumer like a certain fashion image. Towards
this goal, we introduce an interpretability dataset, Fashionpedia-taste,
consist of rich annotation to explain why a subject like or dislike a fashion
image from the following 3 perspectives: 1) localized attributes; 2) human
attention; 3) caption. Furthermore, subjects are asked to provide their
personal attributes and preference on fashion, such as personality and
preferred fashion brands. Our dataset makes it possible for researchers to
build computational models to fully understand and interpret human fashion
taste from different humanistic perspectives and modalities.",None,-1
feecf0f8-72ca-40b6-83c2-fe5ca96440a9,Real-Time High-Resolution Pedestrian Detection in Crowded Scenes via Parallel Edge Offloading,0.120082,"To identify dense and small-size pedestrians in surveillance systems,
high-resolution cameras are widely deployed, where high-resolution images are
captured and delivered to off-the-shelf pedestrian detection models. However,
given the highly computation-intensive workload brought by the high resolution,
the resource-constrained cameras fail to afford accurate inference in real
time. To address that, we propose Hode, an offloaded video analytic framework
that utilizes multiple edge nodes in proximity to expedite pedestrian detection
with high-resolution inputs. Specifically, Hode can intelligently split
high-resolution images into respective regions and then offload them to
distributed edge nodes to perform pedestrian detection in parallel. A
spatio-temporal flow filtering method is designed to enable context-aware
region partitioning, as well as a DRL-based scheduling algorithm to allow
accuracy-aware load balance among heterogeneous edge nodes. Extensive
evaluation results using realistic prototypes show that Hode can achieve up to
2.01% speedup with very mild accuracy loss.",None,-1
a6f38495-ba6d-4e7f-bd73-afb0ea203933,Surface Geometry Processing: An Efficient Normal-based Detail Representation,0.109906,"With the rapid development of high-resolution 3D vision applications, the
traditional way of manipulating surface detail requires considerable memory and
computing time. To address these problems, we introduce an efficient surface
detail processing framework in 2D normal domain, which extracts new normal
feature representations as the carrier of micro geometry structures that are
illustrated both theoretically and empirically in this article. Compared with
the existing state of the arts, we verify and demonstrate that the proposed
normal-based representation has three important properties, including detail
separability, detail transferability and detail idempotence. Finally, three new
schemes are further designed for geometric surface detail processing
applications, including geometric texture synthesis, geometry detail transfer,
and 3D surface super-resolution. Theoretical analysis and experimental results
on the latest benchmark dataset verify the effectiveness and versatility of our
normal-based representation, which accepts 30 times of the input surface
vertices but at the same time only takes 6.5% memory cost and 14.0% running
time in comparison with existing competing algorithms.",None,-1
23095326-b8af-4b2e-876b-421ca7c71759,"Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased",0.923882,"There is a recent trend of applying multi-agent reinforcement learning (MARL)
to train an agent that can cooperate with humans in a zero-shot fashion without
using any human data. The typical workflow is to first repeatedly run self-play
(SP) to build a policy pool and then train the final adaptive policy against
this pool. A crucial limitation of this framework is that every policy in the
pool is optimized w.r.t. the environment reward function, which implicitly
assumes that the testing partners of the adaptive policy will be precisely
optimizing the same reward function as well. However, human objectives are
often substantially biased according to their own preferences, which can differ
greatly from the environment reward. We propose a more general framework,
Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden
reward functions in the self-play objective. By approximating the reward space
as linear functions, HSP adopts an effective technique to generate an augmented
policy pool with biased policies. We evaluate HSP on the Overcooked benchmark.
Empirical results show that our HSP method produces higher rewards than
baselines when cooperating with learned human models, manually scripted
policies, and real humans. The HSP policy is also rated as the most assistive
policy based on human feedback.",None,-1
040554ef-0438-4a50-b1e8-56d93ec3022d,Pedestrian detection with high-resolution event camera,0.383415,"Despite the dynamic development of computer vision algorithms, the
implementation of perception and control systems for autonomous vehicles such
as drones and self-driving cars still poses many challenges. A video stream
captured by traditional cameras is often prone to problems such as motion blur
or degraded image quality due to challenging lighting conditions. In addition,
the frame rate - typically 30 or 60 frames per second - can be a limiting
factor in certain scenarios. Event cameras (DVS -- Dynamic Vision Sensor) are a
potentially interesting technology to address the above mentioned problems. In
this paper, we compare two methods of processing event data by means of deep
learning for the task of pedestrian detection. We used a representation in the
form of video frames, convolutional neural networks and asynchronous sparse
convolutional neural networks. The results obtained illustrate the potential of
event cameras and allow the evaluation of the accuracy and efficiency of the
methods used for high-resolution (1280 x 720 pixels) footage.",None,-1
b21da23c-f8ef-4b17-ac5c-47579f1e8eae,Improving Link Prediction in Social Networks Using Local and Global Features: A Clustering-based Approach,0.185646,"Link prediction problem has increasingly become prominent in many domains
such as social network analyses, bioinformatics experiments, transportation
networks, criminal investigations and so forth. A variety of techniques has
been developed for link prediction problem, categorized into 1) similarity
based approaches which study a set of features to extract similar nodes; 2)
learning based approaches which extract patterns from the input data; 3)
probabilistic statistical approaches which optimize a set of parameters to
establish a model which can best compute formation probability. However,
existing literatures lack approaches which utilize strength of each approach by
integrating them to achieve a much more productive one. To tackle the link
prediction problem, we propose an approach based on the combination of first
and second group methods; the existing studied works use just one of these
categories. Our two-phase developed method firstly determines new features
related to the position and dynamic behavior of nodes, which enforce the
approach more efficiency compared to approaches using mere measures. Then, a
subspace clustering algorithm is applied to group social objects based on the
computed similarity measures which differentiate the strength of clusters;
basically, the usage of local and global indices and the clustering information
plays an imperative role in our link prediction process. Some extensive
experiments held on real datasets including Facebook, Brightkite and HepTh
indicate good performances of our proposal method. Besides, we have
experimentally verified our approach with some previous techniques in the area
to prove the supremacy of ours.",None,-1
21412aad-71a0-4e2f-b5e0-efafbadcfa8f,NeRD: Neural field-based Demosaicking,0.359782,"We introduce NeRD, a new demosaicking method for generating full-color images
from Bayer patterns. Our approach leverages advancements in neural fields to
perform demosaicking by representing an image as a coordinate-based neural
network with sine activation functions. The inputs to the network are spatial
coordinates and a low-resolution Bayer pattern, while the outputs are the
corresponding RGB values. An encoder network, which is a blend of ResNet and
U-net, enhances the implicit neural representation of the image to improve its
quality and ensure spatial consistency through prior learning. Our experimental
results demonstrate that NeRD outperforms traditional and state-of-the-art
CNN-based methods and significantly closes the gap to transformer-based
methods.",None,-1
f620c79f-2690-4189-b3dc-88458a3241eb,Incorporating Unlabelled Data into Bayesian Neural Networks,0.745654,"Conventional Bayesian Neural Networks (BNNs) cannot leverage unlabelled data
to improve their predictions. To overcome this limitation, we introduce
Self-Supervised Bayesian Neural Networks, which use unlabelled data to learn
improved prior predictive distributions by maximising an evidence lower bound
during an unsupervised pre-training step. With a novel methodology developed to
better understand prior predictive distributions, we then show that
self-supervised prior predictives capture image semantics better than
conventional BNN priors. In our empirical evaluations, we see that
self-supervised BNNs offer the label efficiency of self-supervised methods and
the uncertainty estimates of Bayesian methods, particularly outperforming
conventional BNNs in low-to-medium data regimes.",None,-1
55216313-4c32-4a6b-ad86-690389bff7c7,Multi-modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models,0.817643,"Chain-of-thought (CoT) reasoning has exhibited impressive performance in
language models for solving complex tasks and answering questions. However,
many real-world questions require multi-modal information, such as text and
images. Previous research on multi-modal CoT has primarily focused on
extracting fixed image features from off-the-shelf vision models and then
fusing them with text using attention mechanisms. This approach has limitations
because these vision models were not designed for complex reasoning tasks and
do not align well with language thoughts. To overcome this limitation, we
introduce a novel approach for multi-modal CoT reasoning that utilizes latent
space learning via diffusion processes to generate effective image features
that align with language thoughts. Our method fuses image features and text
representations at a deep level and improves the complex reasoning ability of
multi-modal CoT. We demonstrate the efficacy of our proposed method on
multi-modal ScienceQA and machine translation benchmarks, achieving
state-of-the-art performance on ScienceQA. Overall, our approach offers a more
robust and effective solution for multi-modal reasoning in language models,
enhancing their ability to tackle complex real-world problems.",None,-1
f6581a67-c8a8-4d42-89cf-b7ff542a9f93,Learning Dense UV Completion for Human Mesh Recovery,0.402917,"Human mesh reconstruction from a single image is challenging in the presence
of occlusion, which can be caused by self, objects, or other humans. Existing
methods either fail to separate human features accurately or lack proper
supervision for feature completion. In this paper, we propose Dense Inpainting
Human Mesh Recovery (DIMR), a two-stage method that leverages dense
correspondence maps to handle occlusion. Our method utilizes a dense
correspondence map to separate visible human features and completes human
features on a structured UV map dense human with an attention-based feature
completion module. We also design a feature inpainting training procedure that
guides the network to learn from unoccluded features. We evaluate our method on
several datasets and demonstrate its superior performance under heavily
occluded scenarios compared to other methods. Extensive experiments show that
our method obviously outperforms prior SOTA methods on heavily occluded images
and achieves comparable results on the standard benchmarks (3DPW).",None,-1
3186ff07-ce4d-44a5-b404-15b35da421fe,GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,0.847993,"Relation extraction (RE) is a crucial task in natural language processing
(NLP) that aims to identify and classify relationships between entities
mentioned in text. In the financial domain, relation extraction plays a vital
role in extracting valuable information from financial documents, such as news
articles, earnings reports, and company filings. This paper describes our
solution to relation extraction on one such dataset REFinD. The dataset was
released along with shared task as a part of the Fourth Workshop on Knowledge
Discovery from Unstructured Data in Financial Services, co-located with SIGIR
2023. In this paper, we employed OpenAI models under the framework of
in-context learning (ICL). We utilized two retrieval strategies to find top K
relevant in-context learning demonstrations / examples from training data for a
given test example. The first retrieval mechanism, we employed, is a
learning-free dense retriever and the other system is a learning-based
retriever. We were able to achieve 3rd rank overall. Our best F1-score is
0.718.",None,-1
7e9a744b-57e1-4e1d-8af3-f36978ba6f7f,SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training with Adversarial Remarks,0.0375026,"Large Language Models (LLMs) can justify or critique their predictions
through discussions with other models or humans, thereby enriching their
intrinsic understanding of instances. While proactive discussions in the
inference phase have been shown to boost performance, such interactions have
not been extensively explored during the training phase. We hypothesize that
incorporating interactive discussions into the training process can enhance the
models' understanding and improve their reasoning and verbal expression
abilities during inference. This work introduces the SAIE framework, which
facilitates supportive and adversarial discussions between learner and partner
models. The learner model receives responses from the partner, and its
parameters are then updated based on this discussion. This dynamic adjustment
process continues throughout the training phase, responding to the evolving
outputs of the learner model. Our empirical evaluation across various tasks,
including math problems, commonsense reasoning, and multi-domain knowledge,
demonstrates that models fine-tuned with the SAIE framework outperform those
trained with conventional fine-tuning approaches. Furthermore, our method
enhances the models' reasoning capabilities, improving both individual and
multi-agent inference performance.",None,-1
a8dc18f3-706e-4666-8ebd-76db4e2e8082,Cross-Lingual Cross-Age Group Adaptation for Low-Resource Elderly Speech Emotion Recognition,0.269388,"Speech emotion recognition plays a crucial role in human-computer
interactions. However, most speech emotion recognition research is biased
toward English-speaking adults, which hinders its applicability to other
demographic groups in different languages and age groups. In this work, we
analyze the transferability of emotion recognition across three different
languages--English, Mandarin Chinese, and Cantonese; and 2 different age
groups--adults and the elderly. To conduct the experiment, we develop an
English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion,
and a Cantonese speech emotion dataset, YueMotion. This study concludes that
different language and age groups require specific speech features, thus making
cross-lingual inference an unsuitable method. However, cross-group data
augmentation is still beneficial to regularize the model, with linguistic
distance being a significant influence on cross-lingual transferability. We
release publicly release our code at https://github.com/HLTCHKUST/elderly_ser.",None,-1
04a400cf-ea24-4881-8e2d-fc7ef2aa7623,Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation,0.0892476,"Large Language Models (LLMs) have showcased impressive performance. However,
due to their inability to capture relationships among samples, these frozen
LLMs inevitably keep repeating similar mistakes. In this work, we propose our
Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving
their performance by learning from previous mistakes. Considering data arrives
sequentially, LLMs gradually accumulate rules from incorrect cases, forming a
rule collection. These rules are then utilized by the LLMs to avoid making
similar mistakes when processing subsequent inputs. Moreover, the rules remain
independent of the primary prompts, seamlessly complementing prompt design
strategies. Experimentally, we show that TRAN improves over recent baselines by
a large margin.",None,-1
8c9b63dc-760c-4fdb-aa6e-4afcba625f4c,Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games,0.745668,"Self-play (SP) is a popular multi-agent reinforcement learning (MARL)
framework for solving competitive games, where each agent optimizes policy by
treating others as part of the environment. Despite the empirical successes,
the theoretical properties of SP-based methods are limited to two-player
zero-sum games. However, for mixed cooperative-competitive games where agents
on the same team need to cooperate with each other, we can show a simple
counter-example where SP-based methods cannot converge to a global Nash
equilibrium (NE) with high probability. Alternatively, Policy-Space Response
Oracles (PSRO) is an iterative framework for learning NE, where the best
responses w.r.t. previous policies are learned in each iteration. PSRO can be
directly extended to mixed cooperative-competitive settings by jointly learning
team best responses with all convergence properties unchanged. However, PSRO
requires repeatedly training joint policies from scratch till convergence,
which makes it hard to scale to complex games. In this work, we develop a novel
algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both
frameworks. FXP simultaneously trains an SP-based main policy and a counter
population of best response policies. The main policy is trained by fictitious
self-play and cross-play against the counter population, while the counter
policies are trained as the best responses to the main policy's past versions.
We validate our method in matrix games and show that FXP converges to global
NEs while SP methods fail. We also conduct experiments in a gridworld domain,
where FXP achieves higher Elo ratings and lower exploitabilities than
baselines, and a more challenging football game, where FXP defeats SOTA models
with over 94% win rate.",None,-1
6ab26af8-ce70-432d-bf9f-89cfa6bf45c0,Prompt Algebra for Task Composition,0.0796564,"We investigate whether prompts learned independently for different tasks can
be later combined through prompt algebra to obtain a model that supports
composition of tasks. We consider Visual Language Models (VLM) with prompt
tuning as our base classifier and formally define the notion of prompt algebra.
We propose constrained prompt tuning to improve performance of the composite
classifier. In the proposed scheme, prompts are constrained to appear in the
lower dimensional subspace spanned by the basis vectors of the pre-trained
vocabulary. Further regularization is added to ensure that the learned prompt
is grounded correctly to the existing pre-trained vocabulary. We demonstrate
the effectiveness of our method on object classification and object-attribute
classification datasets. On average, our composite model obtains classification
accuracy within 2.5% of the best base model. On UTZappos it improves
classification accuracy over the best base model by 8.45% on average.",None,-1
fd8d8762-9e2a-48d6-8b0e-2142209eadb9,Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs,0.45925,"As foundation models continue to exponentially scale in size, efficient
methods of adaptation become increasingly critical. Parameter-efficient
fine-tuning (PEFT), a recent class of techniques that require only modifying a
small percentage of the model parameters, is currently the most popular method
for adapting large language models (LLMs). Several PEFT techniques have
recently been proposed with varying tradeoffs. We provide a comprehensive and
uniform benchmark of various PEFT techniques across a representative LLM, the
FLAN-T5 model, and evaluate model performance across different data scales of
classification and generation datasets. Based on this, we provide a framework
for choosing the optimal fine-tuning techniques given the task type and data
availability. Contrary to popular belief, we also empirically prove that PEFT
techniques converge slower than full tuning in low data scenarios, and posit
the amount of data required for PEFT methods to both perform well and converge
efficiently. Lastly, we further optimize these PEFT techniques by selectively
choosing which parts of the model to train, and find that these techniques can
be applied with significantly fewer parameters while maintaining and even
improving performance.",None,-1
2d3becbb-ae71-4142-acc9-8e8d158e69c7,Professional Basketball Player Behavior Synthesis via Planning with Diffusion,0.917915,"Dynamically planning in multi-agent systems has been explored to improve
decision-making in various domains. Professional basketball serves as a
compelling example of a dynamic spatio-temporal game, encompassing both
concealed strategic policies and decision-making. However, processing the
diverse on-court signals and navigating the vast space of potential actions and
outcomes makes it difficult for existing approaches to swiftly identify optimal
strategies in response to evolving circumstances. In this study, we first
formulate the sequential decision-making process as a conditional trajectory
generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis),
a method for enhancing player decision-making. We extend the state-of-the-art
generative model, diffusion probabilistic model, to learn challenging
multi-agent environmental dynamics from historical National Basketball
Association (NBA) player motion tracking data. To incorporate data-driven
strategies, an auxiliary value function is trained using the play-by-play data
with corresponding rewards acting as the plan guidance. To accomplish
reward-guided trajectory generation, conditional sampling is introduced to
condition the diffusion model on the value function and conduct
classifier-guided sampling. We validate the effectiveness of PLAYBEST via
comprehensive simulation studies from real-world data, contrasting the
generated trajectories and play strategies with those employed by professional
basketball teams. Our results reveal that the model excels at generating
high-quality basketball trajectories that yield efficient plays, surpassing
conventional planning techniques in terms of adaptability, flexibility, and
overall performance. Moreover, the synthesized play strategies exhibit a
remarkable alignment with professional tactics, highlighting the model's
capacity to capture the intricate dynamics of basketball games.",None,-1
c152b37c-03bf-4993-9a01-574ebe4fc915,Table-GPT: Table-tuned GPT for Diverse Table Tasks,0.560268,"Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable
abilities to follow diverse human instructions and perform a wide range of
tasks. However, when probing language models using a range of basic
table-understanding tasks, we observe that today's language models are still
sub-optimal in many table-related tasks, likely because they are pre-trained
predominantly on \emph{one-dimensional} natural-language texts, whereas
relational tables are \emph{two-dimensional} objects.
  In this work, we propose a new ""\emph{table-tuning}"" paradigm, where we
continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using
diverse table-tasks synthesized from real tables as training data, with the
goal of enhancing language models' ability to understand tables and perform
table tasks. We show that our resulting Table-GPT models demonstrate (1) better
\emph{table-understanding} capabilities, by consistently outperforming the
vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout
unseen tasks, and (2) strong \emph{generalizability}, in its ability to respond
to diverse human instructions to perform new table-tasks, in a manner similar
to GPT-3.5 and ChatGPT.",None,-1
88007850-a64a-4afc-9a60-6862efb63b06,Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation,0.158745,"Diffusion models are able to generate photorealistic images in arbitrary
scenes. However, when applying diffusion models to image translation, there
exists a trade-off between maintaining spatial structure and high-quality
content. Besides, existing methods are mainly based on test-time optimization
or fine-tuning model for each input image, which are extremely time-consuming
for practical applications. To address these issues, we propose a new approach
for flexible image translation by learning a layout-aware image condition
together with a text condition. Specifically, our method co-encodes images and
text into a new domain during the training phase. In the inference stage, we
can choose images/text or both as the conditions for each time step, which
gives users more flexible control over layout and content. Experimental
comparisons of our method with state-of-the-art methods demonstrate our model
performs best in both style image translation and semantic image translation
and took the shortest time.",None,-1
f23f29e5-2361-4e20-a4e7-2ba83d9ab670,ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games,0.28591,"In this work, we investigate the capacity of language models to generate
explicit, interpretable, and interactive world models of scientific and
common-sense reasoning tasks. We operationalize this as a task of generating
text games, expressed as hundreds of lines of Python code. To facilitate this
task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a
corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We
empirically demonstrate that GPT-4 can use these games as templates for
single-shot in-context learning, successfully producing runnable games on
unseen topics in 28% of cases. When allowed to self-reflect on program errors,
game runnability substantially increases to 57%. While evaluating simulation
fidelity is labor-intensive, we introduce a suite of automated metrics to
assess game fidelity, technical validity, adherence to task specifications, and
winnability, showing a high degree of agreement with expert human ratings. We
pose this as a challenge task to spur further development at the juncture of
world modeling and code generation.",None,-1
65a67e87-5873-489e-845f-695838dcc030,FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models,0.093711,"Using model weights pretrained on a high-resource language as a warm start
can reduce the need for data and compute to obtain high-quality language models
for other, especially low-resource, languages. However, if we want to use a new
tokenizer specialized for the target language, we cannot transfer the source
model's embedding matrix. In this paper, we propose FOCUS - Fast Overlapping
Token Combinations Using Sparsemax, a novel embedding initialization method
that initializes the embedding matrix effectively for a new tokenizer based on
information in the source model's embedding matrix. FOCUS represents newly
added tokens as combinations of tokens in the overlap of the source and target
vocabularies. The overlapping tokens are selected based on semantic similarity
in an auxiliary static token embedding space. We focus our study on using the
multilingual XLM-R as a source model and empirically show that FOCUS
outperforms random initialization and previous work in language modeling and on
a range of downstream tasks (NLI, QA, and NER).",None,-1
3b5d3ab3-c4a3-4e01-a9c0-5be29d6ffe1e,Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra,0.786505,"Neural Radiance Fields (NeRFs) are a very recent and very popular approach
for the problems of novel view synthesis and 3D reconstruction. A popular scene
representation used by NeRFs is to combine a uniform, voxel-based subdivision
of the scene with an MLP. Based on the observation that a (sparse) point cloud
of the scene is often available, this paper proposes to use an adaptive
representation based on tetrahedra obtained by Delaunay triangulation instead
of uniform subdivision or point-based representations. We show that such a
representation enables efficient training and leads to state-of-the-art
results. Our approach elegantly combines concepts from 3D geometry processing,
triangle-based rendering, and modern neural radiance fields. Compared to
voxel-based representations, ours provides more detail around parts of the
scene likely to be close to the surface. Compared to point-based
representations, our approach achieves better performance. The source code is
publicly available at: https://jkulhanek.com/tetra-nerf.",None,-1
83586ae9-94de-4774-9b36-8a6938870e10,Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation,0.286218,"Goal-oriented Script Generation is a new task of generating a list of steps
that can fulfill the given goal. In this paper, we propose to extend the task
from the perspective of cognitive theory. Instead of a simple flat structure,
the steps are typically organized hierarchically - Human often decompose a
complex task into subgoals, where each subgoal can be further decomposed into
steps. To establish the benchmark, we contribute a new dataset, propose several
baseline methods, and set up evaluation metrics. Both automatic and human
evaluation verify the high-quality of dataset, as well as the effectiveness of
incorporating subgoals into hierarchical script generation. Furthermore, We
also design and evaluate the model to discover subgoal, and find that it is a
bit more difficult to decompose the goals than summarizing from segmented
steps.",None,-1
7971403d-d6a7-494a-ae41-b7cd16027008,Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning,0.0311588,"We present a model of pragmatic language understanding, where utterances are
produced and understood by searching for regularized equilibria of signaling
games. In this model (which we call ReCo, for Regularized Conventions),
speakers and listeners search for contextually appropriate utterance--meaning
mappings that are both close to game-theoretically optimal conventions and
close to a shared, ''default'' semantics. By characterizing pragmatic
communication as equilibrium search, we obtain principled sampling algorithms
and formal guarantees about the trade-off between communicative success and
naturalness. Across several datasets capturing real and idealized human
judgments about pragmatic implicatures, ReCo matches or improves upon
predictions made by best response and rational speech act models of language
understanding.",None,-1
9fb440ba-072d-4711-9692-0d8be80bb835,Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning,0.548094,"Patients undergoing chest X-rays (CXR) often endure multiple lung diseases.
When evaluating a patient's condition, due to the complex pathologies, subtle
texture changes of different lung lesions in images, and patient condition
differences, radiologists may make uncertain even when they have experienced
long-term clinical training and professional guidance, which makes much noise
in extracting disease labels based on CXR reports. In this paper, we re-extract
disease labels from CXR reports to make them more realistic by considering
disease severity and uncertainty in classification. Our contributions are as
follows: 1. We re-extracted the disease labels with severity and uncertainty by
a rule-based approach with keywords discussed with clinical experts. 2. To
further improve the explainability of chest X-ray diagnosis, we designed a
multi-relationship graph learning method with an expert uncertainty-aware loss
function. 3. Our multi-relationship graph learning method can also interpret
the disease classification results. Our experimental results show that models
considering disease severity and uncertainty outperform previous
state-of-the-art methods.",None,-1
2ad5b8ff-121f-4d1f-bf87-c8fc912288c2,Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features,0.150725,"Detecting out-of-distribution (OOD) inputs is crucial for the safe deployment
of natural language processing (NLP) models. Though existing methods,
especially those based on the statistics in the feature space of fine-tuned
pre-trained language models (PLMs), are claimed to be effective, their
effectiveness on different types of distribution shifts remains underexplored.
In this work, we take the first step to comprehensively evaluate the mainstream
textual OOD detection methods for detecting semantic and non-semantic shifts.
We find that: (1) no existing method behaves well in both settings; (2)
fine-tuning PLMs on in-distribution data benefits detecting semantic shifts but
severely deteriorates detecting non-semantic shifts, which can be attributed to
the distortion of task-agnostic features. To alleviate the issue, we present a
simple yet effective general OOD score named GNOME that integrates the
confidence scores derived from the task-agnostic and task-specific
representations. Experiments show that GNOME works well in both semantic and
non-semantic shift scenarios, and further brings significant improvement on two
cross-task benchmarks where both kinds of shifts simultaneously take place. Our
code is available at https://github.com/lancopku/GNOME.",None,-1
3f452e4d-c6bc-487a-bdbe-9be6879e4192,Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation,0.204981,"Deriving sophisticated 3D motions from sparse keyframes is a particularly
challenging problem, due to continuity and exceptionally skeletal precision.
The action features are often derivable accurately from the full series of
keyframes, and thus, leveraging the global context with transformers has been a
promising data-driven embedding approach. However, existing methods are often
with inputs of interpolated intermediate frame for continuity using basic
interpolation methods with keyframes, which result in a trivial local minimum
during training. In this paper, we propose a novel framework to formulate
latent motion manifolds with keyframe-based constraints, from which the
continuous nature of intermediate token representations is considered.
Particularly, our proposed framework consists of two stages for identifying a
latent motion subspace, i.e., a keyframe encoding stage and an intermediate
token generation stage, and a subsequent motion synthesis stage to extrapolate
and compose motion data from manifolds. Through our extensive experiments
conducted on both the LaFAN1 and CMU Mocap datasets, our proposed method
demonstrates both superior interpolation accuracy and high visual similarity to
ground truth motions.",None,-1
7abf9ea8-ad67-4fee-b9f8-95241d406d07,ADD: An Automatic Desensitization Fisheye Dataset for Autonomous Driving,0.218728,"Autonomous driving systems require many images for analyzing the surrounding
environment. However, there is fewer data protection for private information
among these captured images, such as pedestrian faces or vehicle license
plates, which has become a significant issue. In this paper, in response to the
call for data security laws and regulations and based on the advantages of
large Field of View(FoV) of the fisheye camera, we build the first Autopilot
Desensitization Dataset, called ADD, and formulate the first
deep-learning-based image desensitization framework, to promote the study of
image desensitization in autonomous driving scenarios. The compiled dataset
consists of 650K images, including different face and vehicle license plate
information captured by the surround-view fisheye camera. It covers various
autonomous driving scenarios, including diverse facial characteristics and
license plate colors. Then, we propose an efficient multitask desensitization
network called DesCenterNet as a benchmark on the ADD dataset, which can
perform face and vehicle license plate detection and desensitization tasks.
Based on ADD, we further provide an evaluation criterion for desensitization
performance, and extensive comparison experiments have verified the
effectiveness and superiority of our method on image desensitization.",None,-1
03844230-e955-40b1-886f-466c60c0989a,Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions,0.187024,"Recent advances in Large Language Models (LLMs) have presented new
opportunities for integrating Artificial General Intelligence (AGI) into
biological research and education. This study evaluated the capabilities of
leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in
answering conceptual biology questions. The models were tested on a
108-question multiple-choice exam covering biology topics in molecular biology,
biological techniques, metabolic engineering, and synthetic biology. Among the
models, GPT-4 achieved the highest average score of 90 and demonstrated the
greatest consistency across trials with different prompts. The results
indicated GPT-4's proficiency in logical reasoning and its potential to aid
biology research through capabilities like data analysis, hypothesis
generation, and knowledge integration. However, further development and
validation are still required before the promise of LLMs in accelerating
biological discovery can be realized.",None,-1
ea8683d6-eff5-491c-a89c-d07b369d9514,RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing,0.823442,"Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at https://github.com/thomas0809/RxnScribe.",None,-1
17882a95-3b11-4b10-be08-4d610c8df7e5,Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches,0.751249,"The term emotion analysis in text subsumes various natural language
processing tasks which have in common the goal to enable computers to
understand emotions. Most popular is emotion classification in which one or
multiple emotions are assigned to a predefined textual unit. While such setting
is appropriate for identifying the reader's or author's emotion, emotion role
labeling adds the perspective of mentioned entities and extracts text spans
that correspond to the emotion cause. The underlying emotion theories agree on
one important point; that an emotion is caused by some internal or external
event and comprises several subcomponents, including the subjective feeling and
a cognitive evaluation. We therefore argue that emotions and events are related
in two ways. (1) Emotions are events; and this perspective is the fundament in
natural language processing for emotion role labeling. (2) Emotions are caused
by events; a perspective that is made explicit with research how to incorporate
psychological appraisal theories in NLP models to interpret events. These two
research directions, role labeling and (event-focused) emotion classification,
have by and large been tackled separately. In this paper, we contextualize both
perspectives and discuss open research questions.",None,-1
23202304-37ab-41fe-b2d9-fda552579672,HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution,0.872119,"The rise of large language models (LLMs) had a transformative impact on
search, ushering in a new era of search engines that are capable of generating
search results in natural language text, imbued with citations for supporting
sources. Building generative information-seeking models demands openly
accessible datasets, which currently remain lacking. In this paper, we
introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative
Retrieval for Information-seeking Dataset) for building end-to-end generative
information-seeking models that are capable of retrieving candidate quotes and
generating attributed explanations. Unlike recent efforts that focus on human
evaluation of black-box proprietary search engines, we built our dataset atop
the English subset of MIRACL, a publicly available information retrieval
dataset. HAGRID is constructed based on human and LLM collaboration. We first
automatically collect attributed explanations that follow an in-context
citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to
evaluate the LLM explanations based on two criteria: informativeness and
attributability. HAGRID serves as a catalyst for the development of
information-seeking models with better attribution capabilities.",None,-1
8d0cf620-d723-49d2-b491-847027a6f26c,eXplainable Artificial Intelligence (XAI) in aging clock models,0.50448,"eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the ""aging clocks"" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.",None,-1
3ecf2d5d-2262-4fb5-bac9-68a4d720f641,Exploiting Language Models as a Source of Knowledge for Cognitive Agents,0.694346,"Large language models (LLMs) provide capabilities far beyond sentence
completion, including question answering, summarization, and natural-language
inference. While many of these capabilities have potential application to
cognitive systems, our research is exploiting language models as a source of
task knowledge for cognitive agents, that is, agents realized via a cognitive
architecture. We identify challenges and opportunities for using language
models as an external knowledge source for cognitive systems and possible ways
to improve the effectiveness of knowledge extraction by integrating extraction
with cognitive architecture capabilities, highlighting with examples from our
recent work in this area.",None,-1
0dd4a5b2-d5a9-4d95-8b59-bc104b6ab009,A Reference-less Quality Metric for Automatic Speech Recognition via Contrastive-Learning of a Multi-Language Model with Self-Supervision,0.43102,"The common standard for quality evaluation of automatic speech recognition
(ASR) systems is reference-based metrics such as the Word Error Rate (WER),
computed using manual ground-truth transcriptions that are time-consuming and
expensive to obtain. This work proposes a multi-language referenceless quality
metric, which allows comparing the performance of different ASR models on a
speech dataset without ground truth transcriptions. To estimate the quality of
ASR hypotheses, a pre-trained language model (LM) is fine-tuned with
contrastive learning in a self-supervised learning manner. In experiments
conducted on several unseen test datasets consisting of outputs from top
commercial ASR engines in various languages, the proposed referenceless metric
obtains a much higher correlation with WER scores and their ranks than the
perplexity metric from the state-of-art multi-lingual LM in all experiments,
and also reduces WER by more than $7\%$ when used for ensembling hypotheses.
The fine-tuned model and experiments are made available for the
reproducibility: https://github.com/aixplain/NoRefER",None,-1
8dfc02af-462a-4c30-9312-52fe41a93c39,Confidence-Aware and Self-Supervised Image Anomaly Localisation,0.178878,"Universal anomaly detection still remains a challenging problem in machine
learning and medical image analysis. It is possible to learn an expected
distribution from a single class of normative samples, e.g., through epistemic
uncertainty estimates, auto-encoding models, or from synthetic anomalies in a
self-supervised way. The performance of self-supervised anomaly detection
approaches is still inferior compared to methods that use examples from known
unknown classes to shape the decision boundary. However, outlier exposure
methods often do not identify unknown unknowns. Here we discuss an improved
self-supervised single-class training strategy that supports the approximation
of probabilistic inference with loosen feature locality constraints. We show
that up-scaling of gradients with histogram-equalised images is beneficial for
recently proposed self-supervision tasks. Our method is integrated into several
out-of-distribution (OOD) detection models and we show evidence that our method
outperforms the state-of-the-art on various benchmark datasets.",None,-1
b9fa2297-a439-41f1-8f12-a2f356c099bf,Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization,0.442505,"This work investigates the effectiveness of different pseudonymization
techniques, ranging from rule-based substitutions to using pre-trained Large
Language Models (LLMs), on a variety of datasets and models used for two widely
used NLP tasks: text classification and summarization. Our work provides
crucial insights into the gaps between original and anonymized data (focusing
on the pseudonymization technique) and model quality and fosters future
research into higher-quality anonymization techniques to better balance the
trade-offs between data protection and utility preservation. We make our code,
pseudonymized datasets, and downstream models publicly available",None,-1
ee95dab2-35ef-4a36-8b1d-cb3d0b59943d,Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization,0.44432,"Multilingual sentence representations are the foundation for similarity-based
bitext mining, which is crucial for scaling multilingual neural machine
translation (NMT) system to more languages. In this paper, we introduce MuSR: a
one-for-all Multilingual Sentence Representation model that supports more than
220 languages. Leveraging billions of English-centric parallel corpora, we
train a multilingual Transformer encoder, coupled with an auxiliary Transformer
decoder, by adopting a multilingual NMT framework with CrossConST, a
cross-lingual consistency regularization technique proposed in Gao et al.
(2023). Experimental results on multilingual similarity search and bitext
mining tasks show the effectiveness of our approach. Specifically, MuSR
achieves superior performance over LASER3 (Heffernan et al., 2022) which
consists of 148 independent multilingual sentence encoders.",None,-1
ff2e8939-8cc0-4e45-8bfc-e205229ebdf5,Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis,0.829318,"Performances on standard 3D point cloud benchmarks have plateaued, resulting
in oversized models and complex network design to make a fractional
improvement. We present an alternative to enhance existing deep neural networks
without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter
(SN-Adapter). Building on any trained 3D network, we utilize its learned
encoding capability to extract features of the training dataset and summarize
them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter
retrieves k nearest neighbors (k-NN) from the pre-constructed spatial
prototypes and linearly interpolates the k-NN prediction with that of the
original 3D network. By providing complementary characteristics, the proposed
SN-Adapter serves as a plug-and-play module to economically improve performance
in a non-parametric manner. More importantly, our SN-Adapter can be effectively
generalized to various 3D tasks, including shape classification, part
segmentation, and 3D object detection, demonstrating its superiority and
robustness. We hope our approach could show a new perspective for point cloud
analysis and facilitate future research.",None,-1
05394c16-7376-4211-a6b2-01130f58b5ae,How much can ChatGPT really help Computational Biologists in Programming?,0.308766,"ChatGPT, a recently developed product by openAI, is successfully leaving its
mark as a multi-purpose natural language based chatbot. In this paper, we are
more interested in analyzing its potential in the field of computational
biology. A major share of work done by computational biologists these days
involve coding up bioinformatics algorithms, analyzing data, creating
pipelining scripts and even machine learning modeling and feature extraction.
This paper focuses on the potential influence (both positive and negative) of
ChatGPT in the mentioned aspects with illustrative examples from different
perspectives. Compared to other fields of computer science, computational
biology has - (1) less coding resources, (2) more sensitivity and bias issues
(deals with medical data) and (3) more necessity of coding assistance (people
from diverse background come to this field). Keeping such issues in mind, we
cover use cases such as code writing, reviewing, debugging, converting,
refactoring and pipelining using ChatGPT from the perspective of computational
biologists in this paper.",None,-1
e3aa484e-1727-4656-9a82-2f82bc7f6d59,Loss-Curvature Matching for Dataset Selection and Condensation,0.551799,"Training neural networks on a large dataset requires substantial
computational costs. Dataset reduction selects or synthesizes data instances
based on the large dataset, while minimizing the degradation in generalization
performance from the full dataset. Existing methods utilize the neural network
during the dataset reduction procedure, so the model parameter becomes
important factor in preserving the performance after reduction. By depending
upon the importance of parameters, this paper introduces a new reduction
objective, coined LCMat, which Matches the Loss Curvatures of the original
dataset and reduced dataset over the model parameter space, more than the
parameter point. This new objective induces a better adaptation of the reduced
dataset on the perturbed parameter region than the exact point matching.
Particularly, we identify the worst case of the loss curvature gap from the
local parameter region, and we derive the implementable upper bound of such
worst-case with theoretical analyses. Our experiments on both coreset selection
and condensation benchmarks illustrate that LCMat shows better generalization
performances than existing baselines.",None,-1
bf39bf12-668c-4c6a-90ee-6a20dcdc7f40,Prefix Propagation: Parameter-Efficient Tuning for Long Sequences,0.326133,"Parameter-efficient tuning aims to mitigate the large memory requirements of
adapting pretrained language models for downstream tasks. For example, one
popular method, prefix-tuning, prepends trainable tokens to sequences while
freezing the rest of the model's parameters. Although such models attain
comparable performance with fine-tuning when applied to sequences with short to
moderate lengths, we show their inferior performance when modelling long
sequences. To bridge this gap, we propose prefix-propagation, a simple but
effective approach that conditions prefixes on previous hidden states. We
empirically demonstrate that prefix-propagation outperforms prefix-tuning
across long-document tasks, while using 50% fewer parameters. To further
investigate the proposed architecture, we also show its advantage in
calibration, and perform additional study on its relationship with kernel
attention. To the best of our knowledge, this work is the first to focus on
parameter-efficient learning for long-sequence language tasks.",None,-1
d47d67bb-9955-4067-adae-ca2e250ed500,Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans,0.314388,"The spreading of attention has been proposed as a mechanism for how humans
group features to segment objects. However, such a mechanism has not yet been
implemented and tested in naturalistic images. Here, we leverage the feature
maps from self-supervised vision Transformers and propose a model of human
object-based attention spreading and segmentation. Attention spreads within an
object through the feature affinity signal between different patches of the
image. We also collected behavioral data on people grouping objects in natural
images by judging whether two dots are on the same object or on two different
objects. We found that our models of affinity spread that were built on feature
maps from the self-supervised Transformers showed significant improvement over
baseline and CNN based models on predicting reaction time patterns of humans,
despite not being trained on the task or with any other object labels. Our work
provides new benchmarks for evaluating models of visual representation learning
including Transformers.",None,-1
55a14660-8568-4d54-b8a3-8bf42886c950,Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition,0.234017,"Masked autoencoder (MAE) has attracted unprecedented attention and achieves
remarkable performance in many vision tasks. It reconstructs random masked
image patches (known as proxy task) during pretraining and learns meaningful
semantic representations that can be transferred to downstream tasks. However,
MAE has not been thoroughly explored in ultrasound imaging. In this work, we
investigate the potential of MAE for ultrasound image recognition. Motivated by
the unique property of ultrasound imaging in high noise-to-signal ratio, we
propose a novel deblurring MAE approach that incorporates deblurring into the
proxy task during pretraining. The addition of deblurring facilitates the
pretraining to better recover the subtle details presented in the ultrasound
images, thus improving the performance of the downstream classification task.
Our experimental results demonstrate the effectiveness of our deblurring MAE,
achieving state-of-the-art performance in ultrasound image classification.
Overall, our work highlights the potential of MAE for ultrasound image
recognition and presents a novel approach that incorporates deblurring to
further improve its effectiveness.",None,-1
94d25624-592c-4afb-b53f-416bf2e9f84d,Peer attention enhances student learning,0.0567606,"Human visual attention is susceptible to social influences. In education,
peer effects impact student learning, but their precise role in modulating
attention remains unclear. Our experiment (N=311) demonstrates that displaying
peer visual attention regions when students watch online course videos enhances
their focus and engagement. However, students retain adaptability in following
peer attention cues. Overall, guided peer attention improves learning
experiences and outcomes. These findings elucidate how peer visual attention
shapes students' gaze patterns, deepening understanding of peer influence on
learning. They also offer insights into designing adaptive online learning
interventions leveraging peer attention modelling to optimize student
attentiveness and success.",None,-1
74240bd5-128d-4eba-b940-3eb27b7ebdb1,ScaleDet: A Scalable Multi-Dataset Object Detector,0.324725,"Multi-dataset training provides a viable solution for exploiting
heterogeneous large-scale datasets without extra annotation cost. In this work,
we propose a scalable multi-dataset detector (ScaleDet) that can scale up its
generalization across datasets when increasing the number of training datasets.
Unlike existing multi-dataset learners that mostly rely on manual relabelling
efforts or sophisticated optimizations to unify labels across datasets, we
introduce a simple yet scalable formulation to derive a unified semantic label
space for multi-dataset training. ScaleDet is trained by visual-textual
alignment to learn the label assignment with label semantic similarities across
datasets. Once trained, ScaleDet can generalize well on any given upstream and
downstream datasets with seen and unseen classes. We conduct extensive
experiments using LVIS, COCO, Objects365, OpenImages as upstream datasets, and
13 datasets from Object Detection in the Wild (ODinW) as downstream datasets.
Our results show that ScaleDet achieves compelling strong model performance
with an mAP of 50.7 on LVIS, 58.8 on COCO, 46.8 on Objects365, 76.2 on
OpenImages, and 71.8 on ODinW, surpassing state-of-the-art detectors with the
same backbone.",None,-1
1b3e987a-35d8-4b3e-a248-a593b01ed829,"Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking",0.451024,"Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle
a variety of task-oriented dialogue domains without the cost of collecting
in-domain data. Existing works mainly study common data- or model-level
augmentation methods to enhance the generalization but fail to effectively
decouple the semantics of samples, limiting the zero-shot performance of DST.
In this paper, we present a simple and effective ""divide, conquer and combine""
solution, which explicitly disentangles the semantics of seen data, and
leverages the performance and robustness with the mixture-of-experts mechanism.
Specifically, we divide the seen data into semantically independent subsets and
train corresponding experts, the newly unseen samples are mapped and inferred
with mixture-of-experts with our designed ensemble inference. Extensive
experiments on MultiWOZ2.1 upon the T5-Adapter show our schema significantly
and consistently improves the zero-shot performance, achieving the SOTA on
settings without external knowledge, with only 10M trainable parameters1.",None,-1
1cbf15c7-d71e-44d7-be5a-d3d3975c2731,FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt,0.982352,"Currently, the construction of large language models in specific domains is
done by fine-tuning on a base model. Some models also incorporate knowledge
bases without the need for pre-training. This is because the base model already
contains domain-specific knowledge during the pre-training process. We build a
large language model for food testing. Unlike the above approach, a significant
amount of data in this domain exists in Scanning format for domain standard
documents. In addition, there is a large amount of untrained structured
knowledge. Therefore, we introduce an incremental pre-training step to inject
this knowledge into a large language model. In this paper, we propose a method
for handling structured knowledge and scanned documents in incremental
pre-training. To overcome the problem of machine hallucination, we constructe a
knowledge graph to serve as an external knowledge base for supporting retrieval
in the large language model. It is worth mentioning that this paper is a
technical report of our pre-release version, and we will report our specific
experimental data in future versions.",None,-1
3ffbc5b8-bbfa-4d77-bbba-84ed65df09b8,GH-Feat: Learning Versatile Generative Hierarchical Features from GANs,0.112184,"Recent years witness the tremendous success of generative adversarial
networks (GANs) in synthesizing photo-realistic images. GAN generator learns to
compose realistic images and reproduce the real data distribution. Through
that, a hierarchical visual feature with multi-level semantics spontaneously
emerges. In this work we investigate that such a generative feature learned
from image synthesis exhibits great potentials in solving a wide range of
computer vision tasks, including both generative ones and more importantly
discriminative ones. We first train an encoder by considering the pretrained
StyleGAN generator as a learned loss function. The visual features produced by
our encoder, termed as Generative Hierarchical Features (GH-Feat), highly align
with the layer-wise GAN representations, and hence describe the input image
adequately from the reconstruction perspective. Extensive experiments support
the versatile transferability of GH-Feat across a range of applications, such
as image editing, image processing, image harmonization, face verification,
landmark detection, layout prediction, image retrieval, etc. We further show
that, through a proper spatial expansion, our developed GH-Feat can also
facilitate fine-grained semantic segmentation using only a few annotations.
Both qualitative and quantitative results demonstrate the appealing performance
of GH-Feat.",None,-1
33836917-d83b-40ed-8452-bdf67054693c,Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks,0.79931,"Children possess the ability to learn multiple cognitive tasks sequentially,
which is a major challenge toward the long-term goal of artificial general
intelligence. Existing continual learning frameworks are usually applicable to
Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired,
energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning
mechanisms during child growth and development, we propose Dynamic Structure
Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive
continual learning. When learning a sequence of tasks, the DSD-SNN dynamically
assigns and grows new neurons to new tasks and prunes redundant neurons,
thereby increasing memory capacity and reducing computational overhead. In
addition, the overlapping shared structure helps to quickly leverage all
acquired knowledge to new tasks, empowering a single network capable of
supporting multiple incremental tasks (without the separate sub-network mask
for each task). We validate the effectiveness of the proposed model on multiple
class incremental learning and task incremental learning benchmarks. Extensive
experiments demonstrated that our model could significantly improve
performance, learning speed and memory capacity, and reduce computational
overhead. Besides, our DSD-SNN model achieves comparable performance with the
DNNs-based methods, and significantly outperforms the state-of-the-art (SOTA)
performance for existing SNNs-based continual learning methods.",None,-1
c463cceb-514e-43ce-adb5-4fe8f680a226,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,0.0906091,"Given video demonstrations and paired narrations of an at-home procedural
task such as changing a tire, we present an approach to extract the underlying
task structure -- relevant actions and their temporal dependencies -- via
action-centric task graphs. Learnt structured representations from our method,
Action Dynamics Task Graphs (ADTG), can then be used for understanding such
tasks in unseen videos of humans performing them. Furthermore, ADTG can enable
providing user-centric guidance to humans in these tasks, either for performing
them better or for learning new tasks. Specifically, we show how ADTG can be
used for: (1) tracking an ongoing task, (2) recommending next actions, and (3)
planning a sequence of actions to accomplish a procedural task. We compare
against state-of-the-art Neural Task Graph method and demonstrate substantial
gains on 18 procedural tasks from the CrossTask dataset, including 30.1%
improvement in task tracking accuracy and 20.3% accuracy gain in next action
prediction.",None,-1
6961b17a-b50f-4350-aaca-d612ec7280d3,Contrasting Linguistic Patterns in Human and LLM-Generated Text,0.465616,"We conduct a quantitative analysis contrasting human-written English news
text with comparable large language model (LLM) output from 4 LLMs from the
LLaMa family. Our analysis spans several measurable linguistic dimensions,
including morphological, syntactic, psychometric and sociolinguistic aspects.
The results reveal various measurable differences between human and
AI-generated texts. Among others, human texts exhibit more scattered sentence
length distributions, a distinct use of dependency and constituent types,
shorter constituents, and more aggressive emotions (fear, disgust) than
LLM-generated texts. LLM outputs use more numbers, symbols and auxiliaries
(suggesting objective language) than human texts, as well as more pronouns. The
sexist bias prevalent in human text is also expressed by LLMs.",None,-1
779abb35-8de6-4912-b3a2-6dfde7ea0636,Choice Fusion as Knowledge for Zero-Shot Dialogue State Tracking,0.0729697,"With the demanding need for deploying dialogue systems in new domains with
less cost, zero-shot dialogue state tracking (DST), which tracks user's
requirements in task-oriented dialogues without training on desired domains,
draws attention increasingly. Although prior works have leveraged
question-answering (QA) data to reduce the need for in-domain training in DST,
they fail to explicitly model knowledge transfer and fusion for tracking
dialogue states. To address this issue, we propose CoFunDST, which is trained
on domain-agnostic QA datasets and directly uses candidate choices of
slot-values as knowledge for zero-shot dialogue-state generation, based on a T5
pre-trained language model. Specifically, CoFunDST selects highly-relevant
choices to the reference context and fuses them to initialize the decoder to
constrain the model outputs. Our experimental results show that our proposed
model achieves outperformed joint goal accuracy compared to existing zero-shot
DST approaches in most domains on the MultiWOZ 2.1. Extensive analyses
demonstrate the effectiveness of our proposed approach for improving zero-shot
DST learning from QA.",None,-1
5f67229f-4a64-498c-93dd-2ea015895042,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,0.99808,"We present a method for reconstructing high-quality meshes of large unbounded
real-world scenes suitable for photorealistic novel view synthesis. We first
optimize a hybrid neural volume-surface scene representation designed to have
well-behaved level sets that correspond to surfaces in the scene. We then bake
this representation into a high-quality triangle mesh, which we equip with a
simple and fast view-dependent appearance model based on spherical Gaussians.
Finally, we optimize this baked representation to best reproduce the captured
viewpoints, resulting in a model that can leverage accelerated polygon
rasterization pipelines for real-time view synthesis on commodity hardware. Our
approach outperforms previous scene representations for real-time rendering in
terms of accuracy, speed, and power consumption, and produces high quality
meshes that enable applications such as appearance editing and physical
simulation.",None,-1
353b7ed9-5bfb-4615-81a5-2fba737b93e3,TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition,0.770044,"Emotion Support Conversation (ESC) is an emerging and challenging task with
the goal of reducing the emotional distress of people. Previous attempts fail
to maintain smooth transitions between utterances in ESC because they ignore to
grasp the fine-grained transition information at each dialogue turn. To solve
this problem, we propose to take into account turn-level state
\textbf{Trans}itions of \textbf{ESC} (\textbf{TransESC}) from three
perspectives, including semantics transition, strategy transition and emotion
transition, to drive the conversation in a smooth and natural way.
Specifically, we construct the state transition graph with a two-step way,
named transit-then-interact, to grasp such three types of turn-level transition
information. Finally, they are injected into the transition-aware decoder to
generate more engaging responses. Both automatic and human evaluations on the
benchmark dataset demonstrate the superiority of TransESC to generate more
smooth and effective supportive responses. Our source code is available at
\url{https://github.com/circle-hit/TransESC}.",None,-1
1ea03913-e1c5-452f-99d6-2ece277672f0,C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction,0.284628,"There is an emerging effort to combine the two popular 3D frameworks using
Multi-View Stereo (MVS) and Neural Implicit Surfaces (NIS) with a specific
focus on the few-shot / sparse view setting. In this paper, we introduce a
novel integration scheme that combines the multi-view stereo with neural signed
distance function representations, which potentially overcomes the limitations
of both methods. MVS uses per-view depth estimation and cross-view fusion to
generate accurate surfaces, while NIS relies on a common coordinate volume.
Based on this strategy, we propose to construct per-view cost frustum for finer
geometry estimation, and then fuse cross-view frustums and estimate the
implicit signed distance functions to tackle artifacts that are due to noise
and holes in the produced surface reconstruction. We further apply a cascade
frustum fusion strategy to effectively captures global-local information and
structural consistency. Finally, we apply cascade sampling and a
pseudo-geometric loss to foster stronger integration between the two
architectures. Extensive experiments demonstrate that our method reconstructs
robust surfaces and outperforms existing state-of-the-art methods.",None,-1
11874e53-3dda-426c-aa80-e1d152090a06,4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in varying outfits exhibiting large displacements,0.355813,"This work presents 4DHumanOutfit, a new dataset of densely sampled
spatio-temporal 4D human motion data of different actors, outfits and motions.
The dataset is designed to contain different actors wearing different outfits
while performing different motions in each outfit. In this way, the dataset can
be seen as a cube of data containing 4D motion sequences along 3 axes with
identity, outfit and motion. This rich dataset has numerous potential
applications for the processing and creation of digital humans, e.g. augmented
reality, avatar creation and virtual try on. 4DHumanOutfit is released for
research purposes at https://kinovis.inria.fr/4dhumanoutfit/. In addition to
image data and 4D reconstructions, the dataset includes reference solutions for
each axis. We present independent baselines along each axis that demonstrate
the value of these reference solutions for evaluation tasks.",None,-1
933116b0-5ea6-4f99-992c-cac06600bb35,Probabilistic Prompt Learning for Dense Prediction,0.560982,"Recent progress in deterministic prompt learning has become a promising
alternative to various downstream vision tasks, enabling models to learn
powerful visual representations with the help of pre-trained vision-language
models. However, this approach results in limited performance for dense
prediction tasks that require handling more complex and diverse objects, since
a single and deterministic description cannot sufficiently represent the entire
image. In this paper, we present a novel probabilistic prompt learning to fully
exploit the vision-language knowledge in dense prediction tasks. First, we
introduce learnable class-agnostic attribute prompts to describe universal
attributes across the object class. The attributes are combined with class
information and visual-context knowledge to define the class-specific textual
distribution. Text representations are sampled and used to guide the dense
prediction task using the probabilistic pixel-text matching loss, enhancing the
stability and generalization capability of the proposed method. Extensive
experiments on different dense prediction tasks and ablation studies
demonstrate the effectiveness of our proposed method.",None,-1
32c7fcde-cc46-4bde-b4d2-203341b4d42d,Unlocking Temporal Question Answering for Large Language Models Using Code Execution,0.266391,"Large language models (LLMs) have made significant progress in natural
language processing (NLP), and are utilized extensively in various
applications. Recent works, such as chain-of-thought (CoT), have shown that
intermediate reasoning steps can improve the performance of LLMs for complex
reasoning tasks, such as math problems and symbolic question-answering tasks.
However, we notice the challenge that LLMs face when it comes to temporal
reasoning. Our preliminary experiments show that generating intermediate
reasoning steps does not always boost the performance of complex temporal
question-answering tasks. Therefore, we propose a novel framework that combines
the extraction capability of LLMs and the logical reasoning capability of a
Python solver to tackle this issue. Extensive experiments and analysis
demonstrate the effectiveness of our framework in handling intricate time-bound
reasoning tasks.",None,-1
33a8a7f5-f820-4dbc-ab99-d6cc4097e000,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images,0.216436,"This paper introduces vox2vec - a contrastive method for self-supervised
learning (SSL) of voxel-level representations. vox2vec representations are
modeled by a Feature Pyramid Network (FPN): a voxel representation is a
concatenation of the corresponding feature vectors from different pyramid
levels. The FPN is pre-trained to produce similar representations for the same
voxel in different augmented contexts and distinctive representations for
different voxels. This results in unified multi-scale representations that
capture both global semantics (e.g., body part) and local semantics (e.g.,
different small organs or healthy versus tumor tissue). We use vox2vec to
pre-train a FPN on more than 6500 publicly available computed tomography
images. We evaluate the pre-trained representations by attaching simple heads
on top of them and training the resulting models for 22 segmentation tasks. We
show that vox2vec outperforms existing medical imaging SSL techniques in three
evaluation setups: linear and non-linear probing and end-to-end fine-tuning.
Moreover, a non-linear head trained on top of the frozen vox2vec
representations achieves competitive performance with the FPN trained from
scratch while having 50 times fewer trainable parameters. The code is available
at https://github.com/mishgon/vox2vec .",None,-1
2dddb45d-3fbf-4a0a-aeec-572e2fb7e2b8,RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition,0.542516,"Emotion recognition in conversation (ERC) has received increasing attention
from researchers due to its wide range of applications.As conversation has a
natural graph structure,numerous approaches used to model ERC based on graph
convolutional networks (GCNs) have yielded significant results.However,the
aggregation approach of traditional GCNs suffers from the node information
redundancy problem,leading to node discriminant information
loss.Additionally,single-layer GCNs lack the capacity to capture long-range
contextual information from the graph. Furthermore,the majority of approaches
are based on textual modality or stitching together different modalities,
resulting in a weak ability to capture interactions between modalities. To
address these problems, we present the relational bilevel aggregation graph
convolutional network (RBA-GCN), which consists of three modules: the graph
generation module (GGM), similarity-based cluster building module (SCBM) and
bilevel aggregation module (BiAM). First, GGM constructs a novel graph to
reduce the redundancy of target node information.Then,SCBM calculates the node
similarity in the target node and its structural neighborhood, where noisy
information with low similarity is filtered out to preserve the discriminant
information of the node. Meanwhile, BiAM is a novel aggregation method that can
preserve the information of nodes during the aggregation process. This module
can construct the interaction between different modalities and capture
long-range contextual information based on similarity clusters. On both the
IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a
2.17$\sim$5.21\% improvement over that of the most advanced method.Our code is
available at https://github.com/luftmenscher/RBA-GCN and our article with the
same name has been published in IEEE/ACM Transactions on Audio,Speech,and
Language Processing,vol.31,2023",None,-1
9b209324-c3d5-4b57-b83f-500e2b39fee4,Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri,0.718671,"Recent advancements in Digital Document Restoration (DDR) have led to
significant breakthroughs in analyzing highly damaged written artifacts. Among
those, there has been an increasing interest in applying Artificial
Intelligence techniques for virtually unwrapping and automatically detecting
ink on the Herculaneum papyri collection. This collection consists of
carbonized scrolls and fragments of documents, which have been digitized via
X-ray tomography to allow the development of ad-hoc deep learning-based DDR
solutions. In this work, we propose a modification of the Fast Fourier
Convolution operator for volumetric data and apply it in a segmentation
architecture for ink detection on the challenging Herculaneum papyri,
demonstrating its suitability via deep experimental analysis. To encourage the
research on this task and the application of the proposed operator to other
tasks involving volumetric data, we will release our implementation
(https://github.com/aimagelab/vffc)",None,-1
5640625d-4a69-4b10-af08-7d5136102805,Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning,0.189269,"Modern representation learning methods often struggle to adapt quickly under
non-stationarity because they suffer from catastrophic forgetting and decaying
plasticity. Such problems prevent learners from fast adaptation since they may
forget useful features or have difficulty learning new ones. Hence, these
methods are rendered ineffective for continual learning. This paper proposes
Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm
well-suited for continual learning agents. UPGD protects useful weights or
features from forgetting and perturbs less useful ones based on their
utilities. Our empirical results show that UPGD helps reduce forgetting and
maintain plasticity, enabling modern representation learning methods to work
effectively in continual learning.",None,-1
673fcf5e-1246-46ce-b214-ddb421e9b6f1,T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation,0.90464,"Despite the stunning ability to generate high-quality images by recent
text-to-image models, current approaches often struggle to effectively compose
objects with different attributes and relationships into a complex and coherent
scene. We propose T2I-CompBench, a comprehensive benchmark for open-world
compositional text-to-image generation, consisting of 6,000 compositional text
prompts from 3 categories (attribute binding, object relationships, and complex
compositions) and 6 sub-categories (color binding, shape binding, texture
binding, spatial relationships, non-spatial relationships, and complex
compositions). We further propose several evaluation metrics specifically
designed to evaluate compositional text-to-image generation and explore the
potential and limitations of multimodal LLMs for evaluation. We introduce a new
approach, Generative mOdel fine-tuning with Reward-driven Sample selection
(GORS), to boost the compositional text-to-image generation abilities of
pretrained text-to-image models. Extensive experiments and evaluations are
conducted to benchmark previous methods on T2I-CompBench, and to validate the
effectiveness of our proposed evaluation metrics and GORS approach. Project
page is available at https://karine-h.github.io/T2I-CompBench/.",None,-1
1788a401-9851-47db-aaed-26c8cc8ea901,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,0.81512,"Detecting factual errors in textual information, whether generated by large
language models (LLM) or curated by humans, is crucial for making informed
decisions. LLMs' inability to attribute their claims to external knowledge and
their tendency to hallucinate makes it difficult to rely on their responses.
Humans, too, are prone to factual errors in their writing. Since manual
detection and correction of factual errors is labor-intensive, developing an
automatic approach can greatly reduce human effort. We present FLEEK, a
prototype tool that automatically extracts factual claims from text, gathers
evidence from external knowledge sources, evaluates the factuality of each
claim, and suggests revisions for identified errors using the collected
evidence. Initial empirical evaluation on fact error detection (77-85\% F1)
shows the potential of FLEEK. A video demo of FLEEK can be found at
https://youtu.be/NapJFUlkPdQ.",None,-1
48bdebb1-d7e9-4a7b-8f93-c72a4b853eea,Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances,0.174354,"This paper presents a novel approach for text/speech-driven animation of a
photo-realistic head model based on blend-shape geometry, dynamic textures, and
neural rendering. Training a VAE for geometry and texture yields a parametric
model for accurate capturing and realistic synthesis of facial expressions from
a latent feature vector. Our animation method is based on a conditional CNN
that transforms text or speech into a sequence of animation parameters. In
contrast to previous approaches, our animation model learns
disentangling/synthesizing different acting-styles in an unsupervised manner,
requiring only phonetic labels that describe the content of training sequences.
For realistic real-time rendering, we train a U-Net that refines
rasterization-based renderings by computing improved pixel colors and a
foreground matte. We compare our framework qualitatively/quantitatively against
recent methods for head modeling as well as facial animation and evaluate the
perceived rendering/animation quality in a user-study, which indicates large
improvements compared to state-of-the-art approaches",None,-1
9486623a-317c-465e-86a1-4b614d467929,Anthropomorphization of AI: Opportunities and Risks,0.899339,"Anthropomorphization is the tendency to attribute human-like traits to
non-human entities. It is prevalent in many social contexts -- children
anthropomorphize toys, adults do so with brands, and it is a literary device.
It is also a versatile tool in science, with behavioral psychology and
evolutionary biology meticulously documenting its consequences. With widespread
adoption of AI systems, and the push from stakeholders to make it human-like
through alignment techniques, human voice, and pictorial avatars, the tendency
for users to anthropomorphize it increases significantly. We take a dyadic
approach to understanding this phenomenon with large language models (LLMs) by
studying (1) the objective legal implications, as analyzed through the lens of
the recent blueprint of AI bill of rights and the (2) subtle psychological
aspects customization and anthropomorphization. We find that anthropomorphized
LLMs customized for different user bases violate multiple provisions in the
legislative blueprint. In addition, we point out that anthropomorphization of
LLMs affects the influence they can have on their users, thus having the
potential to fundamentally change the nature of human-AI interaction, with
potential for manipulation and negative influence. With LLMs being
hyper-personalized for vulnerable groups like children and patients among
others, our work is a timely and important contribution. We propose a
conservative strategy for the cautious use of anthropomorphization to improve
trustworthiness of AI systems.",None,-1
6f301cd5-ec2f-40da-aed9-93595852c1bf,SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning,0.0621612,"Recently, commonsense reasoning in text generation has attracted much
attention. Generative commonsense reasoning is the task that requires machines,
given a group of keywords, to compose a single coherent sentence with
commonsense plausibility. While existing datasets targeting generative
commonsense reasoning focus on everyday scenarios, it is unclear how well
machines reason under specific geographical and temporal contexts. We formalize
this challenging task as SituatedGen, where machines with commonsense should
generate a pair of contrastive sentences given a group of keywords including
geographical or temporal entities. We introduce a corresponding English dataset
consisting of 8,268 contrastive sentence pairs, which are built upon several
existing commonsense reasoning benchmarks with minimal manual labor.
Experiments show that state-of-the-art generative language models struggle to
generate sentences with commonsense plausibility and still lag far behind human
performance. Our dataset is publicly available at
https://github.com/yunx-z/situated_gen.",None,-1
5679dbb4-5c4f-4c67-b28b-0cf87d623cc5,Evaluating the Performance of Large Language Models for Spanish Language in Undergraduate Admissions Exams,0.0597474,"This study evaluates the performance of large language models, specifically
GPT-3.5 and BARD (supported by Gemini Pro model), in undergraduate admissions
exams proposed by the National Polytechnic Institute in Mexico. The exams cover
Engineering/Mathematical and Physical Sciences, Biological and Medical
Sciences, and Social and Administrative Sciences. Both models demonstrated
proficiency, exceeding the minimum acceptance scores for respective academic
programs to up to 75% for some academic programs. GPT-3.5 outperformed BARD in
Mathematics and Physics, while BARD performed better in History and questions
related to factual information. Overall, GPT-3.5 marginally surpassed BARD with
scores of 60.94% and 60.42%, respectively.",None,-1
77d4d9fe-7f8b-4251-8285-bb5d9d04738a,4D Panoptic Segmentation as Invariant and Equivariant Field Prediction,0.506944,"In this paper, we develop rotation-equivariant neural networks for 4D
panoptic segmentation. 4D panoptic segmentation is a benchmark task for
autonomous driving that requires recognizing semantic classes and object
instances on the road based on LiDAR scans, as well as assigning temporally
consistent IDs to instances across time. We observe that the driving scenario
is symmetric to rotations on the ground plane. Therefore, rotation-equivariance
could provide better generalization and more robust feature learning.
Specifically, we review the object instance clustering strategies and restate
the centerness-based approach and the offset-based approach as the prediction
of invariant scalar fields and equivariant vector fields. Other sub-tasks are
also unified from this perspective, and different invariant and equivariant
layers are designed to facilitate their predictions. Through evaluation on the
standard 4D panoptic segmentation benchmark of SemanticKITTI, we show that our
equivariant models achieve higher accuracy with lower computational costs
compared to their non-equivariant counterparts. Moreover, our method sets the
new state-of-the-art performance and achieves 1st place on the SemanticKITTI 4D
Panoptic Segmentation leaderboard.",None,-1
0c5a3bc4-2d64-4282-9ef2-1c7fc26a87ed,Empathetic Response Generation via Emotion Cause Transition Graph,0.53496,"Empathetic dialogue is a human-like behavior that requires the perception of
both affective factors (e.g., emotion status) and cognitive factors (e.g.,
cause of the emotion). Besides concerning emotion status in early work, the
latest approaches study emotion causes in empathetic dialogue. These approaches
focus on understanding and duplicating emotion causes in the context to show
empathy for the speaker. However, instead of only repeating the contextual
causes, the real empathic response often demonstrate a logical and
emotion-centered transition from the causes in the context to those in the
responses. In this work, we propose an emotion cause transition graph to
explicitly model the natural transition of emotion causes between two adjacent
turns in empathetic dialogue. With this graph, the concept words of the emotion
causes in the next turn can be predicted and used by a specifically designed
concept-aware decoder to generate the empathic response. Automatic and human
experimental results on the benchmark dataset demonstrate that our method
produces more empathetic, coherent, informative, and specific responses than
existing models.",None,-1
f3226198-9a1c-4a8c-8170-e9ac2e7d6c43,"Better ""CMOS"" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution",0.350254,"Most of the existing blind image Super-Resolution (SR) methods assume that
the blur kernels are space-invariant. However, the blur involved in real
applications are usually space-variant due to object motion, out-of-focus,
etc., resulting in severe performance drop of the advanced SR methods. To
address this problem, we firstly introduce two new datasets with out-of-focus
blur, i.e., NYUv2-BSR and Cityscapes-BSR, to support further researches of
blind SR with space-variant blur. Based on the datasets, we design a novel
Cross-MOdal fuSion network (CMOS) that estimate both blur and semantics
simultaneously, which leads to improved SR results. It involves a feature
Grouping Interactive Attention (GIA) module to make the two modalities interact
more effectively and avoid inconsistency. GIA can also be used for the
interaction of other features because of the universality of its structure.
Qualitative and quantitative experiments compared with state-of-the-art methods
on above datasets and real-world images demonstrate the superiority of our
method, e.g., obtaining PSNR/SSIM by +1.91/+0.0048 on NYUv2-BSR than MANet.",None,-1
18917aa2-6a5a-492a-b755-0be963cf4f61,Redefining Digital Health Interfaces with Large Language Models,0.668982,"Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their adoption remains comparatively limited
due, in part, to challenges surrounding usability and trust. Large Language
Models (LLMs) have emerged as general-purpose models with the ability to
process complex information and produce human-quality text, presenting a wealth
of potential applications in healthcare. Directly applying LLMs in clinical
settings is not straightforward, however, with LLMs susceptible to providing
inconsistent or nonsensical answers. We demonstrate how LLM-based systems can
utilize external tools and provide a novel interface between clinicians and
digital technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLMs
in clinical settings such as hallucinations. We illustrate LLM-based interfaces
with the example of cardiovascular disease risk prediction. We develop a new
prognostic tool using automated machine learning and demonstrate how LLMs can
provide a unique interface to both our model and existing risk scores,
highlighting the benefit compared to traditional interfaces for digital tools.",None,-1
1f6b4854-cd32-4468-9c3d-caf8bf0eab6d,ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data,0.571867,"Compared with traditional RGB-only visual tracking, few datasets have been
constructed for RGB-D tracking. In this paper, we propose ARKitTrack, a new
RGB-D tracking dataset for both static and dynamic scenes captured by
consumer-grade LiDAR scanners equipped on Apple's iPhone and iPad. ARKitTrack
contains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total.
Along with the bounding box annotations and frame-level attributes, we also
annotate this dataset with 123.9K pixel-level target masks. Besides, the camera
intrinsic and camera pose of each frame are provided for future developments.
To demonstrate the potential usefulness of this dataset, we further present a
unified baseline for both box-level and pixel-level tracking, which integrates
RGB features with bird's-eye-view representations to better explore
cross-modality 3D geometry. In-depth empirical analysis has verified that the
ARKitTrack dataset can significantly facilitate RGB-D tracking and that the
proposed baseline method compares favorably against the state of the arts. The
code and dataset is available at https://arkittrack.github.io.",None,-1
a7655a26-ba02-4927-972d-0942b46e04fe,Analyzing Intentional Behavior in Autonomous Agents under Uncertainty,0.0990424,"Principled accountability for autonomous decision-making in uncertain
environments requires distinguishing intentional outcomes from negligent
designs from actual accidents. We propose analyzing the behavior of autonomous
agents through a quantitative measure of the evidence of intentional behavior.
We model an uncertain environment as a Markov Decision Process (MDP). For a
given scenario, we rely on probabilistic model checking to compute the ability
of the agent to influence reaching a certain event. We call this the scope of
agency. We say that there is evidence of intentional behavior if the scope of
agency is high and the decisions of the agent are close to being optimal for
reaching the event. Our method applies counterfactual reasoning to
automatically generate relevant scenarios that can be analyzed to increase the
confidence of our assessment. In a case study, we show how our method can
distinguish between 'intentional' and 'accidental' traffic collisions.",None,-1
bd4825a3-ceb1-47fe-a740-16b0b46b8c5b,Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking,0.968152,"The biomedical field relies heavily on concept linking in various areas such
as literature mining, graph alignment, information retrieval,
question-answering, data, and knowledge integration. Although large language
models (LLMs) have made significant strides in many natural language processing
tasks, their effectiveness in biomedical concept mapping is yet to be fully
explored. This research investigates a method that exploits the in-context
learning (ICL) capabilities of large models for biomedical concept linking. The
proposed approach adopts a two-stage retrieve-and-rank framework. Initially,
biomedical concepts are embedded using language models, and then embedding
similarity is utilized to retrieve the top candidates. These candidates'
contextual information is subsequently incorporated into the prompt and
processed by a large language model to re-rank the concepts. This approach
achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%
in chemical entity normalization, exhibiting a competitive performance relative
to supervised learning methods. Further, it showed a significant improvement,
with an over 20-point absolute increase in F1 score on an oncology matching
dataset. Extensive qualitative assessments were conducted, and the benefits and
potential shortcomings of using large language models within the biomedical
domain were discussed. were discussed.",None,-1
561bfc33-0046-4e99-bdfb-aff324ce6320,Semantic Change Detection for the Romanian Language,0.105282,"Automatic semantic change methods try to identify the changes that appear
over time in the meaning of words by analyzing their usage in diachronic
corpora. In this paper, we analyze different strategies to create static and
contextual word embedding models, i.e., Word2Vec and ELMo, on real-world
English and Romanian datasets. To test our pipeline and determine the
performance of our models, we first evaluate both word embedding models on an
English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a
Romanian dataset, and we underline different aspects of semantic changes in
this low-resource language, such as meaning acquisition and loss. The
experimental results show that, depending on the corpus, the most important
factors to consider are the choice of model and the distance to calculate a
score for detecting semantic change.",None,-1
b21042f0-3773-4695-a49b-b1f40c95210f,PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge,0.175991,"There can be numerous electronic components on a given PCB, making the task
of visual inspection to detect defects very time-consuming and prone to error,
especially at scale. There has thus been significant interest in automatic PCB
component detection, particularly leveraging deep learning. However, deep
neural networks typically require high computational resources, possibly
limiting their feasibility in real-world use cases in manufacturing, which
often involve high-volume and high-throughput detection with constrained edge
computing resource availability. As a result of an exploration of efficient
deep neural network architectures for this use case, we introduce PCBDet, an
attention condenser network design that provides state-of-the-art inference
throughput while achieving superior PCB component detection performance
compared to other state-of-the-art efficient architecture designs. Experimental
results show that PCBDet can achieve up to 2$\times$ inference speed-up on an
ARM Cortex A72 processor when compared to an EfficientNet-based design while
achieving $\sim$2-4\% higher mAP on the FICS-PCB benchmark dataset.",None,-1
6e070efa-a149-4885-a3ae-fd1e768eaf08,Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques,0.242769,"This paper investigates the transferability of debiasing techniques across
different languages within multilingual models. We examine the applicability of
these techniques in English, French, German, and Dutch. Using multilingual BERT
(mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is
not only feasible but also yields promising results. Surprisingly, our findings
reveal no performance disadvantages when applying these techniques to
non-English languages. Using translations of the CrowS-Pairs dataset, our
analysis identifies SentenceDebias as the best technique across different
languages, reducing bias in mBERT by an average of 13%. We also find that
debiasing techniques with additional pretraining exhibit enhanced cross-lingual
effectiveness for the languages included in the analyses, particularly in
lower-resource languages. These novel insights contribute to a deeper
understanding of bias mitigation in multilingual language models and provide
practical guidance for debiasing techniques in different language contexts.",None,-1
1c7956c7-dc03-4db7-aa11-0ce71af84ce6,Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal Data,0.399462,"This paper addresses the ethical concerns arising from the use of
unauthorized public data in deep learning models and proposes a novel solution.
Specifically, building on the work of Huang et al. (2021), we extend their
bi-level optimization approach to generate unlearnable text using a
gradient-based search technique. However, although effective, this approach
faces practical limitations, including the requirement of batches of instances
and model architecture knowledge that is not readily accessible to ordinary
users with limited access to their own data. Furthermore, even with
semantic-preserving constraints, unlearnable noise can alter the text's
semantics. To address these challenges, we extract simple patterns from
unlearnable text produced by bi-level optimization and demonstrate that the
data remains unlearnable for unknown models. Additionally, these patterns are
not instance- or dataset-specific, allowing users to readily apply them to text
classification and question-answering tasks, even if only a small proportion of
users implement them on their public content. We also open-source codes to
generate unlearnable text and assess unlearnable noise to benefit the public
and future studies.",None,-1
44b66100-f44f-43e4-8fe1-dcf8d35ca594,Learning from Children: Improving Image-Caption Pretraining via Curriculum,0.0330592,"Image-caption pretraining has been quite successfully used for downstream
vision tasks like zero-shot image classification and object detection. However,
image-caption pretraining is still a hard problem -- it requires multiple
concepts (nouns) from captions to be aligned to several objects in images. To
tackle this problem, we go to the roots -- the best learner, children. We take
inspiration from cognitive science studies dealing with children's language
learning to propose a curriculum learning framework. The learning begins with
easy-to-align image caption pairs containing one concept per caption. The
difficulty is progressively increased with each new phase by adding one more
concept per caption. Correspondingly, the knowledge acquired in each learning
phase is utilized in subsequent phases to effectively constrain the learning
problem to aligning one new concept-object pair in each phase. We show that
this learning strategy improves over vanilla image-caption training in various
settings -- pretraining from scratch, using a pretrained image or/and
pretrained text encoder, low data regime etc.",None,-1
a23c9fa0-8f64-4e82-8215-ca0444fa0249,Fairness for Workers Who Pull the Arms: An Index Based Policy for Allocation of Restless Bandit Tasks,0.649635,"Motivated by applications such as machine repair, project monitoring, and
anti-poaching patrol scheduling, we study intervention planning of stochastic
processes under resource constraints. This planning problem has previously been
modeled as restless multi-armed bandits (RMAB), where each arm is an
intervention-dependent Markov Decision Process. However, the existing
literature assumes all intervention resources belong to a single uniform pool,
limiting their applicability to real-world settings where interventions are
carried out by a set of workers, each with their own costs, budgets, and
intervention effects. In this work, we consider a novel RMAB setting, called
multi-worker restless bandits (MWRMAB) with heterogeneous workers. The goal is
to plan an intervention schedule that maximizes the expected reward while
satisfying budget constraints on each worker as well as fairness in terms of
the load assigned to each worker. Our contributions are two-fold: (1) we
provide a multi-worker extension of the Whittle index to tackle heterogeneous
costs and per-worker budget and (2) we develop an index-based scheduling policy
to achieve fairness. Further, we evaluate our method on various cost structures
and show that our method significantly outperforms other baselines in terms of
fairness without sacrificing much in reward accumulated.",None,-1
22838a7a-6b4a-41b1-a88a-67595e60cbab,SSN: Stockwell Scattering Network for SAR Image Change Detection,0.554084,"Recently, synthetic aperture radar (SAR) image change detection has become an
interesting yet challenging direction due to the presence of speckle noise.
Although both traditional and modern learning-driven methods attempted to
overcome this challenge, deep convolutional neural networks (DCNNs)-based
methods are still hindered by the lack of interpretability and the requirement
of large computation power. To overcome this drawback, wavelet scattering
network (WSN) and Fourier scattering network (FSN) are proposed. Combining
respective merits of WSN and FSN, we propose Stockwell scattering network (SSN)
based on Stockwell transform which is widely applied against noisy signals and
shows advantageous characteristics in speckle reduction. The proposed SSN
provides noise-resilient feature representation and obtains state-of-art
performance in SAR image change detection as well as high computational
efficiency. Experimental results on three real SAR image datasets demonstrate
the effectiveness of the proposed method.",None,-1
a930dcba-aeef-4520-91e2-87dffd0a3d6e,Evaluating Hallucinations in Chinese Large Language Models,0.35762,"In this paper, we establish a benchmark named HalluQA (Chinese Hallucination
Question-Answering) to measure the hallucination phenomenon in Chinese large
language models. HalluQA contains 450 meticulously designed adversarial
questions, spanning multiple domains, and takes into account Chinese historical
culture, customs, and social phenomena. During the construction of HalluQA, we
consider two types of hallucinations: imitative falsehoods and factual errors,
and we construct adversarial samples based on GLM-130B and ChatGPT. For
evaluation, we design an automated evaluation method using GPT-4 to judge
whether a model output is hallucinated. We conduct extensive experiments on 24
large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk
and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than
50%. This indicates that HalluQA is highly challenging. We analyze the primary
types of hallucinations in different types of models and their causes.
Additionally, we discuss which types of hallucinations should be prioritized
for different types of models.",None,-1
6ab94c89-4fae-4afc-b911-4467e590f904,Weakly-supervised Single-view Image Relighting,0.382014,"We present a learning-based approach to relight a single image of Lambertian
and low-frequency specular objects. Our method enables inserting objects from
photographs into new scenes and relighting them under the new environment
lighting, which is essential for AR applications. To relight the object, we
solve both inverse rendering and re-rendering. To resolve the ill-posed inverse
rendering, we propose a weakly-supervised method by a low-rank constraint. To
facilitate the weakly-supervised training, we contribute Relit, a large-scale
(750K images) dataset of videos with aligned objects under changing
illuminations. For re-rendering, we propose a differentiable specular rendering
layer to render low-frequency non-Lambertian materials under various
illuminations of spherical harmonics. The whole pipeline is end-to-end and
efficient, allowing for a mobile app implementation of AR object insertion.
Extensive evaluations demonstrate that our method achieves state-of-the-art
performance. Project page: https://renjiaoyi.github.io/relighting/.",None,-1
e35377f6-fc01-45d1-8609-cb406aca524e,Feature Collapse,0.054159,"We formalize and study a phenomenon called feature collapse that makes
precise the intuitive idea that entities playing a similar role in a learning
task receive similar representations. As feature collapse requires a notion of
task, we leverage a simple but prototypical NLP task to study it. We start by
showing experimentally that feature collapse goes hand in hand with
generalization. We then prove that, in the large sample limit, distinct words
that play identical roles in this NLP task receive identical local feature
representations in a neural network. This analysis reveals the crucial role
that normalization mechanisms, such as LayerNorm, play in feature collapse and
in generalization.",None,-1
843e4816-6ef2-4df8-a99e-1527ef62b4a6,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,0.133424,"In recent years, many video tasks have achieved breakthroughs by utilizing
the vision transformer and establishing spatial-temporal decoupling for feature
extraction. Although multi-view 3D reconstruction also faces multiple images as
input, it cannot immediately inherit their success due to completely ambiguous
associations between unstructured views. There is not usable prior
relationship, which is similar to the temporally-coherence property in a video.
To solve this problem, we propose a novel transformer network for Unstructured
Multiple Images (UMIFormer). It exploits transformer blocks for decoupled
intra-view encoding and designed blocks for token rectification that mine the
correlation between similar tokens from different views to achieve decoupled
inter-view encoding. Afterward, all tokens acquired from various branches are
compressed into a fixed-size compact representation while preserving rich
information for reconstruction by leveraging the similarities between tokens.
We empirically demonstrate on ShapeNet and confirm that our decoupled learning
method is adaptable for unstructured multiple images. Meanwhile, the
experiments also verify our model outperforms existing SOTA methods by a large
margin. Code will be available at https://github.com/GaryZhu1996/UMIFormer.",None,-1
a164849c-f363-4c2d-b513-155eaeef2e23,SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues,0.816174,"In second language vocabulary learning, existing works have primarily focused
on either the learning interface or scheduling personalized retrieval practices
to maximize memory retention. However, the learning content, i.e., the
information presented on flashcards, has mostly remained constant. Keyword
mnemonic is a notable learning strategy that relates new vocabulary to existing
knowledge by building an acoustic and imagery link using a keyword that sounds
alike. Beyond that, producing verbal and visual cues associated with the
keyword to facilitate building these links requires a manual process and is not
scalable. In this paper, we explore an opportunity to use large language models
to automatically generate verbal and visual cues for keyword mnemonics. Our
approach, an end-to-end pipeline for auto-generating verbal and visual cues,
can automatically generate highly memorable cues. We investigate the
effectiveness of our approach via a human participant experiment by comparing
it with manually generated cues.",None,-1
33930b1b-10e4-44dd-99e8-b75d19d4d563,Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding,0.0695338,"There has been an increased interest in the integration of pretrained speech
recognition (ASR) and language models (LM) into the SLU framework. However,
prior methods often struggle with a vocabulary mismatch between pretrained
models, and LM cannot be directly utilized as they diverge from its NLU
formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system
that effectively integrates ASR and LM subnetworks into the SLU formulation for
sequence generation tasks. In the first pass, our architecture predicts ASR
transcripts using the ASR subnetwork. This is followed by the LM subnetwork,
which makes an initial SLU prediction. Finally, in the third pass, the
deliberation subnetwork conditions on representations from the ASR and LM
subnetworks to make the final prediction. Our proposed three-pass SLU system
shows improved performance over cascaded and E2E SLU models on two benchmark
SLU datasets, SLURP and SLUE, especially on acoustically challenging
utterances.",None,-1
b7ceceff-4100-4ec8-b523-d18730b39834,Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures,0.0222361,"During the past decade, Deep Learning (DL) algorithms, programming systems
and hardware have converged with the High Performance Computing (HPC)
counterparts. Nevertheless, the programming methodology of DL and HPC systems
is stagnant, relying on highly-optimized, yet platform-specific and inflexible
vendor-optimized libraries. Such libraries provide close-to-peak performance on
specific platforms, kernels and shapes thereof that vendors have dedicated
optimizations efforts, while they underperform in the remaining use-cases,
yielding non-portable codes with performance glass-jaws. This work introduces a
framework to develop efficient, portable DL and HPC kernels for modern CPU
architectures. We decompose the kernel development in two steps: 1) Expressing
the computational core using Tensor Processing Primitives (TPPs): a compact,
versatile set of 2D-tensor operators, 2) Expressing the logical loops around
TPPs in a high-level, declarative fashion whereas the exact instantiation
(ordering, tiling, parallelization) is determined via simple knobs. We
demonstrate the efficacy of our approach using standalone kernels and
end-to-end workloads that outperform state-of-the-art implementations on
diverse CPU platforms.",None,-1
104a271e-52d1-4248-ba86-344c628ee41a,JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models,0.999173,"Achieving human-like planning and control with multimodal observations in an
open world is a key milestone for more functional generalist agents. Existing
approaches can handle certain long-horizon tasks in an open world. However,
they still struggle when the number of open-world tasks could potentially be
infinite and lack the capability to progressively enhance task completion as
game time progresses. We introduce JARVIS-1, an open-world agent that can
perceive multimodal input (visual observations and human instructions),
generate sophisticated plans, and perform embodied control, all within the
popular yet challenging open-world Minecraft universe. Specifically, we develop
JARVIS-1 on top of pre-trained multimodal language models, which map visual
observations and textual instructions to plans. The plans will be ultimately
dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a
multimodal memory, which facilitates planning using both pre-trained knowledge
and its actual game survival experiences. JARVIS-1 is the existing most general
agent in Minecraft, capable of completing over 200 different tasks using
control and observation space similar to humans. These tasks range from
short-horizon tasks, e.g., ""chopping trees"" to long-horizon tasks, e.g.,
""obtaining a diamond pickaxe"". JARVIS-1 performs exceptionally well in
short-horizon tasks, achieving nearly perfect performance. In the classic
long-term task of $\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the
reliability of current state-of-the-art agents by 5 times and can successfully
complete longer-horizon and more challenging tasks. The project page is
available at https://craftjarvis.org/JARVIS-1",None,-1
41716c12-1ddd-46f4-993d-b6b9a739805f,RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction,0.676627,"Recently, neural implicit surfaces have become popular for multi-view
reconstruction. To facilitate practical applications like scene editing and
manipulation, some works extend the framework with semantic masks input for the
object-compositional reconstruction rather than the holistic perspective.
Though achieving plausible disentanglement, the performance drops significantly
when processing the indoor scenes where objects are usually partially observed.
We propose RICO to address this by regularizing the unobservable regions for
indoor compositional reconstruction. Our key idea is to first regularize the
smoothness of the occluded background, which then in turn guides the foreground
object reconstruction in unobservable regions based on the object-background
relationship. Particularly, we regularize the geometry smoothness of occluded
background patches. With the improved background surface, the signed distance
function and the reversedly rendered depth of objects can be optimized to bound
them within the background range. Extensive experiments show our method
outperforms other methods on synthetic and real-world indoor scenes and prove
the effectiveness of proposed regularizations. The code is available at
https://github.com/kyleleey/RICO.",None,-1
807a1fa7-a109-4cf7-9847-739c80319e1d,Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment,0.789691,"The egocentric and exocentric viewpoints of a human activity look
dramatically different, yet invariant representations to link them are
essential for many potential applications in robotics and augmented reality.
Prior work is limited to learning view-invariant features from paired
synchronized viewpoints. We relax that strong data assumption and propose to
learn fine-grained action features that are invariant to the viewpoints by
aligning egocentric and exocentric videos in time, even when not captured
simultaneously or in the same environment. To this end, we propose AE2, a
self-supervised embedding approach with two key designs: (1) an object-centric
encoder that explicitly focuses on regions corresponding to hands and active
objects; and (2) a contrastive-based alignment objective that leverages
temporally reversed frames as negative samples. For evaluation, we establish a
benchmark for fine-grained video understanding in the ego-exo context,
comprising four datasets -- including an ego tennis forehand dataset we
collected, along with dense per-frame labels we annotated for each dataset. On
the four datasets, our AE2 method strongly outperforms prior work in a variety
of fine-grained downstream tasks, both in regular and cross-view settings.",None,-1
2223b04a-e61b-469b-9a2b-820af28c8f30,Online Speculative Decoding,0.191866,"Speculative decoding is a pivotal technique to accelerate the inference of
large language models (LLMs) by employing a smaller draft model to predict the
target model's outputs. However, its efficacy can be limited due to the low
predictive accuracy of the draft model, particularly when faced with diverse
text inputs and a significant capability gap between the draft and target
models. We introduce online speculative decoding to address this challenge. The
main idea is to continuously update the (multiple) draft model(s) on observed
user query data. Adapting to query distribution mitigates the shifts between
the training distribution of the draft model and the query distribution,
enabling the draft model to more accurately predict the target model's outputs.
We develop a prototype of online speculative decoding based on knowledge
distillation and evaluate it using both synthetic and real query data. The
results show a substantial increase in the token acceptance rate by 0.1 to
0.65, bringing 1.42x to 2.17x latency reduction. Our code is available at
https://github.com/LiuXiaoxuanPKU/OSD.",None,-1
1bfb44ad-7df9-4767-ad48-df5bcfe11e9e,PixelRNN: In-pixel Recurrent Neural Networks for End-to-end-optimized Perception with Neural Sensors,0.671496,"Conventional image sensors digitize high-resolution images at fast frame
rates, producing a large amount of data that needs to be transmitted off the
sensor for further processing. This is challenging for perception systems
operating on edge devices, because communication is power inefficient and
induces latency. Fueled by innovations in stacked image sensor fabrication,
emerging sensor-processors offer programmability and minimal processing
capabilities directly on the sensor. We exploit these capabilities by
developing an efficient recurrent neural network architecture, PixelRNN, that
encodes spatio-temporal features on the sensor using purely binary operations.
PixelRNN reduces the amount of data to be transmitted off the sensor by a
factor of 64x compared to conventional systems while offering competitive
accuracy for hand gesture recognition and lip reading tasks. We experimentally
validate PixelRNN using a prototype implementation on the SCAMP-5
sensor-processor platform.",None,-1
da82b35c-3008-4d6c-89d6-3f371aac5a32,Progressively Optimized Local Radiance Fields for Robust View Synthesis,0.852064,"We present an algorithm for reconstructing the radiance field of a
large-scale scene from a single casually captured video. The task poses two
core challenges. First, most existing radiance field reconstruction approaches
rely on accurate pre-estimated camera poses from Structure-from-Motion
algorithms, which frequently fail on in-the-wild videos. Second, using a
single, global radiance field with finite representational capacity does not
scale to longer trajectories in an unbounded scene. For handling unknown poses,
we jointly estimate the camera poses with radiance field in a progressive
manner. We show that progressive optimization significantly improves the
robustness of the reconstruction. For handling large unbounded scenes, we
dynamically allocate new local radiance fields trained with frames within a
temporal window. This further improves robustness (e.g., performs well even
under moderate pose drifts) and allows us to scale to large scenes. Our
extensive evaluation on the Tanks and Temples dataset and our collected outdoor
dataset, Static Hikes, show that our approach compares favorably with the
state-of-the-art.",None,-1
01893210-dada-49c4-9029-ecaae60150af,Program-Aided Reasoners (better) Know What They Know,0.200977,"Prior work shows that program-aided reasoning, in which large language models
(LLMs) are combined with programs written in programming languages such as
Python, can significantly improve accuracy on various reasoning tasks. However,
while accuracy is essential, it is also important for such reasoners to ""know
what they know"", which can be quantified through the calibration of the model.
In this paper, we compare the calibration of Program Aided Language Models
(PAL) and text-based Chain-of-thought (COT) prompting techniques over 5
datasets and 2 model types: LLaMA models and OpenAI models. Our results
indicate that PAL leads to improved calibration in 75% of the instances. Our
analysis uncovers that prompting styles that produce lesser diversity in
generations also have more calibrated results, and thus we also experiment with
inducing lower generation diversity using temperature scaling and find that for
certain temperatures, PAL is not only more accurate but is also more calibrated
than COT. Overall, we demonstrate that, in the majority of cases, program-aided
reasoners better know what they know than text-based counterparts.",None,-1
13bd8223-b00d-49f8-af8d-7ee3a9e11a6a,When Prompt-based Incremental Learning Does Not Meet Strong Pretraining,0.658794,"Incremental learning aims to overcome catastrophic forgetting when learning
deep networks from sequential tasks. With impressive learning efficiency and
performance, prompt-based methods adopt a fixed backbone to sequential tasks by
learning task-specific prompts. However, existing prompt-based methods heavily
rely on strong pretraining (typically trained on ImageNet-21k), and we find
that their models could be trapped if the potential gap between the pretraining
task and unknown future tasks is large. In this work, we develop a learnable
Adaptive Prompt Generator (APG). The key is to unify the prompt retrieval and
prompt learning processes into a learnable prompt generator. Hence, the whole
prompting process can be optimized to reduce the negative effects of the gap
between tasks effectively. To make our APG avoid learning ineffective
knowledge, we maintain a knowledge pool to regularize APG with the feature
distribution of each class. Extensive experiments show that our method
significantly outperforms advanced methods in exemplar-free incremental
learning without (strong) pretraining. Besides, under strong retraining, our
method also has comparable performance to existing prompt-based models, showing
that our method can still benefit from pretraining. Codes can be found at
https://github.com/TOM-tym/APG",None,-1
8a4ae0fe-1742-417d-974f-251bb39cab2a,FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users,0.532479,"The federated learning (FL) technique was developed to mitigate data privacy
issues in the traditional machine learning paradigm. While FL ensures that a
user's data always remain with the user, the gradients are shared with the
centralized server to build the global model. This results in privacy leakage,
where the server can infer private information from the shared gradients. To
mitigate this flaw, the next-generation FL architectures proposed encryption
and anonymization techniques to protect the model updates from the server.
However, this approach creates other challenges, such as malicious users
sharing false gradients. Since the gradients are encrypted, the server is
unable to identify rogue users. To mitigate both attacks, this paper proposes a
novel FL algorithm based on a fully homomorphic encryption (FHE) scheme. We
develop a distributed multi-key additive homomorphic encryption scheme that
supports model aggregation in FL. We also develop a novel aggregation scheme
within the encrypted domain, utilizing users' non-poisoning rates, to
effectively address data poisoning attacks while ensuring privacy is preserved
by the proposed encryption scheme. Rigorous security, privacy, convergence, and
experimental analyses have been provided to show that FheFL is novel, secure,
and private, and achieves comparable accuracy at reasonable computational cost.",None,-1
41bd2e84-c5cb-4052-b912-1b38de07bd3a,Evaluation of AI Chatbots for Patient-Specific EHR Questions,0.361319,"This paper investigates the use of artificial intelligence chatbots for
patient-specific question answering (QA) from clinical notes using several
large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google
Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and
coherence of the answers generated by each model using a 5-point Likert scale
on a set of patient-specific questions.",None,-1
27277935-beb2-49fa-99df-2af4819c22c9,TBFormer: Two-Branch Transformer for Image Forgery Localization,0.566443,"Image forgery localization aims to identify forged regions by capturing
subtle traces from high-quality discriminative features. In this paper, we
propose a Transformer-style network with two feature extraction branches for
image forgery localization, and it is named as Two-Branch Transformer
(TBFormer). Firstly, two feature extraction branches are elaborately designed,
taking advantage of the discriminative stacked Transformer layers, for both RGB
and noise domain features. Secondly, an Attention-aware Hierarchical-feature
Fusion Module (AHFM) is proposed to effectively fuse hierarchical features from
two different domains. Although the two feature extraction branches have the
same architecture, their features have significant differences since they are
extracted from different domains. We adopt position attention to embed them
into a unified feature domain for hierarchical feature investigation. Finally,
a Transformer decoder is constructed for feature reconstruction to generate the
predicted mask. Extensive experiments on publicly available datasets
demonstrate the effectiveness of the proposed model.",None,-1
5d46ec9f-776b-4589-b315-f0ddf1ed4893,Designing Long-term Group Fair Policies in Dynamical Systems,0.152956,"Neglecting the effect that decisions have on individuals (and thus, on the
underlying data distribution) when designing algorithmic decision-making
policies may increase inequalities and unfairness in the long term - even if
fairness considerations were taken in the policy design process. In this paper,
we propose a novel framework for achieving long-term group fairness in
dynamical systems, in which current decisions may affect an individual's
features in the next step, and thus, future decisions. Specifically, our
framework allows us to identify a time-independent policy that converges, if
deployed, to the targeted fair stationary state of the system in the long term,
independently of the initial data distribution. We model the system dynamics
with a time-homogeneous Markov chain and optimize the policy leveraging the
Markov chain convergence theorem to ensure unique convergence. We provide
examples of different targeted fair states of the system, encompassing a range
of long-term goals for society and policymakers. Furthermore, we show how our
approach facilitates the evaluation of different long-term targets by examining
their impact on the group-conditional population distribution in the long term
and how it evolves until convergence.",None,-1
43c4a681-85dd-4882-9001-a29bd4b99759,Computability of Optimizers,0.0814393,"Optimization problems are a staple of today's scientific and technical
landscape. However, at present, solvers of such problems are almost exclusively
run on digital hardware. Using Turing machines as a mathematical model for any
type of digital hardware, in this paper, we analyze fundamental limitations of
this conceptual approach of solving optimization problems. Since in most
applications, the optimizer itself is of significantly more interest than the
optimal value of the corresponding function, we will focus on computability of
the optimizer. In fact, we will show that in various situations the optimizer
is unattainable on Turing machines and consequently on digital computers.
Moreover, even worse, there does not exist a Turing machine, which approximates
the optimizer itself up to a certain constant error. We prove such results for
a variety of well-known problems from very different areas, including
artificial intelligence, financial mathematics, and information theory, often
deriving the even stronger result that such problems are not Banach-Mazur
computable, also not even in an approximate sense.",None,-1
ac44a74b-feef-471e-8ccd-2e84fe2fd3e1,Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case,0.676124,"After the pandemic, artificial intelligence (AI) powered support for mental
health care has become increasingly important. The breadth and complexity of
significant challenges required to provide adequate care involve: (a)
Personalized patient understanding, (b) Safety-constrained and medically
validated chatbot patient interactions, and (c) Support for continued
feedback-based refinements in design using chatbot-patient interactions. We
propose Alleviate, a chatbot designed to assist patients suffering from mental
health challenges with personalized care and assist clinicians with
understanding their patients better. Alleviate draws from an array of publicly
available clinically valid mental-health texts and databases, allowing
Alleviate to make medically sound and informed decisions. In addition,
Alleviate's modular design and explainable decision-making lends itself to
robust and continued feedback-based refinements to its design. In this paper,
we explain the different modules of Alleviate and submit a short video
demonstrating Alleviate's capabilities to help patients and clinicians
understand each other better to facilitate optimal care strategies.",None,-1
08f552d8-438f-4654-8d8f-b43fcca3a834,SMPConv: Self-moving Point Representations for Continuous Convolution,0.758071,"Continuous convolution has recently gained prominence due to its ability to
handle irregularly sampled data and model long-term dependency. Also, the
promising experimental results of using large convolutional kernels have
catalyzed the development of continuous convolution since they can construct
large kernels very efficiently. Leveraging neural networks, more specifically
multilayer perceptrons (MLPs), is by far the most prevalent approach to
implementing continuous convolution. However, there are a few drawbacks, such
as high computational costs, complex hyperparameter tuning, and limited
descriptive power of filters. This paper suggests an alternative approach to
building a continuous convolution without neural networks, resulting in more
computationally efficient and improved performance. We present self-moving
point representations where weight parameters freely move, and interpolation
schemes are used to implement continuous functions. When applied to construct
convolutional kernels, the experimental results have shown improved performance
with drop-in replacement in the existing frameworks. Due to its lightweight
structure, we are first to demonstrate the effectiveness of continuous
convolution in a large-scale setting, e.g., ImageNet, presenting the
improvements over the prior arts. Our code is available on
https://github.com/sangnekim/SMPConv",None,-1
45aa21a6-f344-4251-9198-347382fdd2eb,Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives,0.255069,"We present information value, a measure which quantifies the predictability
of an utterance relative to a set of plausible alternatives. We introduce a
method to obtain interpretable estimates of information value using neural text
generators, and exploit their psychometric predictive power to investigate the
dimensions of predictability that drive human comprehension behaviour.
Information value is a stronger predictor of utterance acceptability in written
and spoken dialogue than aggregates of token-level surprisal and it is
complementary to surprisal for predicting eye-tracked reading times.",None,-1
23c41814-f6ce-4f13-a596-add875386fef,Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction,0.345673,"Continual few-shot relation extraction (RE) aims to continuously train a
model for new relations with few labeled training data, of which the major
challenges are the catastrophic forgetting of old relations and the overfitting
caused by data sparsity. In this paper, we propose a new model, namely SCKD, to
accomplish the continual few-shot RE task. Specifically, we design serial
knowledge distillation to preserve the prior knowledge from previous models and
conduct contrastive learning with pseudo samples to keep the representations of
samples in different relations sufficiently distinguishable. Our experiments on
two benchmark datasets validate the effectiveness of SCKD for continual
few-shot RE and its superiority in knowledge transfer and memory utilization
over state-of-the-art models.",None,-1
054359f1-7419-4566-b2a8-9cc8636d3511,Detecting Backdoors in Pre-trained Encoders,0.480161,"Self-supervised learning in computer vision trains on unlabeled data, such as
images or (image, text) pairs, to obtain an image encoder that learns
high-quality embeddings for input data. Emerging backdoor attacks towards
encoders expose crucial vulnerabilities of self-supervised learning, since
downstream classifiers (even further trained on clean data) may inherit
backdoor behaviors from encoders. Existing backdoor detection methods mainly
focus on supervised learning settings and cannot handle pre-trained encoders
especially when input labels are not available. In this paper, we propose
DECREE, the first backdoor detection approach for pre-trained encoders,
requiring neither classifier headers nor input labels. We evaluate DECREE on
over 400 encoders trojaned under 3 paradigms. We show the effectiveness of our
method on image encoders pre-trained on ImageNet and OpenAI's CLIP 400 million
image-text pairs. Our method consistently has a high detection accuracy even if
we have only limited or no access to the pre-training dataset.",None,-1
15d76b52-51a1-4fcd-a3bd-e1c012d37aad,Break It Down: Evidence for Structural Compositionality in Neural Networks,0.718099,"Though modern neural networks have achieved impressive performance in both
vision and language tasks, we know little about the functions that they
implement. One possibility is that neural networks implicitly break down
complex tasks into subroutines, implement modular solutions to these
subroutines, and compose them into an overall solution to a task - a property
we term structural compositionality. Another possibility is that they may
simply learn to match new inputs to learned templates, eliding task
decomposition entirely. Here, we leverage model pruning techniques to
investigate this question in both vision and language across a variety of
architectures, tasks, and pretraining regimens. Our results demonstrate that
models often implement solutions to subroutines via modular subnetworks, which
can be ablated while maintaining the functionality of other subnetworks. This
suggests that neural networks may be able to learn compositionality, obviating
the need for specialized symbolic mechanisms.",None,-1
8ff35999-ede5-4875-85b3-aaed8de7652c,Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models,0.289356,"Generating intermediate steps, or Chain of Thought (CoT), is an effective way
to significantly improve language models' (LM) multi-step reasoning capability.
However, the CoT lengths can grow rapidly with the problem complexity, easily
exceeding the maximum context size. Instead of increasing the context limit,
which has already been heavily investigated, we explore an orthogonal
direction: making LMs divide a problem into multiple contexts. We propose a new
inference framework, called Recursion of Thought (RoT), which introduces
several special tokens that the models can output to trigger context-related
operations. Extensive experiments with multiple architectures including GPT-3
show that RoT dramatically improves LMs' inference capability to solve
problems, whose solution consists of hundreds of thousands of tokens.",None,-1
35211d4b-aca0-4cbb-a3fc-470db90ab96a,A Novel Self-training Approach for Low-resource Speech Recognition,0.88048,"In this paper, we propose a self-training approach for automatic speech
recognition (ASR) for low-resource settings. While self-training approaches
have been extensively developed and evaluated for high-resource languages such
as English, their applications to low-resource languages like Punjabi have been
limited, despite the language being spoken by millions globally. The scarcity
of annotated data has hindered the development of accurate ASR systems,
especially for low-resource languages (e.g., Punjabi and M\=aori languages). To
address this issue, we propose an effective self-training approach that
generates highly accurate pseudo-labels for unlabeled low-resource speech. Our
experimental analysis demonstrates that our approach significantly improves
word error rate, achieving a relative improvement of 14.94% compared to a
baseline model across four real speech datasets. Further, our proposed approach
reports the best results on the Common Voice Punjabi dataset.",None,-1
6ef5f1a0-1b47-4fd3-b7b6-07dcddfd8e43,Real-time volumetric rendering of dynamic humans,0.113959,"We present a method for fast 3D reconstruction and real-time rendering of
dynamic humans from monocular videos with accompanying parametric body fits.
Our method can reconstruct a dynamic human in less than 3h using a single GPU,
compared to recent state-of-the-art alternatives that take up to 72h. These
speedups are obtained by using a lightweight deformation model solely based on
linear blend skinning, and an efficient factorized volumetric representation
for modeling the shape and color of the person in canonical pose. Moreover, we
propose a novel local ray marching rendering which, by exploiting standard GPU
hardware and without any baking or conversion of the radiance field, allows
visualizing the neural human on a mobile VR device at 40 frames per second with
minimal loss of visual quality. Our experimental evaluation shows superior or
competitive results with state-of-the art methods while obtaining large
training speedup, using a simple model, and achieving real-time rendering.",None,-1
312010a5-52b6-41e7-a26e-f8e5c67961e4,Enhancing Low Resource NER Using Assisting Language And Transfer Learning,0.507972,"Named Entity Recognition (NER) is a fundamental task in NLP that is used to
locate the key information in text and is primarily applied in conversational
and search systems. In commercial applications, NER or comparable slot-filling
methods have been widely deployed for popular languages. NER is used in
applications such as human resources, customer service, search engines, content
classification, and academia. In this paper, we draw focus on identifying name
entities for low-resource Indian languages that are closely related, like Hindi
and Marathi. We use various adaptations of BERT such as baseBERT, AlBERT, and
RoBERTa to train a supervised NER model. We also compare multilingual models
with monolingual models and establish a baseline. In this work, we show the
assisting capabilities of the Hindi and Marathi languages for the NER task. We
show that models trained using multiple languages perform better than a single
language. However, we also observe that blind mixing of all datasets doesn't
necessarily provide improvements and data selection methods may be required.",None,-1
09b1228c-842e-4f8e-8a63-4d2d22c91285,Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8,0.937686,"This paper introduces a software architecture for real-time object detection
using machine learning (ML) in an augmented reality (AR) environment. Our
approach uses the recent state-of-the-art YOLOv8 network that runs onboard on
the Microsoft HoloLens 2 head-mounted display (HMD). The primary motivation
behind this research is to enable the application of advanced ML models for
enhanced perception and situational awareness with a wearable, hands-free AR
platform. We show the image processing pipeline for the YOLOv8 model and the
techniques used to make it real-time on the resource-limited edge computing
platform of the headset. The experimental results demonstrate that our solution
achieves real-time processing without needing offloading tasks to the cloud or
any other external servers while retaining satisfactory accuracy regarding the
usual mAP metric and measured qualitative performance",None,-1
4d7d5702-8ab8-42e7-8923-2753a4b10ed2,Gloss Attention for Gloss-free Sign Language Translation,0.705029,"Most sign language translation (SLT) methods to date require the use of gloss
annotations to provide additional supervision information, however, the
acquisition of gloss is not easy. To solve this problem, we first perform an
analysis of existing models to confirm how gloss annotations make SLT easier.
We find that it can provide two aspects of information for the model, 1) it can
help the model implicitly learn the location of semantic boundaries in
continuous sign language videos, 2) it can help the model understand the sign
language video globally. We then propose \emph{gloss attention}, which enables
the model to keep its attention within video segments that have the same
semantics locally, just as gloss helps existing models do. Furthermore, we
transfer the knowledge of sentence-to-sentence similarity from the natural
language model to our gloss attention SLT network (GASLT) to help it understand
sign language videos at the sentence level. Experimental results on multiple
large-scale sign language datasets show that our proposed GASLT model
significantly outperforms existing methods. Our code is provided in
\url{https://github.com/YinAoXiong/GASLT}.",None,-1
882fd1e4-13ba-43fc-b67f-41378400539a,Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,0.707349,"Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies as well as BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to perform long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.",None,-1
c14a8e82-5b86-4d1e-9967-996f878378f3,Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,0.749447,"Facial expressions convey massive information and play a crucial role in
emotional expression. Deep neural network (DNN) accompanied by deep metric
learning (DML) techniques boost the discriminative ability of the model in
facial expression recognition (FER) applications. DNN, equipped with only
classification loss functions such as Cross-Entropy cannot compact intra-class
feature variation or separate inter-class feature distance as well as when it
gets fortified by a DML supporting loss item. The triplet center loss (TCL)
function is applied on all dimensions of the sample's embedding in the
embedding space. In our work, we developed three strategies: fully-synthesized,
semi-synthesized, and prediction-based negative sample selection strategies. To
achieve better results, we introduce a selective attention module that provides
a combination of pixel-wise and element-wise attention coefficients using
high-semantic deep features of input samples. We evaluated the proposed method
on the RAF-DB, a highly imbalanced dataset. The experimental results reveal
significant improvements in comparison to the baseline for all three negative
sample selection strategies.",None,-1
ca21baed-476c-4772-ace2-ded003616139,Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization,0.698806,"Recent computational approaches for combating online hate speech involve the
automatic generation of counter narratives by adapting Pretrained
Transformer-based Language Models (PLMs) with human-curated data. This process,
however, can produce in-domain overfitting, resulting in models generating
acceptable narratives only for hatred similar to training data, with little
portability to other targets or to real-world toxic language. This paper
introduces novel attention regularization methodologies to improve the
generalization capabilities of PLMs for counter narratives generation.
Overfitting to training-specific terms is then discouraged, resulting in more
diverse and richer narratives. We experiment with two attention-based
regularization techniques on a benchmark English dataset. Regularized models
produce better counter narratives than state-of-the-art approaches in most
cases, both in terms of automatic metrics and human evaluation, especially when
hateful targets are not present in the training data. This work paves the way
for better and more flexible counter-speech generation models, a task for which
datasets are highly challenging to produce.",None,-1
37ab9695-82f9-42d0-b3d5-c4520c00fa15,Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation,0.549953,"Very large language models (LLMs) perform extremely well on a spectrum of NLP
tasks in a zero-shot setting. However, little is known about their performance
on human-level NLP problems which rely on understanding psychological concepts,
such as assessing personality traits. In this work, we investigate the
zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'
social media posts. Through a set of systematic experiments, we find that
zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA
for broad classification upon injecting knowledge about the trait in the
prompts. However, when prompted to provide fine-grained classification, its
performance drops to close to a simple most frequent class (MFC) baseline. We
further analyze where GPT-3 performs better, as well as worse, than a
pretrained lexical model, illustrating systematic errors that suggest ways to
improve LLMs on human-level NLP tasks.",None,-1
1f96303e-e8a1-4a72-bb8a-ccd4e5dcf817,Attention-based Point Cloud Edge Sampling,0.828372,"Point cloud sampling is a less explored research topic for this data
representation. The most commonly used sampling methods are still classical
random sampling and farthest point sampling. With the development of neural
networks, various methods have been proposed to sample point clouds in a
task-based learning manner. However, these methods are mostly generative-based,
rather than selecting points directly using mathematical statistics. Inspired
by the Canny edge detection algorithm for images and with the help of the
attention mechanism, this paper proposes a non-generative Attention-based Point
cloud Edge Sampling method (APES), which captures salient points in the point
cloud outline. Both qualitative and quantitative experimental results show the
superior performance of our sampling method on common benchmark tasks.",None,-1
d4a2a229-e015-4087-9ec1-c304144ef8d3,Unifying Grokking and Double Descent,0.397364,"A principled understanding of generalization in deep learning may require
unifying disparate observations under a single conceptual framework. Previous
work has studied \emph{grokking}, a training dynamic in which a sustained
period of near-perfect training performance and near-chance test performance is
eventually followed by generalization, as well as the superficially similar
\emph{double descent}. These topics have so far been studied in isolation. We
hypothesize that grokking and double descent can be understood as instances of
the same learning dynamics within a framework of pattern learning speeds. We
propose that this framework also applies when varying model capacity instead of
optimization steps, and provide the first demonstration of model-wise grokking.",None,-1
85d75303-6ca1-414f-97fa-ab54f273a0d3,Self-improving object detection via disagreement reconciliation,0.0713157,"Object detectors often experience a drop in performance when new
environmental conditions are insufficiently represented in the training data.
This paper studies how to automatically fine-tune a pre-existing object
detector while exploring and acquiring images in a new environment without
relying on human intervention, i.e., in a self-supervised fashion. In our
setting, an agent initially explores the environment using a pre-trained
off-the-shelf detector to locate objects and associate pseudo-labels. By
assuming that pseudo-labels for the same object must be consistent across
different views, we devise a novel mechanism for producing refined predictions
from the consensus among observations. Our approach improves the off-the-shelf
object detector by 2.66% in terms of mAP and outperforms the current state of
the art without relying on ground-truth annotations.",None,-1
399410e2-95a7-47d9-aab7-905781e3d1be,"On the Planning, Search, and Memorization Capabilities of Large Language Models",0.00963941,"The rapid advancement of large language models, such as the Generative
Pre-trained Transformer (GPT) series, has had significant implications across
various disciplines. In this study, we investigate the potential of the
state-of-the-art large language model (GPT-4) for planning tasks. We explore
its effectiveness in multiple planning subfields, highlighting both its
strengths and limitations. Through a comprehensive examination, we identify
areas where large language models excel in solving planning problems and reveal
the constraints that limit their applicability. Our empirical analysis focuses
on GPT-4's performance in planning domain extraction, graph search path
planning, and adversarial planning. We then propose a way of fine-tuning a
domain-specific large language model to improve its Chain of Thought (CoT)
capabilities for the above-mentioned tasks. The results provide valuable
insights into the potential applications of large language models in the
planning domain and pave the way for future research to overcome their
limitations and expand their capabilities.",None,-1
fe304db5-2292-4aca-88c6-b919981eefa4,Do Language Models' Words Refer?,0.0226404,"What do language models (LMs) do with language? Everyone agrees that they can
produce sequences of (mostly) coherent strings of English. But do those
sentences mean something, or are LMs simply babbling in a convincing simulacrum
of language use? Here we will address one aspect of this broad question:
whether LMs' words can refer, that is, achieve ""word-to-world"" connections.
There is prima facie reason to think they do not since LMs do not interact with
the world in the way that ordinary language users do. Drawing on insights from
the externalist tradition in philosophy of language, we argue that those
appearances are misleading: even if the inputs to an LM are simply strings of
text, they are strings of text with natural histories, and that may suffice to
put LMs' words into referential contact with the external world.",None,-1
bb6ca5ae-4320-413a-bb5e-d4ade6d093ab,Video-P2P: Video Editing with Cross-attention Control,0.975588,"This paper presents Video-P2P, a novel framework for real-world video editing
with cross-attention control. While attention control has proven effective for
image editing with pre-trained image generation models, there are currently no
large-scale video generation models publicly available. Video-P2P addresses
this limitation by adapting an image generation diffusion model to complete
various video editing tasks. Specifically, we propose to first tune a
Text-to-Set (T2S) model to complete an approximate inversion and then optimize
a shared unconditional embedding to achieve accurate video inversion with a
small memory cost. For attention control, we introduce a novel
decoupled-guidance strategy, which uses different guidance strategies for the
source and target prompts. The optimized unconditional embedding for the source
prompt improves reconstruction ability, while an initialized unconditional
embedding for the target prompt enhances editability. Incorporating the
attention maps of these two branches enables detailed editing. These technical
designs enable various text-driven editing applications, including word swap,
prompt refinement, and attention re-weighting. Video-P2P works well on
real-world videos for generating new characters while optimally preserving
their original poses and scenes. It significantly outperforms previous
approaches.",None,-1
5d52b646-d9eb-4092-b992-66f9f3e4b33e,Independent Component Alignment for Multi-Task Learning,0.591255,"In a multi-task learning (MTL) setting, a single model is trained to tackle a
diverse set of tasks jointly. Despite rapid progress in the field, MTL remains
challenging due to optimization issues such as conflicting and dominating
gradients. In this work, we propose using a condition number of a linear system
of gradients as a stability criterion of an MTL optimization. We theoretically
demonstrate that a condition number reflects the aforementioned optimization
issues. Accordingly, we present Aligned-MTL, a novel MTL optimization approach
based on the proposed criterion, that eliminates instability in the training
process by aligning the orthogonal components of the linear system of
gradients. While many recent MTL approaches guarantee convergence to a minimum,
task trade-offs cannot be specified in advance. In contrast, Aligned-MTL
provably converges to an optimal point with pre-defined task-specific weights,
which provides more control over the optimization result. Through experiments,
we show that the proposed approach consistently improves performance on a
diverse set of MTL benchmarks, including semantic and instance segmentation,
depth estimation, surface normal estimation, and reinforcement learning. The
source code is publicly available at https://github.com/SamsungLabs/MTL .",None,-1
4d4e3828-f818-4686-be5c-847dcb5d4476,Importance Filtering with Risk Models for Complex Driving Situations,0.104502,"Self-driving cars face complex driving situations with a large amount of
agents when moving in crowded cities. However, some of the agents are actually
not influencing the behavior of the self-driving car. Filtering out unimportant
agents would inherently simplify the behavior or motion planning task for the
system. The planning system can then focus on fewer agents to find optimal
behavior solutions for the ego~agent. This is helpful especially in terms of
computational efficiency. In this paper, therefore, the research topic of
importance filtering with driving risk models is introduced. We give an
overview of state-of-the-art risk models and present newly adapted risk models
for filtering. Their capability to filter out surrounding unimportant agents is
compared in a large-scale experiment. As it turns out, the novel trajectory
distance balances performance, robustness and efficiency well. Based on the
results, we can further derive a novel filter architecture with multiple filter
steps, for which risk models are recommended for each step, to further improve
the robustness. We are confident that this will enable current behavior
planning systems to better solve complex situations in everyday driving.",None,-1
27e3b02a-267d-4c75-b955-063dff9d8bf1,Universal Morphology Control via Contextual Modulation,0.181193,"Learning a universal policy across different robot morphologies can
significantly improve learning efficiency and generalization in continuous
control. However, it poses a challenging multi-task reinforcement learning
problem, as the optimal policy may be quite different across robots and
critically depend on the morphology. Existing methods utilize graph neural
networks or transformers to handle heterogeneous state and action spaces across
different morphologies, but pay little attention to the dependency of a robot's
control policy on its morphology context. In this paper, we propose a
hierarchical architecture to better model this dependency via contextual
modulation, which includes two key submodules: (1) Instead of enforcing hard
parameter sharing across robots, we use hypernetworks to generate
morphology-dependent control parameters; (2) We propose a fixed attention
mechanism that solely depends on the morphology to modulate the interactions
between different limbs in a robot. Experimental results show that our method
not only improves learning performance on a diverse set of training robots, but
also generalizes better to unseen morphologies in a zero-shot fashion.",None,-1
2b6228c4-98f1-47a4-89ea-f163f600aec4,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,0.748095,"The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.",None,-1
53e058c8-f07e-419a-af10-5cfc73566c04,An Adaptive Kernel Approach to Federated Learning of Heterogeneous Causal Effects,0.399205,"We propose a new causal inference framework to learn causal effects from
multiple, decentralized data sources in a federated setting. We introduce an
adaptive transfer algorithm that learns the similarities among the data sources
by utilizing Random Fourier Features to disentangle the loss function into
multiple components, each of which is associated with a data source. The data
sources may have different distributions; the causal effects are independently
and systematically incorporated. The proposed method estimates the similarities
among the sources through transfer coefficients, and hence requiring no prior
information about the similarity measures. The heterogeneous causal effects can
be estimated with no sharing of the raw training data among the sources, thus
minimizing the risk of privacy leak. We also provide minimax lower bounds to
assess the quality of the parameters learned from the disparate sources. The
proposed method is empirically shown to outperform the baselines on
decentralized data sources with dissimilar distributions.",None,-1
1f8e9bc8-4dcd-499f-8e04-fff2384257cf,LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,0.985911,"We propose LLM-Eval, a unified multi-dimensional automatic evaluation method
for open-domain conversations with large language models (LLMs). Existing
evaluation methods often rely on human annotations, ground-truth responses, or
multiple LLM prompts, which can be expensive and time-consuming. To address
these issues, we design a single prompt-based evaluation method that leverages
a unified evaluation schema to cover multiple dimensions of conversation
quality in a single model call. We extensively evaluate the performance of
LLM-Eval on various benchmark datasets, demonstrating its effectiveness,
efficiency, and adaptability compared to state-of-the-art evaluation methods.
Our analysis also highlights the importance of choosing suitable LLMs and
decoding strategies for accurate evaluation results. LLM-Eval offers a
versatile and robust solution for evaluating open-domain conversation systems,
streamlining the evaluation process and providing consistent performance across
diverse scenarios.",None,-1
e5795310-d0f2-4553-9ddf-3ef67954f5d1,Drafting Event Schemas using Language Models,0.819449,"Past work has studied event prediction and event language modeling, sometimes
mediated through structured representations of knowledge in the form of event
schemas. Such schemas can lead to explainable predictions and forecasting of
unseen events given incomplete information. In this work, we look at the
process of creating such schemas to describe complex events. We use large
language models (LLMs) to draft schemas directly in natural language, which can
be further refined by human curators as necessary. Our focus is on whether we
can achieve sufficient diversity and recall of key events and whether we can
produce the schemas in a sufficiently descriptive style. We show that large
language models are able to achieve moderate recall against schemas taken from
two different datasets, with even better results when multiple prompts and
multiple samples are combined. Moreover, we show that textual entailment
methods can be used for both matching schemas to instances of events as well as
evaluating overlap between gold and predicted schemas. Our method paves the way
for easier distillation of event knowledge from large language model into
schemas.",None,-1
4cc01980-201d-4a53-b90a-630b13cfdca7,Reliability Scores from Saliency Map Clusters for Improved Image-based Harvest-Readiness Prediction in Cauliflower,0.748747,"Cauliflower is a hand-harvested crop that must fulfill high-quality standards
in sales making the timing of harvest important. However, accurately
determining harvest-readiness can be challenging due to the cauliflower head
being covered by its canopy. While deep learning enables automated
harvest-readiness estimation, errors can occur due to field-variability and
limited training data. In this paper, we analyze the reliability of a
harvest-readiness classifier with interpretable machine learning. By
identifying clusters of saliency maps, we derive reliability scores for each
classification result using knowledge about the domain and the image
properties. For unseen data, the reliability can be used to (i) inform farmers
to improve their decision-making and (ii) increase the model prediction
accuracy. Using RGB images of single cauliflower plants at different
developmental stages from the GrowliFlower dataset, we investigate various
saliency mapping approaches and find that they result in different quality of
reliability scores. With the most suitable interpretation tool, we adjust the
classification result and achieve a 15.72% improvement of the overall accuracy
to 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for
the GrowliFlower dataset.",None,-1
7f9d7464-f340-4325-bc66-80d23e719c93,Explicit Visual Prompting for Low-Level Structure Segmentations,0.999731,"We consider the generic problem of detecting low-level structures in images,
which includes segmenting the manipulated parts, identifying out-of-focus
pixels, separating shadow regions, and detecting concealed objects. Whereas
each such topic has been typically addressed with a domain-specific solution,
we show that a unified approach performs well across all of them. We take
inspiration from the widely-used pre-training and then prompt tuning protocols
in NLP and propose a new visual prompting model, named Explicit Visual
Prompting (EVP). Different from the previous visual prompting which is
typically a dataset-level implicit embedding, our key insight is to enforce the
tunable parameters focusing on the explicit visual content from each individual
image, i.e., the features from frozen patch embeddings and the input's
high-frequency components. The proposed EVP significantly outperforms other
parameter-efficient tuning protocols under the same amount of tunable
parameters (5.7% extra trainable parameters of each task). EVP also achieves
state-of-the-art performances on diverse low-level structure segmentation tasks
compared to task-specific solutions. Our code is available at:
https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",None,-1
2c8f002d-b17d-46db-92a3-eb0bd8e4a06a,"Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers",0.121976,"The rapid growth of scientific publications, particularly during the COVID-19
pandemic, emphasizes the need for tools to help researchers efficiently
comprehend the latest advancements. One essential part of understanding
scientific literature is research aspect classification, which categorizes
sentences in abstracts to Background, Purpose, Method, and Finding. In this
study, we investigate the impact of different datasets on model performance for
the crowd-annotated CODA-19 research aspect classification task. Specifically,
we explore the potential benefits of using the large, automatically curated
PubMed 200K RCT dataset and evaluate the effectiveness of large language models
(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that
using the PubMed 200K RCT dataset does not improve performance for the CODA-19
task. We also observe that while GPT-4 performs well, it does not outperform
the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance
of a dedicated and task-aligned datasets dataset for the target task. Our code
is available at https://github.com/Crowd-AI-Lab/CODA-19-exp.",None,-1
53bf3d47-8d26-4f52-bff0-f7444c4bf783,HyperAttack: Multi-Gradient-Guided White-box Adversarial Structure Attack of Hypergraph Neural Networks,0.204705,"Hypergraph neural networks (HGNN) have shown superior performance in various
deep learning tasks, leveraging the high-order representation ability to
formulate complex correlations among data by connecting two or more nodes
through hyperedge modeling. Despite the well-studied adversarial attacks on
Graph Neural Networks (GNN), there is few study on adversarial attacks against
HGNN, which leads to a threat to the safety of HGNN applications. In this
paper, we introduce HyperAttack, the first white-box adversarial attack
framework against hypergraph neural networks. HyperAttack conducts a white-box
structure attack by perturbing hyperedge link status towards the target node
with the guidance of both gradients and integrated gradients. We evaluate
HyperAttack on the widely-used Cora and PubMed datasets and three hypergraph
neural networks with typical hypergraph modeling techniques. Compared to
state-of-the-art white-box structural attack methods for GNN, HyperAttack
achieves a 10-20X improvement in time efficiency while also increasing attack
success rates by 1.3%-3.7%. The results show that HyperAttack can achieve
efficient adversarial attacks that balance effectiveness and time costs.",None,-1
49586f63-0878-40a1-9032-a7206385bb61,SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data,0.853472,"Text-to-SQL aims to automate the process of generating SQL queries on a
database from natural language text. In this work, we propose ""SQLPrompt"",
tailored to improve the few-shot prompting capabilities of Text-to-SQL for
Large Language Models (LLMs). Our methods include innovative prompt design,
execution-based consistency decoding strategy which selects the SQL with the
most consistent execution outcome among other SQL proposals, and a method that
aims to improve performance by diversifying the SQL proposals during
consistency selection with different prompt designs (""MixPrompt"") and
foundation models (""MixLLMs""). We show that \emph{SQLPrompt} outperforms
previous approaches for in-context learning with few labeled data by a large
margin, closing the gap with finetuning state-of-the-art with thousands of
labeled data.",None,-1
a6e523ed-0a9d-4108-b925-17fca1b5b7cf,"ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",0.997894,"This paper presents prompt design techniques for software engineering, in the
form of patterns, to solve common problems when using large language models
(LLMs), such as ChatGPT to automate common software engineering activities,
such as ensuring code is decoupled from third-party libraries and simulating a
web application API before it is implemented. This paper provides two
contributions to research on using LLMs for software engineering. First, it
provides a catalog of patterns for software engineering that classifies
patterns according to the types of problems they solve. Second, it explores
several prompt patterns that have been applied to improve requirements
elicitation, rapid prototyping, code quality, refactoring, and system design.",None,-1
4ba9ccde-1e9d-4e8c-b8fb-ee94c7f9e5fd,"An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM",0.184367,"Natural Language Processing (NLP) has emerged as a crucial technology for
understanding and generating human language, playing an essential role in tasks
such as machine translation, sentiment analysis, and more pertinently, question
classification. As a subfield within NLP, question classification focuses on
determining the type of information being sought, a fundamental step for
downstream applications like question answering systems. This study presents an
innovative ensemble approach for question classification, combining the
strengths of Electra, GloVe, and LSTM models. Rigorously tested on the
well-regarded TREC dataset, the model demonstrates how the integration of these
disparate technologies can lead to superior results. Electra brings in its
transformer-based capabilities for complex language understanding, GloVe offers
global vector representations for capturing word-level semantics, and LSTM
contributes its sequence learning abilities to model long-term dependencies. By
fusing these elements strategically, our ensemble model delivers a robust and
efficient solution for the complex task of question classification. Through
rigorous comparisons with well-known models like BERT, RoBERTa, and DistilBERT,
the ensemble approach verifies its effectiveness by attaining an 80% accuracy
score on the test dataset.",None,-1
59dc8b6f-35bc-4326-b33f-bf6463a303d2,LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing,0.583275,"As the interest in autonomous systems continues to grow, one of the major
challenges is collecting sufficient and representative real-world data. Despite
the strong practical and commercial interest in autonomous landing systems in
the aerospace field, there is a lack of open-source datasets of aerial images.
To address this issue, we present a dataset-lard-of high-quality aerial images
for the task of runway detection during approach and landing phases. Most of
the dataset is composed of synthetic images but we also provide manually
labelled images from real landing footages, to extend the detection task to a
more realistic setting. In addition, we offer the generator which can produce
such synthetic front-view images and enables automatic annotation of the runway
corners through geometric transformations. This dataset paves the way for
further research such as the analysis of dataset quality or the development of
models to cope with the detection tasks. Find data, code and more up-to-date
information at https://github.com/deel-ai/LARD",None,-1
1a6d330f-8c49-4e44-9dd1-56f8c4bb4358,Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning,0.0406323,"Prompt engineering and calibration make large language models excel at
reasoning tasks, including multiple choice commonsense reasoning. From a
practical perspective, we investigate and evaluate these strategies on smaller
language models. Through experiments on five commonsense reasoning benchmarks,
we find that each strategy favors certain models, but their joint effects are
mostly negative.",None,-1
31e4337e-25e8-4e28-bdfb-298795db5ae2,UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video,0.404555,"We show how to build a model that allows realistic, free-viewpoint renderings
of a scene under novel lighting conditions from video. Our method -- UrbanIR:
Urban Scene Inverse Rendering -- computes an inverse graphics representation
from the video. UrbanIR jointly infers shape, albedo, visibility, and sun and
sky illumination from a single video of unbounded outdoor scenes with unknown
lighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many
views of the same points in typical NeRF-style estimation). As a result,
standard methods produce poor geometry estimates (for example, roofs), and
there are numerous ''floaters''. Errors in inverse graphics inference can
result in strong rendering artifacts. UrbanIR uses novel losses to control
these and other sources of error. UrbanIR uses a novel loss to make very good
estimates of shadow volumes in the original scene. The resulting
representations facilitate controllable editing, delivering photorealistic
free-viewpoint renderings of relit scenes and inserted objects. Qualitative
evaluation demonstrates strong improvements over the state-of-the-art.",None,-1
891990fd-9ec1-47f1-8ccf-3387a2bd5e30,Visible-Infrared Person Re-Identification via Patch-Mixed Cross-Modality Learning,0.591006,"Visible-infrared person re-identification (VI-ReID) aims to retrieve images
of the same pedestrian from different modalities, where the challenges lie in
the significant modality discrepancy. To alleviate the modality gap, recent
methods generate intermediate images by GANs, grayscaling, or mixup strategies.
However, these methods could introduce extra data distribution, and the
semantic correspondence between the two modalities is not well learned. In this
paper, we propose a Patch-Mixed Cross-Modality framework (PMCM), where two
images of the same person from two modalities are split into patches and
stitched into a new one for model learning. A part-alignment loss is introduced
to regularize representation learning, and a patch-mixed modality learning loss
is proposed to align between the modalities. In this way, the model learns to
recognize a person through patches of different styles, thereby the modality
semantic correspondence can be inferred. In addition, with the flexible image
generation strategy, the patch-mixed images freely adjust the ratio of
different modality patches, which could further alleviate the modality
imbalance problem. On two VI-ReID datasets, we report new state-of-the-art
performance with the proposed method.",None,-1
914bbe7d-1888-4358-9a6e-21023f0f9b42,Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images,0.407987,"Object detection on drone images with low-latency is an important but
challenging task on the resource-constrained unmanned aerial vehicle (UAV)
platform. This paper investigates optimizing the detection head based on the
sparse convolution, which proves effective in balancing the accuracy and
efficiency. Nevertheless, it suffers from inadequate integration of contextual
information of tiny objects as well as clumsy control of the mask ratio in the
presence of foreground with varying scales. To address the issues above, we
propose a novel global context-enhanced adaptive sparse convolutional network
(CEASC). It first develops a context-enhanced group normalization (CE-GN)
layer, by replacing the statistics based on sparsely sampled features with the
global contextual ones, and then designs an adaptive multi-layer masking
strategy to generate optimal mask ratios at distinct scales for compact
foreground coverage, promoting both the accuracy and efficiency. Extensive
experimental results on two major benchmarks, i.e. VisDrone and UAVDT,
demonstrate that CEASC remarkably reduces the GFLOPs and accelerates the
inference procedure when plugging into the typical state-of-the-art detection
frameworks (e.g. RetinaNet and GFL V1) with competitive performance. Code is
available at https://github.com/Cuogeihong/CEASC.",None,-1
3ae59a0c-ceb4-47f8-a051-b98db95de0c3,VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking,0.999998,"3D object detectors usually rely on hand-crafted proxies, e.g., anchors or
centers, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel
features need to be densified and processed by dense prediction heads, which
inevitably costs extra computation. In this paper, we instead propose VoxelNext
for fully sparse 3D object detection. Our core insight is to predict objects
directly based on sparse voxel features, without relying on hand-crafted
proxies. Our strong sparse convolutional network VoxelNeXt detects and tracks
3D objects through voxel features entirely. It is an elegant and efficient
framework, with no need for sparse-to-dense conversion or NMS post-processing.
Our method achieves a better speed-accuracy trade-off than other mainframe
detectors on the nuScenes dataset. For the first time, we show that a fully
sparse voxel-based representation works decently for LIDAR 3D object detection
and tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2
benchmarks validate the effectiveness of our approach. Without bells and
whistles, our model outperforms all existing LIDAR methods on the nuScenes
tracking test benchmark.",None,-1
24211e47-bfd2-4c54-9f28-fe7688c114fa,From Model-Based to Data-Driven Simulation: Challenges and Trends in Autonomous Driving,0.752989,"Simulation is an integral part in the process of developing autonomous
vehicles and advantageous for training, validation, and verification of driving
functions. Even though simulations come with a series of benefits compared to
real-world experiments, various challenges still prevent virtual testing from
entirely replacing physical test-drives. Our work provides an overview of these
challenges with regard to different aspects and types of simulation and
subsumes current trends to overcome them. We cover aspects around perception-,
behavior- and content-realism as well as general hurdles in the domain of
simulation. Among others, we observe a trend of data-driven, generative
approaches and high-fidelity data synthesis to increasingly replace model-based
simulation.",None,-1
57da0ef6-0a13-4f52-805b-dde9430d980e,NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization,0.900466,"Monocular 3D object localization in driving scenes is a crucial task, but
challenging due to its ill-posed nature. Estimating 3D coordinates for each
pixel on the object surface holds great potential as it provides dense 2D-3D
geometric constraints for the underlying PnP problem. However, high-quality
ground truth supervision is not available in driving scenes due to sparsity and
various artifacts of Lidar data, as well as the practical infeasibility of
collecting per-instance CAD models. In this work, we present NeurOCS, a
framework that uses instance masks and 3D boxes as input to learn 3D object
shapes by means of differentiable rendering, which further serves as
supervision for learning dense object coordinates. Our approach rests on
insights in learning a category-level shape prior directly from real driving
scenes, while properly handling single-view ambiguities. Furthermore, we study
and make critical design choices to learn object coordinates more effectively
from an object-centric view. Altogether, our framework leads to new
state-of-the-art in monocular 3D localization that ranks 1st on the
KITTI-Object benchmark among published monocular methods.",None,-1
a8bc7142-638b-4380-b75d-809287185dc3,Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model,0.658321,"We study the incentivized information acquisition problem, where a principal
hires an agent to gather information on her behalf. Such a problem is modeled
as a Stackelberg game between the principal and the agent, where the principal
announces a scoring rule that specifies the payment, and then the agent then
chooses an effort level that maximizes her own profit and reports the
information. We study the online setting of such a problem from the principal's
perspective, i.e., designing the optimal scoring rule by repeatedly interacting
with the strategic agent. We design a provably sample efficient algorithm that
tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a
sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a
delicate estimation procedure for the optimal profit of the principal, and a
conservative correction scheme that ensures the desired agent's actions are
incentivized. Furthermore, a key feature of our regret bound is that it is
independent of the number of states of the environment.",None,-1
d278869c-f130-4ecd-b54e-a0385fda8d8d,RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation,0.819555,"Grammatical Error Correction (GEC) systems play a vital role in assisting
people with their daily writing tasks. However, users may sometimes come across
a GEC system that initially performs well but fails to correct errors when the
inputs are slightly modified. To ensure an ideal user experience, a reliable
GEC system should have the ability to provide consistent and accurate
suggestions when encountering irrelevant context perturbations, which we refer
to as context robustness. In this paper, we introduce RobustGEC, a benchmark
designed to evaluate the context robustness of GEC systems. RobustGEC comprises
5,000 GEC cases, each with one original error-correct sentence pair and five
variants carefully devised by human annotators. Utilizing RobustGEC, we reveal
that state-of-the-art GEC systems still lack sufficient robustness against
context perturbations. In addition, we propose a simple yet effective method
for remitting this issue.",None,-1
2348987f-f7c1-49c1-a84d-99d3be934935,BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling,0.147595,"Reasoning the 3D structure of a non-rigid dynamic scene from a single moving
camera is an under-constrained problem. Inspired by the remarkable progress of
neural radiance fields (NeRFs) in photo-realistic novel view synthesis of
static scenes, extensions have been proposed for dynamic settings. These
methods heavily rely on neural priors in order to regularize the problem. In
this work, we take a step back and reinvestigate how current implementations
may entail deleterious effects, including limited expressiveness, entanglement
of light and density fields, and sub-optimal motion localization. As a remedy,
we advocate for a bridge between classic non-rigid-structure-from-motion
(\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain
the latter. To this end, we propose a framework that factorizes time and space
by formulating a scene as a composition of bandlimited, high-dimensional
signals. We demonstrate compelling results across complex dynamic scenes that
involve changes in lighting, texture and long-range dynamics.",None,-1
fce1f812-5b1c-47e0-8b10-beda292909b5,Generative AI-empowered Effective Physical-Virtual Synchronization in the Vehicular Metaverse,0.505611,"Metaverse seamlessly blends the physical world and virtual space via
ubiquitous communication and computing infrastructure. In transportation
systems, the vehicular Metaverse can provide a fully-immersive and hyperreal
traveling experience (e.g., via augmented reality head-up displays, AR-HUDs) to
drivers and users in autonomous vehicles (AVs) via roadside units (RSUs).
However, provisioning real-time and immersive services necessitates effective
physical-virtual synchronization between physical and virtual entities, i.e.,
AVs and Metaverse AR recommenders (MARs). In this paper, we propose a
generative AI-empowered physical-virtual synchronization framework for the
vehicular Metaverse. In physical-to-virtual synchronization, digital twin (DT)
tasks generated by AVs are offloaded for execution in RSU with future route
generation. In virtual-to-physical synchronization, MARs customize diverse and
personal AR recommendations via generative AI models based on user preferences.
Furthermore, we propose a multi-task enhanced auction-based mechanism to match
and price AVs and MARs for RSUs to provision real-time and effective services.
Finally, property analysis and experimental results demonstrate that the
proposed mechanism is strategy-proof and adverse-selection free while
increasing social surplus by 50%.",None,-1
e2793e2c-8af8-4a1a-a886-c93a079a62a1,OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios,0.308986,"Modern approaches for vision-centric environment perception for autonomous
navigation make extensive use of self-supervised monocular depth estimation
algorithms that output disparity maps. However, when this disparity map is
projected onto 3D space, the errors in disparity are magnified, resulting in a
depth estimation error that increases quadratically as the distance from the
camera increases. Though Light Detection and Ranging (LiDAR) can solve this
issue, it is expensive and not feasible for many applications. To address the
challenge of accurate ranging with low-cost sensors, we propose, OCTraN, a
transformer architecture that uses iterative-attention to convert 2D image
features into 3D occupancy features and makes use of convolution and transpose
convolution to efficiently operate on spatial information. We also develop a
self-supervised training pipeline to generalize the model to any scene by
eliminating the need for LiDAR ground truth by substituting it with
pseudo-ground truth labels obtained from boosted monocular depth estimation.",None,-1
c0308b62-b62b-4e0f-be37-d929c1eee981,Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization,0.57646,"In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset both to the
public and the market. We conduct a comprehensive study on motion style
transfer in the experiment using the state-of-the-art method, and the results
show the proposed dataset's validity for the motion style transfer task.",None,-1
a00d407a-7864-415b-b928-8e3a49f36459,User Friendly and Adaptable Discriminative AI: Using the Lessons from the Success of LLMs and Image Generation Models,0.326914,"While there is significant interest in using generative AI tools as
general-purpose models for specific ML applications, discriminative models are
much more widely deployed currently. One of the key shortcomings of these
discriminative AI tools that have been already deployed is that they are not
adaptable and user-friendly compared to generative AI tools (e.g., GPT4, Stable
Diffusion, Bard, etc.), where a non-expert user can iteratively refine model
inputs and give real-time feedback that can be accounted for immediately,
allowing users to build trust from the start. Inspired by this emerging
collaborative workflow, we develop a new system architecture that enables users
to work with discriminative models (such as for object detection, sentiment
classification, etc.) in a fashion similar to generative AI tools, where they
can easily provide immediate feedback as well as adapt the deployed models as
desired. Our approach has implications on improving trust, user-friendliness,
and adaptability of these versatile but traditional prediction models.",None,-1
7fd4de29-e6f4-498a-a357-494f1568020b,"Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets, Applications and Challenges",0.777271,"The deep learning, which is a dominating technique in artificial
intelligence, has completely changed the image understanding over the past
decade. As a consequence, the sea ice extraction (SIE) problem has reached a
new era. We present a comprehensive review of four important aspects of SIE,
including algorithms, datasets, applications, and the future trends. Our review
focuses on researches published from 2016 to the present, with a specific focus
on deep learning-based approaches in the last five years. We divided all
relegated algorithms into 3 categories, including classical image segmentation
approach, machine learning-based approach and deep learning-based methods. We
reviewed the accessible ice datasets including SAR-based datasets, the
optical-based datasets and others. The applications are presented in 4 aspects
including climate research, navigation, geographic information systems (GIS)
production and others. It also provides insightful observations and inspiring
future research directions.",None,-1
f67f1c95-b041-48c0-bafd-d9751e01624e,Track Anything: Segment Anything Meets Videos,1.0,"Recently, the Segment Anything Model (SAM) gains lots of attention rapidly
due to its impressive segmentation performance on images. Regarding its strong
ability on image segmentation and high interactivity with different prompts, we
found that it performs poorly on consistent segmentation in videos. Therefore,
in this report, we propose Track Anything Model (TAM), which achieves
high-performance interactive tracking and segmentation in videos. To be
detailed, given a video sequence, only with very little human participation,
i.e., several clicks, people can track anything they are interested in, and get
satisfactory results in one-pass inference. Without additional training, such
an interactive design performs impressively on video object tracking and
segmentation. All resources are available on
{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitate
related research.",None,-1
8abb2d15-b21b-4db6-8066-2b1b24d6e7bf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,0.639458,"Multi-target multi-camera tracking (MTMCT) of vehicles, i.e. tracking
vehicles across multiple cameras, is a crucial application for the development
of smart city and intelligent traffic system. The main challenges of MTMCT of
vehicles include the intra-class variability of the same vehicle and
inter-class similarity between different vehicles and how to associate the same
vehicle accurately across different cameras under large search space. Previous
methods for MTMCT usually use hierarchical clustering of trajectories to
conduct cross camera association. However, the search space can be large and
does not take spatial and temporal information into consideration. In this
paper, we proposed a transformer-based camera link model with spatial and
temporal filtering to conduct cross camera tracking. Achieving 73.68% IDF1 on
the Nvidia Cityflow V2 dataset test set, showing the effectiveness of our
camera link model on multi-target multi-camera tracking.",None,-1
f7ad7f9d-fb4e-4219-87e9-8818b3861134,Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection,0.672244,"Prompt tuning with large-scale pretrained vision-language models empowers
open-vocabulary predictions trained on limited base categories, e.g., object
classification and detection. In this paper, we propose compositional prompt
tuning with motion cues: an extended prompt tuning paradigm for compositional
predictions of video data. In particular, we present Relation Prompt (RePro)
for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where
conventional prompt tuning is easily biased to certain subject-object
combinations and motion patterns. To this end, RePro addresses the two
technical challenges of Open-VidVRD: 1) the prompt tokens should respect the
two different semantic roles of subject and object, and 2) the tuning should
account for the diverse spatio-temporal motion patterns of the subject-object
compositions. Without bells and whistles, our RePro achieves a new
state-of-the-art performance on two VidVRD benchmarks of not only the base
training object and predicate categories, but also the unseen ones. Extensive
ablations also demonstrate the effectiveness of the proposed compositional and
multi-mode design of prompts. Code is available at
https://github.com/Dawn-LX/OpenVoc-VidVRD.",None,-1
89926a8e-d9b1-46f4-9eec-875aa72fe099,Principal-Agent Reward Shaping in MDPs,0.342994,"Principal-agent problems arise when one party acts on behalf of another,
leading to conflicts of interest. The economic literature has extensively
studied principal-agent problems, and recent work has extended this to more
complex scenarios such as Markov Decision Processes (MDPs). In this paper, we
further explore this line of research by investigating how reward shaping under
budget constraints can improve the principal's utility. We study a two-player
Stackelberg game where the principal and the agent have different reward
functions, and the agent chooses an MDP policy for both players. The principal
offers an additional reward to the agent, and the agent picks their policy
selfishly to maximize their reward, which is the sum of the original and the
offered reward. Our results establish the NP-hardness of the problem and offer
polynomial approximation algorithms for two classes of instances: Stochastic
trees and deterministic decision processes with a finite horizon.",None,-1
d5657d19-ae50-4275-97f6-e0c5df493c02,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,0.0775872,"To ensure reliable object detection in autonomous systems, the detector must
be able to adapt to changes in appearance caused by environmental factors such
as time of day, weather, and seasons. Continually adapting the detector to
incorporate these changes is a promising solution, but it can be
computationally costly. Our proposed approach is to selectively adapt the
detector only when necessary, using new data that does not have the same
distribution as the current training data. To this end, we investigate three
popular metrics for domain gap evaluation and find that there is a correlation
between the domain gap and detection accuracy. Therefore, we apply the domain
gap as a criterion to decide when to adapt the detector. Our experiments show
that our approach has the potential to improve the efficiency of the detector's
operation in real-world scenarios, where environmental conditions change in a
cyclical manner, without sacrificing the overall performance of the detector.
Our code is publicly available at https://github.com/dadung/DGE-CDA.",None,-1
c645d033-8ceb-4481-9fe9-ff18008b4342,GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark,0.128293,"With a fast developing pace of geographic applications, automatable and
intelligent models are essential to be designed to handle the large volume of
information. However, few researchers focus on geographic natural language
processing, and there has never been a benchmark to build a unified standard.
In this work, we propose a GeoGraphic Language Understanding Evaluation
benchmark, named GeoGLUE. We collect data from open-released geographic
resources and introduce six natural language understanding tasks, including
geographic textual similarity on recall, geographic textual similarity on
rerank, geographic elements tagging, geographic composition analysis,
geographic where what cut, and geographic entity alignment. We also pro vide
evaluation experiments and analysis of general baselines, indicating the
effectiveness and significance of the GeoGLUE benchmark.",None,-1
e3063c25-c6b9-4ece-8a9c-c1b99bef3458,How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases,0.870064,"Accurate syntactic representations are essential for robust generalization in
natural language. Recent work has found that pre-training can teach language
models to rely on hierarchical syntactic features - as opposed to incorrect
linear features - when performing tasks after fine-tuning. We test what aspects
of pre-training are important for endowing encoder-decoder Transformers with an
inductive bias that favors hierarchical syntactic generalizations. We focus on
architectural features (depth, width, and number of parameters), as well as the
genre and size of the pre-training corpus, diagnosing inductive biases using
two syntactic transformation tasks: question formation and passivization, both
in English. We find that the number of parameters alone does not explain
hierarchical generalization: model depth plays greater role than model width.
We also find that pre-training on simpler language, such as child-directed
speech, induces a hierarchical bias using an order-of-magnitude less data than
pre-training on more typical datasets based on web text or Wikipedia; this
suggests that in cognitively plausible language acquisition settings, neural
language models may be more data-efficient than previously thought.",None,-1
4e4b47df-d744-415f-93e0-c4735e71c669,Tackling Clutter in Radar Data -- Label Generation and Detection Using PointNet++,0.770778,"Radar sensors employed for environment perception, e.g. in autonomous
vehicles, output a lot of unwanted clutter. These points, for which no
corresponding real objects exist, are a major source of errors in following
processing steps like object detection or tracking. We therefore present two
novel neural network setups for identifying clutter. The input data, network
architectures and training configuration are adjusted specifically for this
task. Special attention is paid to the downsampling of point clouds composed of
multiple sensor scans. In an extensive evaluation, the new setups display
substantially better performance than existing approaches. Because there is no
suitable public data set in which clutter is annotated, we design a method to
automatically generate the respective labels. By applying it to existing data
with object annotations and releasing its code, we effectively create the first
freely available radar clutter data set representing real-world driving
scenarios. Code and instructions are accessible at
www.github.com/kopp-j/clutter-ds.",None,-1
e281ed69-4aa4-454d-b6d5-f9524f21ac41,Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks,0.128665,"Large language models (LLMs) are proficient at generating fluent text with
minimal task-specific supervision. Yet, their ability to provide well-grounded
rationalizations for knowledge-intensive tasks remains under-explored. Such
tasks, like commonsense multiple-choice questions, require rationales based on
world knowledge to support predictions and refute alternate options. We
consider the task of generating knowledge-guided rationalization in natural
language by using expert-written examples in a few-shot manner. Surprisingly,
crowd-workers preferred knowledge-grounded rationales over crowdsourced
rationalizations, citing their factuality, sufficiency, and comprehensive
refutations. Although LLMs-generated rationales were preferable, further
improvements in conciseness and novelty are required. In another study, we show
how rationalization of incorrect model predictions erodes humans' trust in
LLM-generated rationales. Motivated by these observations, we create a
two-stage pipeline to review task predictions and eliminate potential incorrect
decisions before rationalization, enabling trustworthy rationale generation.",None,-1
1be0dca5-51a9-45a5-87f9-5dc03b8d5677,Analyzing Font Style Usage and Contextual Factors in Real Images,0.699819,"There are various font styles in the world. Different styles give different
impressions and readability. This paper analyzes the relationship between font
styles and contextual factors that might affect font style selection with
large-scale datasets. For example, we will analyze the relationship between
font style and its surrounding object (such as ``bus'') by using about 800,000
words in the Open Images dataset. We also use a book cover dataset to analyze
the relationship between font styles with book genres. Moreover, the meaning of
the word is assumed as another contextual factor. For these numeric analyses,
we utilize our own font-style feature extraction model and word2vec. As a
result of co-occurrence-based relationship analysis, we found several instances
of specific font styles being used for specific contextual factors.",None,-1
9529ca84-3861-415f-bcf7-9d46714f8825,SortedAP: Rethinking evaluation metrics for instance segmentation,0.195995,"Designing metrics for evaluating instance segmentation revolves around
comprehensively considering object detection and segmentation accuracy.
However, other important properties, such as sensitivity, continuity, and
equality, are overlooked in the current study. In this paper, we reveal that
most existing metrics have a limited resolution of segmentation quality. They
are only conditionally sensitive to the change of masks or false predictions.
For certain metrics, the score can change drastically in a narrow range which
could provide a misleading indication of the quality gap between results.
Therefore, we propose a new metric called sortedAP, which strictly decreases
with both object- and pixel-level imperfections and has an uninterrupted
penalization scale over the entire domain. We provide the evaluation toolkit
and experiment code at https://www.github.com/looooongChen/sortedAP.",None,-1
9ceb3aa5-ab74-4760-a8de-6f6665e4d60d,Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence,0.278203,"We discuss the emerging new opportunity for building feedback-rich
computational models of social systems using generative artificial
intelligence. Referred to as Generative Agent-Based Models (GABMs), such
individual-level models utilize large language models such as ChatGPT to
represent human decision-making in social settings. We provide a GABM case in
which human behavior can be incorporated in simulation models by coupling a
mechanistic model of human interactions with a pre-trained large language
model. This is achieved by introducing a simple GABM of social norm diffusion
in an organization. For educational purposes, the model is intentionally kept
simple. We examine a wide range of scenarios and the sensitivity of the results
to several changes in the prompt. We hope the article and the model serve as a
guide for building useful diffusion models that include realistic human
reasoning and decision-making.",None,-1
5246d809-b33e-4d58-b0bb-1141502b167e,Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication,0.311715,"Over the past decade explainable artificial intelligence has evolved from a
predominantly technical discipline into a field that is deeply intertwined with
social sciences. Insights such as human preference for contrastive -- more
precisely, counterfactual -- explanations have played a major role in this
transition, inspiring and guiding the research in computer science. Other
observations, while equally important, have received much less attention. The
desire of human explainees to communicate with artificial intelligence
explainers through a dialogue-like interaction has been mostly neglected by the
community. This poses many challenges for the effectiveness and widespread
adoption of such technologies as delivering a single explanation optimised
according to some predefined objectives may fail to engender understanding in
its recipients and satisfy their unique needs given the diversity of human
knowledge and intention. Using insights elaborated by Niklas Luhmann and, more
recently, Elena Esposito we apply social systems theory to highlight challenges
in explainable artificial intelligence and offer a path forward, striving to
reinvigorate the technical research in this direction. This paper aims to
demonstrate the potential of systems theoretical approaches to communication in
understanding problems and limitations of explainable artificial intelligence.",None,-1
000eb4aa-38b2-415a-af42-2231d248046e,A Unified Generative Approach to Product Attribute-Value Identification,0.956841,"Product attribute-value identification (PAVI) has been studied to link
products on e-commerce sites with their attribute values (e.g., <Material,
Cotton>) using product text as clues. Technical demands from real-world
e-commerce platforms require PAVI methods to handle unseen values,
multi-attribute values, and canonicalized values, which are only partly
addressed in existing extraction- and classification-based approaches.
Motivated by this, we explore a generative approach to the PAVI task. We
finetune a pre-trained generative model, T5, to decode a set of attribute-value
pairs as a target sequence from the given product text. Since the attribute
value pairs are unordered set elements, how to linearize them will matter; we,
thus, explore methods of composing an attribute-value pair and ordering the
pairs for the task. Experimental results confirm that our generation-based
approach outperforms the existing extraction and classification-based methods
on large-scale real-world datasets meant for those methods.",None,-1
e7575bf0-629a-4775-b45c-5120937c3a9d,Semantic Compression With Large Language Models,0.485915,"The rise of large language models (LLMs) is revolutionizing information
retrieval, question answering, summarization, and code generation tasks.
However, in addition to confidently presenting factually inaccurate information
at times (known as ""hallucinations""), LLMs are also inherently limited by the
number of input and output tokens that can be processed at once, making them
potentially less effective on tasks that require processing a large set or
continuous stream of information. A common approach to reducing the size of
data is through lossless or lossy compression. Yet, in some cases it may not be
strictly necessary to perfectly recover every detail from the original data, as
long as a requisite level of semantic precision or intent is conveyed.
  This paper presents three contributions to research on LLMs. First, we
present the results from experiments exploring the viability of approximate
compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT
interfaces. Second, we investigate and quantify the capability of LLMs to
compress text and code, as well as to recall and manipulate compressed
representations of prompts. Third, we present two novel metrics -- Exact
Reconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness
(SRE) -- that quantify the level of preserved intent between text compressed
and decompressed by the LLMs we studied. Our initial results indicate that
GPT-4 can effectively compress and reconstruct text while preserving the
semantic essence of the original text, providing a path to leverage
$\sim$5$\times$ more tokens than present limits allow.",None,-1
8cdde9d9-9b99-4188-b411-7a74d39e4011,How To Build Competitive Multi-gender Speech Translation Models For Controlling Speaker Gender Translation,0.225314,"When translating from notional gender languages (e.g., English) into
grammatical gender languages (e.g., Italian), the generated translation
requires explicit gender assignments for various words, including those
referring to the speaker. When the source sentence does not convey the
speaker's gender, speech translation (ST) models either rely on the
possibly-misleading vocal traits of the speaker or default to the masculine
gender, the most frequent in existing training corpora. To avoid such biased
and not inclusive behaviors, the gender assignment of speaker-related
expressions should be guided by externally-provided metadata about the
speaker's gender. While previous work has shown that the most effective
solution is represented by separate, dedicated gender-specific models, the goal
of this paper is to achieve the same results by integrating the speaker's
gender metadata into a single ""multi-gender"" neural ST model, easier to
maintain. Our experiments demonstrate that a single multi-gender model
outperforms gender-specialized ones when trained from scratch (with gender
accuracy gains up to 12.9 for feminine forms), while fine-tuning from existing
ST models does not lead to competitive results.",None,-1
31750fd6-d909-4cfc-9ab7-26a12938353d,HADES: Fast Singularity Detection with Local Measure Comparison,0.106622,"We introduce Hades, an unsupervised algorithm to detect singularities in
data. This algorithm employs a kernel goodness-of-fit test, and as a
consequence it is much faster and far more scaleable than the existing
topology-based alternatives. Using tools from differential geometry and optimal
transport theory, we prove that Hades correctly detects singularities with high
probability when the data sample lives on a transverse intersection of
equidimensional manifolds. In computational experiments, Hades recovers
singularities in synthetically generated data, branching points in road network
data, intersection rings in molecular conformation space, and anomalies in
image data.",None,-1
17b9e2e3-b5a0-42de-ad2f-a5f587fa2fbd,MegaWika: Millions of reports and their sources across 50 diverse languages,0.277874,"To foster the development of new models for collaborative AI-assisted report
generation, we introduce MegaWika, consisting of 13 million Wikipedia articles
in 50 diverse languages, along with their 71 million referenced source
materials. We process this dataset for a myriad of applications, going beyond
the initial Wikipedia citation extraction and web scraping of content,
including translating non-English articles for cross-lingual applications and
providing FrameNet parses for automated semantic analysis. MegaWika is the
largest resource for sentence-level report generation and the only report
generation dataset that is multilingual. We manually analyze the quality of
this resource through a semantically stratified sample. Finally, we provide
baseline results and trained models for crucial steps in automated report
generation: cross-lingual question answering and citation retrieval.",None,-1
26b9ff16-4c17-402c-9758-d21608239665,Explainability is NOT a Game,0.364452,"Explainable artificial intelligence (XAI) aims to help human decision-makers
in understanding complex machine learning (ML) models. One of the hallmarks of
XAI are measures of relative feature importance, which are theoretically
justified through the use of Shapley values. This paper builds on recent work
and offers a simple argument for why Shapley values can provide misleading
measures of relative feature importance, by assigning more importance to
features that are irrelevant for a prediction, and assigning less importance to
features that are relevant for a prediction. The significance of these results
is that they effectively challenge the many proposed uses of measures of
relative feature importance in a fast-growing range of high-stakes application
domains.",None,-1
f5fba16e-8bcd-46b3-911f-7f95cafaac05,MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing,0.999675,"Despite the success in large-scale text-to-image generation and
text-conditioned image editing, existing methods still struggle to produce
consistent generation and editing results. For example, generation approaches
usually fail to synthesize multiple images of the same objects/characters but
with different views or poses. Meanwhile, existing editing methods either fail
to achieve effective complex non-rigid editing while maintaining the overall
textures and identity, or require time-consuming fine-tuning to capture the
image-specific appearance. In this paper, we develop MasaCtrl, a tuning-free
method to achieve consistent image generation and complex non-rigid image
editing simultaneously. Specifically, MasaCtrl converts existing self-attention
in diffusion models into mutual self-attention, so that it can query correlated
local contents and textures from source images for consistency. To further
alleviate the query confusion between foreground and background, we propose a
mask-guided mutual self-attention strategy, where the mask can be easily
extracted from the cross-attention maps. Extensive experiments show that the
proposed MasaCtrl can produce impressive results in both consistent image
generation and complex non-rigid real image editing.",None,-1
1c29ef09-3a9f-4bd5-bc14-9255cad22732,3D Video Object Detection with Learnable Object-Centric Global Optimization,0.407917,"We explore long-term temporal visual correspondence-based optimization for 3D
video object detection in this work. Visual correspondence refers to one-to-one
mappings for pixels across multiple images. Correspondence-based optimization
is the cornerstone for 3D scene reconstruction but is less studied in 3D video
object detection, because moving objects violate multi-view geometry
constraints and are treated as outliers during scene reconstruction. We address
this issue by treating objects as first-class citizens during
correspondence-based optimization. In this work, we propose BA-Det, an
end-to-end optimizable object detector with object-centric temporal
correspondence learning and featuremetric object bundle adjustment.
Empirically, we verify the effectiveness and efficiency of BA-Det for multiple
baseline 3D detectors under various setups. Our BA-Det achieves SOTA
performance on the large-scale Waymo Open Dataset (WOD) with only marginal
computation cost. Our code is available at
https://github.com/jiaweihe1996/BA-Det.",None,-1
9d1b0f6a-11f3-4014-9bab-179db6e75ccc,"Lightweight, Pre-trained Transformers for Remote Sensing Timeseries",0.98204,"Machine learning methods for satellite data have a range of societally
relevant applications, but labels used to train models can be difficult or
impossible to acquire. Self-supervision is a natural solution in settings with
limited labeled data, but current self-supervised models for satellite data
fail to take advantage of the characteristics of that data, including the
temporal dimension (which is critical for many applications, such as monitoring
crop growth) and availability of data from many complementary sensors (which
can significantly improve a model's predictive performance). We present Presto
(the Pretrained Remote Sensing Transformer), a model pre-trained on remote
sensing pixel-timeseries data. By designing Presto specifically for remote
sensing data, we can create a significantly smaller but performant model.
Presto excels at a wide variety of globally distributed remote sensing tasks
and performs competitively with much larger models while requiring far less
compute. Presto can be used for transfer learning or as a feature extractor for
simple models, enabling efficient deployment at scale.",None,-1
d5001467-caa6-425a-aa91-4ca6f2a08107,Collaborative Auto-encoding for Blind Image Quality Assessment,0.392411,"Blind image quality assessment (BIQA) is a challenging problem with important
real-world applications. Recent efforts attempting to exploit powerful
representations by deep neural networks (DNN) are hindered by the lack of
subjectively annotated data. This paper presents a novel BIQA method which
overcomes this fundamental obstacle. Specifically, we design a pair of
collaborative autoencoders (COAE) consisting of a content autoencoder (CAE) and
a distortion autoencoder (DAE) that work together to extract content and
distortion representations, which are shown to be highly descriptive of image
quality. While the CAE follows a standard codec procedure, we introduce the
CAE-encoded feature as an extra input to the DAE's decoder for reconstructing
distorted images, thus effectively forcing DAE's encoder to extract distortion
representations. The self-supervised learning framework allows the COAE
including two feature extractors to be trained by almost unlimited amount of
data, thus leaving limited samples with annotations to finetune a BIQA model.
We will show that the proposed BIQA method achieves state-of-the-art
performance and has superior generalization capability over other learning
based models. The codes are available at:
https://github.com/Macro-Zhou/NRIQA-VISOR/.",None,-1
0dda35a3-d26b-41cc-85f1-b26004fc57fe,Explicit Correspondence Matching for Generalizable Neural Radiance Fields,0.680604,"We present a new generalizable NeRF method that is able to directly
generalize to new unseen scenarios and perform novel view synthesis with as few
as two source views. The key to our approach lies in the explicitly modeled
correspondence matching information, so as to provide the geometry prior to the
prediction of NeRF color and density for volume rendering. The explicit
correspondence matching is quantified with the cosine similarity between image
features sampled at the 2D projections of a 3D point on different views, which
is able to provide reliable cues about the surface geometry. Unlike previous
methods where image features are extracted independently for each view, we
consider modeling the cross-view interactions via Transformer cross-attention,
which greatly improves the feature matching quality. Our method achieves
state-of-the-art results on different evaluation settings, with the experiments
showing a strong correlation between our learned cosine feature similarity and
volume density, demonstrating the effectiveness and superiority of our proposed
method. Code is at https://github.com/donydchen/matchnerf",None,-1
671ba32f-8275-421e-8193-4a7678b20194,Systematic Visual Reasoning through Object-Centric Relational Abstraction,0.544551,"Human visual reasoning is characterized by an ability to identify abstract
patterns from only a small number of examples, and to systematically generalize
those patterns to novel inputs. This capacity depends in large part on our
ability to represent complex visual inputs in terms of both objects and
relations. Recent work in computer vision has introduced models with the
capacity to extract object-centric representations, leading to the ability to
process multi-object visual inputs, but falling short of the systematic
generalization displayed by human reasoning. Other recent models have employed
inductive biases for relational abstraction to achieve systematic
generalization of learned abstract rules, but have generally assumed the
presence of object-focused inputs. Here, we combine these two approaches,
introducing Object-Centric Relational Abstraction (OCRA), a model that extracts
explicit representations of both objects and abstract relations, and achieves
strong systematic generalization in tasks (including a novel dataset,
CLEVR-ART, with greater visual complexity) involving complex visual displays.",None,-1
3a3ad082-34e6-4de3-b747-d4b53a93056a,TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments,0.701866,"Although the estimation of 3D human pose and shape (HPS) is rapidly
progressing, current methods still cannot reliably estimate moving humans in
global coordinates, which is critical for many applications. This is
particularly challenging when the camera is also moving, entangling human and
camera motion. To address these issues, we adopt a novel 5D representation
(space, time, and identity) that enables end-to-end reasoning about people in
scenes. Our method, called TRACE, introduces several novel architectural
components. Most importantly, it uses two new ""maps"" to reason about the 3D
trajectory of people over time in camera, and world, coordinates. An additional
memory unit enables persistent tracking of people even during long occlusions.
TRACE is the first one-stage method to jointly recover and track 3D humans in
global coordinates from dynamic cameras. By training it end-to-end, and using
full image information, TRACE achieves state-of-the-art performance on tracking
and HPS benchmarks. The code and dataset are released for research purposes.",None,-1
5f8e762d-c520-473c-836d-0b4dbb484dad,Inseq: An Interpretability Toolkit for Sequence Generation Models,0.927388,"Past work in natural language processing interpretability focused mainly on
popular classification tasks while largely overlooking generation settings,
partly due to a lack of dedicated tools. In this work, we introduce Inseq, a
Python library to democratize access to interpretability analyses of sequence
generation models. Inseq enables intuitive and optimized extraction of models'
internal information and feature importance scores for popular decoder-only and
encoder-decoder Transformers architectures. We showcase its potential by
adopting it to highlight gender biases in machine translation models and locate
factual knowledge inside GPT-2. Thanks to its extensible interface supporting
cutting-edge techniques such as contrastive feature attribution, Inseq can
drive future advances in explainable natural language generation, centralizing
good practices and enabling fair and reproducible model evaluations.",None,-1
92da8ce6-9a5c-41b2-ab49-695383ea5ed1,Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain,0.826762,"Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
Parameter-Efficient Fine-Tuning (PEFT) techniques for fine-tuning language
models significantly reduce computational requirements by selectively
fine-tuning small subsets of parameters. In this study, we propose a two-step
PEFT framework and evaluate it in the clinical domain. Our approach combines a
specialised PEFT adapter layer designed for clinical domain adaptation with
another adapter specialised for downstream tasks. We evaluate the framework on
multiple clinical outcome prediction datasets, comparing it to clinically
trained language models. Our framework achieves a better AUROC score averaged
across all clinical downstream tasks compared to clinical language models. In
particular, we observe large improvements of 4-5% AUROC in large-scale
multilabel classification tasks, such as diagnoses and procedures
classification. To our knowledge, this study is the first to provide an
extensive empirical analysis of the interplay between PEFT techniques and
domain adaptation in an important real-world domain of clinical applications.",None,-1
e7fd5928-e3cb-4714-82b2-0f465ed16f58,Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification,0.998303,"For the visible-infrared person re-identification (VIReID) task, one of the
major challenges is the modality gaps between visible (VIS) and infrared (IR)
images. However, the training samples are usually limited, while the modality
gaps are too large, which leads that the existing methods cannot effectively
mine diverse cross-modality clues. To handle this limitation, we propose a
novel augmentation network in the embedding space, called diverse embedding
expansion network (DEEN). The proposed DEEN can effectively generate diverse
embeddings to learn the informative feature representations and reduce the
modality discrepancy between the VIS and IR images. Moreover, the VIReID model
may be seriously affected by drastic illumination changes, while all the
existing VIReID datasets are captured under sufficient illumination without
significant light changes. Thus, we provide a low-light cross-modality (LLCM)
dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9
RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasets
show the superiority of the proposed DEEN over several other state-of-the-art
methods. The code and dataset are released at: https://github.com/ZYK100/LLCM",None,-1
da70bd8e-d31a-4c34-82ca-660e546ac24f,MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding,0.0788653,"International Classification of Diseases (ICD) coding is the task of
assigning ICD diagnosis codes to clinical notes. This can be challenging given
the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000
tokens). However, unlike the single-pass reading process in previous works,
humans tend to read the text and label definitions again to get more confident
answers. Moreover, although pretrained language models have been used to
address these problems, they suffer from huge memory usage. To address the
above problems, we propose a simple but effective model called the Multi-Hop
Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is
deployed to get more precise and informative representations. Extensive
experiments on three benchmark MIMIC datasets indicate that our method achieves
significantly better or competitive performance on all seven metrics, with much
fewer parameters to optimize.",None,-1
b8d50bb1-73ba-4880-8bcd-148cebc498cd,SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition,0.477285,"Multilingual automatic speech recognition (ASR) systems have garnered
attention for their potential to extend language coverage globally. While
self-supervised learning (SSL) models, like MMS, have demonstrated their
effectiveness in multilingual ASR, it is worth noting that various layers'
representations potentially contain distinct information that has not been
fully leveraged. In this study, we propose a novel method that leverages
self-supervised hierarchical representations (SSHR) to fine-tune the MMS model.
We first analyze the different layers of MMS and show that the middle layers
capture language-related information, and the high layers encode
content-related information, which gradually decreases in the final layers.
Then, we extract a language-related frame from correlated middle layers and
guide specific language extraction through self-attention mechanisms.
Additionally, we steer the model toward acquiring more content-related
information in the final layers using our proposed Cross-CTC. We evaluate SSHR
on two multilingual datasets, Common Voice and ML-SUPERB, and the experimental
results demonstrate that our method achieves state-of-the-art performance.",None,-1
742b345a-a218-46ba-902e-26e60b42c236,From Knowledge Representation to Knowledge Organization and Back,0.291615,"Knowledge Representation (KR) and facet-analytical Knowledge Organization
(KO) have been the two most prominent methodologies of data and knowledge
modelling in the Artificial Intelligence community and the Information Science
community, respectively. KR boasts of a robust and scalable ecosystem of
technologies to support knowledge modelling while, often, underemphasizing the
quality of its models (and model-based data). KO, on the other hand, is less
technology-driven but has developed a robust framework of guiding principles
(canons) for ensuring modelling (and model-based data) quality. This paper
elucidates both the KR and facet-analytical KO methodologies in detail and
provides a functional mapping between them. Out of the mapping, the paper
proposes an integrated KO-enriched KR methodology with all the standard
components of a KR methodology plus the guiding canons of modelling quality
provided by KO. The practical benefits of the methodological integration has
been exemplified through a prominent case study of KR-based image annotation
exercise.",None,-1
ccf8950a-2754-4c22-be39-9aa6695e9c5a,Autocorrelations Decay in Texts and Applicability Limits of Language Models,0.0198413,"We show that the laws of autocorrelations decay in texts are closely related
to applicability limits of language models. Using distributional semantics we
empirically demonstrate that autocorrelations of words in texts decay according
to a power law. We show that distributional semantics provides coherent
autocorrelations decay exponents for texts translated to multiple languages.
The autocorrelations decay in generated texts is quantitatively and often
qualitatively different from the literary texts. We conclude that language
models exhibiting Markov behavior, including large autoregressive language
models, may have limitations when applied to long texts, whether analysis or
generation.",None,-1
162e7959-bdb1-459b-9ebb-b5b61e31e61e,Interactive and Explainable Region-guided Radiology Report Generation,0.991841,"The automatic generation of radiology reports has the potential to assist
radiologists in the time-consuming task of report writing. Existing methods
generate the full report from image-level features, failing to explicitly focus
on anatomical regions in the image. We propose a simple yet effective
region-guided report generation model that detects anatomical regions and then
describes individual, salient regions to form the final report. While previous
methods generate reports without the possibility of human intervention and with
limited explainability, our method opens up novel clinical use cases through
additional interactive capabilities and introduces a high degree of
transparency and explainability. Comprehensive experiments demonstrate our
method's effectiveness in report generation, outperforming previous
state-of-the-art models, and highlight its interactive capabilities. The code
and checkpoints are available at https://github.com/ttanida/rgrg .",None,-1
2d3e37f6-2701-4471-947e-a40145faca14,Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency,0.673689,"Monocular 3D object detection has become a mainstream approach in automatic
driving for its easy application. A prominent advantage is that it does not
need LiDAR point clouds during the inference. However, most current methods
still rely on 3D point cloud data for labeling the ground truths used in the
training phase. This inconsistency between the training and inference makes it
hard to utilize the large-scale feedback data and increases the data collection
expenses. To bridge this gap, we propose a new weakly supervised monocular 3D
objection detection method, which can train the model with only 2D labels
marked on images. To be specific, we explore three types of consistency in this
task, i.e. the projection, multi-view and direction consistency, and design a
weakly-supervised architecture based on these consistencies. Moreover, we
propose a new 2D direction labeling method in this task to guide the model for
accurate rotation direction prediction. Experiments show that our
weakly-supervised method achieves comparable performance with some fully
supervised methods. When used as a pre-training method, our model can
significantly outperform the corresponding fully-supervised baseline with only
1/3 3D labels. https://github.com/weakmono3d/weakmono3d",None,-1
00df4005-587a-40a2-a9fe-421786476256,Anaphor Assisted Document-Level Relation Extraction,0.681559,"Document-level relation extraction (DocRE) involves identifying relations
between entities distributed in multiple sentences within a document. Existing
methods focus on building a heterogeneous document graph to model the internal
structure of an entity and the external interaction between entities. However,
there are two drawbacks in existing methods. On one hand, anaphor plays an
important role in reasoning to identify relations between entities but is
ignored by these methods. On the other hand, these methods achieve
cross-sentence entity interactions implicitly by utilizing a document or
sentences as intermediate nodes. Such an approach has difficulties in learning
fine-grained interactions between entities across different sentences,
resulting in sub-optimal performance. To address these issues, we propose an
Anaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the
widely-used datasets demonstrate that our model achieves a new state-of-the-art
performance.",None,-1
350a2926-affb-43e2-8a99-a5bede3450e7,Large-Scale Multi-Hypotheses Cell Tracking Using Ultrametric Contours Maps,0.0757773,"In this work, we describe a method for large-scale 3D cell-tracking through a
segmentation selection approach. The proposed method is effective at tracking
cells across large microscopy datasets on two fronts: (i) It can solve problems
containing millions of segmentation instances in terabyte-scale 3D+t datasets;
(ii) It achieves competitive results with or without deep learning, which
requires 3D annotated data, that is scarce in the fluorescence microscopy
field. The proposed method computes cell tracks and segments using a hierarchy
of segmentation hypotheses and selects disjoint segments by maximizing the
overlap between adjacent frames. We show that this method achieves
state-of-the-art results in 3D images from the cell tracking challenge and has
a faster integer linear programming formulation. Moreover, our framework is
flexible and supports segmentations from off-the-shelf cell segmentation models
and can combine them into an ensemble that improves tracking. The code is
available https://github.com/royerlab/ultrack.",None,-1
f1a4f323-7972-4bdc-87f9-922bd7bcb3a7,Garment Recovery with Shape and Deformation Priors,0.581501,"While modeling people wearing tight-fitting clothing has made great strides
in recent years, loose-fitting clothing remains a challenge. We propose a
method that delivers realistic garment models from real-world images,
regardless of garment shape or deformation. To this end, we introduce a fitting
approach that utilizes shape and deformation priors learned from synthetic data
to accurately capture garment shapes and deformations, including large ones.
Not only does our approach recover the garment geometry accurately, it also
yields models that can be directly used by downstream applications such as
animation and simulation.",None,-1
03e7194c-4e55-4a21-9999-bbcbee191d98,A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model,0.301094,"Recently, the instruction-tuning of large language models is a crucial area
of research in the field of natural language processing. Due to resource and
cost limitations, several researchers have employed parameter-efficient tuning
techniques, such as LoRA, for instruction tuning, and have obtained encouraging
results In comparison to full-parameter fine-tuning, LoRA-based tuning
demonstrates salient benefits in terms of training costs. In this study, we
undertook experimental comparisons between full-parameter fine-tuning and
LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental
results show that the selection of the foundational model, training dataset
scale, learnable parameter quantity, and model training cost are all important
factors. We hope that the experimental conclusions of this paper can provide
inspiration for training large language models, especially in the field of
Chinese, and help researchers find a better trade-off strategy between training
cost and model performance. To facilitate the reproduction of the paper's
results, the dataset, model and code will be released.",None,-1
cbe5c563-5e5f-4c27-b155-4d74d33a5848,System 2 Attention (is something you might need too),0.710487,"Soft attention in Transformer-based Large Language Models (LLMs) is
susceptible to incorporating irrelevant information from the context into its
latent representations, which adversely affects next token generations. To help
rectify these issues, we introduce System 2 Attention (S2A), which leverages
the ability of LLMs to reason in natural language and follow instructions in
order to decide what to attend to. S2A regenerates the input context to only
include the relevant portions, before attending to the regenerated context to
elicit the final response. In experiments, S2A outperforms standard
attention-based LLMs on three tasks containing opinion or irrelevant
information, QA, math word problems and longform generation, where S2A
increases factuality and objectivity, and decreases sycophancy.",None,-1
dc77ff6c-912e-4240-b49d-63d117caf97a,NAP: Neural 3D Articulation Prior,0.491872,"We propose Neural 3D Articulation Prior (NAP), the first 3D deep generative
model to synthesize 3D articulated object models. Despite the extensive
research on generating 3D objects, compositions, or scenes, there remains a
lack of focus on capturing the distribution of articulated objects, a common
object category for human and robot interaction. To generate articulated
objects, we first design a novel articulation tree/graph parameterization and
then apply a diffusion-denoising probabilistic model over this representation
where articulated objects can be generated via denoising from random complete
graphs. In order to capture both the geometry and the motion structure whose
distribution will affect each other, we design a graph-attention denoising
network for learning the reverse diffusion process. We propose a novel distance
that adapts widely used 3D generation metrics to our novel task to evaluate
generation quality, and experiments demonstrate our high performance in
articulated object generation. We also demonstrate several conditioned
generation applications, including Part2Motion, PartNet-Imagination,
Motion2Part, and GAPart2Object.",None,-1
413ed3c4-5c00-40ce-8111-5da674ac7a0e,InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning,0.979533,"Recent advances in personalized image generation allow a pre-trained
text-to-image model to learn a new concept from a set of images. However,
existing personalization approaches usually require heavy test-time finetuning
for each concept, which is time-consuming and difficult to scale. We propose
InstantBooth, a novel approach built upon pre-trained text-to-image models that
enables instant text-guided image personalization without any test-time
finetuning. We achieve this with several major components. First, we learn the
general concept of the input images by converting them to a textual token with
a learnable image encoder. Second, to keep the fine details of the identity, we
learn rich visual feature representation by introducing a few adapter layers to
the pre-trained model. We train our components only on text-image pairs without
using paired images of the same concept. Compared to test-time finetuning-based
methods like DreamBooth and Textual-Inversion, our model can generate
competitive results on unseen concepts concerning language-image alignment,
image fidelity, and identity preservation while being 100 times faster.",None,-1
5d4e39e3-2ffa-475d-86d9-1697dc5d4cb8,The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages,0.505754,"Gender biases in language generation systems are challenging to mitigate. One
possible source for these biases is gender representation disparities in the
training and evaluation data. Despite recent progress in documenting this
problem and many attempts at mitigating it, we still lack shared methodology
and tooling to report gender representation in large datasets. Such
quantitative reporting will enable further mitigation, e.g., via data
augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware
Polyglot Pipeline), an automatic pipeline to characterize gender representation
in large-scale datasets for 55 languages. The pipeline uses a multilingual
lexicon of gendered person-nouns to quantify the gender representation in text.
We showcase it to report gender representation in WMT training data and
development data for the News task, confirming that current data is skewed
towards masculine representation. Having unbalanced datasets may indirectly
optimize our systems towards outperforming one gender over the others. We
suggest introducing our gender quantification pipeline in current datasets and,
ideally, modifying them toward a balanced representation.",None,-1
cd496a84-2e9e-4bb5-96af-167d1dc07ec3,CipherSniffer: Classifying Cipher Types,0.0419295,"Ciphers are a powerful tool for encrypting communication. There are many
different cipher types, which makes it computationally expensive to solve a
cipher using brute force. In this paper, we frame the decryption task as a
classification problem. We first create a dataset of transpositions,
substitutions, text reversals, word reversals, sentence shifts, and unencrypted
text. Then, we evaluate the performance of various tokenizer-model combinations
on this task.",None,-1
cc04d121-ec2e-4462-b5aa-8e2daa53eee1,FlowIBR: Leveraging Pre-Training for Efficient Neural Image-Based Rendering of Dynamic Scenes,0.599058,"We introduce FlowIBR, a novel approach for efficient monocular novel view
synthesis of dynamic scenes. Existing techniques already show impressive
rendering quality but tend to focus on optimization within a single scene
without leveraging prior knowledge, resulting in long optimization times per
scene. FlowIBR circumvents this limitation by integrating a neural image-based
rendering method, pre-trained on a large corpus of widely available static
scenes, with a per-scene optimized scene flow field. Utilizing this flow field,
we bend the camera rays to counteract the scene dynamics, thereby presenting
the dynamic scene as if it were static to the rendering network. The proposed
method reduces per-scene optimization time by an order of magnitude, achieving
comparable rendering quality to existing methods -- all on a single
consumer-grade GPU.",None,-1
698e51f0-0066-4b8e-9c74-071d2e537314,diff History for Neural Language Agents,0.0631455,"Neural Language Models (LMs) offer an exciting solution for general-purpose
embodied control. However, a key technical issue arises when using an LM-based
controller: environment observations must be converted to text, which coupled
with history, results in long and verbose textual prompts. As a result, prior
work in LM agents is limited to restricted domains with small observation size
as well as minimal needs for interaction history or instruction tuning. In this
paper, we introduce diff history, a simple and highly effective solution to
these issues. By applying the Unix diff command on consecutive text
observations in the interaction histories used to prompt LM policies, we can
both abstract away redundant information and focus the content of textual
inputs on the salient changes in the environment. On NetHack, an unsolved video
game that requires long-horizon reasoning for decision-making, LMs tuned with
diff history match state-of-the-art performance for neural agents while needing
1800x fewer training examples compared to prior work. Even on the simpler
BabyAI-Text environment with concise text observations, we find that although
diff history increases the length of prompts, the representation it provides
offers a 25% improvement in the efficiency of low-sample instruction tuning.
Further, we show that diff history scales favorably across different tuning
dataset sizes. We open-source our code and data to
https://diffhistory.github.io.",None,-1
4448bb6b-c553-42aa-a990-37421acbc5f7,Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings,0.44943,"This study investigates the consistency of feedback ratings generated by
OpenAI's GPT-4, a state-of-the-art artificial intelligence language model,
across multiple iterations, time spans and stylistic variations. The model
rated responses to tasks within the Higher Education (HE) subject domain of
macroeconomics in terms of their content and style. Statistical analysis was
conducted in order to learn more about the interrater reliability, consistency
of the ratings across iterations and the correlation between ratings in terms
of content and style. The results revealed a high interrater reliability with
ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting
that GPT-4 is capable of generating consistent ratings across repetitions with
a clear prompt. Style and content ratings show a high correlation of 0.87. When
applying a non-adequate style the average content ratings remained constant,
while style ratings decreased, which indicates that the large language model
(LLM) effectively distinguishes between these two criteria during evaluation.
The prompt used in this study is furthermore presented and explained. Further
research is necessary to assess the robustness and reliability of AI models in
various use cases.",None,-1
350c6158-92ba-420b-bb69-78cf2ef3033a,DEff-GAN: Diverse Attribute Transfer for Few-Shot Image Synthesis,0.0568573,"Requirements of large amounts of data is a difficulty in training many GANs.
Data efficient GANs involve fitting a generators continuous target distribution
with a limited discrete set of data samples, which is a difficult task. Single
image methods have focused on modeling the internal distribution of a single
image and generating its samples. While single image methods can synthesize
image samples with diversity, they do not model multiple images or capture the
inherent relationship possible between two images. Given only a handful of
images, we are interested in generating samples and exploiting the
commonalities in the input images. In this work, we extend the single-image GAN
method to model multiple images for sample synthesis. We modify the
discriminator with an auxiliary classifier branch, which helps to generate a
wide variety of samples and to classify the input labels. Our Data-Efficient
GAN (DEff-GAN) generates excellent results when similarities and
correspondences can be drawn between the input images or classes.",None,-1
6f2310a3-fff2-4be8-96f9-f85d27e03046,mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration,0.997542,"Multi-modal Large Language Models (MLLMs) have demonstrated impressive
instruction abilities across various open-ended tasks. However, previous
methods primarily focus on enhancing multi-modal capabilities. In this work, we
introduce a versatile multi-modal large language model, mPLUG-Owl2, which
effectively leverages modality collaboration to improve performance in both
text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design,
with the language decoder acting as a universal interface for managing
different modalities. Specifically, mPLUG-Owl2 incorporates shared functional
modules to facilitate modality collaboration and introduces a modality-adaptive
module that preserves modality-specific features. Extensive experiments reveal
that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal
tasks and achieving state-of-the-art performances with a single generic model.
Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality
collaboration phenomenon in both pure-text and multi-modal scenarios, setting a
pioneering path in the development of future multi-modal foundation models.",None,-1
448de281-1067-475c-b4e1-9452c1741433,Meta-Learned Models of Cognition,0.725674,"Meta-learning is a framework for learning learning algorithms through
repeated interactions with an environment as opposed to designing them by hand.
In recent years, this framework has established itself as a promising tool for
building models of human cognition. Yet, a coherent research program around
meta-learned models of cognition is still missing. The purpose of this article
is to synthesize previous work in this field and establish such a research
program. We rely on three key pillars to accomplish this goal. We first point
out that meta-learning can be used to construct Bayes-optimal learning
algorithms. This result not only implies that any behavioral phenomenon that
can be explained by a Bayesian model can also be explained by a meta-learned
model but also allows us to draw strong connections to the rational analysis of
cognition. We then discuss several advantages of the meta-learning framework
over traditional Bayesian methods. In particular, we argue that meta-learning
can be applied to situations where Bayesian inference is impossible and that it
enables us to make rational models of cognition more realistic, either by
incorporating limited computational resources or neuroscientific knowledge.
Finally, we reexamine prior studies from psychology and neuroscience that have
applied meta-learning and put them into the context of these new insights. In
summary, our work highlights that meta-learning considerably extends the scope
of rational analysis and thereby of cognitive theories more generally.",None,-1
ff159299-e68f-47a0-b1ba-44e377d49549,Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,0.972379,"Large Language Models (LLMs) excel in various Natural Language Processing
(NLP) tasks, yet their evaluation, particularly in languages beyond the top
$20$, remains inadequate due to existing benchmarks and metrics limitations.
Employing LLMs as evaluators to rank or score other models' outputs emerges as
a viable solution, addressing the constraints tied to human annotators and
established benchmarks. In this study, we explore the potential of LLM-based
evaluators, specifically GPT-4 in enhancing multilingual evaluation by
calibrating them against $20$K human judgments across three text-generation
tasks, five metrics, and eight languages. Our analysis reveals a bias in
GPT4-based evaluators towards higher scores, underscoring the necessity of
calibration with native speaker judgments, especially in low-resource and
non-Latin script languages, to ensure accurate evaluation of LLM performance
across diverse languages.",None,-1
34d62ec3-ad1c-4373-a603-39ca1c1ea15f,Learning Human Mesh Recovery in 3D Scenes,0.792739,"We present a novel method for recovering the absolute pose and shape of a
human in a pre-scanned scene given a single image. Unlike previous methods that
perform sceneaware mesh optimization, we propose to first estimate absolute
position and dense scene contacts with a sparse 3D CNN, and later enhance a
pretrained human mesh recovery network by cross-attention with the derived 3D
scene cues. Joint learning on images and scene geometry enables our method to
reduce the ambiguity caused by depth and occlusion, resulting in more
reasonable global postures and contacts. Encoding scene-aware cues in the
network also allows the proposed method to be optimization-free, and opens up
the opportunity for real-time applications. The experiments show that the
proposed network is capable of recovering accurate and physically-plausible
meshes by a single forward pass and outperforms state-of-the-art methods in
terms of both accuracy and speed.",None,-1
bcfd8f11-9382-4f70-8eed-6a053896f1b7,Area of interest adaption using feature importance,0.263474,"In this paper, we present two approaches and algorithms that adapt areas of
interest (AOI) or regions of interest (ROI), respectively, to the eye tracking
data quality and classification task. The first approach uses feature
importance in a greedy way and grows or shrinks AOIs in all directions. The
second approach is an extension of the first approach, which divides the AOIs
into areas and calculates a direction of growth, i.e. a gradient. Both
approaches improve the classification results considerably in the case of
generalized AOIs, but can also be used for qualitative analysis. In qualitative
analysis, the algorithms presented allow the AOIs to be adapted to the data,
which means that errors and inaccuracies in eye tracking data can be better
compensated for. A good application example is abstract art, where manual AOIs
annotation is hardly possible, and data-driven approaches are mainly used for
initial AOIs.
  Link:
https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FAOIGradient&mode=list",None,-1
ac77efec-ec19-468b-a945-e0a21d811588,The MiniPile Challenge for Data-Efficient Language Models,0.476068,"The ever-growing diversity of pre-training text corpora has equipped language
models with generalization capabilities across various downstream tasks.
However, such diverse datasets are often too large for academic budgets; hence,
most research on Transformer architectures, training procedures, optimizers,
etc. gets conducted on smaller, homogeneous datasets. To this end, we present
The MiniPile Challenge, where one pre-trains a language model on a diverse text
corpus containing at most 1M documents. MiniPile is a 6GB subset of the
deduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple,
three-step data filtering process: we (1) infer embeddings for all documents of
the Pile, (2) cluster the embedding space using $k$-means, and (3) filter out
low-quality clusters. To verify MiniPile's suitability for language model
pre-training, we use it to pre-train a BERT and T5 model, yielding a
performance drop of only $1.9\%$/$2.5\%$ on the GLUE and SNI benchmarks
compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the
amount of data. MiniPile is available at
https://huggingface.co/datasets/JeanKaddour/minipile.",None,-1
6353b1e4-5c25-47af-9692-6c823fb3d558,AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder,0.841606,"To easily obtain the knowledge about autism spectrum disorder and help its
early screening and diagnosis, we create AsdKB, a Chinese knowledge base on
autism spectrum disorder. The knowledge base is built on top of various
sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical
descriptions on mental and behavioural disorders, 2) the diagnostic knowledge
from DSM-5 and different screening tools recommended by social organizations
and medical institutes, and 3) the expert knowledge on professional physicians
and hospitals from the Web. AsdKB contains both ontological and factual
knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The
potential applications of AsdKB are question answering, auxiliary diagnosis,
and expert recommendation, and we illustrate them with a prototype which can be
accessed at http://asdkb.org.cn/.",None,-1
ef183206-51a9-4292-99f3-c690fa2fde19,"Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment",0.758037,"Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.",None,-1
d244adc9-1f44-47e3-838b-d705fd5bc450,Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training,0.112928,"Multimodal reasoning is a challenging task that requires models to reason
across multiple modalities to answer questions. Existing approaches have made
progress by incorporating language and visual modalities into a two-stage
reasoning framework, separating rationale generation from answer inference.
However, these approaches often fall short due to the inadequate quality of the
generated rationales. In this work, we delve into the importance of rationales
in model reasoning. We observe that when rationales are completely accurate,
the model's accuracy significantly improves, highlighting the need for
high-quality rationale generation. Motivated by this, we propose MC-CoT, a
self-consistency training strategy that generates multiple rationales and
answers, subsequently selecting the most accurate through a voting process.
This approach not only enhances the quality of generated rationales but also
leads to more accurate and robust answers. Through extensive experiments, we
demonstrate that our approach significantly improves model performance across
various benchmarks. Remarkably, we show that even smaller base models, when
equipped with our proposed approach, can achieve results comparable to those of
larger models, illustrating the potential of our approach in harnessing the
power of rationales for improved multimodal reasoning. The code is available at
https://github.com/chengtan9907/mc-cot.",None,-1
bff4cb4c-5613-4648-8c9a-11d13be18713,Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations,0.55298,"Discourse analysis is an important task because it models intrinsic semantic
structures between sentences in a document. Discourse markers are natural
representations of discourse in our daily language. One challenge is that the
markers as well as pre-defined and human-labeled discourse relations can be
ambiguous when describing the semantics between sentences. We believe that a
better approach is to use a contextual-dependent distribution over the markers
to express discourse information. In this work, we propose to learn a
Distributed Marker Representation (DMR) by utilizing the (potentially)
unlimited discourse marker data with a latent discourse sense, thereby bridging
markers with sentence pairs. Such representations can be learned automatically
from data without supervision, and in turn provide insights into the data
itself. Experiments show the SOTA performance of our DMR on the implicit
discourse relation recognition task and strong interpretability. Our method
also offers a valuable tool to understand complex ambiguity and entanglement
among discourse markers and manually defined discourse relations.",None,-1
56f3e730-bafc-404f-ad52-d1a85e7d681f,Patton: Language Model Pretraining on Text-Rich Networks,0.699437,"A real-world text corpus sometimes comprises not only text documents but also
semantic links between them (e.g., academic papers in a bibliographic network
are linked by citations and co-authorships). Text documents and semantic
connections form a text-rich network, which empowers a wide range of downstream
tasks such as classification and retrieval. However, pretraining methods for
such structures are still lacking, making it difficult to build one generic
model that can be adapted to various tasks on text-rich networks. Current
pretraining objectives, such as masked language modeling, purely model texts
and do not take inter-document structure information into consideration. To
this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.
Patton includes two pretraining strategies: network-contextualized masked
language modeling and masked node prediction, to capture the inherent
dependency between textual attributes and network structure. We conduct
experiments on four downstream tasks in five datasets from both academic and
e-commerce domains, where Patton outperforms baselines significantly and
consistently.",None,-1
1c913804-0854-4627-b64b-037f69ea0d2c,Grounding for Artificial Intelligence,0.199381,"A core function of intelligence is grounding, which is the process of
connecting the natural language and abstract knowledge to the internal
representation of the real world in an intelligent being, e.g., a human. Human
cognition is grounded in our sensorimotor experiences in the external world and
subjective feelings in our internal world. We use languages to communicate with
each other and the languages are grounded on our shared sensorimotor
experiences and feelings. Without this shard grounding, it is impossible for us
to understand each other because all natural languages are highly abstract and
are only able to describe a tiny portion of what has happened or is happening
in the real world. Although grounding at high or abstract levels has been
studied in different fields and applications, to our knowledge, limited
systematic work at fine-grained levels has been done. With the rapid progress
of large language models (LLMs), it is imperative that we have a sound
understanding of grounding in order to move to the next level of intelligence.
It is also believed that grounding is necessary for Artificial General
Intelligence (AGI). This paper makes an attempt to systematically study this
problem.",None,-1
89b48828-89f7-4e1f-b110-bbea24d51e68,KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases,0.959316,"Large language models (LLMs) have demonstrated impressive impact in the field
of natural language processing, but they still struggle with several issues
regarding, such as completeness, timeliness, faithfulness and adaptability.
While recent efforts have focuses on connecting LLMs with external knowledge
sources, the integration of knowledge bases (KBs) remains understudied and
faces several challenges. In this paper, we introduce KnowledGPT, a
comprehensive framework to bridge LLMs with various knowledge bases,
facilitating both the retrieval and storage of knowledge. The retrieval process
employs the program of thought prompting, which generates search language for
KBs in code format with pre-defined functions for KB operations. Besides
retrieval, KnowledGPT offers the capability to store knowledge in a
personalized KB, catering to individual user demands. With extensive
experiments, we show that by integrating LLMs with KBs, KnowledGPT properly
answers a broader range of questions requiring world knowledge compared with
vanilla LLMs, utilizing both knowledge existing in widely-known KBs and
extracted into personalized KBs.",None,-1
6ab9c6d1-0338-4c35-86b6-1b6d536bc193,GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding,0.697726,"Humans subconsciously engage in geospatial reasoning when reading articles.
We recognize place names and their spatial relations in text and mentally
associate them with their physical locations on Earth. Although pretrained
language models can mimic this cognitive process using linguistic context, they
do not utilize valuable geospatial information in large, widely available
geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a
geospatially grounded language model that enhances the understanding of
geo-entities in natural language. GeoLM leverages geo-entity mentions as
anchors to connect linguistic information in text corpora with geospatial
information extracted from geographical databases. GeoLM connects the two types
of context through contrastive learning and masked language modeling. It also
incorporates a spatial coordinate embedding mechanism to encode distance and
direction relations to capture geospatial context. In the experiment, we
demonstrate that GeoLM exhibits promising capabilities in supporting toponym
recognition, toponym linking, relation extraction, and geo-entity typing, which
bridge the gap between natural language processing and geospatial sciences. The
code is publicly available at https://github.com/knowledge-computing/geolm.",None,-1
71d88cf4-4f20-439e-8c1f-dfcafde35757,On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval,0.35882,"Visually-rich document entity retrieval (VDER), which extracts key
information (e.g. date, address) from document images like invoices and
receipts, has become an important topic in industrial NLP applications. The
emergence of new document types at a constant pace, each with its unique entity
types, presents a unique challenge: many documents contain unseen entity types
that occur only a couple of times. Addressing this challenge requires models to
have the ability of learning entities in a few-shot manner. However, prior
works for Few-shot VDER mainly address the problem at the document level with a
predefined global entity space, which doesn't account for the entity-level
few-shot scenario: target entity types are locally personalized by each task
and entity occurrences vary significantly among documents. To address this
unexplored scenario, this paper studies a novel entity-level few-shot VDER
task. The challenges lie in the uniqueness of the label space for each task and
the increased complexity of out-of-distribution (OOD) contents. To tackle this
novel task, we present a task-aware meta-learning based framework, with a
central focus on achieving effective task personalization that distinguishes
between in-task and out-of-task distribution. Specifically, we adopt a
hierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) to
achieve this goal. Furthermore, we introduce a new dataset, FewVEX, to boost
future research in the field of entity-level few-shot VDER. Experimental
results demonstrate our approaches significantly improve the robustness of
popular meta-learning baselines.",None,-1
35482194-3c62-46f7-a8a5-b095d5308926,Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning,0.0575029,"Preference Based Reinforcement Learning has shown much promise for utilizing
human binary feedback on queried trajectory pairs to recover the underlying
reward model of the Human in the Loop (HiL). While works have attempted to
better utilize the queries made to the human, in this work we make two
observations about the unlabeled trajectories collected by the agent and
propose two corresponding loss functions that ensure participation of unlabeled
trajectories in the reward learning process, and structure the embedding space
of the reward model such that it reflects the structure of state space with
respect to action distances. We validate the proposed method on one locomotion
domain and one robotic manipulation task and compare with the state-of-the-art
baseline PEBBLE. We further present an ablation of the proposed loss components
across both the domains and find that not only each of the loss components
perform better than the baseline, but the synergic combination of the two has
much better reward recovery and human feedback sample efficiency.",None,-1
52be9eb8-e4f7-49aa-9b41-b72ef4e01eb2,Scalable AI Generative Content for Vehicular Network Semantic Communication,0.417907,"Perceiving vehicles in a driver's blind spot is vital for safe driving. The
detection of potentially dangerous vehicles in these blind spots can benefit
from vehicular network semantic communication technology. However, efficient
semantic communication involves a trade-off between accuracy and delay,
especially in bandwidth-limited situations. This paper unveils a scalable
Artificial Intelligence Generated Content (AIGC) system that leverages an
encoder-decoder architecture. This system converts images into textual
representations and reconstructs them into quality-acceptable images,
optimizing transmission for vehicular network semantic communication. Moreover,
when bandwidth allows, auxiliary information is integrated. The encoder-decoder
aims to maintain semantic equivalence with the original images across various
tasks. Then the proposed approach employs reinforcement learning to enhance the
reliability of the generated contents. Experimental results suggest that the
proposed method surpasses the baseline in perceiving vehicles in blind spots
and effectively compresses communication data. While this method is
specifically designed for driving scenarios, this encoder-decoder architecture
also holds potential for wide use across various semantic communication
scenarios.",None,-1
11baf3ba-decd-4dd6-a6fa-82ba60e5ddcc,ParaFormer: Parallel Attention Transformer for Efficient Feature Matching,0.683391,"Heavy computation is a bottleneck limiting deep-learningbased feature
matching algorithms to be applied in many realtime applications. However,
existing lightweight networks optimized for Euclidean data cannot address
classical feature matching tasks, since sparse keypoint based descriptors are
expected to be matched. This paper tackles this problem and proposes two
concepts: 1) a novel parallel attention model entitled ParaFormer and 2) a
graph based U-Net architecture with attentional pooling. First, ParaFormer
fuses features and keypoint positions through the concept of amplitude and
phase, and integrates self- and cross-attention in a parallel manner which
achieves a win-win performance in terms of accuracy and efficiency. Second,
with U-Net architecture and proposed attentional pooling, the ParaFormer-U
variant significantly reduces computational complexity, and minimize
performance loss caused by downsampling. Sufficient experiments on various
applications, including homography estimation, pose estimation, and image
matching, demonstrate that ParaFormer achieves state-of-the-art performance
while maintaining high efficiency. The efficient ParaFormer-U variant achieves
comparable performance with less than 50% FLOPs of the existing attention-based
models.",None,-1
fc25e56e-8939-4075-8cd1-04027d8cb28d,Sparse 3D Reconstruction via Object-Centric Ray Sampling,0.226125,"We propose a novel method for 3D object reconstruction from a sparse set of
views captured from a 360-degree calibrated camera rig. We represent the object
surface through a hybrid model that uses both an MLP-based neural
representation and a triangle mesh. A key contribution in our work is a novel
object-centric sampling scheme of the neural representation, where rays are
shared among all views. This efficiently concentrates and reduces the number of
samples used to update the neural model at each iteration. This sampling scheme
relies on the mesh representation to ensure also that samples are
well-distributed along its normals. The rendering is then performed efficiently
by a differentiable renderer. We demonstrate that this sampling scheme results
in a more effective training of the neural representation, does not require the
additional supervision of segmentation masks, yields state of the art 3D
reconstructions, and works with sparse views on the Google's Scanned Objects,
Tank and Temples and MVMC Car datasets. Code available at:
https://github.com/llukmancerkezi/ROSTER",None,-1
b3441c69-27ed-4890-aa11-8c9db658eb3b,GoogLe2Net: Going Transverse with Convolutions,0.0304891,"Capturing feature information effectively is of great importance in vision
tasks. With the development of convolutional neural networks (CNNs), concepts
like residual connection and multiple scales promote continual performance
gains on diverse deep learning vision tasks. However, the existing methods do
not organically combined advantages of these valid ideas. In this paper, we
propose a novel CNN architecture called GoogLe2Net, it consists of residual
feature-reutilization inceptions (ResFRI) or split residual
feature-reutilization inceptions (Split-ResFRI) which create transverse
passages between adjacent groups of convolutional layers to enable features
flow to latter processing branches and possess residual connections to better
process information. Our GoogLe2Net is able to reutilize information captured
by foregoing groups of convolutional layers and express multi-scale features at
a fine-grained level, which improves performances in image classification. And
the inception we proposed could be embedded into inception-like networks
directly without any migration costs. Moreover, in experiments based on popular
vision datasets, such as CIFAR10 (97.94%), CIFAR100 (85.91%) and Tiny Imagenet
(70.54%), we obtain better results on image classification task compared with
other modern models.",None,-1
75e051f8-5bb0-47f0-a890-bf4cffe701ce,An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback,0.479051,"While search is the predominant method of accessing information, formulating
effective queries remains a challenging task, especially for situations where
the users are not familiar with a domain, or searching for documents in other
languages, or looking for complex information such as events, which are not
easily expressible as queries. Providing example documents or passages of
interest, might be easier for a user, however, such query-by-example scenarios
are prone to concept drift, and are highly sensitive to the query generation
method. This demo illustrates complementary approaches of using LLMs
interactively, assisting and enabling the user to provide edits and feedback at
all stages of the query formulation process. The proposed Query Generation
Assistant is a novel search interface which supports automatic and interactive
query generation over a mono-linguial or multi-lingual document collection.
Specifically, the proposed assistive interface enables the users to refine the
queries generated by different LLMs, to provide feedback on the retrieved
documents or passages, and is able to incorporate the users' feedback as
prompts to generate more effective queries. The proposed interface is a
valuable experimental tool for exploring fine-tuning and prompting of LLMs for
query generation to qualitatively evaluate the effectiveness of retrieval and
ranking models, and for conducting Human-in-the-Loop (HITL) experiments for
complex search tasks where users struggle to formulate queries without such
assistance.",None,-1
fb98b70a-f607-4df9-aad0-cddba5d89dbb,Representation Matters: The Game of Chess Poses a Challenge to Vision Transformers,0.0828452,"While transformers have gained the reputation as the ""Swiss army knife of
AI"", no one has challenged them to master the game of chess, one of the
classical AI benchmarks. Simply using vision transformers (ViTs) within
AlphaZero does not master the game of chess, mainly because ViTs are too slow.
Even making them more efficient using a combination of MobileNet and NextViT
does not beat what actually matters: a simple change of the input
representation and value loss, resulting in a greater boost of up to 180 Elo
points over AlphaZero.",None,-1
245c269b-998a-48b1-b2b9-39aa2a682543,GeoNet: Benchmarking Unsupervised Adaptation across Geographies,0.492142,"In recent years, several efforts have been aimed at improving the robustness
of vision models to domains and environments unseen during training. An
important practical problem pertains to models deployed in a new geography that
is under-represented in the training dataset, posing a direct challenge to fair
and inclusive computer vision. In this paper, we study the problem of
geographic robustness and make three main contributions. First, we introduce a
large-scale dataset GeoNet for geographic adaptation containing benchmarks
across diverse tasks like scene recognition (GeoPlaces), image classification
(GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the
nature of distribution shifts typical to the problem of geographic adaptation
and hypothesize that the major source of domain shifts arise from significant
variations in scene context (context shift), object design (design shift) and
label distribution (prior shift) across geographies. Third, we conduct an
extensive evaluation of several state-of-the-art unsupervised domain adaptation
algorithms and architectures on GeoNet, showing that they do not suffice for
geographical adaptation, and that large-scale pre-training using large vision
models also does not lead to geographic robustness. Our dataset is publicly
available at https://tarun005.github.io/GeoNet.",None,-1
53e52477-13f8-4b9d-90c4-40946b109d04,Solving Travelling Thief Problems using Coordination Based Methods,0.517159,"A travelling thief problem (TTP) is a proxy to real-life problems such as
postal collection. TTP comprises an entanglement of a travelling salesman
problem (TSP) and a knapsack problem (KP) since items of KP are scattered over
cities of TSP, and a thief has to visit cities to collect items. In TTP, city
selection and item selection decisions need close coordination since the
thief's travelling speed depends on the knapsack's weight and the order of
visiting cities affects the order of item collection. Existing TTP solvers deal
with city selection and item selection separately, keeping decisions for one
type unchanged while dealing with the other type. This separation essentially
means very poor coordination between two types of decision. In this paper, we
first show that a simple local search based coordination approach does not work
in TTP. Then, to address the aforementioned problems, we propose a human
designed coordination heuristic that makes changes to collection plans during
exploration of cyclic tours. We further propose another human designed
coordination heuristic that explicitly exploits the cyclic tours in item
selections during collection plan exploration. Lastly, we propose a machine
learning based coordination heuristic that captures characteristics of the two
human designed coordination heuristics. Our proposed coordination based
approaches help our TTP solver significantly outperform existing
state-of-the-art TTP solvers on a set of benchmark problems. Our solver is
named Cooperation Coordination (CoCo) and its source code is available from
https://github.com/majid75/CoCo",None,-1
e3ca8fef-9243-462b-ab84-3f5a91bad962,Structured State Space Models for Multiple Instance Learning in Digital Pathology,0.859252,"Multiple instance learning is an ideal mode of analysis for histopathology
data, where vast whole slide images are typically annotated with a single
global label. In such cases, a whole slide image is modelled as a collection of
tissue patches to be aggregated and classified. Common models for performing
this classification include recurrent neural networks and transformers.
Although powerful compression algorithms, such as deep pre-trained neural
networks, are used to reduce the dimensionality of each patch, the sequences
arising from whole slide images remain excessively long, routinely containing
tens of thousands of patches. Structured state space models are an emerging
alternative for sequence modelling, specifically designed for the efficient
modelling of long sequences. These models invoke an optimal projection of an
input sequence into memory units that compress the entire sequence. In this
paper, we propose the use of state space models as a multiple instance learner
to a variety of problems in digital pathology. Across experiments in metastasis
detection, cancer subtyping, mutation classification, and multitask learning,
we demonstrate the competitiveness of this new class of models with existing
state of the art approaches. Our code is available at
https://github.com/MICS-Lab/s4_digital_pathology.",None,-1
200feff4-7ed6-403e-980f-f964d62edae9,Causal Confirmation Measures: From Simpson's Paradox to COVID-19,0.537795,"When we compare the influences of two causes on an outcome, if the conclusion
from every group is against that from the conflation, we think there is
Simpson's Paradox. The Existing Causal Inference Theory (ECIT) can make the
overall conclusion consistent with the grouping conclusion by removing the
confounder's influence to eliminate the paradox. The ECIT uses relative risk
difference Pd = max(0, (R - 1)/R) (R denotes the risk ratio) as the probability
of causation. In contrast, Philosopher Fitelson uses confirmation measure D
(posterior probability minus prior probability) to measure the strength of
causation. Fitelson concludes that from the perspective of Bayesian
confirmation, we should directly accept the overall conclusion without
considering the paradox. The author proposed a Bayesian confirmation measure b*
similar to Pd before. To overcome the contradiction between the ECIT and
Bayesian confirmation, the author uses the semantic information method with the
minimum cross-entropy criterion to deduce causal confirmation measure Cc = (R
-1)/max(R, 1). Cc is like Pd but has normalizing property (between -1 and 1)
and cause symmetry. It especially fits cases where a cause restrains an
outcome, such as the COVID-19 vaccine controlling the infection. Some examples
(about kidney stone treatments and COVID-19) reveal that Pd and Cc are more
reasonable than D; Cc is more useful than Pd.",None,-1
78bda044-c2ee-47fb-a495-766bb0ee7092,Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,0.524583,"Real-world fact verification task aims to verify the factuality of a claim by
retrieving evidence from the source document. The quality of the retrieved
evidence plays an important role in claim verification. Ideally, the retrieved
evidence should be faithful (reflecting the model's decision-making process in
claim verification) and plausible (convincing to humans), and can improve the
accuracy of verification task. Although existing approaches leverage the
similarity measure of semantic or surface form between claims and documents to
retrieve evidence, they all rely on certain heuristics that prevent them from
satisfying all three requirements. In light of this, we propose a fact
verification model named ReRead to retrieve evidence and verify claim that: (1)
Train the evidence retriever to obtain interpretable evidence (i.e.,
faithfulness and plausibility criteria); (2) Train the claim verifier to
revisit the evidence retrieved by the optimized evidence retriever to improve
the accuracy. The proposed system is able to achieve significant improvements
upon best-reported models under different settings.",None,-1
daab5080-a3c4-46f3-afe1-06edf0904244,Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention,0.385735,"Large transformer models have achieved state-of-the-art results in numerous
natural language processing tasks. Among the pivotal components of the
transformer architecture, the attention mechanism plays a crucial role in
capturing token interactions within sequences through the utilization of
softmax function.
  Conversely, linear attention presents a more computationally efficient
alternative by approximating the softmax operation with linear complexity.
However, it exhibits substantial performance degradation when compared to the
traditional softmax attention mechanism.
  In this paper, we bridge the gap in our theoretical understanding of the
reasons behind the practical performance gap between softmax and linear
attention. By conducting a comprehensive comparative analysis of these two
attention mechanisms, we shed light on the underlying reasons for why softmax
attention outperforms linear attention in most scenarios.",None,-1
fee2d9bf-d5c6-421a-b0ec-cd38a7f684ee,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,0.283784,"Image search engines enable the retrieval of images relevant to a query
image. In this work, we consider the setting where a query for similar images
is derived from a collection of images. For visual search, the similarity
measurements may be made along multiple axes, or views, such as style and
color. We assume access to a set of feature extractors, each of which computes
representations for a specific view. Our objective is to design a retrieval
algorithm that effectively combines similarities computed over representations
from multiple views. To this end, we propose a self-supervised learning method
for extracting disentangled view-specific representations for images such that
the inter-view overlap is minimized. We show how this allows us to compute the
intent of a collection as a distribution over views. We show how effective
retrieval can be performed by prioritizing candidate expansion images that
match the intent of a query collection. Finally, we present a new querying
mechanism for image search enabled by composing multiple collections and
perform retrieval under this setting using the techniques presented in this
paper.",None,-1
7250cedb-9316-4b2d-964c-90aa7117ac73,Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,0.74298,"Large language models (LLMs) such as ChatGPT are increasingly being used for
various use cases, including text content generation at scale. Although
detection methods for such AI-generated text exist already, we investigate
ChatGPT's performance as a detector on such AI-generated text, inspired by
works that use ChatGPT as a data labeler or annotator. We evaluate the
zero-shot performance of ChatGPT in the task of human-written vs. AI-generated
text detection, and perform experiments on publicly available datasets. We
empirically investigate if ChatGPT is symmetrically effective in detecting
AI-generated or human-written text. Our findings provide insight on how ChatGPT
and similar LLMs may be leveraged in automated detection pipelines by simply
focusing on solving a specific aspect of the problem and deriving the rest from
that solution. All code and data is available at
https://github.com/AmritaBh/ChatGPT-as-Detector.",None,-1
2fa2bd7a-0355-4fea-a2d3-1cd162783cf5,Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning,0.569656,"Clinical note classification is a common clinical NLP task. However,
annotated data-sets are scarse. Prompt-based learning has recently emerged as
an effective method to adapt pre-trained models for text classification using
only few training examples. A critical component of prompt design is the
definition of the template (i.e. prompt text). The effect of template position,
however, has been insufficiently investigated. This seems particularly
important in the clinical setting, where task-relevant information is usually
sparse in clinical notes. In this study we develop a keyword-optimized template
insertion method (KOTI) and show how optimizing position can improve
performance on several clinical tasks in a zero-shot and few-shot training
setting.",None,-1
b4f850c7-bfde-4800-ab5f-4f1b22b69db1,Knowledge-Based Counterfactual Queries for Visual Question Answering,0.0368241,"Visual Question Answering (VQA) has been a popular task that combines vision
and language, with numerous relevant implementations in literature. Even though
there are some attempts that approach explainability and robustness issues in
VQA models, very few of them employ counterfactuals as a means of probing such
challenges in a model-agnostic way. In this work, we propose a systematic
method for explaining the behavior and investigating the robustness of VQA
models through counterfactual perturbations. For this reason, we exploit
structured knowledge bases to perform deterministic, optimal and controllable
word-level replacements targeting the linguistic modality, and we then evaluate
the model's response against such counterfactual inputs. Finally, we
qualitatively extract local and global explanations based on counterfactual
responses, which are ultimately proven insightful towards interpreting VQA
model behaviors. By performing a variety of perturbation types, targeting
different parts of speech of the input question, we gain insights to the
reasoning of the model, through the comparison of its responses in different
adversarial circumstances. Overall, we reveal possible biases in the
decision-making process of the model, as well as expected and unexpected
patterns, which impact its performance quantitatively and qualitatively, as
indicated by our analysis.",None,-1
1e45f013-115f-44b0-942c-e90d259ed98f,TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play,0.632699,"Multi-agent football poses an unsolved challenge in AI research. Existing
work has focused on tackling simplified scenarios of the game, or else
leveraging expert demonstrations. In this paper, we develop a multi-agent
system to play the full 11 vs. 11 game mode, without demonstrations. This game
mode contains aspects that present major challenges to modern reinforcement
learning algorithms; multi-agent coordination, long-term planning, and
non-transitivity. To address these challenges, we present TiZero; a
self-evolving, multi-agent system that learns from scratch. TiZero introduces
several innovations, including adaptive curriculum learning, a novel self-play
strategy, and an objective that optimizes the policies of multiple agents
jointly. Experimentally, it outperforms previous systems by a large margin on
the Google Research Football environment, increasing win rates by over 30%. To
demonstrate the generality of TiZero's innovations, they are assessed on
several environments beyond football; Overcooked, Multi-agent
Particle-Environment, Tic-Tac-Toe and Connect-Four.",None,-1
3edcfb06-3927-4b02-ad2f-1ca02f30651e,Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior,0.524051,"We propose a surrogate function for efficient use of score-based priors for
Bayesian inverse imaging. Recent work turned score-based diffusion models into
probabilistic priors for solving ill-posed imaging problems by appealing to an
ODE-based log-probability function. However, evaluating this function is
computationally inefficient and inhibits posterior estimation of
high-dimensional images. Our proposed surrogate prior is based on the evidence
lower-bound of a score-based diffusion model. We demonstrate the surrogate
prior on variational inference for efficient approximate posterior sampling of
large images. Compared to the exact prior in previous work, our surrogate prior
accelerates optimization of the variational image distribution by at least two
orders of magnitude. We also find that our principled approach achieves
higher-fidelity images than non-Bayesian baselines that involve
hyperparameter-tuning at inference. Our work establishes a practical path
forward for using score-based diffusion models as general-purpose priors for
imaging.",None,-1
84996ddc-0a02-4de7-8fc6-4a91f9b49cf1,Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification,0.168199,"This article investigates the knowledge transfer from the RuQTopics dataset.
This Russian topical dataset combines a large sample number (361,560
single-label, 170,930 multi-label) with extensive class coverage (76 classes).
We have prepared this dataset from the ""Yandex Que"" raw data. By evaluating the
RuQTopics - trained models on the six matching classes of the Russian MASSIVE
subset, we have proved that the RuQTopics dataset is suitable for real-world
conversational tasks, as the Russian-only models trained on this dataset
consistently yield an accuracy around 85\% on this subset. We also have figured
out that for the multilingual BERT, trained on the RuQTopics and evaluated on
the same six classes of MASSIVE (for all MASSIVE languages), the language-wise
accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)
with the approximate size of the pretraining BERT's data for the corresponding
language. At the same time, the correlation of the language-wise accuracy with
the linguistical distance from Russian is not statistically significant.",None,-1
ec45a8d0-4f32-4434-9900-a0361ad32060,When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP,0.140177,"Multi-task learning (MTL) aims at achieving a better model by leveraging data
and knowledge from multiple tasks. However, MTL does not always work --
sometimes negative transfer occurs between tasks, especially when aggregating
loosely related skills, leaving it an open question when MTL works. Previous
studies show that MTL performance can be improved by algorithmic tricks.
However, what tasks and skills should be included is less well explored. In
this work, we conduct a case study in Financial NLP where multiple datasets
exist for skills relevant to the domain, such as numeric reasoning and
sentiment analysis. Due to the task difficulty and data scarcity in the
Financial NLP domain, we explore when aggregating such diverse skills from
multiple datasets with MTL can work. Our findings suggest that the key to MTL
success lies in skill diversity, relatedness between tasks, and choice of
aggregation size and shared capacity. Specifically, MTL works well when tasks
are diverse but related, and when the size of the task aggregation and the
shared capacity of the model are balanced to avoid overwhelming certain tasks.",None,-1
0d5964dc-9952-442e-a271-6bc944c09fc9,ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation,0.981731,"We introduce ""ImageDream,"" an innovative image-prompt, multi-view diffusion
model for 3D object generation. ImageDream stands out for its ability to
produce 3D models of higher quality compared to existing state-of-the-art,
image-conditioned methods. Our approach utilizes a canonical camera
coordination for the objects in images, improving visual geometry accuracy. The
model is designed with various levels of control at each block inside the
diffusion model based on the input image, where global control shapes the
overall object layout and local control fine-tunes the image details. The
effectiveness of ImageDream is demonstrated through extensive evaluations using
a standard prompt list. For more information, visit our project page at
https://Image-Dream.github.io.",None,-1
bd49f794-5ade-447d-b80c-6260086bb9a4,TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering,0.0823187,"We present a new pipeline for acquiring a textured mesh in the wild with a
single smartphone which offers access to images, depth maps, and valid poses.
Our method first introduces an RGBD-aided structure from motion, which can
yield filtered depth maps and refines camera poses guided by corresponding
depth. Then, we adopt the neural implicit surface reconstruction method, which
allows for high-quality mesh and develops a new training process for applying a
regularization provided by classical multi-view stereo methods. Moreover, we
apply a differentiable rendering to fine-tune incomplete texture maps and
generate textures which are perceptually closer to the original scene. Our
pipeline can be applied to any common objects in the real world without the
need for either in-the-lab environments or accurate mask images. We demonstrate
results of captured objects with complex shapes and validate our method
numerically against existing 3D reconstruction and texture mapping methods.",None,-1
98d3d72e-4ff5-4d91-b445-abe015ffc706,"SHARP Challenge 2023: Solving CAD History and pArameters Recovery from Point clouds and 3D scans. Overview, Datasets, Metrics, and Baselines",0.394077,"Recent breakthroughs in geometric Deep Learning (DL) and the availability of
large Computer-Aided Design (CAD) datasets have advanced the research on
learning CAD modeling processes and relating them to real objects. In this
context, 3D reverse engineering of CAD models from 3D scans is considered to be
one of the most sought-after goals for the CAD industry. However, recent
efforts assume multiple simplifications limiting the applications in real-world
settings. The SHARP Challenge 2023 aims at pushing the research a step closer
to the real-world scenario of CAD reverse engineering through dedicated
datasets and tracks. In this paper, we define the proposed SHARP 2023 tracks,
describe the provided datasets, and propose a set of baseline methods along
with suitable evaluation metrics to assess the performance of the track
solutions. All proposed datasets along with useful routines and the evaluation
metrics are publicly available.",None,-1
cd6c3015-af40-478c-9fbb-9faa33d4881b,Do LLM Agents Exhibit Social Behavior?,0.91663,"The advances of Large Language Models (LLMs) are expanding their utility in
both academic research and practical applications. Recent social science
research has explored the use of these ``black-box'' LLM agents for simulating
complex social systems and potentially substituting human subjects in
experiments. Our study delves into this emerging domain, investigating the
extent to which LLMs exhibit key social interaction principles, such as social
learning, social preference, and cooperative behavior (indirect reciprocity),
in their interactions with humans and other agents. We develop a framework for
our study, wherein classical laboratory experiments involving human subjects
are adapted to use LLM agents. This approach involves step-by-step reasoning
that mirrors human cognitive processes and zero-shot learning to assess the
innate preferences of LLMs. Our analysis of LLM agents' behavior includes both
the primary effects and an in-depth examination of the underlying mechanisms.
Focusing on GPT-4, our analyses suggest that LLM agents appear to exhibit a
range of human-like social behaviors such as distributional and reciprocity
preferences, responsiveness to group identity cues, engagement in indirect
reciprocity, and social learning capabilities. However, our analysis also
reveals notable differences: LLMs demonstrate a pronounced fairness preference,
weaker positive reciprocity, and a more calculating approach in social learning
compared to humans. These insights indicate that while LLMs hold great promise
for applications in social science research, such as in laboratory experiments
and agent-based modeling, the subtle behavioral differences between LLM agents
and humans warrant further investigation. Careful examination and development
of protocols in evaluating the social behaviors of LLMs are necessary before
directly applying these models to emulate human behavior.",None,-1
cdc4487e-186c-455b-b07e-fee6fefd1b53,Propagating Knowledge Updates to LMs Through Distillation,0.536128,"Modern language models have the capacity to store and use immense amounts of
knowledge about real-world entities, but it remains unclear how to update such
knowledge stored in model parameters. While prior methods for updating
knowledge in LMs successfully inject atomic facts, updated LMs fail to make
inferences based on injected facts. In this work, we demonstrate that a context
distillation-based approach can both impart knowledge about entities and
propagate that knowledge to enable broader inferences. Our approach consists of
two stages: transfer set generation and distillation on the transfer set. We
first generate a transfer set by prompting a language model to generate
continuations from the entity definition. Then, we update the model parameters
so that the distribution of the LM (the student) matches the distribution of
the LM conditioned on the definition (the teacher) on the transfer set. Our
experiments demonstrate that this approach is more effective at propagating
knowledge updates than fine-tuning and other gradient-based knowledge-editing
methods. Moreover, it does not compromise performance in other contexts, even
when injecting the definitions of up to 150 entities at once.",None,-1
cdbc7141-66c2-449d-ad32-74c31a289f55,Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations,0.709483,"The abundance of instructional videos and their narrations over the Internet
offers an exciting avenue for understanding procedural activities. In this
work, we propose to learn video representation that encodes both action steps
and their temporal ordering, based on a large-scale dataset of web
instructional videos and their narrations, without using human annotations. Our
method jointly learns a video representation to encode individual step
concepts, and a deep probabilistic model to capture both temporal dependencies
and immense individual variations in the step ordering. We empirically
demonstrate that learning temporal ordering not only enables new capabilities
for procedure reasoning, but also reinforces the recognition of individual
steps. Our model significantly advances the state-of-the-art results on step
classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting
(+7.4% on COIN). Moreover, our model attains promising results in zero-shot
inference for step classification and forecasting, as well as in predicting
diverse and plausible steps for incomplete procedures. Our code is available at
https://github.com/facebookresearch/ProcedureVRL.",None,-1
2a02b7c1-4200-4aa3-83b9-8c94920bc6c6,BIDRN: A Method of Bidirectional Recurrent Neural Network for Sentiment Analysis,0.184774,"Text mining research has grown in importance in recent years due to the
tremendous increase in the volume of unstructured textual data. This has
resulted in immense potential as well as obstacles in the sector, which may be
efficiently addressed with adequate analytical and study methods. Deep
Bidirectional Recurrent Neural Networks are used in this study to analyze
sentiment. The method is categorized as sentiment polarity analysis because it
may generate a dataset with sentiment labels. This dataset can be used to train
and evaluate sentiment analysis models capable of extracting impartial
opinions. This paper describes the Sentiment Analysis-Deep Bidirectional
Recurrent Neural Networks (SA-BDRNN) Scheme, which seeks to overcome the
challenges and maximize the potential of text mining in the context of Big
Data. The current study proposes a SA-DBRNN Scheme that attempts to give a
systematic framework for sentiment analysis in the context of student input on
institution choice. The purpose of this study is to compare the effectiveness
of the proposed SA- DBRNN Scheme to existing frameworks to establish a robust
deep neural network that might serve as an adequate classification model in the
field of sentiment analysis.",None,-1
72f43b3b-fbd4-4144-a468-7d84886aa42f,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,0.916413,"Although numerous solutions have been proposed for image super-resolution,
they are usually incompatible with low-power devices with many computational
and memory constraints. In this paper, we address this problem by proposing a
simple yet effective deep network to solve image super-resolution efficiently.
In detail, we develop a spatially-adaptive feature modulation (SAFM) mechanism
upon a vision transformer (ViT)-like block. Within it, we first apply the SAFM
block over input features to dynamically select representative feature
representations. As the SAFM block processes the input features from a
long-range perspective, we further introduce a convolutional channel mixer
(CCM) to simultaneously extract local contextual information and perform
channel mixing. Extensive experimental results show that the proposed method is
$3\times$ smaller than state-of-the-art efficient SR methods, e.g., IMDN, in
terms of the network parameters and requires less computational cost while
achieving comparable performance. The code is available at
https://github.com/sunny2109/SAFMN.",None,-1
b3bfda69-5bd9-449d-8514-88c2774624f6,Self-Supervised Temporal Analysis of Spatiotemporal Data,0.291995,"There exists a correlation between geospatial activity temporal patterns and
type of land use. A novel self-supervised approach is proposed to stratify
landscape based on mobility activity time series. First, the time series signal
is transformed to the frequency domain and then compressed into task-agnostic
temporal embeddings by a contractive autoencoder, which preserves cyclic
temporal patterns observed in time series. The pixel-wise embeddings are
converted to image-like channels that can be used for task-based, multimodal
modeling of downstream geospatial tasks using deep semantic segmentation.
Experiments show that temporal embeddings are semantically meaningful
representations of time series data and are effective across different tasks
such as classifying residential area and commercial areas.",None,-1
62ee42be-d0e9-4177-974b-01ac7feefcb5,Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision,0.339314,"Most existing task-oriented dialog (TOD) systems track dialog states in terms
of slots and values and use them to query a database to get relevant knowledge
to generate responses. In real-life applications, user utterances are noisier,
and thus it is more difficult to accurately track dialog states and correctly
secure relevant knowledge. Recently, a progress in question answering and
document-grounded dialog systems is retrieval-augmented methods with a
knowledge retriever. Inspired by such progress, we propose a retrieval-based
method to enhance knowledge selection in TOD systems, which significantly
outperforms the traditional database query method for real-life dialogs.
Further, we develop latent variable model based semi-supervised learning, which
can work with the knowledge retriever to leverage both labeled and unlabeled
dialog data. Joint Stochastic Approximation (JSA) algorithm is employed for
semi-supervised model training, and the whole system is referred to as that
JSA-KRTOD. Experiments are conducted on a real-life dataset from China Mobile
Custom-Service, called MobileCS, and show that JSA-KRTOD achieves superior
performances in both labeled-only and semi-supervised settings.",None,-1
eec7e77f-96f4-4d9e-9b75-ae818fa3fb27,Spatiotemporally Consistent HDR Indoor Lighting Estimation,0.758518,"We propose a physically-motivated deep learning framework to solve a general
version of the challenging indoor lighting estimation problem. Given a single
LDR image with a depth map, our method predicts spatially consistent lighting
at any given image position. Particularly, when the input is an LDR video
sequence, our framework not only progressively refines the lighting prediction
as it sees more regions, but also preserves temporal consistency by keeping the
refinement smooth. Our framework reconstructs a spherical Gaussian lighting
volume (SGLV) through a tailored 3D encoder-decoder, which enables spatially
consistent lighting prediction through volume ray tracing, a hybrid blending
network for detailed environment maps, an in-network Monte-Carlo rendering
layer to enhance photorealism for virtual object insertion, and recurrent
neural networks (RNN) to achieve temporally consistent lighting prediction with
a video sequence as the input. For training, we significantly enhance the
OpenRooms public dataset of photorealistic synthetic indoor scenes with around
360K HDR environment maps of much higher resolution and 38K video sequences,
rendered with GPU-based path tracing. Experiments show that our framework
achieves lighting prediction with higher quality compared to state-of-the-art
single-image or video-based methods, leading to photorealistic AR applications
such as object insertion.",None,-1
9fd3ae82-90d4-4e21-bb41-08c396c8d58b,SOCS: Semantically-aware Object Coordinate Space for Category-Level 6D Object Pose Estimation under Large Shape Variations,0.572665,"Most learning-based approaches to category-level 6D pose estimation are
design around normalized object coordinate space (NOCS). While being
successful, NOCS-based methods become inaccurate and less robust when handling
objects of a category containing significant intra-category shape variations.
This is because the object coordinates induced by global and rigid alignment of
objects are semantically incoherent, making the coordinate regression hard to
learn and generalize. We propose Semantically-aware Object Coordinate Space
(SOCS) built by warping-and-aligning the objects guided by a sparse set of
keypoints with semantically meaningful correspondence. SOCS is semantically
coherent: Any point on the surface of a object can be mapped to a semantically
meaningful location in SOCS, allowing for accurate pose and size estimation
under large shape variations. To learn effective coordinate regression to SOCS,
we propose a novel multi-scale coordinate-based attention network. Evaluations
demonstrate that our method is easy to train, well-generalizing for large
intra-category shape variations and robust to inter-object occlusions.",None,-1
8c1412ed-a8df-4093-a3a5-86b3cd5ef960,Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents,0.988383,"Human-like chatbots necessitate the use of commonsense reasoning in order to
effectively comprehend and respond to implicit information present within
conversations. Achieving such coherence and informativeness in responses,
however, is a non-trivial task. Even for large language models (LLMs), the task
of identifying and aggregating key evidence within a single hop presents a
substantial challenge. This complexity arises because such evidence is
scattered across multiple turns in a conversation, thus necessitating
integration over multiple hops. Hence, our focus is to facilitate such
multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought
(CoT) reasoning. To this end, we propose a knowledge distillation framework
that leverages LLMs as unreliable teachers and selectively distills consistent
and helpful rationales via alignment filters. We further present DOCTOR, a
DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for
response generation. We conduct extensive experiments to show that enhancing
dialogue agents with high-quality rationales from DOCTOR significantly improves
the quality of their responses.",None,-1
bb983cd6-9cc6-4020-967e-e10af613328b,Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination,0.621822,"Enormous hand images with reliable annotations are collected through
marker-based MoCap. Unfortunately, degradations caused by markers limit their
application in hand appearance reconstruction. A clear appearance recovery
insight is an image-to-image translation trained with unpaired data. However,
most frameworks fail because there exists structure inconsistency from a
degraded hand to a bare one. The core of our approach is to first disentangle
the bare hand structure from those degraded images and then wrap the appearance
to this structure with a dual adversarial discrimination (DAD) scheme. Both
modules take full advantage of the semi-supervised learning paradigm: The
structure disentanglement benefits from the modeling ability of ViT, and the
translator is enhanced by the dual discrimination on both translation processes
and translation results. Comprehensive evaluations have been conducted to prove
that our framework can robustly recover photo-realistic hand appearance from
diverse marker-contained and even object-occluded datasets. It provides a novel
avenue to acquire bare hand appearance data for other downstream learning
problems.The codes will be publicly available at https://www.yangangwang.com",None,-1
87293a08-8f08-4e73-bb7a-0c45a101f631,HybridPoint: Point Cloud Registration Based on Hybrid Point Sampling and Matching,0.0877374,"Patch-to-point matching has become a robust way of point cloud registration.
However, previous patch-matching methods employ superpoints with poor
localization precision as nodes, which may lead to ambiguous patch partitions.
In this paper, we propose a HybridPoint-based network to find more robust and
accurate correspondences. Firstly, we propose to use salient points with
prominent local features as nodes to increase patch repeatability, and
introduce some uniformly distributed points to complete the point cloud, thus
constituting hybrid points. Hybrid points not only have better localization
precision but also give a complete picture of the whole point cloud.
Furthermore, based on the characteristic of hybrid points, we propose a
dual-classes patch matching module, which leverages the matching results of
salient points and filters the matching noise of non-salient points.
Experiments show that our model achieves state-of-the-art performance on
3DMatch, 3DLoMatch, and KITTI odometry, especially with 93.0% Registration
Recall on the 3DMatch dataset. Our code and models are available at
https://github.com/liyih/HybridPoint.",None,-1
af88e78d-80e6-46c2-b925-5c174b7e26ac,DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners,0.344929,"State-of-the-art visual grounding models can achieve high detection accuracy,
but they are not designed to distinguish between all objects versus only
certain objects of interest. In natural language, in order to specify a
particular object or set of objects of interest, humans use determiners such as
""my"", ""either"" and ""those"". Determiners, as an important word class, are a type
of schema in natural language about the reference or quantity of the noun.
Existing grounded referencing datasets place much less emphasis on determiners,
compared to other word classes such as nouns, verbs and adjectives. This makes
it difficult to develop models that understand the full variety and complexity
of object referencing. Thus, we have developed and released the DetermiNet
dataset , which comprises 250,000 synthetically generated images and captions
based on 25 determiners. The task is to predict bounding boxes to identify
objects of interest, constrained by the semantics of the given determiner. We
find that current state-of-the-art visual grounding models do not perform well
on the dataset, highlighting the limitations of existing models on reference
and quantification tasks.",None,-1
2ef6ef2f-b474-4200-86dc-a9e3da319daf,Large Language Models as Data Preprocessors,0.57696,"Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's
LLaMA variants, have marked a significant advancement in artificial
intelligence. Trained on vast amounts of text data, LLMs are capable of
understanding and generating human-like text across a diverse range of topics.
This study expands on the applications of LLMs, exploring their potential in
data preprocessing, a critical stage in data mining and analytics applications.
We delve into the applicability of state-of-the-art LLMs such as GPT-3.5,
GPT-4, and Vicuna-13B for error detection, data imputation, schema matching,
and entity matching tasks. Alongside showcasing the inherent capabilities of
LLMs, we highlight their limitations, particularly in terms of computational
expense and inefficiency. We propose an LLM-based framework for data
preprocessing, which integrates cutting-edge prompt engineering techniques,
coupled with traditional methods like contextualization and feature selection,
to improve the performance and efficiency of these models. The effectiveness of
LLMs in data preprocessing is evaluated through an experimental study spanning
12 datasets. GPT-4 emerged as a standout, achieving 100\% accuracy or F1 score
on 4 datasets, suggesting LLMs' immense potential in these tasks. Despite
certain limitations, our study underscores the promise of LLMs in this domain
and anticipates future developments to overcome current hurdles.",None,-1
f09e2bbe-7964-424f-9d19-69456968b634,SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-view 3D Object Detection,0.820325,"Recently, the pure camera-based Bird's-Eye-View (BEV) perception provides a
feasible solution for economical autonomous driving. However, the existing
BEV-based multi-view 3D detectors generally transform all image features into
BEV features, without considering the problem that the large proportion of
background information may submerge the object information. In this paper, we
propose Semantic-Aware BEV Pooling (SA-BEVPool), which can filter out
background information according to the semantic segmentation of image features
and transform image features into semantic-aware BEV features. Accordingly, we
propose BEV-Paste, an effective data augmentation strategy that closely matches
with semantic-aware BEV feature. In addition, we design a Multi-Scale
Cross-Task (MSCT) head, which combines task-specific and cross-task information
to predict depth distribution and semantic segmentation more accurately,
further improving the quality of semantic-aware BEV feature. Finally, we
integrate the above modules into a novel multi-view 3D object detection
framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves
state-of-the-art performance. Code has been available at
https://github.com/mengtan00/SA-BEV.git.",None,-1
e16cc958-cadc-444f-91f8-a4b2b1e45d8c,The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector,0.811518,"Deep learning-based edge detectors heavily rely on pixel-wise labels which
are often provided by multiple annotators. Existing methods fuse multiple
annotations using a simple voting process, ignoring the inherent ambiguity of
edges and labeling bias of annotators. In this paper, we propose a novel
uncertainty-aware edge detector (UAED), which employs uncertainty to
investigate the subjectivity and ambiguity of diverse annotations.
Specifically, we first convert the deterministic label space into a learnable
Gaussian distribution, whose variance measures the degree of ambiguity among
different annotations. Then we regard the learned variance as the estimated
uncertainty of the predicted edge maps, and pixels with higher uncertainty are
likely to be hard samples for edge detection. Therefore we design an adaptive
weighting loss to emphasize the learning from those pixels with high
uncertainty, which helps the network to gradually concentrate on the important
pixels. UAED can be combined with various encoder-decoder backbones, and the
extensive experiments demonstrate that UAED achieves superior performance
consistently across multiple edge detection benchmarks. The source code is
available at \url{https://github.com/ZhouCX117/UAED}",None,-1
dbc7764b-8b8a-4c16-a5bd-38590d1f8495,CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model,0.750882,"With the development of deep generative models, recent years have seen great
success of Chinese landscape painting generation. However, few works focus on
controllable Chinese landscape painting generation due to the lack of data and
limited modeling capabilities. In this work, we propose a controllable Chinese
landscape painting generation method named CCLAP, which can generate painting
with specific content and style based on Latent Diffusion Model. Specifically,
it consists of two cascaded modules, i.e., content generator and style
aggregator. The content generator module guarantees the content of generated
paintings specific to the input text. While the style aggregator module is to
generate paintings of a style corresponding to a reference image. Moreover, a
new dataset of Chinese landscape paintings named CLAP is collected for
comprehensive evaluation. Both the qualitative and quantitative results
demonstrate that our method achieves state-of-the-art performance, especially
in artfully-composed and artistic conception. Codes are available at
https://github.com/Robin-WZQ/CCLAP.",None,-1
08bc5a2e-4da2-4d46-9373-70f173d57f5e,RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture,0.803171,"The techniques for 3D indoor scene capturing are widely used, but the meshes
produced leave much to be desired. In this paper, we propose ""RoomDreamer"",
which leverages powerful natural language to synthesize a new room with a
different style. Unlike existing image synthesis methods, our work addresses
the challenge of synthesizing both geometry and texture aligned to the input
scene structure and prompt simultaneously. The key insight is that a scene
should be treated as a whole, taking into account both scene texture and
geometry. The proposed framework consists of two significant components:
Geometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for
3D Scene guarantees the consistency of the scene style by applying the 2D prior
to the entire scene simultaneously. Mesh Optimization improves the geometry and
texture jointly and eliminates the artifacts in the scanned scene. To validate
the proposed method, real indoor scenes scanned with smartphones are used for
extensive experiments, through which the effectiveness of our method is
demonstrated.",None,-1
e42ffc92-dd20-4f9a-a78c-3b3036d49db6,Cardinality Estimation over Knowledge Graphs with Embeddings and Graph Neural Networks,0.248729,"Cardinality Estimation over Knowledge Graphs (KG) is crucial for query
optimization, yet remains a challenging task due to the semi-structured nature
and complex correlations of typical Knowledge Graphs. In this work, we propose
GNCE, a novel approach that leverages knowledge graph embeddings and Graph
Neural Networks (GNN) to accurately predict the cardinality of conjunctive
queries. GNCE first creates semantically meaningful embeddings for all entities
in the KG, which are then integrated into the given query, which is processed
by a GNN to estimate the cardinality of the query. We evaluate GNCE on several
KGs in terms of q-Error and demonstrate that it outperforms state-of-the-art
approaches based on sampling, summaries, and (machine) learning in terms of
estimation accuracy while also having lower execution time and less parameters.
Additionally, we show that GNCE can inductively generalise to unseen entities,
making it suitable for use in dynamic query processing scenarios. Our proposed
approach has the potential to significantly improve query optimization and
related applications that rely on accurate cardinality estimates of conjunctive
queries.",None,-1
42eb934d-d1bc-4173-848c-fc16d12be63a,Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes,0.709431,"This paper proposes Allophant, a multilingual phoneme recognizer. It requires
only a phoneme inventory for cross-lingual transfer to a target language,
allowing for low-resource recognition. The architecture combines a
compositional phone embedding approach with individually supervised phonetic
attribute classifiers in a multi-task architecture. We also introduce
Allophoible, an extension of the PHOIBLE database. When combined with a
distance based mapping approach for grapheme-to-phoneme outputs, it allows us
to train on PHOIBLE inventories directly. By training and evaluating on 34
languages, we found that the addition of multi-task learning improves the
model's capability of being applied to unseen phonemes and phoneme inventories.
On supervised languages we achieve phoneme error rate improvements of 11
percentage points (pp.) compared to a baseline without multi-task learning.
Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of
2.63 pp. over the baseline.",None,-1
0ad3c8da-ce77-4151-937c-6a80b1623a81,NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes,0.857539,"With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.",None,-1
497dfe1d-2c2e-4d12-97b6-4436e80c1ee2,Referential communication in heterogeneous communities of pre-trained visual deep networks,0.504407,"As large pre-trained image-processing neural networks are being embedded in
autonomous agents such as self-driving cars or robots, the question arises of
how such systems can communicate with each other about the surrounding world,
despite their different architectures and training regimes. As a first step in
this direction, we systematically explore the task of \textit{referential
communication} in a community of heterogeneous state-of-the-art pre-trained
visual networks, showing that they can develop, in a self-supervised way, a
shared protocol to refer to a target object among a set of candidates. This
shared protocol can also be used, to some extent, to communicate about
previously unseen object categories of different granularity. Moreover, a
visual network that was not initially part of an existing community can learn
the community's protocol with remarkable ease. Finally, we study, both
qualitatively and quantitatively, the properties of the emergent protocol,
providing some evidence that it is capturing high-level semantic features of
objects.",None,-1
6837b85f-2b6d-4a7f-b10d-0336e5725095,A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding,0.741395,"Zero-shot dialogue understanding aims to enable dialogue to track the user's
needs without any training data, which has gained increasing attention. In this
work, we investigate the understanding ability of ChatGPT for zero-shot
dialogue understanding tasks including spoken language understanding (SLU) and
dialogue state tracking (DST). Experimental results on four popular benchmarks
reveal the great potential of ChatGPT for zero-shot dialogue understanding. In
addition, extensive analysis shows that ChatGPT benefits from the multi-turn
interactive prompt in the DST task but struggles to perform slot filling for
SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue
understanding tasks, hoping to provide some insights for future research on
building zero-shot dialogue understanding systems with Large Language Models
(LLMs).",None,-1
347cea89-ed3f-4bf8-b77e-a820ba822d41,A Novel Multi-scale Attention Feature Extraction Block for Aerial Remote Sensing Image Classification,0.313259,"Classification of very high-resolution (VHR) aerial remote sensing (RS)
images is a well-established research area in the remote sensing community as
it provides valuable spatial information for decision-making. Existing works on
VHR aerial RS image classification produce an excellent classification
performance; nevertheless, they have a limited capability to well-represent VHR
RS images having complex and small objects, thereby leading to performance
instability. As such, we propose a novel plug-and-play multi-scale attention
feature extraction block (MSAFEB) based on multi-scale convolution at two
levels with skip connection, producing discriminative/salient information at a
deeper/finer level. The experimental study on two benchmark VHR aerial RS image
datasets (AID and NWPU) demonstrates that our proposal achieves a
stable/consistent performance (minimum standard deviation of $0.002$) and
competent overall classification performance (AID: 95.85\% and NWPU: 94.09\%).",None,-1
729873c2-d5eb-4d36-b7fb-79c966de1f58,Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models,0.19515,"The proliferation of video content demands efficient and flexible neural
network based approaches for generating new video content. In this paper, we
propose a novel approach that combines zero-shot text-to-video generation with
ControlNet to improve the output of these models. Our method takes multiple
sketched frames as input and generates video output that matches the flow of
these frames, building upon the Text-to-Video Zero architecture and
incorporating ControlNet to enable additional input conditions. By first
interpolating frames between the inputted sketches and then running
Text-to-Video Zero using the new interpolated frames video as the control
technique, we leverage the benefits of both zero-shot text-to-video generation
and the robust control provided by ControlNet. Experiments demonstrate that our
method excels at producing high-quality and remarkably consistent video content
that more accurately aligns with the user's intended motion for the subject
within the video. We provide a comprehensive resource package, including a demo
video, project website, open-source GitHub repository, and a Colab playground
to foster further research and application of our proposed method.",None,-1
d2642e77-eb36-4334-add5-be7b874623a0,RM-Depth: Unsupervised Learning of Recurrent Monocular Depth in Dynamic Scenes,0.922636,"Unsupervised methods have showed promising results on monocular depth
estimation. However, the training data must be captured in scenes without
moving objects. To push the envelope of accuracy, recent methods tend to
increase their model parameters. In this paper, an unsupervised learning
framework is proposed to jointly predict monocular depth and complete 3D motion
including the motions of moving objects and camera. (1) Recurrent modulation
units are used to adaptively and iteratively fuse encoder and decoder features.
This not only improves the single-image depth inference but also does not
overspend model parameters. (2) Instead of using a single set of filters for
upsampling, multiple sets of filters are devised for the residual upsampling.
This facilitates the learning of edge-preserving filters and leads to the
improved performance. (3) A warping-based network is used to estimate a motion
field of moving objects without using semantic priors. This breaks down the
requirement of scene rigidity and allows to use general videos for the
unsupervised learning. The motion field is further regularized by an
outlier-aware training loss. Despite the depth model just uses a single image
in test time and 2.97M parameters, it achieves state-of-the-art results on the
KITTI and Cityscapes benchmarks.",None,-1
5048975c-ff77-4b25-b8c1-96499cfb5465,EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps,0.133816,"Accurate and reliable ego-localization is critical for autonomous driving. In
this paper, we present EgoVM, an end-to-end localization network that achieves
comparable localization accuracy to prior state-of-the-art methods, but uses
lightweight vectorized maps instead of heavy point-based maps. To begin with,
we extract BEV features from online multi-view images and LiDAR point cloud.
Then, we employ a set of learnable semantic embeddings to encode the semantic
types of map elements and supervise them with semantic segmentation, to make
their feature representation consistent with BEV features. After that, we feed
map queries, composed of learnable semantic embeddings and coordinates of map
elements, into a transformer decoder to perform cross-modality matching with
BEV features. Finally, we adopt a robust histogram-based pose solver to
estimate the optimal pose by searching exhaustively over candidate poses. We
comprehensively validate the effectiveness of our method using both the
nuScenes dataset and a newly collected dataset. The experimental results show
that our method achieves centimeter-level localization accuracy, and
outperforms existing methods using vectorized maps by a large margin.
Furthermore, our model has been extensively tested in a large fleet of
autonomous vehicles under various challenging urban scenes.",None,-1
9ff9384a-5c1a-4d92-8b14-e0d596fe5110,NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts,0.726314,"We introduce NaSGEC, a new dataset to facilitate research on Chinese
grammatical error correction (CGEC) for native speaker texts from multiple
domains. Previous CGEC research primarily focuses on correcting texts from a
single domain, especially learner essays. To broaden the target domain, we
annotate multiple references for 12,500 sentences from three native domains,
i.e., social media, scientific writing, and examination. We provide solid
benchmark results for NaSGEC by employing cutting-edge CGEC models and
different training data. We further perform detailed analyses of the
connections and gaps between our domains from both empirical and statistical
views. We hope this work can inspire future studies on an important but
under-explored direction--cross-domain GEC.",None,-1
27d20f7f-457c-41ed-85c4-3f8587377618,Simplified Continuous High Dimensional Belief Space Planning with Adaptive Probabilistic Belief-dependent Constraints,0.346499,"Online decision making under uncertainty in partially observable domains,
also known as Belief Space Planning, is a fundamental problem in robotics and
Artificial Intelligence. Due to an abundance of plausible future unravelings,
calculating an optimal course of action inflicts an enormous computational
burden on the agent. Moreover, in many scenarios, e.g., information gathering,
it is required to introduce a belief-dependent constraint. Prompted by this
demand, in this paper, we consider a recently introduced probabilistic
belief-dependent constrained POMDP. We present a technique to adaptively accept
or discard a candidate action sequence with respect to a probabilistic
belief-dependent constraint, before expanding a complete set of future
observations samples and without any loss in accuracy. Moreover, using our
proposed framework, we contribute an adaptive method to find a maximal feasible
return (e.g., information gain) in terms of Value at Risk for the candidate
action sequence with substantial acceleration. On top of that, we introduce an
adaptive simplification technique for a probabilistically constrained setting.
Such an approach provably returns an identical-quality solution while
dramatically accelerating online decision making. Our universal framework
applies to any belief-dependent constrained continuous POMDP with parametric
beliefs, as well as nonparametric beliefs represented by particles. In the
context of an information-theoretic constraint, our presented framework
stochastically quantifies if a cumulative information gain along the planning
horizon is sufficiently significant (e.g. for, information gathering, active
SLAM). We apply our method to active SLAM, a highly challenging problem of high
dimensional Belief Space Planning. Extensive realistic simulations corroborate
the superiority of our proposed ideas.",None,-1
4756b0af-6a8b-4ac3-bb04-15402fbc4504,Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks,0.83648,"Artificial Intelligence-Generated Content (AIGC) refers to the use of AI to
automate the information creation process while fulfilling the personalized
requirements of users. However, due to the instability of AIGC models, e.g.,
the stochastic nature of diffusion models, the quality and accuracy of the
generated content can vary significantly. In wireless edge networks, the
transmission of incorrectly generated content may unnecessarily consume network
resources. Thus, a dynamic AIGC service provider (ASP) selection scheme is
required to enable users to connect to the most suited ASP, improving the
users' satisfaction and quality of generated content. In this article, we first
review the AIGC techniques and their applications in wireless networks. We then
present the AIGC-as-a-service (AaaS) concept and discuss the challenges in
deploying AaaS at the edge networks. Yet, it is essential to have performance
metrics to evaluate the accuracy of AIGC services. Thus, we introduce several
image-based perceived quality evaluation metrics. Then, we propose a general
and effective model to illustrate the relationship between computational
resources and user-perceived quality evaluation metrics. To achieve efficient
AaaS and maximize the quality of generated content in wireless edge networks,
we propose a deep reinforcement learning-enabled algorithm for optimal ASP
selection. Simulation results show that the proposed algorithm can provide a
higher quality of generated content to users and achieve fewer crashed tasks by
comparing with four benchmarks, i.e., overloading-avoidance, random,
round-robin policies, and the upper-bound schemes.",None,-1
f8e3afc7-e866-42e3-a709-815ea7786c3e,Text Generation with Speech Synthesis for ASR Data Augmentation,0.143207,"Aiming at reducing the reliance on expensive human annotations, data
synthesis for Automatic Speech Recognition (ASR) has remained an active area of
research. While prior work mainly focuses on synthetic speech generation for
ASR data augmentation, its combination with text generation methods is
considerably less explored. In this work, we explore text augmentation for ASR
using large-scale pre-trained neural networks, and systematically compare those
to traditional text augmentation methods. The generated synthetic texts are
then converted to synthetic speech using a text-to-speech (TTS) system and
added to the ASR training data. In experiments conducted on three datasets, we
find that neural models achieve 9%-15% relative WER improvement and outperform
traditional methods. We conclude that text augmentation, particularly through
modern neural approaches, is a viable tool for improving the accuracy of ASR
systems.",None,-1
7531b7e9-8802-4fa9-9bc7-153e2ed1f63b,Advancing Beyond Identification: Multi-bit Watermark for Large Language Models,0.476187,"We show the viability of tackling misuses of large language models beyond the
identification of machine-generated text. While existing zero-bit watermark
methods focus on detection only, some malicious misuses demand tracing the
adversary user for counteracting them. To address this, we propose Multi-bit
Watermark via Position Allocation, embedding traceable multi-bit information
during language model generation. Through allocating tokens onto different
parts of the messages, we embed longer messages in high corruption settings
without added latency. By independently embedding sub-units of messages, the
proposed method outperforms the existing works in terms of robustness and
latency. Leveraging the benefits of zero-bit watermarking, our method enables
robust extraction of the watermark without any model access, embedding and
extraction of long messages ($\geq$ 32-bit) without finetuning, and maintaining
text quality, while allowing zero-bit detection all at the same time. Code is
released here: https://github.com/bangawayoo/mb-lm-watermarking",None,-1
1f34ed15-f223-42b7-bf2e-34e42f2d8380,Curriculum-guided Abstractive Summarization for Mental Health Online Posts,0.160821,"Automatically generating short summaries from users' online mental health
posts could save counselors' reading time and reduce their fatigue so that they
can provide timely responses to those seeking help for improving their mental
state. Recent Transformers-based summarization models have presented a
promising approach to abstractive summarization. They go beyond sentence
selection and extractive strategies to deal with more complicated tasks such as
novel word generation and sentence paraphrasing. Nonetheless, these models have
a prominent shortcoming; their training strategy is not quite efficient, which
restricts the model's performance. In this paper, we include a curriculum
learning approach to reweigh the training samples, bringing about an efficient
learning procedure. We apply our model on extreme summarization dataset of
MentSum posts -- a dataset of mental health related posts from Reddit social
media. Compared to the state-of-the-art model, our proposed method makes
substantial gains in terms of Rouge and Bertscore evaluation metrics, yielding
3.5% (Rouge-1), 10.4% (Rouge-2), and 4.7% (Rouge-L), 1.5% (Bertscore) relative
improvements.",None,-1
082e50d6-6508-4332-8dac-c50ebac90a09,Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection,0.991498,"Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the
binary anomaly label is only given on the video level, but the output requires
snippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing
in WSVAD. However, MIL is notoriously known to suffer from many false alarms
because the snippet-level detector is easily biased towards the abnormal
snippets with simple context, confused by the normality with the same bias, and
missing the anomaly with a different pattern. To this end, we propose a new MIL
framework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve
WSVAD. At each MIL training iteration, we use the current detector to divide
the samples into two groups with different context biases: the most confident
abnormal/normal snippets and the rest ambiguous ones. Then, by seeking the
invariant features across the two sample groups, we can remove the variant
context biases. Extensive experiments on benchmarks UCF-Crime and TAD
demonstrate the effectiveness of our UMIL. Our code is provided at
https://github.com/ktr-hubrt/UMIL.",None,-1
31ed07ec-d150-4754-9205-fb806debe962,Large Language Models Enable Few-Shot Clustering,0.999304,"Unlike traditional unsupervised clustering, semi-supervised clustering allows
users to provide meaningful structure to the data, which helps the clustering
algorithm to match the user's intent. Existing approaches to semi-supervised
clustering require a significant amount of feedback from an expert to improve
the clusters. In this paper, we ask whether a large language model can amplify
an expert's guidance to enable query-efficient, few-shot semi-supervised text
clustering. We show that LLMs are surprisingly effective at improving
clustering. We explore three stages where LLMs can be incorporated into
clustering: before clustering (improving input features), during clustering (by
providing constraints to the clusterer), and after clustering (using LLMs
post-correction). We find incorporating LLMs in the first two stages can
routinely provide significant improvements in cluster quality, and that LLMs
enable a user to make trade-offs between cost and accuracy to produce desired
clusters. We release our code and LLM prompts for the public to use.",None,-1
2725f6bd-18ec-4489-84ca-cc0be8321c1c,Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-shot Learning with Hyperspherical Embeddings,0.296712,"Distance-based classification is frequently used in transductive few-shot
learning (FSL). However, due to the high-dimensionality of image
representations, FSL classifiers are prone to suffer from the hubness problem,
where a few points (hubs) occur frequently in multiple nearest neighbour lists
of other points. Hubness negatively impacts distance-based classification when
hubs from one class appear often among the nearest neighbors of points from
another class, degrading the classifier's performance. To address the hubness
problem in FSL, we first prove that hubness can be eliminated by distributing
representations uniformly on the hypersphere. We then propose two new
approaches to embed representations on the hypersphere, which we prove optimize
a tradeoff between uniformity and local similarity preservation -- reducing
hubness while retaining class structure. Our experiments show that the proposed
methods reduce hubness, and significantly improves transductive FSL accuracy
for a wide range of classifiers.",None,-1
1cead2e9-e043-4b45-b2fb-523588fc3fa1,AI Text-to-Behavior: A Study In Steerability,0.138603,"The research explores the steerability of Large Language Models (LLMs),
particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology
framework called OCEAN (Openness, Conscientiousness, Extroversion,
Agreeableness, Neuroticism), we quantitatively gauged the model's
responsiveness to tailored prompts. When asked to generate text mimicking an
extroverted personality, OCEAN scored the language alignment to that behavioral
trait. In our analysis, while ""openness"" presented linguistic ambiguity,
""conscientiousness"" and ""neuroticism"" were distinctly evoked in the OCEAN
framework, with ""extroversion"" and ""agreeableness"" showcasing a notable overlap
yet distinct separation from other traits. Our findings underscore GPT's
versatility and ability to discern and adapt to nuanced instructions.
Furthermore, historical figure simulations highlighted the LLM's capacity to
internalize and project instructible personas, precisely replicating their
philosophies and dialogic styles. However, the rapid advancements in LLM
capabilities and the opaque nature of some training techniques make metric
proposals degrade rapidly. Our research emphasizes a quantitative role to
describe steerability in LLMs, presenting both its promise and areas for
further refinement in aligning its progress to human intentions.",None,-1
073a5ae1-42ba-466e-b768-4ac004f3bd36,Koala: An Index for Quantifying Overlaps with Pre-training Corpora,0.614354,"In very recent years more attention has been placed on probing the role of
pre-training data in Large Language Models (LLMs) downstream behaviour. Despite
the importance, there is no public tool that supports such analysis of
pre-training corpora at large scale. To help research in this space, we launch
Koala, a searchable index over large pre-training corpora using compressed
suffix arrays with highly efficient compression rate and search support. In its
first release we index the public proportion of OPT 175B pre-training data.
Koala provides a framework to do forensic analysis on the current and future
benchmarks as well as to assess the degree of memorization in the output from
the LLMs. Koala is available for public use at
https://koala-index.erc.monash.edu/.",None,-1
3afc9b75-17f2-4e16-9f83-56f8810330fd,"CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography",0.868589,"Current image steganography techniques are mainly focused on cover-based
methods, which commonly have the risk of leaking secret images and poor
robustness against degraded container images. Inspired by recent developments
in diffusion models, we discovered that two properties of diffusion models, the
ability to achieve translation between two images without training, and
robustness to noisy data, can be used to improve security and natural
robustness in image steganography tasks. For the choice of diffusion model, we
selected Stable Diffusion, a type of conditional diffusion model, and fully
utilized the latest tools from open-source communities, such as LoRAs and
ControlNets, to improve the controllability and diversity of container images.
In summary, we propose a novel image steganography framework, named
Controllable, Robust and Secure Image Steganography (CRoSS), which has
significant advantages in controllability, robustness, and security compared to
cover-based image steganography methods. These benefits are obtained without
additional training. To our knowledge, this is the first work to introduce
diffusion models to the field of image steganography. In the experimental
section, we conducted detailed experiments to demonstrate the advantages of our
proposed CRoSS framework in controllability, robustness, and security.",None,-1
39239420-3f26-4b20-98ff-90ef7f449d60,Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition,0.896655,"The integration of Language Models (LMs) has proven to be an effective way to
address domain shifts in speech recognition. However, these approaches usually
require a significant amount of target domain text data for the training of
LMs. Different from these methods, in this work, with only a domain-specific
text prompt, we propose two zero-shot ASR domain adaptation methods using
LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two
ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR
system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an
encoder-decoder based ASR system. Experiments show that, with only one domain
prompt, both methods can effectively reduce word error rates (WER) on
out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep
LLM-fusion has the advantage of better recall of entity and out-of-vocabulary
words.",None,-1
084ad6f0-9215-457e-8680-8bb9f594fca9,Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,0.768949,"Recent advances in large language models (LLMs) have shown impressive ability
in biomedical question-answering, but have not been adequately investigated for
more specific biomedical applications. This study investigates the performance
of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical
tasks beyond question-answering. Because no patient data can be passed to the
OpenAI API public interface, we evaluated model performance with over 10000
samples as proxies for two fundamental tasks in the clinical domain -
classification and reasoning. The first task is classifying whether statements
of clinical and policy recommendations in scientific literature constitute
health advice. The second task is causal relation detection from the biomedical
literature. We compared LLMs with simpler models, such as bag-of-words (BoW)
with logistic regression, and fine-tuned BioBERT models. Despite the excitement
around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks
remained the best strategy. The simple BoW model performed on par with the most
complex LLM prompting. Prompt engineering required significant investment.",None,-1
635f7a7a-3680-4d52-809e-b18f95b148e4,Can Large Language Models assist in Hazard Analysis?,0.659057,"Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable
natural language processing and generation capabilities and have been applied
to a variety tasks, such as source code generation. This paper explores the
potential of integrating LLMs in the hazard analysis for safety-critical
systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a
human analyst interacts with an LLM via a context-aware chat session and uses
the responses to support elicitation of possible hazard causes. In this
experiment, we explore CoHA with three increasingly complex versions of a
simple system, using Open AI's ChatGPT service. The quality of ChatGPT's
responses were systematically assessed to determine the feasibility of CoHA
given the current state of LLM technology. The results suggest that LLMs may be
useful for supporting human analysts performing hazard analysis.",None,-1
2f555f0b-95d4-4655-858c-38c89500aa68,Efficient Explainable Face Verification based on Similarity Score Argument Backpropagation,0.710283,"Explainable Face Recognition is gaining growing attention as the use of the
technology is gaining ground in security-critical applications. Understanding
why two faces images are matched or not matched by a given face recognition
system is important to operators, users, anddevelopers to increase trust,
accountability, develop better systems, and highlight unfair behavior. In this
work, we propose xSSAB, an approach to back-propagate similarity score-based
arguments that support or oppose the face matching decision to visualize
spatial maps that indicate similar and dissimilar areas as interpreted by the
underlying FR model. Furthermore, we present Patch-LFW, a new explainable face
verification benchmark that enables along with a novel evaluation protocol, the
first quantitative evaluation of the validity of similarity and dissimilarity
maps in explainable face recognition approaches. We compare our efficient
approach to state-of-the-art approaches demonstrating a superior trade-off
between efficiency and performance. The code as well as the proposed Patch-LFW
is publicly available at: https://github.com/marcohuber/xSSAB.",None,-1
a7e72f2f-147b-4607-b923-231d4d3d62cc,EXnet: Efficient In-context Learning for Data-less Text classification,0.0647183,"Large pre-trained language models (PLMs) have made significant progress in
encoding world knowledge and spawned a new set of learning paradigms including
zero-shot, few-shot, and in-context learning. Many language tasks can be
modeled as a set of prompts (for example, is this text about geography?) and
language models can provide binary answers, i.e., Yes or No. There is evidence
to suggest that the next-word prediction used by many PLMs does not align well
with zero-shot paradigms. Therefore, PLMs are fine-tuned as a
question-answering system. In-context learning extends zero-shot learning by
incorporating prompts and examples, resulting in increased task accuracy. Our
paper presents EXnet, a model specifically designed to perform in-context
learning without any limitations on the number of examples. We argue that
in-context learning is an effective method to increase task accuracy, and
providing examples facilitates cross-task generalization, especially when it
comes to text classification tasks. With extensive experiments, we show that
even our smallest model (15M parameters) generalizes to several unseen
classification tasks and domains.",None,-1
952851af-1c1f-4332-a533-2366a52d1bd0,Addressing Variable Dependency in GNN-based SAT Solving,0.463687,"Boolean satisfiability problem (SAT) is fundamental to many applications.
Existing works have used graph neural networks (GNNs) for (approximate) SAT
solving. Typical GNN-based end-to-end SAT solvers predict SAT solutions
concurrently. We show that for a group of symmetric SAT problems, the
concurrent prediction is guaranteed to produce a wrong answer because it
neglects the dependency among Boolean variables in SAT problems. % We propose
AsymSAT, a GNN-based architecture which integrates recurrent neural networks to
generate dependent predictions for variable assignments. The experiment results
show that dependent variable prediction extends the solving capability of the
GNN-based method as it improves the number of solved SAT instances on large
test sets.",None,-1
49fbcf9e-276b-4f9f-9c30-871fce104f41,FashionTex: Controllable Virtual Try-on with Text and Texture,0.968663,"Virtual try-on attracts increasing research attention as a promising way for
enhancing the user experience for online cloth shopping. Though existing
methods can generate impressive results, users need to provide a well-designed
reference image containing the target fashion clothes that often do not exist.
To support user-friendly fashion customization in full-body portraits, we
propose a multi-modal interactive setting by combining the advantages of both
text and texture for multi-level fashion manipulation. With the carefully
designed fashion editing module and loss functions, FashionTex framework can
semantically control cloth types and local texture patterns without annotated
pairwise training data. We further introduce an ID recovery module to maintain
the identity of input portrait. Extensive experiments have demonstrated the
effectiveness of our proposed pipeline.",None,-1
ea8b9b8f-21b9-4d9b-b8b6-8b16538f0a4d,Hallucination is the last thing you need,0.647517,"The legal profession necessitates a multidimensional approach that involves
synthesizing an in-depth comprehension of a legal issue with insightful
commentary based on personal experience, combined with a comprehensive
understanding of pertinent legislation, regulation, and case law, in order to
deliver an informed legal solution. The present offering with generative AI
presents major obstacles in replicating this, as current models struggle to
integrate and navigate such a complex interplay of understanding, experience,
and fact-checking procedures. It is noteworthy that where generative AI outputs
understanding and experience, which reflect the aggregate of various subjective
views on similar topics, this often deflects the model's attention from the
crucial legal facts, thereby resulting in hallucination. Hence, this paper
delves into the feasibility of three independent LLMs, each focused on
understanding, experience, and facts, synthesising as one single ensemble model
to effectively counteract the current challenges posed by the existing
monolithic generative AI models. We introduce an idea of mutli-length
tokenisation to protect key information assets like common law judgements, and
finally we interrogate the most advanced publicly available models for legal
hallucination, with some interesting results.",None,-1
eb4e1844-38f8-42f7-856c-5739f234f3c2,dacl10k: Benchmark for Semantic Bridge Damage Segmentation,0.737927,"Reliably identifying reinforced concrete defects (RCDs)plays a crucial role
in assessing the structural integrity, traffic safety, and long-term durability
of concrete bridges, which represent the most common bridge type worldwide.
Nevertheless, available datasets for the recognition of RCDs are small in terms
of size and class variety, which questions their usability in real-world
scenarios and their role as a benchmark. Our contribution to this problem is
""dacl10k"", an exceptionally diverse RCD dataset for multi-label semantic
segmentation comprising 9,920 images deriving from real-world bridge
inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge
components that play a key role in the building assessment and recommending
actions, such as restoration works, traffic load limitations or bridge
closures. In addition, we examine baseline models for dacl10k which are
subsequently evaluated. The best model achieves a mean intersection-over-union
of 0.42 on the test set. dacl10k, along with our baselines, will be openly
accessible to researchers and practitioners, representing the currently biggest
dataset regarding number of images and class diversity for semantic
segmentation in the bridge inspection domain.",None,-1
a6e1b05e-6ee3-4170-aba8-a749b174ab67,Robust Object Modeling for Visual Tracking,0.980399,"Object modeling has become a core part of recent tracking frameworks. Current
popular tackers use Transformer attention to extract the template feature
separately or interactively with the search region. However, separate template
learning lacks communication between the template and search regions, which
brings difficulty in extracting discriminative target-oriented features. On the
other hand, interactive template learning produces hybrid template features,
which may introduce potential distractors to the template via the cluttered
search regions. To enjoy the merits of both methods, we propose a robust object
modeling framework for visual tracking (ROMTrack), which simultaneously models
the inherent template and the hybrid template features. As a result, harmful
distractors can be suppressed by combining the inherent features of target
objects with search regions' guidance. Target-related features can also be
extracted using the hybrid template, thus resulting in a more robust object
modeling framework. To further enhance robustness, we present novel variation
tokens to depict the ever-changing appearance of target objects. Variation
tokens are adaptable to object deformation and appearance variations, which can
boost overall performance with negligible computation. Experiments show that
our ROMTrack sets a new state-of-the-art on multiple benchmarks.",None,-1
7751705c-5ff1-4870-9e86-34c3b5f161ca,Statement-based Memory for Neural Source Code Summarization,0.133637,"Source code summarization is the task of writing natural language
descriptions of source code behavior. Code summarization underpins software
documentation for programmers. Short descriptions of code help programmers
understand the program quickly without having to read the code itself. Lately,
neural source code summarization has emerged as the frontier of research into
automated code summarization techniques. By far the most popular targets for
summarization are program subroutines. The idea, in a nutshell, is to train an
encoder-decoder neural architecture using large sets of examples of subroutines
extracted from code repositories. The encoder represents the code and the
decoder represents the summary. However, most current approaches attempt to
treat the subroutine as a single unit. For example, by taking the entire
subroutine as input to a Transformer or RNN-based encoder. But code behavior
tends to depend on the flow from statement to statement. Normally dynamic
analysis may shed light on this flow, but dynamic analysis on hundreds of
thousands of examples in large datasets is not practical. In this paper, we
present a statement-based memory encoder that learns the important elements of
flow during training, leading to a statement-based subroutine representation
without the need for dynamic analysis. We implement our encoder for code
summarization and demonstrate a significant improvement over the
state-of-the-art.",None,-1
4e843a96-03ab-4549-91b6-5b8c08e937c9,Multi-Agent Reinforcement Learning Guided by Signal Temporal Logic Specifications,0.663756,"Reward design is a key component of deep reinforcement learning, yet some
tasks and designer's objectives may be unnatural to define as a scalar cost
function. Among the various techniques, formal methods integrated with DRL have
garnered considerable attention due to their expressiveness and flexibility to
define the reward and requirements for different states and actions of the
agent. However, how to leverage Signal Temporal Logic (STL) to guide
multi-agent reinforcement learning reward design remains unexplored. Complex
interactions, heterogeneous goals and critical safety requirements in
multi-agent systems make this problem even more challenging. In this paper, we
propose a novel STL-guided multi-agent reinforcement learning framework. The
STL requirements are designed to include both task specifications according to
the objective of each agent and safety specifications, and the robustness
values of the STL specifications are leveraged to generate rewards. We validate
the advantages of our method through empirical studies. The experimental
results demonstrate significant reward performance improvements compared to
MARL without STL guidance, along with a remarkable increase in the overall
safety rate of the multi-agent systems.",None,-1
d5bb9cf2-4cc8-403f-9f30-454e5a1cf28f,Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures,0.841617,"Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.",None,-1
e43dc440-418c-4074-b88c-a6b301ef538d,Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,0.392907,"Large Language Models (LLMs) are evolving at an unprecedented pace and have
exhibited considerable capability in the realm of natural language processing
(NLP) with world knowledge. Benefiting from ultra-large-scale training corpora,
a single LLM can manage typical NLP tasks competently. However, its performance
in executing reasoning tasks is still confined by the limitations of its
internal representations. To push this boundary further, we introduce Corex in
this paper, a suite of novel general-purpose strategies that transform LLMs
into autonomous agents pioneering multi-model collaborations for complex
task-solving. Inspired by human behaviors, Corex is constituted by diverse
collaboration paradigms including Debate, Review, and Retrieve modes, which
collectively work towards enhancing the factuality, faithfulness, and
reliability of the reasoning process. These paradigms foster task-agnostic
approaches that enable LLMs to ''think outside the box,'' thereby overcoming
hallucinations and providing better solutions. Through extensive experiments
across four different types of reasoning tasks, we demonstrate that
orchestrating multiple LLMs to work in concert yields substantially better
performance compared to existing methods. Further results and in-depth analysis
demonstrate the cost-effectiveness of our method, facilitating collaboration
among different LLMs and promoting annotation efficiency.",None,-1
0af58f09-837e-4d8b-822e-fe91c95d0895,Geometry-biased Transformers for Novel View Synthesis,0.151908,"We tackle the task of synthesizing novel views of an object given a few input
images and associated camera viewpoints. Our work is inspired by recent
'geometry-free' approaches where multi-view images are encoded as a (global)
set-latent representation, which is then used to predict the color for
arbitrary query rays. While this representation yields (coarsely) accurate
images corresponding to novel viewpoints, the lack of geometric reasoning
limits the quality of these outputs. To overcome this limitation, we propose
'Geometry-biased Transformers' (GBTs) that incorporate geometric inductive
biases in the set-latent representation-based inference to encourage multi-view
geometric consistency. We induce the geometric bias by augmenting the
dot-product attention mechanism to also incorporate 3D distances between rays
associated with tokens as a learnable bias. We find that this, along with
camera-aware embeddings as input, allows our models to generate significantly
more accurate outputs. We validate our approach on the real-world CO3D dataset,
where we train our system over 10 categories and evaluate its view-synthesis
ability for novel objects as well as unseen categories. We empirically validate
the benefits of the proposed geometric biases and show that our approach
significantly improves over prior works.",None,-1
cf865768-0dd9-4203-8b82-504c05dce3f1,Curriculum Learning for Compositional Visual Reasoning,0.106349,"Visual Question Answering (VQA) is a complex task requiring large datasets
and expensive training. Neural Module Networks (NMN) first translate the
question to a reasoning path, then follow that path to analyze the image and
provide an answer. We propose an NMN method that relies on predefined
cross-modal embeddings to ``warm start'' learning on the GQA dataset, then
focus on Curriculum Learning (CL) as a way to improve training and make a
better use of the data. Several difficulty criteria are employed for defining
CL methods. We show that by an appropriate selection of the CL method the cost
of training and the amount of training data can be greatly reduced, with a
limited impact on the final VQA accuracy. Furthermore, we introduce
intermediate losses during training and find that this allows to simplify the
CL strategy.",None,-1
373f8b4e-f315-4e56-822e-882ef0bfa04b,Speech-based Slot Filling using Large Language Models,0.353223,"Recently, advancements in large language models (LLMs) have shown an
unprecedented ability across various language tasks. This paper investigates
the potential application of LLMs to slot filling with noisy ASR
transcriptions, via both in-context learning and task-specific fine-tuning.
Dedicated prompt designs and fine-tuning approaches are proposed to improve the
robustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, a
linearised knowledge injection (LKI) scheme is also proposed to integrate
dynamic external knowledge into LLMs. Experiments were performed on SLURP to
quantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B and
Vicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of the
proposed fine-tuning together with the LKI scheme for LLaMA-13B achieved an
8.3% absolute SLU-F1 improvement compared to the strong Flan-T5-base baseline
system on a limited data setup.",None,-1
9bfee463-863b-4c08-866d-b0a325502a55,Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF,0.661089,"Deep neural networks (DNNs) have been proven extremely susceptible to
adversarial examples, which raises special safety-critical concerns for
DNN-based autonomous driving stacks (i.e., 3D object detection). Although there
are extensive works on image-level attacks, most are restricted to 2D pixel
spaces, and such attacks are not always physically realistic in our 3D world.
Here we present Adv3D, the first exploration of modeling adversarial examples
as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic
appearances and 3D accurate generation, yielding a more realistic and
realizable adversarial example. We train our adversarial NeRF by minimizing the
surrounding objects' confidence predicted by 3D detectors on the training set.
Then we evaluate Adv3D on the unseen validation set and show that it can cause
a large performance reduction when rendering NeRF in any sampled pose. To
generate physically realizable adversarial examples, we propose primitive-aware
sampling and semantic-guided regularization that enable 3D patch attacks with
camouflage adversarial texture. Experimental results demonstrate that the
trained adversarial NeRF generalizes well to different poses, scenes, and 3D
detectors. Finally, we provide a defense method to our attacks that involves
adversarial training through data augmentation. Project page:
https://len-li.github.io/adv3d-web",None,-1
e4ca6a93-2bef-490f-9dd1-d3680396b23c,Fine-grained Affective Processing Capabilities Emerging from Large Language Models,0.364653,"Large language models, in particular generative pre-trained transformers
(GPTs), show impressive results on a wide variety of language-related tasks. In
this paper, we explore ChatGPT's zero-shot ability to perform affective
computing tasks using prompting alone. We show that ChatGPT a) performs
meaningful sentiment analysis in the Valence, Arousal and Dominance dimensions,
b) has meaningful emotion representations in terms of emotion categories and
these affective dimensions, and c) can perform basic appraisal-based emotion
elicitation of situations based on a prompt-based computational implementation
of the OCC appraisal model. These findings are highly relevant: First, they
show that the ability to solve complex affect processing tasks emerges from
language-based token prediction trained on extensive data sets. Second, they
show the potential of large language models for simulating, processing and
analyzing human emotions, which has important implications for various
applications such as sentiment analysis, socially interactive agents, and
social robotics.",None,-1
c22319e5-d834-475f-90cf-1ca5b8539d2c,Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities,0.619073,"The prevalence and impact of toxic discussions online have made content
moderation crucial.Automated systems can play a vital role in identifying
toxicity, and reducing the reliance on human moderation.Nevertheless,
identifying toxic comments for diverse communities continues to present
challenges that are addressed in this paper.The two-part goal of this study is
to(1)identify intuitive variances from annotator disagreement using
quantitative analysis and (2)model the subjectivity of these viewpoints.To
achieve our goal, we published a new
dataset\footnote{\url{https://github.com/XXX}} with expert annotators'
annotations and used two other public datasets to identify the subjectivity of
toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's
ability to mimic diverse viewpoints on toxicity by varying size of the training
data and utilizing same set of annotators as the test set used during model
training and a separate set of annotators as the test set.We conclude that
subjectivity is evident across all annotator groups, demonstrating the
shortcomings of majority-rule voting. Moving forward, subjective annotations
should serve as ground truth labels for training models for domains like
toxicity in diverse communities.",None,-1
153c8308-1918-4986-bfed-a98f1e36dea3,Redesigning Electronic Health Record Systems to Support Developing Countries,0.181433,"Electronic Health Record (EHR) has become an essential tool in the healthcare
ecosystem, providing authorized clinicians with patients' health-related
information for better treatment. While most developed countries are taking
advantage of EHRs to improve their healthcare system, it remains challenging in
developing countries to support clinical decision-making and public health
using a computerized patient healthcare information system. This paper proposes
a novel EHR architecture suitable for developing countries--an architecture
that fosters inclusion and provides solutions tailored to all social classes
and socioeconomic statuses. Our architecture foresees an internet-free
(offline) solution to allow medical transactions between healthcare
organizations, and the storage of EHRs in geographically underserved and rural
areas. Moreover, we discuss how artificial intelligence can leverage anonymous
health-related information to enable better public health policy and
surveillance.",None,-1
fdb8b42c-6647-4f46-b4ee-978f4b9620d6,Conversation Derailment Forecasting with Graph Convolutional Networks,0.36691,"Online conversations are particularly susceptible to derailment, which can
manifest itself in the form of toxic communication patterns like disrespectful
comments or verbal abuse. Forecasting conversation derailment predicts signs of
derailment in advance enabling proactive moderation of conversations. Current
state-of-the-art approaches to address this problem rely on sequence models
that treat dialogues as text streams. We propose a novel model based on a graph
convolutional neural network that considers dialogue user dynamics and the
influence of public perception on conversation utterances. Through empirical
evaluation, we show that our model effectively captures conversation dynamics
and outperforms the state-of-the-art models on the CGA and CMV benchmark
datasets by 1.5\% and 1.7\%, respectively.",None,-1
a5564152-b1d4-412d-91f0-5910b2084e44,Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4,0.999682,"Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.",None,-1
1e760575-1898-4055-8ca3-853b7003431b,EdgeYOLO: An Edge-Real-Time Object Detector,0.474289,"This paper proposes an efficient, low-complexity and anchor-free object
detector based on the state-of-the-art YOLO framework, which can be implemented
in real time on edge computing platforms. We develop an enhanced data
augmentation method to effectively suppress overfitting during training, and
design a hybrid random loss function to improve the detection accuracy of small
objects. Inspired by FCOS, a lighter and more efficient decoupled head is
proposed, and its inference speed can be improved with little loss of
precision. Our baseline model can reach the accuracy of 50.6% AP50:95 and 69.8%
AP50 in MS COCO2017 dataset, 26.4% AP50:95 and 44.8% AP50 in VisDrone2019-DET
dataset, and it meets real-time requirements (FPS>=30) on edge-computing device
Nvidia Jetson AGX Xavier. We also designed lighter models with less parameters
for edge computing devices with lower computing power, which also show better
performances. Our source code, hyper-parameters and model weights are all
available at https://github.com/LSH9832/edgeyolo.",None,-1
1f2602a3-17b9-4668-bc15-c4073bab31df,Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games,0.407755,"Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.",None,-1
b5d4916e-4285-46b9-8cc7-e7329d8fb39e,Human Pose Estimation in Extremely Low-Light Conditions,0.670756,"We study human pose estimation in extremely low-light images. This task is
challenging due to the difficulty of collecting real low-light images with
accurate labels, and severely corrupted inputs that degrade prediction quality
significantly. To address the first issue, we develop a dedicated camera system
and build a new dataset of real low-light images with accurate pose labels.
Thanks to our camera system, each low-light image in our dataset is coupled
with an aligned well-lit image, which enables accurate pose labeling and is
used as privileged information during training. We also propose a new model and
a new training strategy that fully exploit the privileged information to learn
representation insensitive to lighting conditions. Our method demonstrates
outstanding performance on real extremely low light images, and extensive
analyses validate that both of our model and dataset contribute to the success.",None,-1
bc4bfcac-b0b3-4c20-b53b-fae8bdcc640b,UnLoc: A Unified Framework for Video Localization Tasks,0.994713,"While large-scale image-text pretrained models such as CLIP have been used
for multiple video-level tasks on trimmed videos, their use for temporal
localization in untrimmed videos is still a relatively unexplored task. We
design a new approach for this called UnLoc, which uses pretrained image and
text towers, and feeds tokens to a video-text fusion model. The output of the
fusion module are then used to construct a feature pyramid in which each level
connects to a head to predict a per-frame relevancy score and start/end time
displacements. Unlike previous works, our architecture enables Moment
Retrieval, Temporal Localization, and Action Segmentation with a single stage
model, without the need for action proposals, motion based pretrained features
or representation masking. Unlike specialized models, we achieve state of the
art results on all three different localization tasks with a unified approach.
Code will be available at: \url{https://github.com/google-research/scenic}.",None,-1
464fa096-ecb1-46e7-99ee-861d54445766,Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,0.996279,"Reconstructing deformable tissues from endoscopic stereo videos in robotic
surgery is crucial for various clinical applications. However, existing methods
relying only on implicit representations are computationally expensive and
require dozens of hours, which limits further practical applications. To
address this challenge, we introduce LerPlane, a novel method for fast and
accurate reconstruction of surgical scenes under a single-viewpoint setting.
LerPlane treats surgical procedures as 4D volumes and factorizes them into
explicit 2D planes of static and dynamic fields, leading to a compact memory
footprint and significantly accelerated optimization. The efficient
factorization is accomplished by fusing features obtained through linear
interpolation of each plane and enables using lightweight neural networks to
model surgical scenes. Besides, LerPlane shares static fields, significantly
reducing the workload of dynamic tissue modeling. We also propose a novel
sample scheme to boost optimization and improve performance in regions with
tool occlusion and large motions. Experiments on DaVinci robotic surgery videos
demonstrate that LerPlane accelerates optimization by over 100$\times$ while
maintaining high quality across various non-rigid deformations, showing
significant promise for future intraoperative surgery applications.",None,-1
694bb45e-15ad-4dac-82c2-1b91f8289c7c,Leveraging Large Language Models to Generate Answer Set Programs,0.388971,"Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.",None,-1
356c3497-9f7d-4c49-a800-05b59eee833d,MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds,0.3404,"3D semantic segmentation on multi-scan large-scale point clouds plays an
important role in autonomous systems. Unlike the single-scan-based semantic
segmentation task, this task requires distinguishing the motion states of
points in addition to their semantic categories. However, methods designed for
single-scan-based segmentation tasks perform poorly on the multi-scan task due
to the lacking of an effective way to integrate temporal information. We
propose MarS3D, a plug-and-play motion-aware module for semantic segmentation
on multi-scan 3D point clouds. This module can be flexibly combined with
single-scan models to allow them to have multi-scan perception abilities. The
model encompasses two key designs: the Cross-Frame Feature Embedding module for
enriching representation learning and the Motion-Aware Feature Learning module
for enhancing motion awareness. Extensive experiments show that MarS3D can
improve the performance of the baseline model by a large margin. The code is
available at https://github.com/CVMI-Lab/MarS3D.",None,-1
43e21d61-7845-4e24-8ffe-019d3e253e92,Champion Solution for the WSDM2023 Toloka VQA Challenge,0.066922,"In this report, we present our champion solution to the WSDM2023 Toloka
Visual Question Answering (VQA) Challenge. Different from the common VQA and
visual grounding (VG) tasks, this challenge involves a more complex scenario,
i.e. inferring and locating the object implicitly specified by the given
interrogative question. For this task, we leverage ViT-Adapter, a
pre-training-free adapter network, to adapt multi-modal pre-trained
Uni-Perceiver for better cross-modal localization. Our method ranks first on
the leaderboard, achieving 77.5 and 76.347 IoU on public and private test sets,
respectively. It shows that ViT-Adapter is also an effective paradigm for
adapting the unified perception model to vision-language downstream tasks. Code
and models will be released at
https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023.",None,-1
4f8a90f6-5e2e-4b60-aa0b-d31d8e7e7751,Nonverbal Cues in Human-Robot Interaction: A Communication Studies Perspective,0.905412,"Communication between people is characterized by a broad range of nonverbal
cues. Transferring these cues into the design of robots and other artificial
agents that interact with people may foster more natural, inviting, and
accessible experiences. In this position paper, we offer a series of definitive
nonverbal codes for human-robot interaction (HRI) that address the five human
sensory systems (visual, auditory, haptic, olfactory, gustatory) drawn from the
field of communication studies. We discuss how these codes can be translated
into design patterns for HRI using a curated sample of the communication
studies and HRI literatures. As nonverbal codes are an essential mode in human
communication, we argue that integrating robotic nonverbal codes in HRI will
afford robots a feeling of ""aliveness"" or ""social agency"" that would otherwise
be missing. We end with suggestions for research directions to stimulate work
on nonverbal communication within the field of HRI and improve communication
between human and robots.",None,-1
d85191c0-6e85-456c-916b-717175ace12f,Findings of the VarDial Evaluation Campaign 2023,0.410659,"This report presents the results of the shared tasks organized as part of the
VarDial Evaluation Campaign 2023. The campaign is part of the tenth workshop on
Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects
(VarDial), co-located with EACL 2023. Three separate shared tasks were included
this year: Slot and intent detection for low-resource language varieties
(SID4LR), Discriminating Between Similar Languages -- True Labels (DSL-TL), and
Discriminating Between Similar Languages -- Speech (DSL-S). All three tasks
were organized for the first time this year.",None,-1
9627376e-fd0b-4bef-aa41-f37fe0674859,What Food Do We Tweet about on a Rainy Day?,0.0963629,"Food choice is a complex phenomenon shaped by factors such as taste,
ambience, culture or weather. In this paper, we explore food-related tweeting
in different weather conditions. We inspect a Latvian food tweet dataset
spanning the past decade in conjunction with a weather observation dataset
consisting of average temperature, precipitation, and other phenomena. We find
which weather conditions lead to specific food information sharing;
automatically classify tweet sentiment and discuss how it changes depending on
the weather. This research contributes to the growing area of large-scale
social network data understanding of food consumers' choices and perceptions.",None,-1
e541aedb-dd3f-43a6-9912-9dac35fea56e,Enriching the NArabizi Treebank: A Multifaceted Approach to Supporting an Under-Resourced Language,0.258181,"In this paper we address the scarcity of annotated data for NArabizi, a
Romanized form of North African Arabic used mostly on social media, which poses
challenges for Natural Language Processing (NLP). We introduce an enriched
version of NArabizi Treebank (Seddah et al., 2020) with three main
contributions: the addition of two novel annotation layers (named entity
recognition and offensive language detection) and a re-annotation of the
tokenization, morpho-syntactic and syntactic layers that ensure annotation
consistency. Our experimental results, using different tokenization schemes,
showcase the value of our contributions and highlight the impact of working
with non-gold tokenization for NER and dependency parsing. To facilitate future
research, we make these annotations publicly available. Our enhanced NArabizi
Treebank paves the way for creating sophisticated language models and NLP tools
for this under-represented language.",None,-1
bf292d98-55c9-4dd9-9dd8-5698b63daed3,CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction,0.656728,"Recent advances in neural reconstruction using posed image sequences have
made remarkable progress. However, due to the lack of depth information,
existing volumetric-based techniques simply duplicate 2D image features of the
object surface along the entire camera ray. We contend this duplication
introduces noise in empty and occluded spaces, posing challenges for producing
high-quality 3D geometry. Drawing inspiration from traditional multi-view
stereo methods, we propose an end-to-end 3D neural reconstruction framework
CVRecon, designed to exploit the rich geometric embedding in the cost volumes
to facilitate 3D geometric feature learning. Furthermore, we present
Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature
representation that encodes view-dependent information with improved integrity
and robustness. Through comprehensive experiments, we demonstrate that our
approach significantly improves the reconstruction quality in various metrics
and recovers clear fine details of the 3D geometries. Our extensive ablation
studies provide insights into the development of effective 3D geometric feature
learning schemes. Project page: https://cvrecon.ziyue.cool/",None,-1
871eb78a-ed8b-4e10-a732-7ce4548f8241,Language Model Tokenizers Introduce Unfairness Between Languages,0.718888,"Recent language models have shown impressive multilingual performance, even
when not explicitly trained for it. Despite this, there are concerns about the
quality of their outputs across different languages. In this paper, we show how
disparity in the treatment of different languages arises at the tokenization
stage, well before a model is even invoked. The same text translated into
different languages can have drastically different tokenization lengths, with
differences up to 15 times in some cases. These disparities persist even for
tokenizers that are intentionally trained for multilingual support.
Character-level and byte-level models also exhibit over 4 times the difference
in the encoding length for some language pairs. This induces unfair treatment
for some language communities in regard to the cost of accessing commercial
language services, the processing time and latency, as well as the amount of
content that can be provided as context to the models. Therefore, we make the
case that we should train future language models using multilingually fair
subword tokenizers.",None,-1
e0264538-cf35-4a74-b27d-dafd13847110,Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains,0.511877,"As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.",None,-1
31e1b877-3aa9-4f3c-be89-bf4c3354c3b3,CLIPER: A Unified Vision-Language Framework for In-the-Wild Facial Expression Recognition,0.874825,"Facial expression recognition (FER) is an essential task for understanding
human behaviors. As one of the most informative behaviors of humans, facial
expressions are often compound and variable, which is manifested by the fact
that different people may express the same expression in very different ways.
However, most FER methods still use one-hot or soft labels as the supervision,
which lack sufficient semantic descriptions of facial expressions and are less
interpretable. Recently, contrastive vision-language pre-training (VLP) models
(e.g., CLIP) use text as supervision and have injected new vitality into
various computer vision tasks, benefiting from the rich semantics in text.
Therefore, in this work, we propose CLIPER, a unified framework for both static
and dynamic facial Expression Recognition based on CLIP. Besides, we introduce
multiple expression text descriptors (METD) to learn fine-grained expression
representations that make CLIPER more interpretable. We conduct extensive
experiments on several popular FER benchmarks and achieve state-of-the-art
performance, which demonstrates the effectiveness of CLIPER.",None,-1
23ec6865-f578-4ffb-b6bd-4290793c7fdb,What about em? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns,0.599046,"As 3rd-person pronoun usage shifts to include novel forms, e.g., neopronouns,
we need more research on identity-inclusive NLP. Exclusion is particularly
harmful in one of the most popular NLP applications, machine translation (MT).
Wrong pronoun translations can discriminate against marginalized groups, e.g.,
non-binary individuals (Dev et al., 2021). In this ``reality check'', we study
how three commercial MT systems translate 3rd-person pronouns. Concretely, we
compare the translations of gendered vs. gender-neutral pronouns from English
to five other languages (Danish, Farsi, French, German, Italian), and vice
versa, from Danish to English. Our error analysis shows that the presence of a
gender-neutral pronoun often leads to grammatical and semantic translation
errors. Similarly, gender neutrality is often not preserved. By surveying the
opinions of affected native speakers from diverse languages, we provide
recommendations to address the issue in future MT research.",None,-1
0103746f-b70d-4920-8a3d-b41bb58ea759,Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA,0.807384,"Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with factual information such as entities and relations in KBs.
However, traditional Pre-trained Language Models (PLMs) are directly
pre-trained on large-scale natural language corpus, which poses challenges for
them in understanding and representing complex subgraphs in structured KBs. To
bridge the gap between texts and structured KBs, we propose a Structured
Knowledge-aware Pre-training method (SKP). In the pre-training stage, we
introduce two novel structured knowledge-aware tasks, guiding the model to
effectively learn the implicit relationship and better representations of
complex subgraphs. In downstream KBQA task, we further design an efficient
linearization strategy and an interval attention mechanism, which assist the
model to better encode complex subgraphs and shield the interference of
irrelevant subgraphs during reasoning respectively. Detailed experiments and
analyses on WebQSP verify the effectiveness of SKP, especially the significant
improvement in subgraph retrieval (+4.08% H@10).",None,-1
bef1c3fb-3e2c-4b3d-aff6-bc9c7a37453c,Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining,0.346755,"Medical artificial general intelligence (MAGI) enables one foundation model
to solve different medical tasks, which is very practical in the medical
domain. It can significantly reduce the requirement of large amounts of
task-specific data by sufficiently sharing medical knowledge among different
tasks. However, due to the challenges of designing strongly generalizable
models with limited and complex medical data, most existing approaches tend to
develop task-specific models. To take a step towards MAGI, we propose a new
paradigm called Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR). In
MOTOR, we combine two kinds of basic medical knowledge, i.e., general and
specific knowledge, in a complementary manner to boost the general pretraining
process. As a result, the foundation model with comprehensive basic knowledge
can learn compact representations from pretraining radiographic data for better
cross-modal alignment. MOTOR unifies the understanding and generation, which
are two kinds of core intelligence of an AI system, into a single medical
foundation model, to flexibly handle more diverse medical tasks. To enable a
comprehensive evaluation and facilitate further research, we construct a
medical multimodal benchmark including a wide range of downstream tasks, such
as chest x-ray report generation and medical visual question answering.
Extensive experiments on our benchmark show that MOTOR obtains promising
results through simple task-oriented adaptation. The visualization shows that
the injected knowledge successfully highlights key information in the medical
data, demonstrating the excellent interpretability of MOTOR. Our MOTOR
successfully mimics the human practice of fulfilling a ""medical student"" to
accelerate the process of becoming a ""specialist"". We believe that our work
makes a significant stride in realizing MAGI.",None,-1
5c35a529-4e80-43e1-a353-8c50c32490ff,From Database Repairs to Causality in Databases and Beyond,0.705425,"We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.",None,-1
0949d463-45dd-4ccb-b3d2-e538a21f48b1,Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation,0.128533,"Multi-view image generation attracts particular attention these days due to
its promising 3D-related applications, e.g., image viewpoint editing. Most
existing methods follow a paradigm where a 3D representation is first
synthesized, and then rendered into 2D images to ensure photo-consistency
across viewpoints. However, such explicit bias for photo-consistency sacrifices
photo-realism, causing geometry artifacts and loss of fine-scale details when
these methods are applied to edit real images. To address this issue, we
propose ray conditioning, a geometry-free alternative that relaxes the
photo-consistency constraint. Our method generates multi-view images by
conditioning a 2D GAN on a light field prior. With explicit viewpoint control,
state-of-the-art photo-realism and identity consistency, our method is
particularly suited for the viewpoint editing task.",None,-1
1c2d1942-c114-4b92-a937-ffde93afe780,C-Pack: Packaged Resources To Advance General Chinese Embedding,0.999983,"We introduce C-Pack, a package of resources that significantly advance the
field of general Chinese embeddings. C-Pack includes three critical resources.
1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6
tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated
from labeled and unlabeled Chinese corpora for training embedding models. 3)
C-TEM is a family of embedding models covering multiple sizes. Our models
outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the
time of the release. We also integrate and optimize the entire suite of
training methods for C-TEM. Along with our resources on general Chinese
embedding, we release our data and models for English text embeddings. The
English models achieve state-of-the-art performance on MTEB benchmark;
meanwhile, our released English data is 2 times larger than the Chinese data.
All these resources are made publicly available at
https://github.com/FlagOpen/FlagEmbedding.",None,-1
d0b9c206-fedf-4c2d-a18b-cf56474230de,Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance,0.239742,"Retrieval augmentation enhances performance of traditional language models by
incorporating additional context. However, the computational demands for
retrieval augmented large language models (LLMs) pose a challenge when applying
them to real-time tasks, such as composition assistance. To address this
limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG)
framework, a novel approach that efficiently combines a cloud-based LLM with a
smaller, client-side, language model through retrieval augmented memory. This
integration enables the client model to generate effective responses,
benefiting from the LLM's capabilities and contextual information.
Additionally, through an asynchronous memory update mechanism, the client model
can deliver real-time completions swiftly to user inputs without the need to
wait for responses from the cloud. Our experiments on five benchmark datasets
demonstrate that HybridRAG significantly improves utility over client-only
models while maintaining low latency.",None,-1
0226dafe-5806-4a73-b3fc-6fa7ab3ecdb3,Learning Machine Morality through Experience and Interaction,0.432169,"Increasing interest in ensuring safety of next-generation Artificial
Intelligence (AI) systems calls for novel approaches to embedding morality into
autonomous agents. Traditionally, this has been done by imposing explicit
top-down rules or hard constraints on systems, for example by filtering system
outputs through pre-defined ethical rules. Recently, instead, entirely
bottom-up methods for learning implicit preferences from human behavior have
become increasingly popular, such as those for training and fine-tuning Large
Language Models. In this paper, we provide a systematization of existing
approaches to the problem of introducing morality in machines - modeled as a
continuum, and argue that the majority of popular techniques lie at the
extremes - either being fully hard-coded, or entirely learned, where no
explicit statement of any moral principle is required. Given the relative
strengths and weaknesses of each type of methodology, we argue that more hybrid
solutions are needed to create adaptable and robust, yet more controllable and
interpretable agents.
  In particular, we present three case studies of recent works which use
learning from experience (i.e., Reinforcement Learning) to explicitly provide
moral principles to learning agents - either as intrinsic rewards, moral
logical constraints or textual principles for language models. For example,
using intrinsic rewards in Social Dilemma games, we demonstrate how it is
possible to represent classical moral frameworks for agents. We also present an
overview of the existing work in this area in order to provide empirical
evidence for the potential of this hybrid approach. We then discuss strategies
for evaluating the effectiveness of moral learning agents. Finally, we present
open research questions and implications for the future of AI safety and ethics
which are emerging from this framework.",None,-1
9f23eb9d-d6f6-490d-b5c5-0629a418716f,REST: Retrieval-Based Speculative Decoding,0.663257,"We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
designed to speed up language model generation. The key insight driving the
development of REST is the observation that the process of text generation
often includes certain common phases and patterns. Unlike previous methods that
rely on a draft language model for speculative decoding, REST harnesses the
power of retrieval to generate draft tokens. This method draws from the
reservoir of existing knowledge, retrieving and employing relevant tokens based
on the current context. Its plug-and-play nature allows for seamless
integration and acceleration of any language models, all without necessitating
additional training. When benchmarked on 7B and 13B language models in a
single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on
code or text generation. The code of REST is available at
https://github.com/FasterDecoding/REST.",None,-1
749c02ad-f825-4f9b-abb3-bc3529aa6274,Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements,0.704882,"Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.",None,-1
cf9e5bc2-bb9c-4ccc-ae4f-4a4df22a51d6,Vision Transformer for Action Units Detection,0.963505,"Facial Action Units detection (FAUs) represents a fine-grained classification
problem that involves identifying different units on the human face, as defined
by the Facial Action Coding System. In this paper, we present a simple yet
efficient Vision Transformer-based approach for addressing the task of Action
Units (AU) detection in the context of Affective Behavior Analysis in-the-wild
(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to
capture the temporal facial change in the video. Besides, to reduce massive
size of the Vision Transformers model, we replace the ViViT feature extraction
layers with the CNN backbone (Regnet). Our model outperform the baseline model
of ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,
the achieved results are comparable to those of the top three teams in the
previous ABAW 2022 challenge.",None,-1
d80848c7-591b-41da-9eb4-4b900d42acdd,Incremental Satisfiability Modulo Theory for Verification of Deep Neural Networks,0.142637,"Constraint solving is an elementary way for verification of deep neural
networks (DNN). In the domain of AI safety, a DNN might be modified in its
structure and parameters for its repair or attack. For such situations, we
propose the incremental DNN verification problem, which asks whether a safety
property still holds after the DNN is modified. To solve the problem, we
present an incremental satisfiability modulo theory (SMT) algorithm based on
the Reluplex framework. We simulate the most important features of the
configurations that infers the verification result of the searching branches in
the old solving procedure (with respect to the original network), and
heuristically check whether the proofs are still valid for the modified DNN. We
implement our algorithm as an incremental solver called DeepInc, and
exerimental results show that DeepInc is more efficient in most cases. For the
cases that the property holds both before and after modification, the
acceleration can be faster by several orders of magnitude, showing that DeepInc
is outstanding in incrementally searching for counterexamples. Moreover, based
on the framework, we propose the multi-objective DNN repair problem and give an
algorithm based on our incremental SMT solving algorithm. Our repair method
preserves more potential safety properties on the repaired DNNs compared with
state-of-the-art.",None,-1
67b4452a-d03d-42db-96b2-1a0b0e1cd93d,Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West,0.35998,"Large Language Models (LLMs), now used daily by millions of users, can encode
societal biases, exposing their users to representational harms. A large body
of scholarship on LLM bias exists but it predominantly adopts a Western-centric
frame and attends comparatively less to bias levels and potential harms in the
Global South. In this paper, we quantify stereotypical bias in popular LLMs
according to an Indian-centric frame and compare bias levels between the Indian
and Western contexts. To do this, we develop a novel dataset which we call
Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and
anti-stereotypical examples for caste and religion contexts. We find that the
majority of LLMs tested are strongly biased towards stereotypes in the Indian
context, especially as compared to the Western context. We finally investigate
Instruction Prompting as a simple intervention to mitigate such bias and find
that it significantly reduces both stereotypical and anti-stereotypical biases
in the majority of cases for GPT-3.5. The findings of this work highlight the
need for including more diverse voices when evaluating LLMs.",None,-1
284106a6-069e-48d4-aa35-884043ce2eb8,"Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models",0.840644,"While adversarial training has been extensively studied for ResNet
architectures and low resolution datasets like CIFAR, much less is known for
ImageNet. Given the recent debate about whether transformers are more robust
than convnets, we revisit adversarial training on ImageNet comparing ViTs and
ConvNeXts. Extensive experiments show that minor changes in architecture, most
notably replacing PatchStem with ConvStem, and training scheme have a
significant impact on the achieved robustness. These changes not only increase
robustness in the seen $\ell_\infty$-threat model, but even more so improve
generalization to unseen $\ell_1/\ell_2$-attacks. Our modified ConvNeXt,
ConvNeXt + ConvStem, yields the most robust $\ell_\infty$-models across
different ranges of model parameters and FLOPs, while our ViT + ConvStem yields
the best generalization to unseen threat models.",None,-1
52f7659c-c338-4d4c-ad03-18d736acb548,Neuro-symbolic Commonsense Social Reasoning,0.122994,"Social norms underlie all human social interactions, yet formalizing and
reasoning with them remains a major challenge for AI systems. We present a
novel system for taking social rules of thumb (ROTs) in natural language from
the Social Chemistry 101 dataset and converting them to first-order logic where
reasoning is performed using a neuro-symbolic theorem prover. We accomplish
this in several steps. First, ROTs are converted into Abstract Meaning
Representation (AMR), which is a graphical representation of the concepts in a
sentence, and align the AMR with RoBERTa embeddings. We then generate alternate
simplified versions of the AMR via a novel algorithm, recombining and merging
embeddings for added robustness against different wordings of text, and
incorrect AMR parses. The AMR is then converted into first-order logic, and is
queried with a neuro-symbolic theorem prover. The goal of this paper is to
develop and evaluate a neuro-symbolic method which performs explicit reasoning
about social situations in a logical form.",None,-1
b2bdf73d-8d94-4c7d-976a-becee14423a3,Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging,0.944483,"While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with general, aggregate human preferences, it is suboptimal for
learning diverse, individual perspectives. In this work, we study Reinforcement
Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are
aligned to multiple (sometimes conflicting) preferences by modeling alignment
as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong
single-objective baselines, we show that we can achieve personalized alignment
by decomposing preferences into multiple dimensions. These dimensions are
defined based on personalizations that are declared as desirable by the user.
In this work, we show that they can be efficiently trained independently in a
distributed manner and combined effectively post-hoc through parameter merging.
The code is available at https://github.com/joeljang/RLPHF.",None,-1
3026aba9-4633-4f44-ba43-da22311cbb7f,A new perspective on building efficient and expressive 3D equivariant graph neural networks,0.972468,"Geometric deep learning enables the encoding of physical symmetries in
modeling 3D objects. Despite rapid progress in encoding 3D symmetries into
Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness
of these networks through a local-to-global analysis lacks today. In this
paper, we propose a local hierarchy of 3D isomorphism to evaluate the
expressive power of equivariant GNNs and investigate the process of
representing global geometric information from local patches. Our work leads to
two crucial modules for designing expressive and efficient geometric GNNs;
namely local substructure encoding (LSE) and frame transition encoding (FTE).
To demonstrate the applicability of our theory, we propose LEFTNet which
effectively implements these modules and achieves state-of-the-art performance
on both scalar-valued and vector-valued molecular property prediction tasks. We
further point out the design space for future developments of equivariant graph
neural networks. Our codes are available at
\url{https://github.com/yuanqidu/LeftNet}.",None,-1
75d8eb7e-982e-42c9-acd7-eab15feb23cc,Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee with Differentiable Convex Programming,0.128908,"Despite remarkable achievements in artificial intelligence, the deployability
of learning-enabled systems in high-stakes real-world environments still faces
persistent challenges. For example, in safety-critical domains like autonomous
driving, robotic manipulation, and healthcare, it is crucial not only to
achieve high performance but also to comply with given constraints.
Furthermore, adaptability becomes paramount in non-stationary domains, where
environmental parameters are subject to change. While safety and adaptability
are recognized as key qualities for the new generation of AI, current
approaches have not demonstrated effective adaptable performance in constrained
settings. Hence, this paper breaks new ground by studying the unique challenges
of ensuring safety in non-stationary environments by solving constrained
problems through the lens of the meta-learning approach (learning-to-learn).
While unconstrained meta-learning al-ready encounters complexities in
end-to-end differentiation of the loss due to the bi-level nature, its
constrained counterpart introduces an additional layer of difficulty, since the
constraints imposed on task-level updates complicate the differentiation
process. To address the issue, we first employ successive convex-constrained
policy updates across multiple tasks with differentiable convexprogramming,
which allows meta-learning in constrained scenarios by enabling end-to-end
differentiation. This approach empowers the agent to rapidly adapt to new tasks
under non-stationarity while ensuring compliance with safety constraints.",None,-1
97064003-7925-44f1-a06b-ade79b5453d4,Language Models as a Service: Overview of a New Paradigm and its Challenges,0.00852568,"Some of the most powerful language models currently are proprietary systems,
accessible only via (typically restrictive) web or software programming
interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. In
contrast with scenarios where full model access is available, as in the case of
open-source models, such closed-off language models present specific challenges
for evaluating, benchmarking, and testing them. This paper has two goals: on
the one hand, we delineate how the aforementioned challenges act as impediments
to the accessibility, replicability, reliability, and trustworthiness of LMaaS.
We systematically examine the issues that arise from a lack of information
about language models for each of these four aspects. We conduct a detailed
analysis of existing solutions and put forth a number of considered
recommendations, and highlight the directions for future advancements. On the
other hand, it serves as a comprehensive resource for existing knowledge on
current, major LMaaS, offering a synthesized overview of the licences and
capabilities their interfaces offer.",None,-1
15ee6c55-a57c-4468-9c0c-f0cc1407d7eb,ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations,0.374525,"Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data
for model training. Empirical studies show that SSL can achieve promising
performance in distribution shift scenarios, where the downstream and training
distributions differ. However, the theoretical understanding of its
transferability remains limited. In this paper, we develop a theoretical
framework to analyze the transferability of self-supervised contrastive
learning, by investigating the impact of data augmentation on it. Our results
reveal that the downstream performance of contrastive learning depends largely
on the choice of data augmentation. Moreover, we show that contrastive learning
fails to learn domain-invariant features, which limits its transferability.
Based on these theoretical insights, we propose a novel method called
Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn
domain-invariant features and can be easily integrated with existing
contrastive learning algorithms. We conduct experiments on several datasets and
show that ArCL significantly improves the transferability of contrastive
learning.",None,-1
57f5129f-38d4-4a43-a9d2-70ee1351c3a2,Balancing Logit Variation for Long-tailed Semantic Segmentation,0.53828,"Semantic segmentation usually suffers from a long-tail data distribution. Due
to the imbalanced number of samples across categories, the features of those
tail classes may get squeezed into a narrow area in the feature space. Towards
a balanced feature distribution, we introduce category-wise variation into the
network predictions in the training phase such that an instance is no longer
projected to a feature point, but a small region instead. Such a perturbation
is highly dependent on the category scale, which appears as assigning smaller
variation to head classes and larger variation to tail classes. In this way, we
manage to close the gap between the feature areas of different categories,
resulting in a more balanced representation. It is noteworthy that the
introduced variation is discarded at the inference stage to facilitate a
confident prediction. Although with an embarrassingly simple implementation,
our method manifests itself in strong generalizability to various datasets and
task settings. Extensive experiments suggest that our plug-in design lends
itself well to a range of state-of-the-art approaches and boosts the
performance on top of them.",None,-1
320ae880-a54b-4b3e-b7d4-0f83a687990e,Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning,0.375229,"Hateful memes have emerged as a significant concern on the Internet.
Detecting hateful memes requires the system to jointly understand the visual
and textual modalities. Our investigation reveals that the embedding space of
existing CLIP-based systems lacks sensitivity to subtle differences in memes
that are vital for correct hatefulness classification. We propose constructing
a hatefulness-aware embedding space through retrieval-guided contrastive
training. Our approach achieves state-of-the-art performance on the
HatefulMemes dataset with an AUROC of 87.0, outperforming much larger
fine-tuned large multimodal models. We demonstrate a retrieval-based hateful
memes detection system, which is capable of identifying hatefulness based on
data unseen in training. This allows developers to update the hateful memes
detection system by simply adding new examples without retraining, a desirable
feature for real services in the constantly evolving landscape of hateful memes
on the Internet.",None,-1
dd5432c2-6c0b-4901-bc1e-80961ac95f96,AdvFAS: A robust face anti-spoofing framework against adversarial examples,0.7578,"Ensuring the reliability of face recognition systems against presentation
attacks necessitates the deployment of face anti-spoofing techniques. Despite
considerable advancements in this domain, the ability of even the most
state-of-the-art methods to defend against adversarial examples remains
elusive. While several adversarial defense strategies have been proposed, they
typically suffer from constrained practicability due to inevitable trade-offs
between universality, effectiveness, and efficiency. To overcome these
challenges, we thoroughly delve into the coupled relationship between
adversarial detection and face anti-spoofing. Based on this, we propose a
robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled
scores to accurately distinguish between correctly detected and wrongly
detected face images. Extensive experiments demonstrate the effectiveness of
our framework in a variety of settings, including different attacks, datasets,
and backbones, meanwhile enjoying high accuracy on clean examples. Moreover, we
successfully apply the proposed method to detect real-world adversarial
examples.",None,-1
dba3f44b-26ed-4dd0-bfa4-0b836a52a5ee,Predicting Spine Geometry and Scoliosis from DXA Scans,0.551582,"Our objective in this paper is to estimate spine curvature in DXA scans. To
this end we first train a neural network to predict the middle spine curve in
the scan, and then use an integral-based method to determine the curvature
along the spine curve. We use the curvature to compare to the standard angle
scoliosis measure obtained using the DXA Scoliosis Method (DSM). The
performance improves over the prior work of Jamaludin et al. 2018. We show that
the maximum curvature can be used as a scoring function for ordering the
severity of spinal deformation.",None,-1
eb7f45b0-2947-4977-87b4-a6f408b39be6,DTrOCR: Decoder-only Transformer for Optical Character Recognition,0.998487,"Typical text recognition methods rely on an encoder-decoder structure, in
which the encoder extracts features from an image, and the decoder produces
recognized text from these features. In this study, we propose a simpler and
more effective method for text recognition, known as the Decoder-only
Transformer for Optical Character Recognition (DTrOCR). This method uses a
decoder-only Transformer to take advantage of a generative language model that
is pre-trained on a large corpus. We examined whether a generative language
model that has been successful in natural language processing can also be
effective for text recognition in computer vision. Our experiments demonstrated
that DTrOCR outperforms current state-of-the-art methods by a large margin in
the recognition of printed, handwritten, and scene text in both English and
Chinese.",None,-1
6d9477d5-a5bc-4f1a-a925-cbcf75803ff9,DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph,0.367551,"In this work, we present a web application named DBLPLink, which performs
entity linking over the DBLP scholarly knowledge graph. DBLPLink uses
text-to-text pre-trained language models, such as T5, to produce entity label
spans from an input text question. Entity candidates are fetched from a
database based on the labels, and an entity re-ranker sorts them based on
entity embeddings, such as TransE, DistMult and ComplEx. The results are
displayed so that users may compare and contrast the results between T5-small,
T5-base and the different KG embeddings used. The demo can be accessed at
https://ltdemos.informatik.uni-hamburg.de/dblplink/.",None,-1
10a85511-9252-40ab-8465-9de80b3f1d5c,A Comparative Analysis of Pretrained Language Models for Text-to-Speech,0.151134,"State-of-the-art text-to-speech (TTS) systems have utilized pretrained
language models (PLMs) to enhance prosody and create more natural-sounding
speech. However, while PLMs have been extensively researched for natural
language understanding (NLU), their impact on TTS has been overlooked. In this
study, we aim to address this gap by conducting a comparative analysis of
different PLMs for two TTS tasks: prosody prediction and pause prediction.
Firstly, we trained a prosody prediction model using 15 different PLMs. Our
findings revealed a logarithmic relationship between model size and quality, as
well as significant performance differences between neutral and expressive
prosody. Secondly, we employed PLMs for pause prediction and found that the
task was less sensitive to small models. We also identified a strong
correlation between our empirical results and the GLUE scores obtained for
these language models. To the best of our knowledge, this is the first study of
its kind to investigate the impact of different PLMs on TTS.",None,-1
94922363-1c0f-4c63-aa81-d70a531fff61,Cognitive Architecture Toward Common Ground Sharing Among Humans and Generative AIs: Trial on Model-Model Interactions in Tangram Naming Task,0.441829,"For generative AIs to be trustworthy, establishing transparent common
grounding with humans is essential. As a preparation toward human-model common
grounding, this study examines the process of model-model common grounding. In
this context, common ground is defined as a cognitive framework shared among
agents in communication, enabling the connection of symbols exchanged between
agents to the meanings inherent in each agent. This connection is facilitated
by a shared cognitive framework among the agents involved. In this research, we
focus on the tangram naming task (TNT) as a testbed to examine the
common-ground-building process. Unlike previous models designed for this task,
our approach employs generative AIs to visualize the internal processes of the
model. In this task, the sender constructs a metaphorical image of an abstract
figure within the model and generates a detailed description based on this
image. The receiver interprets the generated description from the partner by
constructing another image and reconstructing the original abstract figure.
Preliminary results from the study show an improvement in task performance
beyond the chance level, indicating the effect of the common cognitive
framework implemented in the models. Additionally, we observed that incremental
backpropagations leveraging successful communication cases for a component of
the model led to a statistically significant increase in performance. These
results provide valuable insights into the mechanisms of common grounding made
by generative AIs, improving human communication with the evolving intelligent
machines in our future society.",None,-1
e239e4ec-a2d3-4d21-b7f3-1d419ac9ce0d,Stanford MLab at SemEval-2023 Task 10: Exploring GloVe- and Transformer-Based Methods for the Explainable Detection of Online Sexism,0.164044,"In this paper, we discuss the methods we applied at SemEval-2023 Task 10:
Towards the Explainable Detection of Online Sexism. Given an input text, we
perform three classification tasks to predict whether the text is sexist and
classify the sexist text into subcategories in order to provide an additional
explanation as to why the text is sexist. We explored many different types of
models, including GloVe embeddings as the baseline approach, transformer-based
deep learning models like BERT, RoBERTa, and DeBERTa, ensemble models, and
model blending. We explored various data cleaning and augmentation methods to
improve model performance. Pre-training transformer models yielded significant
improvements in performance, and ensembles and blending slightly improved
robustness in the F1 score.",None,-1
2a9b7c22-0729-49f6-bc4f-e82279c50812,Challenges and Applications of Large Language Models,0.720336,"Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.",None,-1
0cf8e63c-29f5-47e2-9676-ce72c4c7593f,Regularization of polynomial networks for image recognition,0.148266,"Deep Neural Networks (DNNs) have obtained impressive performance across
tasks, however they still remain as black boxes, e.g., hard to theoretically
analyze. At the same time, Polynomial Networks (PNs) have emerged as an
alternative method with a promising performance and improved interpretability
but have yet to reach the performance of the powerful DNN baselines. In this
work, we aim to close this performance gap. We introduce a class of PNs, which
are able to reach the performance of ResNet across a range of six benchmarks.
We demonstrate that strong regularization is critical and conduct an extensive
study of the exact regularization schemes required to match performance. To
further motivate the regularization schemes, we introduce D-PolyNets that
achieve a higher-degree of expansion than previously proposed polynomial
networks. D-PolyNets are more parameter-efficient while achieving a similar
performance as other polynomial networks. We expect that our new models can
lead to an understanding of the role of elementwise activation functions (which
are no longer required for training PNs). The source code is available at
https://github.com/grigorisg9gr/regularized_polynomials.",None,-1
184240e6-9f15-4b12-a52d-a15d26b413df,Bayesian Networks for Named Entity Prediction in Programming Community Question Answering,0.121969,"Within this study, we propose a new approach for natural language processing
using Bayesian networks to predict and analyze the context and how this
approach can be applied to the Community Question Answering domain. We discuss
how Bayesian networks can detect semantic relationships and dependencies
between entities, and this is connected to different score-based approaches of
structure-learning. We compared the Bayesian networks with different score
metrics, such as the BIC, BDeu, K2 and Chow-Liu trees. Our proposed approach
out-performs the baseline model at the precision metric. We also discuss the
influence of penalty terms on the structure of Bayesian networks and how they
can be used to analyze the relationships between entities. In addition, we
examine the visualization of directed acyclic graphs to analyze semantic
relationships. The article further identifies issues with detecting certain
semantic classes that are separated in the structure of directed acyclic
graphs. Finally, we evaluate potential improvements for the Bayesian network
approach.",None,-1
81b00e6a-90a1-4606-8b4f-80e9520905a0,Improved Diffusion-based Image Colorization via Piggybacked Models,0.591464,"Image colorization has been attracting the research interests of the
community for decades. However, existing methods still struggle to provide
satisfactory colorized results given grayscale images due to a lack of
human-like global understanding of colors. Recently, large-scale Text-to-Image
(T2I) models have been exploited to transfer the semantic information from the
text prompts to the image domain, where text provides a global control for
semantic objects in the image. In this work, we introduce a colorization model
piggybacking on the existing powerful T2I diffusion model. Our key idea is to
exploit the color prior knowledge in the pre-trained T2I diffusion model for
realistic and diverse colorization. A diffusion guider is designed to
incorporate the pre-trained weights of the latent diffusion model to output a
latent color prior that conforms to the visual semantics of the grayscale
input. A lightness-aware VQVAE will then generate the colorized result with
pixel-perfect alignment to the given grayscale image. Our model can also
achieve conditional colorization with additional inputs (e.g. user hints and
texts). Extensive experiments show that our method achieves state-of-the-art
performance in terms of perceptual quality.",None,-1
50f1a2e7-80cf-4e93-baa2-2d2e9d4de2a1,Zero-shot Temporal Relation Extraction with ChatGPT,0.999708,"The goal of temporal relation extraction is to infer the temporal relation
between two events in the document. Supervised models are dominant in this
task. In this work, we investigate ChatGPT's ability on zero-shot temporal
relation extraction. We designed three different prompt techniques to break
down the task and evaluate ChatGPT. Our experiments show that ChatGPT's
performance has a large gap with that of supervised methods and can heavily
rely on the design of prompts. We further demonstrate that ChatGPT can infer
more small relation classes correctly than supervised methods. The current
shortcomings of ChatGPT on temporal relation extraction are also discussed in
this paper. We found that ChatGPT cannot keep consistency during temporal
inference and it fails in actively long-dependency temporal inference.",None,-1
2b053d90-a550-41c2-b9c2-2cdacb89b55c,"SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",0.797587,"Reliable automatic evaluation of summarization systems is challenging due to
the multifaceted and subjective nature of the task. This is especially the case
for languages other than English, where human evaluations are scarce. In this
work, we introduce SEAHORSE, a dataset for multilingual, multifaceted
summarization evaluation. SEAHORSE consists of 96K summaries with human ratings
along 6 dimensions of text quality: comprehensibility, repetition, grammar,
attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4
datasets. As a result of its size and scope, SEAHORSE can serve both as a
benchmark to evaluate learnt metrics, as well as a large-scale resource for
training such metrics. We show that metrics trained with SEAHORSE achieve
strong performance on the out-of-domain meta-evaluation benchmarks TRUE
(Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE
dataset and metrics publicly available for future research on multilingual and
multifaceted summarization evaluation.",None,-1
49f52ab0-dd6a-43e7-9800-14842e7c2629,Informed Named Entity Recognition Decoding for Generative Language Models,0.719237,"Ever-larger language models with ever-increasing capabilities are by now
well-established text processing tools. Alas, information extraction tasks such
as named entity recognition are still largely unaffected by this progress as
they are primarily based on the previous generation of encoder-only transformer
models. Here, we propose a simple yet effective approach, Informed Named Entity
Recognition Decoding (iNERD), which treats named entity recognition as a
generative process. It leverages the language understanding capabilities of
recent generative models in a future-proof manner and employs an informed
decoding scheme incorporating the restricted nature of information extraction
into open-ended text generation, improving performance and eliminating any risk
of hallucinations. We coarse-tune our model on a merged named entity corpus to
strengthen its performance, evaluate five generative language models on eight
named entity recognition datasets, and achieve remarkable results, especially
in an environment with an unknown entity class set, demonstrating the
adaptability of the approach.",None,-1
5b171864-ee29-4158-9602-d2bd8740fd0b,DistillCSE: Distilled Contrastive Learning for Sentence Embeddings,0.156515,"This paper proposes the DistillCSE framework, which performs contrastive
learning under the self-training paradigm with knowledge distillation. The
potential advantage of DistillCSE is its self-enhancing feature: using a base
model to provide additional supervision signals, a stronger model may be
learned through knowledge distillation. However, the vanilla DistillCSE through
the standard implementation of knowledge distillation only achieves marginal
improvements due to severe overfitting. The further quantitative analyses
demonstrate the reason that the standard knowledge distillation exhibits a
relatively large variance of the teacher model's logits due to the essence of
contrastive learning. To mitigate the issue induced by high variance, this
paper accordingly proposed two simple yet effective solutions for knowledge
distillation: a Group-P shuffling strategy as an implicit regularization and
the averaging logits from multiple teacher components. Experiments on standard
benchmarks demonstrate that the proposed DistillCSE outperforms many strong
baseline methods and yields a new state-of-the-art performance.",None,-1
e8235584-747c-484e-9974-e5db45370a3b,Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning,0.195651,"Generative artificial intelligence (AI) is a promising direction for
augmenting clinical diagnostic decision support and reducing diagnostic errors,
a leading contributor to medical errors. To further the development of clinical
AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a
comprehensive generative AI framework, comprised of six tasks representing key
components in clinical reasoning. We present a comparative analysis of
in-domain versus out-of-domain language models as well as multi-task versus
single task training with a focus on the problem summarization task in DR.BENCH
(Gao et al., 2023). We demonstrate that a multi-task, clinically trained
language model outperforms its general domain counterpart by a large margin,
establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.
This research underscores the value of domain-specific training for optimizing
clinical diagnostic reasoning tasks.",None,-1
267e96c6-8238-4140-a8af-9147abfaa1de,DISPEL: Domain Generalization via Domain-Specific Liberating,0.317435,"Domain generalization aims to learn a generalization model that can perform
well on unseen test domains by only training on limited source domains.
However, existing domain generalization approaches often bring in
prediction-irrelevant noise or require the collection of domain labels. To
address these challenges, we consider the domain generalization problem from a
different perspective by categorizing underlying feature groups into
domain-shared and domain-specific features. Nevertheless, the domain-specific
features are difficult to be identified and distinguished from the input data.
In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing
fine-grained masking approach that can filter out undefined and
indistinguishable domain-specific features in the embedding space.
Specifically, DISPEL utilizes a mask generator that produces a unique mask for
each input data to filter domain-specific features. The DISPEL framework is
highly flexible to be applied to any fine-tuned models. We derive a
generalization error bound to guarantee the generalization performance by
optimizing a designed objective loss. The experimental results on five
benchmarks demonstrate DISPEL outperforms existing methods and can further
generalize various algorithms.",None,-1
5ad7d3b8-ceea-43a3-9b75-3a2a3909ea2e,Towards human-like spoken dialogue generation between AI agents from written dialogue,0.577001,"The advent of large language models (LLMs) has made it possible to generate
natural written dialogues between two agents. However, generating human-like
spoken dialogues from these written dialogues remains challenging. Spoken
dialogues have several unique characteristics: they frequently include
backchannels and laughter, and the smoothness of turn-taking significantly
influences the fluidity of conversation. This study proposes CHATS - CHatty
Agents Text-to-Speech - a discrete token-based system designed to generate
spoken dialogues based on written dialogues. Our system can generate speech for
both the speaker side and the listener side simultaneously, using only the
transcription from the speaker side, which eliminates the need for
transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural
turn-taking; it determines the appropriate duration of silence after each
utterance in the absence of overlap, and it initiates the generation of
overlapping speech based on the phoneme sequence of the next utterance in case
of overlap. Experimental evaluations indicate that CHATS outperforms the
text-to-speech baseline, producing spoken dialogues that are more interactive
and fluid while retaining clarity and intelligibility.",None,-1
f6cacd5f-8500-4dbb-a2e0-1ffccf683b65,HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization,0.145137,"Sparse neural networks are a key factor in developing resource-efficient
machine learning applications. We propose the novel and powerful sparse
learning method Adaptive Regularized Training (ART) to compress dense into
sparse networks. Instead of the commonly used binary mask during training to
reduce the number of model weights, we inherently shrink weights close to zero
in an iterative manner with increasing weight regularization. Our method
compresses the pre-trained model knowledge into the weights of highest
magnitude. Therefore, we introduce a novel regularization loss named
HyperSparse that exploits the highest weights while conserving the ability of
weight exploration. Extensive experiments on CIFAR and TinyImageNet show that
our method leads to notable performance gains compared to other sparsification
methods, especially in extremely high sparsity regimes up to 99.8 percent model
sparsity. Additional investigations provide new insights into the patterns that
are encoded in weights with high magnitudes.",None,-1
47c12479-8b40-41dd-8e5c-931cb5d77593,Large Language Models can Learn Rules,0.981307,"When prompted with a few examples and intermediate steps, large language
models (LLMs) have demonstrated impressive performance in various reasoning
tasks. However, prompting methods that rely on implicit knowledge in an LLM
often generate incorrect answers when the implicit knowledge is wrong or
inconsistent with the task. To tackle this problem, we present
Hypotheses-to-Theories (HtT), a framework that learns a rule library for
reasoning with LLMs. HtT contains two stages, an induction stage and a
deduction stage. In the induction stage, an LLM is first asked to generate and
verify rules over a set of training examples. Rules that appear and lead to
correct answers sufficiently often are collected to form a rule library. In the
deduction stage, the LLM is then prompted to employ the learned rule library to
perform reasoning to answer test questions. Experiments on relational
reasoning, numerical reasoning and concept learning problems show that HtT
improves existing prompting methods, with an absolute gain of 10-30% in
accuracy. The learned rules are also transferable to different models and to
different forms of the same problem.",None,-1
edaeabe8-64a9-4d59-aa3f-6929a8cfcbd6,Graph Laplacian for Semi-Supervised Learning,0.19118,"Semi-supervised learning is highly useful in common scenarios where labeled
data is scarce but unlabeled data is abundant. The graph (or nonlocal)
Laplacian is a fundamental smoothing operator for solving various learning
tasks. For unsupervised clustering, a spectral embedding is often used, based
on graph-Laplacian eigenvectors. For semi-supervised problems, the common
approach is to solve a constrained optimization problem, regularized by a
Dirichlet energy, based on the graph-Laplacian. However, as supervision
decreases, Dirichlet optimization becomes suboptimal. We therefore would like
to obtain a smooth transition between unsupervised clustering and
low-supervised graph-based classification. In this paper, we propose a new type
of graph-Laplacian which is adapted for Semi-Supervised Learning (SSL)
problems. It is based on both density and contrastive measures and allows the
encoding of the labeled data directly in the operator. Thus, we can perform
successfully semi-supervised learning using spectral clustering. The benefits
of our approach are illustrated for several SSL problems.",None,-1
e0ccc4c7-48e3-4553-af99-3cfe95f76ed3,Conformal Nucleus Sampling,0.234533,"Language models generate text based on successively sampling the next word. A
decoding procedure based on nucleus (top-$p$) sampling chooses from the
smallest possible set of words whose cumulative probability exceeds the
probability $p$. In this work, we assess whether a top-$p$ set is indeed
aligned with its probabilistic meaning in various linguistic contexts. We
employ conformal prediction, a calibration procedure that focuses on the
construction of minimal prediction sets according to a desired confidence
level, to calibrate the parameter $p$ as a function of the entropy of the next
word distribution. We find that OPT models are overconfident, and that
calibration shows a moderate inverse scaling with model size.",None,-1
b2c4a2e5-f5be-44aa-83fd-26b7dd6f7c76,Topological RANSAC for instance verification and retrieval without fine-tuning,0.15872,"This paper presents an innovative approach to enhancing explainable image
retrieval, particularly in situations where a fine-tuning set is unavailable.
The widely-used SPatial verification (SP) method, despite its efficacy, relies
on a spatial model and the hypothesis-testing strategy for instance
recognition, leading to inherent limitations, including the assumption of
planar structures and neglect of topological relations among features. To
address these shortcomings, we introduce a pioneering technique that replaces
the spatial model with a topological one within the RANSAC process. We propose
bio-inspired saccade and fovea functions to verify the topological consistency
among features, effectively circumventing the issues associated with SP's
spatial model. Our experimental results demonstrate that our method
significantly outperforms SP, achieving state-of-the-art performance in
non-fine-tuning retrieval. Furthermore, our approach can enhance performance
when used in conjunction with fine-tuned features. Importantly, our method
retains high explainability and is lightweight, offering a practical and
adaptable solution for a variety of real-world applications.",None,-1
56df5935-b385-454a-b1a0-aab8241cc845,Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification,0.0383446,"A well-defined reward function is crucial for successful training of an
reinforcement learning (RL) agent. However, defining a suitable reward function
is a notoriously challenging task, especially in complex, multi-objective
environments. Developers often have to resort to starting with an initial,
potentially misspecified reward function, and iteratively adjusting its
parameters, based on observed learned behavior. In this work, we aim to
automate this process by proposing ITERS, an iterative reward shaping approach
using human feedback for mitigating the effects of a misspecified reward
function. Our approach allows the user to provide trajectory-level feedback on
agent's behavior during training, which can be integrated as a reward shaping
signal in the following training iteration. We also allow the user to provide
explanations of their feedback, which are used to augment the feedback and
reduce user effort and feedback frequency. We evaluate ITERS in three
environments and show that it can successfully correct misspecified reward
functions.",None,-1
2c131d7a-692e-4d7b-9851-417e9d317999,RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose,0.992503,"Recent studies on 2D pose estimation have achieved excellent performance on
public benchmarks, yet its application in the industrial community still
suffers from heavy model parameters and high latency. In order to bridge this
gap, we empirically explore key factors in pose estimation including paradigm,
model architecture, training strategy, and deployment, and present a
high-performance real-time multi-person pose estimation framework, RTMPose,
based on MMPose. Our RTMPose-m achieves 75.8% AP on COCO with 90+ FPS on an
Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-l
achieves 67.0% AP on COCO-WholeBody with 130+ FPS. To further evaluate
RTMPose's capability in critical real-time applications, we also report the
performance after deploying on the mobile device. Our RTMPose-s achieves 72.2%
AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existing
open-source libraries. Code and models are released at
https://github.com/open-mmlab/mmpose/tree/1.x/projects/rtmpose.",None,-1
86efafed-09c8-434d-bab2-8a23f54ad13f,Region and Spatial Aware Anomaly Detection for Fundus Images,0.200977,"Recently anomaly detection has drawn much attention in diagnosing ocular
diseases. Most existing anomaly detection research in fundus images has
relatively large anomaly scores in the salient retinal structures, such as
blood vessels, optical cups and discs. In this paper, we propose a Region and
Spatial Aware Anomaly Detection (ReSAD) method for fundus images, which obtains
local region and long-range spatial information to reduce the false positives
in the normal structure. ReSAD transfers a pre-trained model to extract the
features of normal fundus images and applies the Region-and-Spatial-Aware
feature Combination module (ReSC) for pixel-level features to build a memory
bank. In the testing phase, ReSAD uses the memory bank to determine
out-of-distribution samples as abnormalities. Our method significantly
outperforms the existing anomaly detection methods for fundus images on two
publicly benchmark datasets.",None,-1
c51165c0-5e9d-489c-8109-e0ea88d34441,Fine-grained building roof instance segmentation based on domain adapted pretraining and composite dual-backbone,0.468793,"The diversity of building architecture styles of global cities situated on
various landforms, the degraded optical imagery affected by clouds and shadows,
and the significant inter-class imbalance of roof types pose challenges for
designing a robust and accurate building roof instance segmentor. To address
these issues, we propose an effective framework to fulfill semantic
interpretation of individual buildings with high-resolution optical satellite
imagery. Specifically, the leveraged domain adapted pretraining strategy and
composite dual-backbone greatly facilitates the discriminative feature
learning. Moreover, new data augmentation pipeline, stochastic weight averaging
(SWA) training and instance segmentation based model ensemble in testing are
utilized to acquire additional performance boost. Experiment results show that
our approach ranks in the first place of the 2023 IEEE GRSS Data Fusion Contest
(DFC) Track 1 test phase ($mAP_{50}$:50.6\%). Note-worthily, we have also
explored the potential of multimodal data fusion with both optical satellite
imagery and SAR data.",None,-1
14a5e82e-0607-4fd2-8d4e-1cbde94831c0,Machine Learning Approaches in Agile Manufacturing with Recycled Materials for Sustainability,0.307125,"It is important to develop sustainable processes in materials science and
manufacturing that are environmentally friendly. AI can play a significant role
in decision support here as evident from our earlier research leading to tools
developed using our proposed machine learning based approaches. Such tools
served the purpose of computational estimation and expert systems. This
research addresses environmental sustainability in materials science via
decision support in agile manufacturing using recycled and reclaimed materials.
It is a safe and responsible way to turn a specific waste stream to value-added
products. We propose to use data-driven methods in AI by applying machine
learning models for predictive analysis to guide decision support in
manufacturing. This includes harnessing artificial neural networks to study
parameters affecting heat treatment of materials and impacts on their
properties; deep learning via advances such as convolutional neural networks to
explore grain size detection; and other classifiers such as Random Forests to
analyze phrase fraction detection. Results with all these methods seem
promising to embark on further work, e.g. ANN yields accuracy around 90\% for
predicting micro-structure development as per quench tempering, a heat
treatment process. Future work entails several challenges: investigating
various computer vision models (VGG, ResNet etc.) to find optimal accuracy,
efficiency and robustness adequate for sustainable processes; creating
domain-specific tools using machine learning for decision support in agile
manufacturing; and assessing impacts on sustainability with metrics
incorporating the appropriate use of recycled materials as well as the
effectiveness of developed products. Our work makes impacts on green technology
for smart manufacturing, and is motivated by related work in the highly
interesting realm of AI for materials science.",None,-1
095ebbb6-235c-49dd-bbd1-ce202945da2c,Relighting Neural Radiance Fields with Shadow and Highlight Hints,0.839207,"This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.",None,-1
e12b0497-5eb0-492b-8b29-a1c9d045e7cb,Robust Dynamic Radiance Fields,0.971927,"Dynamic radiance field reconstruction methods aim to model the time-varying
structure and appearance of a dynamic scene. Existing methods, however, assume
that accurate camera poses can be reliably estimated by Structure from Motion
(SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often
fail or produce erroneous poses on challenging videos with highly dynamic
objects, poorly textured surfaces, and rotating camera motion. We address this
robustness issue by jointly estimating the static and dynamic radiance fields
along with the camera parameters (poses and focal length). We demonstrate the
robustness of our approach via extensive quantitative and qualitative
experiments. Our results show favorable performance over the state-of-the-art
dynamic view synthesis methods.",None,-1
5ad25e5d-0e40-4ccf-80e6-0c39aa6c0ae8,Chit-Chat or Deep Talk: Prompt Engineering for Process Mining,0.669416,"This research investigates the application of Large Language Models (LLMs) to
augment conversational agents in process mining, aiming to tackle its inherent
complexity and diverse skill requirements. While LLM advancements present novel
opportunities for conversational process mining, generating efficient outputs
is still a hurdle. We propose an innovative approach that amend many issues in
existing solutions, informed by prior research on Natural Language Processing
(NLP) for conversational agents. Leveraging LLMs, our framework improves both
accessibility and agent performance, as demonstrated by experiments on public
question and data sets. Our research sets the stage for future explorations
into LLMs' role in process mining and concludes with propositions for enhancing
LLM memory, implementing real-time user testing, and examining diverse data
sets.",None,-1
04709b5b-fbe5-4215-9918-3e02f57632f4,An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development,0.361627,"Causal networks are widely used in many fields, including epidemiology,
social science, medicine, and engineering, to model the complex relationships
between variables. While it can be convenient to algorithmically infer these
models directly from observational data, the resulting networks are often
plagued with erroneous edges. Auditing and correcting these networks may
require domain expertise frequently unavailable to the analyst. We propose the
use of large language models such as ChatGPT as an auditor for causal networks.
Our method presents ChatGPT with a causal network, one edge at a time, to
produce insights about edge directionality, possible confounders, and mediating
variables. We ask ChatGPT to reflect on various aspects of each causal link and
we then produce visualizations that summarize these viewpoints for the human
analyst to direct the edge, gather more data, or test further hypotheses. We
envision a system where large language models, automated causal inference, and
the human analyst and domain expert work hand in hand as a team to derive
holistic and comprehensive causal models for any given case scenario. This
paper presents first results obtained with an emerging prototype.",None,-1
54e16218-ceb2-46a6-991f-4b6634f03e65,Deep Task-specific Bottom Representation Network for Multi-Task Recommendation,0.147916,"Neural-based multi-task learning (MTL) has gained significant improvement,
and it has been successfully applied to recommendation system (RS). Recent deep
MTL methods for RS (e.g. MMoE, PLE) focus on designing soft gating-based
parameter-sharing networks that implicitly learn a generalized representation
for each task. However, MTL methods may suffer from performance degeneration
when dealing with conflicting tasks, as negative transfer effects can occur on
the task-shared bottom representation. This can result in a reduced capacity
for MTL methods to capture task-specific characteristics, ultimately impeding
their effectiveness and hindering the ability to generalize well on all tasks.
In this paper, we focus on the bottom representation learning of MTL in RS and
propose the Deep Task-specific Bottom Representation Network (DTRN) to
alleviate the negative transfer problem. DTRN obtains task-specific bottom
representation explicitly by making each task have its own representation
learning network in the bottom representation modeling stage. Specifically, it
extracts the user's interests from multiple types of behavior sequences for
each task through the parameter-efficient hypernetwork. To further obtain the
dedicated representation for each task, DTRN refines the representation of each
feature by employing a SENet-like network for each task. The two proposed
modules can achieve the purpose of getting task-specific bottom representation
to relieve tasks' mutual interference. Moreover, the proposed DTRN is flexible
to combine with existing MTL methods. Experiments on one public dataset and one
industrial dataset demonstrate the effectiveness of the proposed DTRN.",None,-1
550bd658-64d7-4c9e-95eb-626ba8cf9110,Conditional Denoising Diffusion for Sequential Recommendation,0.624146,"Generative models have attracted significant interest due to their ability to
handle uncertainty by learning the inherent data distributions. However, two
prominent generative models, namely Generative Adversarial Networks (GANs) and
Variational AutoEncoders (VAEs), exhibit challenges that impede achieving
optimal performance in sequential recommendation tasks. Specifically, GANs
suffer from unstable optimization, while VAEs are prone to posterior collapse
and over-smoothed generations. The sparse and noisy nature of sequential
recommendation further exacerbates these issues. In response to these
limitations, we present a conditional denoising diffusion model, which includes
a sequence encoder, a cross-attentive denoising decoder, and a step-wise
diffuser. This approach streamlines the optimization and generation process by
dividing it into easier and tractable steps in a conditional autoregressive
manner. Furthermore, we introduce a novel optimization schema that incorporates
both cross-divergence loss and contrastive loss. This novel training schema
enables the model to generate high-quality sequence/item representations and
meanwhile precluding collapse. We conducted comprehensive experiments on four
benchmark datasets, and the superior performance achieved by our model attests
to its efficacy.",None,-1
47cad226-1207-4434-ab36-2865448cfaef,Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions,0.997889,"As systems based on opaque Artificial Intelligence (AI) continue to flourish
in diverse real-world applications, understanding these black box models has
become paramount. In response, Explainable AI (XAI) has emerged as a field of
research with practical and ethical benefits across various domains. This paper
not only highlights the advancements in XAI and its application in real-world
scenarios but also addresses the ongoing challenges within XAI, emphasizing the
need for broader perspectives and collaborative efforts. We bring together
experts from diverse fields to identify open problems, striving to synchronize
research agendas and accelerate XAI in practical applications. By fostering
collaborative discussion and interdisciplinary cooperation, we aim to propel
XAI forward, contributing to its continued success. Our goal is to put forward
a comprehensive proposal for advancing XAI. To achieve this goal, we present a
manifesto of 27 open problems categorized into nine categories. These
challenges encapsulate the complexities and nuances of XAI and offer a road map
for future research. For each problem, we provide promising research directions
in the hope of harnessing the collective intelligence of interested
stakeholders.",None,-1
420e93b1-7320-46ea-9a94-d981ba7d69d3,KL-Divergence Guided Temperature Sampling,0.124823,"Temperature sampling is a conventional approach to diversify large language
model predictions. As temperature increases, the prediction becomes diverse but
also vulnerable to hallucinations -- generating tokens that are sensible but
not factual. One common approach to mitigate hallucinations is to provide
source/grounding documents and the model is trained to produce predictions that
bind to and are attributable to the provided source. It appears that there is a
trade-off between diversity and attribution. To mitigate any such trade-off, we
propose to relax the constraint of having a fixed temperature over decoding
steps, and a mechanism to guide the dynamic temperature according to its
relevance to the source through KL-divergence. Our experiments justifies the
trade-off, and shows that our sampling algorithm outperforms the conventional
top-k and top-p algorithms in conversational question-answering and
summarization tasks.",None,-1
c8d78c08-174e-48d6-a2eb-0844ffd373b5,Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment,0.47402,"Text-video retrieval is a challenging cross-modal task, which aims to align
visual entities with natural language descriptions. Current methods either fail
to leverage the local details or are computationally expensive. What's worse,
they fail to leverage the heterogeneous concepts in data. In this paper, we
propose the Disentangled Conceptualization and Set-to-set Alignment (DiCoSA) to
simulate the conceptualizing and reasoning process of human beings. For
disentangled conceptualization, we divide the coarse feature into multiple
latent factors related to semantic concepts. For set-to-set alignment, where a
set of visual concepts correspond to a set of textual concepts, we propose an
adaptive pooling method to aggregate semantic concepts to address the partial
matching. In particular, since we encode concepts independently in only a few
dimensions, DiCoSA is superior at efficiency and granularity, ensuring
fine-grained interactions using a similar computational complexity as
coarse-grained alignment. Extensive experiments on five datasets, including
MSR-VTT, LSMDC, MSVD, ActivityNet, and DiDeMo, demonstrate that our method
outperforms the existing state-of-the-art methods.",None,-1
2ba4bb46-4738-4eec-8c67-fcc623e3f93d,SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,0.59387,"Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast ""world knowledge"". Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions' feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.",None,-1
67e5ce40-096a-4625-9d62-5e2d4fb78744,Adaptive Policy with Wait-$k$ Model for Simultaneous Translation,0.314506,"Simultaneous machine translation (SiMT) requires a robust read/write policy
in conjunction with a high-quality translation model. Traditional methods rely
on either a fixed wait-$k$ policy coupled with a standalone wait-$k$
translation model, or an adaptive policy jointly trained with the translation
model. In this study, we propose a more flexible approach by decoupling the
adaptive policy model from the translation model. Our motivation stems from the
observation that a standalone multi-path wait-$k$ model performs competitively
with adaptive policies utilized in state-of-the-art SiMT approaches.
Specifically, we introduce DaP, a divergence-based adaptive policy, that makes
read/write decisions for any translation model based on the potential
divergence in translation distributions resulting from future information. DaP
extends a frozen wait-$k$ model with lightweight parameters, and is both memory
and computation efficient. Experimental results across various benchmarks
demonstrate that our approach offers an improved trade-off between translation
accuracy and latency, outperforming strong baselines.",None,-1
9a5cd29e-5777-4a3d-962c-8f3bdcc9b39e,"Pre-train, Adapt and Detect: Multi-Task Adapter Tuning for Camouflaged Object Detection",0.0558728,"Camouflaged object detection (COD), aiming to segment camouflaged objects
which exhibit similar patterns with the background, is a challenging task. Most
existing works are dedicated to establishing specialized modules to identify
camouflaged objects with complete and fine details, while the boundary can not
be well located for the lack of object-related semantics. In this paper, we
propose a novel ``pre-train, adapt and detect"" paradigm to detect camouflaged
objects. By introducing a large pre-trained model, abundant knowledge learned
from massive multi-modal data can be directly transferred to COD. A lightweight
parallel adapter is inserted to adjust the features suitable for the downstream
COD task. Extensive experiments on four challenging benchmark datasets
demonstrate that our method outperforms existing state-of-the-art COD models by
large margins. Moreover, we design a multi-task learning scheme for tuning the
adapter to exploit the shareable knowledge across different semantic classes.
Comprehensive experimental results showed that the generalization ability of
our model can be substantially improved with multi-task adapter initialization
on source tasks and multi-task adaptation on target tasks.",None,-1
1057776b-5565-423c-a61f-dbe49e6148bc,LogAI: A Library for Log Analytics and Intelligence,0.172941,"Software and System logs record runtime information about processes executing
within a system. These logs have become the most critical and ubiquitous forms
of observability data that help developers understand system behavior, monitor
system health and resolve issues. However, the volume of logs generated can be
humongous (of the order of petabytes per day) especially for complex
distributed systems, such as cloud, search engine, social media, etc. This has
propelled a lot of research on developing AI-based log based analytics and
intelligence solutions that can process huge volume of raw logs and generate
insights. In order to enable users to perform multiple types of AI-based log
analysis tasks in a uniform manner, we introduce LogAI
(https://github.com/salesforce/logai), a one-stop open source library for log
analytics and intelligence. LogAI supports tasks such as log summarization, log
clustering and log anomaly detection. It adopts the OpenTelemetry data model,
to enable compatibility with different log management platforms. LogAI provides
a unified model interface and provides popular time-series, statistical
learning and deep learning models. Alongside this, LogAI also provides an
out-of-the-box GUI for users to conduct interactive analysis. With LogAI, we
can also easily benchmark popular deep learning algorithms for log anomaly
detection without putting in redundant effort to process the logs. We have
opensourced LogAI to cater to a wide range of applications benefiting both
academic research and industrial prototyping.",None,-1
0399c74f-aad7-4799-98ad-4058eff9daa0,DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation,0.436157,"We present DeblurSR, a novel motion deblurring approach that converts a
blurry image into a sharp video. DeblurSR utilizes event data to compensate for
motion ambiguities and exploits the spiking representation to parameterize the
sharp output video as a mapping from time to intensity. Our key contribution,
the Spiking Representation (SR), is inspired by the neuromorphic principles
determining how biological neurons communicate with each other in living
organisms. We discuss why the spikes can represent sharp edges and how the
spiking parameters are interpreted from the neuromorphic perspective. DeblurSR
has higher output quality and requires fewer computing resources than
state-of-the-art event-based motion deblurring methods. We additionally show
that our approach easily extends to video super-resolution when combined with
recent advances in implicit neural representation. The implementation and
animated visualization of DeblurSR are available at
https://github.com/chensong1995/DeblurSR.",None,-1
d9d78007-739a-4566-b340-dffe996fe848,Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems,0.329734,"In the realm of cybersecurity, intrusion detection systems (IDS) detect and
prevent attacks based on collected computer and network data. In recent
research, IDS models have been constructed using machine learning (ML) and deep
learning (DL) methods such as Random Forest (RF) and deep neural networks
(DNN). Feature selection (FS) can be used to construct faster, more
interpretable, and more accurate models. We look at three different FS
techniques; RF information gain (RF-IG), correlation feature selection using
the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our
results show CFS-BA to be the most efficient of the FS methods, building in 55%
of the time of the best RF-IG model while achieving 99.99% of its accuracy.
This reinforces prior contributions attesting to CFS-BA's accuracy while
building upon the relationship between subset size, CFS score, and RF-IG score
in final results.",None,-1
16c801aa-181f-4a6c-9109-015e952beea1,Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models,0.169119,"Large Language Models (LLMs) are trained primarily on minimally processed web
text, which exhibits the same wide range of social biases held by the humans
who created that content. Consequently, text generated by LLMs can
inadvertently perpetuate stereotypes towards marginalized groups, like the
LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs
generate text describing people with different sexual identities. Analyzing
bias in the text generated by an LLM using regard score shows measurable bias
against queer people. We then show that a post-hoc method based on
chain-of-thought prompting using SHAP analysis can increase the regard of the
sentence, representing a promising approach towards debiasing the output of
LLMs in this setting.",None,-1
b732dcdd-0c5a-4c94-8813-50e113c6bdec,Transformer-based model for monocular visual odometry: a video understanding approach,0.717169,"Estimating the camera's pose given images of a single camera is a traditional
task in mobile robots and autonomous vehicles. This problem is called monocular
visual odometry and it often relies on geometric approaches that require
considerable engineering effort for a specific scenario. Deep learning methods
have shown to be generalizable after proper training and a large amount of
available data. Transformer-based architectures have dominated the
state-of-the-art in natural language processing and computer vision tasks, such
as image and video understanding. In this work, we deal with the monocular
visual odometry as a video understanding task to estimate the 6-DoF camera's
pose. We contribute by presenting the TSformer-VO model based on
spatio-temporal self-attention mechanisms to extract features from clips and
estimate the motions in an end-to-end manner. Our approach achieved competitive
state-of-the-art performance compared with geometry-based and deep
learning-based methods on the KITTI visual odometry dataset, outperforming the
DeepVO implementation highly accepted in the visual odometry community.",None,-1
6a327e52-53b6-4235-9779-53d52469a435,GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods,0.515653,"A key goal for the advancement of AI is to develop technologies that serve
the needs not just of one group but of all communities regardless of their
geographical region. In fact, a significant proportion of knowledge is locally
shared by people from certain regions but may not apply equally in other
regions because of cultural differences. If a model is unaware of regional
characteristics, it may lead to performance disparity across regions and result
in bias against underrepresented groups. We propose GIVL, a Geographically
Inclusive Vision-and-Language Pre-trained model. There are two attributes of
geo-diverse visual concepts which can help to learn geo-diverse knowledge: 1)
concepts under similar categories have unique knowledge and visual
characteristics, 2) concepts with similar visual features may fall in
completely different categories. Motivated by the attributes, we design new
pre-training objectives Image Knowledge Matching (IKM) and Image Edit Checking
(IEC) to pre-train GIVL. Compared with similar-size models pre-trained with
similar scale of data, GIVL achieves state-of-the-art (SOTA) and more balanced
performance on geo-diverse V&L tasks.",None,-1
dbdcd7b3-a487-47c0-acea-fb0e0b99d998,LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning,0.872162,"Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)
competition is detailed in this paper. Unlike conventional video captioning
tasks, GEBC demands that the captioning model possess an understanding of
immediate changes in status around the designated video boundary, making it a
difficult task. This paper proposes an effective model LLMVA-GEBC (Large
Language Model with Video Adapter for Generic Event Boundary Captioning): (1)
We utilize a pretrained LLM for generating human-like captions with high
quality. (2) To adapt the model to the GEBC task, we take the video Q-former as
an adapter and train it with the frozen visual feature extractors and LLM. Our
proposed method achieved a 76.14 score on the test set and won the first place
in the challenge. Our code is available at
https://github.com/zjr2000/LLMVA-GEBC .",None,-1
1b5aab62-d1ab-4b00-a834-fb937c32a1f8,GAM : Gradient Attention Module of Optimization for Point Clouds Analysis,0.646464,"In point cloud analysis tasks, the existing local feature aggregation
descriptors (LFAD) are unable to fully utilize information in the neighborhood
of central points. Previous methods rely solely on Euclidean distance to
constrain the local aggregation process, which can be easily affected by
abnormal points and cannot adequately fit with the original geometry of the
point cloud. We believe that fine-grained geometric information (FGGI) is
significant for the aggregation of local features. Therefore, we propose a
gradient-based local attention module, termed as Gradient Attention Module
(GAM), to address the aforementioned problem. Our proposed GAM simplifies the
process that extracts gradient information in the neighborhood and uses the
Zenith Angle matrix and Azimuth Angle matrix as explicit representation, which
accelerates the module by 35X. Comprehensive experiments were conducted on five
benchmark datasets to demonstrate the effectiveness and generalization
capability of the proposed GAM for 3D point cloud analysis. Especially on S3DIS
dataset, GAM achieves the best performance among current point-based models
with mIoU/OA/mAcc of 74.4%/90.6%/83.2%, respectively.",None,-1
f0890dfe-2cf3-40db-988a-a7056d53dbfd,DualGenerator: Information Interaction-based Generative Network for Point Cloud Completion,0.0849805,"Point cloud completion estimates complete shapes from incomplete point clouds
to obtain higher-quality point cloud data. Most existing methods only consider
global object features, ignoring spatial and semantic information of adjacent
points. They cannot distinguish structural information well between different
object parts, and the robustness of models is poor. To tackle these challenges,
we propose an information interaction-based generative network for point cloud
completion ($\mathbf{DualGenerator}$). It contains an adversarial generation
path and a variational generation path, which interact with each other and
share weights. DualGenerator introduces a local refinement module in generation
paths, which captures general structures from partial inputs, and then refines
shape details of the point cloud. It promotes completion in the unknown region
and makes a distinction between different parts more obvious. Moreover, we
design DGStyleGAN to improve the generation quality further. It promotes the
robustness of this network combined with fusion analysis of dual-path
completion results. Qualitative and quantitative evaluations demonstrate that
our method is superior on MVP and Completion3D datasets. The performance will
not degrade significantly after adding noise interference or sparse sampling.",None,-1
35d51693-927a-4972-9608-641df083ceba,Audio Visual Language Maps for Robot Navigation,0.78493,"While interacting in the world is a multi-sensory experience, many robots
continue to predominantly rely on visual perception to map and navigate in
their environments. In this work, we propose Audio-Visual-Language Maps
(AVLMaps), a unified 3D spatial map representation for storing cross-modal
information from audio, visual, and language cues. AVLMaps integrate the
open-vocabulary capabilities of multimodal foundation models pre-trained on
Internet-scale data by fusing their features into a centralized 3D voxel grid.
In the context of navigation, we show that AVLMaps enable robot systems to
index goals in the map based on multimodal queries, e.g., textual descriptions,
images, or audio snippets of landmarks. In particular, the addition of audio
information enables robots to more reliably disambiguate goal locations.
Extensive experiments in simulation show that AVLMaps enable zero-shot
multimodal goal navigation from multimodal prompts and provide 50% better
recall in ambiguous scenarios. These capabilities extend to mobile robots in
the real world - navigating to landmarks referring to visual, audio, and
spatial concepts. Videos and code are available at: https://avlmaps.github.io.",None,-1
2865e255-f22c-4ddb-9171-a77f9c038645,How Many Demonstrations Do You Need for In-context Learning?,0.468882,"Large language models (LLMs) are capable to perform complex reasoning by
in-context learning (ICL) when provided with a few input-output demonstrations
(demos) and more powerful when intermediate reasoning steps (""chain of thoughts
(CoT)"") of the demos are given. Is it necessary to use multi-demo in ICL? In
this paper, we study ICL using fewer demos for each test query on the tasks
in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation
when using only one randomly chosen demo. To study this phenomenon, for each
test query, we categorize demos into ""correct demos"" leading to the correct
answer, and ""wrong demos"" resulting in wrong answers. Our analysis reveals an
inherent bias in those widely studied datasets: most demos are correct for a
majority of test queries, which explains the good performance of using one
random demo. Moreover, ICL (with and w/o CoT) using only one correct demo
significantly outperforms all-demo ICL adopted by most previous works,
indicating the weakness of LLMs in finding correct demo(s) for input queries,
which is difficult to evaluate on the biased datasets. Furthermore, we observe
a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy
degrades(improves) when given more correct(wrong) demos. This implies that ICL
can be easily misguided by interference among demos and their spurious
correlations. Our analyses highlight several fundamental challenges that need
to be addressed in LLMs training, ICL, and benchmark design.",None,-1
259668fe-6e97-40c6-8671-9657a52e577a,Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving,0.357841,"Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous
vehicles (AVs) to make informed decisions and respond proactively in critical
road scenarios. Promising results of 3D HPE have been gained in several domains
such as human-computer interaction, robotics, sports and medical analytics,
often based on data collected in well-controlled laboratory environments.
Nevertheless, the transfer of 3D HPE methods to AVs has received limited
research attention, due to the challenges posed by obtaining accurate 3D pose
annotations and the limited suitability of data from other domains.
  We present a simple yet efficient weakly supervised approach for 3D HPE in
the AV context by employing a high-level sensor fusion between camera and LiDAR
data. The weakly supervised setting enables training on the target datasets
without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor
and pseudo labels generated from LiDAR to image projections. Our approach
outperforms state-of-the-art results by up to $\sim$ 13% on the Waymo Open
Dataset in the weakly supervised setting and achieves state-of-the-art results
in the supervised setting.",None,-1
4a9d769b-20a5-475a-a9a0-a92e24912d49,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,0.998479,"We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models. Code and data are
available at https://github.com/yuweihao/MM-Vet.",None,-1
b52d15f8-6a56-47c7-a0f6-cbc9b876e660,Can Language Models Employ the Socratic Method? Experiments with Code Debugging,0.802069,"When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark",None,-1
703c49ec-3c41-44c6-9e29-98c9fc135519,DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields,0.696845,"Recent works such as BARF and GARF can bundle adjust camera poses with neural
radiance fields (NeRF) which is based on coordinate-MLPs. Despite the
impressive results, these methods cannot be applied to Generalizable NeRFs
(GeNeRFs) which require image feature extractions that are often based on more
complicated 3D CNN or transformer architectures. In this work, we first analyze
the difficulties of jointly optimizing camera poses with GeNeRFs, and then
further propose our DBARF to tackle these issues. Our DBARF which bundle
adjusts camera poses by taking a cost feature map as an implicit cost function
can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF
and its follow-up works, which can only be applied to per-scene optimized NeRFs
and need accurate initial camera poses with the exception of forward-facing
scenes, our method can generalize across scenes and does not require any good
initialization. Experiments show the effectiveness and generalization ability
of our DBARF when evaluated on real-world datasets. Our code is available at
\url{https://aibluefisher.github.io/dbarf}.",None,-1
2c8d64b9-5aa6-4f2b-b6cf-619f61e6b2e9,Hi4D: 4D Instance Segmentation of Close Human Interaction,0.893827,"We propose Hi4D, a method and dataset for the automatic analysis of
physically close human-human interaction under prolonged contact. Robustly
disentangling several in-contact subjects is a challenging task due to
occlusions and complex shapes. Hence, existing multi-view systems typically
fuse 3D surfaces of close subjects into a single, connected mesh. To address
this issue we leverage i) individually fitted neural implicit avatars; ii) an
alternating optimization scheme that refines pose and surface through periods
of close proximity; and iii) thus segment the fused raw scans into individual
instances. From these instances we compile Hi4D dataset of 4D textured scans of
20 subject pairs, 100 sequences, and a total of more than 11K frames. Hi4D
contains rich interaction-centric annotations in 2D and 3D alongside accurately
registered parametric body models. We define varied human pose and shape
estimation tasks on this dataset and provide results from state-of-the-art
methods on these benchmarks.",None,-1
7e51949a-72c6-47bd-a587-20cd39a738e5,"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",0.994502,"Large Language Models (LLMs) have made remarkable strides in various tasks.
Whether LLMs are competitive few-shot solvers for information extraction (IE)
tasks, however, remains an open problem. In this work, we aim to provide a
thorough answer to this question. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate that current advanced LLMs
consistently exhibit inferior performance, higher latency, and increased budget
requirements compared to fine-tuned SLMs under most settings. Therefore, we
conclude that LLMs are not effective few-shot information extractors in
general. Nonetheless, we illustrate that with appropriate prompting strategies,
LLMs can effectively complement SLMs and tackle challenging samples that SLMs
struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm
to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as
filters and LLMs serve as rerankers. By prompting LLMs to rerank a small
portion of difficult samples identified by SLMs, our preliminary system
consistently achieves promising improvements (2.4% F1-gain on average) on
various IE tasks, with an acceptable time and cost investment.",None,-1
f3b82c29-373a-4176-9961-3002fca19233,Deep Learning based Multi-Label Image Classification of Protest Activities,0.323237,"With the rise of internet technology amidst increasing rates of urbanization,
sharing information has never been easier thanks to globally-adopted platforms
for digital communication. The resulting output of massive amounts of
user-generated data can be used to enhance our understanding of significant
societal issues particularly for urbanizing areas. In order to better analyze
protest behavior, we enhanced the GSR dataset and manually labeled all the
images. We used deep learning techniques to analyze social media data to detect
social unrest through image classification, which performed good in predict
multi-attributes, then also used map visualization to display protest behaviors
across the country.",None,-1
1020c957-d403-4c82-abc0-aeca107d14b3,Image To Tree with Recursive Prompting,0.0770791,"Extracting complex structures from grid-based data is a common key step in
automated medical image analysis. The conventional solution to recovering
tree-structured geometries typically involves computing the minimal cost path
through intermediate representations derived from segmentation masks. However,
this methodology has significant limitations in the context of projective
imaging of tree-structured 3D anatomical data such as coronary arteries, since
there are often overlapping branches in the 2D projection. In this work, we
propose a novel approach to predicting tree connectivity structure which
reformulates the task as an optimization problem over individual steps of a
recursive process. We design and train a two-stage model which leverages the
UNet and Transformer architectures and introduces an image-based prompting
technique. Our proposed method achieves compelling results on a pair of
synthetic datasets, and outperforms a shortest-path baseline.",None,-1
30df3d38-ff9d-4786-b40d-dcb205cf78a5,SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image,0.0896446,"Recently, Segmenting Anything has taken an important step towards general
artificial intelligence. At the same time, its reliability and fairness have
also attracted great attention, especially in the field of health care. In this
study, we propose multi-box prompts triggered uncertainty estimation for SAM
cues to demonstrate the reliability of segmented lesions or tissues. We
estimate the distribution of SAM predictions via Monte Carlo with prior
distribution parameters, which employs different prompts as formulation of
test-time augmentation. Our experimental results found that multi-box prompts
augmentation improve the SAM performance, and endowed each pixel with
uncertainty. This provides the first paradigm for a reliable SAM.",None,-1
25c01cda-cecb-477b-82df-23e7a12eea45,Direct Parameterization of Lipschitz-Bounded Deep Networks,0.996865,"This paper introduces a new parameterization of deep neural networks (both
fully-connected and convolutional) with guaranteed $\ell^2$ Lipschitz bounds,
i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are
equivalent to the tightest-known bounds based on certification via a
semidefinite program (SDP). We provide a ``direct'' parameterization, i.e., a
smooth mapping from $\mathbb R^N$ onto the set of weights satisfying the
SDP-based bound. Moreover, our parameterization is complete, i.e. a neural
network satisfies the SDP bound if and only if it can be represented via our
parameterization. This enables training using standard gradient methods,
without any inner approximation or computationally intensive tasks (e.g.
projections or barrier terms) for the SDP constraint. The new parameterization
can equivalently be thought of as either a new layer type (the \textit{sandwich
layer}), or a novel parameterization of standard feedforward networks with
parameter sharing between neighbouring layers. A comprehensive set of
experiments on image classification shows that sandwich layers outperform
previous approaches on both empirical and certified robust accuracy. Code is
available at \url{https://github.com/acfr/LBDN}.",None,-1
c4a96aac-cae3-4db6-a5a0-fc73b8379eca,A Novel Deep Reinforcement Learning-based Approach for Enhancing Spectral Efficiency of IRS-assisted Wireless Systems,0.164745,"This letter investigates an intelligent reflecting surfaces (IRS)-enhanced
network from spectral efficiency enhancement point of view for downlink
multi-user (MU) multi-input-single-output systems (MISO). In contrast to
previous works which mainly focused on alternative optimization methods, we
investigate the non-convex joint optimization problem of the active transmit
beamforming matrix at the base station together with the passive phase shift
matrix at the IRS by utilizing two deep reinforcement learning frameworks, i.
e., deep deterministic policy gradient (DDPG) and twin delayed DDPG (TD3).
Simulation results reveal that the neural networks in the latter scheme perform
generally more satisfactorily in various situations.",None,-1
cef3e81c-ad83-4504-85ff-e4940bd13abe,ModelScope Text-to-Video Technical Report,0.996967,"This paper introduces ModelScopeT2V, a text-to-video synthesis model that
evolves from a text-to-image synthesis model (i.e., Stable Diffusion).
ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame
generation and smooth movement transitions. The model could adapt to varying
frame numbers during training and inference, rendering it suitable for both
image-text and video-text datasets. ModelScopeT2V brings together three
components (i.e., VQGAN, a text encoder, and a denoising UNet), totally
comprising 1.7 billion parameters, in which 0.5 billion parameters are
dedicated to temporal capabilities. The model demonstrates superior performance
over state-of-the-art methods across three evaluation metrics. The code and an
online demo are available at
\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.",None,-1
71b9bc2d-f155-4121-a9cd-42698bf4e524,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,0.596228,"Temporal sentence grounding aims to detect the event timestamps described by
the natural language query from given untrimmed videos. The existing
fully-supervised setting achieves great performance but requires expensive
annotation costs; while the weakly-supervised setting adopts cheap labels but
performs poorly. To pursue high performance with less annotation cost, this
paper introduces an intermediate partially-supervised setting, i.e., only
short-clip or even single-frame labels are available during training. To take
full advantage of partial labels, we propose a novel quadruple constraint
pipeline to comprehensively shape event-query aligned representations, covering
intra- and inter-samples, uni- and multi-modalities. The former raises
intra-cluster compactness and inter-cluster separability; while the latter
enables event-background separation and event-query gather. To achieve more
powerful performance with explicit grounding optimization, we further introduce
a partial-full union framework, i.e., bridging with an additional
fully-supervised branch, to enjoy its impressive grounding bonus, and be robust
to partial annotations. Extensive experiments and ablations on Charades-STA and
ActivityNet Captions demonstrate the significance of partial supervision and
our superior performance.",None,-1
304cfeb0-a6d6-41d2-a42a-3f4cf42e50ae,Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details,0.257852,"We propose Text2Scene, a method to automatically create realistic textures
for virtual scenes composed of multiple objects. Guided by a reference image
and text descriptions, our pipeline adds detailed texture on labeled 3D
geometries in the room such that the generated colors respect the hierarchical
structure or semantic parts that are often composed of similar materials.
Instead of applying flat stylization on the entire scene at a single step, we
obtain weak semantic cues from geometric segmentation, which are further
clarified by assigning initial colors to segmented parts. Then we add texture
details for individual objects such that their projections on image space
exhibit feature embedding aligned with the embedding of the input. The
decomposition makes the entire pipeline tractable to a moderate amount of
computation resources and memory. As our framework utilizes the existing
resources of image and text embedding, it does not require dedicated datasets
with high-quality textures designed by skillful artists. To the best of our
knowledge, it is the first practical and scalable approach that can create
detailed and realistic textures of the desired style that maintain structural
context for scenes with multiple objects.",None,-1
773db850-b8e9-4e35-8f21-6c127c0a6b08,MN-Pair Contrastive Damage Representation and Clustering for Prognostic Explanation,0.0483439,"For infrastructure inspections, damage representation does not constantly
match the predefined classes of damage grade, resulting in detailed clusters of
unseen damages or more complex clusters from overlapped space between two
grades. The damage representation has fundamentally complex features;
consequently, not all the damage classes can be perfectly predefined. The
proposed MN-pair contrastive learning method helps to explore an embedding
damage representation beyond the predefined classes by including more detailed
clusters. It maximizes both the similarity of M-1 positive images close to an
anchor and dissimilarity of N-1 negative images using both weighting loss
functions. It learns faster than the N-pair algorithm using one positive image.
We proposed a pipeline to obtain the damage representation and used a
density-based clustering on a 2-D reduction space to automate finer cluster
discrimination. We also visualized the explanation of the damage feature using
Grad-CAM for MN-pair damage metric learning. We demonstrated our method in
three experimental studies: steel product defect, concrete crack, and the
effectiveness of our method and discuss future works.",None,-1
67b09660-08f0-4eae-bc54-80feb21b631d,Distributed Trust Through the Lens of Software Architecture,0.394839,"Distributed trust is a nebulous concept that has evolved from different
perspectives in recent years. While one can attribute its current prominence to
blockchain and cryptocurrency, the distributed trust concept has been
cultivating progress in federated learning, trustworthy and responsible AI in
an ecosystem setting, data sharing, privacy issues across organizational
boundaries, and zero trust cybersecurity. This paper will survey the concept of
distributed trust in multiple disciplines. It will take a system/software
architecture point of view to look at trust redistribution/shift and the
associated tradeoffs in systems and applications enabled by distributed trust
technologies.",None,-1
093c54f6-b20a-43e3-9e85-fc7c6708faca,Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings,0.192463,"Designing a new clinical trial entails many decisions, such as defining a
cohort and setting the study objectives to name a few, and therefore can
benefit from recommendations based on exhaustive mining of past clinical trial
records. Here, we propose a novel recommendation methodology, based on neural
embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We
addressed several important research questions in this context, including
designing a knowledge graph (KG) for clinical trial data, effectiveness of
various KG embedding (KGE) methods for it, a novel inductive inference using
KGE, and its use in generating recommendations for clinical trial design. We
used publicly available data from clinicaltrials.gov for the study. Results
show that our recommendations approach achieves relevance scores of 70%-83%,
measured as the text similarity to actual clinical trial elements, and the most
relevant recommendation can be found near the top of list. Our study also
suggests potential improvement in training KGE using node semantics.",None,-1
37700bb1-6916-43ac-8fad-ef602adf08c4,SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities,0.88527,"The rapid advance in artificial intelligence technology has facilitated the
prosperity of digital humanities research. Against such backdrop, research
methods need to be transformed in the intelligent processing of ancient texts,
which is a crucial component of digital humanities research, so as to adapt to
new development trends in the wave of AIGC. In this study, we propose a GPT
model called SikuGPT based on the corpus of Siku Quanshu. The model's
performance in tasks such as intralingual translation and text classification
exceeds that of other GPT-type models aimed at processing ancient texts.
SikuGPT's ability to process traditional Chinese ancient texts can help promote
the organization of ancient information and knowledge services, as well as the
international dissemination of Chinese ancient culture.",None,-1
b3dfc184-be80-4d26-af17-aa36be3d420c,RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions,0.67988,"Generative AI models have shown impressive ability to produce images with
text prompts, which could benefit creativity in visual art creation and
self-expression. However, it is unclear how precisely the generated images
express contexts and emotions from the input texts. We explored the emotional
expressiveness of AI-generated images and developed RePrompt, an automatic
method to refine text prompts toward precise expression of the generated
images. Inspired by crowdsourced editing strategies, we curated intuitive text
features, such as the number and concreteness of nouns, and trained a proxy
model to analyze the feature effects on the AI-generated image. With model
explanations of the proxy model, we curated a rubric to adjust text prompts to
optimize image generation for precise emotion expression. We conducted
simulation and user studies, which showed that RePrompt significantly improves
the emotional expressiveness of AI-generated images, especially for negative
emotions.",None,-1
508df2d3-e979-43d0-9038-6e9e56202a7c,Intent Induction from Conversations for Task-Oriented Dialogue Track at DSTC 11,0.137583,"With increasing demand for and adoption of virtual assistants, recent work
has investigated ways to accelerate bot schema design through the automatic
induction of intents or the induction of slots and dialogue states. However, a
lack of dedicated benchmarks and standardized evaluation has made progress
difficult to track and comparisons between systems difficult to make. This
challenge track, held as part of the Eleventh Dialog Systems Technology
Challenge, introduces a benchmark that aims to evaluate methods for the
automatic induction of customer intents in a realistic setting of customer
service interactions between human agents and customers. We propose two
subtasks for progressively tackling the automatic induction of intents and
corresponding evaluation methodologies. We then present three datasets suitable
for evaluating the tasks and propose simple baselines. Finally, we summarize
the submissions and results of the challenge track, for which we received
submissions from 34 teams.",None,-1
199e1f09-177f-4dc8-b061-7dce9cf75244,On Formal Feature Attribution and Its Approximation,0.644615,"Recent years have witnessed the widespread use of artificial intelligence
(AI) algorithms and machine learning (ML) models. Despite their tremendous
success, a number of vital problems like ML model brittleness, their fairness,
and the lack of interpretability warrant the need for the active developments
in explainable artificial intelligence (XAI) and formal ML model verification.
The two major lines of work in XAI include feature selection methods, e.g.
Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their
promise, most of the existing feature selection and attribution approaches are
susceptible to a range of critical issues, including explanation unsoundness
and out-of-distribution sampling. A recent formal approach to XAI (FXAI)
although serving as an alternative to the above and free of these issues
suffers from a few other limitations. For instance and besides the scalability
limitation, the formal approach is unable to tackle the feature attribution
problem. Additionally, a formal explanation despite being formally sound is
typically quite large, which hampers its applicability in practical settings.
Motivated by the above, this paper proposes a way to apply the apparatus of
formal XAI to the case of feature attribution based on formal explanation
enumeration. Formal feature attribution (FFA) is argued to be advantageous over
the existing methods, both formal and non-formal. Given the practical
complexity of the problem, the paper then proposes an efficient technique for
approximating exact FFA. Finally, it offers experimental evidence of the
effectiveness of the proposed approximate FFA in comparison to the existing
feature attribution algorithms not only in terms of feature importance and but
also in terms of their relative order.",None,-1
dfb90138-ba80-47ac-84b6-3ce6ae5a7187,"Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media",0.405171,"We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.",None,-1
757cf691-43db-4921-8fac-335899298882,Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection,0.999987,"In this paper, we propose a long-sequence modeling framework, named
StreamPETR, for multi-view 3D object detection. Built upon the sparse query
design in the PETR series, we systematically develop an object-centric temporal
mechanism. The model is performed in an online manner and the long-term
historical information is propagated through object queries frame by frame.
Besides, we introduce a motion-aware layer normalization to model the movement
of the objects. StreamPETR achieves significant performance improvements only
with negligible computation cost, compared to the single-frame baseline. On the
standard nuScenes benchmark, it is the first online multi-view method that
achieves comparable performance (67.6% NDS & 65.3% AMOTA) with lidar-based
methods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming
the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code
has been available at https://github.com/exiawsh/StreamPETR.git.",None,-1
63c8c4f8-f25d-4969-aadb-d98939251abc,VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer,0.783639,"Current talking face generation methods mainly focus on speech-lip
synchronization. However, insufficient investigation on the facial talking
style leads to a lifeless and monotonous avatar. Most previous works fail to
imitate expressive styles from arbitrary video prompts and ensure the
authenticity of the generated video. This paper proposes an unsupervised
variational style transfer model (VAST) to vivify the neutral photo-realistic
avatars. Our model consists of three key components: a style encoder that
extracts facial style representations from the given video prompts; a hybrid
facial expression decoder to model accurate speech-related movements; a
variational style enhancer that enhances the style space to be highly
expressive and meaningful. With our essential designs on facial style learning,
our model is able to flexibly capture the expressive facial style from
arbitrary video prompts and transfer it onto a personalized image renderer in a
zero-shot manner. Experimental results demonstrate the proposed approach
contributes to a more vivid talking avatar with higher authenticity and richer
expressiveness.",None,-1
1a667b77-157b-420b-bcb3-87ed5276fd1c,"3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",0.729421,"Disentangling uncorrelated information in speech utterances is a crucial
research topic within speech community. Different speech-related tasks focus on
extracting distinct speech representations while minimizing the affects of
other uncorrelated information. We present a large-scale speech corpus to
facilitate the research of speech representation disentanglement. 3D-Speaker
contains over 10,000 speakers, each of whom are simultaneously recorded by
multiple Devices, locating at different Distances, and some speakers are
speaking multiple Dialects. The controlled combinations of multi-dimensional
audio data yield a matrix of a diverse blend of speech representation
entanglement, thereby motivating intriguing methods to untangle them. The
multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate
large universal speech models and experiment methods of out-of-domain learning
and self-supervised learning. https://3dspeaker.github.io/",None,-1
01424dd7-1d6a-478c-810e-23f959c86801,Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition,0.697824,"Micro-expressions are spontaneous, rapid and subtle facial movements that can
neither be forged nor suppressed. They are very important nonverbal
communication clues, but are transient and of low intensity thus difficult to
recognize. Recently deep learning based methods have been developed for
micro-expression (ME) recognition using feature extraction and fusion
techniques, however, targeted feature learning and efficient feature fusion
still lack further study according to the ME characteristics. To address these
issues, we propose a novel framework Feature Representation Learning with
adaptive Displacement Generation and Transformer fusion (FRL-DGT), in which a
convolutional Displacement Generation Module (DGM) with self-supervised
learning is used to extract dynamic features from onset/apex frames targeted to
the subsequent ME recognition task, and a well-designed Transformer Fusion
mechanism composed of three Transformer-based fusion modules (local, global
fusions based on AU regions and full-face fusion) is applied to extract the
multi-level informative features after DGM for the final ME prediction. The
extensive experiments with solid leave-one-subject-out (LOSO) evaluation
results have demonstrated the superiority of our proposed FRL-DGT to
state-of-the-art methods.",None,-1
f692f071-2538-469f-976b-692b858348d4,Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization,0.274842,"Large Language Models (LLMs) are proficient in natural language processing
tasks, but their deployment is often restricted by extensive parameter sizes
and computational demands. This paper focuses on post-training quantization
(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)
quantization, to enhance computational efficiency -- a topic less explored
compared to weight-only quantization. We present two innovative techniques:
activation-quantization-aware scaling (AQAS) and sequence-length-aware
calibration (SLAC) to enhance PTQ by considering the combined effects on
weights and activations and aligning calibration sequence lengths to target
tasks. Moreover, we introduce dINT, a hybrid data format combining integer and
denormal representations, to address the underflow issue in W4A8 quantization,
where small values are rounded to zero. Through rigorous evaluations of LLMs,
including OPT and LLaMA, we demonstrate that our techniques significantly boost
task accuracies to levels comparable with full-precision models. By developing
arithmetic units compatible with dINT, we further confirm that our methods
yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC
unit.",None,-1
a404d560-d5e0-40ab-bdb9-9986fd34e201,Low-Light Image Enhancement by Learning Contrastive Representations in Spatial and Frequency Domains,0.606564,"Images taken under low-light conditions tend to suffer from poor visibility,
which can decrease image quality and even reduce the performance of the
downstream tasks. It is hard for a CNN-based method to learn generalized
features that can recover normal images from the ones under various unknow
low-light conditions. In this paper, we propose to incorporate the contrastive
learning into an illumination correction network to learn abstract
representations to distinguish various low-light conditions in the
representation space, with the purpose of enhancing the generalizability of the
network. Considering that light conditions can change the frequency components
of the images, the representations are learned and compared in both spatial and
frequency domains to make full advantage of the contrastive learning. The
proposed method is evaluated on LOL and LOL-V2 datasets, the results show that
the proposed method achieves better qualitative and quantitative results
compared with other state-of-the-arts.",None,-1
4f5613ee-9c7e-4f6e-a94a-3db35c0a5e4c,Learning and interpreting asymmetry-labeled DAGs: a case study on COVID-19 fear,0.497615,"Bayesian networks are widely used to learn and reason about the dependence
structure of discrete variables. However, they are only capable of formally
encoding symmetric conditional independence, which in practice is often too
strict to hold. Asymmetry-labeled DAGs have been recently proposed to both
extend the class of Bayesian networks by relaxing the symmetric assumption of
independence and denote the type of dependence existing between the variables
of interest. Here, we introduce novel structural learning algorithms for this
class of models which, whilst being efficient, allow for a straightforward
interpretation of the underlying dependence structure. A comprehensive
computational study highlights the efficiency of the algorithms. A real-world
data application using data from the Fear of COVID-19 Scale collected in Italy
showcases their use in practice.",None,-1
2c6205ec-6a0e-4bda-b596-8f80355e2bcd,Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers,0.573969,"Recent applications of LLMs in Machine Reading Comprehension (MRC) systems
have shown impressive results, but the use of shortcuts, mechanisms triggered
by features spuriously correlated to the true label, has emerged as a potential
threat to their reliability. We analyze the problem from two angles: LLMs as
editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer
questions based on the edited text. We introduce a framework that guides an
editor to add potential shortcuts-triggers to samples. Using GPT4 as the
editor, we find it can successfully edit trigger shortcut in samples that fool
LLMs. Analysing LLMs as readers, we observe that even capable LLMs can be
deceived using shortcut knowledge. Strikingly, we discover that GPT4 can be
deceived by its own edits (15% drop in F1). Our findings highlight inherent
vulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a
curated dataset generated by our framework for future research.",None,-1
b74ccc49-0c08-4448-9df1-587a3f55c740,Adaptive Sparse Pairwise Loss for Object Re-Identification,0.874729,"Object re-identification (ReID) aims to find instances with the same identity
as the given probe from a large gallery. Pairwise losses play an important role
in training a strong ReID network. Existing pairwise losses densely exploit
each instance as an anchor and sample its triplets in a mini-batch. This dense
sampling mechanism inevitably introduces positive pairs that share few visual
similarities, which can be harmful to the training. To address this problem, we
propose a novel loss paradigm termed Sparse Pairwise (SP) loss that only
leverages few appropriate pairs for each class in a mini-batch, and empirically
demonstrate that it is sufficient for the ReID tasks. Based on the proposed
loss framework, we propose an adaptive positive mining strategy that can
dynamically adapt to diverse intra-class variations. Extensive experiments show
that SP loss and its adaptive variant AdaSP loss outperform other pairwise
losses, and achieve state-of-the-art performance across several ReID
benchmarks. Code is available at https://github.com/Astaxanthin/AdaSP.",None,-1
25f23df1-469a-4b63-9bbb-a18ac54a9091,KBNet: Kernel Basis Network for Image Restoration,0.959461,"How to aggregate spatial information plays an essential role in
learning-based image restoration. Most existing CNN-based networks adopt static
convolutional kernels to encode spatial information, which cannot aggregate
spatial information adaptively. Recent transformer-based architectures achieve
adaptive spatial aggregation. But they lack desirable inductive biases of
convolutions and require heavy computational costs. In this paper, we propose a
kernel basis attention (KBA) module, which introduces learnable kernel bases to
model representative image patterns for spatial information aggregation.
Different kernel bases are trained to model different local structures. At each
spatial location, they are linearly and adaptively fused by predicted
pixel-wise coefficients to obtain aggregation weights. Based on the KBA module,
we further design a multi-axis feature fusion (MFF) block to encode and fuse
channel-wise, spatial-invariant, and pixel-adaptive features for image
restoration. Our model, named kernel basis network (KBNet), achieves
state-of-the-art performances on more than ten benchmarks over image denoising,
deraining, and deblurring tasks while requiring less computational cost than
previous SOTA methods.",None,-1
99d37568-c6fb-4c1d-b164-cce50cc18865,Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence,0.730899,"Turbulence in fluids, gases, and plasmas remains an open problem of both
practical and fundamental importance. Its irreducible complexity usually cannot
be tackled computationally in a brute-force style. Here, we combine Large Eddy
Simulation (LES) techniques with Machine Learning (ML) to retain only the
largest dynamics explicitly, while small-scale dynamics are described by an
ML-based sub-grid-scale model. Applying this novel approach to self-driven
plasma turbulence allows us to remove large parts of the inertial range,
reducing the computational effort by about three orders of magnitude, while
retaining the statistical physical properties of the turbulent system.",None,-1
0ec16f31-15e9-4109-8b88-7fc1081aaf24,TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental Implant Position Prediction,0.845083,"In implant prosthesis treatment, the surgical guide of implant is used to
ensure accurate implantation. However, such design heavily relies on the manual
location of the implant position. When deep neural network has been proposed to
assist the dentist in locating the implant position, most of them take a single
slice as input, which do not fully explore 3D contextual information and
ignoring the influence of implant slope. In this paper, we design a Text Guided
3D Context and Slope Aware Triple Network (TCSloT) which enables the perception
of contextual information from multiple adjacent slices and awareness of
variation of implant slopes. A Texture Variation Perception (TVP) module is
correspondingly elaborated to process the multiple slices and capture the
texture variation among slices and a Slope-Aware Loss (SAL) is proposed to
dynamically assign varying weights for the regression head. Additionally, we
design a conditional text guidance (CTG) module to integrate the text condition
(i.e., left, middle and right) from the CLIP for assisting the implant position
prediction. Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed TCSloT achieves superior
performance than existing methods.",None,-1
dddbd4ed-c9f5-4569-9318-f5a08fee7040,Location-Free Scene Graph Generation,0.189234,"Scene Graph Generation (SGG) is a challenging visual understanding task. It
combines the detection of entities and relationships between them in a scene.
Both previous works and existing evaluation metrics rely on bounding box
labels, even though many downstream scene graph applications do not need
location information. The need for localization labels significantly increases
the annotation cost and hampers the creation of more and larger scene graph
datasets. We suggest breaking the dependency of scene graphs on bounding box
labels by proposing location-free scene graph generation (LF-SGG). This new
task aims at predicting instances of entities, as well as their relationships,
without spatial localization. To objectively evaluate the task, the predicted
and ground truth scene graphs need to be compared. We solve this NP-hard
problem through an efficient algorithm using branching. Additionally, we design
the first LF-SGG method, Pix2SG, using autoregressive sequence modeling. Our
proposed method is evaluated on Visual Genome and 4D-OR. Although using
significantly fewer labels during training, we achieve 74.12\% of the
location-supervised SOTA performance on Visual Genome and even outperform the
best method on 4D-OR.",None,-1
e184f202-723b-4478-ba5c-4f1b694a169c,Deep Learning-based Eye-Tracking Analysis for Diagnosis of Alzheimer's Disease Using 3D Comprehensive Visual Stimuli,0.307483,"Alzheimer's Disease (AD) causes a continuous decline in memory, thinking, and
judgment. Traditional diagnoses are usually based on clinical experience, which
is limited by some realistic factors. In this paper, we focus on exploiting
deep learning techniques to diagnose AD based on eye-tracking behaviors. Visual
attention, as typical eye-tracking behavior, is of great clinical value to
detect cognitive abnormalities in AD patients. To better analyze the
differences in visual attention between AD patients and normals, we first
conduct a 3D comprehensive visual task on a non-invasive eye-tracking system to
collect visual attention heatmaps. We then propose a multi-layered comparison
convolution neural network (MC-CNN) to distinguish the visual attention
differences between AD patients and normals. In MC-CNN, the multi-layered
representations of heatmaps are obtained by hierarchical convolution to better
encode eye-movement behaviors, which are further integrated into a distance
vector to benefit the comprehensive visual task. Extensive experimental results
on the collected dataset demonstrate that MC-CNN achieves consistent validity
in classifying AD patients and normals with eye-tracking data.",None,-1
0401b320-4b18-4f19-a017-07c8b6825ec5,Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation,0.401611,"This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation.",None,-1
8adaa9e3-91d7-4819-932a-e801787e0234,Asymptotic Convergence and Performance of Multi-Agent Q-Learning Dynamics,0.806154,"Achieving convergence of multiple learning agents in general $N$-player games
is imperative for the development of safe and reliable machine learning (ML)
algorithms and their application to autonomous systems. Yet it is known that,
outside the bounds of simple two-player games, convergence cannot be taken for
granted.
  To make progress in resolving this problem, we study the dynamics of smooth
Q-Learning, a popular reinforcement learning algorithm which quantifies the
tendency for learning agents to explore their state space or exploit their
payoffs. We show a sufficient condition on the rate of exploration such that
the Q-Learning dynamics is guaranteed to converge to a unique equilibrium in
any game. We connect this result to games for which Q-Learning is known to
converge with arbitrary exploration rates, including weighted Potential games
and weighted zero sum polymatrix games.
  Finally, we examine the performance of the Q-Learning dynamic as measured by
the Time Averaged Social Welfare, and comparing this with the Social Welfare
achieved by the equilibrium. We provide a sufficient condition whereby the
Q-Learning dynamic will outperform the equilibrium even if the dynamics do not
converge.",None,-1
a620f597-cbea-4cdc-9cc5-ef40faf5edf0,Grandma Karl is 27 years old -- research agenda for pseudonymization of research data,0.825978,"Accessibility of research data is critical for advances in many research
fields, but textual data often cannot be shared due to the personal and
sensitive information which it contains, e.g names or political opinions.
General Data Protection Regulation (GDPR) suggests pseudonymization as a
solution to secure open access to research data, but we need to learn more
about pseudonymization as an approach before adopting it for manipulation of
research data. This paper outlines a research agenda within pseudonymization,
namely need of studies into the effects of pseudonymization on unstructured
data in relation to e.g. readability and language assessment, as well as the
effectiveness of pseudonymization as a way of protecting writer identity, while
also exploring different ways of developing context-sensitive algorithms for
detection, labelling and replacement of personal information in unstructured
data. The recently granted project on pseudonymization Grandma Karl is 27 years
old addresses exactly those challenges.",None,-1
898aaa8d-0371-4648-9514-b2e41db3b5ab,Parameterized Decision-making with Multi-modal Perception for Autonomous Driving,0.346105,"Autonomous driving is an emerging technology that has advanced rapidly over
the last decade. Modern transportation is expected to benefit greatly from a
wise decision-making framework of autonomous vehicles, including the
improvement of mobility and the minimization of risks and travel time. However,
existing methods either ignore the complexity of environments only fitting
straight roads, or ignore the impact on surrounding vehicles during
optimization phases, leading to weak environmental adaptability and incomplete
optimization objectives. To address these limitations, we propose a
parameterized decision-making framework with multi-modal perception based on
deep reinforcement learning, called AUTO. We conduct a comprehensive perception
to capture the state features of various traffic participants around the
autonomous vehicle, based on which we design a graph-based model to learn a
state representation of the multi-modal semantic features. To distinguish
between lane-following and lane-changing, we decompose an action of the
autonomous vehicle into a parameterized action structure that first decides
whether to change lanes and then computes an exact action to execute. A hybrid
reward function takes into account aspects of safety, traffic efficiency,
passenger comfort, and impact to guide the framework to generate optimal
actions. In addition, we design a regularization term and a multi-worker
paradigm to enhance the training. Extensive experiments offer evidence that
AUTO can advance state-of-the-art in terms of both macroscopic and microscopic
effectiveness.",None,-1
6ca7942e-bc94-4ff6-805d-431b47d8c22e,Causality Analysis for Evaluating the Security of Large Language Models,0.0470595,"Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted
in many safety-critical applications. Their security is thus essential. Even
with considerable efforts spent on reinforcement learning from human feedback
(RLHF), recent studies have shown that LLMs are still subject to attacks such
as adversarial perturbation and Trojan attacks. Further research is thus needed
to evaluate their security and/or understand the lack of it. In this work, we
propose a framework for conducting light-weight causality-analysis of LLMs at
the token, layer, and neuron level. We applied our framework to open-source
LLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based
on a layer-level causality analysis, we show that RLHF has the effect of
overfitting a model to harmful prompts. It implies that such security can be
easily overcome by `unusual' harmful prompts. As evidence, we propose an
adversarial perturbation method that achieves 100\% attack success rate on the
red-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we
show the existence of one mysterious neuron in both Llama2 and Vicuna that has
an unreasonably high causal effect on the output. While we are uncertain on why
such a neuron exists, we show that it is possible to conduct a ``Trojan''
attack targeting that particular neuron to completely cripple the LLM, i.e., we
can generate transferable suffixes to prompts that frequently make the LLM
produce meaningless responses.",None,-1
2c6bed76-03bd-4fc4-860a-b2bf7da81240,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,0.858947,"In this study, we applied the PointRend (Point-based Rendering) method to
semiconductor defect segmentation. PointRend is an iterative segmentation
algorithm inspired by image rendering in computer graphics, a new image
segmentation method that can generate high-resolution segmentation masks. It
can also be flexibly integrated into common instance segmentation
meta-architecture such as Mask-RCNN and semantic meta-architecture such as FCN.
We implemented a model, termed as SEMI-PointRend, to generate precise
segmentation masks by applying the PointRend neural network module. In this
paper, we focus on comparing the defect segmentation predictions of
SEMI-PointRend and Mask-RCNN for various defect types (line-collapse, single
bridge, thin bridge, multi bridge non-horizontal). We show that SEMI-PointRend
can outperforms Mask R-CNN by up to 18.8% in terms of segmentation mean average
precision.",None,-1
8e55b149-50b9-4927-9622-1c4130cf3021,Language Models are Few-shot Learners for Prognostic Prediction,0.967815,"Clinical prediction is an essential task in the healthcare industry. However,
the recent success of transformers, on which large language models are built,
has not been extended to this domain. In this research, we explore the use of
transformers and language models in prognostic prediction for immunotherapy
using real-world patients' clinical data and molecular profiles. This paper
investigates the potential of transformers to improve clinical prediction
compared to conventional machine learning approaches and addresses the
challenge of few-shot learning in predicting rare disease areas. The study
benchmarks the efficacy of baselines and language models on prognostic
prediction across multiple cancer types and investigates the impact of
different pretrained language models under few-shot regimes. The results
demonstrate significant improvements in accuracy and highlight the potential of
NLP in clinical research to improve early detection and intervention for
different diseases.",None,-1
568d9a74-1415-42c0-ad92-491b2d41e935,Enhancing Continual Relation Extraction via Classifier Decomposition,0.815964,"Continual relation extraction (CRE) models aim at handling emerging new
relations while avoiding catastrophically forgetting old ones in the streaming
data. Though improvements have been shown by previous CRE studies, most of them
only adopt a vanilla strategy when models first learn representations of new
relations. In this work, we point out that there exist two typical biases after
training of this vanilla strategy: classifier bias and representation bias,
which causes the previous knowledge that the model learned to be shaded. To
alleviate those biases, we propose a simple yet effective classifier
decomposition framework that splits the last FFN layer into separated previous
and current classifiers, so as to maintain previous knowledge and encourage the
model to learn more robust representations at this training stage. Experimental
results on two standard benchmarks show that our proposed framework
consistently outperforms the state-of-the-art CRE models, which indicates that
the importance of the first training stage to CRE models may be underestimated.
Our code is available at https://github.com/hemingkx/CDec.",None,-1
8d42270f-4733-46f8-b381-35634817842d,Controllable Guide-Space for Generalizable Face Forgery Detection,0.813225,"Recent studies on face forgery detection have shown satisfactory performance
for methods involved in training datasets, but are not ideal enough for unknown
domains. This motivates many works to improve the generalization, but
forgery-irrelevant information, such as image background and identity, still
exists in different domain features and causes unexpected clustering, limiting
the generalization. In this paper, we propose a controllable guide-space (GS)
method to enhance the discrimination of different forgery domains, so as to
increase the forgery relevance of features and thereby improve the
generalization. The well-designed guide-space can simultaneously achieve both
the proper separation of forgery domains and the large distance between
real-forgery domains in an explicit and controllable manner. Moreover, for
better discrimination, we use a decoupling module to weaken the interference of
forgery-irrelevant correlations between domains. Furthermore, we make
adjustments to the decision boundary manifold according to the clustering
degree of the same domain features within the neighborhood. Extensive
experiments in multiple in-domain and cross-domain settings confirm that our
method can achieve state-of-the-art generalization.",None,-1
5ca1d7cf-5106-478c-80c9-3227d07b926d,Event Causality Extraction with Event Argument Correlations,0.720348,"Event Causality Identification (ECI), which aims to detect whether a
causality relation exists between two given textual events, is an important
task for event causality understanding. However, the ECI task ignores crucial
event structure and cause-effect causality component information, making it
struggle for downstream applications. In this paper, we explore a novel task,
namely Event Causality Extraction (ECE), aiming to extract the cause-effect
event causality pairs with their structured event information from plain texts.
The ECE task is more challenging since each event can contain multiple event
arguments, posing fine-grained correlations between events to decide the
causeeffect event pair. Hence, we propose a method with a dual grid tagging
scheme to capture the intra- and inter-event argument correlations for ECE.
Further, we devise a event type-enhanced model architecture to realize the dual
grid tagging scheme. Experiments demonstrate the effectiveness of our method,
and extensive analyses point out several future directions for ECE.",None,-1
c3e69851-6663-480f-94f1-263ff0d0320f,Towards Multifaceted Human-Centered AI,0.05461,"Human-centered AI workflows involve stakeholders with multiple roles
interacting with each other and automated agents to accomplish diverse tasks.
In this paper, we call for a holistic view when designing support mechanisms,
such as interaction paradigms, interfaces, and systems, for these multifaceted
workflows.",None,-1
dd17f918-26ad-4fd1-8ac5-045ef339d1ac,Writing Assistants Should Model Social Factors of Language,0.113538,"Intelligent writing assistants powered by large language models (LLMs) are
more popular today than ever before, but their further widespread adoption is
precluded by sub-optimal performance. In this position paper, we argue that a
major reason for this sub-optimal performance and adoption is a singular focus
on the information content of language while ignoring its social aspects. We
analyze the different dimensions of these social factors in the context of
writing assistants and propose their incorporation into building smarter, more
effective, and truly personalized writing assistants that would enrich the user
experience and contribute to increased user adoption.",None,-1
1c34f595-ad04-4813-9d36-becda5afe216,SummIt: Iterative Text Summarization via ChatGPT,0.954572,"Text summarization systems have made significant progress in recent years,
but typically generate summaries in one single step. However, the one-shot
summarization setting is sometimes inadequate, as the generated summary may
contain hallucinations or overlook essential details related to the reader's
interests. This paper addresses this limitation by proposing SummIt, an
iterative text summarization framework based on large language models like
ChatGPT. Our framework enables the model to refine the generated summary
iteratively through self-evaluation and feedback, resembling humans' iterative
process when drafting and revising summaries. Furthermore, we explore the
potential benefits of integrating knowledge and topic extractors into the
framework to enhance summary faithfulness and controllability. We automatically
evaluate the performance of our framework on three benchmark summarization
datasets. We also conduct a human evaluation to validate the effectiveness of
the iterative refinements and identify a potential issue of over-correction.",None,-1
7164a418-afde-4db7-9b7c-b042dd4d6b66,Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning,0.660441,"The offline reinforcement learning (RL) paradigm provides a general recipe to
convert static behavior datasets into policies that can perform better than the
policy that collected the data. While policy constraints, conservatism, and
other methods for mitigating distributional shifts have made offline
reinforcement learning more effective, the continuous action setting often
necessitates various approximations for applying these techniques. Many of
these challenges are greatly alleviated in discrete action settings, where
offline RL constraints and regularizers can often be computed more precisely or
even exactly. In this paper, we propose an adaptive scheme for action
quantization. We use a VQ-VAE to learn state-conditioned action quantization,
avoiding the exponential blowup that comes with na\""ive discretization of the
action space. We show that several state-of-the-art offline RL methods such as
IQL, CQL, and BRAC improve in performance on benchmarks when combined with our
proposed discretization scheme. We further validate our approach on a set of
challenging long-horizon complex robotic manipulation tasks in the Robomimic
environment, where our discretized offline RL algorithms are able to improve
upon their continuous counterparts by 2-3x. Our project page is at
https://saqrl.github.io/",None,-1
79a1a327-edc4-494a-8a0f-edeba964bb7e,Boosting Video Object Segmentation via Space-time Correspondence Learning,0.66963,"Current top-leading solutions for video object segmentation (VOS) typically
follow a matching-based regime: for each query frame, the segmentation mask is
inferred according to its correspondence to previously processed and the first
annotated frames. They simply exploit the supervisory signals from the
groundtruth masks for learning mask prediction only, without posing any
constraint on the space-time correspondence matching, which, however, is the
fundamental building block of such regime. To alleviate this crucial yet
commonly ignored issue, we devise a correspondence-aware training framework,
which boosts matching-based VOS solutions by explicitly encouraging robust
correspondence matching during network learning. Through comprehensively
exploring the intrinsic coherence in videos on pixel and object levels, our
algorithm reinforces the standard, fully supervised training of mask
segmentation with label-free, contrastive correspondence learning. Without
neither requiring extra annotation cost during training, nor causing speed
delay during deployment, nor incurring architectural modification, our
algorithm provides solid performance gains on four widely used benchmarks,
i.e., DAVIS2016&2017, and YouTube-VOS2018&2019, on the top of famous
matching-based VOS solutions.",None,-1
2bd2ddc6-1c65-4f09-8fe3-b628f5cd1a28,Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning,0.697368,"Large language models (LLMs) have shown impressive performance in following
natural language instructions to solve unseen tasks. However, it remains
unclear whether models truly understand task definitions and whether the
human-written definitions are optimal. In this paper, we systematically study
the role of task definitions in instruction learning. We first conduct an
ablation analysis informed by human annotations to understand which parts of a
task definition are most important, and find that model performance only drops
substantially when removing contents describing the task output, in particular
label information. Next, we propose an automatic algorithm to compress task
definitions to a minimal supporting set of tokens, and find that 60\% of tokens
can be removed while maintaining or even improving model performance. Based on
these results, we propose two strategies to help models better leverage task
instructions: (1) providing only key information for tasks in a common
structured format, and (2) adding a meta-tuning stage to help the model better
understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L
improvement over 119 unseen test tasks.",None,-1
16702568-1e4a-4402-abee-bb9ef602e13b,"BEVERS: A General, Simple, and Performant Framework for Automatic Fact Verification",0.494342,"Automatic fact verification has become an increasingly popular topic in
recent years and among datasets the Fact Extraction and VERification (FEVER)
dataset is one of the most popular. In this work we present BEVERS, a tuned
baseline system for the FEVER dataset. Our pipeline uses standard approaches
for document retrieval, sentence selection, and final claim classification,
however, we spend considerable effort ensuring optimal performance for each
component. The results are that BEVERS achieves the highest FEVER score and
label accuracy among all systems, published or unpublished. We also apply this
pipeline to another fact verification dataset, Scifact, and achieve the highest
label accuracy among all systems on that dataset as well. We also make our full
code available.",None,-1
a4df94d3-e160-4443-94ec-ed20a1feba45,Toward Stronger Textual Attack Detectors,0.596943,"The landscape of available textual adversarial attacks keeps growing, posing
severe threats and raising concerns regarding the deep NLP system's integrity.
However, the crucial problem of defending against malicious attacks has only
drawn the attention of the NLP community. The latter is nonetheless
instrumental in developing robust and trustworthy systems. This paper makes two
important contributions in this line of search: (i) we introduce LAROUSSE, a
new framework to detect textual adversarial attacks and (ii) we introduce
STAKEOUT, a new benchmark composed of nine popular attack methods, three
datasets, and two pre-trained models. LAROUSSE is ready-to-use in production as
it is unsupervised, hyperparameter-free, and non-differentiable, protecting it
against gradient-based methods. Our new benchmark STAKEOUT allows for a robust
evaluation framework: we conduct extensive numerical experiments which
demonstrate that LAROUSSE outperforms previous methods, and which allows to
identify interesting factors of detection rate variations.",None,-1
eab5c9c2-102d-4866-8704-95dd295f63bb,Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts,0.517645,"Transformer-based pre-trained models have achieved great improvements in
semantic matching. However, existing models still suffer from insufficient
ability to capture subtle differences. The modification, addition and deletion
of words in sentence pairs may make it difficult for the model to predict their
relationship. To alleviate this problem, we propose a novel Dual Path Modeling
Framework to enhance the model's ability to perceive subtle differences in
sentence pairs by separately modeling affinity and difference semantics. Based
on dual-path modeling framework we design the Dual Path Modeling Network
(DPM-Net) to recognize semantic relations. And we conduct extensive experiments
on 10 well-studied semantic matching and robustness test datasets, and the
experimental results show that our proposed method achieves consistent
improvements over baselines.",None,-1
0a0f2815-f4db-4f47-83fe-d2dfdde235f8,Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,0.594047,"Recent model editing techniques promise to mitigate the problem of memorizing
false or outdated associations during LLM training. However, we show that these
techniques can introduce large unwanted side effects which are not detected by
existing specificity benchmarks. We extend the existing CounterFact benchmark
to include a dynamic component and dub our benchmark CounterFact+.
Additionally, we extend the metrics used for measuring specificity by a
principled KL divergence-based metric. We use this improved benchmark to
evaluate recent model editing techniques and find that they suffer from low
specificity. Our findings highlight the need for improved specificity
benchmarks that identify and prevent unwanted side effects.",None,-1
18a6502d-1ed7-49f9-b278-a1d49a28c08d,Updated Corpora and Benchmarks for Long-Form Speech Recognition,0.466743,"The vast majority of ASR research uses corpora in which both the training and
test data have been pre-segmented into utterances. In most real-word ASR
use-cases, however, test audio is not segmented, leading to a mismatch between
inference-time conditions and models trained on segmented utterances. In this
paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and
VoxPopuli-en - with updated transcription and alignments to enable their use
for long-form ASR research. We use these reconstituted corpora to study the
train-test mismatch problem for transducers and attention-based
encoder-decoders (AEDs), confirming that AEDs are more susceptible to this
issue. Finally, we benchmark a simple long-form training for these models,
showing its efficacy for model robustness under this domain shift.",None,-1
03589907-fc8b-426e-a512-c22048703429,Backpack Language Models,0.840257,"We present Backpacks: a new neural architecture that marries strong modeling
performance with an interface for interpretability and control. Backpacks learn
multiple non-contextual sense vectors for each word in a vocabulary, and
represent a word in a sequence as a context-dependent, non-negative linear
combination of sense vectors in this sequence. We find that, after training,
sense vectors specialize, each encoding a different aspect of a word. We can
interpret a sense vector by inspecting its (non-contextual, linear) projection
onto the output space, and intervene on these interpretable hooks to change the
model's behavior in predictable ways. We train a 170M-parameter Backpack
language model on OpenWebText, matching the loss of a GPT-2 small
(124Mparameter) Transformer. On lexical similarity evaluations, we find that
Backpack sense vectors outperform even a 6B-parameter Transformer LM's word
embeddings. Finally, we present simple algorithms that intervene on sense
vectors to perform controllable text generation and debiasing. For example, we
can edit the sense vocabulary to tend more towards a topic, or localize a
source of gender bias to a sense vector and globally suppress that sense.",None,-1
35eebdef-5452-46ee-87cc-58ce74ab35d4,Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations,0.996819,"Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations in a supervised manner. SACL applies
contrast-aware adversarial training to generate worst-case samples and uses
joint class-spread contrastive learning to extract structured representations.
It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training (CAT) strategy to learn more diverse features from context and enhance
the model's context robustness. Under the framework with CAT, we develop a
sequence-based SACL-LSTM to learn label-consistent and context-robust features
for ERC. Experiments on three datasets show that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of SACL and CAT.",None,-1
83c1b25c-f397-42f5-a3b2-bad4f56811cf,Rigorously Assessing Natural Language Explanations of Neurons,0.840006,"Natural language is an appealing medium for explaining how large language
models process and store information, but evaluating the faithfulness of such
explanations is challenging. To help address this, we develop two modes of
evaluation for natural language explanations that claim individual neurons
represent a concept in a text input. In the observational mode, we evaluate
claims that a neuron $a$ activates on all and only input strings that refer to
a concept picked out by the proposed explanation $E$. In the intervention mode,
we construe $E$ as a claim that the neuron $a$ is a causal mediator of the
concept denoted by $E$. We apply our framework to the GPT-4-generated
explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the
most confident explanations have high error rates and little to no causal
efficacy. We close the paper by critically assessing whether natural language
is a good choice for explanations and whether neurons are the best level of
analysis.",None,-1
5abc0e56-ad25-49b6-9f5f-0c8e9879b55e,"Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",0.505754,"Agent-based modeling has been around for decades, and applied widely across
the social and natural sciences. The scope of this research method is now
poised to grow dramatically as it absorbs the new affordances provided by Large
Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just
classic Agent-Based Models (ABM)s where the agents talk to one another. Rather,
GABMs are constructed using an LLM to apply common sense to situations, act
""reasonably"", recall common semantic knowledge, produce API calls to control
digital technologies like apps, and communicate both within the simulation and
to researchers viewing it from the outside. Here we present Concordia, a
library to facilitate constructing and working with GABMs. Concordia makes it
easy to construct language-mediated simulations of physically- or
digitally-grounded environments. Concordia agents produce their behavior using
a flexible component system which mediates between two fundamental operations:
LLM calls and associative memory retrieval. A special agent called the Game
Master (GM), which was inspired by tabletop role-playing games, is responsible
for simulating the environment where the agents interact. Agents take actions
by describing what they want to do in natural language. The GM then translates
their actions into appropriate implementations. In a simulated physical world,
the GM checks the physical plausibility of agent actions and describes their
effects. In digital environments simulating technologies such as apps and
services, the GM may handle API calls to integrate with external tools such as
general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,
Email, Search, etc.). Concordia was designed to support a wide array of
applications both in scientific research and for evaluating performance of real
digital services by simulating users and/or generating synthetic data.",None,-1
555f4b64-52fb-4565-99bd-e4f4926c6484,The Change You Want to See (Now in 3D),0.322405,"The goal of this paper is to detect what has changed, if anything, between
two ""in the wild"" images of the same 3D scene acquired from different camera
positions and at different temporal instances. The open-set nature of this
problem, occlusions/dis-occlusions due to the shift in viewpoint, and the lack
of suitable training datasets, presents substantial challenges in devising a
solution.
  To address this problem, we contribute a change detection model that is
trained entirely on synthetic data and is class-agnostic, yet it is performant
out-of-the-box on real world images without requiring fine-tuning. Our solution
entails a ""register and difference"" approach that leverages self-supervised
frozen embeddings and feature differences, which allows the model to generalise
to a wide variety of scenes and domains. The model is able to operate directly
on two RGB images, without requiring access to ground truth camera intrinsics,
extrinsics, depth maps, point clouds, or additional before-after images.
Finally, we collect and release a new evaluation dataset consisting of
real-world image pairs with human-annotated differences and demonstrate the
efficacy of our method. The code, datasets and pre-trained model can be found
at: https://github.com/ragavsachdeva/CYWS-3D",None,-1
7dfe3f22-6e3d-4f65-8a2d-618358076405,Sequential Integrated Gradients: a simple but effective method for explaining language models,0.445952,"Several explanation methods such as Integrated Gradients (IG) can be
characterised as path-based methods, as they rely on a straight line between
the data and an uninformative baseline. However, when applied to language
models, these methods produce a path for each word of a sentence
simultaneously, which could lead to creating sentences from interpolated words
either having no clear meaning, or having a significantly different meaning
compared to the original sentence. In order to keep the meaning of these
sentences as close as possible to the original one, we propose Sequential
Integrated Gradients (SIG), which computes the importance of each word in a
sentence by keeping fixed every other words, only creating interpolations
between the baseline and the word of interest. Moreover, inspired by the
training procedure of several language models, we also propose to replace the
baseline token ""pad"" with the trained token ""mask"". While being a simple
improvement over the original IG method, we show on various models and datasets
that SIG proves to be a very effective method for explaining language models.",None,-1
dd3347c0-6efe-497f-9431-b95cc76d982c,CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages,0.161027,"We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of
the South Slavic languages, which is based on the Stanza natural language
processing pipeline. We describe the main improvements in CLASSLA-Stanza with
respect to Stanza, and give a detailed description of the model training
process for the latest 2.1 release of the pipeline. We also report performance
scores produced by the pipeline for different languages and varieties.
CLASSLA-Stanza exhibits consistently high performance across all the supported
languages and outperforms or expands its parent pipeline Stanza at all the
supported tasks. We also present the pipeline's new functionality enabling
efficient processing of web data and the reasons that led to its
implementation.",None,-1
9577dde8-d889-4920-aef2-72af8b2addf0,Sampling to Distill: Knowledge Transfer from Open-World Data,0.601225,"Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train
high-performance student models using only the teacher network without original
training data. Despite encouraging results, existing DFKD methods rely heavily
on generation modules with high computational costs. Meanwhile, they ignore the
fact that the generated and original data exist domain shifts due to the lack
of supervision information. Moreover, knowledge is transferred through each
example, ignoring the implicit relationship among multiple examples. To this
end, we propose a novel Open-world Data Sampling Distillation (ODSD) method
without a redundant generation process. First, we try to sample open-world data
close to the original data's distribution by an adaptive sampling module. Then,
we introduce a low-noise representation to alleviate the domain shifts and
build a structured relationship of multiple data examples to exploit data
knowledge. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet
show that our ODSD method achieves state-of-the-art performance. Especially, we
improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with the
existing results.",None,-1
1e06cd41-ed2b-449f-a69c-3f57e058547e,Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and Model Lineage Analysis,0.721556,"The generation of high-quality images has become widely accessible and is a
rapidly evolving process. As a result, anyone can generate images that are
indistinguishable from real ones. This leads to a wide range of applications,
including malicious usage with deceptive intentions. Despite advances in
detection techniques for generated images, a robust detection method still
eludes us. Furthermore, model personalization techniques might affect the
detection capabilities of existing methods. In this work, we utilize the
architectural properties of convolutional neural networks (CNNs) to develop a
new detection method. Our method can detect images from a known generative
model and enable us to establish relationships between fine-tuned generative
models. We tested the method on images produced by both Generative Adversarial
Networks (GANs) and recent large text-to-image models (LTIMs) that rely on
Diffusion Models. Our approach outperforms others trained under identical
conditions and achieves comparable performance to state-of-the-art pre-trained
detection methods on images generated by Stable Diffusion and MidJourney, with
significantly fewer required train samples.",None,-1
c95ae8b5-9ada-4a41-9792-67469e5bccd6,Semi-supervised Multimodal Representation Learning through a Global Workspace,0.192352,"Recent deep learning models can efficiently combine inputs from different
modalities (e.g., images and text) and learn to align their latent
representations, or to translate signals from one domain to another (as in
image captioning, or text-to-image generation). However, current approaches
mainly rely on brute-force supervised training over large multimodal datasets.
In contrast, humans (and other animals) can learn useful multimodal
representations from only sparse experience with matched cross-modal data. Here
we evaluate the capabilities of a neural network architecture inspired by the
cognitive notion of a ""Global Workspace"": a shared representation for two (or
more) input modalities. Each modality is processed by a specialized system
(pretrained on unimodal data, and subsequently frozen). The corresponding
latent representations are then encoded to and decoded from a single shared
workspace. Importantly, this architecture is amenable to self-supervised
training via cycle-consistency: encoding-decoding sequences should approximate
the identity function. For various pairings of vision-language modalities and
across two datasets of varying complexity, we show that such an architecture
can be trained to align and translate between two modalities with very little
need for matched data (from 4 to 7 times less than a fully supervised
approach). The global workspace representation can be used advantageously for
downstream classification tasks and for robust transfer learning. Ablation
studies reveal that both the shared workspace and the self-supervised
cycle-consistency training are critical to the system's performance.",None,-1
0778462b-3779-4011-a580-b648fbde1232,MoStGAN-V: Video Generation with Temporal Motion Styles,0.700254,"Video generation remains a challenging task due to spatiotemporal complexity
and the requirement of synthesizing diverse motions with temporal consistency.
Previous works attempt to generate videos in arbitrary lengths either in an
autoregressive manner or regarding time as a continuous signal. However, they
struggle to synthesize detailed and diverse motions with temporal coherence and
tend to generate repetitive scenes after a few time steps. In this work, we
argue that a single time-agnostic latent vector of style-based generator is
insufficient to model various and temporally-consistent motions. Hence, we
introduce additional time-dependent motion styles to model diverse motion
patterns. In addition, a Motion Style Attention modulation mechanism, dubbed as
MoStAtt, is proposed to augment frames with vivid dynamics for each specific
scale (i.e., layer), which assigns attention score for each motion style w.r.t
deconvolution filter weights in the target synthesis layer and softly attends
different motion styles for weight modulation. Experimental results show our
model achieves state-of-the-art performance on four unconditional $256^2$ video
synthesis benchmarks trained with only 3 frames per clip and produces better
qualitative results with respect to dynamic motions. Code and videos have been
made available at https://github.com/xiaoqian-shen/MoStGAN-V.",None,-1
068285f9-b606-4251-841e-37d0b924b6f0,Towards Multi-Layered 3D Garments Animation,0.491815,"Mimicking realistic dynamics in 3D garment animations is a challenging task
due to the complex nature of multi-layered garments and the variety of outer
forces involved. Existing approaches mostly focus on single-layered garments
driven by only human bodies and struggle to handle general scenarios. In this
paper, we propose a novel data-driven method, called LayersNet, to model
garment-level animations as particle-wise interactions in a micro physics
system. We improve simulation efficiency by representing garments as
patch-level particles in a two-level structural hierarchy. Moreover, we
introduce a novel Rotation Equivalent Transformation that leverages the
rotation invariance and additivity of physics systems to better model outer
forces. To verify the effectiveness of our approach and bridge the gap between
experimental environments and real-world scenarios, we introduce a new
challenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900
different combinations of multi-layered garments driven by both human bodies
and randomly sampled wind. Our experiments show that LayersNet achieves
superior performance both quantitatively and qualitatively. We will make the
dataset and code publicly available at
https://mmlab-ntu.github.io/project/layersnet/index.html .",None,-1
408d331f-aaf3-4bec-bd1f-6a45468c89ed,Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach,0.863341,"The remarkable progress in Large Language Models (LLMs) opens up new avenues
for addressing planning and decision-making problems in Multi-Agent Systems
(MAS). However, as the number of agents increases, the issues of hallucination
in LLMs and coordination in MAS have become increasingly prominent.
Additionally, the efficient utilization of tokens emerges as a critical
consideration when employing LLMs to facilitate the interactions among a
substantial number of agents. In this paper, we develop a modular framework
called LLaMAC to mitigate these challenges. LLaMAC implements a value
distribution encoding similar to that found in the human brain, utilizing
internal and external feedback mechanisms to facilitate collaboration and
iterative reasoning among its modules. Through evaluations involving system
resource allocation and robot grid transportation, we demonstrate the
considerable advantages afforded by our proposed approach.",None,-1
ee892d05-9056-42b5-ae86-6483a056357c,Fair Multi-Exit Framework for Facial Attribute Classification,0.345673,"Fairness has become increasingly pivotal in facial recognition. Without bias
mitigation, deploying unfair AI would harm the interest of the underprivileged
population. In this paper, we observe that though the higher accuracy that
features from the deeper layer of a neural networks generally offer, fairness
conditions deteriorate as we extract features from deeper layers. This
phenomenon motivates us to extend the concept of multi-exit framework. Unlike
existing works mainly focusing on accuracy, our multi-exit framework is
fairness-oriented, where the internal classifiers are trained to be more
accurate and fairer. During inference, any instance with high confidence from
an internal classifier is allowed to exit early. Moreover, our framework can be
applied to most existing fairness-aware frameworks. Experiment results show
that the proposed framework can largely improve the fairness condition over the
state-of-the-art in CelebA and UTK Face datasets.",None,-1
c2ef6b54-ac3b-4cb9-b89e-ef2185a32f66,Affect Recognition in Conversations Using Large Language Models,0.177109,"Affect recognition, encompassing emotions, moods, and feelings, plays a
pivotal role in human communication. In the realm of conversational artificial
intelligence (AI), the ability to discern and respond to human affective cues
is a critical factor for creating engaging and empathetic interactions. This
study delves into the capacity of large language models (LLMs) to recognise
human affect in conversations, with a focus on both open-domain chit-chat
dialogues and task-oriented dialogues. Leveraging three diverse datasets,
namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from
casual conversations to clinical interviews, we evaluated and compared LLMs'
performance in affect recognition. Our investigation explores the zero-shot and
few-shot capabilities of LLMs through in-context learning (ICL) as well as
their model capacities through task-specific fine-tuning. Additionally, this
study takes into account the potential impact of automatic speech recognition
(ASR) errors on LLM predictions. With this work, we aim to shed light on the
extent to which LLMs can replicate human-like affect recognition capabilities
in conversations.",None,-1
c5d98113-9008-418e-9b99-dcdb36f558db,Effective Real Image Editing with Accelerated Iterative Diffusion Inversion,0.737315,"Despite all recent progress, it is still challenging to edit and manipulate
natural images with modern generative models. When using Generative Adversarial
Network (GAN), one major hurdle is in the inversion process mapping a real
image to its corresponding noise vector in the latent space, since its
necessary to be able to reconstruct an image to edit its contents. Likewise for
Denoising Diffusion Implicit Models (DDIM), the linearization assumption in
each inversion step makes the whole deterministic inversion process unreliable.
Existing approaches that have tackled the problem of inversion stability often
incur in significant trade-offs in computational efficiency. In this work we
propose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that
significantly improves reconstruction accuracy with minimal additional overhead
in space and time complexity. By using a novel blended guidance technique, we
show that effective results can be obtained on a large range of image editing
tasks without large classifier-free guidance in inversion. Furthermore, when
compared with other diffusion inversion based works, our proposed process is
shown to be more robust for fast image editing in the 10 and 20 diffusion
steps' regimes.",None,-1
07c0ed2c-2443-4585-80a7-8ea0d628fc31,Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation,0.682554,"The CLIP model has been recently proven to be very effective for a variety of
cross-modal tasks, including the evaluation of captions generated from
vision-and-language architectures. In this paper, we propose a new recipe for a
contrastive-based evaluation metric for image captioning, namely
Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way
unifies the learning of a contrastive visual-semantic space with the addition
of generated images and text on curated data. Experiments spanning several
datasets demonstrate that our new metric achieves the highest correlation with
human judgments on both images and videos, outperforming existing
reference-based metrics like CIDEr and SPICE and reference-free metrics like
CLIP-Score. Finally, we test the system-level correlation of the proposed
metric when considering popular image captioning approaches, and assess the
impact of employing different cross-modal features. Our source code and trained
models are publicly available at: https://github.com/aimagelab/pacscore.",None,-1
686f9626-2866-4631-89e2-03675ab9be0a,"KBody: Towards general, robust, and aligned monocular whole-body estimation",0.387632,"KBody is a method for fitting a low-dimensional body model to an image. It
follows a predict-and-optimize approach, relying on data-driven model estimates
for the constraints that will be used to solve for the body's parameters.
Acknowledging the importance of high quality correspondences, it leverages
``virtual joints"" to improve fitting performance, disentangles the optimization
between the pose and shape parameters, and integrates asymmetric distance
fields to strike a balance in terms of pose and shape capturing capacity, as
well as pixel alignment. We also show that generative model inversion offers a
strong appearance prior that can be used to complete partial human images and
used as a building block for generalized and robust monocular body fitting.
Project page: https://zokin.github.io/KBody.",None,-1
fc800a40-2a65-4a4b-8468-1caa53a2dcf6,Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling,0.713495,"The emergence of on-demand ride pooling services allows each vehicle to serve
multiple passengers at a time, thus increasing drivers' income and enabling
passengers to travel at lower prices than taxi/car on-demand services (only one
passenger can be assigned to a car at a time like UberX and Lyft). Although
on-demand ride pooling services can bring so many benefits, ride pooling
services need a well-defined matching strategy to maximize the benefits for all
parties (passengers, drivers, aggregation companies and environment), in which
the regional dispatching of vehicles has a significant impact on the matching
and revenue. Existing algorithms often only consider revenue maximization,
which makes it difficult for requests with unusual distribution to get a ride.
How to increase revenue while ensuring a reasonable assignment of requests
brings a challenge to ride pooling service companies (aggregation companies).
In this paper, we propose a framework for vehicle dispatching for ride pooling
tasks, which splits the city into discrete dispatching regions and uses the
reinforcement learning (RL) algorithm to dispatch vehicles in these regions. We
also consider the mutual information (MI) between vehicle and order
distribution as the intrinsic reward of the RL algorithm to improve the
correlation between their distributions, thus ensuring the possibility of
getting a ride for unusually distributed requests. In experimental results on a
real-world taxi dataset, we demonstrate that our framework can significantly
increase revenue up to an average of 3\% over the existing best on-demand ride
pooling method.",None,-1
653317ee-54d3-4107-aa04-2a35583fbcbd,ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought,0.857491,"Recently Large Language Models (LLMs) have been proven to have strong
abilities in various domains and tasks. We study the problem of prompt
designing in the text-to-SQL task and attempt to improve the LLMs' reasoning
ability when generating SQL queries. Besides the trivial few-shot in-context
learning setting, we design our chain-of-thought (CoT) prompt with a similar
method to schema linking. We provide a method named ACT-SQL to automatically
generate auto-CoT exemplars and thus the whole process doesn't need manual
labeling. Our approach is cost-saving since we only use the LLMs' API call once
when generating one SQL query. Furthermore, we extend our in-context learning
method to the multi-turn text-to-SQL task. The experiment results show that the
LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves
SOTA performance on the Spider dev set among existing in-context learning
approaches.",None,-1
c71a6e2d-17bc-4cbb-b765-cbad7492f368,Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions,0.999704,"The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves
participants finding text strings or ``flags'' by exploiting system
vulnerabilities. Large Language Models (LLMs) are natural-language models
trained on vast amounts of words to understand and generate text; they can
perform well on many CTF challenges. Such LLMs are freely available to
students. In the context of CTF exercises in the classroom, this raises
concerns about academic integrity. Educators must understand LLMs' capabilities
to modify their teaching to accommodate generative AI assistance. This research
investigates the effectiveness of LLMs, particularly in the realm of CTF
challenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,
Google Bard, and Microsoft Bing. First, we assess the LLMs' question-answering
performance on five Cisco certifications with varying difficulty levels. Next,
we qualitatively study the LLMs' abilities in solving CTF challenges to
understand their limitations. We report on the experience of using the LLMs for
seven test cases in all five types of CTF challenges. In addition, we
demonstrate how jailbreak prompts can bypass and break LLMs' ethical
safeguards. The paper concludes by discussing LLM's impact on CTF exercises and
its implications.",None,-1
62e63047-1773-43a8-967f-206eec444352,Can Generative Large Language Models Perform ASR Error Correction?,0.925919,"ASR error correction is an interesting option for post processing speech
recognition system outputs. These error correction models are usually trained
in a supervised fashion using the decoding results of a target ASR system. This
approach can be computationally intensive and the model is tuned to a specific
ASR system. Recently generative large language models (LLMs) have been applied
to a wide range of natural language processing tasks, as they can operate in a
zero-shot or few shot fashion. In this paper we investigate using ChatGPT, a
generative LLM, for ASR error correction. Based on the ASR N-best output, we
propose both unconstrained and constrained, where a member of the N-best list
is selected, approaches. Additionally, zero and 1-shot settings are evaluated.
Experiments show that this generative LLM approach can yield performance gains
for two different state-of-the-art ASR architectures, transducer and
attention-encoder-decoder based, and multiple test sets.",None,-1
2ec4e85d-3029-4b05-bac0-cf6c00a3aa2a,Syntactically Robust Training on Partially-Observed Data for Open Information Extraction,0.516583,"Open Information Extraction models have shown promising results with
sufficient supervision. However, these models face a fundamental challenge that
the syntactic distribution of training data is partially observable in
comparison to the real world. In this paper, we propose a syntactically robust
training framework that enables models to be trained on a syntactic-abundant
distribution based on diverse paraphrase generation. To tackle the intrinsic
problem of knowledge deformation of paraphrasing, two algorithms based on
semantic similarity matching and syntactic tree walking are used to restore the
expressionally transformed knowledge. The training framework can be generally
applied to other syntactic partial observable domains. Based on the proposed
framework, we build a new evaluation set called CaRB-AutoPara, a syntactically
diverse dataset consistent with the real-world setting for validating the
robustness of the models. Experiments including a thorough analysis show that
the performance of the model degrades with the increase of the difference in
syntactic distribution, while our framework gives a robust boundary. The source
code is publicly available at https://github.com/qijimrc/RobustOIE.",None,-1
d7f5668d-cbcb-4eae-87f3-915364f1ebe4,TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement,0.362689,"As demand for robotics manipulation application increases, accurate
vision-based 6D pose estimation becomes essential for autonomous operations.
Convolutional Neural Networks (CNNs) based approaches for pose estimation have
been previously introduced. However, the quest for better performance still
persists especially for accurate robotics manipulation. This quest extends to
the Agri-robotics domain. In this paper, we propose TransPose, an improved
Transformer-based 6D pose estimation with a depth refinement module. The
architecture takes in only an RGB image as input with no additional
supplementing modalities such as depth or thermal images. The architecture
encompasses an innovative lighter depth estimation network that estimates depth
from an RGB image using feature pyramid with an up-sampling method. A
transformer-based detection network with additional prediction heads is
proposed to directly regress the object's centre and predict the 6D pose of the
target. A novel depth refinement module is then used alongside the predicted
centers, 6D poses and depth patches to refine the accuracy of the estimated 6D
pose. We extensively compared our results with other state-of-the-art methods
and analysed our results for fruit-picking applications. The results we
achieved show that our proposed technique outperforms the other methods
available in the literature.",None,-1
5484dd23-d77d-425f-b78c-f7ea38ae1794,MMANet: Margin-aware Distillation and Modality-aware Regularization for Incomplete Multimodal Learning,0.33423,"Multimodal learning has shown great potentials in numerous scenes and
attracts increasing interest recently. However, it often encounters the problem
of missing modality data and thus suffers severe performance degradation in
practice. To this end, we propose a general framework called MMANet to assist
incomplete multimodal learning. It consists of three components: the deployment
network used for inference, the teacher network transferring comprehensive
multimodal information to the deployment network, and the regularization
network guiding the deployment network to balance weak modality combinations.
Specifically, we propose a novel margin-aware distillation (MAD) to assist the
information transfer by weighing the sample contribution with the
classification uncertainty. This encourages the deployment network to focus on
the samples near decision boundaries and acquire the refined inter-class
margin. Besides, we design a modality-aware regularization (MAR) algorithm to
mine the weak modality combinations and guide the regularization network to
calculate prediction loss for them. This forces the deployment network to
improve its representation ability for the weak modality combinations
adaptively. Finally, extensive experiments on multimodal classification and
segmentation tasks demonstrate that our MMANet outperforms the state-of-the-art
significantly. Code is available at: https://github.com/shicaiwei123/MMANet",None,-1
e54a412d-3c42-4ec2-b8e4-98cc033ad5f4,"Knowledge is Power, Understanding is Impact: Utility and Beyond Goals, Explanation Quality, and Fairness in Path Reasoning Recommendation",0.415414,"Path reasoning is a notable recommendation approach that models high-order
user-product relations, based on a Knowledge Graph (KG). This approach can
extract reasoning paths between recommended products and already experienced
products and, then, turn such paths into textual explanations for the user.
Unfortunately, evaluation protocols in this field appear heterogeneous and
limited, making it hard to contextualize the impact of the existing methods. In
this paper, we replicated three state-of-the-art relevant path reasoning
recommendation methods proposed in top-tier conferences. Under a common
evaluation protocol, based on two public data sets and in comparison with other
knowledge-aware methods, we then studied the extent to which they meet
recommendation utility and beyond objectives, explanation quality, and consumer
and provider fairness. Our study provides a picture of the progress in this
field, highlighting open issues and future directions. Source code:
\url{https://github.com/giacoballoccu/rep-path-reasoning-recsys}.",None,-1
1795a5fc-9759-4917-af40-c20968d164a6,Heterogeneous Forgetting Compensation for Class-Incremental Learning,0.833928,"Class-incremental learning (CIL) has achieved remarkable successes in
learning new classes consecutively while overcoming catastrophic forgetting on
old categories. However, most existing CIL methods unreasonably assume that all
old categories have the same forgetting pace, and neglect negative influence of
forgetting heterogeneity among different old classes on forgetting
compensation. To surmount the above challenges, we develop a novel
Heterogeneous Forgetting Compensation (HFC) model, which can resolve
heterogeneous forgetting of easy-to-forget and hard-to-forget old categories
from both representation and gradient aspects. Specifically, we design a
task-semantic aggregation block to alleviate heterogeneous forgetting from
representation aspect. It aggregates local category information within each
task to learn task-shared global representations. Moreover, we develop two
novel plug-and-play losses: a gradient-balanced forgetting compensation loss
and a gradient-balanced relation distillation loss to alleviate forgetting from
gradient aspect. They consider gradient-balanced compensation to rectify
forgetting heterogeneity of old categories and heterogeneous relation
consistency. Experiments on several representative datasets illustrate
effectiveness of our HFC model. The code is available at
https://github.com/JiahuaDong/HFC.",None,-1
bb997a6c-9de6-42c3-856e-357085ddd17b,Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision,0.242535,"This paper introduces a new learning-based approach for approximately solving
the Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problem
consists of, given a pool of kidney donors and patients waiting for kidney
donations, optimally selecting a set of donations to optimize the quantity and
quality of transplants performed while respecting a set of constraints about
the arrangement of these donations. The proposed technique consists of two main
steps: the first is a Graph Neural Network (GNN) trained without supervision;
the second is a deterministic non-learned search heuristic that uses the output
of the GNN to find paths and cycles. To allow for comparisons, we also
implemented and tested an exact solution method using integer programming, two
greedy search heuristics without the machine learning module, and the GNN alone
without a heuristic. We analyze and compare the methods and conclude that the
learning-based two-stage approach is the best solution quality, outputting
approximate solutions on average 1.1 times more valuable than the ones from the
deterministic heuristic alone.",None,-1
34d994c1-2347-4b6b-aec6-d0ad2f20277d,Concept Learning for Interpretable Multi-Agent Reinforcement Learning,0.724004,"Multi-agent robotic systems are increasingly operating in real-world
environments in close proximity to humans, yet are largely controlled by policy
models with inscrutable deep neural network representations. We introduce a
method for incorporating interpretable concepts from a domain expert into
models trained through multi-agent reinforcement learning, by requiring the
model to first predict such concepts then utilize them for decision making.
This allows an expert to both reason about the resulting concept policy models
in terms of these high-level concepts at run-time, as well as intervene and
correct mispredictions to improve performance. We show that this yields
improved interpretability and training stability, with benefits to policy
performance and sample efficiency in a simulated and real-world
cooperative-competitive multi-agent game.",None,-1
df204016-fb34-4425-a4c7-6d7f9ca2723e,Improving neural network representations using human similarity judgments,0.66225,"Deep neural networks have reached human-level performance on many computer
vision tasks. However, the objectives used to train these networks enforce only
that similar images are embedded at similar locations in the representation
space, and do not directly constrain the global structure of the resulting
space. Here, we explore the impact of supervising this global structure by
linearly aligning it with human similarity judgments. We find that a naive
approach leads to large changes in local representational structure that harm
downstream performance. Thus, we propose a novel method that aligns the global
structure of representations while preserving their local structure. This
global-local transform considerably improves accuracy across a variety of
few-shot learning and anomaly detection tasks. Our results indicate that human
visual representations are globally organized in a way that facilitates
learning from few examples, and incorporating this global structure into neural
network representations improves performance on downstream tasks.",None,-1
ed20b9d4-752f-4696-ad03-632f4e97c5c0,Exploring Invariant Representation for Visible-Infrared Person Re-Identification,0.738437,"Cross-spectral person re-identification, which aims to associate identities
to pedestrians across different spectra, faces a main challenge of the modality
discrepancy. In this paper, we address the problem from both image-level and
feature-level in an end-to-end hybrid learning framework named robust feature
mining network (RFM). In particular, we observe that the reflective intensity
of the same surface in photos shot in different wavelengths could be
transformed using a linear model. Besides, we show the variable linear factor
across the different surfaces is the main culprit which initiates the modality
discrepancy. We integrate such a reflection observation into an image-level
data augmentation by proposing the linear transformation generator (LTG).
Moreover, at the feature level, we introduce a cross-center loss to explore a
more compact intra-class distribution and modality-aware spatial attention to
take advantage of textured regions more efficiently. Experiment results on two
standard cross-spectral person re-identification datasets, i.e., RegDB and
SYSU-MM01, have demonstrated state-of-the-art performance.",None,-1
e872d741-a85b-4212-8a41-5e3380cb9729,Unlearnable Graph: Protecting Graphs from Unauthorized Exploitation,0.149928,"While the use of graph-structured data in various fields is becoming
increasingly popular, it also raises concerns about the potential unauthorized
exploitation of personal data for training commercial graph neural network
(GNN) models, which can compromise privacy. To address this issue, we propose a
novel method for generating unlearnable graph examples. By injecting delusive
but imperceptible noise into graphs using our Error-Minimizing Structural
Poisoning (EMinS) module, we are able to make the graphs unexploitable.
Notably, by modifying only $5\%$ at most of the potential edges in the graph
data, our method successfully decreases the accuracy from ${77.33\%}$ to
${42.47\%}$ on the COLLAB dataset.",None,-1
555d7f72-dc0a-4b78-8d4c-dcadb42c966e,Weakly Supervised 3D Instance Segmentation without Instance-level Annotations,0.208335,"3D semantic scene understanding tasks have achieved great success with the
emergence of deep learning, but often require a huge amount of manually
annotated training data. To alleviate the annotation cost, we propose the first
weakly-supervised 3D instance segmentation method that only requires
categorical semantic labels as supervision, and we do not need instance-level
labels. The required semantic annotations can be either dense or extreme sparse
(e.g. 0.02% of total points). Even without having any instance-related
ground-truth, we design an approach to break point clouds into raw fragments
and find the most confident samples for learning instance centroids.
Furthermore, we construct a recomposed dataset using pseudo instances, which is
used to learn our defined multilevel shape-aware objectness signal. An
asymmetrical object inference algorithm is followed to process core points and
boundary points with different strategies, and generate high-quality pseudo
instance labels to guide iterative training. Experiments demonstrate that our
method can achieve comparable results with recent fully supervised methods. By
generating pseudo instance labels from categorical semantic labels, our
designed approach can also assist existing methods for learning 3D instance
segmentation at reduced annotation cost.",None,-1
4c020e89-52e1-4587-bcf6-334026edce2a,VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions,0.15088,"Video-grounded dialogue understanding is a challenging problem that requires
machine to perceive, parse and reason over situated semantics extracted from
weakly aligned video and dialogues. Most existing benchmarks treat both
modalities the same as a frame-independent visual understanding task, while
neglecting the intrinsic attributes in multimodal dialogues, such as scene and
topic transitions. In this paper, we present Video-grounded Scene&Topic AwaRe
dialogue (VSTAR) dataset, a large scale video-grounded dialogue understanding
dataset based on 395 TV series. Based on VSTAR, we propose two benchmarks for
video-grounded dialogue understanding: scene segmentation and topic
segmentation, and one benchmark for video-grounded dialogue generation.
Comprehensive experiments are performed on these benchmarks to demonstrate the
importance of multimodal information and segments in video-grounded dialogue
understanding and generation.",None,-1
1c21cb8b-53fc-44ae-979d-4cc0f07c5f50,Sem-CS: Semantic CLIPStyler for Text-Based Image Style Transfer,0.0920407,"CLIPStyler demonstrated image style transfer with realistic textures using
only a style text description (instead of requiring a reference style image).
However, the ground semantics of objects in the style transfer output is lost
due to style spill-over on salient and background objects (content mismatch) or
over-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS), that
performs semantic style transfer. Sem-CS first segments the content image into
salient and non-salient objects and then transfers artistic style based on a
given style text description. The semantic style transfer is achieved using
global foreground loss (for salient objects) and global background loss (for
non-salient objects). Our empirical results, including DISTS, NIMA and user
study scores, show that our proposed framework yields superior qualitative and
quantitative performance. Our code is available at
github.com/chandagrover/sem-cs.",None,-1
cda470f7-dd54-4ac4-8132-ce1b25018c25,Motif: Intrinsic Motivation from Artificial Intelligence Feedback,0.548875,"Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.",None,-1
9bdb2bc3-ac3f-4ea9-a7dc-982adc59d0ae,A Theory of Bounded Inductive Rationality,0.811881,"The dominant theories of rational choice assume logical omniscience. That is,
they assume that when facing a decision problem, an agent can perform all
relevant computations and determine the truth value of all relevant
logical/mathematical claims. This assumption is unrealistic when, for example,
we offer bets on remote digits of pi or when an agent faces a computationally
intractable planning problem. Furthermore, the assumption of logical
omniscience creates contradictions in cases where the environment can contain
descriptions of the agent itself. Importantly, strategic interactions as
studied in game theory are decision problems in which a rational agent is
predicted by its environment (the other players). In this paper, we develop a
theory of rational decision making that does not assume logical omniscience. We
consider agents who repeatedly face decision problems (including ones like
betting on digits of pi or games against other agents). The main contribution
of this paper is to provide a sensible theory of rationality for such agents.
Roughly, we require that a boundedly rational inductive agent tests each
efficiently computable hypothesis infinitely often and follows those hypotheses
that keep their promises of high rewards. We then prove that agents that are
rational in this sense have other desirable properties. For example, they learn
to value random and pseudo-random lotteries at their expected reward. Finally,
we consider strategic interactions between different agents and prove a folk
theorem for what strategies bounded rational inductive agents can converge to.",None,-1
0aad4ff8-4f76-480c-92eb-fcbb6b931335,Learning Signed Distance Functions from Noisy 3D Point Clouds via Noise to Noise Mapping,0.475243,"Learning signed distance functions (SDFs) from 3D point clouds is an
important task in 3D computer vision. However, without ground truth signed
distances, point normals or clean point clouds, current methods still struggle
from learning SDFs from noisy point clouds. To overcome this challenge, we
propose to learn SDFs via a noise to noise mapping, which does not require any
clean point cloud or ground truth supervision for training. Our novelty lies in
the noise to noise mapping which can infer a highly accurate SDF of a single
object or scene from its multiple or even single noisy point cloud
observations. Our novel learning manner is supported by modern Lidar systems
which capture multiple noisy observations per second. We achieve this by a
novel loss which enables statistical reasoning on point clouds and maintains
geometric consistency although point clouds are irregular, unordered and have
no point correspondence among noisy observations. Our evaluation under the
widely used benchmarks demonstrates our superiority over the state-of-the-art
methods in surface reconstruction, point cloud denoising and upsampling. Our
code, data, and pre-trained models are available at
https://github.com/mabaorui/Noise2NoiseMapping/",None,-1
70753f64-def9-4184-a717-d7b1ae48cd71,To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support,0.757846,"Optimizing the phrasing of argumentative text is crucial in higher education
and professional development. However, assessing whether and how the different
claims in a text should be revised is a hard task, especially for novice
writers. In this work, we explore the main challenges to identifying
argumentative claims in need of specific revisions. By learning from
collaborative editing behaviors in online debates, we seek to capture implicit
revision patterns in order to develop approaches aimed at guiding writers in
how to further improve their arguments. We systematically compare the ability
of common word embedding models to capture the differences between different
versions of the same text, and we analyze their impact on various types of
writing issues. To deal with the noisy nature of revision-based corpora, we
propose a new sampling strategy based on revision distance. Opposed to
approaches from prior work, such sampling can be done without employing
additional annotations and judgments. Moreover, we provide evidence that using
contextual information and domain knowledge can further improve prediction
results. How useful a certain type of context is, depends on the issue the
claim is suffering from, though.",None,-1
e1578e65-8b56-4dad-b633-6c6e34f2d181,From task structures to world models: What do LLMs know?,0.513251,"In what sense does a large language model have knowledge? The answer to this
question extends beyond the capabilities of a particular AI system, and
challenges our assumptions about the nature of knowledge and intelligence. We
answer by granting LLMs ""instrumental knowledge""; knowledge defined by a
certain set of abilities. We then ask how such knowledge is related to the more
ordinary, ""worldly"" knowledge exhibited by human agents, and explore this in
terms of the degree to which instrumental knowledge can be said to incorporate
the structured world models of cognitive science. We discuss ways LLMs could
recover degrees of worldly knowledge, and suggest such recovery will be
governed by an implicit, resource-rational tradeoff between world models and
task demands.",None,-1
c8d90a0a-c8d4-400e-a9df-3a383fe3c855,The role of causality in explainable artificial intelligence,0.211293,"Causality and eXplainable Artificial Intelligence (XAI) have developed as
separate fields in computer science, even though the underlying concepts of
causation and explanation share common ancient roots. This is further enforced
by the lack of review works jointly covering these two fields. In this paper,
we investigate the literature to try to understand how and to what extent
causality and XAI are intertwined. More precisely, we seek to uncover what
kinds of relationships exist between the two concepts and how one can benefit
from them, for instance, in building trust in AI systems. As a result, three
main perspectives are identified. In the first one, the lack of causality is
seen as one of the major limitations of current AI and XAI approaches, and the
""optimal"" form of explanations is investigated. The second is a pragmatic
perspective and considers XAI as a tool to foster scientific exploration for
causal inquiry, via the identification of pursue-worthy experimental
manipulations. Finally, the third perspective supports the idea that causality
is propaedeutic to XAI in three possible manners: exploiting concepts borrowed
from causality to support or improve XAI, utilizing counterfactuals for
explainability, and considering accessing a causal model as explaining itself.
To complement our analysis, we also provide relevant software solutions used to
automate causal tasks. We believe our work provides a unified view of the two
fields of causality and XAI by highlighting potential domain bridges and
uncovering possible limitations.",None,-1
267f6654-785d-4059-8211-a303756d50a9,Semantic Feature Verification in FLAN-T5,0.35718,"This study evaluates the potential of a large language model for aiding in
generation of semantic feature norms - a critical tool for evaluating
conceptual structure in cognitive science. Building from an existing
human-generated dataset, we show that machine-verified norms capture aspects of
conceptual structure beyond what is expressed in human norms alone, and better
explain human judgments of semantic similarity amongst items that are distally
related. The results suggest that LLMs can greatly enhance traditional methods
of semantic feature norm verification, with implications for our understanding
of conceptual representation in humans and machines.",None,-1
49dc905c-24fa-4c6d-a48d-5f581c3fd35e,Causal Explanations for Sequential Decision-Making in Multi-Agent Systems,0.20624,"We present CEMA: Causal Explanations in Multi-Agent systems; a framework for
creating causal natural language explanations of an agent's decisions in
dynamic sequential multi-agent systems to build more trustworthy autonomous
agents. Unlike prior work that assumes a fixed causal structure, CEMA only
requires a probabilistic model for forward-simulating the state of the system.
Using such a model, CEMA simulates counterfactual worlds that identify the
salient causes behind the agent's decisions. We evaluate CEMA on the task of
motion planning for autonomous driving and test it in diverse simulated
scenarios. We show that CEMA correctly and robustly identifies the causes
behind the agent's decisions, even when a large number of other agents is
present, and show via a user study that CEMA's explanations have a positive
effect on participants' trust in autonomous vehicles and are rated as high as
high-quality baseline explanations elicited from other participants. We release
the collected explanations with annotations as the HEADD dataset.",None,-1
fe6060d3-324c-42df-a179-290368ddd79f,Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning,0.25845,"Large Language models (LLMs) possess the capability to engage In-context
Learning (ICL) by leveraging a few demonstrations pertaining to a new
downstream task as conditions. However, this particular learning paradigm
suffers from high instability stemming from substantial variances induced by
factors such as the input distribution of selected examples, their ordering,
and prompt formats. In this work, we demonstrate that even when all these
factors are held constant, the random selection of examples still results in
high variance. Consequently, we aim to explore the informative ability of data
examples by quantifying the Information Gain (IG) obtained in prediction after
observing a given example candidate. Then we propose to sample those with
maximum IG. Additionally, we identify the presence of template bias, which can
lead to unfair evaluations of IG during the sampling process. To mitigate this
bias, we introduce Calibration Before Sampling strategy. The experimental
results illustrate that our proposed method can yield an average relative
improvement of 14.3% across six classification tasks using three LLMs.",None,-1
b0a7c5ba-5e65-4385-905a-726c286d6960,Accented Speech Recognition With Accent-specific Codebooks,0.230901,"Speech accents pose a significant challenge to state-of-the-art automatic
speech recognition (ASR) systems. Degradation in performance across
underrepresented accents is a severe deterrent to the inclusive adoption of
ASR. In this work, we propose a novel accent adaptation approach for end-to-end
ASR systems using cross-attention with a trainable set of codebooks. These
learnable codebooks capture accent-specific information and are integrated
within the ASR encoder layers. The model is trained on accented English speech,
while the test data also contained accents which were not seen during training.
On the Mozilla Common Voice multi-accented dataset, we show that our proposed
approach yields significant performance gains not only on the seen English
accents (up to $37\%$ relative improvement in word error rate) but also on the
unseen accents (up to $5\%$ relative improvement in WER). Further, we
illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We
also compare the performance with other approaches based on accent adversarial
training.",None,-1
b86bc976-94b0-4094-aec5-42ffebc70cdc,Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning,0.411869,"Retrosynthesis consists of breaking down a chemical compound recursively
step-by-step into molecular precursors until a set of commercially available
molecules is found with the goal to provide a synthesis route. Its two primary
research directions, single-step retrosynthesis prediction, which models the
chemical reaction logic, and multi-step synthesis planning, which tries to find
the correct sequence of reactions, are inherently intertwined. Still, this
connection is not reflected in contemporary research. In this work, we combine
these two major research directions by applying multiple single-step
retrosynthesis models within multi-step synthesis planning and analyzing their
impact using public and proprietary reaction data. We find a disconnection
between high single-step performance and potential route-finding success,
suggesting that single-step models must be evaluated within synthesis planning
in the future. Furthermore, we show that the commonly used single-step
retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation
task does not represent model performance and scalability on larger and more
diverse datasets. For multi-step synthesis planning, we show that the choice of
the single-step model can improve the overall success rate of synthesis
planning by up to +28% compared to the commonly used baseline model. Finally,
we show that each single-step model finds unique synthesis routes, and differs
in aspects such as route-finding success, the number of found synthesis routes,
and chemical validity, making the combination of single-step retrosynthesis
prediction and multi-step synthesis planning a crucial aspect when developing
future methods.",None,-1
22348d2c-17a1-4c78-84a8-1ef6db3d0027,Financial News Analytics Using Fine-Tuned Llama 2 GPT Model,0.995654,"The paper considers the possibility to fine-tune Llama 2 GPT large language
model (LLM) for the multitask analysis of financial news. For fine-tuning, the
PEFT/LoRA based approach was used. In the study, the model was fine-tuned for
the following tasks: analysing a text from financial market perspectives,
highlighting main points of a text, summarizing a text and extracting named
entities with appropriate sentiments. The obtained results show that the
fine-tuned Llama 2 model can perform a multitask financial news analysis with a
specified structure of response, part of response can be a structured text and
another part of data can have JSON format for further processing. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models with quantitative target variables.",None,-1
06e8c969-e172-4ed3-ba90-22917af15654,LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation,0.956689,"Empowering chatbots in the field of mental health is receiving increasing
amount of attention, while there still lacks exploration in developing and
evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus
on exploring the potential of ChatGPT in powering chatbots for psychiatrist and
patient simulation. We collaborate with psychiatrists to identify objectives
and iteratively develop the dialogue system to closely align with real-world
scenarios. In the evaluation experiments, we recruit real psychiatrists and
patients to engage in diagnostic conversations with the chatbots, collecting
their ratings for assessment. Our findings demonstrate the feasibility of using
ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of
prompt designs on chatbot behavior and user experience.",None,-1
ecbe1b02-0d32-43ae-b1ee-645a6a161bf8,Efficient Large-scale Scene Representation with a Hybrid of High-resolution Grid and Plane Features,0.521483,"Existing neural radiance fields (NeRF) methods for large-scale scene modeling
require days of training using multiple GPUs, hindering their applications in
scenarios with limited computing resources. Despite fast optimization NeRF
variants have been proposed based on the explicit dense or hash grid features,
their effectivenesses are mainly demonstrated in object-scale scene
representation. In this paper, we point out that the low feature resolution in
explicit representation is the bottleneck for large-scale unbounded scene
representation. To address this problem, we introduce a new and efficient
hybrid feature representation for NeRF that fuses the 3D hash-grids and
high-resolution 2D dense plane features. Compared with the dense-grid
representation, the resolution of a dense 2D plane can be scaled up more
efficiently. Based on this hybrid representation, we propose a fast
optimization NeRF variant, called GP-NeRF, that achieves better rendering
results while maintaining a compact model size. Extensive experiments on
multiple large-scale unbounded scene datasets show that our model can converge
in 1.5 hours using a single GPU while achieving results comparable to or even
better than the existing method that requires about one day's training with 8
GPUs.",None,-1
3e293699-ff48-49d4-8aaf-5025f72554f0,POPGym: Benchmarking Partially Observable Reinforcement Learning,0.998967,"Real world applications of Reinforcement Learning (RL) are often partially
observable, thus requiring memory. Despite this, partial observability is still
largely ignored by contemporary RL benchmarks and libraries. We introduce
Partially Observable Process Gym (POPGym), a two-part library containing (1) a
diverse collection of 15 partially observable environments, each with multiple
difficulties and (2) implementations of 13 memory model baselines -- the most
in a single RL library. Existing partially observable benchmarks tend to fixate
on 3D visual navigation, which is computationally expensive and only one type
of POMDP. In contrast, POPGym environments are diverse, produce smaller
observations, use less memory, and often converge within two hours of training
on a consumer-grade GPU. We implement our high-level memory API and memory
baselines on top of the popular RLlib framework, providing plug-and-play
compatibility with various training algorithms, exploration strategies, and
distributed training paradigms. Using POPGym, we execute the largest comparison
across RL memory models to date. POPGym is available at
https://github.com/proroklab/popgym.",None,-1
3fdc265c-4bbe-4bb2-b094-3d6ec301058e,System Combination via Quality Estimation for Grammatical Error Correction,0.289983,"Quality estimation models have been developed to assess the corrections made
by grammatical error correction (GEC) models when the reference or
gold-standard corrections are not available. An ideal quality estimator can be
utilized to combine the outputs of multiple GEC systems by choosing the best
subset of edits from the union of all edits proposed by the GEC base systems.
However, we found that existing GEC quality estimation models are not good
enough in differentiating good corrections from bad ones, resulting in a low
F0.5 score when used for system combination. In this paper, we propose GRECO, a
new state-of-the-art quality estimation model that gives a better estimate of
the quality of a corrected sentence, as indicated by having a higher
correlation to the F0.5 score of a corrected sentence. It results in a combined
GEC system with a higher F0.5 score. We also propose three methods for
utilizing GEC quality estimation models for system combination with varying
generality: model-agnostic, model-agnostic with voting bias, and
model-dependent method. The combined GEC system outperforms the state of the
art on the CoNLL-2014 test set and the BEA-2019 test set, achieving the highest
F0.5 scores published to date.",None,-1
b139d6d8-cfa8-47f2-8484-964bf7197af4,Framework for Quality Evaluation of Smart Roadside Infrastructure Sensors for Automated Driving Applications,0.684205,"The use of smart roadside infrastructure sensors is highly relevant for
future applications of connected and automated vehicles. External sensor
technology in the form of intelligent transportation system stations (ITS-Ss)
can provide safety-critical real-time information about road users in the form
of a digital twin. The choice of sensor setups has a major influence on the
downstream function as well as the data quality. To date, there is insufficient
research on which sensor setups result in which levels of ITS-S data quality.
We present a novel approach to perform detailed quality assessment for smart
roadside infrastructure sensors. Our framework is multimodal across different
sensor types and is evaluated on the DAIR-V2X dataset. We analyze the
composition of different lidar and camera sensors and assess them in terms of
accuracy, latency, and reliability. The evaluations show that the framework can
be used reliably for several future ITS-S applications.",None,-1
df759dd6-1aa5-4f18-9dd6-b86e2feccc0d,DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles,0.434178,"We propose a new object-centric video prediction algorithm based on the deep
latent particle (DLP) representation. In comparison to existing slot- or
patch-based representations, DLPs model the scene using a set of keypoints with
learned parameters for properties such as position and size, and are both
efficient and interpretable. Our method, deep dynamic latent particles (DDLP),
yields state-of-the-art object-centric video prediction results on several
challenging datasets. The interpretable nature of DDLP allows us to perform
``what-if'' generation -- predict the consequence of changing properties of
objects in the initial frames, and DLP's compact structure enables efficient
diffusion-based unconditional video generation. Videos, code and pre-trained
models are available: https://taldatech.github.io/ddlp-web",None,-1
1c9adb41-06b9-46e7-a14a-0c1570011645,RobArch: Designing Robust Architectures against Adversarial Attacks,0.302191,"Adversarial Training is the most effective approach for improving the
robustness of Deep Neural Networks (DNNs). However, compared to the large body
of research in optimizing the adversarial training process, there are few
investigations into how architecture components affect robustness, and they
rarely constrain model capacity. Thus, it is unclear where robustness precisely
comes from. In this work, we present the first large-scale systematic study on
the robustness of DNN architecture components under fixed parameter budgets.
Through our investigation, we distill 18 actionable robust network design
guidelines that empower model developers to gain deep insights. We demonstrate
these guidelines' effectiveness by introducing the novel Robust Architecture
(RobArch) model that instantiates the guidelines to build a family of
top-performing models across parameter capacities against strong adversarial
attacks. RobArch achieves the new state-of-the-art AutoAttack accuracy on the
RobustBench ImageNet leaderboard. The code is available at
$\href{https://github.com/ShengYun-Peng/RobArch}{\text{this url}}$.",None,-1
6a437abc-4311-470f-b9d7-131d0f19ebc5,Counting Crowds in Bad Weather,0.683705,"Crowd counting has recently attracted significant attention in the field of
computer vision due to its wide applications to image understanding. Numerous
methods have been proposed and achieved state-of-the-art performance for
real-world tasks. However, existing approaches do not perform well under
adverse weather such as haze, rain, and snow since the visual appearances of
crowds in such scenes are drastically different from those images in clear
weather of typical datasets. In this paper, we propose a method for robust
crowd counting in adverse weather scenarios. Instead of using a two-stage
approach that involves image restoration and crowd counting modules, our model
learns effective features and adaptive queries to account for large appearance
variations. With these weather queries, the proposed model can learn the
weather information according to the degradation of the input image and
optimize with the crowd counting module simultaneously. Experimental results
show that the proposed algorithm is effective in counting crowds under
different weather types on benchmark datasets. The source code and trained
models will be made available to the public.",None,-1
1ab6168c-2a42-4b2e-b809-8448e9b1de4c,Systematic Assessment of Factual Knowledge in Large Language Models,0.0116175,"Previous studies have relied on existing question-answering benchmarks to
evaluate the knowledge stored in large language models (LLMs). However, this
approach has limitations regarding factual knowledge coverage, as it mostly
focuses on generic domains which may overlap with the pretraining data. This
paper proposes a framework to systematically assess the factual knowledge of
LLMs by leveraging knowledge graphs (KGs). Our framework automatically
generates a set of questions and expected answers from the facts stored in a
given KG, and then evaluates the accuracy of LLMs in answering these questions.
We systematically evaluate the state-of-the-art LLMs with KGs in generic and
specific domains. The experiment shows that ChatGPT is consistently the top
performer across all domains. We also find that LLMs performance depends on the
instruction finetuning, domain and question complexity and is prone to
adversarial context.",None,-1
a5b37004-49c0-4c47-aac6-8eaea6aadb8e,Grammar Prompting for Domain-Specific Language Generation with Large Language Models,0.484226,"Large language models (LLMs) can learn to perform a wide range of natural
language tasks from just a handful of in-context examples. However, for
generating strings from highly structured languages (e.g., semantic parsing to
complex domain-specific languages), it is challenging for the LLM to generalize
from just a few exemplars. We propose \emph{grammar prompting}, a simple
approach to enable LLMs to use external knowledge and domain-specific
constraints, expressed through a grammar in Backus--Naur Form (BNF), during
in-context learning. Grammar prompting augments each demonstration example with
a specialized grammar that is minimally sufficient for generating the
particular output example, where the specialized grammar is a subset of the
full DSL grammar. For inference, the LLM first predicts a BNF grammar given a
test input, and then generates the output according to the rules of the
grammar. Experiments demonstrate that grammar prompting can enable LLMs to
perform competitively on a diverse set of DSL generation tasks, including
semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and
SMILES-based molecule generation.",None,-1
c2fab99e-09ea-4ef2-974d-fa529a75b538,Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes,0.611931,"The BioNLP Workshop 2023 initiated the launch of a shared task on Problem
List Summarization (ProbSum) in January 2023. The aim of this shared task is to
attract future research efforts in building NLP models for real-world
diagnostic decision support applications, where a system generating relevant
and accurate diagnoses will augment the healthcare providers decision-making
process and improve the quality of care for patients. The goal for participants
is to develop models that generated a list of diagnoses and problems using
input from the daily care notes collected from the hospitalization of
critically ill patients. Eight teams submitted their final systems to the
shared task leaderboard. In this paper, we describe the tasks, datasets,
evaluation metrics, and baseline systems. Additionally, the techniques and
results of the evaluation of the different approaches tried by the
participating teams are summarized.",None,-1
7a5ea646-b7e8-471e-8579-4f49f69b261b,Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,0.735778,"Hate speech is a severe issue that affects many online platforms. So far,
several studies have been performed to develop robust hate speech detection
systems. Large language models like ChatGPT have recently shown a great promise
in performing several tasks, including hate speech detection. However, it is
crucial to comprehend the limitations of these models to build robust hate
speech detection systems. To bridge this gap, our study aims to evaluate the
strengths and weaknesses of the ChatGPT model in detecting hate speech at a
granular level across 11 languages. Our evaluation employs a series of
functionality tests that reveals various intricate failures of the model which
the aggregate metrics like macro F1 or accuracy are not able to unfold. In
addition, we investigate the influence of complex emotions, such as the use of
emojis in hate speech, on the performance of the ChatGPT model. Our analysis
highlights the shortcomings of the generative models in detecting certain types
of hate speech and highlighting the need for further research and improvements
in the workings of these models.",None,-1
26106c7d-143e-443e-9c5e-e3394a4ebe00,PrivateLoRA For Efficient Privacy Preserving LLM,0.0603557,"End users face a choice between privacy and efficiency in current Large
Language Model (LLM) service paradigms. In cloud-based paradigms, users are
forced to compromise data locality for generation quality and processing speed.
Conversely, edge device paradigms maintain data locality but fail to deliver
satisfactory performance. In this work, we propose a novel LLM service paradigm
that distributes privacy-sensitive computation on edge devices and shared
computation in the cloud. Only activations are transmitted between the central
cloud and edge devices to ensure data locality. Our core innovation,
PrivateLoRA, addresses the challenging communication overhead by exploiting the
low rank of residual activations, achieving over 95% communication reduction.
Consequently, PrivateLoRA effectively maintains data locality and is extremely
resource efficient. Under standard 5G networks, PrivateLoRA achieves throughput
over 300% of device-only solutions for 7B models and over 80% of an A100 GPU
for 33B models. PrivateLoRA also provides tuning performance comparable to LoRA
for advanced personalization. Our approach democratizes access to
state-of-the-art generative AI for edge devices, paving the way for more
tailored LLM experiences for the general public. To our knowledge, our proposed
framework is the first efficient and privacy-preserving LLM solution in the
literature.",None,-1
444e4256-e8fd-4535-a685-b2d30c5c5535,Hallucination Reduction in Long Input Text Summarization,0.311939,"Hallucination in text summarization refers to the phenomenon where the model
generates information that is not supported by the input source document.
Hallucination poses significant obstacles to the accuracy and reliability of
the generated summaries. In this paper, we aim to reduce hallucinated outputs
or hallucinations in summaries of long-form text documents. We have used the
PubMed dataset, which contains long scientific research documents and their
abstracts. We have incorporated the techniques of data filtering and joint
entity and summary generation (JAENS) in the fine-tuning of the Longformer
Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the
quality of the generated summary. We have used the following metrics to measure
factual consistency at the entity level: precision-source, and F1-target. Our
experiments show that the fine-tuned LED model performs well in generating the
paper abstract. Data filtering techniques based on some preprocessing steps
reduce entity-level hallucinations in the generated summaries in terms of some
of the factual consistency metrics.",None,-1
f47934a8-4699-4616-86c0-581ebdd5d5f6,Explainable Predictive Maintenance,0.766345,"Explainable Artificial Intelligence (XAI) fills the role of a critical
interface fostering interactions between sophisticated intelligent systems and
diverse individuals, including data scientists, domain experts, end-users, and
more. It aids in deciphering the intricate internal mechanisms of ``black box''
Machine Learning (ML), rendering the reasons behind their decisions more
understandable. However, current research in XAI primarily focuses on two
aspects; ways to facilitate user trust, or to debug and refine the ML model.
The majority of it falls short of recognising the diverse types of explanations
needed in broader contexts, as different users and varied application areas
necessitate solutions tailored to their specific needs.
  One such domain is Predictive Maintenance (PdM), an exploding area of
research under the Industry 4.0 \& 5.0 umbrella. This position paper highlights
the gap between existing XAI methodologies and the specific requirements for
explanations within industrial applications, particularly the Predictive
Maintenance field. Despite explainability's crucial role, this subject remains
a relatively under-explored area, making this paper a pioneering attempt to
bring relevant challenges to the research community's attention. We provide an
overview of predictive maintenance tasks and accentuate the need and varying
purposes for corresponding explanations. We then list and describe XAI
techniques commonly employed in the literature, discussing their suitability
for PdM tasks. Finally, to make the ideas and claims more concrete, we
demonstrate XAI applied in four specific industrial use cases: commercial
vehicles, metro trains, steel plants, and wind farms, spotlighting areas
requiring further research.",None,-1
530b661b-53d3-4453-8b15-85dc38ba196b,Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance,0.304897,"Adopting a two-stage paradigm of pretraining followed by fine-tuning,
Pretrained Language Models (PLMs) have achieved substantial advancements in the
field of natural language processing. However, in real-world scenarios, data
labels are often noisy due to the complex annotation process, making it
essential to develop strategies for fine-tuning PLMs with such noisy labels. To
this end, we introduce an innovative approach for fine-tuning PLMs using noisy
labels, which incorporates the guidance of Large Language Models (LLMs) like
ChatGPT. This guidance assists in accurately distinguishing between clean and
noisy samples and provides supplementary information beyond the noisy labels,
thereby boosting the learning process during fine-tuning PLMs. Extensive
experiments on synthetic and real-world noisy datasets further demonstrate the
superior advantages of our framework over the state-of-the-art baselines.",None,-1
d6f9a11a-b90b-43a6-86cf-6ebc86f25113,Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning,0.917915,"Planning for goal-oriented dialogue often requires simulating future dialogue
interactions and estimating task progress. Many approaches thus consider
training neural networks to perform look-ahead search algorithms such as A*
search and Monte Carlo Tree Search (MCTS). However, this training often
requires abundant annotated data, which creates challenges when faced with
noisy annotations or low-resource settings. We introduce GDP-Zero, an approach
using Open-Loop MCTS to perform goal-oriented dialogue policy planning without
any model training. GDP-Zero prompts a large language model to act as a policy
prior, value function, user simulator, and system model during the tree search.
We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that
its responses are preferred over ChatGPT up to 59.32% of the time, and are
rated more persuasive than ChatGPT during interactive evaluations.",None,-1
c9f56b6a-4a31-4438-8abe-9e6180afa35a,Speech-Gesture GAN: Gesture Generation for Robots and Embodied Agents,0.025813,"Embodied agents, in the form of virtual agents or social robots, are rapidly
becoming more widespread. In human-human interactions, humans use nonverbal
behaviours to convey their attitudes, feelings, and intentions. Therefore, this
capability is also required for embodied agents in order to enhance the quality
and effectiveness of their interactions with humans. In this paper, we propose
a novel framework that can generate sequences of joint angles from the speech
text and speech audio utterances. Based on a conditional Generative Adversarial
Network (GAN), our proposed neural network model learns the relationships
between the co-speech gestures and both semantic and acoustic features from the
speech input. In order to train our neural network model, we employ a public
dataset containing co-speech gestures with corresponding speech audio
utterances, which were captured from a single male native English speaker. The
results from both objective and subjective evaluations demonstrate the efficacy
of our gesture-generation framework for Robots and Embodied Agents.",None,-1
5328878b-6ef5-4d66-8ae1-0acab17d92d1,Faithfulness Tests for Natural Language Explanations,0.669655,"Explanations of neural models aim to reveal a model's decision-making process
for its predictions. However, recent work shows that current methods giving
explanations such as saliency maps or counterfactuals can be misleading, as
they are prone to present reasons that are unfaithful to the model's inner
workings. This work explores the challenging question of evaluating the
faithfulness of natural language explanations (NLEs). To this end, we present
two tests. First, we propose a counterfactual input editor for inserting
reasons that lead to counterfactual predictions but are not reflected by the
NLEs. Second, we reconstruct inputs from the reasons stated in the generated
NLEs and check how often they lead to the same predictions. Our tests can
evaluate emerging NLE models, proving a fundamental tool in the development of
faithful NLEs.",None,-1
257dffe1-7a21-40a0-9431-995bc5fd6b37,"Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras",0.633467,"Semantic segmentation plays a vital role in computer vision tasks, enabling
precise pixel-level understanding of images. In this paper, we present a
comprehensive library for semantic segmentation, which contains implementations
of popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also
evaluate and compare these models on several datasets, offering researchers and
practitioners a powerful toolset for tackling diverse segmentation challenges.",None,-1
57e66691-4fdc-4d14-93b1-51eafa01622f,MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting,0.424158,"Large language models (LLMs) have achieved impressive performance on various
reasoning tasks. To further improve the performance, we propose MultiTool-CoT,
a novel framework that leverages chain-of-thought (CoT) prompting to
incorporate multiple external tools, such as a calculator and a knowledge
retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2
dataset of NumGLUE, which requires both numerical reasoning and domain-specific
knowledge. The experiments show that our method significantly outperforms
strong baselines and achieves state-of-the-art performance.",None,-1
11bb9573-fac6-41c5-9c4f-99014ac61653,Argumentation Element Annotation Modeling using XLNet,0.773344,"This study demonstrates the effectiveness of XLNet, a transformer-based
language model, for annotating argumentative elements in persuasive essays.
XLNet's architecture incorporates a recurrent mechanism that allows it to model
long-term dependencies in lengthy texts. Fine-tuned XLNet models were applied
to three datasets annotated with different schemes - a proprietary dataset
using the Annotations for Revisions and Reflections on Writing (ARROW) scheme,
the PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNet
models achieved strong performance across all datasets, even surpassing human
agreement levels in some cases. This shows XLNet capably handles diverse
annotation schemes and lengthy essays. Comparisons between the model outputs on
different datasets also revealed insights into the relationships between the
annotation tags. Overall, XLNet's strong performance on modeling argumentative
structures across diverse datasets highlights its suitability for providing
automated feedback on essay organization.",None,-1
3c4da41b-9b78-4b3d-bd9e-f09ee09669c3,Interactive Video Corpus Moment Retrieval using Reinforcement Learning,0.0912843,"Known-item video search is effective with human-in-the-loop to interactively
investigate the search result and refine the initial query. Nevertheless, when
the first few pages of results are swamped with visually similar items, or the
search target is hidden deep in the ranked list, finding the know-item target
usually requires a long duration of browsing and result inspection. This paper
tackles the problem by reinforcement learning, aiming to reach a search target
within a few rounds of interaction by long-term learning from user feedbacks.
Specifically, the system interactively plans for navigation path based on
feedback and recommends a potential target that maximizes the long-term reward
for user comment. We conduct experiments for the challenging task of video
corpus moment retrieval (VCMR) to localize moments from a large video corpus.
The experimental results on TVR and DiDeMo datasets verify that our proposed
work is effective in retrieving the moments that are hidden deep inside the
ranked lists of CONQUER and HERO, which are the state-of-the-art auto-search
engines for VCMR.",None,-1
6c12fd68-a44f-44c1-9b84-764f5e93496b,Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding,0.522717,"Masked signal modeling has greatly advanced self-supervised pre-training for
language and 2D images. However, it is still not fully explored in 3D scene
understanding. Thus, this paper introduces Masked Shape Prediction (MSP), a new
framework to conduct masked signal modeling in 3D scenes. MSP uses the
essential 3D semantic cue, i.e., geometric shape, as the prediction target for
masked points. The context-enhanced shape target consisting of explicit shape
context and implicit deep shape feature is proposed to facilitate exploiting
contextual cues in shape prediction. Meanwhile, the pre-training architecture
in MSP is carefully designed to alleviate the masked shape leakage from point
coordinates. Experiments on multiple 3D understanding tasks on both indoor and
outdoor datasets demonstrate the effectiveness of MSP in learning good feature
representations to consistently boost downstream performance.",None,-1
d90663f8-ef08-45fd-9588-ca2cf7d5861c,CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population,0.941907,"Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task
in NLP, as it tackles knowledge from external sources with unseen events and
entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an
evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that
suffer from a substantial fraction of incorrect answers, and the evaluation set
is not well-aligned with the external knowledge source as a result of random
sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB
Population benchmark, which addresses the two mentioned problems by using
experts instead of crowd-sourced annotation and by adding diversified
adversarial samples to make the evaluation set more representative. We conduct
extensive experiments comparing state-of-the-art methods for CSKB Population on
the new evaluation set for future research comparisons. Empirical results show
that the population task is still challenging, even for large language models
(LLM) such as ChatGPT. Codes and data are available at
https://github.com/HKUST-KnowComp/CSKB-Population.",None,-1
48b4c516-8195-4369-9eef-47bab3efac51,TransWorldNG: Traffic Simulation via Foundation Model,0.773864,"Traffic simulation is a crucial tool for transportation decision-making and
policy development. However, achieving realistic simulations in the face of the
high dimensionality and heterogeneity of traffic environments is a longstanding
challenge. In this paper, we present TransWordNG, a traffic simulator that uses
Data-driven algorithms and Graph Computing techniques to learn traffic dynamics
from real data. The functionality and structure of TransWorldNG are introduced,
which utilize a foundation model for transportation management and control. The
results demonstrate that TransWorldNG can generate more realistic traffic
patterns compared to traditional simulators. Additionally, TransWorldNG
exhibits better scalability, as it shows linear growth in computation time as
the scenario scale increases. To the best of our knowledge, this is the first
traffic simulator that can automatically learn traffic patterns from real-world
data and efficiently generate accurate and realistic traffic environments.",None,-1
3eabfc99-3f56-4d45-95c8-4fc0e918d5a5,Face Animation with an Attribute-Guided Diffusion Model,0.583971,"Face animation has achieved much progress in computer vision. However,
prevailing GAN-based methods suffer from unnatural distortions and artifacts
due to sophisticated motion deformation. In this paper, we propose a Face
Animation framework with an attribute-guided Diffusion Model (FADM), which is
the first work to exploit the superior modeling capacity of diffusion models
for photo-realistic talking-head generation. To mitigate the uncontrollable
synthesis effect of the diffusion model, we design an Attribute-Guided
Conditioning Network (AGCN) to adaptively combine the coarse animation features
and 3D face reconstruction results, which can incorporate appearance and motion
conditions into the diffusion process. These specific designs help FADM rectify
unnatural artifacts and distortions, and also enrich high-fidelity facial
details through iterative diffusion refinements with accurate animation
attributes. FADM can flexibly and effectively improve existing animation
videos. Extensive experiments on widely used talking-head benchmarks validate
the effectiveness of FADM over prior arts.",None,-1
90f7a7a8-0661-4c16-8d06-9bba4155cb6d,Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected,0.848982,"Meta AI Research has recently released SAM (Segment Anything Model) which is
trained on a large segmentation dataset of over 1 billion masks. As a
foundation model in the field of computer vision, SAM (Segment Anything Model)
has gained attention for its impressive performance in generic object
segmentation. Despite its strong capability in a wide range of zero-shot
transfer tasks, it remains unknown whether SAM can detect things in challenging
setups like transparent objects. In this work, we perform an empirical
evaluation of two glass-related challenging scenarios: mirror and transparent
objects. We found that SAM often fails to detect the glass in both scenarios,
which raises concern for deploying the SAM in safety-critical situations that
have various forms of glass.",None,-1
fdc39234-8e49-401f-8a9b-34155a25ba78,RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models,0.145095,"Retrieval-augmented large language models (R-LLMs) combine pre-trained large
language models (LLMs) with information retrieval systems to improve the
accuracy of factual question-answering. However, current libraries for building
R-LLMs provide high-level abstractions without sufficient transparency for
evaluating and optimizing prompts within specific inference processes such as
retrieval and generation. To address this gap, we present RaLLe, an open-source
framework designed to facilitate the development, evaluation, and optimization
of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily
develop and evaluate R-LLMs, improving hand-crafted prompts, assessing
individual inference processes, and objectively measuring overall system
performance quantitatively. By leveraging these features, developers can
enhance the performance and accuracy of their R-LLMs in knowledge-intensive
generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.",None,-1
be47d1ea-cea8-4a47-91bb-b133529f8c87,Grand Challenge On Detecting Cheapfakes,0.0942796,"Cheapfake is a recently coined term that encompasses non-AI (""cheap"")
manipulations of multimedia content. Cheapfakes are known to be more prevalent
than deepfakes. Cheapfake media can be created using editing software for
image/video manipulations, or even without using any software, by simply
altering the context of an image/video by sharing the media alongside
misleading claims. This alteration of context is referred to as out-of-context
(OOC) misuse of media. OOC media is much harder to detect than fake media,
since the images and videos are not tampered. In this challenge, we focus on
detecting OOC images, and more specifically the misuse of real photographs with
conflicting image captions in news items. The aim of this challenge is to
develop and benchmark models that can be used to detect whether given samples
(news image and associated captions) are OOC, based on the recently compiled
COSMOS dataset.",None,-1
8de98abe-374e-4178-8b8f-c160e53a1727,K-UniMorph: Korean Universal Morphology and its Feature Schema,0.218935,"We present in this work a new Universal Morphology dataset for Korean.
Previously, the Korean language has been underrepresented in the field of
morphological paradigms amongst hundreds of diverse world languages. Hence, we
propose this Universal Morphological paradigms for the Korean language that
preserve its distinct characteristics. For our K-UniMorph dataset, we outline
each grammatical criterion in detail for the verbal endings, clarify how to
extract inflected forms, and demonstrate how we generate the morphological
schemata. This dataset adopts morphological feature schema from Sylak-Glassman
et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract
inflected verb forms from the Sejong morphologically analyzed corpus that is
one of the largest annotated corpora for Korean. During the data creation, our
methodology also includes investigating the correctness of the conversion from
the Sejong corpus. Furthermore, we carry out the inflection task using three
different Korean word forms: letters, syllables and morphemes. Finally, we
discuss and describe future perspectives on Korean morphological paradigms and
the dataset.",None,-1
9f049fe7-17ab-49c3-b3a4-491be1b04b9f,Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop,0.525956,"This paper presents our project proposal for extracting biomedical
information from German clinical narratives with limited amounts of
annotations. We first describe the applied strategies in transfer learning and
active learning for solving our problem. After that, we discuss the design of
the user interface for both supplying model inspection and obtaining user
annotations in the interactive environment.",None,-1
3109ba93-1473-43e3-9fdb-9331b05c6937,Hallucinated Adversarial Control for Conservative Offline Policy Evaluation,0.473275,"We study the problem of conservative off-policy evaluation (COPE) where given
an offline dataset of environment interactions, collected by other agents, we
seek to obtain a (tight) lower bound on a policy's performance. This is crucial
when deciding whether a given policy satisfies certain minimal
performance/safety criteria before it can be deployed in the real world. To
this end, we introduce HAMBO, which builds on an uncertainty-aware learned
model of the transition dynamics. To form a conservative estimate of the
policy's performance, HAMBO hallucinates worst-case trajectories that the
policy may take, within the margin of the models' epistemic confidence regions.
We prove that the resulting COPE estimates are valid lower bounds, and, under
regularity conditions, show their convergence to the true expected return.
Finally, we discuss scalable variants of our approach based on Bayesian Neural
Networks and empirically demonstrate that they yield reliable and tight lower
bounds in various continuous control environments.",None,-1
8b3fba75-57f9-497e-9d44-e89d599815f0,The language of sounds unheard: Exploring musical timbre semantics of large language models,0.669247,"Semantic dimensions of sound have been playing a central role in
understanding the nature of auditory sensory experience as well as the broader
relation between perception, language, and meaning. Accordingly, and given the
recent proliferation of large language models (LLMs), here we asked whether
such models exhibit an organisation of perceptual semantics similar to those
observed in humans. Specifically, we prompted ChatGPT, a chatbot based on a
state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic
scales. We elicited multiple responses in separate chats, analogous to having
multiple human raters. ChatGPT generated semantic profiles that only partially
correlated with human ratings, yet showed robust agreement along well-known
psychophysical dimensions of musical sounds such as brightness (bright-dark)
and pitch height (deep-high). Exploratory factor analysis suggested the same
dimensionality but different spatial configuration of a latent factor space
between the chatbot and human ratings. Unexpectedly, the chatbot showed degrees
of internal variability that were comparable in magnitude to that of human
ratings. Our work highlights the potential of LLMs to capture salient
dimensions of human sensory experience.",None,-1
437b4bb2-a15a-432a-b0f2-b5f7141d801d,SGAligner : 3D Scene Alignment with Scene Graphs,0.580902,"Building 3D scene graphs has recently emerged as a topic in scene
representation for several embodied AI applications to represent the world in a
structured and rich manner. With their increased use in solving downstream
tasks (eg, navigation and room rearrangement), can we leverage and recycle them
for creating 3D maps of environments, a pivotal step in agent operation? We
focus on the fundamental problem of aligning pairs of 3D scene graphs whose
overlap can range from zero to partial and can contain arbitrary changes. We
propose SGAligner, the first method for aligning pairs of 3D scene graphs that
is robust to in-the-wild scenarios (ie, unknown overlap -- if any -- and
changes in the environment). We get inspired by multi-modality knowledge graphs
and use contrastive learning to learn a joint, multi-modal embedding space. We
evaluate on the 3RScan dataset and further showcase that our method can be used
for estimating the transformation between pairs of 3D scenes. Since benchmarks
for these tasks are missing, we create them on this dataset. The code,
benchmark, and trained models are available on the project website.",None,-1
ee5cb5c3-0499-4bf4-bf6b-1aae1bcbcde7,"Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis",0.999831,"Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a
major driver of financial market returns. We construct the largest tokenized
and annotated dataset of FOMC speeches, meeting minutes, and press conference
transcripts in order to understand how monetary policy influences financial
markets. In this study, we develop a novel task of hawkish-dovish
classification and benchmark various pre-trained language models on the
proposed dataset. Using the best-performing model (RoBERTa-large), we construct
a measure of monetary policy stance for the FOMC document release days. To
evaluate the constructed measure, we study its impact on the treasury market,
stock market, and macroeconomic indicators. Our dataset, models, and code are
publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.",None,-1
7606ff91-197f-4445-baca-366dc03e539b,Vanishing Activations: A Symptom of Deep Capsule Networks,0.168993,"Capsule Networks, an extension to Neural Networks utilizing vector or matrix
representations instead of scalars, were initially developed to create a
dynamic parse tree where visual concepts evolve from parts to complete objects.
Early implementations of Capsule Networks achieved and maintain
state-of-the-art results on various datasets. However, recent studies have
revealed shortcomings in the original Capsule Network architecture, notably its
failure to construct a parse tree and its susceptibility to vanishing gradients
when deployed in deeper networks. This paper extends the investigation to a
range of leading Capsule Network architectures, demonstrating that these issues
are not confined to the original design. We argue that the majority of Capsule
Network research has produced architectures that, while modestly divergent from
the original Capsule Network, still retain a fundamentally similar structure.
We posit that this inherent design similarity might be impeding the scalability
of Capsule Networks. Our study contributes to the broader discussion on
improving the robustness and scalability of Capsule Networks.",None,-1
9037468a-91bc-48b4-82e7-8a59d97a1203,Safe AI for health and beyond -- Monitoring to transform a health service,0.106844,"Machine learning techniques are effective for building predictive models
because they identify patterns in large datasets. Development of a model for
complex real-life problems often stop at the point of publication, proof of
concept or when made accessible through some mode of deployment. However, a
model in the medical domain risks becoming obsolete as patient demographics,
systems and clinical practices change. The maintenance and monitoring of
predictive model performance post-publication is crucial to enable their safe
and effective long-term use. We will assess the infrastructure required to
monitor the outputs of a machine learning algorithm, and present two scenarios
with examples of monitoring and updates of models, firstly on a breast cancer
prognosis model trained on public longitudinal data, and secondly on a
neurodegenerative stratification algorithm that is currently being developed
and tested in clinic.",None,-1
79a4665f-216c-472e-9b55-ea7e9f99704f,A Corpus for Sentence-level Subjectivity Detection on English News Articles,0.28675,"We develop novel annotation guidelines for sentence-level subjectivity
detection, which are not limited to language-specific cues. We use our
guidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective
sentences extracted from English news articles on controversial topics. Our
corpus paves the way for subjectivity detection in English and across other
languages without relying on language-specific tools, such as lexicons or
machine translation. We evaluate state-of-the-art multilingual
transformer-based models on the task in mono-, multi-, and cross-language
settings. For this purpose, we re-annotate an existing Italian corpus. We
observe that models trained in the multilingual setting achieve the best
performance on the task.",None,-1
4df5a4c7-f636-4bb2-8ff6-294f81bcd013,IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function,0.102796,"Exact computation of the partition function is known to be intractable,
necessitating approximate inference techniques. Existing methods for
approximate inference are slow to converge for many benchmarks. The control of
accuracy-complexity trade-off is also non-trivial in many of these methods. We
propose a novel incremental build-infer-approximate (IBIA) framework for
approximate inference that addresses these issues. In this framework, the
probabilistic graphical model is converted into a sequence of clique tree
forests (SCTF) with bounded clique sizes. We show that the SCTF can be used to
efficiently compute the partition function. We propose two new algorithms which
are used to construct the SCTF and prove the correctness of both. The first is
an algorithm for incremental construction of CTFs that is guaranteed to give a
valid CTF with bounded clique sizes and the second is an approximation
algorithm that takes a calibrated CTF as input and yields a valid and
calibrated CTF with reduced clique sizes as the output. We have evaluated our
method using several benchmark sets from recent UAI competitions and our
results show good accuracies with competitive runtimes.",None,-1
31d84caf-9ea9-4b98-8284-0c03e61e3fe6,An Embedding-based Approach to Inconsistency-tolerant Reasoning with Inconsistent Ontologies,0.215623,"Inconsistency handling is an important issue in knowledge management.
Especially in ontology engineering, logical inconsistencies may occur during
ontology construction. A natural way to reason with an inconsistent ontology is
to utilize the maximal consistent subsets of the ontology. However, previous
studies on selecting maximum consistent subsets have rarely considered the
semantics of the axioms, which may result in irrational inference. In this
paper, we propose a novel approach to reasoning with inconsistent ontologies in
description logics based on the embeddings of axioms. We first give a method
for turning axioms into distributed semantic vectors to compute the semantic
connections between the axioms. We then define an embedding-based method for
selecting the maximum consistent subsets and use it to define an
inconsistency-tolerant inference relation. We show the rationality of our
inference relation by considering some logical properties. Finally, we conduct
experiments on several ontologies to evaluate the reasoning power of our
inference relation. The experimental results show that our embedding-based
method can outperform existing inconsistency-tolerant reasoning methods based
on maximal consistent subsets.",None,-1
5341b3ae-61d4-4599-9f35-19e085c04e7e,DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion,0.104187,"Motion capture from a limited number of body-worn sensors, such as inertial
measurement units (IMUs) and pressure insoles, has important applications in
health, human performance, and entertainment. Recent work has focused on
accurately reconstructing whole-body motion from a specific sensor
configuration using six IMUs. While a common goal across applications is to use
the minimal number of sensors to achieve required accuracy, the optimal
arrangement of the sensors might differ from application to application. We
propose a single diffusion model, DiffusionPoser, which reconstructs human
motion in real-time from an arbitrary combination of sensors, including IMUs
placed at specified locations, and, pressure insoles. Unlike existing methods,
our model grants users the flexibility to determine the number and arrangement
of sensors tailored to the specific activity of interest, without the need for
retraining. A novel autoregressive inferencing scheme ensures real-time motion
reconstruction that closely aligns with measured sensor signals. The generative
nature of DiffusionPoser ensures realistic behavior, even for
degrees-of-freedom not directly measured. Qualitative results can be found on
our website: https://diffusionposer.github.io/.",None,-1
70921c22-6257-4a04-bcd5-f4329e1ac323,Curriculum Learning for ab initio Deep Learned Refractive Optics,0.329597,"Deep optical optimization has recently emerged as a new paradigm for
designing computational imaging systems using only the output image as the
objective. However, it has been limited to either simple optical systems
consisting of a single element such as a diffractive optical element (DOE) or
metalens, or the fine-tuning of compound lenses from good initial designs. Here
we present a DeepLens design method based on curriculum learning, which is able
to learn optical designs of compound lenses ab initio from randomly initialized
surfaces without human intervention, therefore overcoming the need for a good
initial design. We demonstrate the effectiveness of our approach by fully
automatically designing both classical imaging lenses and a large field-of-view
extended depth-of-field computational lens in a cellphone-style form factor,
with highly aspheric surfaces and a short back focal length.",None,-1
7c59d3d3-3f5c-473a-b739-02a52600df1b,JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning,0.302063,"In the ongoing wave of impact driven by large language models (LLMs) like
ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial
research frontier. Since mainstream LLMs tend to be designed for
general-purpose applications, constructing a medical LLM through domain
adaptation is a huge challenge. While instruction-tuning is used to fine-tune
some LLMs, its precise roles in domain adaptation remain unknown. Here we show
the contribution of LoRA-based instruction-tuning to performance in Japanese
medical question-answering tasks. In doing so, we employ a multifaceted
evaluation for multiple-choice questions, including scoring based on ""Exact
match"" and ""Gestalt distance"" in addition to the conventional accuracy. Our
findings suggest that LoRA-based instruction-tuning can partially incorporate
domain-specific knowledge into LLMs, with larger models demonstrating more
pronounced effects. Furthermore, our results underscore the potential of
adapting English-centric models for Japanese applications in domain adaptation,
while also highlighting the persisting limitations of Japanese-centric models.
This initiative represents a pioneering effort in enabling medical institutions
to fine-tune and operate models without relying on external services.",None,-1
e9ea385a-6d0f-4d50-b4ac-2764f308fa85,Question Decomposition Tree for Answering Complex Questions over Knowledge Bases,0.633472,"Knowledge base question answering (KBQA) has attracted a lot of interest in
recent years, especially for complex questions which require multiple facts to
answer. Question decomposition is a promising way to answer complex questions.
Existing decomposition methods split the question into sub-questions according
to a single compositionality type, which is not sufficient for questions
involving multiple compositionality types. In this paper, we propose Question
Decomposition Tree (QDT) to represent the structure of complex questions.
Inspired by recent advances in natural language generation (NLG), we present a
two-staged method called Clue-Decipher to generate QDT. It can leverage the
strong ability of NLG model and simultaneously preserve the original questions.
To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA
system called QDTQA. Extensive experiments show that QDTQA outperforms previous
state-of-the-art methods on ComplexWebQuestions dataset. Besides, our
decomposition method improves an existing KBQA system by 12% and sets a new
state-of-the-art on LC-QuAD 1.0.",None,-1
83ffd7f0-c5e2-4514-98dc-40e81d9a1aef,A Formal Perspective on Byte-Pair Encoding,0.739265,"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in
NLP, despite being devised initially as a compression method. BPE appears to be
a greedy algorithm at face value, but the underlying optimization problem that
BPE seeks to solve has not yet been laid down. We formalize BPE as a
combinatorial optimization problem. Via submodular functions, we prove that the
iterative greedy version is a
$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation
of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the
total backward curvature with respect to the optimal merge sequence
$\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is
$\approx 0.37$.
  We provide a faster implementation of BPE which improves the runtime
complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log
M\right)$, where $N$ is the sequence length and $M$ is the merge count.
Finally, we optimize the brute-force algorithm for optimal BPE using
memoization.",None,-1
bbb1bdd0-dd36-45bf-9b7d-fd71db9ea223,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,0.0942663,"The demand for multimodal dialogue systems has been rising in various
domains, emphasizing the importance of interpreting multimodal inputs from
conversational and situational contexts. We explore three methods to tackle
this problem and evaluate them on the largest situated dialogue dataset, SIMMC
2.1. Our best method, scene-dialogue alignment, improves the performance by
~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and
discussion regarding the limitation of our methods and the potential directions
for future works. Our code is publicly available at
https://github.com/holylovenia/multimodal-object-identification.",None,-1
4b1df5d7-b576-4480-b904-3e1e6cc3ce20,DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations,0.383559,"In open-domain dialogue generation tasks, contexts and responses in most
datasets are one-to-one mapped, violating an important many-to-many
characteristic: a context leads to various responses, and a response answers
multiple contexts. Without such patterns, models poorly generalize and prefer
responding safely. Many attempts have been made in either multi-turn settings
from a one-to-many perspective or in a many-to-many perspective but limited to
single-turn settings. The major challenge to many-to-many augment multi-turn
dialogues is that discretely replacing each turn with semantic similarity
breaks fragile context coherence. In this paper, we propose DialoGue Path
Sampling (DialoGPS) method in continuous semantic space, the first many-to-many
augmentation method for multi-turn dialogues. Specifically, we map a dialogue
to our extended Brownian Bridge, a special Gaussian process. We sample latent
variables to form coherent dialogue paths in the continuous space. A dialogue
path corresponds to a new multi-turn dialogue and is used as augmented training
data. We show the effect of DialoGPS with both automatic and human evaluation.",None,-1
8b6ec826-4f93-4c9f-8335-c46cbbc3a163,Facilitating Contrastive Learning of Discourse Relational Senses by Exploiting the Hierarchy of Sense Relations,0.990947,"Implicit discourse relation recognition is a challenging task that involves
identifying the sense or senses that hold between two adjacent spans of text,
in the absence of an explicit connective between them. In both PDTB-2 and
PDTB-3, discourse relational senses are organized into a three-level hierarchy
ranging from four broad top-level senses, to more specific senses below them.
Most previous work on implicit discourse relation recognition have used the
sense hierarchy simply to indicate what sense labels were available. Here we do
more -- incorporating the sense hierarchy into the recognition process itself
and using it to select the negative examples used in contrastive learning. With
no additional effort, the approach achieves state-of-the-art performance on the
task.",None,-1
b39146ed-242f-4e3d-a7c8-f109050cdceb,IFAN: An Explainability-Focused Interaction Framework for Humans and NLP Models,0.106662,"Interpretability and human oversight are fundamental pillars of deploying
complex NLP models into real-world applications. However, applying
explainability and human-in-the-loop methods requires technical proficiency.
Despite existing toolkits for model understanding and analysis, options to
integrate human feedback are still limited. We propose IFAN, a framework for
real-time explanation-based interaction with NLP models. Through IFAN's
interface, users can provide feedback to selected model explanations, which is
then integrated through adapter layers to align the model with human rationale.
We show the system to be effective in debiasing a hate speech classifier with
minimal impact on performance. IFAN also offers a visual admin system and API
to manage models (and datasets) as well as control access rights. A demo is
live at https://ifan.ml.",None,-1
60edea07-eaae-4cb9-8442-3871a4fe2bdc,Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT,0.13549,"Large pre-trained language models have recently gained significant traction
due to their improved performance on various down-stream tasks like text
classification and question answering, requiring only few epochs of
fine-tuning. However, their large model sizes often prohibit their applications
on resource-constrained edge devices. Existing solutions of yielding
parameter-efficient BERT models largely rely on compute-exhaustive training and
fine-tuning. Moreover, they often rely on additional compute heavy models to
mitigate the performance gap. In this paper, we present Sensi-BERT, a
sensitivity driven efficient fine-tuning of BERT models that can take an
off-the-shelf pre-trained BERT model and yield highly parameter-efficient
models for downstream tasks. In particular, we perform sensitivity analysis to
rank each individual parameter tensor, that then is used to trim them
accordingly during fine-tuning for a given parameter or FLOPs budget. Our
experiments show the efficacy of Sensi-BERT across different downstream tasks
including MNLI, QQP, QNLI, SST-2 and SQuAD, showing better performance at
similar or smaller parameter budget compared to various alternatives.",None,-1
07f7f024-a195-487a-9e27-4b75706bff2e,Safer Conversational AI as a Source of User Delight,0.0641045,"This work explores the impact of moderation on users' enjoyment of
conversational AI systems. While recent advancements in Large Language Models
(LLMs) have led to highly capable conversational AIs that are increasingly
deployed in real-world settings, there is a growing concern over AI safety and
the need to moderate systems to encourage safe language and prevent harm.
However, some users argue that current approaches to moderation limit the
technology, compromise free expression, and limit the value delivered by the
technology. This study takes an unbiased stance and shows that moderation does
not necessarily detract from user enjoyment. Heavy handed moderation does seem
to have a nefarious effect, but models that are moderated to be safer can lead
to a better user experience. By deploying various conversational AIs in the
Chai platform, the study finds that user retention can increase with a level of
moderation and safe system design. These results demonstrate the importance of
appropriately defining safety in models in a way that is both responsible and
focused on serving users.",None,-1
7dfb3f57-03f4-4b65-81ad-a24c4b96272e,ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction,0.461554,"Most advanced unsupervised anomaly detection (UAD) methods rely on modeling
feature representations of frozen encoder networks pre-trained on large-scale
datasets, e.g. ImageNet. However, the features extracted from the encoders that
are borrowed from natural image domains coincide little with the features
required in the target UAD domain, such as industrial inspection and medical
imaging. In this paper, we propose a novel epistemic UAD method, namely
ReContrast, which optimizes the entire network to reduce biases towards the
pre-trained image domain and orients the network in the target domain. We start
with a feature reconstruction approach that detects anomalies from errors.
Essentially, the elements of contrastive learning are elegantly embedded in
feature reconstruction to prevent the network from training instability,
pattern collapse, and identical shortcut, while simultaneously optimizing both
the encoder and decoder on the target domain. To demonstrate our transfer
ability on various image domains, we conduct extensive experiments across two
popular industrial defect detection benchmarks and three medical image UAD
tasks, which shows our superiority over current state-of-the-art methods.",None,-1
67a768c8-b9df-4fd1-9484-efdf2c89d7d7,AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web,0.81599,"Existing datasets for automated fact-checking have substantial limitations,
such as relying on artificial claims, lacking annotations for evidence and
intermediate reasoning, or including evidence published after the claim. In
this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims
covering fact-checks by 50 different organizations. Each claim is annotated
with question-answer pairs supported by evidence available online, as well as
textual justifications explaining how the evidence combines to produce a
verdict. Through a multi-round annotation process, we avoid common pitfalls
including context dependence, evidence insufficiency, and temporal leakage, and
reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We
develop a baseline as well as an evaluation scheme for verifying claims through
several question-answering steps against the open web.",None,-1
fccb36b6-2484-4642-b2c6-2f259dc6c1c6,Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks,0.256188,"We propose a novel gradient-based attack against transformer-based language
models that searches for an adversarial example in a continuous space of token
probabilities. Our algorithm mitigates the gap between adversarial loss for
continuous and discrete text representations by performing multi-step
quantization in a quantization-compensation loop. Experiments show that our
method significantly outperforms other approaches on various natural language
processing (NLP) tasks.",None,-1
3ad61236-4360-4b7b-8056-ed9c48b7464e,WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models,0.703456,"This paper describes our submission to the MEDIQA-Chat 2023 shared task for
automatic clinical note generation from doctor-patient conversations. We report
results for two approaches: the first fine-tunes a pre-trained language model
(PLM) on the shared task data, and the second uses few-shot in-context learning
(ICL) with a large language model (LLM). Both achieve high performance as
measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and
first, respectively, of all submissions to the shared task. Expert human
scrutiny indicates that notes generated via the ICL-based approach with GPT-4
are preferred about as often as human-written notes, making it a promising path
toward automated note generation from doctor-patient conversations.",None,-1
439b36ca-c2d7-417c-a628-f99845bfc799,Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean,0.439939,"We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.",None,-1
3135a5d6-eb84-4c95-acfb-b620e237e448,A Multi-Modal Transformer Network for Action Detection,0.740992,"This paper proposes a novel multi-modal transformer network for detecting
actions in untrimmed videos. To enrich the action features, our transformer
network utilizes a new multi-modal attention mechanism that computes the
correlations between different spatial and motion modalities combinations.
Exploring such correlations for actions has not been attempted previously. To
use the motion and spatial modality more effectively, we suggest an algorithm
that corrects the motion distortion caused by camera movement. Such motion
distortion, common in untrimmed videos, severely reduces the expressive power
of motion features such as optical flow fields. Our proposed algorithm
outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and
ActivityNet. We also conducted comparative experiments on our new instructional
activity dataset, including a large set of challenging classroom videos
captured from elementary schools.",None,-1
810d35cb-1e2d-4432-aec3-9c4f38ee6427,Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation,0.275396,"A challenge in the Dialogue State Tracking (DST) field is adapting models to
new domains without using any supervised data, zero-shot domain adaptation.
Parameter-Efficient Transfer Learning (PETL) has the potential to address this
problem due to its robustness. However, it has yet to be applied to the
zero-shot scenarios, as it is not clear how to apply it unsupervisedly.
  Our method, Prompter, uses descriptions of target domain slots to generate
dynamic prefixes that are concatenated to the key and values at each layer's
self-attention mechanism. This allows for the use of prefix-tuning in
zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD
benchmarks. In generating prefixes, our analyses find that Prompter not only
utilizes the semantics of slot descriptions but also how often the slots appear
together in conversation. Moreover, Prompter's gains are due to its improved
ability to distinguish ""none""-valued dialogue slots, compared against
baselines.",None,-1
04a4e465-4892-448b-ae45-c0540c30f8dc,Predictive auxiliary objectives in deep RL mimic learning in the brain,0.28918,"The ability to predict upcoming events has been hypothesized to comprise a
key aspect of natural and machine cognition. This is supported by trends in
deep reinforcement learning (RL), where self-supervised auxiliary objectives
such as prediction are widely used to support representation learning and
improve task performance. Here, we study the effects predictive auxiliary
objectives have on representation learning across different modules of an RL
system and how these mimic representational changes observed in the brain. We
find that predictive objectives improve and stabilize learning particularly in
resource-limited architectures, and we identify settings where longer
predictive horizons better support representational transfer. Furthermore, we
find that representational changes in this RL system bear a striking
resemblance to changes in neural activity observed in the brain across various
experiments. Specifically, we draw a connection between the auxiliary
predictive model of the RL system and hippocampus, an area thought to learn a
predictive model to support memory-guided behavior. We also connect the encoder
network and the value learning network of the RL system to visual cortex and
striatum in the brain, respectively. This work demonstrates how representation
learning in deep RL systems can provide an interpretable framework for modeling
multi-region interactions in the brain. The deep RL perspective taken here also
suggests an additional role of the hippocampus in the brain -- that of an
auxiliary learning system that benefits representation learning in other
regions.",None,-1
4aad606d-3923-41ec-898e-8e6c5c2a36aa,ChiroDiff: Modelling chirographic data with Diffusion Models,0.343674,"Generative modelling over continuous-time geometric constructs, a.k.a such as
handwriting, sketches, drawings etc., have been accomplished through
autoregressive distributions. Such strictly-ordered discrete factorization
however falls short of capturing key properties of chirographic data -- it
fails to build holistic understanding of the temporal concept due to one-way
visibility (causality). Consequently, temporal data has been modelled as
discrete token sequences of fixed sampling rate instead of capturing the true
underlying concept. In this paper, we introduce a powerful model-class namely
""Denoising Diffusion Probabilistic Models"" or DDPMs for chirographic data that
specifically addresses these flaws. Our model named ""ChiroDiff"", being
non-autoregressive, learns to capture holistic concepts and therefore remains
resilient to higher temporal sampling rate up to a good extent. Moreover, we
show that many important downstream utilities (e.g. conditional sampling,
creative mixing) can be flexibly implemented using ChiroDiff. We further show
some unique use-cases like stochastic vectorization, de-noising/healing,
abstraction are also possible with this model-class. We perform quantitative
and qualitative evaluation of our framework on relevant datasets and found it
to be better or on par with competing approaches.",None,-1
9a9fc235-a21c-4b63-bdd5-1fd314e16dc8,Evaluating Self-Supervised Speech Representations for Indigenous American Languages,0.618935,"The application of self-supervision to speech representation learning has
garnered significant interest in recent years, due to its scalability to large
amounts of unlabeled data. However, much progress, both in terms of
pre-training and downstream evaluation, has remained concentrated in
monolingual models that only consider English. Few models consider other
languages, and even fewer consider indigenous ones. In our submission to the
New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR
corpus for Quechua, an indigenous South American Language. We benchmark the
efficacy of large SSL models on Quechua, along with 6 other indigenous
languages such as Guarani and Bribri, on low-resource ASR. Our results show
surprisingly strong performance by state-of-the-art SSL models, showing the
potential generalizability of large-scale models to real-world data.",None,-1
adcb25cc-287b-449f-93d5-142aa2849092,Computational thematics: Comparing algorithms for clustering the genres of literary fiction,0.343994,"What are the best methods of capturing thematic similarity between literary
texts? Knowing the answer to this question would be useful for automatic
clustering of book genres, or any other thematic grouping. This paper compares
a variety of algorithms for unsupervised learning of thematic similarities
between texts, which we call ""computational thematics"". These algorithms belong
to three steps of analysis: text preprocessing, extraction of text features,
and measuring distances between the lists of features. Each of these steps
includes a variety of options. We test all the possible combinations of these
options: every combination of algorithms is given a task to cluster a corpus of
books belonging to four pre-tagged genres of fiction. This clustering is then
validated against the ""ground truth"" genre labels. Such comparison of
algorithms allows us to learn the best and the worst combinations for
computational thematic analysis. To illustrate the sharp difference between the
best and the worst methods, we then cluster 5000 random novels from the
HathiTrust corpus of fiction.",None,-1
3d5b5051-e937-4715-94c7-23d0f114146f,Removing Image Artifacts From Scratched Lens Protectors,0.0993372,"A protector is placed in front of the camera lens for mobile devices to avoid
damage, while the protector itself can be easily scratched accidentally,
especially for plastic ones. The artifacts appear in a wide variety of
patterns, making it difficult to see through them clearly. Removing image
artifacts from the scratched lens protector is inherently challenging due to
the occasional flare artifacts and the co-occurring interference within mixed
artifacts. Though different methods have been proposed for some specific
distortions, they seldom consider such inherent challenges. In our work, we
consider the inherent challenges in a unified framework with two cooperative
modules, which facilitate the performance boost of each other. We also collect
a new dataset from the real world to facilitate training and evaluation
purposes. The experimental results demonstrate that our method outperforms the
baselines qualitatively and quantitatively. The code and datasets will be
released after acceptance.",None,-1
b79d9c0c-fb04-4168-bf24-21cea2d59073,Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks,0.556798,"Large Language Models (LLMs) evaluation is a patchy and inconsistent
landscape, and it is becoming clear that the quality of automatic evaluation
metrics is not keeping up with the pace of development of generative models. We
aim to improve the understanding of current models' performance by providing a
preliminary and hybrid evaluation on a range of open and closed-source
generative LLMs on three NLP benchmarks: text summarisation, text
simplification and grammatical error correction (GEC), using both automatic and
human evaluation. We also explore the potential of the recently released GPT-4
to act as an evaluator. We find that ChatGPT consistently outperforms many
other popular models according to human reviewers on the majority of metrics,
while scoring much more poorly when using classic automatic evaluation metrics.
We also find that human reviewers rate the gold reference as much worse than
the best models' outputs, indicating the poor quality of many popular
benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs
in a way which aligns reasonably closely to human judgement despite
task-specific variations, with a lower alignment in the GEC task.",None,-1
4aa997b5-7977-4e36-92af-1cfa978790b9,Contrastive Loss is All You Need to Recover Analogies as Parallel Lines,0.352717,"While static word embedding models are known to represent linguistic
analogies as parallel lines in high-dimensional space, the underlying mechanism
as to why they result in such geometric structures remains obscure. We find
that an elementary contrastive-style method employed over distributional
information performs competitively with popular word embedding models on
analogy recovery tasks, while achieving dramatic speedups in training time.
Further, we demonstrate that a contrastive loss is sufficient to create these
parallel structures in word embeddings, and establish a precise relationship
between the co-occurrence statistics and the geometric structure of the
resulting word embeddings.",None,-1
1938fdc3-1dd4-4add-93ef-786368060978,Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,0.441321,"We investigate security concerns of the emergent instruction tuning paradigm,
that models are trained on crowdsourced datasets with task instructions to
achieve superior performance. Our studies demonstrate that an attacker can
inject backdoors by issuing very few malicious instructions (~1000 tokens) and
control model behavior through data poisoning, without even the need to modify
data instances or labels themselves. Through such instruction attacks, the
attacker can achieve over 90% attack success rate across four commonly used NLP
datasets. As an empirical study on instruction attacks, we systematically
evaluated unique perspectives of instruction attacks, such as poison transfer
where poisoned models can transfer to 15 diverse generative datasets in a
zero-shot manner; instruction transfer where attackers can directly apply
poisoned instruction on many other datasets; and poison resistance to continual
finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate
such backdoors to some degree. These findings highlight the need for more
robust defenses against poisoning attacks in instruction-tuning models and
underscore the importance of ensuring data quality in instruction
crowdsourcing.",None,-1
057ad090-2dc7-43a3-9a0a-a27862d6ec7a,Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning,0.0424762,"Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity
as a behavior emergent with scale, commonly manifesting as chain-of-thoughts
(CoT) reasoning. However, multiple empirical findings suggest that this prowess
is exclusive to LLMs with exorbitant sizes (beyond 50 billion parameters).
Meanwhile, educational neuroscientists suggest that symbolic algebraic
manipulation be introduced around the same time as arithmetic word problems to
modularize language-to-formulation, symbolic manipulation of the formulation,
and endgame arithmetic. In this paper, we start with the hypothesis that much
smaller LMs, which are weak at multi-step reasoning, can achieve reasonable
arithmetic reasoning if arithmetic word problems are posed as a
formalize-then-solve task. In our architecture, which we call SYRELM, the LM
serves the role of a translator to map natural language arithmetic questions
into a formal language (FL) description. A symbolic solver then evaluates the
FL expression to obtain the answer. A small frozen LM, equipped with an
efficient low-rank adapter, is capable of generating FL expressions that
incorporate natural language descriptions of the arithmetic problem (e.g.,
variable names and their purposes, formal expressions combining variables,
etc.). We adopt policy-gradient reinforcement learning to train the adapted LM,
informed by the non-differentiable symbolic solver. This marks a sharp
departure from the recent development in tool-augmented LLMs, in which the
external tools (e.g., calculator, Web search, etc.) are essentially detached
from the learning phase of the LM. SYRELM shows massive improvements (e.g.,
+30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J
6B model) over base LMs, while keeping our testbed easy to diagnose, interpret
and within reach of most researchers.",None,-1
2b1bdd88-f644-45d6-a056-df16d5fde592,Metacognitive threshold: a computational account,0.640985,"This paper will explore ways of computationally accounting for the
metacognitive threshold -- the minimum amount of stimulus needed for a mental
state to be perceived -- and discuss potential cognitive mechanisms by which
this threshold can be influenced through metacognitive training and meditation.",None,-1
13be8f37-5872-4102-a45e-2d5c2782603d,Semantic Channel Equalizer: Modelling Language Mismatch in Multi-User Semantic Communications,0.700326,"We consider a multi-user semantic communications system in which agents
(transmitters and receivers) interact through the exchange of semantic messages
to convey meanings. In this context, languages are instrumental in structuring
the construction and consolidation of knowledge, influencing conceptual
representation and semantic extraction and interpretation. Yet, the crucial
role of languages in semantic communications is often overlooked. When this is
not the case, agent languages are assumed compatible and unambiguously
interoperable, ignoring practical limitations that may arise due to language
mismatching. This is the focus of this work. When agents use distinct
languages, message interpretation is prone to semantic noise resulting from
critical distortion introduced by semantic channels. To address this problem,
this paper proposes a new semantic channel equalizer to counteract and limit
the critical ambiguity in message interpretation. Our proposed solution models
the mismatch of languages with measurable transformations over semantic
representation spaces. We achieve this using optimal transport theory, where we
model such transformations as transportation maps. Then, to recover at the
receiver the meaning intended by the teacher we operate semantic equalization
to compensate for the transformation introduced by the semantic channel, either
before transmission and/or after the reception of semantic messages. We
implement the proposed approach as an operation over a codebook of
transformations specifically designed for successful communication. Numerical
results show that the proposed semantic channel equalizer outperforms
traditional approaches in terms of operational complexity and transmission
accuracy.",None,-1
5a7a8cef-23ca-4827-9442-25fede035853,Inter-Annotator Agreement in the Wild: Uncovering Its Emerging Roles and Considerations in Real-World Scenarios,0.388692,"Inter-Annotator Agreement (IAA) is commonly used as a measure of label
consistency in natural language processing tasks. However, in real-world
scenarios, IAA has various roles and implications beyond its traditional usage.
In this paper, we not only consider IAA as a measure of consistency but also as
a versatile tool that can be effectively utilized in practical applications.
Moreover, we discuss various considerations and potential concerns when
applying IAA and suggest strategies for effectively navigating these
challenges.",None,-1
ded32d16-ea2b-48e0-bfbb-6b7920f134e7,End-to-End 3D Dense Captioning with Vote2Cap-DETR,0.948967,"3D dense captioning aims to generate multiple captions localized with their
associated object regions. Existing methods follow a sophisticated
``detect-then-describe'' pipeline equipped with numerous hand-crafted
components. However, these hand-crafted components would yield suboptimal
performance given cluttered object spatial and class distributions among
different scenes. In this paper, we propose a simple-yet-effective transformer
framework Vote2Cap-DETR based on recent popular \textbf{DE}tection
\textbf{TR}ansformer (DETR). Compared with prior arts, our framework has
several appealing advantages: 1) Without resorting to numerous hand-crafted
components, our method is based on a full transformer encoder-decoder
architecture with a learnable vote query driven object decoder, and a caption
decoder that produces the dense captions in a set-prediction manner. 2) In
contrast to the two-stage scheme, our method can perform detection and
captioning in one-stage. 3) Without bells and whistles, extensive experiments
on two commonly used datasets, ScanRefer and Nr3D, demonstrate that our
Vote2Cap-DETR surpasses current state-of-the-arts by 11.13\% and 7.11\% in
CIDEr@0.5IoU, respectively. Codes will be released soon.",None,-1
efacce17-b848-4ced-ba8b-06fd5ebca033,The Secret of Metaphor on Expressing Stronger Emotion,0.864665,"Metaphors are proven to have stronger emotional impact than literal
expressions. Although this conclusion is shown to be promising in benefiting
various NLP applications, the reasons behind this phenomenon are not well
studied. This paper conducts the first study in exploring how metaphors convey
stronger emotion than their literal counterparts. We find that metaphors are
generally more specific than literal expressions. The more specific property of
metaphor can be one of the reasons for metaphors' superiority in emotion
expression. When we compare metaphors with literal expressions with the same
specificity level, the gap of emotion expressing ability between both reduces
significantly. In addition, we observe specificity is crucial in literal
language as well, as literal language can express stronger emotion by making it
more specific.",None,-1
b9e23330-9182-48aa-a797-3077b019873d,Imbalanced Node Classification Beyond Homophilic Assumption,0.517457,"Imbalanced node classification widely exists in real-world networks where
graph neural networks (GNNs) are usually highly inclined to majority classes
and suffer from severe performance degradation on classifying minority class
nodes. Various imbalanced node classification methods have been proposed
recently which construct synthetic nodes and edges w.r.t. minority classes to
balance the label and topology distribution. However, they are all based on the
homophilic assumption that nodes of the same label tend to connect despite the
wide existence of heterophilic edges in real-world graphs. Thus, they uniformly
aggregate features from both homophilic and heterophilic neighbors and rely on
feature similarity to generate synthetic edges, which cannot be applied to
imbalanced graphs in high heterophily. To address this problem, we propose a
novel GraphSANN for imbalanced node classification on both homophilic and
heterophilic graphs. Firstly, we propose a unified feature mixer to generate
synthetic nodes with both homophilic and heterophilic interpolation in a
unified way. Next, by randomly sampling edges between synthetic nodes and
existing nodes as candidate edges, we design an adaptive subgraph extractor to
adaptively extract the contextual subgraphs of candidate edges with flexible
ranges. Finally, we develop a multi-filter subgraph encoder that constructs
different filter channels to discriminatively aggregate neighbor's information
along the homophilic and heterophilic edges. Extensive experiments on eight
datasets demonstrate the superiority of our model for imbalanced node
classification on both homophilic and heterophilic graphs.",None,-1
01c4a4ca-578c-430d-943e-5b7a6b4ed6cf,"LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages",0.140267,"Knowing the language of an input text/audio is a necessary first step for
using almost every NLP tool such as taggers, parsers, or translation systems.
Language identification is a well-studied problem, sometimes even considered
solved; in reality, due to lack of data and computational challenges, current
systems cannot accurately identify most of the world's 7000 languages. To
tackle this bottleneck, we first compile a corpus, MCS-350, of 50K multilingual
and parallel children's stories in 350+ languages. MCS-350 can serve as a
benchmark for language identification of short texts and for 1400+ new
translation directions in low-resource Indian and African languages. Second, we
propose a novel misprediction-resolution hierarchical model, LIMIt, for
language identification that reduces error by 55% (from 0.71 to 0.32) on our
compiled children's stories dataset and by 40% (from 0.23 to 0.14) on the
FLORES-200 benchmark. Our method can expand language identification coverage
into low-resource languages by relying solely on systemic misprediction
patterns, bypassing the need to retrain large models from scratch.",None,-1
0a0f1523-7b71-4a06-8beb-72da622d1efa,Text-to-SQL Error Correction with Language Models of Code,0.632121,"Despite recent progress in text-to-SQL parsing, current semantic parsers are
still not accurate enough for practical use. In this paper, we investigate how
to build automatic text-to-SQL error correction models. Noticing that
token-level edits are out of context and sometimes ambiguous, we propose
building clause-level edit models instead. Besides, while most language models
of code are not specifically pre-trained for SQL, they know common data
structures and their operations in programming languages such as Python. Thus,
we propose a novel representation for SQL queries and their edits that adheres
more closely to the pre-training corpora of language models of code. Our error
correction model improves the exact set match accuracy of different parsers by
2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong
baselines. Our code and data are available at
https://github.com/OSU-NLP-Group/Auto-SQL-Correction.",None,-1
398ad47a-9af1-4ab4-ad8e-eec143cdcb13,Multi-View Keypoints for Reliable 6D Object Pose Estimation,0.572846,"6D Object pose estimation is a fundamental component in robotics enabling
efficient interaction with the environment. It is particularly challenging in
bin-picking applications, where many objects are low-feature and reflective,
and self-occlusion between objects of the same type is common. We propose a
novel multi-view approach leveraging known camera transformations from an
eye-in-hand setup to combine heatmap and keypoint estimates into a probability
density map over 3D space. The result is a robust approach that is scalable in
the number of views. It relies on a confidence score composed of keypoint
probabilities and point-cloud alignment error, which allows reliable rejection
of false positives. We demonstrate an average pose estimation error of
approximately 0.5mm and 2 degrees across a variety of difficult low-feature and
reflective objects in the ROBI dataset, while also surpassing the state-of-art
correct detection rate, measured using the 10% object diameter threshold on ADD
error.",None,-1
33eabe1a-9d58-421c-9fbc-86e2764ba9a8,Granger-Causal Hierarchical Skill Discovery,0.0392876,"Reinforcement Learning (RL) has demonstrated promising results in learning
policies for complex tasks, but it often suffers from low sample efficiency and
limited transferability. Hierarchical RL (HRL) methods aim to address the
difficulty of learning long-horizon tasks by decomposing policies into skills,
abstracting states, and reusing skills in new tasks. However, many HRL methods
require some initial task success to discover useful skills, which
paradoxically may be very unlikely without access to useful skills. On the
other hand, reward-free HRL methods often need to learn far too many skills to
achieve proper coverage in high-dimensional domains. In contrast, we introduce
the Chain of Interaction Skills (COInS) algorithm, which focuses on
controllability in factored domains to identify a small number of task-agnostic
skills that still permit a high degree of control. COInS uses learned detectors
to identify interactions between state factors and then trains a chain of
skills to control each of these factors successively. We evaluate COInS on a
robotic pushing task with obstacles-a challenging domain where other RL and HRL
methods fall short. We also demonstrate the transferability of skills learned
by COInS, using variants of Breakout, a common RL benchmark, and show 2-3x
improvement in both sample efficiency and final performance compared to
standard RL baselines.",None,-1
9a592558-cb0a-4bdb-ad2b-670d17b8aca4,Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization,0.221994,"Existing vector quantization (VQ) based autoregressive models follow a
two-stage generation paradigm that first learns a codebook to encode images as
discrete codes, and then completes generation based on the learned codebook.
However, they encode fixed-size image regions into fixed-length codes and
ignore their naturally different information densities, which results in
insufficiency in important regions and redundancy in unimportant ones, and
finally degrades the generation quality and speed. Moreover, the fixed-length
coding leads to an unnatural raster-scan autoregressive generation. To address
the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization
VAE (DQ-VAE) which encodes image regions into variable-length codes based on
their information densities for an accurate and compact code representation.
(2) DQ-Transformer which thereby generates images autoregressively from
coarse-grained (smooth regions with fewer codes) to fine-grained (details
regions with more codes) by modeling the position and content of codes in each
granularity alternately, through a novel stacked-transformer architecture and
shared-content, non-shared position input layers designs. Comprehensive
experiments on various generation tasks validate our superiorities in both
effectiveness and efficiency. Code will be released at
https://github.com/CrossmodalGroup/DynamicVectorQuantization.",None,-1
b8fcd734-1071-4539-8208-eafd476f8970,Discriminative Deep Feature Visualization for Explainable Face Recognition,0.375596,"Despite the huge success of deep convolutional neural networks in face
recognition (FR) tasks, current methods lack explainability for their
predictions because of their ""black-box"" nature. In recent years, studies have
been carried out to give an interpretation of the decision of a deep FR system.
However, the affinity between the input facial image and the extracted deep
features has not been explored. This paper contributes to the problem of
explainable face recognition by first conceiving a face reconstruction-based
explanation module, which reveals the correspondence between the deep feature
and the facial regions. To further interpret the decision of an FR model, a
novel visual saliency explanation algorithm has been proposed. It provides
insightful explanation by producing visual saliency maps that represent similar
and dissimilar regions between input faces. A detailed analysis has been
presented for the generated visual explanation to show the effectiveness of the
proposed method.",None,-1
75874c14-a8d1-477d-bfa2-efa4a26fe71a,ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation,0.779296,"Joint speech-language training is challenging due to the large demand for
training data and GPU consumption, as well as the modality gap between speech
and language. We present ComSL, a speech-language model built atop a composite
architecture of public pretrained speech-only and language-only models and
optimized data-efficiently for spoken language tasks. Particularly, we propose
to incorporate cross-modality learning into transfer learning and conduct them
simultaneously for downstream tasks in a multi-task learning manner. Our
approach has demonstrated effectiveness in end-to-end speech-to-text
translation tasks, achieving a new state-of-the-art average BLEU score of 31.5
on the multilingual speech to English text translation task for 21 languages,
as measured on the public CoVoST2 evaluation set.",None,-1
316036a6-acb8-4f6b-bca2-3635688fd1ee,Inline Citation Classification using Peripheral Context and Time-evolving Augmentation,0.0572609,"Citation plays a pivotal role in determining the associations among research
articles. It portrays essential information in indicative, supportive, or
contrastive studies. The task of inline citation classification aids in
extrapolating these relationships; However, existing studies are still immature
and demand further scrutiny. Current datasets and methods used for inline
citation classification only use citation-marked sentences constraining the
model to turn a blind eye to domain knowledge and neighboring contextual
sentences. In this paper, we propose a new dataset, named 3Cext, which along
with the cited sentences, provides discourse information using the vicinal
sentences to analyze the contrasting and entailing relationships as well as
domain information. We propose PeriCite, a Transformer-based deep neural
network that fuses peripheral sentences and domain knowledge. Our model
achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best
baseline. We conduct extensive ablations to analyze the efficacy of the
proposed dataset and model fusion methods.",None,-1
3d74e05b-382c-4a6c-8469-94bb86b4ec5c,TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models,0.65292,"Data augmentation has been established as an efficacious approach to
supplement useful information for low-resource datasets. Traditional
augmentation techniques such as noise injection and image transformations have
been widely used. In addition, generative data augmentation (GDA) has been
shown to produce more diverse and flexible data. While generative adversarial
networks (GANs) have been frequently used for GDA, they lack diversity and
controllability compared to text-to-image diffusion models. In this paper, we
propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the
capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image
(T2I) generative models for data augmentation. By conditioning the T2I model on
detailed descriptions produced by T2T models, we are able to generate
photo-realistic labeled images in a flexible and controllable manner.
Experiments on in-domain classification, cross-domain classification, and image
captioning tasks show consistent improvements over other data augmentation
baselines. Analytical studies in varied settings, including few-shot,
long-tail, and adversarial, further reinforce the effectiveness of TTIDA in
enhancing performance and increasing robustness.",None,-1
10b0403a-11f7-4635-b6e6-ba57e04f1380,Continual Detection Transformer for Incremental Object Detection,0.706437,"Incremental object detection (IOD) aims to train an object detector in
phases, each with annotations for new object categories. As other incremental
settings, IOD is subject to catastrophic forgetting, which is often addressed
by techniques such as knowledge distillation (KD) and exemplar replay (ER).
However, KD and ER do not work well if applied directly to state-of-the-art
transformer-based object detectors such as Deformable DETR and UP-DETR. In this
paper, we solve these issues by proposing a ContinuaL DEtection TRansformer
(CL-DETR), a new method for transformer-based IOD which enables effective usage
of KD and ER in this context. First, we introduce a Detector Knowledge
Distillation (DKD) loss, focusing on the most informative and reliable
predictions from old versions of the model, ignoring redundant background
predictions, and ensuring compatibility with the available ground-truth labels.
We also improve ER by proposing a calibration strategy to preserve the label
distribution of the training set, therefore better matching training and
testing statistics. We conduct extensive experiments on COCO 2017 and
demonstrate that CL-DETR achieves state-of-the-art results in the IOD setting.",None,-1
9246689d-0f04-4f97-a2fd-28ebd7357e0a,The 1st-place Solution for CVPR 2023 OpenLane Topology in Autonomous Driving Challenge,0.238964,"We present the 1st-place solution of OpenLane Topology in Autonomous Driving
Challenge. Considering that topology reasoning is based on centerline detection
and traffic element detection, we develop a multi-stage framework for high
performance. Specifically, the centerline is detected by the powerful PETRv2
detector and the popular YOLOv8 is employed to detect the traffic elements.
Further, we design a simple yet effective MLP-based head for topology
prediction. Our method achieves 55\% OLS on the OpenLaneV2 test set, surpassing
the 2nd solution by 8 points.",None,-1
42fcddd4-e919-47d9-bec3-445869493d71,A Unified View of Evaluation Metrics for Structured Prediction,0.369884,"We present a conceptual framework that unifies a variety of evaluation
metrics for different structured prediction tasks (e.g. event and relation
extraction, syntactic and semantic parsing). Our framework requires
representing the outputs of these tasks as objects of certain data types, and
derives metrics through matching of common substructures, possibly followed by
normalization. We demonstrate how commonly used metrics for a number of tasks
can be succinctly expressed by this framework, and show that new metrics can be
naturally derived in a bottom-up way based on an output structure. We release a
library that enables this derivation to create new metrics. Finally, we
consider how specific characteristics of tasks motivate metric design
decisions, and suggest possible modifications to existing metrics in line with
those motivations.",None,-1
145714a0-6f94-4c42-b927-da12e49a7417,LE2Fusion: A novel local edge enhancement module for infrared and visible image fusion,0.13255,"Infrared and visible image fusion task aims to generate a fused image which
contains salient features and rich texture details from multi-source images.
However, under complex illumination conditions, few algorithms pay attention to
the edge information of local regions which is crucial for downstream tasks. To
this end, we propose a fusion network based on the local edge enhancement,
named LE2Fusion. Specifically, a local edge enhancement (LE2) module is
proposed to improve the edge information under complex illumination conditions
and preserve the essential features of image. For feature extraction, a
multi-scale residual attention (MRA) module is applied to extract rich
features. Then, with LE2, a set of enhancement weights are generated which are
utilized in feature fusion strategy and used to guide the image reconstruction.
To better preserve the local detail information and structure information, the
pixel intensity loss function based on the local region is also presented. The
experiments demonstrate that the proposed method exhibits better fusion
performance than the state-of-the-art fusion methods on public datasets.",None,-1
724faf02-7b3a-48bf-965f-97d5d083d192,Document Flattening: Beyond Concatenating Context for Document-Level Neural Machine Translation,0.901852,"Existing work in document-level neural machine translation commonly
concatenates several consecutive sentences as a pseudo-document, and then
learns inter-sentential dependencies. This strategy limits the model's ability
to leverage information from distant context. We overcome this limitation with
a novel Document Flattening (DocFlat) technique that integrates Flat-Batch
Attention (FBA) and Neural Context Gate (NCG) into Transformer model to utilize
information beyond the pseudo-document boundaries. FBA allows the model to
attend to all the positions in the batch and learns the relationships between
positions explicitly and NCG identifies the useful information from the distant
context. We conduct comprehensive experiments and analyses on three benchmark
datasets for English-German translation, and validate the effectiveness of two
variants of DocFlat. Empirical results show that our approach outperforms
strong baselines with statistical significance on BLEU, COMET and accuracy on
the contrastive test set. The analyses highlight that DocFlat is highly
effective in capturing the long-range information.",None,-1
ee0666d5-5ceb-4f89-b8d2-7db84f07f11b,Deep Learning Mental Health Dialogue System,0.942598,"Mental health counseling remains a major challenge in modern society due to
cost, stigma, fear, and unavailability. We posit that generative artificial
intelligence (AI) models designed for mental health counseling could help
improve outcomes by lowering barriers to access. To this end, we have developed
a deep learning (DL) dialogue system called Serena. The system consists of a
core generative model and post-processing algorithms. The core generative model
is a 2.7 billion parameter Seq2Seq Transformer fine-tuned on thousands of
transcripts of person-centered-therapy (PCT) sessions. The series of
post-processing algorithms detects contradictions, improves coherency, and
removes repetitive answers. Serena is implemented and deployed on
\url{https://serena.chat}, which currently offers limited free services. While
the dialogue system is capable of responding in a qualitatively empathetic and
engaging manner, occasionally it displays hallucination and long-term
incoherence. Overall, we demonstrate that a deep learning mental health
dialogue system has the potential to provide a low-cost and effective
complement to traditional human counselors with less barriers to access.",None,-1
906f2db3-8175-4d1b-94d0-219003156021,Spherical Transformer for LiDAR-based 3D Recognition,0.999911,"LiDAR-based 3D point cloud recognition has benefited various applications.
Without specially considering the LiDAR point distribution, most current
methods suffer from information disconnection and limited receptive field,
especially for the sparse distant points. In this work, we study the
varying-sparsity distribution of LiDAR points and present SphereFormer to
directly aggregate information from dense close points to the sparse distant
ones. We design radial window self-attention that partitions the space into
multiple non-overlapping narrow and long windows. It overcomes the
disconnection issue and enlarges the receptive field smoothly and dramatically,
which significantly boosts the performance of sparse distant points. Moreover,
to fit the narrow and long windows, we propose exponential splitting to yield
fine-grained position encoding and dynamic feature selection to increase model
representation ability. Notably, our method ranks 1st on both nuScenes and
SemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU,
respectively. Also, we achieve the 3rd place on nuScenes object detection
benchmark with 72.8% NDS and 68.5% mAP. Code is available at
https://github.com/dvlab-research/SphereFormer.git.",None,-1
1082180b-9fc7-479c-b7e8-e735fa9301a1,MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model,0.900333,"Due to patient privacy protection concerns, machine learning research in
healthcare has been undeniably slower and limited than in other application
domains. High-quality, realistic, synthetic electronic health records (EHRs)
can be leveraged to accelerate methodological developments for research
purposes while mitigating privacy concerns associated with data sharing. The
current state-of-the-art model for synthetic EHR generation is generative
adversarial networks, which are notoriously difficult to train and can suffer
from mode collapse. Denoising Diffusion Probabilistic Models, a class of
generative models inspired by statistical thermodynamics, have recently been
shown to generate high-quality synthetic samples in certain domains. It is
unknown whether these can generalize to generation of large-scale,
high-dimensional EHRs. In this paper, we present a novel generative model based
on diffusion models that is the first successful application on electronic
health records. Our model proposes a mechanism to perform class-conditional
sampling to preserve label information. We also introduce a new sampling
strategy to accelerate the inference speed. We empirically show that our model
outperforms existing state-of-the-art synthetic EHR generation methods.",None,-1
afe46827-c928-4015-a666-2eff23330e4b,GarmentTracking: Category-Level Garment Pose Tracking,0.741504,"Garments are important to humans. A visual system that can estimate and track
the complete garment pose can be useful for many downstream tasks and
real-world applications. In this work, we present a complete package to address
the category-level garment pose tracking task: (1) A recording system
VR-Garment, with which users can manipulate virtual garment models in
simulation through a VR interface. (2) A large-scale dataset VR-Folding, with
complex garment pose configurations in manipulation like flattening and
folding. (3) An end-to-end online tracking framework GarmentTracking, which
predicts complete garment pose both in canonical space and task space given a
point cloud sequence. Extensive experiments demonstrate that the proposed
GarmentTracking achieves great performance even when the garment has large
non-rigid deformation. It outperforms the baseline approach on both speed and
accuracy. We hope our proposed solution can serve as a platform for future
research. Codes and datasets are available in
https://garment-tracking.robotflow.ai.",None,-1
87023de7-6dfa-499a-8b89-ea4f0506f4d0,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,0.701527,"In this paper, we propose CGI-Stereo, a novel neural network architecture
that can concurrently achieve real-time performance, competitive accuracy, and
strong generalization ability. The core of our CGI-Stereo is a Context and
Geometry Fusion (CGF) block which adaptively fuses context and geometry
information for more effective cost aggregation and meanwhile provides feedback
to feature learning to guide more effective contextual feature extraction. The
proposed CGF can be easily embedded into many existing stereo matching
networks, such as PSMNet, GwcNet and ACVNet. The resulting networks show a
significant improvement in accuracy. Specially, the model which incorporates
our CGF with ACVNet ranks $1^{st}$ on the KITTI 2012 and 2015 leaderboards
among all the published methods. We further propose an informative and concise
cost volume, named Attention Feature Volume (AFV), which exploits a correlation
volume as attention weights to filter a feature volume. Based on CGF and AFV,
the proposed CGI-Stereo outperforms all other published real-time methods on
KITTI benchmarks and shows better generalization ability than other real-time
methods. Code is available at https://github.com/gangweiX/CGI-Stereo.",None,-1
8332a29e-b0d3-4fca-9768-158739dc76e6,"knn-seq: Efficient, Extensible kNN-MT Framework",0.263336,"k-nearest-neighbor machine translation (kNN-MT) boosts the translation
quality of a pre-trained neural machine translation (NMT) model by utilizing
translation examples during decoding. Translation examples are stored in a
vector database, called a datastore, which contains one entry for each target
token from the parallel data it is made from. Due to its size, it is
computationally expensive both to construct and to retrieve examples from the
datastore. In this paper, we present an efficient and extensible kNN-MT
framework, knn-seq, for researchers and developers that is carefully designed
to run efficiently, even with a billion-scale large datastore. knn-seq is
developed as a plug-in on fairseq and easy to switch models and kNN indexes.
Experimental results show that our implemented kNN-MT achieves a comparable
gain to the original kNN-MT, and the billion-scale datastore construction took
2.21 hours in the WMT'19 German-to-English translation task. We publish our
knn-seq as an MIT-licensed open-source project and the code is available on
https://github.com/naist-nlp/knn-seq . The demo video is available on
https://youtu.be/zTDzEOq80m0 .",None,-1
97a23dce-67bc-4eb8-a4f5-e4ac63b8b27d,Diffusion Models for Imperceptible and Transferable Adversarial Attack,0.831842,"Many existing adversarial attacks generate $L_p$-norm perturbations on image
RGB space. Despite some achievements in transferability and attack success
rate, the crafted adversarial examples are easily perceived by human eyes.
Towards visual imperceptibility, some recent works explore unrestricted attacks
without $L_p$-norm constraints, yet lacking transferability of attacking
black-box models. In this work, we propose a novel imperceptible and
transferable attack by leveraging both the generative and discriminative power
of diffusion models. Specifically, instead of direct manipulation in pixel
space, we craft perturbations in the latent space of diffusion models. Combined
with well-designed content-preserving structures, we can generate
human-insensitive perturbations embedded with semantic clues. For better
transferability, we further ""deceive"" the diffusion model which can be viewed
as an implicit recognition surrogate, by distracting its attention away from
the target regions. To our knowledge, our proposed method, DiffAttack, is the
first that introduces diffusion models into the adversarial attack field.
Extensive experiments on various model structures, datasets, and defense
methods have demonstrated the superiority of our attack over the existing
attack methods.",None,-1
9a9cb5a5-3f6a-48ab-b479-9a3548a38437,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,0.890582,"This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)
framework. It emphasizes spatio-temporal continuity and integrates both past
and future reasoning for tracked objects. Thus, we name it ""Past-and-Future
reasoning for Tracking"" (PF-Track). Specifically, our method adapts the
""tracking by attention"" framework and represents tracked instances coherently
over time with object queries. To explicitly use historical cues, our ""Past
Reasoning"" module learns to refine the tracks and enhance the object features
by cross-attending to queries from previous frames and other objects. The
""Future Reasoning"" module digests historical information and predicts robust
future trajectories. In the case of long-term occlusions, our method maintains
the object positions and enables re-association by integrating motion
predictions. On the nuScenes dataset, our method improves AMOTA by a large
margin and remarkably reduces ID-Switches by 90% compared to prior approaches,
which is an order of magnitude less. The code and models are made available at
https://github.com/TRI-ML/PF-Track.",None,-1
cc29f938-f803-4cf2-8292-3b17aa178137,GlotScript: A Resource and Tool for Low Resource Writing System Identification,0.984707,"We present GlotScript, an open resource and tool for low resource writing
system identification. GlotScript-R is a resource that provides the attested
writing systems for more than 7,000 languages. It is compiled by aggregating
information from existing writing system resources. GlotScript-T is a writing
system identification tool that covers all 161 Unicode 15.0 scripts. For an
input text, it returns its script distribution where scripts are identified by
ISO 15924 codes. We also present two use cases for GlotScript. First, we
demonstrate that GlotScript can help cleaning multilingual corpora such as mC4
and OSCAR. Second, we analyze the tokenization of a number of language models
such as GPT-4 using GlotScript and provide insights on the coverage of low
resource scripts and languages by each language model. We hope that GlotScript
will become a useful resource for work on low resource languages in the NLP
community. GlotScript-R and GlotScript-T are available at
https://github.com/cisnlp/GlotScript.",None,-1
f545c4a2-1b74-400c-8978-06e4ee200b7b,Data Quality-aware Mixed-precision Quantization via Hybrid Reinforcement Learning,0.905327,"Mixed-precision quantization mostly predetermines the model bit-width
settings before actual training due to the non-differential bit-width sampling
process, obtaining sub-optimal performance. Worse still, the conventional
static quality-consistent training setting, i.e., all data is assumed to be of
the same quality across training and inference, overlooks data quality changes
in real-world applications which may lead to poor robustness of the quantized
models. In this paper, we propose a novel Data Quality-aware Mixed-precision
Quantization framework, dubbed DQMQ, to dynamically adapt quantization
bit-widths to different data qualities. The adaption is based on a bit-width
decision policy that can be learned jointly with the quantization training.
Concretely, DQMQ is modeled as a hybrid reinforcement learning (RL) task that
combines model-based policy optimization with supervised quantization training.
By relaxing the discrete bit-width sampling to a continuous probability
distribution that is encoded with few learnable parameters, DQMQ is
differentiable and can be directly optimized end-to-end with a hybrid
optimization target considering both task performance and quantization
benefits. Trained on mixed-quality image datasets, DQMQ can implicitly select
the most proper bit-width for each layer when facing uneven input qualities.
Extensive experiments on various benchmark datasets and networks demonstrate
the superiority of DQMQ against existing fixed/mixed-precision quantization
methods.",None,-1
2dee8712-b8af-46e3-96c2-be6daa083532,Fauno: The Italian Large Language Model that will leave you senza parole!,0.58143,"This paper presents Fauno, the first and largest open-source Italian
conversational Large Language Model (LLM). Our goal with Fauno is to
democratize the study of LLMs in Italian, demonstrating that obtaining a
fine-tuned conversational bot with a single GPU is possible. In addition, we
release a collection of datasets for conversational AI in Italian. The datasets
on which we fine-tuned Fauno include various topics such as general question
answering, computer science, and medical questions. We release our code and
datasets on \url{https://github.com/RSTLess-research/Fauno-Italian-LLM}",None,-1
f96f1af9-4cce-4d3d-8736-b8e9c84eba4b,Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation,0.833826,"The left ventricular of ejection fraction is one of the most important metric
of cardiac function. It is used by cardiologist to identify patients who are
eligible for lifeprolonging therapies. However, the assessment of ejection
fraction suffers from inter-observer variability. To overcome this challenge,
we propose a deep learning approach, based on hierarchical vision Transformers,
to estimate the ejection fraction from echocardiogram videos. The proposed
method can estimate ejection fraction without the need for left ventrice
segmentation first, make it more efficient than other methods. We evaluated our
method on EchoNet-Dynamic dataset resulting 5.59, 7.59 and 0.59 for MAE, RMSE
and R2 respectivelly. This results are better compared to the state-of-the-art
method, Ultrasound Video Transformer (UVT). The source code is available on
https://github.com/lhfazry/UltraSwin.",None,-1
6f756058-8e17-47b6-9425-ed8ef1ab7436,EventCLIP: Adapting CLIP for Event-based Object Recognition,0.644146,"Recent advances in zero-shot and few-shot classification heavily rely on the
success of pre-trained vision-language models (VLMs) such as CLIP. Due to a
shortage of large-scale datasets, training such models for event camera data
remains infeasible. Thus, adapting existing VLMs across modalities to event
vision is an important research challenge. In this work, we introduce
EventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot
event-based object recognition. We first generalize CLIP's image encoder to
event data by converting raw events to 2D grid-based representations. To
further enhance performance, we propose a feature adapter to aggregate temporal
information over event frames and refine text embeddings to better align with
the visual inputs. We evaluate EventCLIP on N-Caltech, N-Cars, and N-ImageNet
datasets, achieving state-of-the-art few-shot performance. When fine-tuned on
the entire dataset, our method outperforms all existing event classifiers.
Moreover, we explore practical applications of EventCLIP including robust event
classification and label-free event recognition, where our approach surpasses
previous baselines designed specifically for these tasks.",None,-1
f76d584f-87cf-447b-9b6b-ce39d068a302,Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques,0.706398,"Compositional and domain generalization present significant challenges in
semantic parsing, even for state-of-the-art semantic parsers based on
pre-trained language models (LMs). In this study, we empirically investigate
improving an LM's generalization in semantic parsing with two simple
techniques: at the token level, we introduce a token preprocessing method to
preserve the semantic boundaries of tokens produced by LM tokenizers; at the
sequence level, we propose to use special tokens to mark the boundaries of
components aligned between input and output. Our experimental results on two
text-to-SQL semantic parsing datasets show that our token preprocessing,
although simple, can substantially improve the LM performance on both types of
generalization, and our component boundary marking method is particularly
helpful for compositional generalization.",None,-1
8ef3a02d-a468-4dc2-90ff-9374dbc44cfa,Bipartite Graph Diffusion Model for Human Interaction Generation,0.601755,"The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.",None,-1
c2192a1d-e5ed-4c14-917e-af103daa271c,Explore the difficulty of words and its influential attributes based on the Wordle game,0.681788,"We adopt the distribution and expectation of guessing times in game Wordle as
metrics to predict the difficulty of words and explore their influence factors.
In order to predictthe difficulty distribution, we use Monte Carlo to simulate
the guessing process of players and then narrow the gap between raw and actual
distribution of guessing times for each word with Markov which generates the
associativity of words. Afterwards, we take advantage of lasso regression to
predict the deviation of guessing times expectation and quadratic programming
to obtain the correction of the original distribution.To predict the difficulty
levels, we first use hierarchical clustering to classify the difficulty levels
based on the expectation of guessing times. Afterwards we downscale the
variables of lexical attributes based on factor analysis. Significant factors
include the number of neighboring words, letter similarity, sub-string
similarity, and word frequency. Finally, we build the relationship between
lexical attributes and difficulty levels through ordered logistic regression.",None,-1
2cf5bcc5-77ed-4b40-802d-92e3a924bfc8,VITAL: Vision Transformer Neural Networks for Accurate Smartphone Heterogeneity Resilient Indoor Localization,0.760349,"Wi-Fi fingerprinting-based indoor localization is an emerging embedded
application domain that leverages existing Wi-Fi access points (APs) in
buildings to localize users with smartphones. Unfortunately, the heterogeneity
of wireless transceivers across diverse smartphones carried by users has been
shown to reduce the accuracy and reliability of localization algorithms. In
this paper, we propose a novel framework based on vision transformer neural
networks called VITAL that addresses this important challenge. Experiments
indicate that VITAL can reduce the uncertainty created by smartphone
heterogeneity while improving localization accuracy from 41% to 68% over the
best-known prior works. We also demonstrate the generalizability of our
approach and propose a data augmentation technique that can be integrated into
most deep learning-based localization frameworks to improve accuracy.",None,-1
97432ffc-5a3e-42f2-849f-2420edd4aeca,New Perspectives on Regularization and Computation in Optimal Transport-Based Distributionally Robust Optimization,0.975029,"We study optimal transport-based distributionally robust optimization
problems where a fictitious adversary, often envisioned as nature, can choose
the distribution of the uncertain problem parameters by reshaping a prescribed
reference distribution at a finite transportation cost. In this framework, we
show that robustification is intimately related to various forms of variation
and Lipschitz regularization even if the transportation cost function fails to
be (some power of) a metric. We also derive conditions for the existence and
the computability of a Nash equilibrium between the decision-maker and nature,
and we demonstrate numerically that nature's Nash strategy can be viewed as a
distribution that is supported on remarkably deceptive adversarial samples.
Finally, we identify practically relevant classes of optimal transport-based
distributionally robust optimization problems that can be addressed with
efficient gradient descent algorithms even if the loss function or the
transportation cost function are nonconvex (but not both at the same time).",None,-1
e300e96f-fc87-4a05-9549-dcaca40ca812,Self-Sufficient Framework for Continuous Sign Language Recognition,0.908278,"The goal of this work is to develop self-sufficient framework for Continuous
Sign Language Recognition (CSLR) that addresses key issues of sign language
recognition. These include the need for complex multi-scale features such as
hands, face, and mouth for understanding, and absence of frame-level
annotations. To this end, we propose (1) Divide and Focus Convolution (DFConv)
which extracts both manual and non-manual features without the need for
additional networks or annotations, and (2) Dense Pseudo-Label Refinement
(DPLR) which propagates non-spiky frame-level pseudo-labels by combining the
ground truth gloss sequence labels with the predicted sequence. We demonstrate
that our model achieves state-of-the-art performance among RGB-based methods on
large-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing
comparable results with better efficiency when compared to other approaches
that use multi-modality or extra annotations.",None,-1
206fc83f-8931-46c0-b4a2-13167ec29514,Entity Tracking in Language Models,0.39283,"Keeping track of how states of entities change as a text or dialog unfolds is
a key prerequisite to discourse understanding. Yet, there have been few
systematic investigations into the ability of large language models (LLMs) to
track discourse entities. In this work, we present a task probing to what
extent a language model can infer the final state of an entity given an English
description of the initial state and a series of state-changing operations. We
use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track
the state of entities, and find that only GPT-3.5 models, which have been
pretrained on large amounts of code, exhibit this ability. We then investigate
whether smaller models pretrained primarily on text can learn to track
entities, through finetuning T5 on several training/evaluation splits. While
performance degrades for more complex splits, we find that even when evaluated
on a different set of entities from training or longer operation sequences, a
finetuned model can perform non-trivial entity tracking. Taken together, these
results suggest that language models can learn to track entities but
pretraining on text corpora alone does not make this capacity surface.",None,-1
42e70fe5-622d-45e6-bb40-04b72f981478,Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference!,0.16228,"Diffusion models have been successfully adapted to text generation tasks by
mapping the discrete text into the continuous space. However, there exist
nonnegligible gaps between training and inference, owing to the absence of the
forward process during inference. Thus, the model only predicts based on the
previously generated reverse noise rather than the noise computed by the
forward process. Besides, the widely-used downsampling strategy in speeding up
the inference will cause the mismatch of diffusion trajectories between
training and inference. To understand and mitigate the above two types of
training-inference discrepancies, we launch a thorough preliminary study. Based
on our observations, we propose two simple yet effective methods to bridge the
gaps mentioned above, named Distance Penalty and Adaptive Decay Sampling.
Extensive experiments on \textbf{6} generation tasks confirm the superiority of
our methods, which can achieve $100\times \rightarrow 200\times$ speedup with
better performance.",None,-1
2cf08edc-01e2-46ce-bda9-cf3a21da1a2d,Learning to Compress Prompts with Gist Tokens,0.774314,"Prompting is the primary way to utilize the multitask capabilities of
language models (LMs), but prompts occupy valuable space in the input context
window, and repeatedly encoding the same prompt is computationally inefficient.
Finetuning and distillation methods allow for specialization of LMs without
prompting, but require retraining the model for each task. To avoid this
trade-off entirely, we present gisting, which trains an LM to compress prompts
into smaller sets of ""gist"" tokens which can be cached and reused for compute
efficiency. Gist models can be trained with no additional cost over standard
instruction finetuning by simply modifying Transformer attention masks to
encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder
(FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting
in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings,
all with minimal loss in output quality.",None,-1
adcf6e75-e972-4bc7-936b-b3fd8164213d,A Benchmark for Chinese-English Scene Text Image Super-resolution,0.250377,"Scene Text Image Super-resolution (STISR) aims to recover high-resolution
(HR) scene text images with visually pleasant and readable text content from
the given low-resolution (LR) input. Most existing works focus on recovering
English texts, which have relatively simple character structures, while little
work has been done on the more challenging Chinese texts with diverse and
complex character structures. In this paper, we propose a real-world
Chinese-English benchmark dataset, namely Real-CE, for the task of STISR with
the emphasis on restoring structurally complex Chinese characters. The
benchmark provides 1,935/783 real-world LR-HR text image pairs~(contains 33,789
text lines in total) for training/testing in 2$\times$ and 4$\times$ zooming
modes, complemented by detailed annotations, including detection boxes and text
transcripts. Moreover, we design an edge-aware learning method, which provides
structural supervision in image and feature domains, to effectively reconstruct
the dense structures of Chinese characters. We conduct experiments on the
proposed Real-CE benchmark and evaluate the existing STISR models with and
without our edge-aware loss. The benchmark, including data and source code, is
available at https://github.com/mjq11302010044/Real-CE.",None,-1
0d31f764-9559-4372-b8b4-02ae09295b64,Anytime Approximate Formal Feature Attribution,0.631019,"Widespread use of artificial intelligence (AI) algorithms and machine
learning (ML) models on the one hand and a number of crucial issues pertaining
to them warrant the need for explainable artificial intelligence (XAI). A key
explainability question is: given this decision was made, what are the input
features which contributed to the decision? Although a range of XAI approaches
exist to tackle this problem, most of them have significant limitations.
Heuristic XAI approaches suffer from the lack of quality guarantees, and often
try to approximate Shapley values, which is not the same as explaining which
features contribute to a decision. A recent alternative is so-called formal
feature attribution (FFA), which defines feature importance as the fraction of
formal abductive explanations (AXp's) containing the given feature. This
measures feature importance from the view of formally reasoning about the
model's behavior. It is challenging to compute FFA using its definition because
that involves counting AXp's, although one can approximate it. Based on these
results, this paper makes several contributions. First, it gives compelling
evidence that computing FFA is intractable, even if the set of contrastive
formal explanations (CXp's) is provided, by proving that the problem is
#P-hard. Second, by using the duality between AXp's and CXp's, it proposes an
efficient heuristic to switch from CXp enumeration to AXp enumeration
on-the-fly resulting in an adaptive explanation enumeration algorithm
effectively approximating FFA in an anytime fashion. Finally, experimental
results obtained on a range of widely used datasets demonstrate the
effectiveness of the proposed FFA approximation approach in terms of the error
of FFA approximation as well as the number of explanations computed and their
diversity given a fixed time limit.",None,-1
180521c7-9b2e-47f1-bb98-133c5c881458,Topological Data Analysis Guided Segment Anything Model Prompt Optimization for Zero-Shot Segmentation in Biological Imaging,0.0371441,"Emerging foundation models in machine learning are models trained on vast
amounts of data that have been shown to generalize well to new tasks. Often
these models can be prompted with multi-modal inputs that range from natural
language descriptions over images to point clouds. In this paper, we propose
topological data analysis (TDA) guided prompt optimization for the Segment
Anything Model (SAM) and show preliminary results in the biological image
segmentation domain. Our approach replaces the standard grid search approach
that is used in the original implementation and finds point locations based on
their topological significance. Our results show that the TDA optimized point
cloud is much better suited for finding small objects and massively reduces
computational complexity despite the extra step in scenarios which require many
segmentations.",None,-1
95693b8c-3cf0-4bb1-891a-9ddd16654a66,A Case-Based Persistent Memory for a Large Language Model,0.0109463,"Case-based reasoning (CBR) as a methodology for problem-solving can use any
appropriate computational technique. This position paper argues that CBR
researchers have somewhat overlooked recent developments in deep learning and
large language models (LLMs). The underlying technical developments that have
enabled the recent breakthroughs in AI have strong synergies with CBR and could
be used to provide a persistent memory for LLMs to make progress towards
Artificial General Intelligence.",None,-1
63759177-45a0-4f14-8be3-52fa2e9432b2,Domain-Specific Fine-Tuning of Large Language Models for Interactive Robot Programming,0.219173,"Industrial robots are applied in a widening range of industries, but robot
programming mostly remains a task limited to programming experts. We propose a
natural language-based assistant for programming of advanced, industrial
robotic applications and investigate strategies for domain-specific fine-tuning
of foundation models with limited data and compute.",None,-1
390e4f97-3bfa-4d05-b25c-581489fd5222,Rethinking the BERT-like Pretraining for DNA Sequences,0.692554,"With the success of large-scale pretraining in NLP, there is an increasing
trend of applying it to the domain of life sciences. In particular, pretraining
methods based on DNA sequences have garnered growing attention due to their
potential to capture generic information about genes. However, existing
pretraining methods for DNA sequences largely rely on direct adoptions of BERT
pretraining from NLP, lacking a comprehensive understanding and a specifically
tailored approach. To address this research gap, we first conducted a series of
exploratory experiments and gained several insightful observations: 1) In the
fine-tuning phase of downstream tasks, when using K-mer overlapping
tokenization instead of K-mer non-overlapping tokenization, both overlapping
and non-overlapping pretraining weights show consistent performance
improvement.2) During the pre-training process, using K-mer overlapping
tokenization quickly produces clear K-mer embeddings and reduces the loss to a
very low level, while using K-mer non-overlapping tokenization results in less
distinct embeddings and continuously decreases the loss. 3) Using overlapping
tokenization causes the self-attention in the intermediate layers of
pre-trained models to tend to overly focus on certain tokens, reflecting that
these layers are not adequately optimized. In summary, overlapping tokenization
can benefit the fine-tuning of downstream tasks but leads to inadequate
pretraining with fast convergence. To unleash the pretraining potential, we
introduce a novel approach called RandomMask, which gradually increases the
task difficulty of BERT-like pretraining by continuously expanding its mask
boundary, forcing the model to learn more knowledge. RandomMask is simple but
effective, achieving top-tier performance across 26 datasets of 28 datasets
spanning 7 downstream tasks.",None,-1
50070c78-5b7b-4897-b0b1-30641a5debbc,Process Mining for Unstructured Data: Challenges and Research Directions,0.447108,"The application of process mining for unstructured data might significantly
elevate novel insights into disciplines where unstructured data is a common
data format. To efficiently analyze unstructured data by process mining and to
convey confidence into the analysis result, requires bridging multiple
challenges. The purpose of this paper is to discuss these challenges, present
initial solutions and describe future research directions. We hope that this
article lays the foundations for future collaboration on this topic.",None,-1
49b3f04c-ffb4-4efd-94f4-4558e7c7aaca,QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration,0.475077,"QDax is an open-source library with a streamlined and modular API for
Quality-Diversity (QD) optimization algorithms in Jax. The library serves as a
versatile tool for optimization purposes, ranging from black-box optimization
to continuous control. QDax offers implementations of popular QD,
Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by
various examples. All the implementations can be just-in-time compiled with
Jax, facilitating efficient execution across multiple accelerators, including
GPUs and TPUs. These implementations effectively demonstrate the framework's
flexibility and user-friendliness, easing experimentation for research
purposes. Furthermore, the library is thoroughly documented and tested with
95\% coverage.",None,-1
b3456b05-8b4c-488e-8cbb-a75427ab116d,How are Prompts Different in Terms of Sensitivity?,0.268575,"In-context learning (ICL) has become one of the most popular learning
paradigms. While there is a growing body of literature focusing on prompt
engineering, there is a lack of systematic analysis comparing the effects of
prompts across different models and tasks. To address this gap, we present a
comprehensive prompt analysis based on the sensitivity of a function. Our
analysis reveals that sensitivity is an unsupervised proxy for model
performance, as it exhibits a strong negative correlation with accuracy. We use
gradient-based saliency scores to empirically demonstrate how different prompts
affect the relevance of input tokens to the output, resulting in different
levels of sensitivity. Furthermore, we introduce sensitivity-aware decoding
which incorporates sensitivity estimation as a penalty term in the standard
greedy decoding. We show that this approach is particularly helpful when
information in the input is scarce. Our work provides a fresh perspective on
the analysis of prompts, and contributes to a better understanding of the
mechanism of ICL.",None,-1
7643ce67-347a-4820-99f6-c5c746c0bcca,Learning and Aggregating Lane Graphs for Urban Automated Driving,0.894731,"Lane graph estimation is an essential and highly challenging task in
automated driving and HD map learning. Existing methods using either onboard or
aerial imagery struggle with complex lane topologies, out-of-distribution
scenarios, or significant occlusions in the image space. Moreover, merging
overlapping lane graphs to obtain consistent large-scale graphs remains
difficult. To overcome these challenges, we propose a novel bottom-up approach
to lane graph estimation from aerial imagery that aggregates multiple
overlapping graphs into a single consistent graph. Due to its modular design,
our method allows us to address two complementary tasks: predicting
ego-respective successor lane graphs from arbitrary vehicle positions using a
graph neural network and aggregating these predictions into a consistent global
lane graph. Extensive experiments on a large-scale lane graph dataset
demonstrate that our approach yields highly accurate lane graphs, even in
regions with severe occlusions. The presented approach to graph aggregation
proves to eliminate inconsistent predictions while increasing the overall graph
quality. We make our large-scale urban lane graph dataset and code publicly
available at http://urbanlanegraph.cs.uni-freiburg.de.",None,-1
baee610c-6fcd-43cf-ba97-582e75d0a995,Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games,0.663078,"We introduce a class of networked Markov potential games in which agents are
associated with nodes in a network. Each agent has its own local potential
function, and the reward of each agent depends only on the states and actions
of the agents within a neighborhood. In this context, we propose a localized
actor-critic algorithm. The algorithm is scalable since each agent uses only
local information and does not need access to the global state. Further, the
algorithm overcomes the curse of dimensionality through the use of function
approximation. Our main results provide finite-sample guarantees up to a
localization error and a function approximation error. Specifically, we achieve
an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by
the averaged Nash regret. This is the first finite-sample bound for multi-agent
competitive games that does not depend on the number of agents.",None,-1
f850ff96-7e59-4d95-8524-f84e734f58ed,Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations,0.465649,"The rapid development and application of foundation models have
revolutionized the field of artificial intelligence. Large diffusion models
have gained significant attention for their ability to generate photorealistic
images and support various tasks. On-device deployment of these models provides
benefits such as lower server costs, offline functionality, and improved user
privacy. However, common large diffusion models have over 1 billion parameters
and pose challenges due to restricted computational and memory resources on
devices. We present a series of implementation optimizations for large
diffusion models that achieve the fastest reported inference latency to-date
(under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung
S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile
devices. These enhancements broaden the applicability of generative AI and
improve the overall user experience across a wide range of devices.",None,-1
3616370b-6143-4a21-b483-b573586016c8,Towards Self-Explainability of Deep Neural Networks with Heatmap Captioning and Large-Language Models,0.506483,"Heatmaps are widely used to interpret deep neural networks, particularly for
computer vision tasks, and the heatmap-based explainable AI (XAI) techniques
are a well-researched topic. However, most studies concentrate on enhancing the
quality of the generated heatmap or discovering alternate heatmap generation
techniques, and little effort has been devoted to making heatmap-based XAI
automatic, interactive, scalable, and accessible. To address this gap, we
propose a framework that includes two modules: (1) context modelling and (2)
reasoning. We proposed a template-based image captioning approach for context
modelling to create text-based contextual information from the heatmap and
input data. The reasoning module leverages a large language model to provide
explanations in combination with specialised knowledge. Our qualitative
experiments demonstrate the effectiveness of our framework and heatmap
captioning approach. The code for the proposed template-based heatmap
captioning approach will be publicly available.",None,-1
54644710-1976-45c9-afbd-35c7294ba92a,LightGlue: Local Feature Matching at Light Speed,1.0,"We introduce LightGlue, a deep neural network that learns to match local
features across images. We revisit multiple design decisions of SuperGlue, the
state of the art in sparse matching, and derive simple but effective
improvements. Cumulatively, they make LightGlue more efficient - in terms of
both memory and computation, more accurate, and much easier to train. One key
property is that LightGlue is adaptive to the difficulty of the problem: the
inference is much faster on image pairs that are intuitively easy to match, for
example because of a larger visual overlap or limited appearance change. This
opens up exciting prospects for deploying deep matchers in latency-sensitive
applications like 3D reconstruction. The code and trained models are publicly
available at https://github.com/cvg/LightGlue.",None,-1
31c296e5-603c-4713-bd5a-a76b7449c2a9,Wasserstein Actor-Critic: Directed Exploration via Optimism for Continuous-Actions Control,0.0292539,"Uncertainty quantification has been extensively used as a means to achieve
efficient directed exploration in Reinforcement Learning (RL). However,
state-of-the-art methods for continuous actions still suffer from high sample
complexity requirements. Indeed, they either completely lack strategies for
propagating the epistemic uncertainty throughout the updates, or they mix it
with aleatoric uncertainty while learning the full return distribution (e.g.,
distributional RL). In this paper, we propose Wasserstein Actor-Critic (WAC),
an actor-critic architecture inspired by the recent Wasserstein Q-Learning
(WQL) \citep{wql}, that employs approximate Q-posteriors to represent the
epistemic uncertainty and Wasserstein barycenters for uncertainty propagation
across the state-action space. WAC enforces exploration in a principled way by
guiding the policy learning process with the optimization of an upper bound of
the Q-value estimates. Furthermore, we study some peculiar issues that arise
when using function approximation, coupled with the uncertainty estimation, and
propose a regularized loss for the uncertainty estimation. Finally, we evaluate
our algorithm on standard MujoCo tasks as well as suite of continuous-actions
domains, where exploration is crucial, in comparison with state-of-the-art
baselines.",None,-1
69e1b657-08f7-415e-baeb-9b8ceed7c099,A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,0.435382,"Logical reasoning has been an ongoing pursuit in the field of AI. Despite
significant advancements made by large language models (LLMs), they still
struggle with complex logical reasoning problems. To enhance reasoning
performance, one promising direction is scalable oversight, which requires LLMs
to identify their own errors and then improve by themselves. Various
self-verification methods have been proposed in pursuit of this goal.
Nevertheless, whether existing models understand their own errors well is still
under investigation. In this paper, we take a closer look at the
self-verification abilities of LLMs in the context of logical reasoning,
focusing on their ability to identify logical fallacies accurately. We
introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies
categorized in a hierarchical taxonomy. By conducting exhaustive experiments on
FALLACIES, we obtain comprehensive and detailed analyses of a series of models
on their verification abilities. Our main findings suggest that existing LLMs
could struggle to identify fallacious reasoning steps accurately and may fall
short of guaranteeing the validity of self-verification methods. Drawing from
these observations, we offer suggestions for future research and practical
applications of self-verification methods.",None,-1
3cc9358c-be8c-46fa-9ce0-fed3e151f358,DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation,0.482733,"Empathy is a crucial factor in open-domain conversations, which naturally
shows one's caring and understanding to others. Though several methods have
been proposed to generate empathetic responses, existing works often lead to
monotonous empathy that refers to generic and safe expressions. In this paper,
we propose to use explicit control to guide the empathy expression and design a
framework DiffusEmp based on conditional diffusion language model to unify the
utilization of dialogue context and attribute-oriented control signals.
Specifically, communication mechanism, intent, and semantic frame are imported
as multi-grained signals that control the empathy realization from coarse to
fine levels. We then design a specific masking strategy to reflect the
relationship between multi-grained signals and response tokens, and integrate
it into the diffusion model to influence the generative process. Experimental
results on a benchmark dataset EmpatheticDialogue show that our framework
outperforms competitive baselines in terms of controllability, informativeness,
and diversity without the loss of context-relatedness.",None,-1
4ec59f39-d8de-4e2a-9d56-a11b85a4d51b,Continual Multimodal Knowledge Graph Construction,0.877479,"Current Multimodal Knowledge Graph Construction (MKGC) models struggle with
the real-world dynamism of continuously emerging entities and relations, often
succumbing to catastrophic forgetting-loss of previously acquired knowledge.
This study introduces benchmarks aimed at fostering the development of the
continual MKGC domain. We further introduce MSPT framework, designed to
surmount the shortcomings of existing MKGC approaches during multimedia data
processing. MSPT harmonizes the retention of learned knowledge (stability) and
the integration of new data (plasticity), outperforming current continual
learning and multimodal methods. Our results confirm MSPT's superior
performance in evolving knowledge environments, showcasing its capacity to
navigate balance between stability and plasticity.",None,-1
5ff73c35-7f94-41a8-be12-74e6000157be,Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data,0.76178,"Large language models (LLMs) provide a new way to build chatbots by accepting
natural language prompts. Yet, it is unclear how to design prompts to power
chatbots to carry on naturalistic conversations while pursuing a given goal,
such as collecting self-report data from users. We explore what design factors
of prompts can help steer chatbots to talk naturally and collect data reliably.
To this aim, we formulated four prompt designs with different structures and
personas. Through an online study (N = 48) where participants conversed with
chatbots driven by different designs of prompts, we assessed how prompt designs
and conversation topics affected the conversation flows and users' perceptions
of chatbots. Our chatbots covered 79% of the desired information slots during
conversations, and the designs of prompts and topics significantly influenced
the conversation flows and the data collection performance. We discuss the
opportunities and challenges of building chatbots with LLMs.",None,-1
e4398fb7-e904-4958-8eb3-2759e8f2abbe,Revisiting Machine Translation for Cross-lingual Classification,0.917145,"Machine Translation (MT) has been widely used for cross-lingual
classification, either by translating the test set into English and running
inference with a monolingual model (translate-test), or translating the
training set into the target languages and finetuning a multilingual model
(translate-train). However, most research in the area focuses on the
multilingual models rather than the MT component. We show that, by using a
stronger MT system and mitigating the mismatch between training on original
text and running inference on machine translated text, translate-test can do
substantially better than previously assumed. The optimal approach, however, is
highly task dependent, as we identify various sources of cross-lingual transfer
gap that affect different tasks and approaches differently. Our work calls into
question the dominance of multilingual models for cross-lingual classification,
and prompts to pay more attention to MT-based baselines.",None,-1
7bd908f0-8db6-4b81-b884-ed1ece8ee6b9,Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion,0.846209,"Although generative AI has been successful in many areas, its ability to
model geospatial data is still underexplored. Urban flow, a typical kind of
geospatial data, is critical for a wide range of urban applications. Existing
studies mostly focus on predictive modeling of urban flow that predicts the
future flow based on historical flow data, which may be unavailable in
data-sparse areas or newly planned regions. Some other studies aim to predict
OD flow among regions but they fail to model dynamic changes of urban flow over
time. In this work, we study a new problem of urban flow generation that
generates dynamic urban flow for regions without historical flow data. To
capture the effect of multiple factors on urban flow, such as region features
and urban environment, we employ diffusion model to generate urban flow for
regions under different conditions. We first construct an urban knowledge graph
(UKG) to model the urban environment and relationships between regions, based
on which we design a knowledge-enhanced spatio-temporal diffusion model
(KSTDiff) to generate urban flow for each region. Specifically, to accurately
generate urban flow for regions with different flow volumes, we design a novel
diffusion process guided by a volume estimator, which is learnable and
customized for each region. Moreover, we propose a knowledge-enhanced denoising
network to capture the spatio-temporal dependencies of urban flow as well as
the impact of urban environment in the denoising process. Extensive experiments
on four real-world datasets validate the superiority of our model over
state-of-the-art baselines in urban flow generation. Further in-depth studies
demonstrate the utility of generated urban flow data and the ability of our
model for long-term flow generation and urban flow prediction. Our code is
released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.",None,-1
f1c28345-28b8-43bc-a584-29b8a59bd37e,TopEx: Topic-based Explanations for Model Comparison,0.0813301,"Meaningfully comparing language models is challenging with current
explanation methods. Current explanations are overwhelming for humans due to
large vocabularies or incomparable across models. We present TopEx, an
explanation method that enables a level playing field for comparing language
models via model-agnostic topics. We demonstrate how TopEx can identify
similarities and differences between DistilRoBERTa and GPT-2 on a variety of
NLP tasks.",None,-1
272e5e2e-f014-41a1-b751-1d063c911b8e,Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond,0.855567,"Recently, multi-modality scene perception tasks, e.g., image fusion and scene
understanding, have attracted widespread attention for intelligent vision
systems. However, early efforts always consider boosting a single task
unilaterally and neglecting others, seldom investigating their underlying
connections for joint promotion. To overcome these limitations, we establish
the hierarchical dual tasks-driven deep model to bridge these tasks.
Concretely, we firstly construct an image fusion module to fuse complementary
characteristics and cascade dual task-related modules, including a
discriminator for visual effects and a semantic network for feature
measurement. We provide a bi-level perspective to formulate image fusion and
follow-up downstream tasks. To incorporate distinct task-related responses for
image fusion, we consider image fusion as a primary goal and dual modules as
learnable constraints. Furthermore, we develop an efficient first-order
approximation to compute corresponding gradients and present dynamic weighted
aggregation to balance the gradients for fusion learning. Extensive experiments
demonstrate the superiority of our method, which not only produces visually
pleasant fused results but also realizes significant promotion for detection
and segmentation than the state-of-the-art approaches.",None,-1
14324299-8c8c-4dd0-b20c-4ed11ce5db08,Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,0.535989,"Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research.",None,-1
52b2ae75-060b-465d-972a-a594cc9e0e21,Single Domain Dynamic Generalization for Iris Presentation Attack Detection,0.400591,"Iris presentation attack detection (PAD) has achieved great success under
intra-domain settings but easily degrades on unseen domains. Conventional
domain generalization methods mitigate the gap by learning domain-invariant
features. However, they ignore the discriminative information in the
domain-specific features. Moreover, we usually face a more realistic scenario
with only one single domain available for training. To tackle the above issues,
we propose a Single Domain Dynamic Generalization (SDDG) framework, which
simultaneously exploits domain-invariant and domain-specific features on a
per-sample basis and learns to generalize to various unseen domains with
numerous natural images. Specifically, a dynamic block is designed to
adaptively adjust the network with a dynamic adaptor. And an information
maximization loss is further combined to increase diversity. The whole network
is integrated into the meta-learning paradigm. We generate amplitude perturbed
images and cover diverse domains with natural images. Therefore, the network
can learn to generalize to the perturbed domains in the meta-test phase.
Extensive experiments show the proposed method is effective and outperforms the
state-of-the-art on LivDet-Iris 2017 dataset.",None,-1
3dd0c468-03d3-4599-b408-577a0faf19b2,Adaptive Spot-Guided Transformer for Consistent Local Feature Matching,0.636494,"Local feature matching aims at finding correspondences between a pair of
images. Although current detector-free methods leverage Transformer
architecture to obtain an impressive performance, few works consider
maintaining local consistency. Meanwhile, most methods struggle with large
scale variations. To deal with the above issues, we propose Adaptive
Spot-Guided Transformer (ASTR) for local feature matching, which jointly models
the local consistency and scale variations in a unified coarse-to-fine
architecture. The proposed ASTR enjoys several merits. First, we design a
spot-guided aggregation module to avoid interfering with irrelevant areas
during feature aggregation. Second, we design an adaptive scaling module to
adjust the size of grids according to the calculated depth information at fine
stage. Extensive experimental results on five standard benchmarks demonstrate
that our ASTR performs favorably against state-of-the-art methods. Our code
will be released on https://astr2023.github.io.",None,-1
90155e7c-10f4-4082-be4b-09db747662d7,Arabic Dialect Identification under Scrutiny: Limitations of Single-label Classification,0.972463,"Automatic Arabic Dialect Identification (ADI) of text has gained great
popularity since it was introduced in the early 2010s. Multiple datasets were
developed, and yearly shared tasks have been running since 2018. However, ADI
systems are reported to fail in distinguishing between the micro-dialects of
Arabic. We argue that the currently adopted framing of the ADI task as a
single-label classification problem is one of the main reasons for that. We
highlight the limitation of the incompleteness of the Dialect labels and
demonstrate how it impacts the evaluation of ADI systems. A manual error
analysis for the predictions of an ADI, performed by 7 native speakers of
different Arabic dialects, revealed that $\approx$ 66% of the validated errors
are not true errors. Consequently, we propose framing ADI as a multi-label
classification task and give recommendations for designing new ADI datasets.",None,-1
8dd7d443-3e99-40df-93b6-75aa633a66be,Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers,0.756061,"Fine-grained classification is a challenging task that involves identifying
subtle differences between objects within the same category. This task is
particularly challenging in scenarios where data is scarce. Visual transformers
(ViT) have recently emerged as a powerful tool for image classification, due to
their ability to learn highly expressive representations of visual data using
self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine
tuned using semi-supervised learning techniques, suitable for situations where
we have lack of annotated data. This is particularly common in e-commerce,
where images are readily available but labels are noisy, nonexistent, or
expensive to obtain. Our results demonstrate that Semi-ViT outperforms
traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned
with limited annotated data. These findings indicate that Semi-ViTs hold
significant promise for applications that require precise and fine-grained
classification of visual data.",None,-1
0c284c18-c843-4e48-b6a4-1e8801cebfbf,Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine,0.862896,"One of the major barriers to using large language models (LLMs) in medicine
is the perception they use uninterpretable methods to make clinical decisions
that are inherently different from the cognitive processes of clinicians. In
this manuscript we develop novel diagnostic reasoning prompts to study whether
LLMs can perform clinical reasoning to accurately form a diagnosis. We find
that GPT4 can be prompted to mimic the common clinical reasoning processes of
clinicians without sacrificing diagnostic accuracy. This is significant because
an LLM that can use clinical reasoning to provide an interpretable rationale
offers physicians a means to evaluate whether LLMs can be trusted for patient
care. Novel prompting methods have the potential to expose the black box of
LLMs, bringing them one step closer to safe and effective use in medicine.",None,-1
98eb63f4-6aa9-4f90-b3b6-621850c84e26,Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers,0.970653,"Abstracts derived from biomedical literature possess distinct domain-specific
characteristics, including specialised writing styles and biomedical
terminologies, which necessitate a deep understanding of the related
literature. As a result, existing language models struggle to generate
technical summaries that are on par with those produced by biomedical experts,
given the absence of domain-specific background knowledge. This paper aims to
enhance the performance of language models in biomedical abstractive
summarisation by aggregating knowledge from external papers cited within the
source article. We propose a novel attention-based citation aggregation model
that integrates domain-specific knowledge from citation papers, allowing neural
networks to generate summaries by leveraging both the paper content and
relevant knowledge from citation papers. Furthermore, we construct and release
a large-scale biomedical summarisation dataset that serves as a foundation for
our research. Extensive experiments demonstrate that our model outperforms
state-of-the-art approaches and achieves substantial improvements in
abstractive biomedical text summarisation.",None,-1
9f80a3ca-ceb4-4f23-9448-fba3552b7e6b,Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers,0.79087,"In recent years machine translation has become very successful for
high-resource language pairs. This has also sparked new interest in research on
the automatic translation of low-resource languages, including Indigenous
languages. However, the latter are deeply related to the ethnic and cultural
groups that speak (or used to speak) them. The data collection, modeling and
deploying machine translation systems thus result in new ethical questions that
must be addressed. Motivated by this, we first survey the existing literature
on ethical considerations for the documentation, translation, and general
natural language processing for Indigenous languages. Afterward, we conduct and
analyze an interview study to shed light on the positions of community leaders,
teachers, and language activists regarding ethical concerns for the automatic
translation of their languages. Our results show that the inclusion, at
different degrees, of native speakers and community members is vital to
performing better and more ethical research on Indigenous languages.",None,-1
4187c73e-0e56-4590-9283-043e809e0531,ICICLE: Interpretable Class Incremental Continual Learning,0.676481,"Continual learning enables incremental learning of new tasks without
forgetting those previously learned, resulting in positive knowledge transfer
that can enhance performance on both new and old tasks. However, continual
learning poses new challenges for interpretability, as the rationale behind
model predictions may change over time, leading to interpretability concept
drift. We address this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts a prototypical
part-based approach. It consists of three crucial novelties: interpretability
regularization that distills previously learned concepts while preserving
user-friendly positive reasoning; proximity-based prototype initialization
strategy dedicated to the fine-grained setting; and task-recency bias
compensation devoted to prototypical parts. Our experimental results
demonstrate that ICICLE reduces the interpretability concept drift and
outperforms the existing exemplar-free methods of common class-incremental
learning when applied to concept-based models.",None,-1
60bfe8f3-1d77-4fb5-9c2a-9ab05f7a0f2b,Mathematical Structure of Syntactic Merge,0.613637,"The syntactic Merge operation of the Minimalist Program in linguistics can be
described mathematically in terms of Hopf algebras, with a formalism similar to
the one arising in the physics of renormalization. This mathematical
formulation of Merge has good descriptive power, as phenomena empirically
observed in linguistics can be justified from simple mathematical arguments. It
also provides a possible mathematical model for externalization and for the
role of syntactic parameters.",None,-1
8bcf5043-d302-488e-83f4-67b8f2130031,Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty,0.888327,"Open Information Extraction (OIE) task aims at extracting structured facts
from unstructured text, typically in the form of (subject, relation, object)
triples. Despite the potential of large language models (LLMs) like ChatGPT as
a general task solver, they lag behind state-of-the-art (supervised) methods in
OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant
context from relevant relations and generate structured output due to the
restrictions on fine-tuning the model. Second, LLMs generates responses
autoregressively based on probability, which makes the predicted relations lack
confidence. In this paper, we assess the capabilities of LLMs in improving the
OIE task. Particularly, we propose various in-context learning strategies to
enhance LLM's instruction-following ability and a demonstration uncertainty
quantification module to enhance the confidence of the generated relations. Our
experiments on three OIE benchmark datasets show that our approach holds its
own against established supervised methods, both quantitatively and
qualitatively.",None,-1
587a74b4-2949-4aeb-a12a-494fb91b15f2,Understanding Retrieval Augmentation for Long-Form Question Answering,0.910743,"We present a study of retrieval-augmented language models (LMs) on long-form
question answering. We analyze how retrieval augmentation impacts different
LMs, by comparing answers generated from models while using the same evidence
documents, and how differing quality of retrieval document set impacts the
answers generated from the same LM. We study various attributes of generated
answers (e.g., fluency, length, variance) with an emphasis on the attribution
of generated long-form answers to in-context evidence documents. We collect
human annotations of answer attribution and evaluate methods for automatically
judging attribution. Our study provides new insights on how retrieval
augmentation impacts long, knowledge-rich text generation of LMs. We further
identify attribution patterns for long text generation and analyze the main
culprits of attribution errors. Together, our analysis reveals how retrieval
augmentation impacts long knowledge-rich text generation and provide directions
for future work.",None,-1
eb9b8a1a-e551-4036-bcf4-d935f636010a,Spatial Knowledge-Infused Hierarchical Learning: An Application in Flood Mapping on Earth Imagery,0.522586,"Deep learning for Earth imagery plays an increasingly important role in
geoscience applications such as agriculture, ecology, and natural disaster
management. Still, progress is often hindered by the limited training labels.
Given Earth imagery with limited training labels, a base deep neural network
model, and a spatial knowledge base with label constraints, our problem is to
infer the full labels while training the neural network. The problem is
challenging due to the sparse and noisy input labels, spatial uncertainty
within the label inference process, and high computational costs associated
with a large number of sample locations. Existing works on neuro-symbolic
models focus on integrating symbolic logic into neural networks (e.g., loss
function, model architecture, and training label augmentation), but these
methods do not fully address the challenges of spatial data (e.g., spatial
uncertainty, the trade-off between spatial granularity and computational
costs). To bridge this gap, we propose a novel Spatial Knowledge-Infused
Hierarchical Learning (SKI-HL) framework that iteratively infers sample labels
within a multi-resolution hierarchy. Our framework consists of a module to
selectively infer labels in different resolutions based on spatial uncertainty
and a module to train neural network parameters with uncertainty-aware
multi-instance learning. Extensive experiments on real-world flood mapping
datasets show that the proposed model outperforms several baseline methods. The
code is available at \url{https://github.com/ZelinXu2000/SKI-HL}.",None,-1
2920539c-8d5e-4df9-9524-dd040396921c,"ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis",0.661963,"The computational analysis of poetry is limited by the scarcity of tools to
automatically analyze and scan poems. In a multilingual settings, the problem
is exacerbated as scansion and rhyme systems only exist for individual
languages, making comparative studies very challenging and time consuming. In
this work, we present \textsc{Alberti}, the first multilingual pre-trained
large language model for poetry. Through domain-specific pre-training (DSP), we
further trained multilingual BERT on a corpus of over 12 million verses from 12
languages. We evaluated its performance on two structural poetry tasks: Spanish
stanza type classification, and metrical pattern prediction for Spanish,
English and German. In both cases, \textsc{Alberti} outperforms multilingual
BERT and other transformers-based models of similar sizes, and even achieves
state-of-the-art results for German when compared to rule-based systems,
demonstrating the feasibility and effectiveness of DSP in the poetry domain.",None,-1
1d756b24-bb87-4ac7-8eed-8d7ef9dfe17c,Evaluating Factual Consistency of Summaries with Large Language Models,0.0665569,"Detecting factual errors in summaries has been an important and challenging
subject in summarization research. Inspired by the emergent ability of large
language models (LLMs), we explore evaluating factual consistency of summaries
by directly prompting LLMs. We present a comprehensive empirical study to
assess the ability of LLMs as factual consistency evaluators, which consists of
(1) analyzing different LLMs such as the GPT model series and Flan-T5; (2)
investigating a variety of prompting methods including vanilla prompting,
chain-of-thought prompting, and a sentence-by-sentence prompting method to
tackle long summaries; and (3) evaluating on diverse summaries generated by
multiple summarization systems, ranging from pre-transformer methods to SOTA
pretrained models. Our experiments demonstrate that prompting LLMs is able to
outperform the previous best factuality systems in all settings, by up to 12.2
absolute points in terms of the binary classification accuracy on inconsistency
detection.",None,-1
edc3b873-5cc8-4fdf-8b99-3607529412ed,Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers,0.963345,"This paper addresses the problem of cross-modal object tracking from RGB
videos and event data. Rather than constructing a complex cross-modal fusion
network, we explore the great potential of a pre-trained vision Transformer
(ViT). Particularly, we delicately investigate plug-and-play training
augmentations that encourage the ViT to bridge the vast distribution gap
between the two modalities, enabling comprehensive cross-modal information
interaction and thus enhancing its ability. Specifically, we propose a mask
modeling strategy that randomly masks a specific modality of some tokens to
enforce the interaction between tokens from different modalities interacting
proactively. To mitigate network oscillations resulting from the masking
strategy and further amplify its positive effect, we then theoretically propose
an orthogonal high-rank loss to regularize the attention matrix. Extensive
experiments demonstrate that our plug-and-play training augmentation techniques
can significantly boost state-of-the-art one-stream and twostream trackers to a
large extent in terms of both tracking precision and success rate. Our new
perspective and findings will potentially bring insights to the field of
leveraging powerful pre-trained ViTs to model cross-modal data. The code will
be publicly available.",None,-1
f6d6444c-48fd-46cf-9174-6eebd94d4975,Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation,0.0252183,"This work aims at decreasing the end-to-end generation latency of large
language models (LLMs). One of the major causes of the high generation latency
is the sequential decoding approach adopted by almost all state-of-the-art
LLMs. In this work, motivated by the thinking and writing process of humans, we
propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the
skeleton of the answer, and then conducts parallel API calls or batched
decoding to complete the contents of each skeleton point in parallel. Not only
does SoT provide considerable speed-ups across 12 LLMs, but it can also
potentially improve the answer quality on several question categories. SoT is
an initial attempt at data-centric optimization for inference efficiency, and
showcases the potential of eliciting high-quality answers by explicitly
planning the answer structure in language.",None,-1
67a46229-b4de-4635-9072-0b7621598cbb,A Study on Deep CNN Structures for Defect Detection From Laser Ultrasonic Visualization Testing Images,0.288904,"The importance of ultrasonic nondestructive testing has been increasing in
recent years, and there are high expectations for the potential of laser
ultrasonic visualization testing, which combines laser ultrasonic testing with
scattered wave visualization technology. Even if scattered waves are
visualized, inspectors still need to carefully inspect the images. To automate
this, this paper proposes a deep neural network for automatic defect detection
and localization in LUVT images. To explore the structure of a neural network
suitable to this task, we compared the LUVT image analysis problem with the
generic object detection problem. Numerical experiments using real-world data
from a SUS304 flat plate showed that the proposed method is more effective than
the general object detection model in terms of prediction performance. We also
show that the computational time required for prediction is faster than that of
the general object detection model.",None,-1
87656744-a699-44b2-9085-aaa56fab126b,HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention,0.797609,"The success of large-scale contrastive vision-language pretraining (CLIP) has
benefited both visual recognition and multimodal content understanding. The
concise design brings CLIP the advantage in inference efficiency against other
vision-language models with heavier cross-attention fusion layers, making it a
popular choice for a wide spectrum of downstream tasks. However, CLIP does not
explicitly capture the hierarchical nature of high-level and fine-grained
semantics conveyed in images and texts, which is arguably critical to
vision-language understanding and reasoning. To this end, we equip both the
visual and language branches in CLIP with hierarchy-aware attentions, namely
Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies
layer-by-layer from both images and texts in an unsupervised manner. As a
result, such hierarchical aggregation significantly improves the cross-modal
alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative
analysis on its unsupervised hierarchy induction during inference, as well as
extensive quantitative experiments on both visual recognition and
vision-language downstream tasks.",None,-1
531115c0-7476-47f0-8ccb-5376b4164fa4,MMVP: Motion-Matrix-based Video Prediction,0.727268,"A central challenge of video prediction lies where the system has to reason
the objects' future motions from image frames while simultaneously maintaining
the consistency of their appearances across frames. This work introduces an
end-to-end trainable two-stream video prediction framework, Motion-Matrix-based
Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that
usually handle motion prediction and appearance maintenance within the same set
of modules, MMVP decouples motion and appearance information by constructing
appearance-agnostic motion matrices. The motion matrices represent the temporal
similarity of each and every pair of feature patches in the input frames, and
are the sole input of the motion prediction module in MMVP. This design
improves video prediction in both accuracy and efficiency, and reduces the
model size. Results of extensive experiments demonstrate that MMVP outperforms
state-of-the-art systems on public data sets by non-negligible large margins
(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the
size or smaller).",None,-1
aefe32e1-5105-4422-881e-5638c59a22d6,Structured Thoughts Automaton: First Formalized Execution Model for Auto-Regressive Language Models,0.2957,"In recent months, Language Models (LMs) have become a part of daily
discourse, with focus on OpenAI and the potential of Artificial General
Intelligence (AGI). Furthermore, the leaking of LLama's weights to the public
has led to an influx of innovations demonstrating the impressive capabilities
of generative LMs. While we believe that AGI is still a distant goal, we
recognize the potential of LMs in solving tasks such as searching complex
documents, compiling reports with basic analysis, and providing assistance in
problem-solving. In this paper, we propose formalizing the execution model of
language models. We investigate current execution models, to find that this
formalism has received little attention, and present our contribution: the
first formalized execution model for LMs. We introduce a new algorithm for
sampling the predictions of LMs, which we use to build a reliable and
inspectable execution model. We introduce a low-level language to write
""cognitive program"" for this execution model. We hope to shed light on the need
for execution models for LMs and encourage further research in this area.",None,-1
865403a3-b1c2-4b2f-bc3e-b9255536663e,Deep Axial Hypercomplex Networks,0.0304891,"Over the past decade, deep hypercomplex-inspired networks have enhanced
feature extraction for image classification by enabling weight sharing across
input channels. Recent works make it possible to improve representational
capabilities by using hypercomplex-inspired networks which consume high
computational costs. This paper reduces this cost by factorizing a quaternion
2D convolutional module into two consecutive vectormap 1D convolutional
modules. Also, we use 5D parameterized hypercomplex multiplication based fully
connected layers. Incorporating both yields our proposed hypercomplex network,
a novel architecture that can be assembled to construct deep axial-hypercomplex
networks (DANs) for image classifications. We conduct experiments on CIFAR
benchmarks, SVHN, and Tiny ImageNet datasets and achieve better performance
with fewer trainable parameters and FLOPS. Our proposed model achieves almost
2% higher performance for CIFAR and SVHN datasets, and more than 3% for the
ImageNet-Tiny dataset and takes six times fewer parameters than the real-valued
ResNets. Also, it shows state-of-the-art performance on CIFAR benchmarks in
hypercomplex space.",None,-1
82306b70-ff5c-45a0-b0ab-04cd5cbc3c6d,Learning Interpretable Style Embeddings via Prompting LLMs,0.999127,"Style representation learning builds content-independent representations of
author style in text. Stylometry, the analysis of style in text, is often
performed by expert forensic linguists and no large dataset of stylometric
annotations exists for training. Current style representation learning uses
neural methods to disentangle style from content to create style vectors,
however, these approaches result in uninterpretable representations,
complicating their usage in downstream applications like authorship attribution
where auditing and explainability is critical. In this work, we use prompting
to perform stylometry on a large number of texts to create a synthetic dataset
and train human-interpretable style representations we call LISA embeddings. We
release our synthetic stylometry dataset and our interpretable style models as
resources.",None,-1
bf658324-019f-4f04-aee4-2a4a2ec36997,Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming,0.89483,"The rapid advancements in artificial intelligence (AI) have led to a growing
trend of human-AI teaming (HAT) in various fields. As machines continue to
evolve from mere automation to a state of autonomy, they are increasingly
exhibiting unexpected behaviors and human-like cognitive/intelligent
capabilities, including situation awareness (SA). This shift has the potential
to enhance the performance of mixed human-AI teams over all-human teams,
underscoring the need for a better understanding of the dynamic SA interactions
between humans and machines. To this end, we provide a review of leading SA
theoretical models and a new framework for SA in the HAT context based on the
key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA)
framework unifies human and AI behavior, and involves bidirectional, and
dynamic interaction. The framework is based on the individual and team SA
models and elaborates on the cognitive mechanisms for modeling HAT. Similar
perceptual cycles are adopted for the individual (including both human and AI)
and the whole team, which is tailored to the unique requirements of the HAT
context. ATSA emphasizes cohesive and effective HAT through structures and
components, including teaming understanding, teaming control, and the world, as
well as adhesive transactive part. We further propose several future research
directions to expand on the distinctive contributions of ATSA and address the
specific and pressing next steps.",None,-1
2d2e915b-6705-4ae1-8d92-1d55bbede320,Benchmarking Large Language Model Capabilities for Conditional Generation,0.562593,"Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.",None,-1
b977d56d-c234-47a2-bc26-56a0c2c07ad4,Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering,0.510248,"The nodes in the commonsense knowledge graph (CSKG) are normally represented
by free-form short text (e.g., word or phrase). Different nodes may represent
the same concept. This leads to the problems of edge sparsity and node
redundancy, which challenges CSKG representation and completion. On the one
hand, edge sparsity limits the performance of graph representation learning; On
the other hand, node redundancy makes different nodes corresponding to the same
concept have inconsistent relations with other nodes. To address the two
problems, we propose a new CSKG completion framework based on Contrastive
Pretraining and Node Clustering (CPNC). Contrastive Pretraining constructs
positive and negative head-tail node pairs on CSKG and utilizes contrastive
learning to obtain better semantic node representation. Node Clustering
aggregates nodes with the same concept into a latent concept, assisting the
task of CSKG completion. We evaluate our CPNC approach on two CSKG completion
benchmarks (CN-100K and ATOMIC), where CPNC outperforms the state-of-the-art
methods. Extensive experiments demonstrate that both Contrastive Pretraining
and Node Clustering can significantly improve the performance of CSKG
completion. The source code of CPNC is publicly available on
\url{https://github.com/NUSTM/CPNC}.",None,-1
ff62ba9c-abbd-4147-a6ed-60935f356c1f,MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions,0.684426,"Audio-driven portrait animation aims to synthesize portrait videos that are
conditioned by given audio. Animating high-fidelity and multimodal video
portraits has a variety of applications. Previous methods have attempted to
capture different motion modes and generate high-fidelity portrait videos by
training different models or sampling signals from given videos. However,
lacking correlation learning between lip-sync and other movements (e.g., head
pose/eye blinking) usually leads to unnatural results. In this paper, we
propose a unified system for multi-person, diverse, and high-fidelity talking
portrait generation. Our method contains three stages, i.e., 1) Mapping-Once
network with Dual Attentions (MODA) generates talking representation from given
audio. In MODA, we design a dual-attention module to encode accurate mouth
movements and diverse modalities. 2) Facial composer network generates dense
and detailed face landmarks, and 3) temporal-guided renderer syntheses stable
videos. Extensive evaluations demonstrate that the proposed system produces
more natural and realistic video portraits compared to previous methods.",None,-1
8f6f0688-ac48-4784-9810-1fcf2226fa7b,ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters,0.580708,"We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via
the use of language adapters (LAs). Most of the earlier works have explored
training with adapter of a single source (often English), and testing either
using the target LA or LA of another related language. Training target LA
requires unlabeled data, which may not be readily available for low resource
unseen languages: those that are neither seen by the underlying multilingual
language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data
for them. We posit that for more effective cross-lingual transfer, instead of
just one source LA, we need to leverage LAs of multiple (linguistically or
geographically related) source languages, both at train and test-time - which
we investigate via our novel neural architecture, ZGUL. Extensive
experimentation across four language groups, covering 15 unseen target
languages, demonstrates improvements of up to 3.2 average F1 points over
standard fine-tuning and other strong baselines on POS tagging and NER tasks.
We also extend ZGUL to settings where either (1) some unlabeled data or (2)
few-shot training examples are available for the target language. We find that
ZGUL continues to outperform baselines in these settings too.",None,-1
c6b49f00-8808-4f0f-bfa0-7d811cba7e2c,Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings,0.772201,"The human brain possesses the extraordinary capability to contextualize the
information it receives from our environment. The entorhinal-hippocampal plays
a critical role in this function, as it is deeply engaged in memory processing
and constructing cognitive maps using place and grid cells. Comprehending and
leveraging this ability could significantly augment the field of artificial
intelligence. The multi-scale successor representation serves as a good model
for the functionality of place and grid cells and has already shown promise in
this role. Here, we introduce a model that employs successor representations
and neural networks, along with word embedding vectors, to construct a
cognitive map of three separate concepts. The network adeptly learns two
different scaled maps and situates new information in proximity to related
pre-existing representations. The dispersion of information across the
cognitive map varies according to its scale - either being heavily
concentrated, resulting in the formation of the three concepts, or spread
evenly throughout the map. We suggest that our model could potentially improve
current AI models by providing multi-modal context information to any input,
based on a similarity metric for the input and pre-existing knowledge
representations.",None,-1
25fe6c04-5d24-4ec4-93d6-d91da6ad8cd6,SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models,0.913992,"Current speech large language models build upon discrete speech
representations, which can be categorized into semantic tokens and acoustic
tokens. However, existing speech tokens are not specifically designed for
speech language modeling. To assess the suitability of speech tokens for
building speech language models, we established the first benchmark,
SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are
ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech
tokenizer for speech large language models. SpeechTokenizer adopts the
Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying
semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of
speech information hierarchically across different RVQ layers. Furthermore, We
construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer.
Experiments show that SpeechTokenizer performs comparably to EnCodec in speech
reconstruction and demonstrates strong performance on the SLMTokBench
benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks.
Code and models are available at
https://github.com/ZhangXInFD/SpeechTokenizer/.",None,-1
096d9f96-d376-474c-bc11-0597f620adbb,HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation,0.841917,"Panoptic Scene Graph generation (PSG) is a recently proposed task in image
scene understanding that aims to segment the image and extract triplets of
subjects, objects and their relations to build a scene graph. This task is
particularly challenging for two reasons. First, it suffers from a long-tail
problem in its relation categories, making naive biased methods more inclined
to high-frequency relations. Existing unbiased methods tackle the long-tail
problem by data/loss rebalancing to favor low-frequency relations. Second, a
subject-object pair can have two or more semantically overlapping relations.
While existing methods favor one over the other, our proposed HiLo framework
lets different network branches specialize on low and high frequency relations,
enforce their consistency and fuse the results. To the best of our knowledge we
are the first to propose an explicitly unbiased PSG method. In extensive
experiments we show that our HiLo framework achieves state-of-the-art results
on the PSG task. We also apply our method to the Scene Graph Generation task
that predicts boxes instead of masks and see improvements over all baseline
methods. Code is available at https://github.com/franciszzj/HiLo.",None,-1
4f495207-777d-4cdc-92d3-779e71f9aa57,OVO: Open-Vocabulary Occupancy,0.527836,"Semantic occupancy prediction aims to infer dense geometry and semantics of
surroundings for an autonomous agent to operate safely in the 3D environment.
Existing occupancy prediction methods are almost entirely trained on
human-annotated volumetric data. Although of high quality, the generation of
such 3D annotations is laborious and costly, restricting them to a few specific
object categories in the training dataset. To address this limitation, this
paper proposes Open Vocabulary Occupancy (OVO), a novel approach that allows
semantic occupancy prediction of arbitrary classes but without the need for 3D
annotations during training. Keys to our approach are (1) knowledge
distillation from a pre-trained 2D open-vocabulary segmentation model to the 3D
occupancy network, and (2) pixel-voxel filtering for high-quality training data
generation. The resulting framework is simple, compact, and compatible with
most state-of-the-art semantic occupancy prediction models. On NYUv2 and
SemanticKITTI datasets, OVO achieves competitive performance compared to
supervised semantic occupancy prediction approaches. Furthermore, we conduct
extensive analyses and ablation studies to offer insights into the design of
the proposed framework. Our code is publicly available at
https://github.com/dzcgaara/OVO.",None,-1
3effb34d-1fc3-4794-a68d-5f77d27b3546,Variation and Instability in Dialect-Based Embedding Spaces,0.317651,"This paper measures variation in embedding spaces which have been trained on
different regional varieties of English while controlling for instability in
the embeddings. While previous work has shown that it is possible to
distinguish between similar varieties of a language, this paper experiments
with two follow-up questions: First, does the variety represented in the
training data systematically influence the resulting embedding space after
training? This paper shows that differences in embeddings across varieties are
significantly higher than baseline instability. Second, is such dialect-based
variation spread equally throughout the lexicon? This paper shows that specific
parts of the lexicon are particularly subject to variation. Taken together,
these experiments confirm that embedding spaces are significantly influenced by
the dialect represented in the training data. This finding implies that there
is semantic variation across dialects, in addition to previously-studied
lexical and syntactic variation.",None,-1
f240740a-bad7-4ec1-a6c4-415a047d312b,PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter,0.592891,"The Retrieval Question Answering (ReQA) task employs the retrieval-augmented
framework, composed of a retriever and generator. The generator formulates the
answer based on the documents retrieved by the retriever. Incorporating Large
Language Models (LLMs) as generators is beneficial due to their advanced QA
capabilities, but they are typically too large to be fine-tuned with budget
constraints while some of them are only accessible via APIs. To tackle this
issue and further improve ReQA performance, we propose a trainable Pluggable
Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box.
Positioned between the retriever and generator in a Pluggable manner, PRCA
refines the retrieved information by operating in a token-autoregressive
strategy via maximizing rewards of the reinforcement learning phase. Our
experiments validate PRCA's effectiveness in enhancing ReQA performance on
three datasets by up to 20% improvement to fit black-box LLMs into existing
frameworks, demonstrating its considerable potential in the LLMs era.",None,-1
201645a4-e5c4-421d-ac4d-40ebf0151705,CIRO: COVID-19 infection risk ontology,0.333431,"Public health authorities perform contact tracing for highly contagious
agents to identify close contacts with the infected cases. However, during the
pandemic caused by coronavirus disease 2019 (COVID-19), this operation was not
employed in countries with high patient volumes. Meanwhile, the Japanese
government conducted this operation, thereby contributing to the control of
infections, at the cost of arduous manual labor by public health officials. To
ease the burden of the officials, this study attempted to automate the
assessment of each person's infection risk through an ontology, called COVID-19
Infection Risk Ontology (CIRO). This ontology expresses infection risks of
COVID-19 formulated by the Japanese government, toward automated assessment of
infection risks of individuals, using Resource Description Framework (RDF) and
SPARQL (SPARQL Protocol and RDF Query Language) queries. For evaluation, we
demonstrated that the knowledge graph built could infer the risks, formulated
by the government. Moreover, we conducted reasoning experiments to analyze the
computational efficiency. The experiments demonstrated usefulness of the
knowledge processing, and identified issues left for deployment.",None,-1
0785548b-b6ae-4102-839d-c5587d9b88e1,Finetuning Is a Surprisingly Effective Domain Adaptation Baseline in Handwriting Recognition,0.248615,"In many machine learning tasks, a large general dataset and a small
specialized dataset are available. In such situations, various domain
adaptation methods can be used to adapt a general model to the target dataset.
We show that in the case of neural networks trained for handwriting recognition
using CTC, simple finetuning with data augmentation works surprisingly well in
such scenarios and that it is resistant to overfitting even for very small
target domain datasets. We evaluated the behavior of finetuning with respect to
augmentation, training data size, and quality of the pre-trained network, both
in writer-dependent and writer-independent settings. On a large real-world
dataset, finetuning provided an average relative CER improvement of 25 % with
16 text lines for new writers and 50 % for 256 text lines.",None,-1
ded85fc3-b467-4ca0-a013-b67504a5674d,Examining Autoexposure for Challenging Scenes,0.864665,"Autoexposure (AE) is a critical step applied by camera systems to ensure
properly exposed images. While current AE algorithms are effective in well-lit
environments with constant illumination, these algorithms still struggle in
environments with bright light sources or scenes with abrupt changes in
lighting. A significant hurdle in developing new AE algorithms for challenging
environments, especially those with time-varying lighting, is the lack of
suitable image datasets. To address this issue, we have captured a new 4D
exposure dataset that provides a large solution space (i.e., shutter speed
range from (1/500 to 15 seconds) over a temporal sequence with moving objects,
bright lights, and varying lighting. In addition, we have designed a software
platform to allow AE algorithms to be used in a plug-and-play manner with the
dataset. Our dataset and associate platform enable repeatable evaluation of
different AE algorithms and provide a much-needed starting point to develop
better AE methods. We examine several existing AE strategies using our dataset
and show that most users prefer a simple saliency method for challenging
lighting conditions.",None,-1
9b5db3eb-c4c1-4cf4-aa53-5f9f5978f23f,Spot-the-Camel: Computer Vision for Safer Roads,0.0921796,"As the population grows and more land is being used for urbanization,
ecosystems are disrupted by our roads and cars. This expansion of
infrastructure cuts through wildlife territories, leading to many instances of
Wildlife-Vehicle Collision (WVC). These instances of WVC are a global issue
that is having a global socio-economic impact, resulting in billions of dollars
in property damage and, at times, fatalities for vehicle occupants. In Saudi
Arabia, this issue is similar, with instances of Camel-Vehicle Collision (CVC)
being particularly deadly due to the large size of camels, which results in a
25% fatality rate [1]. The focus of this work is to test different object
detection models on the task of detecting camels on the road. The Deep Learning
(DL) object detection models used in the experiments are: Center Net, Efficient
Det, Faster R-CNN, SSD, and YOLOv8. Results of the experiments show that YOLOv8
performed the best in terms of accuracy and was the most efficient in training.
In the future, the plan is to expand on this work by developing a system to
make countryside roads safer.",None,-1
d07f1fb6-24a1-4cbe-8d19-f6c72d991330,TADA: Task-Agnostic Dialect Adapters for English,0.563746,"Large Language Models, the dominant starting point for Natural Language
Processing (NLP) applications, fail at a higher rate for speakers of English
dialects other than Standard American English (SAE). Prior work addresses this
using task-specific data or synthetic data augmentation, both of which require
intervention for each dialect and task pair. This poses a scalability issue
that prevents the broad adoption of robust dialectal English NLP. We introduce
a simple yet effective method for task-agnostic dialect adaptation by aligning
non-SAE dialects using adapters and composing them with task-specific adapters
from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on
4 dialectal variants of the GLUE benchmark without task-specific supervision.",None,-1
0221c9ef-aad6-4622-9508-4bfc93eabbf2,Long-range Multimodal Pretraining for Movie Understanding,0.101349,"Learning computer vision models from (and for) movies has a long-standing
history. While great progress has been attained, there is still a need for a
pretrained multimodal model that can perform well in the ever-growing set of
movie understanding tasks the community has been establishing. In this work, we
introduce Long-range Multimodal Pretraining, a strategy, and a model that
leverages movie data to train transferable multimodal and cross-modal encoders.
Our key idea is to learn from all modalities in a movie by observing and
extracting relationships over a long-range. After pretraining, we run ablation
studies on the LVU benchmark and validate our modeling choices and the
importance of learning from long-range time spans. Our model achieves
state-of-the-art on several LVU tasks while being much more data efficient than
previous works. Finally, we evaluate our model's transferability by setting a
new state-of-the-art in five different benchmarks.",None,-1
a2096b88-b62e-4ba3-91c3-8949065e6fde,"Mathematics, word problems, common sense, and artificial intelligence",0.878225,"The paper discusses the capacities and limitations of current artificial
intelligence (AI) technology to solve word problems that combine elementary
knowledge with commonsense reasoning. No existing AI systems can solve these
reliably. We review three approaches that have been developed, using AI natural
language technology: outputting the answer directly, outputting a computer
program that solves the problem, and outputting a formalized representation
that can be input to an automated theorem verifier. We review some benchmarks
that have been developed to evaluate these systems and some experimental
studies. We discuss the limitations of the existing technology at solving these
kinds of problems. We argue that it is not clear whether these kinds of
limitations will be important in developing AI technology for pure mathematical
research, but that they will be important in applications of mathematics, and
may well be important in developing programs capable of reading and
understanding mathematical content written by humans.",None,-1
1127bbc9-2b86-47ca-a014-ac0a7005dac6,Trade-Offs Between Fairness and Privacy in Language Modeling,0.0556893,"Protecting privacy in contemporary NLP models is gaining in importance. So
does the need to mitigate social biases of such models. But can we have both at
the same time? Existing research suggests that privacy preservation comes at
the price of worsening biases in classification tasks. In this paper, we
explore the extent to which this tradeoff really holds when we incorporate both
privacy preservation and de-biasing techniques into training text generation
models. How does improving the model along one dimension affect the other
dimension as well as the utility of the model? We conduct an extensive set of
experiments that include bias detection, privacy attacks, language modeling,
and performance on downstream tasks.",None,-1
bd1e7b09-7ac4-4bc7-8e51-f80ed4b1296e,Privacy-Preserving Face Recognition Using Random Frequency Components,0.926324,"The ubiquitous use of face recognition has sparked increasing privacy
concerns, as unauthorized access to sensitive face images could compromise the
information of individuals. This paper presents an in-depth study of the
privacy protection of face images' visual information and against recovery.
Drawing on the perceptual disparity between humans and models, we propose to
conceal visual information by pruning human-perceivable low-frequency
components. For impeding recovery, we first elucidate the seeming paradox
between reducing model-exploitable information and retaining high recognition
accuracy. Based on recent theoretical insights and our observation on model
attention, we propose a solution to the dilemma, by advocating for the training
and inference of recognition models on randomly selected frequency components.
We distill our findings into a novel privacy-preserving face recognition
method, PartialFace. Extensive experiments demonstrate that PartialFace
effectively balances privacy protection goals and recognition accuracy. Code is
available at: https://github.com/Tencent/TFace.",None,-1
2eb31c42-c179-446c-bf77-2d388058da7f,History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System,0.74007,"With the evolution of pre-trained language models, current open-domain
dialogue systems have achieved great progress in conducting one-session
conversations. In contrast, Multi-Session Conversation (MSC), which consists of
multiple sessions over a long term with the same user, is under-investigated.
In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for
multi-session open-domain dialogue. HAHT maintains a long-term memory of
history conversations and utilizes history information to understand current
conversation context and generate well-informed and context-relevant responses.
Specifically, HAHT first encodes history conversation sessions hierarchically
into a history memory. Then, HAHT leverages historical information to
facilitate the understanding of the current conversation context by encoding
the history memory together with the current context with attention-based
mechanisms. Finally, to explicitly utilize historical information, HAHT uses a
history-aware response generator that switches between a generic vocabulary and
a history-aware vocabulary. Experimental results on a large-scale MSC dataset
suggest that the proposed HAHT model consistently outperforms baseline models.
Human evaluation results support that HAHT generates more human-like,
context-relevant and history-relevant responses than baseline models.",None,-1
a276fd3a-9ab8-4e4a-aa96-cd96aeec6427,Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science,0.986305,"Large Language Models (LLMs) have democratized synthetic data generation,
which in turn has the potential to simplify and broaden a wide gamut of NLP
tasks. Here, we tackle a pervasive problem in synthetic data generation: its
generative distribution often differs from the distribution of real-world data
researchers care about (in other words, it is unfaithful). In a case study on
sarcasm detection, we study three strategies to increase the faithfulness of
synthetic data: grounding, filtering, and taxonomy-based generation. We
evaluate these strategies using the performance of classifiers trained with
generated synthetic data on real-world data. While all three strategies improve
the performance of classifiers, we find that grounding works best for the task
at hand. As synthetic data generation plays an ever-increasing role in NLP
research, we expect this work to be a stepping stone in improving its utility.
We conclude this paper with some recommendations on how to generate
high(er)-fidelity synthetic data for specific tasks.",None,-1
b0cf899d-fb75-4ae7-9b85-79447766e5a4,LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise,0.285635,"Facial expression recognition (FER) remains a challenging task due to the
ambiguity of expressions. The derived noisy labels significantly harm the
performance in real-world scenarios. To address this issue, we present a new
FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks
to mitigate the impact of label noise from two perspectives. Firstly, LA-Net
uses landmark information to suppress the uncertainty in expression space and
constructs the label distribution of each sample by neighborhood aggregation,
which in turn improves the quality of training supervision. Secondly, the model
incorporates landmark information into expression representations using the
devised expression-landmark contrastive loss. The enhanced expression feature
extractor can be less susceptible to label noise. Our method can be integrated
with any deep neural network for better training supervision without
introducing extra inference costs. We conduct extensive experiments on both
in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net
achieves state-of-the-art performance.",None,-1
ba4e31c0-66c0-4787-ac1c-ef1fa2391ab3,Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking,0.47278,"Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification, and the datasets for evaluation are
relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have corresponding KB entities
by matching them to a special NIL entity. To better utilize BERT, we propose
new techniques including NIL entity representation and classification, with
synonym enhancement. We also apply KB Pruning and Versioning strategies to
automatically construct out-of-KB datasets from common in-KB EL datasets.
Results on five datasets of clinical notes, biomedical publications, and
Wikipedia articles in various domains show the advantages of BLINKout over
existing methods to identify out-of-KB mentions for the medical ontologies,
UMLS, SNOMED CT, and the general KB, WikiData.",None,-1
a6214a1a-d665-410a-a022-733c57ad0574,HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation,0.423082,"The Shapley value is widely regarded as a trustworthy attribution metric.
However, when people use Shapley values to explain the attribution of input
variables of a deep neural network (DNN), it usually requires a very high
computational cost to approximate relatively accurate Shapley values in
real-world applications. Therefore, we propose a novel network architecture,
the HarsanyiNet, which makes inferences on the input sample and simultaneously
computes the exact Shapley values of the input variables in a single forward
propagation. The HarsanyiNet is designed on the theoretical foundation that the
Shapley value can be reformulated as the redistribution of Harsanyi
interactions encoded by the network.",None,-1
522765fe-8152-4fb1-bb46-ebda30812291,Benchmarking Probabilistic Deep Learning Methods for License Plate Recognition,0.774977,"Learning-based algorithms for automated license plate recognition implicitly
assume that the training and test data are well aligned. However, this may not
be the case under extreme environmental conditions, or in forensic applications
where the system cannot be trained for a specific acquisition device.
Predictions on such out-of-distribution images have an increased chance of
failing. But this failure case is oftentimes hard to recognize for a human
operator or an automated system. Hence, in this work we propose to model the
prediction uncertainty for license plate recognition explicitly. Such an
uncertainty measure allows to detect false predictions, indicating an analyst
when not to trust the result of the automated license plate recognition. In
this paper, we compare three methods for uncertainty quantification on two
architectures. The experiments on synthetic noisy or blurred low-resolution
images show that the predictive uncertainty reliably finds wrong predictions.
We also show that a multi-task combination of classification and
super-resolution improves the recognition performance by 109\% and the
detection of wrong predictions by 29 %.",None,-1
3facc911-241f-4d16-a6d2-dba3beb99460,DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields,0.248984,"Advances in neural fields are enabling high-fidelity capture of the shape and
appearance of dynamic 3D scenes. However, their capabilities lag behind those
offered by conventional representations such as 2D videos because of
algorithmic challenges and the lack of large-scale multi-view real-world
datasets. We address the dataset limitation with DiVa-360, a real-world 360
dynamic visual dataset that contains synchronized high-resolution and
long-duration multi-view video sequences of table-scale scenes captured using a
customized low-cost system with 53 cameras. It contains 21 object-centric
sequences categorized by different motion types, 25 intricate hand-object
interaction sequences, and 8 long-duration sequences for a total of 17.4 M
image frames. In addition, we provide foreground-background segmentation masks,
synchronized audio, and text descriptions. We benchmark the state-of-the-art
dynamic neural field methods on DiVa-360 and provide insights about existing
methods and future challenges on long-duration neural field capture.",None,-1
1709902f-a57f-478f-907d-01b6cfd7fd00,Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation,0.797734,"One of the mainstream schemes for 2D human pose estimation (HPE) is learning
keypoints heatmaps by a neural network. Existing methods typically improve the
quality of heatmaps by customized architectures, such as high-resolution
representation and vision Transformers. In this paper, we propose
\textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypoints
heatmaps generation problem from noised heatmaps. During training, the
keypoints are diffused to random distribution by adding noises and the
diffusion model learns to recover ground-truth heatmaps from noised heatmaps
with respect to conditions constructed by image feature. During inference, the
diffusion model generates heatmaps from initialized heatmaps in a progressive
denoising way. Moreover, we further explore improving the performance of
DiffusionPose with conditions from human structural information. Extensive
experiments show the prowess of our DiffusionPose, with improvements of 1.6,
1.2, and 1.2 mAP on widely-used COCO, CrowdPose, and AI Challenge datasets,
respectively.",None,-1
82083f41-476e-4aa0-a9ad-bcee160427ff,Human-machine cooperation for semantic feature listing,0.0862105,"Semantic feature norms, lists of features that concepts do and do not
possess, have played a central role in characterizing human conceptual
knowledge, but require extensive human labor. Large language models (LLMs)
offer a novel avenue for the automatic generation of such feature lists, but
are prone to significant error. Here, we present a new method for combining a
learned model of human lexical-semantics from limited data with LLM-generated
data to efficiently generate high-quality feature norms.",None,-1
89a8ff85-7eab-49cd-a355-a31cca770290,Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration,0.439626,"Recent text-to-image generation models have shown promising results in
generating high-fidelity photo-realistic images. In parallel, the problem of
data scarcity has brought a growing interest in employing AIGC technology for
high-quality data expansion. However, this paradigm requires well-designed
prompt engineering that cost-less data expansion and labeling remain
under-explored. Inspired by LLM's powerful capability in task guidance, we
propose a new paradigm of annotated data expansion named as ChatGenImage. The
core idea behind it is to leverage the complementary strengths of diverse
models to establish a highly effective and user-friendly pipeline for
interactive data augmentation. In this work, we extensively study how LLMs
communicate with AIGC model to achieve more controllable image generation and
make the first attempt to collaborate them for automatic data augmentation for
a variety of downstream tasks. Finally, we present fascinating results obtained
from our ChatGenImage framework and demonstrate the powerful potential of our
synthetic data for systematic vision adaptation. Our codes are available at
https://github.com/Yuqifan1117/Labal-Anything-Pipeline.",None,-1
d014836a-e3a4-4c43-bb4a-9a40babd0735,Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving,0.999872,"Robotic perception requires the modeling of both 3D geometry and semantics.
Existing methods typically focus on estimating 3D bounding boxes, neglecting
finer geometric details and struggling to handle general, out-of-vocabulary
objects. 3D occupancy prediction, which estimates the detailed occupancy states
and semantics of a scene, is an emerging task to overcome these limitations. To
support 3D occupancy prediction, we develop a label generation pipeline that
produces dense, visibility-aware labels for any given scene. This pipeline
comprises three stages: voxel densification, occlusion reasoning, and
image-guided voxel refinement. We establish two benchmarks, derived from the
Waymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and
Occ3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of the
proposed dataset with various baseline models. Lastly, we propose a new model,
dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior
performance on the Occ3D benchmarks. The code, data, and benchmarks are
released at https://tsinghua-mars-lab.github.io/Occ3D/.",None,-1
8ef08ff5-3151-4e51-83b4-46bc7a25530d,ScribbleVC: Scribble-supervised Medical Image Segmentation with Vision-Class Embedding,0.569771,"Medical image segmentation plays a critical role in clinical decision-making,
treatment planning, and disease monitoring. However, accurate segmentation of
medical images is challenging due to several factors, such as the lack of
high-quality annotation, imaging noise, and anatomical differences across
patients. In addition, there is still a considerable gap in performance between
the existing label-efficient methods and fully-supervised methods. To address
the above challenges, we propose ScribbleVC, a novel framework for
scribble-supervised medical image segmentation that leverages vision and class
embeddings via the multimodal information enhancement mechanism. In addition,
ScribbleVC uniformly utilizes the CNN features and Transformer features to
achieve better visual feature extraction. The proposed method combines a
scribble-based approach with a segmentation network and a class-embedding
module to produce accurate segmentation masks. We evaluate ScribbleVC on three
benchmark datasets and compare it with state-of-the-art methods. The
experimental results demonstrate that our method outperforms existing
approaches in terms of accuracy, robustness, and efficiency. The datasets and
code are released on GitHub.",None,-1
1fc7a0c8-1788-4ec8-ba4f-0a0d8fc2db2d,Mao-Zedong At SemEval-2023 Task 4: Label Represention Multi-Head Attention Model With Contrastive Learning-Enhanced Nearest Neighbor Mechanism For Multi-Label Text Classification,0.205627,"The study of human values is essential in both practical and theoretical
domains. With the development of computational linguistics, the creation of
large-scale datasets has made it possible to automatically recognize human
values accurately. SemEval 2023 Task 4\cite{kiesel:2023} provides a set of
arguments and 20 types of human values that are implicitly expressed in each
argument. In this paper, we present our team's solution. We use the
Roberta\cite{liu_roberta_2019} model to obtain the word vector encoding of the
document and propose a multi-head attention mechanism to establish connections
between specific labels and semantic components. Furthermore, we use a
contrastive learning-enhanced K-nearest neighbor
mechanism\cite{su_contrastive_2022} to leverage existing instance information
for prediction. Our approach achieved an F1 score of 0.533 on the test set and
ranked fourth on the leaderboard.",None,-1
54eb8bcf-d2e3-48b3-8ae8-b72fc0df7e03,Linearity of Relation Decoding in Transformer Language Models,0.797188,"Much of the knowledge encoded in transformer language models (LMs) may be
expressed in terms of relations: relations between words and their synonyms,
entities and their attributes, etc. We show that, for a subset of relations,
this computation is well-approximated by a single linear transformation on the
subject representation. Linear relation representations may be obtained by
constructing a first-order approximation to the LM from a single prompt, and
they exist for a variety of factual, commonsense, and linguistic relations.
However, we also identify many cases in which LM predictions capture relational
knowledge accurately, but this knowledge is not linearly encoded in their
representations. Our results thus reveal a simple, interpretable, but
heterogeneously deployed knowledge representation strategy in transformer LMs.",None,-1
8e4ca00d-d8e5-4e8c-84ba-c67c5eacd7a0,Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO),0.592809,"This paper introduces the Hamilton-Jacobi-Bellman Proximal Policy
Optimization (HJBPPO) algorithm into reinforcement learning. The
Hamilton-Jacobi-Bellman (HJB) equation is used in control theory to evaluate
the optimality of the value function. Our work combines the HJB equation with
reinforcement learning in continuous state and action spaces to improve the
training of the value network. We treat the value network as a Physics-Informed
Neural Network (PINN) to solve for the HJB equation by computing its
derivatives with respect to its inputs exactly. The Proximal Policy
Optimization (PPO)-Clipped algorithm is improvised with this implementation as
it uses a value network to compute the objective function for its policy
network. The HJBPPO algorithm shows an improved performance compared to PPO on
the MuJoCo environments.",None,-1
7fe929f7-c9a6-4412-9256-7a2caa5ba3af,Implicit Neural Head Synthesis via Controllable Local Deformation Fields,0.524946,"High-quality reconstruction of controllable 3D head avatars from 2D videos is
highly desirable for virtual human applications in movies, games, and
telepresence. Neural implicit fields provide a powerful representation to model
3D head avatars with personalized shape, expressions, and facial parts, e.g.,
hair and mouth interior, that go beyond the linear 3D morphable model (3DMM).
However, existing methods do not model faces with fine-scale facial features,
or local control of facial parts that extrapolate asymmetric expressions from
monocular videos. Further, most condition only on 3DMM parameters with poor(er)
locality, and resolve local features with a global neural field. We build on
part-based implicit shape models that decompose a global deformation field into
local ones. Our novel formulation models multiple implicit deformation fields
with local semantic rig-like control via 3DMM-based parameters, and
representative facial landmarks. Further, we propose a local control loss and
attention mask mechanism that promote sparsity of each learned deformation
field. Our formulation renders sharper locally controllable nonlinear
deformations than previous implicit monocular approaches, especially mouth
interior, asymmetric expressions, and facial details.",None,-1
2bf433e4-1808-4a81-9354-a78a49aa27f6,CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training,0.0300982,"Speech or text representation generated by pre-trained models contains
modal-specific information that could be combined for benefiting spoken
language understanding (SLU) tasks. In this work, we propose a novel
pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training
(CIF-PT). It relies on a simple but effective frame-to-token alignment:
continuous integrate-and-fire (CIF) to bridge the representations between
speech and text. It jointly performs speech-to-text training and language model
distillation through CIF as the pre-training (PT). Evaluated on SLU benchmark
SLURP dataset, CIF-PT outperforms the state-of-the-art model by 1.94% of
accuracy and 2.71% of SLU-F1 on the tasks of intent classification and slot
filling, respectively. We also observe the cross-modal representation extracted
by CIF-PT obtains better performance than other neural interfaces for the tasks
of SLU, including the dominant speech representation learned from
self-supervised pre-training.",None,-1
bbea2e83-ac8f-4d6f-a752-d7b18806ec17,Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,0.789293,"Large-scale pre-trained language models have shown outstanding performance in
a variety of NLP tasks. However, they are also known to be significantly
brittle against specifically crafted adversarial examples, leading to
increasing interest in probing the adversarial robustness of NLP systems. We
introduce RSMI, a novel two-stage framework that combines randomized smoothing
(RS) with masked inference (MI) to improve the adversarial robustness of NLP
systems. RS transforms a classifier into a smoothed classifier to obtain robust
representations, whereas MI forces a model to exploit the surrounding context
of a masked token in an input sequence. RSMI improves adversarial robustness by
2 to 3 times over existing state-of-the-art methods on benchmark datasets. We
also perform in-depth qualitative analysis to validate the effectiveness of the
different stages of RSMI and probe the impact of its components through
extensive ablations. By empirically proving the stability of RSMI, we put it
forward as a practical method to robustly train large-scale NLP models. Our
code and datasets are available at https://github.com/Han8931/rsmi_nlp",None,-1
c280c954-afbb-4f67-865d-a498736cded4,Measuring axiomatic soundness of counterfactual image models,0.907701,"We present a general framework for evaluating image counterfactuals. The
power and flexibility of deep generative models make them valuable tools for
learning mechanisms in structural causal models. However, their flexibility
makes counterfactual identifiability impossible in the general case. Motivated
by these issues, we revisit Pearl's axiomatic definition of counterfactuals to
determine the necessary constraints of any counterfactual inference model:
composition, reversibility, and effectiveness. We frame counterfactuals as
functions of an input variable, its parents, and counterfactual parents and use
the axiomatic constraints to restrict the set of functions that could represent
the counterfactual, thus deriving distance metrics between the approximate and
ideal functions. We demonstrate how these metrics can be used to compare and
choose between different approximate counterfactual inference models and to
provide insight into a model's shortcomings and trade-offs.",None,-1
f3ab8afc-9936-40f7-9c23-18e53e269e3e,MixTeacher: Mining Promising Labels with Mixed Scale Teacher for Semi-Supervised Object Detection,0.677103,"Scale variation across object instances remains a key challenge in object
detection task. Despite the remarkable progress made by modern detection
models, this challenge is particularly evident in the semi-supervised case.
While existing semi-supervised object detection methods rely on strict
conditions to filter high-quality pseudo labels from network predictions, we
observe that objects with extreme scale tend to have low confidence, resulting
in a lack of positive supervision for these objects. In this paper, we propose
a novel framework that addresses the scale variation problem by introducing a
mixed scale teacher to improve pseudo label generation and scale-invariant
learning. Additionally, we propose mining pseudo labels using score promotion
of predictions across scales, which benefits from better predictions from mixed
scale features. Our extensive experiments on MS COCO and PASCAL VOC benchmarks
under various semi-supervised settings demonstrate that our method achieves new
state-of-the-art performance. The code and models are available at
\url{https://github.com/lliuz/MixTeacher}.",None,-1
c77768d7-68f2-4b88-8be6-1c77f4baa9a0,Erasure of Unaligned Attributes from Neural Representations,0.63754,"We present the Assignment-Maximization Spectral Attribute removaL (AMSAL)
algorithm, which erases information from neural representations when the
information to be erased is implicit rather than directly being aligned to each
input example. Our algorithm works by alternating between two steps. In one, it
finds an assignment of the input representations to the information to be
erased, and in the other, it creates projections of both the input
representations and the information to be erased into a joint latent space. We
test our algorithm on an extensive array of datasets, including a Twitter
dataset with multiple guarded attributes, the BiasBios dataset and the
BiasBench benchmark. The last benchmark includes four datasets with various
types of protected attributes. Our results demonstrate that bias can often be
removed in our setup. We also discuss the limitations of our approach when
there is a strong entanglement between the main task and the information to be
erased.",None,-1
b01e3b60-9044-46ce-b570-717d82d9102c,Application-Agnostic Language Modeling for On-Device ASR,0.639628,"On-device automatic speech recognition systems face several challenges
compared to server-based systems. They have to meet stricter constraints in
terms of speed, disk size and memory while maintaining the same accuracy. Often
they have to serve several applications with different distributions at once,
such as communicating with a virtual assistant and speech-to-text. The simplest
solution to serve multiple applications is to build application-specific
(language) models, but this leads to an increase in memory. Therefore, we
explore different data- and architecture-driven language modeling approaches to
build a single application-agnostic model. We propose two novel feed-forward
architectures that find an optimal trade off between different on-device
constraints. In comparison to the application-specific solution, one of our
novel approaches reduces the disk size by half, while maintaining speed and
accuracy of the original model.",None,-1
5e938ceb-2c74-4873-80ed-91dc58c4085d,OBJECT 3DIT: Language-guided 3D-aware Image Editing,0.843393,"Existing image editing tools, while powerful, typically disregard the
underlying 3D geometry from which the image is projected. As a result, edits
made using these tools may become detached from the geometry and lighting
conditions that are at the foundation of the image formation process. In this
work, we formulate the newt ask of language-guided 3D-aware editing, where
objects in an image should be edited according to a language instruction in
context of the underlying 3D scene. To promote progress towards this goal, we
release OBJECT: a dataset consisting of 400K editing examples created from
procedurally generated 3D scenes. Each example consists of an input image,
editing instruction in language, and the edited image. We also introduce 3DIT :
single and multi-task models for four editing tasks. Our models show impressive
abilities to understand the 3D composition of entire scenes, factoring in
surrounding objects, surfaces, lighting conditions, shadows, and
physically-plausible object configurations. Surprisingly, training on only
synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to
real-world images.",None,-1
aa43867f-1a84-4b9a-89c8-9a878893a128,Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering,0.40027,"Whereas the recent emergence of large language models (LLMs) like ChatGPT has
exhibited impressive general performance, it still has a large gap with
fully-supervised models on specific tasks such as multi-span question
answering. Previous researches found that in-context learning is an effective
approach to exploiting LLM, by using a few task-related labeled data as
demonstration examples to construct a few-shot prompt for answering new
questions. A popular implementation is to concatenate a few questions and their
correct answers through simple templates, informing LLM of the desired output.
In this paper, we propose a novel way of employing labeled data such that it
also informs LLM of some undesired output, by extending demonstration examples
with feedback about answers predicted by an off-the-shelf model, e.g., correct,
incorrect, or incomplete. Experiments on three multi-span question answering
datasets as well as a keyphrase extraction dataset show that our new prompting
strategy consistently improves LLM's in-context learning performance.",None,-1
671ab3ae-336f-4364-8914-488facd24631,Bright Channel Prior Attention for Multispectral Pedestrian Detection,0.53994,"Multispectral methods have gained considerable attention due to their
promising performance across various fields. However, most existing methods
cannot effectively utilize information from two modalities while optimizing
time efficiency. These methods often prioritize accuracy or time efficiency,
leaving room for improvement in their performance. To this end, we propose a
new method bright channel prior attention for enhancing pedestrian detection in
low-light conditions by integrating image enhancement and detection within a
unified framework. The method uses the V-channel of the HSV image of the
thermal image as an attention map to trigger the unsupervised auto-encoder for
visible light images, which gradually emphasizes pedestrian features across
layers. Moreover, we utilize unsupervised bright channel prior algorithms to
address light compensation in low light images. The proposed method includes a
self-attention enhancement module and a detection module, which work together
to improve object detection. An initial illumination map is estimated using the
BCP, guiding the learning of the self-attention map from the enhancement
network to obtain more informative representation focused on pedestrians. The
extensive experiments show effectiveness of the proposed method is demonstrated
through.",None,-1
943392e8-f1e9-427f-aadb-00ac6b6a1776,Rationale-Enhanced Language Models are Better Continual Relation Learners,0.926837,"Continual relation extraction (CRE) aims to solve the problem of catastrophic
forgetting when learning a sequence of newly emerging relations. Recent CRE
studies have found that catastrophic forgetting arises from the model's lack of
robustness against future analogous relations. To address the issue, we
introduce rationale, i.e., the explanations of relation classification results
generated by large language models (LLM), into CRE task. Specifically, we
design the multi-task rationale tuning strategy to help the model learn current
relations robustly. We also conduct contrastive rationale replay to further
distinguish analogous relations. Experimental results on two standard
benchmarks demonstrate that our method outperforms the state-of-the-art CRE
models.",None,-1
73d99270-8b87-4f88-8a2c-fbe5578212e8,Nonrigid Object Contact Estimation With Regional Unwrapping Transformer,0.393469,"Acquiring contact patterns between hands and nonrigid objects is a common
concern in the vision and robotics community. However, existing learning-based
methods focus more on contact with rigid ones from monocular images. When
adopting them for nonrigid contact, a major problem is that the existing
contact representation is restricted by the geometry of the object.
Consequently, contact neighborhoods are stored in an unordered manner and
contact features are difficult to align with image cues. At the core of our
approach lies a novel hand-object contact representation called RUPs (Region
Unwrapping Profiles), which unwrap the roughly estimated hand-object surfaces
as multiple high-resolution 2D regional profiles. The region grouping strategy
is consistent with the hand kinematic bone division because they are the
primitive initiators for a composite contact pattern. Based on this
representation, our Regional Unwrapping Transformer (RUFormer) learns the
correlation priors across regions from monocular inputs and predicts
corresponding contact and deformed transformations. Our experiments demonstrate
that the proposed framework can robustly estimate the deformed degrees and
deformed transformations, which makes it suitable for both nonrigid and rigid
contact.",None,-1
98757642-4cb1-4478-a709-d20040da9e35,Multi-Head Feature Pyramid Networks for Breast Mass Detection,0.316048,"Analysis of X-ray images is one of the main tools to diagnose breast cancer.
The ability to quickly and accurately detect the location of masses from the
huge amount of image data is the key to reducing the morbidity and mortality of
breast cancer. Currently, the main factor limiting the accuracy of breast mass
detection is the unequal focus on the mass boxes, leading the network to focus
too much on larger masses at the expense of smaller ones. In the paper, we
propose the multi-head feature pyramid module (MHFPN) to solve the problem of
unbalanced focus of target boxes during feature map fusion and design a
multi-head breast mass detection network (MBMDnet). Experimental studies show
that, comparing to the SOTA detection baselines, our method improves by 6.58%
(in AP@50) and 5.4% (in TPR@50) on the commonly used INbreast dataset, while
about 6-8% improvements (in AP@20) are also observed on the public MIAS and
BCS-DBT datasets.",None,-1
4aab7eb1-cecb-4ecf-9b47-89c9bab006f8,DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep Inconsistency Prior,0.904193,"RGB-NIR fusion is a promising method for low-light imaging. However,
high-intensity noise in low-light images amplifies the effect of structure
inconsistency between RGB-NIR images, which fails existing algorithms. To
handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net
(DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior
(DIP). The Deep Structure extracts clear structure details in deep multiscale
feature space rather than raw input space, which is more robust to noisy
inputs. Based on the deep structures from both RGB and NIR domains, we
introduce the DIP to leverage the structure inconsistency to guide the fusion
of RGB-NIR. Benefiting from this, the proposed DVN obtains high-quality
lowlight images without the visual artifacts. We also propose a new dataset
called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as
the first public RGBNIR fusion benchmark. Quantitative and qualitative results
on the proposed benchmark show that DVN significantly outperforms other
comparison algorithms in PSNR and SSIM, especially in extremely low light
conditions.",None,-1
8d07388e-1485-4847-bccb-d6f139e9ca0f,AI model GPT-3 (dis)informs us better than humans,0.831456,"Artificial intelligence is changing the way we create and evaluate
information, and this is happening during an infodemic, which has been having
dramatic effects on global health. In this paper we evaluate whether recruited
individuals can distinguish disinformation from accurate information,
structured in the form of tweets, and determine whether a tweet is organic or
synthetic, i.e., whether it has been written by a Twitter user or by the AI
model GPT-3. Our results show that GPT-3 is a double-edge sword, which, in
comparison with humans, can produce accurate information that is easier to
understand, but can also produce more compelling disinformation. We also show
that humans cannot distinguish tweets generated by GPT-3 from tweets written by
human users. Starting from our results, we reflect on the dangers of AI for
disinformation, and on how we can improve information campaigns to benefit
global health.",None,-1
6e2947dc-efaf-4041-b39d-e620749ce2f1,Improving Seq2Seq Grammatical Error Correction via Decoding Interventions,0.745858,"The sequence-to-sequence (Seq2Seq) approach has recently been widely used in
grammatical error correction (GEC) and shows promising performance. However,
the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC
model can only be trained on parallel data, which, in GEC task, is often noisy
and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an
explicit awareness of the correctness of the token being generated. In this
paper, we propose a unified decoding intervention framework that employs an
external critic to assess the appropriateness of the token to be generated
incrementally, and then dynamically influence the choice of the next token. We
discover and investigate two types of critics: a pre-trained left-to-right
language model critic and an incremental target-side grammatical error detector
critic. Through extensive experiments on English and Chinese datasets, our
framework consistently outperforms strong baselines and achieves results
competitive with state-of-the-art methods.",None,-1
debfd6b9-86be-4b14-a369-87b23652d445,vMAP: Vectorised Object Mapping for Neural Field SLAM,0.868703,"We present vMAP, an object-level dense SLAM system using neural field
representations. Each object is represented by a small MLP, enabling efficient,
watertight object modelling without the need for 3D priors. As an RGB-D camera
browses a scene with no prior information, vMAP detects object instances
on-the-fly, and dynamically adds them to its map. Specifically, thanks to the
power of vectorised training, vMAP can optimise as many as 50 individual
objects in a single scene, with an extremely efficient training speed of 5Hz
map update. We experimentally demonstrate significantly improved scene-level
and object-level reconstruction quality compared to prior neural field SLAM
systems. Project page: https://kxhit.github.io/vMAP.",None,-1
3c04f56e-bb64-4aea-9b06-8d6057c4d20c,A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation,0.204025,"Recent studies have shown that the multi-encoder models are agnostic to the
choice of context, and the context encoder generates noise which helps improve
the models in terms of BLEU score. In this paper, we further explore this idea
by evaluating with context-aware pronoun translation test set by training
multi-encoder models trained on three different context settings viz, previous
two sentences, random two sentences, and a mix of both as context.
Specifically, we evaluate the models on the ContraPro test set to study how
different contexts affect pronoun translation accuracy. The results show that
the model can perform well on the ContraPro test set even when the context is
random. We also analyze the source representations to study whether the context
encoder generates noise. Our analysis shows that the context encoder provides
sufficient information to learn discourse-level information. Additionally, we
observe that mixing the selected context (the previous two sentences in this
case) and the random context is generally better than the other settings.",None,-1
45b89870-f813-4cff-90bc-5a76265809d3,Empower Nested Boolean Logic via Self-Supervised Curriculum Learning,0.211346,"Beyond the great cognitive powers showcased by language models, it is crucial
to scrutinize whether their reasoning capabilities stem from strong
generalization or merely exposure to relevant data. As opposed to constructing
increasingly complex logic, this paper probes into the boolean logic, the root
capability of a logical reasoner. We find that any pre-trained language models
even including large language models only behave like a random selector in the
face of multi-nested boolean logic, a task that humans can handle with ease. To
empower language models with this fundamental capability, this paper proposes a
new self-supervised learning method \textit{Curriculum Logical Reasoning}
(\textsc{Clr}), where we augment the training data with nested boolean logic
chain step-by-step, and program the training from simpler logical patterns
gradually to harder ones. This new training paradigm allows language models to
effectively generalize to much harder and longer-hop logic, which can hardly be
learned through naive training. Furthermore, we show that boolean logic is a
great foundation for improving the subsequent general logical tasks.",None,-1
e85e2f55-d372-48f7-b5b0-ed9f8adf61d8,An Empirical Study on the Transferability of Transformer Modules in Parameter-Efficient Fine-Tuning,0.0327163,"Parameter-efficient fine-tuning approaches have recently garnered a lot of
attention. Having considerably lower number of trainable weights, these methods
can bring about scalability and computational effectiveness. In this paper, we
look for optimal sub-networks and investigate the capability of different
transformer modules in transferring knowledge from a pre-trained model to a
downstream task. Our empirical results suggest that every transformer module in
BERT can act as a winning ticket: fine-tuning each specific module while
keeping the rest of the network frozen can lead to comparable performance to
the full fine-tuning. Among different modules, LayerNorms exhibit the best
capacity for knowledge transfer with limited trainable weights, to the extent
that, with only 0.003% of all parameters in the layer-wise analysis, they show
acceptable performance on various target tasks. On the reasons behind their
effectiveness, we argue that their notable performance could be attributed to
their high-magnitude weights compared to that of the other modules in the
pre-trained BERT.",None,-1
b6a394a0-4a23-4e33-85e2-2d44422b3621,VCSUM: A Versatile Chinese Meeting Summarization Dataset,0.235001,"Compared to news and chat summarization, the development of meeting
summarization is hugely decelerated by the limited data. To this end, we
introduce a versatile Chinese meeting summarization dataset, dubbed VCSum,
consisting of 239 real-life meetings, with a total duration of over 230 hours.
We claim our dataset is versatile because we provide the annotations of topic
segmentation, headlines, segmentation summaries, overall meeting summaries, and
salient sentences for each meeting transcript. As such, the dataset can adapt
to various summarization tasks or methods, including segmentation-based
summarization, multi-granularity summarization and retrieval-then-generate
summarization. Our analysis confirms the effectiveness and robustness of VCSum.
We also provide a set of benchmark models regarding different downstream
summarization tasks on VCSum to facilitate further research. The dataset and
code will be released at https://github.com/hahahawu/VCSum.",None,-1
ec727e4c-9749-4807-a2d3-5fb6e8af96b1,Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild,0.0617196,"Engaging in the deliberate generation of abnormal outputs from large language
models (LLMs) by attacking them is a novel human activity. This paper presents
a thorough exposition of how and why people perform such attacks. Using a
formal qualitative methodology, we interviewed dozens of practitioners from a
broad range of backgrounds, all contributors to this novel work of attempting
to cause LLMs to fail. We relate and connect this activity between its
practitioners' motivations and goals; the strategies and techniques they
deploy; and the crucial role the community plays. As a result, this paper
presents a grounded theory of how and why people attack large language models:
LLM red teaming in the wild.",None,-1
e1a64e9a-1937-4e81-b6f6-7427ae18c9bf,Leveraging triplet loss for unsupervised action segmentation,0.0667523,"In this paper, we propose a novel fully unsupervised framework that learns
action representations suitable for the action segmentation task from the
single input video itself, without requiring any training data. Our method is a
deep metric learning approach rooted in a shallow network with a triplet loss
operating on similarity distributions and a novel triplet selection strategy
that effectively models temporal and semantic priors to discover actions in the
new representational space. Under these circumstances, we successfully recover
temporal boundaries in the learned action representations with higher quality
compared with existing unsupervised approaches. The proposed method is
evaluated on two widely used benchmark datasets for the action segmentation
task and it achieves competitive performance by applying a generic clustering
algorithm on the learned representations.",None,-1
3c831ad4-cbd8-4fa6-b7fe-316e33141203,Toward A Logical Theory Of Fairness and Bias,0.248584,"Fairness in machine learning is of considerable interest in recent years
owing to the propensity of algorithms trained on historical data to amplify and
perpetuate historical biases. In this paper, we argue for a formal
reconstruction of fairness definitions, not so much to replace existing
definitions but to ground their application in an epistemic setting and allow
for rich environmental modelling. Consequently we look into three notions:
fairness through unawareness, demographic parity and counterfactual fairness,
and formalise these in the epistemic situation calculus.",None,-1
b80806d6-4085-4259-84fc-3792210a5ea0,Dynamic Planning with a LLM,0.989497,"While Large Language Models (LLMs) can solve many NLP tasks in zero-shot
settings, applications involving embodied agents remain problematic. In
particular, complex plans that require multi-step reasoning become difficult
and too costly as the context window grows. Planning requires understanding the
likely effects of one's actions and identifying whether the current environment
satisfies the goal state. While symbolic planners find optimal solutions
quickly, they require a complete and accurate representation of the planning
problem, severely limiting their use in practical scenarios. In contrast,
modern LLMs cope with noisy observations and high levels of uncertainty when
reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a
neuro-symbolic framework where an LLM works hand-in-hand with a traditional
planner to solve an embodied task. Given action-descriptions, LLM-DP solves
Alfworld faster and more efficiently than a naive LLM ReAct baseline.",None,-1
5fa1473f-91df-4c43-b15a-7bc6010c9bc9,In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making,0.933393,"The current literature on AI-advised decision making -- involving explainable
AI systems advising human decision makers -- presents a series of inconclusive
and confounding results. To synthesize these findings, we propose a simple
theory that elucidates the frequent failure of AI explanations to engender
appropriate reliance and complementary decision making performance. We argue
explanations are only useful to the extent that they allow a human decision
maker to verify the correctness of an AI's prediction, in contrast to other
desiderata, e.g., interpretability or spelling out the AI's reasoning process.
Prior studies find in many decision making contexts AI explanations do not
facilitate such verification. Moreover, most tasks fundamentally do not allow
easy verification, regardless of explanation method, limiting the potential
benefit of any type of explanation. We also compare the objective of
complementary performance with that of appropriate reliance, decomposing the
latter into the notions of outcome-graded and strategy-graded reliance.",None,-1
3f4ddbf6-1650-4654-bea3-78337325e1a1,SeqXGPT: Sentence-Level AI-Generated Text Detection,0.871646,"Widely applied large language models (LLMs) can generate human-like content,
raising concerns about the abuse of LLMs. Therefore, it is important to build
strong AI-generated text (AIGT) detectors. Current works only consider
document-level AIGT detection, therefore, in this paper, we first introduce a
sentence-level detection challenge by synthesizing a dataset that contains
documents that are polished with LLMs, that is, the documents contain sentences
written by humans and sentences modified by LLMs. Then we propose
\textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes
log probability lists from white-box LLMs as features for sentence-level AIGT
detection. These features are composed like \textit{waves} in speech processing
and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution
and self-attention networks. We test it in both sentence and document-level
detection challenges. Experimental results show that previous methods struggle
in solving sentence-level AIGT detection, while our method not only
significantly surpasses baseline methods in both sentence and document-level
detection challenges but also exhibits strong generalization capabilities.",None,-1
3e72d740-38b4-42f9-aefd-7f7ac3029ba1,WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents,0.764767,"In this paper, we introduce WeLayout, a novel system for segmenting the
layout of corporate documents, which stands for WeChat Layout Analysis System.
Our approach utilizes a sophisticated ensemble of DINO and YOLO models,
specifically developed for the ICDAR 2023 Competition on Robust Layout
Segmentation. Our method significantly surpasses the baseline, securing a top
position on the leaderboard with a mAP of 70.0. To achieve this performance, we
concentrated on enhancing various aspects of the task, such as dataset
augmentation, model architecture, bounding box refinement, and model ensemble
techniques. Additionally, we trained the data separately for each document
category to ensure a higher mean submission score. We also developed an
algorithm for cell matching to further improve our performance. To identify the
optimal weights and IoU thresholds for our model ensemble, we employed a
Bayesian optimization algorithm called the Tree-Structured Parzen Estimator.
Our approach effectively demonstrates the benefits of combining query-based and
anchor-free models for achieving robust layout segmentation in corporate
documents.",None,-1
48a9ee20-3414-4ace-8f8a-7744adf94bd7,Mind the Backbone: Minimizing Backbone Distortion for Robust Object Detection,0.0367524,"Building object detectors that are robust to domain shifts is critical for
real-world applications. Prior approaches fine-tune a pre-trained backbone and
risk overfitting it to in-distribution (ID) data and distorting features useful
for out-of-distribution (OOD) generalization. We propose to use Relative
Gradient Norm (RGN) as a way to measure the vulnerability of a backbone to
feature distortion, and show that high RGN is indeed correlated with lower OOD
performance. Our analysis of RGN yields interesting findings: some backbones
lose OOD robustness during fine-tuning, but others gain robustness because
their architecture prevents the parameters from changing too much from the
initial model. Given these findings, we present recipes to boost OOD robustness
for both types of backbones. Specifically, we investigate regularization and
architectural choices for minimizing gradient updates so as to prevent the
tuned backbone from losing generalizable features. Our proposed techniques
complement each other and show substantial improvements over baselines on
diverse architectures and datasets. Code is available at
https://github.com/VisionLearningGroup/mind_back.",None,-1
63c34b89-30a8-47a9-b96f-e54ec10314f2,SepicNet: Sharp Edges Recovery by Parametric Inference of Curves in 3D Shapes,0.235579,"3D scanning as a technique to digitize objects in reality and create their 3D
models, is used in many fields and areas. Though the quality of 3D scans
depends on the technical characteristics of the 3D scanner, the common drawback
is the smoothing of fine details, or the edges of an object. We introduce
SepicNet, a novel deep network for the detection and parametrization of sharp
edges in 3D shapes as primitive curves. To make the network end-to-end
trainable, we formulate the curve fitting in a differentiable manner. We
develop an adaptive point cloud sampling technique that captures the sharp
features better than uniform sampling. The experiments were conducted on a
newly introduced large-scale dataset of 50k 3D scans, where the sharp edge
annotations were extracted from their parametric CAD models, and demonstrate
significant improvement over state-of-the-art methods.",None,-1
cad6baa3-f348-4fa7-b30a-208aade330f9,Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer,0.43445,"Recent research has shown that independently trained encoders and decoders,
combined through a shared fixed-size representation, can achieve competitive
performance in speech-to-text translation. In this work, we show that this type
of approach can be further improved with multilingual training. We observe
significant improvements in zero-shot cross-modal speech translation, even
outperforming a supervised approach based on XLSR for several languages.",None,-1
ddc1be83-ab4d-48ff-b9e9-0a6f9be9f7f1,Linguistic representations for fewer-shot relation extraction across domains,0.693802,"Recent work has demonstrated the positive impact of incorporating linguistic
representations as additional context and scaffolding on the in-domain
performance of several NLP tasks. We extend this work by exploring the impact
of linguistic representations on cross-domain performance in a few-shot
transfer setting. An important question is whether linguistic representations
enhance generalizability by providing features that function as cross-domain
pivots. We focus on the task of relation extraction on three datasets of
procedural text in two domains, cooking and materials science. Our approach
augments a popular transformer-based architecture by alternately incorporating
syntactic and semantic graphs constructed by freely available off-the-shelf
tools. We examine their utility for enhancing generalization, and investigate
whether earlier findings, e.g. that semantic representations can be more
helpful than syntactic ones, extend to relation extraction in multiple domains.
We find that while the inclusion of these graphs results in significantly
higher performance in few-shot transfer, both types of graph exhibit roughly
equivalent utility.",None,-1
e62eb71b-e987-4bb0-a0e1-6e6c4881c79c,Prompt2Model: Generating Deployable Models from Natural Language Instructions,0.379631,"Large language models (LLMs) enable system builders today to create competent
NLP systems through prompting, where they only need to describe the task in
natural language and provide a few examples. However, in other ways, LLMs are a
step backward from traditional special-purpose NLP models; they require
extensive computational resources for deployment and can be gated behind APIs.
In this paper, we propose Prompt2Model, a general-purpose method that takes a
natural language task description like the prompts provided to LLMs, and uses
it to train a special-purpose model that is conducive to deployment. This is
done through a multi-step process of retrieval of existing datasets and
pretrained models, dataset generation using LLMs, and supervised fine-tuning on
these retrieved and generated datasets. Over three tasks, we demonstrate that
given the same few-shot prompt as input, Prompt2Model trains models that
outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%
while being up to 700 times smaller. We also show that this data can be used to
obtain reliable performance estimates of model performance, enabling model
developers to assess model reliability before deployment. Prompt2Model is
available open-source at https://github.com/neulab/prompt2model.",None,-1
626168a9-1afd-4793-9e3c-bf589286c537,Glancing Future for Simultaneous Machine Translation,0.541831,"Simultaneous machine translation (SiMT) outputs translation while reading the
source sentence. Unlike conventional sequence-to-sequence (seq2seq) training,
existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training,
where the model predicts target tokens based on partial source tokens. However,
the prefix2prefix training diminishes the ability of the model to capture
global information and introduces forced predictions due to the absence of
essential source information. Consequently, it is crucial to bridge the gap
between the prefix2prefix training and seq2seq training to enhance the
translation capability of the SiMT model. In this paper, we propose a novel
method that glances future in curriculum learning to achieve the transition
from the seq2seq training to prefix2prefix training. Specifically, we gradually
reduce the available source information from the whole sentence to the prefix
corresponding to that latency. Our method is applicable to a wide range of SiMT
methods and experiments demonstrate that our method outperforms strong
baselines.",None,-1
eb423407-f085-4f8f-a3a9-7f90b40e9229,PassGPT: Password Modeling and (Guided) Generation with Large Language Models,0.476516,"Large language models (LLMs) successfully model natural language from vast
amounts of text without the need for explicit supervision. In this paper, we
investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a
LLM trained on password leaks for password generation. PassGPT outperforms
existing methods based on generative adversarial networks (GAN) by guessing
twice as many previously unseen passwords. Furthermore, we introduce the
concept of guided password generation, where we leverage PassGPT sampling
procedure to generate passwords matching arbitrary constraints, a feat lacking
in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the
entropy and probability distribution that PassGPT defines over passwords and
discuss their use in enhancing existing password strength estimators.",None,-1
85f31b57-2b2b-4187-8220-4c54dda97aa4,CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design,0.436021,"We formalize the problem of contextual optimization through the lens of
Bayesian experimental design and propose CO-BED -- a general, model-agnostic
framework for designing contextual experiments using information-theoretic
principles. After formulating a suitable information-based objective, we employ
black-box variational methods to simultaneously estimate it and optimize the
designs in a single stochastic gradient scheme. In addition, to accommodate
discrete actions within our framework, we propose leveraging continuous
relaxation schemes, which can naturally be integrated into our variational
objective. As a result, CO-BED provides a general and automated solution to a
wide range of contextual optimization problems. We illustrate its effectiveness
in a number of experiments, where CO-BED demonstrates competitive performance
even when compared to bespoke, model-specific alternatives.",None,-1
0710b563-6e10-42d3-a3ea-a6fab959c8d6,Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers,0.104671,"Autoregressive transformers have shown remarkable success in video
generation. However, the transformers are prohibited from directly learning the
long-term dependency in videos due to the quadratic complexity of
self-attention, and inherently suffering from slow inference time and error
propagation due to the autoregressive process. In this paper, we propose
Memory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of
long-term dependency in videos and fast inference. Based on recent advances in
bidirectional transformers, our method learns to decode the entire
spatio-temporal volume of a video in parallel from partially observed patches.
The proposed transformer achieves a linear time complexity in both encoding and
decoding, by projecting observable context tokens into a fixed number of latent
tokens and conditioning them to decode the masked tokens through the
cross-attention. Empowered by linear complexity and bidirectional modeling, our
method demonstrates significant improvement over the autoregressive
Transformers for generating moderately long videos in both quality and speed.
Videos and code are available at https://sites.google.com/view/mebt-cvpr2023 .",None,-1
1919af6a-a293-4920-ac54-53179843de6d,A Latent Space Theory for Emergent Abilities in Large Language Models,0.161444,"Languages are not created randomly but rather to communicate information.
There is a strong association between languages and their underlying meanings,
resulting in a sparse joint distribution that is heavily peaked according to
their correlations. Moreover, these peak values happen to match with the
marginal distribution of languages due to the sparsity. With the advent of LLMs
trained on big data and large models, we can now precisely assess the marginal
distribution of languages, providing a convenient means of exploring the sparse
structures in the joint distribution for effective inferences. In this paper,
we categorize languages as either unambiguous or {\epsilon}-ambiguous and
present quantitative results to demonstrate that the emergent abilities of
LLMs, such as language understanding, in-context learning, chain-of-thought
prompting, and effective instruction fine-tuning, can all be attributed to
Bayesian inference on the sparse joint distribution of languages.",None,-1
98ad5da4-4be5-476c-af34-2f363e4e7a40,Noisy Image Segmentation With Soft-Dice,0.166288,"This paper presents a study on the soft-Dice loss, one of the most popular
loss functions in medical image segmentation, for situations where noise is
present in target labels. In particular, the set of optimal solutions are
characterized and sharp bounds on the volume bias of these solutions are
provided. It is further shown that a sequence of soft segmentations converging
to optimal soft-Dice also converges to optimal Dice when converted to hard
segmentations using thresholding. This is an important result because soft-Dice
is often used as a proxy for maximizing the Dice metric. Finally, experiments
confirming the theoretical results are provided.",None,-1
ddcf358e-abaf-4def-8095-0866631eccfe,Conformal Prediction for Time Series with Modern Hopfield Networks,0.751651,"To quantify uncertainty, conformal prediction methods are gaining
continuously more interest and have already been successfully applied to
various domains. However, they are difficult to apply to time series as the
autocorrelative structure of time series violates basic assumptions required by
conformal prediction. We propose HopCPT, a novel conformal prediction approach
for time series that not only copes with temporal structures but leverages
them. We show that our approach is theoretically well justified for time series
where temporal dependencies are present. In experiments, we demonstrate that
our new approach outperforms state-of-the-art conformal prediction methods on
multiple real-world time series datasets from four different domains.",None,-1
f89cdbb4-3426-4735-b8b4-8790efa337ed,Detection of healthy and diseased crops in drone captured images using Deep Learning,0.240794,"Monitoring plant health is crucial for maintaining agricultural productivity
and food safety. Disruptions in the plant's normal state, caused by diseases,
often interfere with essential plant activities, and timely detection of these
diseases can significantly mitigate crop loss. In this study, we propose a deep
learning-based approach for efficient detection of plant diseases using
drone-captured imagery. A comprehensive database of various plant species,
exhibiting numerous diseases, was compiled from the Internet and utilized as
the training and test dataset. A Convolutional Neural Network (CNN), renowned
for its performance in image classification tasks, was employed as our primary
predictive model. The CNN model, trained on this rich dataset, demonstrated
superior proficiency in crop disease categorization and detection, even under
challenging imaging conditions. For field implementation, we deployed a
prototype drone model equipped with a high-resolution camera for live
monitoring of extensive agricultural fields. The captured images served as the
input for our trained model, enabling real-time identification of healthy and
diseased plants. Our approach promises an efficient and scalable solution for
improving crop health monitoring systems.",None,-1
f7512f67-211a-4c4d-9c57-3c4c443ac661,Dual Associated Encoder for Face Restoration,0.175146,"Restoring facial details from low-quality (LQ) images has remained a
challenging problem due to its ill-posedness induced by various degradations in
the wild. The existing codebook prior mitigates the ill-posedness by leveraging
an autoencoder and learned codebook of high-quality (HQ) features, achieving
remarkable quality. However, existing approaches in this paradigm frequently
depend on a single encoder pre-trained on HQ data for restoring HQ images,
disregarding the domain gap between LQ and HQ images. As a result, the encoding
of LQ inputs may be insufficient, resulting in suboptimal performance. To
tackle this problem, we propose a novel dual-branch framework named DAEFR. Our
method introduces an auxiliary LQ branch that extracts crucial information from
the LQ inputs. Additionally, we incorporate association training to promote
effective synergy between the two branches, enhancing code prediction and
output quality. We evaluate the effectiveness of DAEFR on both synthetic and
real-world datasets, demonstrating its superior performance in restoring facial
details. Project page: https://liagm.github.io/DAEFR/",None,-1
fe8d22b4-d3e4-44c7-bdff-60a5adc4beee,TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement,0.686279,"Speech enhancement models have greatly progressed in recent years, but still
show limits in perceptual quality of their speech outputs. We propose an
objective for perceptual quality based on temporal acoustic parameters. These
are fundamental speech features that play an essential role in various
applications, including speaker recognition and paralinguistic analysis. We
provide a differentiable estimator for four categories of low-level acoustic
descriptors involving: frequency-related parameters, energy or
amplitude-related parameters, spectral balance parameters, and temporal
features. Unlike prior work that looks at aggregated acoustic parameters or a
few categories of acoustic parameters, our temporal acoustic parameter (TAP)
loss enables auxiliary optimization and improvement of many fine-grain speech
characteristics in enhancement workflows. We show that adding TAPLoss as an
auxiliary objective in speech enhancement produces speech with improved
perceptual quality and intelligibility. We use data from the Deep Noise
Suppression 2020 Challenge to demonstrate that both time-domain models and
time-frequency domain models can benefit from our method.",None,-1
f379f2a7-044d-4bef-a520-f8fc432df276,CLRerNet: Improving Confidence of Lane Detection with LaneIoU,0.478081,"Lane marker detection is a crucial component of the autonomous driving and
driver assistance systems. Modern deep lane detection methods with row-based
lane representation exhibit excellent performance on lane detection benchmarks.
Through preliminary oracle experiments, we firstly disentangle the lane
representation components to determine the direction of our approach. We show
that correct lane positions are already among the predictions of an existing
row-based detector, and the confidence scores that accurately represent
intersection-over-union (IoU) with ground truths are the most beneficial. Based
on the finding, we propose LaneIoU that better correlates with the metric, by
taking the local lane angles into consideration. We develop a novel detector
coined CLRerNet featuring LaneIoU for the target assignment cost and loss
functions aiming at the improved quality of confidence scores. Through careful
and fair benchmark including cross validation, we demonstrate that CLRerNet
outperforms the state-of-the-art by a large margin - enjoying F1 score of
81.43% compared with 80.47% of the existing method on CULane, and 86.47%
compared with 86.10% on CurveLanes.",None,-1
12785056-049a-492b-99a6-f7382d3f53d5,Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction,0.961645,"Large language models (LLMs) have great potential for synthetic data
generation. This work shows that useful data can be synthetically generated
even for tasks that cannot be solved directly by LLMs: for problems with
structured outputs, it is possible to prompt an LLM to perform the task in the
reverse direction, by generating plausible input text for a target output
structure. Leveraging this asymmetry in task difficulty makes it possible to
produce large-scale, high-quality data for complex tasks. We demonstrate the
effectiveness of this approach on closed information extraction, where
collecting ground-truth data is challenging, and no satisfactory dataset exists
to date. We synthetically generate a dataset of 1.8M data points, establish its
superior quality compared to existing datasets in a human evaluation, and use
it to finetune small models (220M and 770M parameters), termed SynthIE, that
outperform the prior state of the art (with equal model size) by a substantial
margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data,
and models are available at https://github.com/epfl-dlab/SynthIE.",None,-1
f63bbea8-7baa-4f98-b051-c3633eb68804,AutoTrial: Prompting Language Models for Clinical Trial Design,0.627051,"Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60% winning rate against the GPT-3.5
baselines via human evaluations.",None,-1
3907b192-b092-4185-b7c7-643ca14e5255,PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment,0.986745,"Camera pose estimation is a long-standing computer vision problem that to
date often relies on classical methods, such as handcrafted keypoint matching,
RANSAC and bundle adjustment. In this paper, we propose to formulate the
Structure from Motion (SfM) problem inside a probabilistic diffusion framework,
modelling the conditional distribution of camera poses given input images. This
novel view of an old problem has several advantages. (i) The nature of the
diffusion framework mirrors the iterative procedure of bundle adjustment. (ii)
The formulation allows a seamless integration of geometric constraints from
epipolar geometry. (iii) It excels in typically difficult scenarios such as
sparse views with wide baselines. (iv) The method can predict intrinsics and
extrinsics for an arbitrary amount of images. We demonstrate that our method
PoseDiffusion significantly improves over the classic SfM pipelines and the
learned approaches on two real-world datasets. Finally, it is observed that our
method can generalize across datasets without further training. Project page:
https://posediffusion.github.io/",None,-1
426d130e-bbd6-42a3-bd5a-9df5f295e680,Tracking Objects and Activities with Attention for Temporal Sentence Grounding,0.2382,"Temporal sentence grounding (TSG) aims to localize the temporal segment which
is semantically aligned with a natural language query in an untrimmed
video.Most existing methods extract frame-grained features or object-grained
features by 3D ConvNet or detection network under a conventional TSG framework,
failing to capture the subtle differences between frames or to model the
spatio-temporal behavior of core persons/objects. In this paper, we introduce a
new perspective to address the TSG task by tracking pivotal objects and
activities to learn more fine-grained spatio-temporal behaviors. Specifically,
we propose a novel Temporal Sentence Tracking Network (TSTNet), which contains
(A) a Cross-modal Targets Generator to generate multi-modal templates and
search space, filtering objects and activities, and (B) a Temporal Sentence
Tracker to track multi-modal targets for modeling the targets' behavior and to
predict query-related segment. Extensive experiments and comparisons with
state-of-the-arts are conducted on challenging benchmarks: Charades-STA and
TACoS. And our TSTNet achieves the leading performance with a considerable
real-time speed.",None,-1
7c2461dc-6089-4ab8-9622-b605a8364626,Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks,0.329145,"Robustness has become an important consideration in deep learning. With the
help of explainable AI, mismatches between an explained model's decision
strategy and the user's domain knowledge (e.g. Clever Hans effects) have been
identified as a starting point for improving faulty models. However, it is less
clear what to do when the user and the explanation agree. In this paper, we
demonstrate that acceptance of explanations by the user is not a guarantee for
a machine learning model to be robust against Clever Hans effects, which may
remain undetected. Such hidden flaws of the model can nevertheless be
mitigated, and we demonstrate this by contributing a new method,
Explanation-Guided Exposure Minimization (EGEM), that preemptively prunes
variations in the ML model that have not been the subject of positive
explanation feedback. Experiments demonstrate that our approach leads to models
that strongly reduce their reliance on hidden Clever Hans strategies, and
consequently achieve higher accuracy on new data.",None,-1
63b5f17b-ff82-4c77-a277-84fecfdd279c,Coarse Is Better? A New Pipeline Towards Self-Supervised Learning with Uncurated Images,0.155583,"Most self-supervised learning (SSL) methods often work on curated datasets
where the object-centric assumption holds. This assumption breaks down in
uncurated images. Existing scene image SSL methods try to find the two views
from original scene images that are well matched or dense, which is both
complex and computationally heavy. This paper proposes a conceptually different
pipeline: first find regions that are coarse objects (with adequate
objectness), crop them out as pseudo object-centric images, then any SSL method
can be directly applied as in a real object-centric dataset. That is, coarse
crops benefits scene images SSL. A novel cropping strategy that produces coarse
object box is proposed. The new pipeline and cropping strategy successfully
learn quality features from uncurated datasets without ImageNet. Experiments
show that our pipeline outperforms existing SSL methods (MoCo-v2, DenseCL and
MAE) on classification, detection and segmentation tasks. We further conduct
extensively ablations to verify that: 1) the pipeline do not rely on pretrained
models; 2) the cropping strategy is better than existing object discovery
methods; 3) our method is not sensitive to hyperparameters and data
augmentations.",None,-1
5ff5d10b-f619-49ac-b5f2-3eec674cf389,Pre-training Contextual Location Embeddings in Personal Trajectories via Efficient Hierarchical Location Representations,0.208703,"Pre-training the embedding of a location generated from human mobility data
has become a popular method for location based services. In practice, modeling
the location embedding is too expensive, due to the large number of locations
to be trained in situations with fine-grained resolution or extensive target
regions. Previous studies have handled less than ten thousand distinct
locations, which is insufficient in the real-world applications. To tackle this
problem, we propose a Geo-Tokenizer, designed to efficiently reduce the number
of locations to be trained by representing a location as a combination of
several grids at different scales. In the Geo-Tokenizer, a grid at a larger
scale shares the common set of grids at smaller scales, which is a key factor
in reducing the size of the location vocabulary. The sequences of locations
preprocessed with the Geo-Tokenizer are utilized by a causal location embedding
model to capture the temporal dependencies of locations. This model dynamically
calculates the embedding vector of a target location, which varies depending on
its trajectory. In addition, to efficiently pre-train the location embedding
model, we propose the Hierarchical Auto-regressive Location Model objective to
effectively train decomposed locations in the Geo-Tokenizer. We conducted
experiments on two real-world user trajectory datasets using our pre-trained
location model. The experimental results show that our model significantly
improves the performance of downstream tasks with fewer model parameters
compared to existing location embedding methods.",None,-1
5be47d08-a770-4897-8f84-97d207fd6c93,NumHG: A Dataset for Number-Focused Headline Generation,0.192214,"Headline generation, a key task in abstractive summarization, strives to
condense a full-length article into a succinct, single line of text. Notably,
while contemporary encoder-decoder models excel based on the ROUGE metric, they
often falter when it comes to the precise generation of numerals in headlines.
We identify the lack of datasets providing fine-grained annotations for
accurate numeral generation as a major roadblock. To address this, we introduce
a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news
articles for detailed investigation. Further, we evaluate five well-performing
models from previous headline generation tasks using human evaluation in terms
of numerical accuracy, reasonableness, and readability. Our study reveals a
need for improvement in numerical accuracy, demonstrating the potential of the
NumHG dataset to drive progress in number-focused headline generation and
stimulate further discussions in numeral-focused text generation.",None,-1
07cbaa65-bccd-40a3-9822-2ece9a24dd3f,KEST: Kernel Distance Based Efficient Self-Training for Improving Controllable Text Generation,0.0224181,"Self-training (ST) has come to fruition in language understanding tasks by
producing pseudo labels, which reduces the labeling bottleneck of language
model fine-tuning. Nevertheless, in facilitating semi-supervised controllable
language generation, ST faces two key challenges. First, augmented by
self-generated pseudo text, generation models tend to over-exploit the
previously learned text distribution, suffering from mode collapse and poor
generation diversity. Second, generating pseudo text in each iteration is
time-consuming, severely decelerating the training process. In this work, we
propose KEST, a novel and efficient self-training framework to handle these
problems. KEST utilizes a kernel-based loss, rather than standard cross
entropy, to learn from the soft pseudo text produced by a shared
non-autoregressive generator. We demonstrate both theoretically and empirically
that KEST can benefit from more diverse pseudo text in an efficient manner,
which allows not only refining and exploiting the previously fitted
distribution but also enhanced exploration towards a larger potential text
space, providing a guarantee of improved performance. Experiments on three
controllable generation tasks demonstrate that KEST significantly improves
control accuracy while maintaining comparable text fluency and generation
diversity against several strong baselines.",None,-1
4afe46d7-6e13-45a0-91dc-a3cf32d4e194,Multi-Label Self-Supervised Learning with Scene Images,0.395866,"Self-supervised learning (SSL) methods targeting scene images have seen a
rapid growth recently, and they mostly rely on either a dedicated dense
matching mechanism or a costly unsupervised object discovery module. This paper
shows that instead of hinging on these strenuous operations, quality image
representations can be learned by treating scene/multi-label image SSL simply
as a multi-label classification problem, which greatly simplifies the learning
framework. Specifically, multiple binary pseudo-labels are assigned for each
input image by comparing its embeddings with those in two dictionaries, and the
network is optimized using the binary cross entropy loss. The proposed method
is named Multi-Label Self-supervised learning (MLS). Visualizations
qualitatively show that clearly the pseudo-labels by MLS can automatically find
semantically similar pseudo-positive pairs across different images to
facilitate contrastive learning. MLS learns high quality representations on
MS-COCO and achieves state-of-the-art results on classification, detection and
segmentation benchmarks. At the same time, MLS is much simpler than existing
methods, making it easier to deploy and for further exploration.",None,-1
4e6af1a9-aca9-4855-80a3-c18a25cd732a,Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization,0.21046,"Generating persona consistent dialogue response is important for developing
an intelligent conversational agent. Recent works typically fine-tune
large-scale pre-trained models on this task by concatenating persona texts and
dialogue history as a single input sequence to generate the target response.
While simple and effective, our analysis shows that this popular practice is
seriously affected by order sensitivity where different input orders of persona
sentences significantly impact the quality and consistency of generated
response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and
83.2% on BART). To mitigate the order sensitivity problem, we propose a
model-agnostic framework, ORder Insensitive Generation (ORIG), which enables
dialogue models to learn robust representation under different persona orders
and improve the consistency of response generation. Experiments on the
Persona-Chat dataset justify the effectiveness and superiority of our method
with two dominant pre-trained models (GPT2 and BART).",None,-1
aaf34424-a27e-4d75-9770-ea8a017e2443,Ten New Benchmarks for Optimization,0.0381487,"Benchmarks are used for testing new optimization algorithms and their
variants to evaluate their performance. Most existing benchmarks are smooth
functions. This chapter introduces ten new benchmarks with different
properties, including noise, discontinuity, parameter estimation and unknown
paths.",None,-1
f42fbe1c-5477-4d3c-bc52-cd194a612f59,Multilingual Word Error Rate Estimation: e-WER3,0.595366,"The success of the multilingual automatic speech recognition systems
empowered many voice-driven applications. However, measuring the performance of
such systems remains a major challenge, due to its dependency on manually
transcribed speech data in both mono- and multilingual scenarios. In this
paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on
acoustic and lexical representation to estimate word error rate. We demonstrate
the effectiveness of eWER3 to (i) predict WER without using any internal states
from the ASR and (ii) use the multilingual shared latent space to push the
performance of the close-related languages. We show our proposed multilingual
model outperforms the previous monolingual word error rate estimation method
(eWER2) by an absolute 9\% increase in Pearson correlation coefficient (PCC),
with better overall estimation between the predicted and reference WER.",None,-1
be629738-090d-4898-966b-765d471f3428,E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition,0.507972,"Most named entity recognition (NER) systems focus on improving model
performance, ignoring the need to quantify model uncertainty, which is critical
to the reliability of NER systems in open environments. Evidential deep
learning (EDL) has recently been proposed as a promising solution to explicitly
model predictive uncertainty for classification tasks. However, directly
applying EDL to NER applications faces two challenges, i.e., the problems of
sparse entities and OOV/OOD entities in NER tasks. To address these challenges,
we propose a trustworthy NER framework named E-NER by introducing two
uncertainty-guided loss terms to the conventional EDL, along with a series of
uncertainty-guided training strategies. Experiments show that E-NER can be
applied to multiple NER paradigms to obtain accurate uncertainty estimation.
Furthermore, compared to state-of-the-art baselines, the proposed method
achieves a better OOV/OOD detection performance and better generalization
ability on OOV entities.",None,-1
3cddcc1f-e785-453b-a25f-71c5428f0cb0,Robustifying Language Models with Test-Time Adaptation,0.0192019,"Large-scale language models achieved state-of-the-art performance over a
number of language tasks. However, they fail on adversarial language examples,
which are sentences optimized to fool the language models but with similar
semantic meanings for humans. While prior work focuses on making the language
model robust at training time, retraining for robustness is often unrealistic
for large-scale foundation models. Instead, we propose to make the language
models robust at test time. By dynamically adapting the input sentence with
predictions from masked words, we show that we can reverse many language
adversarial attacks. Since our approach does not require any training, it works
for novel tasks at test time and can adapt to novel adversarial corruptions.
Visualizations and empirical results on two popular sentence classification
datasets demonstrate that our method can repair adversarial language attacks
over 65% o",None,-1
8da13d93-3785-4c2b-bbbc-012e3fef93ce,AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial,0.0966447,"Art curatorial practice is characterized by the presentation of an art
collection in a knowledgeable way. Machine processes are characterized by their
capacity to manage and analyze large amounts of data. This paper envisages AI
curation and audience interaction to explore the implications of contemporary
machine learning models for the curatorial world. This project was developed
for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May
Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city
of Helsinki through the lens of machine perception. We use visual-textual
models to place indoor artworks in public spaces, assigning fictional
coordinates based on similarity scores. We transform the space that each
artwork inhabits in the city by generating synthetic 360 art panoramas. We
guide the generation estimating depth values from 360 panoramas at each artwork
location, and machine-generated prompts of the artworks. The result of this
project is an AI curation that places the artworks in their imagined physical
space, blurring the lines of artwork, context, and machine perception. The work
is virtually presented as a web-based installation on this link
http://newlyformedcity.net/, where users can navigate an alternative version of
the city while exploring and interacting with its cultural heritage at scale.",None,-1
1df0ff14-ebf9-4169-afee-f25705999829,Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy,0.427371,"We address an important gap in detecting political bias in news articles.
Previous works that perform document classification can be influenced by the
writing style of each news outlet, leading to overfitting and limited
generalizability. Our approach overcomes this limitation by considering both
the sentence-level semantics and the document-level rhetorical structure,
resulting in a more robust and style-agnostic approach to detecting political
bias in news articles. We introduce a novel multi-head hierarchical attention
model that effectively encodes the structure of long documents through a
diverse ensemble of attention heads. While journalism follows a formalized
rhetorical structure, the writing style may vary by news outlet. We demonstrate
that our method overcomes this domain dependency and outperforms previous
approaches for robustness and accuracy. Further analysis and human evaluation
demonstrate the ability of our model to capture common discourse structures in
journalism. Our code is available at:
https://github.com/xfactlab/emnlp2023-Document-Hierarchy",None,-1
e16acb9b-875f-4252-8336-1ace1553b6dd,CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation,0.587348,"Annotated data plays a critical role in Natural Language Processing (NLP) in
training models and evaluating their performance. Given recent developments in
Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot
capability on many text-annotation tasks, comparable with or even exceeding
human annotators. Such LLMs can serve as alternatives for manual annotation,
due to lower costs and higher scalability. However, limited work has leveraged
LLMs as complementary annotators, nor explored how annotation work is best
allocated among humans and LLMs to achieve both quality and cost objectives. We
propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of
unstructured texts at scale. Under this framework, we utilize uncertainty to
estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to
be an effective means to allocate work from results on different datasets, with
up to 21% performance improvement over random baseline. For code
implementation, see https://github.com/SALT-NLP/CoAnnotating.",None,-1
31e5b551-b731-4b98-8751-bf4d22c0f86a,On the Calibration of Uncertainty Estimation in LiDAR-based Semantic Segmentation,0.367418,"The confidence calibration of deep learning-based perception models plays a
crucial role in their reliability. Especially in the context of autonomous
driving, downstream tasks like prediction and planning depend on accurate
confidence estimates. In point-wise multiclass classification tasks like
sematic segmentation the model has to deal with heavy class imbalances. Due to
their underrepresentation, the confidence calibration of classes with smaller
instances is challenging but essential, not only for safety reasons. We propose
a metric to measure the confidence calibration quality of a semantic
segmentation model with respect to individual classes. It is calculated by
computing sparsification curves for each class based on the uncertainty
estimates. We use the classification calibration metric to evaluate uncertainty
estimation methods with respect to their confidence calibration of
underrepresented classes. We furthermore suggest a double use for the method to
automatically find label problems to improve the quality of hand- or
auto-annotated datasets.",None,-1
3cfcfc7b-81df-4776-915e-2bec30c5fd5d,Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs,0.0370053,"Text-to-video (T2V) synthesis has gained increasing attention in the
community, in which the recently emerged diffusion models (DMs) have
promisingly shown stronger performance than the past approaches. While existing
state-of-the-art DMs are competent to achieve high-resolution video generation,
they may largely suffer from key limitations (e.g., action occurrence
disorders, crude video motions) with respect to the intricate temporal dynamics
modeling, one of the crux of video synthesis. In this work, we investigate
strengthening the awareness of video dynamics for DMs, for high-quality T2V
generation. Inspired by human intuition, we design an innovative dynamic scene
manager (dubbed as Dysen) module, which includes (step-1) extracting from input
text the key actions with proper time-order arrangement, (step-2) transforming
the action schedules into the dynamic scene graph (DSG) representations, and
(step-3) enriching the scenes in the DSG with sufficient and reasonable
details. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via
in-context learning, Dysen realizes (nearly) human-level temporal dynamics
understanding. Finally, the resulting video DSG with rich action scene details
is encoded as fine-grained spatio-temporal features, integrated into the
backbone T2V DM for video generating. Experiments on popular T2V datasets
suggest that our Dysen-VDM consistently outperforms prior arts with significant
margins, especially in scenarios with complex actions. Codes at
https://haofei.vip/Dysen-VDM",None,-1
dad00369-aa47-42d6-ac2b-dc7f80e82332,Efficient Enumeration of Markov Equivalent DAGs,0.218245,"Enumerating the directed acyclic graphs (DAGs) of a Markov equivalence class
(MEC) is an important primitive in causal analysis. The central resource from
the perspective of computational complexity is the delay, that is, the time an
algorithm that lists all members of the class requires between two consecutive
outputs. Commonly used algorithms for this task utilize the rules proposed by
Meek (1995) or the transformational characterization by Chickering (1995), both
resulting in superlinear delay. In this paper, we present the first linear-time
delay algorithm. On the theoretical side, we show that our algorithm can be
generalized to enumerate DAGs represented by models that incorporate background
knowledge, such as MPDAGs; on the practical side, we provide an efficient
implementation and evaluate it in a series of experiments. Complementary to the
linear-time delay algorithm, we also provide intriguing insights into Markov
equivalence itself: All members of an MEC can be enumerated such that two
successive DAGs have structural Hamming distance at most three.",None,-1
35875124-1766-40c3-b8d7-569e29c18d01,Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification,0.168171,"Recent advances in weakly supervised text classification mostly focus on
designing sophisticated methods to turn high-level human heuristics into
quality pseudo-labels. In this paper, we revisit the seed matching-based
method, which is arguably the simplest way to generate pseudo-labels, and show
that its power was greatly underestimated. We show that the limited performance
of seed matching is largely due to the label bias injected by the simple
seed-match rule, which prevents the classifier from learning reliable
confidence for selecting high-quality pseudo-labels. Interestingly, simply
deleting the seed words present in the matched input texts can mitigate the
label bias and help learn better confidence. Subsequently, the performance
achieved by seed matching can be improved significantly, making it on par with
or even better than the state-of-the-art. Furthermore, to handle the case when
the seed words are not made known, we propose to simply delete the word tokens
in the input text randomly with a high deletion ratio. Remarkably, seed
matching equipped with this random deletion method can often achieve even
better performance than that with seed deletion.",None,-1
a1aae083-0e4f-40a7-a727-cc9cc210580b,OPE-SR: Orthogonal Position Encoding for Designing a Parameter-free Upsampling Module in Arbitrary-scale Image Super-Resolution,0.624222,"Implicit neural representation (INR) is a popular approach for
arbitrary-scale image super-resolution (SR), as a key component of INR,
position encoding improves its representation ability. Motivated by position
encoding, we propose orthogonal position encoding (OPE) - an extension of
position encoding - and an OPE-Upscale module to replace the INR-based
upsampling module for arbitrary-scale image super-resolution. Same as INR, our
OPE-Upscale Module takes 2D coordinates and latent code as inputs; however it
does not require training parameters. This parameter-free feature allows the
OPE-Upscale Module to directly perform linear combination operations to
reconstruct an image in a continuous manner, achieving an arbitrary-scale image
reconstruction. As a concise SR framework, our method has high computing
efficiency and consumes less memory comparing to the state-of-the-art (SOTA),
which has been confirmed by extensive experiments and evaluations. In addition,
our method has comparable results with SOTA in arbitrary scale image
super-resolution. Last but not the least, we show that OPE corresponds to a set
of orthogonal basis, justifying our design principle.",None,-1
0efa094b-238f-4eef-ac08-b8834f0c95a4,Gemini Pro Defeated by GPT-4V: Evidence from Education,0.893935,"This study compared the classification performance of Gemini Pro and GPT-4V
in educational settings. Employing visual question answering (VQA) techniques,
the study examined both models' abilities to read text-based rubrics and then
automatically score student-drawn models in science education. We employed both
quantitative and qualitative analyses using a dataset derived from
student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics
for Image Feedback) prompting methods. The findings reveal that GPT-4V
significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic
Weighted Kappa. The qualitative analysis reveals that the differences may be
due to the models' ability to process fine-grained texts in images and overall
image classification performance. Even adapting the NERIF approach by further
de-sizing the input images, Gemini Pro seems not able to perform as well as
GPT-4V. The findings suggest GPT-4V's superior capability in handling complex
multimodal educational tasks. The study concludes that while both models
represent advancements in AI, GPT-4V's higher performance makes it a more
suitable tool for educational applications involving multimodal data
interpretation.",None,-1
5c795e7d-8eeb-4e2c-b4ac-4a56413dd9b2,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,0.887438,"Semantic segmentation of point clouds in autonomous driving datasets requires
techniques that can process large numbers of points efficiently. Sparse 3D
convolutions have become the de-facto tools to construct deep neural networks
for this task: they exploit point cloud sparsity to reduce the memory and
computational loads and are at the core of today's best methods. In this paper,
we propose an alternative method that reaches the level of state-of-the-art
methods without requiring sparse convolutions. We actually show that such level
of performance is achievable by relying on tools a priori unfit for large scale
and high-performing 3D perception. In particular, we propose a novel 3D
backbone, WaffleIron, made almost exclusively of MLPs and dense 2D convolutions
and present how to train it to reach high performance on SemanticKITTI and
nuScenes. We believe that WaffleIron is a compelling alternative to backbones
using sparse 3D convolutions, especially in frameworks and on hardware where
those convolutions are not readily available.",None,-1
5f7a2b5b-d9f4-4145-8ec1-6a610eea85a3,Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery,0.352663,"Discovering novel concepts from unlabelled data and in a continuous manner is
an important desideratum of lifelong learners. In the literature such problems
have been partially addressed under very restricted settings, where either
access to labelled data is provided for discovering novel concepts (e.g., NCD)
or learning occurs for a limited number of incremental steps (e.g.,
class-iNCD). In this work we challenge the status quo and propose a more
challenging and practical learning paradigm called MSc-iNCD, where learning
occurs continuously and unsupervisedly, while exploiting the rich priors from
large-scale pre-trained models. To this end, we propose simple baselines that
are not only resilient under longer learning scenarios, but are surprisingly
strong when compared with sophisticated state-of-the-art methods. We conduct
extensive empirical evaluation on a multitude of benchmarks and show the
effectiveness of our proposed baselines, which significantly raises the bar.",None,-1
3a9601ae-c930-471d-a294-77b8950bace1,Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation,0.542866,"Generating images with both photorealism and multiview 3D consistency is
crucial for 3D-aware GANs, yet existing methods struggle to achieve them
simultaneously. Improving the photorealism via CNN-based 2D super-resolution
can break the strict 3D consistency, while keeping the 3D consistency by
learning high-resolution 3D representations for direct rendering often
compromises image quality. In this paper, we propose a novel learning strategy,
namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate
high-quality images while maintaining their strict 3D consistency, by letting
the images synthesized by the generator's 3D rendering branch to mimic those
generated by its 2D super-resolution branch. We also introduce 3D-aware
convolutions into the generator for better 3D representation learning, which
further improves the image generation quality. With the above strategies, our
method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats,
respectively, at 512x512 resolution, largely outperforming existing 3D-aware
GANs using direct 3D rendering and coming very close to the previous
state-of-the-art method that leverages 2D super-resolution. Project website:
https://seanchenxy.github.io/Mimic3DWeb.",None,-1
2659ceb9-2513-4328-8407-f0856dc3af95,"QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models",0.767023,"Misinformation spreading in mainstream and social media has been misleading
users in different ways. Manual detection and verification efforts by
journalists and fact-checkers can no longer cope with the great scale and quick
spread of misleading information. This motivated research and industry efforts
to develop systems for analyzing and verifying news spreading online. The
SemEval-2023 Task 3 is an attempt to address several subtasks under this
overarching problem, targeting writing techniques used in news articles to
affect readers' opinions. The task addressed three subtasks with six languages,
in addition to three ``surprise'' test languages, resulting in 27 different
test setups. This paper describes our participating system to this task. Our
team is one of the 6 teams that successfully submitted runs for all setups. The
official results show that our system is ranked among the top 3 systems for 10
out of the 27 setups.",None,-1
79b164a3-4b8f-4116-848b-5e6456917862,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,0.742348,"Predicting the pose of objects from a single image is an important but
difficult computer vision problem. Methods that predict a single point estimate
do not predict the pose of objects with symmetries well and cannot represent
uncertainty. Alternatively, some works predict a distribution over orientations
in $\mathrm{SO}(3)$. However, training such models can be computation- and
sample-inefficient. Instead, we propose a novel mapping of features from the
image domain to the 3D rotation manifold. Our method then leverages
$\mathrm{SO}(3)$ equivariant layers, which are more sample efficient, and
outputs a distribution over rotations that can be sampled at arbitrary
resolution. We demonstrate the effectiveness of our method at object
orientation prediction, and achieve state-of-the-art performance on the popular
PASCAL3D+ dataset. Moreover, we show that our method can model complex object
symmetries, without any modifications to the parameters or loss function. Code
is available at https://dmklee.github.io/image2sphere.",None,-1
a5536ade-70d2-42e4-9554-5d2afe7e217b,Characterizing Financial Market Coverage using Artificial Intelligence,0.377661,"This paper scrutinizes a database of over 4900 YouTube videos to characterize
financial market coverage. Financial market coverage generates a large number
of videos. Therefore, watching these videos to derive actionable insights could
be challenging and complex. In this paper, we leverage Whisper, a
speech-to-text model from OpenAI, to generate a text corpus of market coverage
videos from Bloomberg and Yahoo Finance. We employ natural language processing
to extract insights regarding language use from the market coverage. Moreover,
we examine the prominent presence of trending topics and their evolution over
time, and the impacts that some individuals and organizations have on the
financial market. Our characterization highlights the dynamics of the financial
market coverage and provides valuable insights reflecting broad discussions
regarding recent financial events and the world economy.",None,-1
026143af-cdb9-4b7d-a045-0224ac9d4528,WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models,0.692917,"The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the
development of large models, leading to the creation of numerous impressive
large language models(LLMs) and multimodal large language models (MLLMs). These
cutting-edge models owe their remarkable performance to high-quality data.
However, the details of the training data used in leading paradigms are often
kept confidential. This lack of transparency, coupled with the scarcity of
open-source data, impedes further developments within the community. As a
response, this paper presents ""Wan Juan"", a large-scale multimodal dataset
composed of both Chinese and English data, collected from a wide range of web
sources. The dataset incorporates text, image-text, and video modalities, with
a total volume exceeding 2TB. It was utilized in the training of InternLM, a
model that demonstrated significant advantages in multi-dimensional evaluations
when compared to models of a similar scale. All data can be accessed at
https://opendatalab.org.cn/WanJuan1.0.",None,-1
83816abd-4a22-477f-ad6d-562b61e8ae9b,Diffusion Models for Interferometric Satellite Aperture Radar,0.834891,"Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.",None,-1
b1e1f03e-bc28-4ee0-bf2f-de1fcb78dff1,Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering,0.0672368,"The field of visual question answering (VQA) has recently seen a surge in
research focused on providing explanations for predicted answers. However,
current systems mostly rely on separate models to predict answers and generate
explanations, leading to less grounded and frequently inconsistent results. To
address this, we propose a multitask learning approach towards a Unified Model
for Answer and Explanation generation (UMAE). Our approach involves the
addition of artificial prompt tokens to training data and fine-tuning a
multimodal encoder-decoder model on a variety of VQA-related tasks. In our
experiments, UMAE models surpass the prior state-of-the-art answer accuracy on
A-OKVQA by 10~15%, show competitive results on OK-VQA, achieve new
state-of-the-art explanation scores on A-OKVQA and VCR, and demonstrate
promising out-of-domain performance on VQA-X.",None,-1
ab275335-6ee0-46a3-9360-e2c0835c26b9,UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning,0.407268,"Multimodal learning aims to imitate human beings to acquire complementary
information from multiple modalities for various downstream tasks. However,
traditional aggregation-based multimodal fusion methods ignore the
inter-modality relationship, treat each modality equally, suffer sensor noise,
and thus reduce multimodal learning performance. In this work, we propose a
novel multimodal contrastive method to explore more reliable multimodal
representations under the weak supervision of unimodal predicting.
Specifically, we first capture task-related unimodal representations and the
unimodal predictions from the introduced unimodal predicting task. Then the
unimodal representations are aligned with the more effective one by the
designed multimodal contrastive method under the supervision of the unimodal
predictions. Experimental results with fused features on two image-text
classification benchmarks UPMC-Food-101 and N24News show that our proposed
Unimodality-Supervised MultiModal Contrastive UniS-MMC learning method
outperforms current state-of-the-art multimodal methods. The detailed ablation
study and analysis further demonstrate the advantage of our proposed method.",None,-1
9eb78906-b0bf-47cf-a9e1-4e22c42b0fd6,Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question,0.186197,"Hybrid question answering (HybridQA) over the financial report contains both
textual and tabular data, and requires the model to select the appropriate
evidence for the numerical reasoning task. Existing methods based on
encoder-decoder framework employ a expression tree-based decoder to solve
numerical reasoning problems. However, encoders rely more on Machine Reading
Comprehension (MRC) methods, which take table serialization and text splicing
as input, damaging the granularity relationship between table and text as well
as the spatial structure information of table itself. In order to solve these
problems, the paper proposes a Multi-View Graph (MVG) Encoder to take the
relations among the granularity into account and capture the relations from
multiple view. By utilizing MVGE as a module, we constuct Tabular View,
Relation View and Numerical View which aim to retain the original
characteristics of the hybrid data. We validate our model on the publicly
available table-text hybrid QA benchmark (TAT-QA) and outperform the
state-of-the-art model.",None,-1
59c61a8a-40e5-4b76-9b3a-5ba8d335d401,Heteroskedastic Geospatial Tracking with Distributed Camera Networks,0.250813,"Visual object tracking has seen significant progress in recent years.
However, the vast majority of this work focuses on tracking objects within the
image plane of a single camera and ignores the uncertainty associated with
predicted object locations. In this work, we focus on the geospatial object
tracking problem using data from a distributed camera network. The goal is to
predict an object's track in geospatial coordinates along with uncertainty over
the object's location while respecting communication constraints that prohibit
centralizing raw image data. We present a novel single-object geospatial
tracking data set that includes high-accuracy ground truth object locations and
video data from a network of four cameras. We present a modeling framework for
addressing this task including a novel backbone model and explore how
uncertainty calibration and fine-tuning through a differentiable tracker affect
performance.",None,-1
8a0266de-c65f-48a8-a65a-b72769a8bce1,Distribution-Aware Prompt Tuning for Vision-Language Models,0.135272,"Pre-trained vision-language models (VLMs) have shown impressive performance
on various downstream tasks by utilizing knowledge learned from large data. In
general, the performance of VLMs on target tasks can be further improved by
prompt tuning, which adds context to the input image or text. By leveraging
data from target tasks, various prompt-tuning methods have been studied in the
literature. A key to prompt tuning is the feature space alignment between two
modalities via learnable vectors with model parameters fixed. We observed that
the alignment becomes more effective when embeddings of each modality are
`well-arranged' in the latent space. Inspired by this observation, we proposed
distribution-aware prompt tuning (DAPT) for vision-language models, which is
simple yet effective. Specifically, the prompts are learned by maximizing
inter-dispersion, the distance between classes, as well as minimizing the
intra-dispersion measured by the distance between embeddings from the same
class. Our extensive experiments on 11 benchmark datasets demonstrate that our
method significantly improves generalizability. The code is available at
https://github.com/mlvlab/DAPT.",None,-1
60c355ee-7c77-44cb-8c80-acb583cec9bc,DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion,0.571497,"While the community of 3D point cloud generation has witnessed a big growth
in recent years, there still lacks an effective way to enable intuitive user
control in the generation process, hence limiting the general utility of such
methods. Since an intuitive way of decomposing a shape is through its parts, we
propose to tackle the task of controllable part-based point cloud generation.
We introduce DiffFacto, a novel probabilistic generative model that learns the
distribution of shapes with part-level control. We propose a factorization that
models independent part style and part configuration distributions and presents
a novel cross-diffusion network that enables us to generate coherent and
plausible shapes under our proposed factorization. Experiments show that our
method is able to generate novel shapes with multiple axes of control. It
achieves state-of-the-art part-level generation quality and generates plausible
and coherent shapes while enabling various downstream editing applications such
as shape interpolation, mixing, and transformation editing. Project website:
https://difffacto.github.io/",None,-1
2c9cbf11-f5bb-47df-8850-2c148aac205c,Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision,0.0857378,"Although existing image anomaly detection methods yield impressive results,
they are mostly an offline learning paradigm that requires excessive data
pre-collection, limiting their adaptability in industrial scenarios with online
streaming data. Online learning-based image anomaly detection methods are more
compatible with industrial online streaming data but are rarely noticed. For
the first time, this paper presents a fully online learning image anomaly
detection method, namely LeMO, learning memory for online image anomaly
detection. LeMO leverages learnable memory initialized with orthogonal random
noise, eliminating the need for excessive data in memory initialization and
circumventing the inefficiencies of offline data collection. Moreover, a
contrastive learning-based loss function for anomaly detection is designed to
enable online joint optimization of memory and image target-oriented features.
The presented method is simple and highly effective. Extensive experiments
demonstrate the superior performance of LeMO in the online setting.
Additionally, in the offline setting, LeMO is also competitive with the current
state-of-the-art methods and achieves excellent performance in few-shot
scenarios.",None,-1
507fc34a-ebbe-489b-a8ba-41b3a888e998,Algorithmic Transparency and Manipulation,0.773198,"A series of recent papers raises worries about the manipulative potential of
algorithmic transparency. But while the concern is apt and relevant, it is
based on a fraught understanding of manipulation. Therefore, this paper draws
attention to the indifference view of manipulation, which explains better than
the vulnerability view why algorithmic transparency has manipulative potential.
The paper also raises pertinent research questions for future studies of
manipulation in the context of algorithmic transparency.",None,-1
326481fb-fd35-4136-9682-81da4c9e4b6c,RLHF and IIA: Perverse Incentives,0.0648904,"Existing algorithms for reinforcement learning from human feedback (RLHF) can
incentivize responses at odds with preferences because they are based on models
that assume independence of irrelevant alternatives (IIA). The perverse
incentives induced by IIA hinder innovations on query formats and learning
algorithms.",None,-1
4641647a-0731-4d57-b8a8-f56136d0ed9d,Together We Make Sense -- Learning Meta-Sense Embeddings from Pretrained Static Sense Embeddings,0.078704,"Sense embedding learning methods learn multiple vectors for a given ambiguous
word, corresponding to its different word senses. For this purpose, different
methods have been proposed in prior work on sense embedding learning that use
different sense inventories, sense-tagged corpora and learning methods.
However, not all existing sense embeddings cover all senses of ambiguous words
equally well due to the discrepancies in their training resources. To address
this problem, we propose the first-ever meta-sense embedding method --
Neighbour Preserving Meta-Sense Embeddings, which learns meta-sense embeddings
by combining multiple independently trained source sense embeddings such that
the sense neighbourhoods computed from the source embeddings are preserved in
the meta-embedding space. Our proposed method can combine source sense
embeddings that cover different sets of word senses. Experimental results on
Word Sense Disambiguation (WSD) and Word-in-Context (WiC) tasks show that the
proposed meta-sense embedding method consistently outperforms several
competitive baselines.",None,-1
4fa6e12e-8c87-4bc3-ac9d-3d87f1437b06,Generating Topic Pages for Scientific Concepts Using Scientific Publications,0.314966,"In this paper, we describe Topic Pages, an inventory of scientific concepts
and information around them extracted from a large collection of scientific
books and journals. The main aim of Topic Pages is to provide all the necessary
information to the readers to understand scientific concepts they come across
while reading scholarly content in any scientific domain. Topic Pages are a
collection of automatically generated information pages using NLP and ML, each
corresponding to a scientific concept. Each page contains three pieces of
information: a definition, related concepts, and the most relevant snippets,
all extracted from scientific peer-reviewed publications. In this paper, we
discuss the details of different components to extract each of these elements.
The collection of pages in production contains over 360,000 Topic Pages across
20 different scientific domains with an average of 23 million unique visits per
month, constituting it a popular source for scientific information.",None,-1
c1750945-9e63-4443-95a6-0b789a5216c6,Towards Better Entity Linking with Multi-View Enhanced Distillation,0.224034,"Dense retrieval is widely used for entity linking to retrieve entities from
large-scale knowledge bases. Mainstream techniques are based on a dual-encoder
framework, which encodes mentions and entities independently and calculates
their relevances via rough interaction metrics, resulting in difficulty in
explicitly modeling multiple mention-relevant parts within entities to match
divergent mentions. Aiming at learning entity representations that can match
divergent mentions, this paper proposes a Multi-View Enhanced Distillation
(MVD) framework, which can effectively transfer knowledge of multiple
fine-grained and mention-relevant parts within entities from cross-encoders to
dual-encoders. Each entity is split into multiple views to avoid irrelevant
information being over-squashed into the mention-relevant view. We further
design cross-alignment and self-alignment mechanisms for this framework to
facilitate fine-grained knowledge distillation from the teacher model to the
student model. Meanwhile, we reserve a global-view that embeds the entity as a
whole to prevent dispersal of uniform information. Experiments show our method
achieves state-of-the-art performance on several entity linking benchmarks.",None,-1
5794df41-2a33-466c-90ac-5ac674ac7e4e,Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution,0.919068,"Using diffusion models to solve inverse problems is a growing field of
research. Current methods assume the degradation to be known and provide
impressive results in terms of restoration quality and diversity. In this work,
we leverage the efficiency of those models to jointly estimate the restored
image and unknown parameters of the degradation model such as blur kernel. In
particular, we designed an algorithm based on the well-known
Expectation-Minimization (EM) estimation method and diffusion models. Our
method alternates between approximating the expected log-likelihood of the
inverse problem using samples drawn from a diffusion model and a maximization
step to estimate unknown model parameters. For the maximization step, we also
introduce a novel blur kernel regularization based on a Plug \& Play denoiser.
Diffusion models are long to run, thus we provide a fast version of our
algorithm. Extensive experiments on blind image deblurring demonstrate the
effectiveness of our method when compared to other state-of-the-art approaches.",None,-1
f5b54165-2999-4015-892c-1720d5cf7df4,Localized Text-to-Image Generation for Free via Cross Attention Control,0.410139,"Despite the tremendous success in text-to-image generative models, localized
text-to-image generation (that is, generating objects or features at specific
locations in an image while maintaining a consistent overall generation) still
requires either explicit training or substantial additional inference time. In
this work, we show that localized generation can be achieved by simply
controlling cross attention maps during inference. With no additional training,
model architecture modification or inference time, our proposed cross attention
control (CAC) provides new open-vocabulary localization abilities to standard
text-to-image models. CAC also enhances models that are already trained for
localized generation when deployed at inference time. Furthermore, to assess
localized text-to-image generation performance automatically, we develop a
standardized suite of evaluations using large pretrained recognition models.
Our experiments show that CAC improves localized generation performance with
various types of location information ranging from bounding boxes to semantic
segmentation maps, and enhances the compositional capability of
state-of-the-art text-to-image generative models.",None,-1
04c6aacf-80ab-487d-92c5-4943a5302420,An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters,0.31722,"In this paper, we propose an algorithmic framework to automatically generate
efficient deep neural networks and optimize their associated hyperparameters.
The framework is based on evolving directed acyclic graphs (DAGs), defining a
more flexible search space than the existing ones in the literature. It allows
mixtures of different classical operations: convolutions, recurrences and dense
layers, but also more newfangled operations such as self-attention. Based on
this search space we propose neighbourhood and evolution search operators to
optimize both the architecture and hyper-parameters of our networks. These
search operators can be used with any metaheuristic capable of handling mixed
search spaces. We tested our algorithmic framework with an evolutionary
algorithm on a time series prediction benchmark. The results demonstrate that
our framework was able to find models outperforming the established baseline on
numerous datasets.",None,-1
ccb13440-515d-4eac-bcd3-e39c8b4d3667,DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting,0.523711,"Multi-class cell detection and counting is an essential task for many
pathological diagnoses. Manual counting is tedious and often leads to
inter-observer variations among pathologists. While there exist multiple,
general-purpose, deep learning-based object detection and counting methods,
they may not readily transfer to detecting and counting cells in medical
images, due to the limited data, presence of tiny overlapping objects, multiple
cell types, severe class-imbalance, minute differences in size/shape of cells,
etc. In response, we propose guided posterior regularization (DeGPR), which
assists an object detector by guiding it to exploit discriminative features
among cells. The features may be pathologist-provided or inferred directly from
visual data. We validate our model on two publicly available datasets (CoNSeP
and MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists
of 55 biopsy images of the human duodenum for predicting celiac disease. We
perform extensive experimentation with three object detection baselines on
three datasets to show that DeGPR is model-agnostic, and consistently improves
baselines obtaining up to 9% (absolute) mAP gains.",None,-1
2bc3376f-f5c6-4c79-8cb4-813b0cf02937,Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy,0.337667,"To mitigate potential risks associated with language models, recent AI
detection research proposes incorporating watermarks into machine-generated
text through random vocabulary restrictions and utilizing this information for
detection. While these watermarks only induce a slight deterioration in
perplexity, our empirical investigation reveals a significant detriment to the
performance of conditional text generation. To address this issue, we introduce
a simple yet effective semantic-aware watermarking algorithm that considers the
characteristics of conditional text generation and the input context.
Experimental results demonstrate that our proposed method yields substantial
improvements across various text generation models, including BART and Flan-T5,
in tasks such as summarization and data-to-text generation while maintaining
detection ability.",None,-1
3a099514-ca9e-4278-bf36-d24f281beaa2,Prompting Large Language Model for Machine Translation: A Case Study,0.999956,"Research on prompting has shown excellent performance with little or even no
supervised training across many tasks. However, prompting for machine
translation is still under-explored in the literature. We fill this gap by
offering a systematic study on prompting strategies for translation, examining
various factors for prompt template and demonstration example selection. We
further explore the use of monolingual data and the feasibility of
cross-lingual, cross-domain, and sentence-to-document transfer learning in
prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the
testbed show that 1) the number and the quality of prompt examples matter,
where using suboptimal examples degenerates translation; 2) several features of
prompt examples, such as semantic similarity, show significant Spearman
correlation with their prompting performance; yet, none of the correlations are
strong enough; 3) using pseudo parallel prompt examples constructed from
monolingual data via zero-shot prompting could improve translation; and 4)
improved performance is achievable by transferring knowledge from prompt
examples selected in other settings. We finally provide an analysis on the
model outputs and discuss several problems that prompting still suffers from.",None,-1
14825bf0-c13d-4622-8700-1bf7e6c27dbe,Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,0.794954,"Building agents with large language models (LLMs) for computer control is a
burgeoning research area, where the agent receives computer states and performs
actions to complete complex tasks. Previous computer agents have demonstrated
the benefits of in-context learning (ICL); however, their performance is
hindered by several issues. First, the limited context length of LLMs and
complex computer states restrict the number of exemplars, as a single webpage
can consume the entire context. Second, the exemplars in current methods, such
as high-level plans and multi-choice questions, cannot represent complete
trajectories, leading to suboptimal performance in long-horizon tasks. Third,
existing computer agents rely on task-specific exemplars and overlook the
similarity among tasks, resulting in poor generalization to novel tasks. To
address these challenges, we introduce Synapse, a computer agent featuring
three key components: i) state abstraction, which filters out task-irrelevant
information from raw states, allowing more exemplars within the limited
context, ii) trajectory-as-exemplar prompting, which prompts the LLM with
complete trajectories of the abstracted states and actions to improve
multi-step decision-making, and iii) exemplar memory, which stores the
embeddings of exemplars and retrieves them via similarity search for
generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard
task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse
achieves a 99.2% average success rate (a 10% relative improvement) across 64
tasks using demonstrations from only 48 tasks. Notably, Synapse is the first
ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a
56% relative improvement in average step success rate over the previous
state-of-the-art prompting scheme in Mind2Web.",None,-1
2656fff6-f008-40e3-b0b5-4468938f5de2,Comparing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication,0.687926,"Traditionally, writing assistance systems have focused on short or even
single-word suggestions. Recently, large language models like GPT-3 have made
it possible to generate significantly longer natural-sounding suggestions,
offering more advanced assistance opportunities. This study explores the
trade-offs between sentence- vs. message-level suggestions for AI-mediated
communication. We recruited 120 participants to act as staffers from
legislators' offices who often need to respond to large volumes of constituent
concerns. Participants were asked to reply to emails with different types of
assistance. The results show that participants receiving message-level
suggestions responded faster and were more satisfied with the experience, as
they mainly edited the suggested drafts. In addition, the texts they wrote were
evaluated as more helpful by others. In comparison, participants receiving
sentence-level assistance retained a higher sense of agency, but took longer
for the task as they needed to plan the flow of their responses and decide when
to use suggestions. Our findings have implications for designing
task-appropriate communication assistance systems.",None,-1
9022a067-39a1-45e7-a47c-5468d77ef534,Retrieval-augmented Multi-label Text Classification,0.182009,"Multi-label text classification (MLC) is a challenging task in settings of
large label sets, where label support follows a Zipfian distribution. In this
paper, we address this problem through retrieval augmentation, aiming to
improve the sample efficiency of classification models. Our approach closely
follows the standard MLC architecture of a Transformer-based encoder paired
with a set of classification heads. In our case, however, the input document
representation is augmented through cross-attention to similar documents
retrieved from the training set and represented in a task-specific manner. We
evaluate this approach on four datasets from the legal and biomedical domains,
all of which feature highly skewed label distributions. Our experiments show
that retrieval augmentation substantially improves model performance on the
long tail of infrequent labels especially so for lower-resource training
scenarios and more challenging long-document data scenarios.",None,-1
9ff1cde0-72f8-4d53-8180-53ab6a5b480f,A Game-Theoretic Framework for Joint Forecasting and Planning,0.874188,"Planning safe robot motions in the presence of humans requires reliable
forecasts of future human motion. However, simply predicting the most likely
motion from prior interactions does not guarantee safety. Such forecasts fail
to model the long tail of possible events, which are rarely observed in limited
datasets. On the other hand, planning for worst-case motions leads to overtly
conservative behavior and a ""frozen robot"". Instead, we aim to learn forecasts
that predict counterfactuals that humans guard against. We propose a novel
game-theoretic framework for joint planning and forecasting with the payoff
being the performance of the planner against the demonstrator, and present
practical algorithms to train models in an end-to-end fashion. We demonstrate
that our proposed algorithm results in safer plans in a crowd navigation
simulator and real-world datasets of pedestrian motion. We release our code at
https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.",None,-1
bd3d74d6-532c-4fc0-83ef-3af04924c141,Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI,0.867297,"Explainable AI (XAI) systems are sociotechnical in nature; thus, they are
subject to the sociotechnical gap--divide between the technical affordances and
the social needs. However, charting this gap is challenging. In the context of
XAI, we argue that charting the gap improves our problem understanding, which
can reflexively provide actionable insights to improve explainability.
Utilizing two case studies in distinct domains, we empirically derive a
framework that facilitates systematic charting of the sociotechnical gap by
connecting AI guidelines in the context of XAI and elucidating how to use them
to address the gap. We apply the framework to a third case in a new domain,
showcasing its affordances. Finally, we discuss conceptual implications of the
framework, share practical considerations in its operationalization, and offer
guidance on transferring it to new contexts. By making conceptual and practical
contributions to understanding the sociotechnical gap in XAI, the framework
expands the XAI design space.",None,-1
ed7e9af1-5cad-4ddf-b6e9-60d653491bb7,Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models,0.108623,"There have been wide spread claims in the literature about the emergent
reasoning capabilities of Pretrained Large Language Models. However, recent
studies, have found that their ability to plan remains questionable. Through
our experiments using GPT-2, we empirically demonstrate that the performance of
a finetuned baseline remains poor because it violates pre-conditions of actions
in the plans that it generates. To improve the planning capabilities of a
finetuned LLM, we train a verifier, which can classify actions as being valid
or invalid in a particular state. By randomly sampling actions from the same
dataset, we generate examples of invalid actions which are then used to train a
verifier which can check for action applicability. In the presence of diverse
sampling from a generator and a verifier which can prune invalid trajectories,
we show significant gains in the success rate on the Blocksworld domain.
Additionally, we show that finetuning the GPT-2 generator itself to create the
verifier generalizes better than finetuning the base GPT-2. Lastly, we
investigate the role of the sampling temperature which can be used to control
the exploration-exploitation tradeoff.",None,-1
4d731077-5035-444b-b02c-aac7432e5ac6,Enhancing ResNet Image Classification Performance by using Parameterized Hypercomplex Multiplication,0.142535,"Recently, many deep networks have introduced hypercomplex and related
calculations into their architectures. In regard to convolutional networks for
classification, these enhancements have been applied to the convolution
operations in the frontend to enhance accuracy and/or reduce the parameter
requirements while maintaining accuracy. Although these enhancements have been
applied to the convolutional frontend, it has not been studied whether adding
hypercomplex calculations improves performance when applied to the densely
connected backend. This paper studies ResNet architectures and incorporates
parameterized hypercomplex multiplication (PHM) into the backend of residual,
quaternion, and vectormap convolutional neural networks to assess the effect.
We show that PHM does improve classification accuracy performance on several
image datasets, including small, low-resolution CIFAR 10/100 and large
high-resolution ImageNet and ASL, and can achieve state-of-the-art accuracy for
hypercomplex networks.",None,-1
32ab892c-96c4-4be2-b69a-5c755e7e5ca4,Incremental 3D Semantic Scene Graph Prediction from RGB Sequences,0.736691,"3D semantic scene graphs are a powerful holistic representation as they
describe the individual objects and depict the relation between them. They are
compact high-level graphs that enable many tasks requiring scene reasoning. In
real-world settings, existing 3D estimation methods produce robust predictions
that mostly rely on dense inputs. In this work, we propose a real-time
framework that incrementally builds a consistent 3D semantic scene graph of a
scene given an RGB image sequence. Our method consists of a novel incremental
entity estimation pipeline and a scene graph prediction network. The proposed
pipeline simultaneously reconstructs a sparse point map and fuses entity
estimation from the input images. The proposed network estimates 3D semantic
scene graphs with iterative message passing using multi-view and geometric
features extracted from the scene entities. Extensive experiments on the 3RScan
dataset show the effectiveness of the proposed method in this challenging task,
outperforming state-of-the-art approaches.",None,-1
0a6ea6b3-6147-46a8-a1d0-bbcfcf737131,Knowledge Acquisition and Completion for Long-Term Human-Robot Interactions using Knowledge Graph Embedding,0.25404,"In Human-Robot Interaction (HRI) systems, a challenging task is sharing the
representation of the operational environment, fusing symbolic knowledge and
perceptions, between users and robots. With the existing HRI pipelines, users
can teach the robots some concepts to increase their knowledge base.
Unfortunately, the data coming from the users are usually not enough dense for
building a consistent representation. Furthermore, the existing approaches are
not able to incrementally build up their knowledge base, which is very
important when robots have to deal with dynamic contexts. To this end, we
propose an architecture to gather data from users and environments in long-runs
of continual learning. We adopt Knowledge Graph Embedding techniques to
generalize the acquired information with the goal of incrementally extending
the robot's inner representation of the environment. We evaluate the
performance of the overall continual learning architecture by measuring the
capabilities of the robot of learning entities and relations coming from
unknown contexts through a series of incremental learning sessions.",None,-1
34732c58-4a07-4322-9965-97578f8e0793,Backdoor Defense via Deconfounded Representation Learning,0.763817,"Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor
attacks, where attackers embed hidden backdoors in the DNN model by injecting a
few poisoned examples into the training dataset. While extensive efforts have
been made to detect and remove backdoors from backdoored DNNs, it is still not
clear whether a backdoor-free clean model can be directly obtained from
poisoned datasets. In this paper, we first construct a causal graph to model
the generation process of poisoned data and find that the backdoor attack acts
as the confounder, which brings spurious associations between the input images
and target labels, making the model predictions less reliable. Inspired by the
causal understanding, we propose the Causality-inspired Backdoor Defense (CBD),
to learn deconfounded representations for reliable classification.
Specifically, a backdoored model is intentionally trained to capture the
confounding effects. The other clean model dedicates to capturing the desired
causal effects by minimizing the mutual information with the confounding
representations from the backdoored model and employing a sample-wise
re-weighting scheme. Extensive experiments on multiple benchmark datasets
against 6 state-of-the-art attacks verify that our proposed defense method is
effective in reducing backdoor threats while maintaining high accuracy in
predicting benign samples. Further analysis shows that CBD can also resist
potential adaptive attacks. The code is available at
\url{https://github.com/zaixizhang/CBD}.",None,-1
e9f067d5-837c-4587-9d66-1a2508d63d7d,Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization,0.0302576,"Large Language Models (LLMs) have reshaped natural language processing with
their impressive capabilities. However, their ever-increasing size has raised
concerns about their effective deployment and the need for LLM compression.
This study introduces the Divergent Token Metrics (DTMs), a novel approach to
assessing compressed LLMs, addressing the limitations of traditional perplexity
or accuracy measures that fail to accurately reflect text generation quality.
DTMs measure token divergences that allow deeper insights into the subtleties
of model compression, in particular, when evaluating components' impacts
individually. Utilizing the First Divergent Token Metric (FDTM) in model
sparsification reveals that 25% of all attention components can be pruned
beyond 90% on the Llama-2 model family, still keeping SOTA performance. For
quantization, FDTM suggests that more than 80% of parameters can be naively
transformed to int8 without special outlier management. These evaluations
indicate the necessity of choosing appropriate compressions for parameters
individually -- and that FDTM can identify those -- while standard metrics
result in deteriorated outcomes.",None,-1
1a35fc2a-c2d3-4670-b39a-97e81a707c78,WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models,0.853202,"The open road poses many challenges to autonomous perception, including poor
visibility from extreme weather conditions. Models trained on good-weather
datasets frequently fail at detection in these out-of-distribution settings. To
aid adversarial robustness in perception, we introduce WEDGE (WEather images by
DALL-E GEneration): a synthetic dataset generated with a vision-language
generative model via prompting. WEDGE consists of 3360 images in 16 extreme
weather conditions manually annotated with 16513 bounding boxes, supporting
research in the tasks of weather classification and 2D object detection. We
have analyzed WEDGE from research standpoints, verifying its effectiveness for
extreme-weather autonomous perception. We establish baseline performance for
classification and detection with 53.87% test accuracy and 45.41 mAP. Most
importantly, WEDGE can be used to fine-tune state-of-the-art detectors,
improving SOTA performance on real-world weather benchmarks (such as DAWN) by
4.48 AP for well-generated classes like trucks. WEDGE has been collected under
OpenAI's terms of use and is released for public use under the CC BY-NC-SA 4.0
license. The repository for this work and dataset is available at
https://infernolia.github.io/WEDGE.",None,-1
8b22e3c0-3084-48ca-91e9-59fcb015682a,Asking More Informative Questions for Grounded Retrieval,0.210012,"When a model is trying to gather information in an interactive setting, it
benefits from asking informative questions. However, in the case of a grounded
multi-turn image identification task, previous studies have been constrained to
polar yes/no questions, limiting how much information the model can gain in a
single turn. We present an approach that formulates more informative,
open-ended questions. In doing so, we discover that off-the-shelf visual
question answering (VQA) models often make presupposition errors, which
standard information gain question selection methods fail to account for. To
address this issue, we propose a method that can incorporate presupposition
handling into both question selection and belief updates. Specifically, we use
a two-stage process, where the model first filters out images which are
irrelevant to a given question, then updates its beliefs about which image the
user intends. Through self-play and human evaluations, we show that our method
is successful in asking informative open-ended questions, increasing accuracy
over the past state-of-the-art by 14%, while resulting in 48% more efficient
games in human evaluations.",None,-1
15eafe64-a7ef-4435-aa1e-6efb6788a5a9,"Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection",0.748567,"This paper aims for high-performance offline LiDAR-based 3D object detection.
We first observe that experienced human annotators annotate objects from a
track-centric perspective. They first label the objects with clear shapes in a
track, and then leverage the temporal coherence to infer the annotations of
obscure objects. Drawing inspiration from this, we propose a high-performance
offline detector in a track-centric perspective instead of the conventional
object-centric perspective. Our method features a bidirectional tracking module
and a track-centric learning module. Such a design allows our detector to infer
and refine a complete track once the object is detected at a certain moment. We
refer to this characteristic as ""onCe detecTed, neveR Lost"" and name the
proposed system CTRL. Extensive experiments demonstrate the remarkable
performance of our method, surpassing the human-level annotating accuracy and
the previous state-of-the-art methods in the highly competitive Waymo Open
Dataset without model ensemble. The code will be made publicly available at
https://github.com/tusen-ai/SST.",None,-1
6a622433-a047-428e-8ddd-b32dbb5755dd,CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis,0.947794,"In this work, we focus on a novel task of category-level functional
hand-object manipulation synthesis covering both rigid and articulated object
categories. Given an object geometry, an initial human hand pose as well as a
sparse control sequence of object poses, our goal is to generate a physically
reasonable hand-object manipulation sequence that performs like human beings.
To address such a challenge, we first design CAnonicalized Manipulation Spaces
(CAMS), a two-level space hierarchy that canonicalizes the hand poses in an
object-centric and contact-centric view. Benefiting from the representation
capability of CAMS, we then present a two-stage framework for synthesizing
human-like manipulation animations. Our framework achieves state-of-the-art
performance for both rigid and articulated categories with impressive visual
effects. Codes and video results can be found at our project homepage:
https://cams-hoi.github.io/",None,-1
42fc565f-e7ee-4c3d-bbce-a80d100ea72b,Adaptive questionnaires for facilitating patient data entry in clinical decision support systems: Methods and application to STOPP/START v2,0.258075,"Clinical decision support systems are software tools that help clinicians to
make medical decisions. However, their acceptance by clinicians is usually
rather low. A known problem is that they often require clinicians to manually
enter lots of patient data, which is long and tedious. Existing solutions, such
as the automatic data extraction from electronic health record, are not fully
satisfying, because of low data quality and availability. In practice, many
systems still include long questionnaire for data entry.
  In this paper, we propose an original solution to simplify patient data
entry, using an adaptive questionnaire, i.e. a questionnaire that evolves
during user interaction, showing or hiding questions dynamically. Considering a
rule-based decision support systems, we designed methods for translating the
system's clinical rules into display rules that determine the items to show in
the questionnaire, and methods for determining the optimal order of priority
among the items in the questionnaire. We applied this approach to a decision
support system implementing STOPP/START v2, a guideline for managing
polypharmacy. We show that it permits reducing by about two thirds the number
of clinical conditions displayed in the questionnaire. Presented to clinicians
during focus group sessions, the adaptive questionnaire was found ""pretty easy
to use"". In the future, this approach could be applied to other guidelines, and
adapted for data entry by patients.",None,-1
484b4a10-e957-4a48-bff1-17e0e182120f,Analysis over vision-based models for pedestrian action anticipation,0.527633,"Anticipating human actions in front of autonomous vehicles is a challenging
task. Several papers have recently proposed model architectures to address this
problem by combining multiple input features to predict pedestrian crossing
actions. This paper focuses specifically on using images of the pedestrian's
context as an input feature. We present several spatio-temporal model
architectures that utilize standard CNN and Transformer modules to serve as a
backbone for pedestrian anticipation. However, the objective of this paper is
not to surpass state-of-the-art benchmarks but rather to analyze the positive
and negative predictions of these models. Therefore, we provide insights on the
explainability of vision-based Transformer models in the context of pedestrian
action prediction. We will highlight cases where the model can achieve correct
quantitative results but falls short in providing human-like explanations
qualitatively, emphasizing the importance of investing in explainability for
pedestrian action anticipation problems.",None,-1
0793908c-fcb9-4d46-8919-913a094f285c,EdgeFace: Efficient Face Recognition Model for Edge Devices,0.722207,"In this paper, we present EdgeFace, a lightweight and efficient face
recognition network inspired by the hybrid architecture of EdgeNeXt. By
effectively combining the strengths of both CNN and Transformer models, and a
low rank linear layer, EdgeFace achieves excellent face recognition performance
optimized for edge devices. The proposed EdgeFace network not only maintains
low computational costs and compact storage, but also achieves high face
recognition accuracy, making it suitable for deployment on edge devices.
Extensive experiments on challenging benchmark face datasets demonstrate the
effectiveness and efficiency of EdgeFace in comparison to state-of-the-art
lightweight models and deep face recognition models. Our EdgeFace model with
1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B
(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger
computational complexities. The code to replicate the experiments will be made
available publicly.",None,-1
37c65e4c-5997-4670-ac6b-88e7ea32470f,Empowering Networks With Scale and Rotation Equivariance Using A Similarity Convolution,0.566248,"The translational equivariant nature of Convolutional Neural Networks (CNNs)
is a reason for its great success in computer vision. However, networks do not
enjoy more general equivariance properties such as rotation or scaling,
ultimately limiting their generalization performance. To address this
limitation, we devise a method that endows CNNs with simultaneous equivariance
with respect to translation, rotation, and scaling. Our approach defines a
convolution-like operation and ensures equivariance based on our proposed
scalable Fourier-Argand representation. The method maintains similar efficiency
as a traditional network and hardly introduces any additional learnable
parameters, since it does not face the computational issue that often occurs in
group-convolution operators. We validate the efficacy of our approach in the
image classification task, demonstrating its robustness and the generalization
ability to both scaled and rotated inputs.",None,-1
327ad7ac-0b10-4062-a434-572149ca2d1c,Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems,0.37019,"Understanding causal relationships within Gene Regulatory Networks (GRNs) is
essential for unraveling the gene interactions in cellular processes. However,
causal discovery in GRNs is a challenging problem for multiple reasons
including the existence of cyclic feedback loops and uncertainty that yields
diverse possible causal structures. Previous works in this area either ignore
cyclic dynamics (assume acyclic structure) or struggle with scalability. We
introduce Swift-DynGFN as a novel framework that enhances causal structure
learning in GRNs while addressing scalability concerns. Specifically,
Swift-DynGFN exploits gene-wise independence to boost parallelization and to
lower computational cost. Experiments on real single-cell RNA velocity and
synthetic GRN datasets showcase the advancement in learning causal structure in
GRNs and scalability in larger systems.",None,-1
1cacd27e-1f34-4029-9d10-1333527bf39b,NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery,0.961583,"Norms, which are culturally accepted guidelines for behaviours, can be
integrated into conversational models to generate utterances that are
appropriate for the socio-cultural context. Existing methods for norm
recognition tend to focus only on surface-level features of dialogues and do
not take into account the interactions within a conversation. To address this
issue, we propose NormMark, a probabilistic generative Markov model to carry
the latent features throughout a dialogue. These features are captured by
discrete and continuous latent variables conditioned on the conversation
history, and improve the model's ability in norm recognition. The model is
trainable on weakly annotated data using the variational technique. On a
dataset with limited norm annotations, we show that our approach achieves
higher F1 score, outperforming current state-of-the-art methods, including
GPT3.",None,-1
5b5f1cf8-8645-4a0c-9873-248e783e7056,Deep Learning-Enabled Sleep Staging From Vital Signs and Activity Measured Using a Near-Infrared Video Camera,0.713048,"Conventional sleep monitoring is time-consuming, expensive and uncomfortable,
requiring a large number of contact sensors to be attached to the patient.
Video data is commonly recorded as part of a sleep laboratory assessment. If
accurate sleep staging could be achieved solely from video, this would overcome
many of the problems of traditional methods. In this work we use heart rate,
breathing rate and activity measures, all derived from a near-infrared video
camera, to perform sleep stage classification. We use a deep transfer learning
approach to overcome data scarcity, by using an existing contact-sensor dataset
to learn effective representations from the heart and breathing rate time
series. Using a dataset of 50 healthy volunteers, we achieve an accuracy of
73.4\% and a Cohen's kappa of 0.61 in four-class sleep stage classification,
establishing a new state-of-the-art for video-based sleep staging.",None,-1
67b78059-a2a5-4d03-b47d-99448d83ce1f,Toward Defining a Domain Complexity Measure Across Domains,0.513514,"Artificial Intelligence (AI) systems planned for deployment in real-world
applications frequently are researched and developed in closed simulation
environments where all variables are controlled and known to the simulator or
labeled benchmark datasets are used. Transition from these simulators,
testbeds, and benchmark datasets to more open-world domains poses significant
challenges to AI systems, including significant increases in the complexity of
the domain and the inclusion of real-world novelties; the open-world
environment contains numerous out-of-distribution elements that are not part in
the AI systems' training set. Here, we propose a path to a general,
domain-independent measure of domain complexity level. We distinguish two
aspects of domain complexity: intrinsic and extrinsic. The intrinsic domain
complexity is the complexity that exists by itself without any action or
interaction from an AI agent performing a task on that domain. This is an
agent-independent aspect of the domain complexity. The extrinsic domain
complexity is agent- and task-dependent. Intrinsic and extrinsic elements
combined capture the overall complexity of the domain. We frame the components
that define and impact domain complexity levels in a domain-independent light.
Domain-independent measures of complexity could enable quantitative predictions
of the difficulty posed to AI systems when transitioning from one testbed or
environment to another, when facing out-of-distribution data in open-world
tasks, and when navigating the rapidly expanding solution and search spaces
encountered in open-world domains.",None,-1
0b801ccb-f886-4c0d-a98f-fe532926a573,Disease X vaccine production and supply chains: risk assessing healthcare systems operating with artificial intelligence and industry 4.0,0.943527,"A set of six algorithmic solutions is presented for resolving vaccine
production and supply chain bottlenecks. A different set of algorithmic
solutions is presented for forecasting risks during a Disease X event.",None,-1
25286b7e-f19d-4bda-89fe-f2617dc4d657,KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation,0.571333,"In text classification tasks, fine tuning pretrained language models like
BERT and GPT-3 yields competitive accuracy; however, both methods require
pretraining on large text datasets. In contrast, general topic modeling methods
possess the advantage of analyzing documents to extract meaningful patterns of
words without the need of pretraining. To leverage topic modeling's
unsupervised insights extraction on text classification tasks, we develop the
Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires
no pretrained embeddings, few labeled documents and is efficient to train,
making it ideal under resource constrained settings. Across a variety of
datasets, our method outperforms existing supervised topic modeling methods in
classification accuracy, robustness and efficiency and achieves similar
performance compare to state of the art weakly supervised text classification
methods.",None,-1
0e36df8a-bf6e-43fe-b44e-008c8af9052c,StableVideo: Text-driven Consistency-aware Diffusion Video Editing,0.982773,"Diffusion-based methods can generate realistic images and videos, but they
struggle to edit existing objects in a video while preserving their appearance
over time. This prevents diffusion models from being applied to natural video
editing in practical scenarios. In this paper, we tackle this problem by
introducing temporal dependency to existing text-driven diffusion models, which
allows them to generate consistent appearance for the edited objects.
Specifically, we develop a novel inter-frame propagation mechanism for
diffusion video editing, which leverages the concept of layered representations
to propagate the appearance information from one frame to the next. We then
build up a text-driven video editing framework based on this mechanism, namely
StableVideo, which can achieve consistency-aware video editing. Extensive
experiments demonstrate the strong editing capability of our approach. Compared
with state-of-the-art video editing methods, our approach shows superior
qualitative and quantitative results. Our code is available at
\href{https://github.com/rese1f/StableVideo}{this https URL}.",None,-1
6d6ca738-1fcc-475f-8bec-34ac90164d9d,Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA,0.292794,"Large Language Models (LLMs) have shown outstanding performance across wide
range of downstream tasks. This competency is attributed to their substantial
parameter size and pre-training on extensive corpus. Moreover, LLMs have
exhibited enhanced reasoning capabilities in tackling complex reasoning tasks,
owing to the utilization of a method named ``Chain-of-Thought (CoT)
prompting''. This method is designed to generate intermediate reasoning steps
that guide the inference of the final answer. However, it is essential to
highlight that these advanced reasoning abilities appear to emerge in models
with a minimum of 10 billion parameters, thereby limiting its efficacy in
situations where computational resources are constrained. In this paper, we
investigate the possibility of transferring the reasoning capabilities of LLMs
to smaller models via knowledge distillation. Specifically, we propose Sci-CoT,
a two-stage framework that separates the processes of generating rationales and
inferring answers. This method enables a more efficient use of rationales
during the answer inference stage, leading to improved performance on
scientific question-answering tasks. Utilizing Sci-CoT, our 80-million
parameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy
dataset under the few shot setting.",None,-1
9f609f87-b5e1-4038-ad2b-96f1b52606a1,Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting,0.175744,"Images contain rich relational knowledge that can help machines understand
the world. Existing methods on visual knowledge extraction often rely on the
pre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation
types), restricting the expressiveness of the extracted knowledge. In this
work, we take a first exploration to a new paradigm of open visual knowledge
extraction. To achieve this, we present OpenVik which consists of an open
relational region detector to detect regions potentially containing relational
knowledge and a visual knowledge generator that generates format-free knowledge
by prompting the large multimodality model with the detected region of
interest. We also explore two data enhancement techniques for diversifying the
generated format-free visual knowledge. Extensive knowledge quality evaluations
highlight the correctness and uniqueness of the extracted open visual knowledge
by OpenVik. Moreover, integrating our extracted knowledge across various visual
reasoning applications shows consistent improvements, indicating the real-world
applicability of OpenVik.",None,-1
def0f636-3760-4f7c-8f06-519909cb49c8,A Novel Convolutional Neural Network Architecture with a Continuous Symmetry,0.0846499,"This paper introduces a new Convolutional Neural Network (ConvNet)
architecture inspired by a class of partial differential equations (PDEs)
called quasi-linear hyperbolic systems. With comparable performance on the
image classification task, it allows for the modification of the weights via a
continuous group of symmetry. This is a significant shift from traditional
models where the architecture and weights are essentially fixed. We wish to
promote the (internal) symmetry as a new desirable property for a neural
network, and to draw attention to the PDE perspective in analyzing and
interpreting ConvNets in the broader Deep Learning community.",None,-1
c4e520c3-4100-42c2-8d66-85d753eeaed7,Robust Natural Language Understanding with Residual Attention Debiasing,0.216436,"Natural language understanding (NLU) models often suffer from unintended
dataset biases. Among bias mitigation methods, ensemble-based debiasing
methods, especially product-of-experts (PoE), have stood out for their
impressive empirical success. However, previous ensemble-based debiasing
methods typically apply debiasing on top-level logits without directly
addressing biased attention patterns. Attention serves as the main media of
feature interaction and aggregation in PLMs and plays a crucial role in
providing robust prediction. In this paper, we propose REsidual Attention
Debiasing (READ), an end-to-end debiasing method that mitigates unintended
biases from attention. Experiments on three NLU tasks show that READ
significantly improves the performance of BERT-based models on OOD data with
shortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on
FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the
crucial role of unbiased attention in robust NLU models and that READ
effectively mitigates biases in attention. Code is available at
https://github.com/luka-group/READ.",None,-1
5a6f3413-f09f-4f95-a91e-2bb13f492830,Towards Invertible Semantic-Preserving Embeddings of Logical Formulae,0.0701431,"Logic is the main formal language to perform automated reasoning, and it is
further a human-interpretable language, at least for small formulae. Learning
and optimising logic requirements and rules has always been an important
problem in Artificial Intelligence. State of the art Machine Learning (ML)
approaches are mostly based on gradient descent optimisation in continuous
spaces, while learning logic is framed in the discrete syntactic space of
formulae. Using continuous optimisation to learn logic properties is a
challenging problem, requiring to embed formulae in a continuous space in a
meaningful way, i.e. preserving the semantics. Current methods are able to
construct effective semantic-preserving embeddings via kernel methods (for
linear temporal logic), but the map they define is not invertible. In this work
we address this problem, learning how to invert such an embedding leveraging
deep architectures based on the Graph Variational Autoencoder framework. We
propose a novel model specifically designed for this setting, justifying our
design choices through an extensive experimental evaluation. Reported results
in the context of propositional logic are promising, and several challenges
regarding learning invertible embeddings of formulae are highlighted and
addressed.",None,-1
90669141-87cc-4256-8d7d-2d484fafa256,A Multi-Level Framework for the AI Alignment Problem,0.0591154,"AI alignment considers how we can encode AI systems in a way that is
compatible with human values. The normative side of this problem asks what
moral values or principles, if any, we should encode in AI. To this end, we
present a framework to consider the question at four levels: Individual,
Organizational, National, and Global. We aim to illustrate how AI alignment is
made up of value alignment problems at each of these levels, where values at
each level affect the others and effects can flow in either direction. We
outline key questions and considerations of each level and demonstrate an
application of this framework to the topic of AI content moderation.",None,-1
383d1029-29b2-4442-b662-12b79aa65192,Robust Camera Pose Refinement for Multi-Resolution Hash Encoding,0.324621,"Multi-resolution hash encoding has recently been proposed to reduce the
computational cost of neural renderings, such as NeRF. This method requires
accurate camera poses for the neural renderings of given scenes. However,
contrary to previous methods jointly optimizing camera poses and 3D scenes, the
naive gradient-based camera pose refinement method using multi-resolution hash
encoding severely deteriorates performance. We propose a joint optimization
algorithm to calibrate the camera pose and learn a geometric representation
using efficient multi-resolution hash encoding. Showing that the oscillating
gradient flows of hash encoding interfere with the registration of camera
poses, our method addresses the issue by utilizing smooth interpolation
weighting to stabilize the gradient oscillation for the ray samplings across
hash grids. Moreover, the curriculum training procedure helps to learn the
level-wise hash encoding, further increasing the pose refinement. Experiments
on the novel-view synthesis datasets validate that our learning frameworks
achieve state-of-the-art performance and rapid convergence of neural rendering,
even when initial camera poses are unknown.",None,-1
5c7f4a01-d861-4102-8f11-022f5d05d103,Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation,0.235767,"Pre-training and fine-tuning is a paradigm for alleviating the data scarcity
problem in end-to-end speech translation (E2E ST). The commonplace ""modality
gap"" between speech and text data often leads to inconsistent inputs between
pre-training and fine-tuning. However, we observe that this gap occurs in the
early stages of fine-tuning, but does not have a major impact on the final
performance. On the other hand, we find that there has another gap, which we
call the ""capacity gap"": high resource tasks (such as ASR and MT) always
require a large model to fit, when the model is reused for a low resource task
(E2E ST), it will get a sub-optimal performance due to the over-fitting. In a
case study, we find that the regularization plays a more important role than
the well-designed modality adaption method, which achieves 29.0 for en-de and
40.3 for en-fr on the MuST-C dataset. Code and models are available at
https://github.com/hannlp/TAB.",None,-1
143b519a-f32d-455f-985a-ef34d8e44479,Diverse Inpainting and Editing with GAN Inversion,0.793123,"Recent inversion methods have shown that real images can be inverted into
StyleGAN's latent space and numerous edits can be achieved on those images
thanks to the semantically rich feature representations of well-trained GAN
models. However, extensive research has also shown that image inversion is
challenging due to the trade-off between high-fidelity reconstruction and
editability. In this paper, we tackle an even more difficult task, inverting
erased images into GAN's latent space for realistic inpaintings and editings.
Furthermore, by augmenting inverted latent codes with different latent samples,
we achieve diverse inpaintings. Specifically, we propose to learn an encoder
and mixing network to combine encoded features from erased images with
StyleGAN's mapped features from random samples. To encourage the mixing network
to utilize both inputs, we train the networks with generated data via a novel
set-up. We also utilize higher-rate features to prevent color inconsistencies
between the inpainted and unerased parts. We run extensive experiments and
compare our method with state-of-the-art inversion and inpainting methods.
Qualitative metrics and visual comparisons show significant improvements.",None,-1
ce8d2562-5ee5-4c3c-b913-551ce189fb35,General Method for Solving Four Types of SAT Problems,0.641623,"Existing methods provide varying algorithms for different types of Boolean
satisfiability problems (SAT), lacking a general solution framework.
Accordingly, this study proposes a unified framework DCSAT based on integer
programming and reinforcement learning (RL) algorithm to solve different types
of SAT problems such as MaxSAT, Weighted MaxSAT, PMS, WPMS. Specifically, we
first construct a consolidated integer programming representation for four
types of SAT problems by adjusting objective function coefficients. Secondly,
we construct an appropriate reinforcement learning models based on the 0-1
integer programming for SAT problems. Based on the binary tree search
structure, we apply the Monte Carlo tree search (MCTS) method on SAT problems.
Finally, we prove that this method can find all optimal Boolean assignments
based on Wiener-khinchin law of large Numbers. We experimentally verify that
this paradigm can prune the unnecessary search space to find the optimal
Boolean assignments for the problem. Furthermore, the proposed method can
provide diverse labels for supervised learning methods for SAT problems.",None,-1
8fc6a82a-082d-4876-92bd-6f486b6ddeac,InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models,0.227927,"Large language models~(LLMs) are instruction followers, but it can be
challenging to find the best instruction for different situations, especially
for black-box LLMs on which backpropagation is forbidden. Instead of directly
optimizing the discrete instruction, we optimize a low-dimensional soft prompt
applied to an open-source LLM to generate the instruction for the black-box
LLM. On each iteration of the proposed method, which we call InstructZero, a
soft prompt is converted into an instruction using the open-source LLM, which
is then submitted to the black-box LLM for zero-shot evaluation, and the
performance is sent to Bayesian optimization to produce new soft prompts
improving the zero-shot performance. We evaluate InstructZero on different
combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our
results show that InstructZero outperforms SOTA auto-instruction methods across
a variety of downstream tasks. Our code and data are publicly available at
https://github.com/Lichang-Chen/InstructZero.",None,-1
7c50c7c0-a948-4bc4-84dd-3776c6836254,ControlVideo: Training-free Controllable Text-to-Video Generation,0.991939,"Text-driven diffusion models have unlocked unprecedented abilities in image
generation, whereas their video counterpart still lags behind due to the
excessive training cost of temporal modeling. Besides the training burden, the
generated videos also suffer from appearance inconsistency and structural
flickers, especially in long video synthesis. To address these challenges, we
design a \emph{training-free} framework called \textbf{ControlVideo} to enable
natural and efficient text-to-video generation. ControlVideo, adapted from
ControlNet, leverages coarsely structural consistency from input motion
sequences, and introduces three modules to improve video generation. Firstly,
to ensure appearance coherence between frames, ControlVideo adds fully
cross-frame interaction in self-attention modules. Secondly, to mitigate the
flicker effect, it introduces an interleaved-frame smoother that employs frame
interpolation on alternated frames. Finally, to produce long videos
efficiently, it utilizes a hierarchical sampler that separately synthesizes
each short clip with holistic coherency. Empowered with these modules,
ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs
quantitatively and qualitatively. Notably, thanks to the efficient designs, it
generates both short and long videos within several minutes using one NVIDIA
2080Ti. Code is available at https://github.com/YBYBZhang/ControlVideo.",None,-1
f9dbd5e3-3634-4144-a9c2-61348839e5c2,Meta Generative Attack on Person Reidentification,0.188594,"Adversarial attacks have been recently investigated in person
re-identification. These attacks perform well under cross dataset or cross
model setting. However, the challenges present in cross-dataset cross-model
scenario does not allow these models to achieve similar accuracy. To this end,
we propose our method with the goal of achieving better transferability against
different models and across datasets. We generate a mask to obtain better
performance across models and use meta learning to boost the generalizability
in the challenging cross-dataset cross-model setting. Experiments on
Market-1501, DukeMTMC-reID and MSMT-17 demonstrate favorable results compared
to other attacks.",None,-1
60ea49d6-028c-4aa2-8b6f-24443001cb59,Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy,0.131613,"This paper introduces a multifaceted methodology for fine-tuning and
evaluating large language models (LLMs) for specialized monetization tasks. The
goal is to balance general language proficiency with domain-specific skills.
The methodology has three main components: 1) Carefully blending in-domain and
general-purpose data during fine-tuning to achieve an optimal balance between
general and specialized capabilities; 2) Designing a comprehensive evaluation
framework with 45 questions tailored to assess performance on functionally
relevant dimensions like reliability, consistency, and business impact; 3)
Analyzing how model size and continual training influence metrics to guide
efficient resource allocation during fine-tuning. The paper details the design,
data collection, analytical techniques, and results validating the proposed
frameworks. It aims to provide businesses and researchers with actionable
insights on effectively adapting LLMs for specialized contexts. We also intend
to make public the comprehensive evaluation framework, which includes the 45
tailored questions and their respective scoring guidelines, to foster
transparency and collaboration in adapting LLMs for specialized tasks.",None,-1
c308d234-b96e-465a-9cdb-60e78191350f,Large AI Model-Based Semantic Communications,0.962695,"Semantic communication (SC) is an emerging intelligent paradigm, offering
solutions for various future applications like metaverse, mixed-reality, and
the Internet of everything. However, in current SC systems, the construction of
the knowledge base (KB) faces several issues, including limited knowledge
representation, frequent knowledge updates, and insecure knowledge sharing.
Fortunately, the development of the large AI model provides new solutions to
overcome above issues. Here, we propose a large AI model-based SC framework
(LAM-SC) specifically designed for image data, where we first design the
segment anything model (SAM)-based KB (SKB) that can split the original image
into different semantic segments by universal semantic knowledge. Then, we
present an attention-based semantic integration (ASI) to weigh the semantic
segments generated by SKB without human participation and integrate them as the
semantic-aware image. Additionally, we propose an adaptive semantic compression
(ASC) encoding to remove redundant information in semantic features, thereby
reducing communication overhead. Finally, through simulations, we demonstrate
the effectiveness of the LAM-SC framework and the significance of the large AI
model-based KB development in future SC paradigms.",None,-1
32e56e46-0169-4edc-b8b1-1b2753826221,Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks,0.76322,"In the real world, image degradations caused by rain often exhibit a
combination of rain streaks and raindrops, thereby increasing the challenges of
recovering the underlying clean image. Note that the rain streaks and raindrops
have diverse shapes, sizes, and locations in the captured image, and thus
modeling the correlation relationship between irregular degradations caused by
rain artifacts is a necessary prerequisite for image deraining. This paper aims
to present an efficient and flexible mechanism to learn and model degradation
relationships in a global view, thereby achieving a unified removal of
intricate rain scenes. To do so, we propose a Sparse Sampling Transformer based
on Uncertainty-Driven Ranking, dubbed UDR-S2Former. Compared to previous
methods, our UDR-S2Former has three merits. First, it can adaptively sample
relevant image degradation information to model underlying degradation
relationships. Second, explicit application of the uncertainty-driven ranking
strategy can facilitate the network to attend to degradation features and
understand the reconstruction process. Finally, experimental results show that
our UDR-S2Former clearly outperforms state-of-the-art methods for all
benchmarks.",None,-1
12fdb36a-29af-4597-868d-887cee29680d,Exploiting Inductive Bias in Transformer for Point Cloud Classification and Segmentation,0.200436,"Discovering inter-point connection for efficient high-dimensional feature
extraction from point coordinate is a key challenge in processing point cloud.
Most existing methods focus on designing efficient local feature extractors
while ignoring global connection, or vice versa. In this paper, we design a new
Inductive Bias-aided Transformer (IBT) method to learn 3D inter-point
relations, which considers both local and global attentions. Specifically,
considering local spatial coherence, local feature learning is performed
through Relative Position Encoding and Attentive Feature Pooling. We
incorporate the learned locality into the Transformer module. The local feature
affects value component in Transformer to modulate the relationship between
channels of each point, which can enhance self-attention mechanism with
locality based channel interaction. We demonstrate its superiority
experimentally on classification and segmentation tasks. The code is available
at: https://github.com/jiamang/IBT",None,-1
2391bc66-374b-461a-a20d-c3e4aa705297,Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,0.722883,"Retrieval augmentation can aid language models (LMs) in knowledge-intensive
tasks by supplying them with external information. Prior works on retrieval
augmentation usually jointly fine-tune the retriever and the LM, making them
closely coupled. In this paper, we explore the scheme of generic retrieval
plug-in: the retriever is to assist target LMs that may not be known beforehand
or are unable to be fine-tuned together. To retrieve useful documents for
unseen target LMs, we propose augmentation-adapted retriever (AAR), which
learns LM's preferences obtained from a known source LM. Experiments on the
MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM
is able to significantly improve the zero-shot generalization of larger target
LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates
that the preferences of different LMs overlap, enabling AAR trained with a
single source LM to serve as a generic plug-in for various target LMs. Our code
is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",None,-1
4665dc25-2b29-4f5d-9cd9-d095ea384059,Robust Knowledge Distillation from RNN-T Models With Noisy Training Labels Using Full-Sum Loss,0.0915751,"This work studies knowledge distillation (KD) and addresses its constraints
for recurrent neural network transducer (RNN-T) models. In hard distillation, a
teacher model transcribes large amounts of unlabelled speech to train a student
model. Soft distillation is another popular KD method that distills the output
logits of the teacher model. Due to the nature of RNN-T alignments, applying
soft distillation between RNN-T architectures having different posterior
distributions is challenging. In addition, bad teachers having high
word-error-rate (WER) reduce the efficacy of KD. We investigate how to
effectively distill knowledge from variable quality ASR teachers, which has not
been studied before to the best of our knowledge. We show that a sequence-level
KD, full-sum distillation, outperforms other distillation methods for RNN-T
models, especially for bad teachers. We also propose a variant of full-sum
distillation that distills the sequence discriminative knowledge of the teacher
leading to further improvement in WER. We conduct experiments on public
datasets namely SpeechStew and LibriSpeech, and on in-house production data.",None,-1
7a31e619-3418-48cc-a82e-365918bb5d7d,Decentralized Monte Carlo Tree Search for Partially Observable Multi-agent Pathfinding,0.811397,"The Multi-Agent Pathfinding (MAPF) problem involves finding a set of
conflict-free paths for a group of agents confined to a graph. In typical MAPF
scenarios, the graph and the agents' starting and ending vertices are known
beforehand, allowing the use of centralized planning algorithms. However, in
this study, we focus on the decentralized MAPF setting, where the agents may
observe the other agents only locally and are restricted in communications with
each other. Specifically, we investigate the lifelong variant of MAPF, where
new goals are continually assigned to the agents upon completion of previous
ones. Drawing inspiration from the successful AlphaZero approach, we propose a
decentralized multi-agent Monte Carlo Tree Search (MCTS) method for MAPF tasks.
Our approach utilizes the agent's observations to recreate the intrinsic Markov
decision process, which is then used for planning with a tailored for
multi-agent tasks version of neural MCTS. The experimental results show that
our approach outperforms state-of-the-art learnable MAPF solvers. The source
code is available at https://github.com/AIRI-Institute/mats-lp.",None,-1
dd36e8a0-8e6c-49be-8771-f6f216b767a6,MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video,0.0434858,"We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRF
avatars are rendered in real-time on mobile devices, learned from monocular
videos, and have high realism. We use SMPL-X as a proxy geometry and render it
with DNR (neural texture and image-2-image network). We improve on prior work,
by overfitting per-frame warping fields in the neural texture space, allowing
to better align the training signal between different frames. We also refine
SMPL-X mesh fitting procedure to improve the overall avatar quality. In the
comparisons to other monocular video-based avatar systems, MoRF avatars achieve
higher image sharpness and temporal consistency. Participants of our user study
also preferred avatars generated by MoRF.",None,-1
2e91d036-d76d-414c-900e-9b58193edde2,Agent-based Learning of Materials Datasets from Scientific Literature,0.863402,"Advancements in machine learning and artificial intelligence are transforming
materials discovery. Yet, the availability of structured experimental data
remains a bottleneck. The vast corpus of scientific literature presents a
valuable and rich resource of such data. However, manual dataset creation from
these resources is challenging due to issues in maintaining quality and
consistency, scalability limitations, and the risk of human error and bias.
Therefore, in this work, we develop a chemist AI agent, powered by large
language models (LLMs), to overcome these challenges by autonomously creating
structured datasets from natural language text, ranging from sentences and
paragraphs to extensive scientific research articles. Our chemist AI agent,
Eunomia, can plan and execute actions by leveraging the existing knowledge from
decades of scientific research articles, scientists, the Internet and other
tools altogether. We benchmark the performance of our approach in three
different information extraction tasks with various levels of complexity,
including solid-state impurity doping, metal-organic framework (MOF) chemical
formula, and property relations. Our results demonstrate that our zero-shot
agent, with the appropriate tools, is capable of attaining performance that is
either superior or comparable to the state-of-the-art fine-tuned materials
information extraction methods. This approach simplifies compilation of machine
learning-ready datasets for various materials discovery applications, and
significantly ease the accessibility of advanced natural language processing
tools for novice users in natural language. The methodology in this work is
developed as an open-source software on https://github.com/AI4ChemS/Eunomia.",None,-1
62ecd736-7cef-43dd-acb0-e5ae8956fb12,Content-based Unrestricted Adversarial Attack,0.859301,"Unrestricted adversarial attacks typically manipulate the semantic content of
an image (e.g., color or texture) to create adversarial examples that are both
effective and photorealistic, demonstrating their ability to deceive human
perception and deep neural networks with stealth and success. However, current
works usually sacrifice unrestricted degrees and subjectively select some image
content to guarantee the photorealism of unrestricted adversarial examples,
which limits its attack performance. To ensure the photorealism of adversarial
examples and boost attack performance, we propose a novel unrestricted attack
framework called Content-based Unrestricted Adversarial Attack. By leveraging a
low-dimensional manifold that represents natural images, we map the images onto
the manifold and optimize them along its adversarial direction. Therefore,
within this framework, we implement Adversarial Content Attack based on Stable
Diffusion and can generate high transferable unrestricted adversarial examples
with various adversarial contents. Extensive experimentation and visualization
demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art
attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models
and defense methods, respectively.",None,-1
3c8f981a-28b4-424c-867e-c2d9b506d698,PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds,0.984794,"In order to deal with the sparse and unstructured raw point clouds, LiDAR
based 3D object detection research mostly focuses on designing dedicated local
point aggregators for fine-grained geometrical modeling. In this paper, we
revisit the local point aggregators from the perspective of allocating
computational resources. We find that the simplest pillar based models perform
surprisingly well considering both accuracy and latency. Additionally, we show
that minimal adaptions from the success of 2D object detection, such as
enlarging receptive field, significantly boost the performance. Extensive
experiments reveal that our pillar based networks with modernized designs in
terms of architecture and training render the state-of-the-art performance on
the two popular benchmarks: Waymo Open Dataset and nuScenes. Our results
challenge the common intuition that the detailed geometry modeling is essential
to achieve high performance for 3D object detection.",None,-1
1e8e252f-9d96-41c5-a84a-0bfdf04a647d,Automatic Spell Checker and Correction for Under-represented Spoken Languages: Case Study on Wolof,0.93919,"This paper presents a spell checker and correction tool specifically designed
for Wolof, an under-represented spoken language in Africa. The proposed spell
checker leverages a combination of a trie data structure, dynamic programming,
and the weighted Levenshtein distance to generate suggestions for misspelled
words. We created novel linguistic resources for Wolof, such as a lexicon and a
corpus of misspelled words, using a semi-automatic approach that combines
manual and automatic annotation methods. Despite the limited data available for
the Wolof language, the spell checker's performance showed a predictive
accuracy of 98.31% and a suggestion accuracy of 93.33%. Our primary focus
remains the revitalization and preservation of Wolof as an Indigenous and
spoken language in Africa, providing our efforts to develop novel linguistic
resources. This work represents a valuable contribution to the growth of
computational tools and resources for the Wolof language and provides a strong
foundation for future studies in the automatic spell checking and correction
field.",None,-1
19b9c7c5-2dff-499d-97b9-865f257b5056,Online Map Vectorization for Autonomous Driving: A Rasterization Perspective,0.588363,"Vectorized high-definition (HD) map is essential for autonomous driving,
providing detailed and precise environmental information for advanced
perception and planning. However, current map vectorization methods often
exhibit deviations, and the existing evaluation metric for map vectorization
lacks sufficient sensitivity to detect these deviations. To address these
limitations, we propose integrating the philosophy of rasterization into map
vectorization. Specifically, we introduce a new rasterization-based evaluation
metric, which has superior sensitivity and is better suited to real-world
autonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization
via Rasterization), a novel framework that applies differentiable rasterization
to vectorized outputs and then performs precise and geometry-aware supervision
on rasterized HD maps. Notably, MapVR designs tailored rasterization strategies
for various geometric shapes, enabling effective adaptation to a wide range of
map elements. Experiments show that incorporating rasterization into map
vectorization greatly enhances performance with no extra computational cost
during inference, leading to more accurate map perception and ultimately
promoting safer autonomous driving.",None,-1
e7036f6f-fd17-4616-a987-5a9834ccded1,Do large language models solve verbal analogies like children do?,0.585665,"Analogy-making lies at the heart of human cognition. Adults solve analogies
such as \textit{Horse belongs to stable like chicken belongs to ...?} by
mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In
contrast, children often use association, e.g., answering \textit{egg}. This
paper investigates whether large language models (LLMs) solve verbal analogies
in A:B::C:? form using associations, similar to what children do. We use verbal
analogies extracted from an online adaptive learning environment, where 14,002
7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six
tested Dutch monolingual and multilingual LLMs performed around the same level
as children, with MGPT performing worst, around the 7-year-old level, and XLM-V
and GPT-3 the best, slightly above the 11-year-old level. However, when we
control for associative processes this picture changes and each model's
performance level drops 1-2 years. Further experiments demonstrate that
associative processes often underlie correctly solved analogies. We conclude
that the LLMs we tested indeed tend to solve verbal analogies by association
with C like children do.",None,-1
494c9a1f-5c31-4701-9b64-e0074fde69e0,$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models,0.529348,"Fine-tuning a language model on a new domain is standard practice for domain
adaptation. However, it can be infeasible when it comes to modern large-scale
language models such as GPT-3, which can only be accessed through APIs, making
it difficult to access the internal parameters of the model. In this paper, we
propose $k$NN-Adapter, a method to effectively adapt these black-box large
language models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the
retrieval-augmented language model, and adaptively learns to interpolate the
output of the language model with retrieval results from a datastore consisting
of the target domain data. Our experiments on four different domains
demonstrate that $k$NN-Adapter significantly improves perplexity, and works
particularly well in settings with limited access to LLMs. Additionally, we
show that $k$NN-Adapter is more effective than fine-tuning when the amount of
training data is limited. We also release a dataset to encourage further study.",None,-1
3015c09e-e631-4e28-9779-a59d998d0310,Neural LiDAR Fields for Novel View Synthesis,0.750486,"We present Neural Fields for LiDAR (NFL), a method to optimise a neural field
scene representation from LiDAR measurements, with the goal of synthesizing
realistic LiDAR scans from novel viewpoints. NFL combines the rendering power
of neural fields with a detailed, physically motivated model of the LiDAR
sensing process, thus enabling it to accurately reproduce key sensor behaviors
like beam divergence, secondary returns, and ray dropping. We evaluate NFL on
synthetic and real LiDAR scans and show that it outperforms explicit
reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR
novel view synthesis task. Moreover, we show that the improved realism of the
synthesized views narrows the domain gap to real scans and translates to better
registration and semantic segmentation performance.",None,-1
b21630e1-6af5-4f80-93ba-bf569aaecc41,Vision Conformer: Incorporating Convolutions into Vision Transformer Layers,0.0422858,"Transformers are popular neural network models that use layers of
self-attention and fully-connected nodes with embedded tokens. Vision
Transformers (ViT) adapt transformers for image recognition tasks. In order to
do this, the images are split into patches and used as tokens. One issue with
ViT is the lack of inductive bias toward image structures. Because ViT was
adapted for image data from language modeling, the network does not explicitly
handle issues such as local translations, pixel information, and information
loss in the structures and features shared by multiple patches. Conversely,
Convolutional Neural Networks (CNN) incorporate this information. Thus, in this
paper, we propose the use of convolutional layers within ViT. Specifically, we
propose a model called a Vision Conformer (ViC) which replaces the Multi-Layer
Perceptron (MLP) in a ViT layer with a CNN. In addition, to use the CNN, we
proposed to reconstruct the image data after the self-attention in a reverse
embedding layer. Through the evaluation, we demonstrate that the proposed
convolutions help improve the classification ability of ViT.",None,-1
5e1be299-0d97-4aa2-9736-3d1b1aca6d7c,Full Parameter Fine-tuning for Large Language Models with Limited Resources,0.75408,"Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.Code and data are available at
https://github.com/OpenLMLab/LOMO.",None,-1
8202bfcd-c5f4-434c-9f9f-76f6d094e4ec,Masked Contrastive Graph Representation Learning for Age Estimation,0.883153,"Age estimation of face images is a crucial task with various practical
applications in areas such as video surveillance and Internet access control.
While deep learning-based age estimation frameworks, e.g., convolutional neural
network (CNN), multi-layer perceptrons (MLP), and transformers have shown
remarkable performance, they have limitations when modelling complex or
irregular objects in an image that contains a large amount of redundant
information. To address this issue, this paper utilizes the robustness property
of graph representation learning in dealing with image redundancy information
and proposes a novel Masked Contrastive Graph Representation Learning (MCGRL)
method for age estimation. Specifically, our approach first leverages CNN to
extract semantic features of the image, which are then partitioned into patches
that serve as nodes in the graph. Then, we use a masked graph convolutional
network (GCN) to derive image-based node representations that capture rich
structural information. Finally, we incorporate multiple losses to explore the
complementary relationship between structural information and semantic
features, which improves the feature representation capability of GCN.
Experimental results on real-world face image datasets demonstrate the
superiority of our proposed method over other state-of-the-art age estimation
approaches.",None,-1
5afe47c4-eb5c-40ca-9a83-69cbb8fcc669,Flexible Techniques for Differentiable Rendering with 3D Gaussians,0.641828,"Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus",None,-1
71691c59-9443-4516-a5c4-eb95b98ce518,Evaluating Pragmatic Abilities of Image Captioners on A3DS,0.074502,"Evaluating grounded neural language model performance with respect to
pragmatic qualities like the trade off between truthfulness, contrastivity and
overinformativity of generated utterances remains a challenge in absence of
data collected from humans. To enable such evaluation, we present a novel open
source image-text dataset ""Annotated 3D Shapes"" (A3DS) comprising over nine
million exhaustive natural language annotations and over 12 million
variable-granularity captions for the 480,000 images provided by Burges & Kim
(2018). We showcase the evaluation of pragmatic abilities developed by a
task-neutral image captioner fine-tuned in a multi-agent communication setting
to produce contrastive captions. The evaluation is enabled by the dataset
because the exhaustive annotations allow to quantify the presence of
contrastive features in the model's generations. We show that the model
develops human-like patterns (informativity, brevity, over-informativity for
specific features (e.g., shape, color biases)).",None,-1
e96baa38-46e4-46ce-9c8d-3765515e6449,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,0.588705,"We introduce ProtoSeg, a novel model for interpretable semantic image
segmentation, which constructs its predictions using similar patches from the
training set. To achieve accuracy comparable to baseline methods, we adapt the
mechanism of prototypical parts and introduce a diversity loss function that
increases the variety of prototypes within each class. We show that ProtoSeg
discovers semantic concepts, in contrast to standard segmentation models.
Experiments conducted on Pascal VOC and Cityscapes datasets confirm the
precision and transparency of the presented method.",None,-1
0f1806d0-3386-4ec6-81bb-efd4ccba1533,TemporalMaxer: Maximize Temporal Context with only Max Pooling for Temporal Action Localization,0.889456,"Temporal Action Localization (TAL) is a challenging task in video
understanding that aims to identify and localize actions within a video
sequence. Recent studies have emphasized the importance of applying long-term
temporal context modeling (TCM) blocks to the extracted video clip features
such as employing complex self-attention mechanisms. In this paper, we present
the simplest method ever to address this task and argue that the extracted
video clip features are already informative to achieve outstanding performance
without sophisticated architectures. To this end, we introduce TemporalMaxer,
which minimizes long-term temporal context modeling while maximizing
information from the extracted video clip features with a basic,
parameter-free, and local region operating max-pooling block. Picking out only
the most critical information for adjacent and local clip embeddings, this
block results in a more efficient TAL model. We demonstrate that TemporalMaxer
outperforms other state-of-the-art methods that utilize long-term TCM such as
self-attention on various TAL datasets while requiring significantly fewer
parameters and computational resources. The code for our approach is publicly
available at https://github.com/TuanTNG/TemporalMaxer",None,-1
cd89e94b-8221-46b4-b07a-bb23dce2ad6f,PrOnto: Language Model Evaluations for 859 Languages,0.0218046,"Evaluation datasets are critical resources for measuring the quality of
pretrained language models. However, due to the high cost of dataset
annotation, these resources are scarce for most languages other than English,
making it difficult to assess the quality of language models. In this work, we
present a new method for evaluation dataset construction which enables any
language with a New Testament translation to receive a suite of evaluation
datasets suitable for pretrained language model evaluation. The method
critically involves aligning verses with those in the New Testament portion of
English OntoNotes, and then projecting annotations from English to the target
language, with no manual annotation required. We apply this method to 1051 New
Testament translations in 859 and make them publicly available. Additionally,
we conduct experiments which demonstrate the efficacy of our method for
creating evaluation tasks which can assess language model quality.",None,-1
e0bbee37-5f61-4bab-b1f1-6a1d9e37e5dd,LaMD: Latent Motion Diffusion for Video Generation,0.513936,"Generating coherent and natural movement is the key challenge in video
generation. This research proposes to condense video generation into a problem
of motion generation, to improve the expressiveness of motion and make video
generation more manageable. This can be achieved by breaking down the video
generation process into latent motion generation and video reconstruction. We
present a latent motion diffusion (LaMD) framework, which consists of a
motion-decomposed video autoencoder and a diffusion-based motion generator, to
implement this idea. Through careful design, the motion-decomposed video
autoencoder can compress patterns in movement into a concise latent motion
representation. Meanwhile, the diffusion-based motion generator is able to
efficiently generate realistic motion on a continuous latent space under
multi-modal conditions, at a cost that is similar to that of image diffusion
models. Results show that LaMD generates high-quality videos with a wide range
of motions, from stochastic dynamics to highly controllable movements. It
achieves new state-of-the-art performance on benchmark datasets, including
BAIR, Landscape and CATER-GENs, for Image-to-Video (I2V) and
Text-Image-to-Video (TI2V) generation. The source code of LaMD will be made
available soon.",None,-1
18f531fa-035b-4c0b-bcc4-486c30f6eca0,Cloud Services Enable Efficient AI-Guided Simulation Workflows across Heterogeneous Resources,0.268991,"Applications that fuse machine learning and simulation can benefit from the
use of multiple computing resources, with, for example, simulation codes
running on highly parallel supercomputers and AI training and inference tasks
on specialized accelerators. Here, we present our experiences deploying two
AI-guided simulation workflows across such heterogeneous systems. A unique
aspect of our approach is our use of cloud-hosted management services to manage
challenging aspects of cross-resource authentication and authorization,
function-as-a-service (FaaS) function invocation, and data transfer.
  We show that these methods can achieve performance parity with systems that
rely on direct connection between resources. We achieve parity by integrating
the FaaS system and data transfer capabilities with a system that passes data
by reference among managers and workers, and a user-configurable steering
algorithm to hide data transfer latencies. We anticipate that this ease of use
can enable routine use of heterogeneous resources in computational science.",None,-1
40017c50-24f5-4b3c-bcdd-2d4b595a80f4,Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning,0.517276,"Many real-world domains require safe decision making in uncertain
environments. In this work, we introduce a deep reinforcement learning
framework for approaching this important problem. We consider a distribution
over transition models, and apply a risk-averse perspective towards model
uncertainty through the use of coherent distortion risk measures. We provide
robustness guarantees for this framework by showing it is equivalent to a
specific class of distributionally robust safe reinforcement learning problems.
Unlike existing approaches to robustness in deep reinforcement learning,
however, our formulation does not involve minimax optimization. This leads to
an efficient, model-free implementation of our approach that only requires
standard data collection from a single training environment. In experiments on
continuous control tasks with safety constraints, we demonstrate that our
framework produces robust performance and safety at deployment time across a
range of perturbed test environments.",None,-1
8b4cb05d-8018-4149-940c-b28c7d260160,Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models,0.770145,"Document-level Relation Extraction (DocRE), which aims to extract relations
from a long context, is a critical challenge in achieving fine-grained
structural comprehension and generating interpretable document representations.
Inspired by recent advances in in-context learning capabilities emergent from
large language models (LLMs), such as ChatGPT, we aim to design an automated
annotation method for DocRE with minimum human effort. Unfortunately, vanilla
in-context learning is infeasible for document-level relation extraction due to
the plenty of predefined fine-grained relation types and the uncontrolled
generations of LLMs. To tackle this issue, we propose a method integrating a
large language model (LLM) and a natural language inference (NLI) module to
generate relation triples, thereby augmenting document-level relation datasets.
We demonstrate the effectiveness of our approach by introducing an enhanced
dataset known as DocGNRE, which excels in re-annotating numerous long-tail
relation types. We are confident that our method holds the potential for
broader applications in domain-specific relation type definitions and offers
tangible benefits in advancing generalized language semantic comprehension.",None,-1
5c530bbd-1742-4830-bd28-f1378fd68b6c,Learnable Heterogeneous Convolution: Learning both topology and strength,0.0172791,"Existing convolution techniques in artificial neural networks suffer from
huge computation complexity, while the biological neural network works in a
much more powerful yet efficient way. Inspired by the biological plasticity of
dendritic topology and synaptic strength, our method, Learnable Heterogeneous
Convolution, realizes joint learning of kernel shape and weights, which unifies
existing handcrafted convolution techniques in a data-driven way. A model based
on our method can converge with structural sparse weights and then be
accelerated by devices of high parallelism. In the experiments, our method
either reduces VGG16/19 and ResNet34/50 computation by nearly 5x on CIFAR10 and
2x on ImageNet without harming the performance, where the weights are
compressed by 10x and 4x respectively; or improves the accuracy by up to 1.0%
on CIFAR10 and 0.5% on ImageNet with slightly higher efficiency. The code will
be available on www.github.com/Genera1Z/LearnableHeterogeneousConvolution.",None,-1
2f4ce541-bc44-4c7b-b450-e60c170c5090,"If our aim is to build morality into an artificial agent, how might we begin to go about doing so?",0.584772,"As Artificial Intelligence (AI) becomes pervasive in most fields, from
healthcare to autonomous driving, it is essential that we find successful ways
of building morality into our machines, especially for decision-making.
However, the question of what it means to be moral is still debated,
particularly in the context of AI. In this paper, we highlight the different
aspects that should be considered when building moral agents, including the
most relevant moral paradigms and challenges. We also discuss the top-down and
bottom-up approaches to design and the role of emotion and sentience in
morality. We then propose solutions including a hybrid approach to design and a
hierarchical approach to combining moral paradigms. We emphasize how governance
and policy are becoming ever more critical in AI Ethics and in ensuring that
the tasks we set for moral agents are attainable, that ethical behavior is
achieved, and that we obtain good AI.",None,-1
062eccfa-bcac-462f-ab6a-cf5ff0eebff1,Tab2KG: Semantic Table Interpretation with Lightweight Semantic Profiles,0.447764,"Tabular data plays an essential role in many data analytics and machine
learning tasks. Typically, tabular data does not possess any machine-readable
semantics. In this context, semantic table interpretation is crucial for making
data analytics workflows more robust and explainable. This article proposes
Tab2KG - a novel method that targets at the interpretation of tables with
previously unseen data and automatically infers their semantics to transform
them into semantic data graphs. We introduce original lightweight semantic
profiles that enrich a domain ontology's concepts and relations and represent
domain and table characteristics. We propose a one-shot learning approach that
relies on these profiles to map a tabular dataset containing previously unseen
instances to a domain ontology. In contrast to the existing semantic table
interpretation approaches, Tab2KG relies on the semantic profiles only and does
not require any instance lookup. This property makes Tab2KG particularly
suitable in the data analytics context, in which data tables typically contain
new instances. Our experimental evaluation on several real-world datasets from
different application domains demonstrates that Tab2KG outperforms
state-of-the-art semantic table interpretation baselines.",None,-1
d731da42-a8a5-4ae1-bda9-3a043f33f521,FEED PETs: Further Experimentation and Expansion on the Disambiguation of Potentially Euphemistic Terms,0.696679,"Transformers have been shown to work well for the task of English euphemism
disambiguation, in which a potentially euphemistic term (PET) is classified as
euphemistic or non-euphemistic in a particular context. In this study, we
expand on the task in two ways. First, we annotate PETs for vagueness, a
linguistic property associated with euphemisms, and find that transformers are
generally better at classifying vague PETs, suggesting linguistic differences
in the data that impact performance. Second, we present novel euphemism corpora
in three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform
euphemism disambiguation experiments in each language using multilingual
transformer models mBERT and XLM-RoBERTa, establishing preliminary results from
which to launch future work.",None,-1
9be5680e-ed10-440b-9113-51aab79cc7d6,Neuromorphic Event-based Facial Expression Recognition,0.949129,"Recently, event cameras have shown large applicability in several computer
vision fields especially concerning tasks that require high temporal
resolution. In this work, we investigate the usage of such kind of data for
emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based
Facial Expression Recognition. NEFER is composed of paired RGB and event videos
representing human faces labeled with the respective emotions and also
annotated with face bounding boxes and facial landmarks. We detail the data
acquisition process as well as providing a baseline method for RGB and event
data. The collected data captures subtle micro-expressions, which are hard to
spot with RGB data, yet emerge in the event domain. We report a double
recognition accuracy for the event-based approach, proving the effectiveness of
a neuromorphic approach for analyzing fast and hardly detectable expressions
and the emotions they conceal.",None,-1
6431b4b6-bb71-4b5a-b019-1e2f041db108,DaMuEL: A Large Multilingual Dataset for Entity Linking,0.236521,"We present DaMuEL, a large Multilingual Dataset for Entity Linking containing
data in 53 languages. DaMuEL consists of two components: a knowledge base that
contains language-agnostic information about entities, including their claims
from Wikidata and named entity types (PER, ORG, LOC, EVENT, BRAND, WORK_OF_ART,
MANUFACTURED); and Wikipedia texts with entity mentions linked to the knowledge
base, along with language-specific text from Wikidata such as labels, aliases,
and descriptions, stored separately for each language. The Wikidata QID is used
as a persistent, language-agnostic identifier, enabling the combination of the
knowledge base with language-specific texts and information for each entity.
Wikipedia documents deliberately annotate only a single mention for every
entity present; we further automatically detect all mentions of named entities
linked from each document. The dataset contains 27.9M named entities in the
knowledge base and 12.3G tokens from Wikipedia texts. The dataset is published
under the CC BY-SA license at https://hdl.handle.net/11234/1-5047.",None,-1
8c4da732-8024-4351-88b7-25125cb5e3b9,AutoSTL: Automated Spatio-Temporal Multi-Task Learning,0.785725,"Spatio-Temporal prediction plays a critical role in smart city construction.
Jointly modeling multiple spatio-temporal tasks can further promote an
intelligent city life by integrating their inseparable relationship. However,
existing studies fail to address this joint learning problem well, which
generally solve tasks individually or a fixed task combination. The challenges
lie in the tangled relation between different properties, the demand for
supporting flexible combinations of tasks and the complex spatio-temporal
dependency. To cope with the problems above, we propose an Automated
Spatio-Temporal multi-task Learning (AutoSTL) method to handle multiple
spatio-temporal tasks jointly. Firstly, we propose a scalable architecture
consisting of advanced spatio-temporal operations to exploit the complicated
dependency. Shared modules and feature fusion mechanism are incorporated to
further capture the intrinsic relationship between tasks. Furthermore, our
model automatically allocates the operations and fusion weight. Extensive
experiments on benchmark datasets verified that our model achieves
state-of-the-art performance. As we can know, AutoSTL is the first automated
spatio-temporal multi-task learning method.",None,-1
6b6b0ef0-ec9f-489a-a7f9-37178842ea4f,Fake News Detectors are Biased against Texts Generated by Large Language Models,0.878794,"The spread of fake news has emerged as a critical challenge, undermining
trust and posing threats to society. In the era of Large Language Models
(LLMs), the capability to generate believable fake content has intensified
these concerns. In this study, we present a novel paradigm to evaluate fake
news detectors in scenarios involving both human-written and LLM-generated
misinformation. Intriguingly, our findings reveal a significant bias in many
existing detectors: they are more prone to flagging LLM-generated content as
fake news while often misclassifying human-written fake news as genuine. This
unexpected bias appears to arise from distinct linguistic patterns inherent to
LLM outputs. To address this, we introduce a mitigation strategy that leverages
adversarial training with LLM-paraphrased genuine news. The resulting model
yielded marked improvements in detection accuracy for both human and
LLM-generated news. To further catalyze research in this domain, we release two
comprehensive datasets, \texttt{GossipCop++} and \texttt{PolitiFact++}, thus
amalgamating human-validated articles with LLM-generated fake and real news.",None,-1
8382944c-77e7-44f2-9770-1c38d69e57d1,Reducing Training Demands for 3D Gait Recognition with Deep Koopman Operator Constraints,0.328931,"Deep learning research has made many biometric recognition solution viable,
but it requires vast training data to achieve real-world generalization. Unlike
other biometric traits, such as face and ear, gait samples cannot be easily
crawled from the web to form massive unconstrained datasets. As the human body
has been extensively studied for different digital applications, one can rely
on prior shape knowledge to overcome data scarcity. This work follows the
recent trend of fitting a 3D deformable body model into gait videos using deep
neural networks to obtain disentangled shape and pose representations for each
frame. To enforce temporal consistency in the network, we introduce a new
Linear Dynamical Systems (LDS) module and loss based on Koopman operator
theory, which provides an unsupervised motion regularization for the periodic
nature of gait, as well as a predictive capacity for extending gait sequences.
We compare LDS to the traditional adversarial training approach and use the USF
HumanID and CASIA-B datasets to show that LDS can obtain better accuracy with
less training data. Finally, we also show that our 3D modeling approach is much
better than other 3D gait approaches in overcoming viewpoint variation under
normal, bag-carrying and clothing change conditions.",None,-1
786352a8-cd25-4bed-89d4-f2617b36824f,Solving Quantum-Inspired Perfect Matching Problems via Tutte's Theorem-Based Hybrid Boolean Constraints,0.491822,"Determining the satisfiability of Boolean constraint-satisfaction problems
with different types of constraints, that is hybrid constraints, is a
well-studied problem with important applications. We study here a new
application of hybrid Boolean constraints, which arises in quantum computing.
The problem relates to constrained perfect matching in edge-colored graphs.
While general-purpose hybrid constraint solvers can be powerful, we show that
direct encodings of the constrained-matching problem as hybrid constraints
scale poorly and special techniques are still needed. We propose a novel
encoding based on Tutte's Theorem in graph theory as well as optimization
techniques. Empirical results demonstrate that our encoding, in suitable
languages with advanced SAT solvers, scales significantly better than a number
of competing approaches on constrained-matching benchmarks. Our study
identifies the necessity of designing problem-specific encodings when applying
powerful general-purpose constraint solvers.",None,-1
8f8b16e0-5d83-46a1-8928-bc30b8599390,SepVAE: a contrastive VAE to separate pathological patterns from healthy ones,0.360889,"Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders
(VAEs) that aims at separating the common factors of variation between a
background dataset (BG) (i.e., healthy subjects) and a target dataset (TG)
(i.e., patients) from the ones that only exist in the target dataset. To do so,
these methods separate the latent space into a set of salient features (i.e.,
proper to the target dataset) and a set of common features (i.e., exist in both
datasets). Currently, all models fail to prevent the sharing of information
between latent spaces effectively and to capture all salient factors of
variation. To this end, we introduce two crucial regularization losses: a
disentangling term between common and salient representations and a
classification term between background and target samples in the salient space.
We show a better performance than previous CA-VAEs methods on three medical
applications and a natural images dataset (CelebA). Code and datasets are
available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.",None,-1
da16153c-8487-423b-8f83-de59e89d4223,Low-Light Image Enhancement via Structure Modeling and Guidance,0.999538,"This paper proposes a new framework for low-light image enhancement by
simultaneously conducting the appearance as well as structure modeling. It
employs the structural feature to guide the appearance enhancement, leading to
sharp and realistic results. The structure modeling in our framework is
implemented as the edge detection in low-light images. It is achieved with a
modified generative model via designing a structure-aware feature extractor and
generator. The detected edge maps can accurately emphasize the essential
structural information, and the edge prediction is robust towards the noises in
dark areas. Moreover, to improve the appearance modeling, which is implemented
with a simple U-Net, a novel structure-guided enhancement module is proposed
with structure-guided feature synthesis layers. The appearance modeling, edge
detector, and enhancement module can be trained end-to-end. The experiments are
conducted on representative datasets (sRGB and RAW domains), showing that our
model consistently achieves SOTA performance on all datasets with the same
architecture.",None,-1
b8c2e35f-7ad7-4a17-b03e-ecd46734a5a9,SpinDOE: A ball spin estimation method for table tennis robot,0.701458,"Spin plays a considerable role in table tennis, making a shot's trajectory
harder to read and predict. However, the spin is challenging to measure because
of the ball's high velocity and the magnitude of the spin values. Existing
methods either require extremely high framerate cameras or are unreliable
because they use the ball's logo, which may not always be visible. Because of
this, many table tennis-playing robots ignore the spin, which severely limits
their capabilities. This paper proposes an easily implementable and reliable
spin estimation method. We developed a dotted-ball orientation estimation (DOE)
method, that can then be used to estimate the spin. The dots are first
localized on the image using a CNN and then identified using geometric hashing.
The spin is finally regressed from the estimated orientations. Using our
algorithm, the ball's orientation can be estimated with a mean error of
2.4{\deg} and the spin estimation has an relative error lower than 1%. Spins up
to 175 rps are measurable with a camera of 350 fps in real time. Using our
method, we generated a dataset of table tennis ball trajectories with position
and spin, available on our project page.",None,-1
86519ba7-ff50-4ccf-94b7-d0132df651df,Denoising Diffusion for 3D Hand Pose Estimation from Images,0.424571,"Hand pose estimation from a single image has many applications. However,
approaches to full 3D body pose estimation are typically trained on day-to-day
activities or actions. As such, detailed hand-to-hand interactions are poorly
represented, especially during motion. We see this in the failure cases of
techniques such as OpenPose or MediaPipe. However, accurate hand pose
estimation is crucial for many applications where the global body motion is
less important than accurate hand pose estimation.
  This paper addresses the problem of 3D hand pose estimation from monocular
images or sequences. We present a novel end-to-end framework for 3D hand
regression that employs diffusion models that have shown excellent ability to
capture the distribution of data for generative purposes. Moreover, we enforce
kinematic constraints to ensure realistic poses are generated by incorporating
an explicit forward kinematic layer as part of the network. The proposed model
provides state-of-the-art performance when lifting a 2D single-hand image to
3D. However, when sequence data is available, we add a Transformer module over
a temporal window of consecutive frames to refine the results, overcoming
jittering and further increasing accuracy.
  The method is quantitatively and qualitatively evaluated showing
state-of-the-art robustness, generalization, and accuracy on several different
datasets.",None,-1
a9bb4ddf-af4f-48d3-bf2d-ef1363d31b63,Boosting Theory-of-Mind Performance in Large Language Models via Prompting,0.306653,"Large language models (LLMs) excel in many tasks in 2023, but they still face
challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require
understanding agents' beliefs, goals, and mental states, are essential for
common-sense reasoning involving humans, making it crucial to enhance LLM
performance in this area. This study measures the ToM performance of GPT-4 and
three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates
the effectiveness of in-context learning in improving their ToM comprehension.
We evaluated prompts featuring two-shot chain of thought reasoning and
step-by-step thinking instructions. We found that LLMs trained with
Reinforcement Learning from Human Feedback (RLHF) (all models excluding
Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed
best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell
short of the 87% human accuracy on the test set. However, when supplied with
prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM
accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate
prompting enhances LLM ToM reasoning, and they underscore the context-dependent
nature of LLM cognitive capacities.",None,-1
cb13cd9d-d3f1-4265-9d86-7d89f4cd5e47,Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?,0.0813632,"Compositionality is a pivotal property of symbolic reasoning. However, how
well recent neural models capture compositionality remains underexplored in the
symbolic reasoning tasks. This study empirically addresses this question by
systematically examining recently published pre-trained seq2seq models with a
carefully controlled dataset of multi-hop arithmetic symbolic reasoning. We
introduce a skill tree on compositionality in arithmetic symbolic reasoning
that defines the hierarchical levels of complexity along with three
compositionality dimensions: systematicity, productivity, and substitutivity.
Our experiments revealed that among the three types of composition, the models
struggled most with systematicity, performing poorly even with relatively
simple compositions. That difficulty was not resolved even after training the
models with intermediate reasoning steps.",None,-1
6805790c-553e-4c99-afda-afe41972e9d7,Multilingual Bias Detection and Mitigation for Indian Languages,0.550969,"Lack of diverse perspectives causes neutrality bias in Wikipedia content
leading to millions of worldwide readers getting exposed by potentially
inaccurate information. Hence, neutrality bias detection and mitigation is a
critical problem. Although previous studies have proposed effective solutions
for English, no work exists for Indian languages. First, we contribute two
large datasets, mWikiBias and mWNC, covering 8 languages, for the bias
detection and mitigation tasks respectively. Next, we investigate the
effectiveness of popular multilingual Transformer-based models for the two
tasks by modeling detection as a binary classification problem and mitigation
as a style transfer problem. We make the code and data publicly available.",None,-1
98d4b190-071a-4b31-bfe5-12c23b7cf1ab,Training Machine Learning Models to Characterize Temporal Evolution of Disadvantaged Communities,0.0387908,"Disadvantaged communities (DAC), as defined by the Justice40 initiative of
the Department of Energy (DOE), USA, identifies census tracts across the USA to
determine where benefits of climate and energy investments are or are not
currently accruing. The DAC status not only helps in determining the
eligibility for future Justice40-related investments but is also critical for
exploring ways to achieve equitable distribution of resources. However,
designing inclusive and equitable strategies not just requires a good
understanding of current demographics, but also a deeper analysis of the
transformations that happened in those demographics over the years. In this
paper, machine learning (ML) models are trained on publicly available census
data from recent years to classify the DAC status at the census tracts level
and then the trained model is used to classify DAC status for historical years.
A detailed analysis of the feature and model selection along with the evolution
of disadvantaged communities between 2013 and 2018 is presented in this study.",None,-1
78373268-0602-4c69-b764-9cf467374179,ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing,0.893339,"English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is available only for research purposes.",None,-1
d169df25-e301-47f8-a582-c86533b92689,Challenges in Context-Aware Neural Machine Translation,0.390509,"Context-aware neural machine translation involves leveraging information
beyond sentence-level context to resolve inter-sentential discourse
dependencies and improve document-level translation quality, and has given rise
to a number of recent techniques. However, despite well-reasoned intuitions,
most context-aware translation models show only modest improvements over
sentence-level systems. In this work, we investigate several challenges that
impede progress within this field, relating to discourse phenomena, context
usage, model architectures, and document-level evaluation. To address these
problems, we propose a more realistic setting for document-level translation,
called paragraph-to-paragraph (para2para) translation, and collect a new
dataset of Chinese-English novels to promote future research.",None,-1
4318b605-04ec-4cd0-a7af-8114386d1a9a,Diving Deep into Modes of Fact Hallucinations in Dialogue Systems,0.525736,"Knowledge Graph(KG) grounded conversations often use large pre-trained models
and usually suffer from fact hallucination. Frequently entities with no
references in knowledge sources and conversation history are introduced into
responses, thus hindering the flow of the conversation -- existing work attempt
to overcome this issue by tweaking the training procedure or using a multi-step
refining method. However, minimal effort is put into constructing an
entity-level hallucination detection system, which would provide fine-grained
signals that control fallacious content while generating responses. As a first
step to address this issue, we dive deep to identify various modes of
hallucination in KG-grounded chatbots through human feedback analysis.
Secondly, we propose a series of perturbation strategies to create a synthetic
dataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally,
we conduct comprehensive data analyses and create multiple baseline models for
hallucination detection to compare against human-verified data and already
established benchmarks.",None,-1
050fae2f-c30e-4f3b-98a2-b19827248fab,Understanding and Unifying Fourteen Attribution Methods with Taylor Interactions,0.603966,"Various attribution methods have been developed to explain deep neural
networks (DNNs) by inferring the attribution/importance/contribution score of
each input variable to the final output. However, existing attribution methods
are often built upon different heuristics. There remains a lack of a unified
theoretical understanding of why these methods are effective and how they are
related. To this end, for the first time, we formulate core mechanisms of
fourteen attribution methods, which were designed on different heuristics, into
the same mathematical system, i.e., the system of Taylor interactions.
Specifically, we prove that attribution scores estimated by fourteen
attribution methods can all be reformulated as the weighted sum of two types of
effects, i.e., independent effects of each individual input variable and
interaction effects between input variables. The essential difference among the
fourteen attribution methods mainly lies in the weights of allocating different
effects. Based on the above findings, we propose three principles for a fair
allocation of effects to evaluate the faithfulness of the fourteen attribution
methods.",None,-1
93412ab3-8512-445a-b218-f75ff4847c13,Crystal: Introspective Reasoners Reinforced with Self-Feedback,0.355776,"Extensive work has shown that the performance and interpretability of
commonsense reasoning can be improved via knowledge-augmented reasoning
methods, where the knowledge that underpins the reasoning process is explicitly
verbalized and utilized. However, existing implementations, including
""chain-of-thought"" and its variants, fall short in capturing the introspective
nature of knowledge required in commonsense reasoning, and in accounting for
the mutual adaptation between the generation and utilization of knowledge. We
propose a novel method to develop an introspective commonsense reasoner,
Crystal. To tackle commonsense problems, it first introspects for knowledge
statements related to the given question, and subsequently makes an informed
prediction that is grounded in the previously introspected knowledge. The
knowledge introspection and knowledge-grounded reasoning modes of the model are
tuned via reinforcement learning to mutually adapt, where the reward derives
from the feedback given by the model itself. Experiments show that Crystal
significantly outperforms both the standard supervised finetuning and
chain-of-thought distilled methods, and enhances the transparency of the
commonsense reasoning process. Our work ultimately validates the feasibility
and potential of reinforcing a neural model with self-feedback.",None,-1
8ea53281-ab67-4938-82f9-960baaa4c415,DarkBERT: A Language Model for the Dark Side of the Internet,0.726194,"Recent research has suggested that there are clear differences in the
language used in the Dark Web compared to that of the Surface Web. As studies
on the Dark Web commonly require textual analysis of the domain, language
models specific to the Dark Web may provide valuable insights to researchers.
In this work, we introduce DarkBERT, a language model pretrained on Dark Web
data. We describe the steps taken to filter and compile the text data used to
train DarkBERT to combat the extreme lexical and structural diversity of the
Dark Web that may be detrimental to building a proper representation of the
domain. We evaluate DarkBERT and its vanilla counterpart along with other
widely used language models to validate the benefits that a Dark Web domain
specific model offers in various use cases. Our evaluations show that DarkBERT
outperforms current language models and may serve as a valuable resource for
future research on the Dark Web.",None,-1
69db896f-a53b-44a9-84b3-a90db0a4bef2,Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks,0.311274,"Feature selection that selects an informative subset of variables from data
not only enhances the model interpretability and performance but also
alleviates the resource demands. Recently, there has been growing attention on
feature selection using neural networks. However, existing methods usually
suffer from high computational costs when applied to high-dimensional datasets.
In this paper, inspired by evolution processes, we propose a novel
resource-efficient supervised feature selection method using sparse neural
networks, named \enquote{NeuroFS}. By gradually pruning the uninformative
features from the input layer of a sparse neural network trained from scratch,
NeuroFS derives an informative subset of features efficiently. By performing
several experiments on $11$ low and high-dimensional real-world benchmarks of
different types, we demonstrate that NeuroFS achieves the highest ranking-based
score among the considered state-of-the-art supervised feature selection
models. The code is available on GitHub.",None,-1
e7a38496-6fef-4798-a417-588581ab84b5,Guided Image Synthesis via Initial Image Editing in Diffusion Model,0.660078,"Diffusion models have the ability to generate high quality images by
denoising pure Gaussian noise images. While previous research has primarily
focused on improving the control of image generation through adjusting the
denoising process, we propose a novel direction of manipulating the initial
noise to control the generated image. Through experiments on stable diffusion,
we show that blocks of pixels in the initial latent images have a preference
for generating specific content, and that modifying these blocks can
significantly influence the generated image. In particular, we show that
modifying a part of the initial image affects the corresponding region of the
generated image while leaving other regions unaffected, which is useful for
repainting tasks. Furthermore, we find that the generation preferences of pixel
blocks are primarily determined by their values, rather than their position. By
moving pixel blocks with a tendency to generate user-desired content to
user-specified regions, our approach achieves state-of-the-art performance in
layout-to-image generation. Our results highlight the flexibility and power of
initial image manipulation in controlling the generated image.",None,-1
7f3a34e3-161f-4e3a-90d9-2e50609c8983,Privacy-Preserving Tree-Based Inference with TFHE,0.148955,"Privacy enhancing technologies (PETs) have been proposed as a way to protect
the privacy of data while still allowing for data analysis. In this work, we
focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for
arbitrary computations to be performed on encrypted data. FHE has received lots
of attention in the past few years and has reached realistic execution times
and correctness.
  More precisely, we explain in this paper how we apply FHE to tree-based
models and get state-of-the-art solutions over encrypted tabular data. We show
that our method is applicable to a wide range of tree-based models, including
decision trees, random forests, and gradient boosted trees, and has been
implemented within the Concrete-ML library, which is open-source at
https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we
demonstrate that our FHE version is very close to the unprotected version in
terms of accuracy.",None,-1
15523a39-a1b0-47b7-ae05-d513b44a7781,Coherent Concept-based Explanations in Medical Image and Its Application to Skin Lesion Diagnosis,0.733773,"Early detection of melanoma is crucial for preventing severe complications
and increasing the chances of successful treatment. Existing deep learning
approaches for melanoma skin lesion diagnosis are deemed black-box models, as
they omit the rationale behind the model prediction, compromising the
trustworthiness and acceptability of these diagnostic methods. Attempts to
provide concept-based explanations are based on post-hoc approaches, which
depend on an additional model to derive interpretations. In this paper, we
propose an inherently interpretable framework to improve the interpretability
of concept-based models by incorporating a hard attention mechanism and a
coherence loss term to assure the visual coherence of concept activations by
the concept encoder, without requiring the supervision of additional
annotations. The proposed framework explains its decision in terms of
human-interpretable concepts and their respective contribution to the final
prediction, as well as a visual interpretation of the locations where the
concept is present in the image. Experiments on skin image datasets demonstrate
that our method outperforms existing black-box and concept-based models for
skin lesion classification.",None,-1
9dd7e023-46ea-4d4c-9515-23818d01d38c,Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion,0.803374,"Temporal Knowledge graph completion (TKGC) is a crucial task that involves
reasoning at known timestamps to complete the missing part of facts and has
attracted more and more attention in recent years. Most existing methods focus
on learning representations based on graph neural networks while inaccurately
extracting information from timestamps and insufficiently utilizing the implied
information in relations. To address these problems, we propose a novel TKGC
model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We
convert a series of sampled quadruples into pre-trained language model inputs
and convert intervals between timestamps into different prompts to make
coherent sentences with implicit semantic information. We train our model with
a masking strategy to convert TKGC task into a masked token prediction task,
which can leverage the semantic information in pre-trained language models.
Experiments on three benchmark datasets and extensive analysis demonstrate that
our model has great competitiveness compared to other models with four metrics.
Our model can effectively incorporate information from temporal knowledge
graphs into the language models.",None,-1
45e2cfd9-ea23-47c4-8581-0ba5acba8a7d,Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur,0.404098,"Rendering novel view images is highly desirable for many applications.
Despite recent progress, it remains challenging to render high-fidelity and
view-consistent novel views of large-scale scenes from in-the-wild images with
inevitable artifacts (e.g., motion blur). To this end, we develop a hybrid
neural rendering model that makes image-based representation and neural 3D
representation join forces to render high-quality, view-consistent images.
Besides, images captured in the wild inevitably contain artifacts, such as
motion blur, which deteriorates the quality of rendered images. Accordingly, we
propose strategies to simulate blur effects on the rendered images to mitigate
the negative influence of blurriness images and reduce their importance during
training based on precomputed quality-aware weights. Extensive experiments on
real and synthetic data demonstrate our model surpasses state-of-the-art
point-based methods for novel view synthesis. The code is available at
https://daipengwa.github.io/Hybrid-Rendering-ProjectPage.",None,-1
26ab1dd6-a718-41ac-9642-69995193b64a,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,0.296836,"Lung nodule malignancy prediction has been enhanced by advanced deep-learning
techniques and effective tricks. Nevertheless, current methods are mainly
trained with cross-entropy loss using one-hot categorical labels, which results
in difficulty in distinguishing those nodules with closer progression labels.
Interestingly, we observe that clinical text information annotated by
radiologists provides us with discriminative knowledge to identify challenging
samples. Drawing on the capability of the contrastive language-image
pre-training (CLIP) model to learn generalized visual representations from text
annotations, in this paper, we propose CLIP-Lung, a textual knowledge-guided
framework for lung nodule malignancy prediction. First, CLIP-Lung introduces
both class and attribute annotations into the training of the lung nodule
classifier without any additional overheads in inference. Second, we designed a
channel-wise conditional prompt (CCP) module to establish consistent
relationships between learnable context prompts and specific feature maps.
Third, we align image features with both class and attribute features via
contrastive learning, rectifying false positives and false negatives in latent
space. The experimental results on the benchmark LIDC-IDRI dataset have
demonstrated the superiority of CLIP-Lung, both in classification performance
and interpretability of attention maps.",None,-1
68a1633d-e406-4095-a139-3ab507a8fa07,Unpaired Image-to-Image Translation via Neural Schrdinger Bridge,0.860262,"Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. While
diffusion models have achieved remarkable progress, they have limitations in
unpaired image-to-image (I2I) translation tasks due to the Gaussian prior
assumption. Schr\""{o}dinger Bridge (SB), which learns an SDE to translate
between two arbitrary distributions, have risen as an attractive solution to
this problem. Yet, to our best knowledge, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose Unpaired Neural Schr\""{o}dinger Bridge (UNSB), which expresses
the SB problem as a sequence of adversarial learning problems. This allows us
to incorporate advanced discriminators and regularization to learn a SB between
unpaired data. We show that UNSB is scalable and successfully solves various
unpaired I2I translation tasks. Code: \url{https://github.com/cyclomon/UNSB}",None,-1
43766e22-6bad-47b5-867d-67d51165fe85,TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation,0.450764,"Recent success of Contrastive Language-Image Pre-training~(CLIP) has shown
great promise in pixel-level open-vocabulary learning tasks. A general paradigm
utilizes CLIP's text and patch embeddings to generate semantic masks. However,
existing models easily misidentify input pixels from unseen classes, thus
confusing novel classes with semantically-similar ones. In our work, we
disentangle the ill-posed optimization problem into two parallel processes: one
performs semantic matching individually, and the other judges reliability for
improving discrimination ability. Motivated by special tokens in language
modeling that represents sentence-level embeddings, we design a trusty token
that decouples the known and novel category prediction tendency. With almost no
extra overhead, we upgrade the pixel-level generalization capacity of existing
models effectively. Our TagCLIP (CLIP adapting with Trusty-guidance) boosts the
IoU of unseen classes by 7.4% and 1.7% on PASCAL VOC 2012 and COCO-Stuff 164K.",None,-1
4175ba89-1736-41e0-8e18-58172825e209,NLLB-CLIP -- train performant multilingual image retrieval model on a budget,0.556889,"Today, the exponential rise of large models developed by academic and
industrial institutions with the help of massive computing resources raises the
question of whether someone without access to such resources can make a
valuable scientific contribution. To explore this, we tried to solve the
challenging task of multilingual image retrieval having a limited budget of
$1,000. As a result, we present NLLB-CLIP - CLIP model with a text encoder from
the NLLB model. To train the model, we used an automatically created dataset of
106,246 good-quality images with captions in 201 languages derived from the
LAION COCO dataset. We trained multiple models using image and text encoders of
various sizes and kept different parts of the model frozen during the training.
We thoroughly analyzed the trained models using existing evaluation datasets
and newly created XTD200 and Flickr30k-200 datasets. We show that NLLB-CLIP is
comparable in quality to state-of-the-art models and significantly outperforms
them on low-resource languages.",None,-1
968ee8f7-268b-4739-a34a-01e2c4d45a5f,Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models,0.0798047,"While pre-trained language models (PLMs) have shown evidence of acquiring
vast amounts of knowledge, it remains unclear how much of this parametric
knowledge is actually usable in performing downstream tasks. We propose a
systematic framework to measure parametric knowledge utilization in PLMs. Our
framework first extracts knowledge from a PLM's parameters and subsequently
constructs a downstream task around this extracted knowledge. Performance on
this task thus depends exclusively on utilizing the model's possessed
knowledge, avoiding confounding factors like insufficient signal. As an
instantiation, we study factual knowledge of PLMs and measure utilization
across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps -
in acquired vs. utilized knowledge, (2) they show limited robustness in
utilizing knowledge under distribution shifts, and (3) larger models close the
acquired knowledge gap but the utilized knowledge gap remains. Overall, our
study provides insights into PLMs' capabilities beyond their acquired
knowledge.",None,-1
328aeed9-1ae8-4734-870e-47dc5ff38580,Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,0.667992,"Synthesizing interaction-involved human motions has been challenging due to
the high complexity of 3D environments and the diversity of possible human
behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to
synthesize natural and plausible long-term human movements in complex indoor
environments. The key motivation of LAMA is to build a unified framework to
encompass a series of everyday motions including locomotion, scene interaction,
and object manipulation. Unlike existing methods that require motion data
""paired"" with scanned 3D scenes for supervision, we formulate the problem as a
test-time optimization by using human motion capture data only for synthesis.
LAMA leverages a reinforcement learning framework coupled with a motion
matching algorithm for optimization, and further exploits a motion editing
framework via manifold learning to cover possible variations in interaction and
manipulation. Throughout extensive experiments, we demonstrate that LAMA
outperforms previous approaches in synthesizing realistic motions in various
challenging scenarios. Project page: https://jiyewise.github.io/projects/LAMA/ .",None,-1
881ada31-cd49-451a-b779-f0271f86f713,How optimal transport can tackle gender biases in multi-class neural-network classifiers for job recommendations?,0.232933,"Automatic recommendation systems based on deep neural networks have become
extremely popular during the last decade. Some of these systems can however be
used for applications which are ranked as High Risk by the European Commission
in the A.I. act, as for instance for online job candidate recommendation. When
used in the European Union, commercial AI systems for this purpose will then be
required to have to proper statistical properties with regard to potential
discrimination they could engender. This motivated our contribution, where we
present a novel optimal transport strategy to mitigate undesirable algorithmic
biases in multi-class neural-network classification. Our stratey is model
agnostic and can be used on any multi-class classification neural-network
model. To anticipate the certification of recommendation systems using textual
data, we then used it on the Bios dataset, for which the learning task consists
in predicting the occupation of female and male individuals, based on their
LinkedIn biography. Results show that it can reduce undesired algorithmic
biases in this context to lower levels than a standard strategy.",None,-1
491146e0-8b2b-482a-85c3-9707e952f620,DOLCE: A Descriptive Ontology for Linguistic and Cognitive Engineering,0.994987,"DOLCE, the first top-level (foundational) ontology to be axiomatized, has
remained stable for twenty years and today is broadly used in a variety of
domains. DOLCE is inspired by cognitive and linguistic considerations and aims
to model a commonsense view of reality, like the one human beings exploit in
everyday life in areas as diverse as socio-technical systems, manufacturing,
financial transactions and cultural heritage. DOLCE clearly lists the
ontological choices it is based upon, relies on philosophical principles, is
richly formalized, and is built according to well-established ontological
methodologies, e.g. OntoClean. Because of these features, it has inspired most
of the existing top-level ontologies and has been used to develop or improve
standards and public domain resources (e.g. CIDOC CRM, DBpedia and WordNet).
Being a foundational ontology, DOLCE is not directly concerned with domain
knowledge. Its purpose is to provide the general categories and relations
needed to give a coherent view of reality, to integrate domain knowledge, and
to mediate across domains. In these 20 years DOLCE has shown that applied
ontologies can be stable and that interoperability across reference and domain
ontologies is a reality. This paper briefly introduces the ontology and shows
how to use it on a few modeling cases.",None,-1
dbbefbcd-7ce2-4c4d-9fd7-6cbf9e618990,Ambient Technology & Intelligence,0.280592,"Today, we have a mixture of young and older individuals, people with special
needs, and people who can care for themselves. Over 1 billion people are
estimated to be disabled; this figure corresponds to about 15% of the world's
population, with 3.8% (approximately 190 million people) accounting for people
aged 15 and up (Organization, 2011). The number of people with disabilities is
upward due to the increase in chronic health conditions and many other things.
These and other factors have made the need for proper care facilities urgent in
today's society. Several care facilities are built to help people with
disabilities live their everyday lives and not be left out of the community.",None,-1
7b732d49-c53e-4030-889a-bc8980afec09,Acquiring Qualitative Explainable Graphs for Automated Driving Scene Interpretation,0.282159,"The future of automated driving (AD) is rooted in the development of robust,
fair and explainable artificial intelligence methods. Upon request, automated
vehicles must be able to explain their decisions to the driver and the car
passengers, to the pedestrians and other vulnerable road users and potentially
to external auditors in case of accidents. However, nowadays, most explainable
methods still rely on quantitative analysis of the AD scene representations
captured by multiple sensors. This paper proposes a novel representation of AD
scenes, called Qualitative eXplainable Graph (QXG), dedicated to qualitative
spatiotemporal reasoning of long-term scenes. The construction of this graph
exploits the recent Qualitative Constraint Acquisition paradigm. Our
experimental results on NuScenes, an open real-world multi-modal dataset, show
that the qualitative eXplainable graph of an AD scene composed of 40 frames can
be computed in real-time and light in space storage which makes it a
potentially interesting tool for improved and more trustworthy perception and
control processes in AD.",None,-1
4190f9cb-75f7-4678-acdd-703af1dfa76f,AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL,0.921708,"LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present
AutoPlanBench, a novel method for automatically converting planning benchmarks
written in PDDL into textual descriptions and offer a benchmark dataset created
with our method. We show that while the best LLM planners do well on some
planning tasks, others remain out of reach of current methods.",None,-1
54574fe7-ca60-4ebd-9792-0f397ad3f1d5,Few-Shot 3D Point Cloud Semantic Segmentation via Stratified Class-Specific Attention Based Transformer Network,0.597376,"3D point cloud semantic segmentation aims to group all points into different
semantic categories, which benefits important applications such as point cloud
scene reconstruction and understanding. Existing supervised point cloud
semantic segmentation methods usually require large-scale annotated point
clouds for training and cannot handle new categories. While a few-shot learning
method was proposed recently to address these two problems, it suffers from
high computational complexity caused by graph construction and inability to
learn fine-grained relationships among points due to the use of pooling
operations. In this paper, we further address these problems by developing a
new multi-layer transformer network for few-shot point cloud semantic
segmentation. In the proposed network, the query point cloud features are
aggregated based on the class-specific support features in different scales.
Without using pooling operations, our method makes full use of all pixel-level
features from the support samples. By better leveraging the support features
for few-shot learning, the proposed method achieves the new state-of-the-art
performance, with 15\% less inference time, over existing few-shot 3D point
cloud segmentation models on the S3DIS dataset and the ScanNet dataset.",None,-1
b424628a-b82c-4b4f-8ba0-02d5cff67a96,Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension,0.0464275,"Prompting has become an important mechanism by which users can more
effectively interact with many flavors of foundation model. Indeed, the last
several years have shown that well-honed prompts can sometimes unlock emergent
capabilities within such models. While there has been a substantial amount of
empirical exploration of prompting within the community, relatively few works
have studied prompting at a mathematical level. In this work we aim to take a
first step towards understanding basic geometric properties induced by prompts
in Stable Diffusion, focusing on the intrinsic dimension of internal
representations within the model. We find that choice of prompt has a
substantial impact on the intrinsic dimension of representations at both layers
of the model which we explored, but that the nature of this impact depends on
the layer being considered. For example, in certain bottleneck layers of the
model, intrinsic dimension of representations is correlated with prompt
perplexity (measured using a surrogate model), while this correlation is not
apparent in the latent layers. Our evidence suggests that intrinsic dimension
could be a useful tool for future studies of the impact of different prompts on
text-to-image models.",None,-1
512fd9af-6dec-4f89-b293-3aed968580d9,Syllable-level lyrics generation from melody exploiting character-level language model,0.0215351,"The generation of lyrics tightly connected to accompanying melodies involves
establishing a mapping between musical notes and syllables of lyrics. This
process requires a deep understanding of music constraints and semantic
patterns at syllable-level, word-level, and sentence-level semantic meanings.
However, pre-trained language models specifically designed at the syllable
level are publicly unavailable. To solve these challenging issues, we propose
to exploit fine-tuning character-level language models for syllable-level
lyrics generation from symbolic melody. In particular, our method endeavors to
incorporate linguistic knowledge of the language model into the beam search
process of a syllable-level Transformer generator network. Additionally, by
exploring ChatGPT-based evaluation for generated lyrics, along with human
subjective evaluation, we demonstrate that our approach enhances the coherence
and correctness of the generated lyrics, eliminating the need to train
expensive new language models.",None,-1
bb1d1f56-9997-46d5-921b-c7fd1121345b,A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases,0.332776,"Enterprise applications of Large Language Models (LLMs) hold promise for
question answering on enterprise SQL databases. However, the extent to which
LLMs can accurately respond to enterprise questions in such databases remains
unclear, given the absence of suitable Text-to-SQL benchmarks tailored to
enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to
enhance LLM-based question answering by providing business context is not well
understood. This study aims to evaluate the accuracy of LLM-powered question
answering systems in the context of enterprise questions and SQL databases,
while also exploring the role of knowledge graphs in improving accuracy. To
achieve this, we introduce a benchmark comprising an enterprise SQL schema in
the insurance domain, a range of enterprise queries encompassing reporting to
metrics, and a contextual layer incorporating an ontology and mappings that
define a knowledge graph. Our primary finding reveals that question answering
using GPT-4, with zero-shot prompts directly on SQL databases, achieves an
accuracy of 16%. Notably, this accuracy increases to 54% when questions are
posed over a Knowledge Graph representation of the enterprise SQL database.
Therefore, investing in Knowledge Graph provides higher accuracy for LLM
powered question answering systems.",None,-1
d01e8b08-68e7-4eae-b82a-8e9c56903268,Prompted LLMs as Chatbot Modules for Long Open-domain Conversation,0.655466,"In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for
creating high-quality conversational agents without the need for fine-tuning.
Our method utilizes pre-trained large language models (LLMs) as individual
modules for long-term consistency and flexibility, by using techniques such as
few-shot prompting, chain-of-thought (CoT), and external memory. Our human
evaluation results show that MPC is on par with fine-tuned chatbot models in
open-domain conversations, making it an effective solution for creating
consistent and engaging chatbots.",None,-1
e6f9500d-a80e-41a5-a942-75bf8c2d2aec,Memory Maps for Video Object Detection and Tracking on UAVs,0.374675,"This paper introduces a novel approach to video object detection detection
and tracking on Unmanned Aerial Vehicles (UAVs). By incorporating metadata, the
proposed approach creates a memory map of object locations in actual world
coordinates, providing a more robust and interpretable representation of object
locations in both, image space and the real world. We use this representation
to boost confidences, resulting in improved performance for several temporal
computer vision tasks, such as video object detection, short and long-term
single and multi-object tracking, and video anomaly detection. These findings
confirm the benefits of metadata in enhancing the capabilities of UAVs in the
field of temporal computer vision and pave the way for further advancements in
this area.",None,-1
73900614-7169-462d-96bc-1b69fc9ffab1,Graph-Transporter: A Graph-based Learning Method for Goal-Conditioned Deformable Object Rearranging Task,0.259471,"Rearranging deformable objects is a long-standing challenge in robotic
manipulation for the high dimensionality of configuration space and the complex
dynamics of deformable objects. We present a novel framework,
Graph-Transporter, for goal-conditioned deformable object rearranging tasks. To
tackle the challenge of complex configuration space and dynamics, we represent
the configuration space of a deformable object with a graph structure and the
graph features are encoded by a graph convolution network. Our framework adopts
an architecture based on Fully Convolutional Network (FCN) to output pixel-wise
pick-and-place actions from only visual input. Extensive experiments have been
conducted to validate the effectiveness of the graph representation of
deformable object configuration. The experimental results also demonstrate that
our framework is effective and general in handling goal-conditioned deformable
object rearranging tasks.",None,-1
092cf19e-3366-4d35-98e3-48ee86d2a131,GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps,0.483634,"Data augmentation is now an essential part of the image training process, as
it effectively prevents overfitting and makes the model more robust against
noisy datasets. Recent mixing augmentation strategies have advanced to generate
the mixup mask that can enrich the saliency information, which is a supervisory
signal. However, these methods incur a significant computational burden to
optimize the mixup mask. From this motivation, we propose a novel
saliency-aware mixup method, GuidedMixup, which aims to retain the salient
regions in mixup images with low computational overhead. We develop an
efficient pairing algorithm that pursues to minimize the conflict of salient
regions of paired images and achieve rich saliency in mixup images. Moreover,
GuidedMixup controls the mixup ratio for each pixel to better preserve the
salient region by interpolating two paired images smoothly. The experiments on
several datasets demonstrate that GuidedMixup provides a good trade-off between
augmentation overhead and generalization performance on classification
datasets. In addition, our method shows good performance in experiments with
corrupted or reduced datasets.",None,-1
ab6dc8d4-14ee-408e-a3d8-aaff4fd4b678,Understanding Inter-Session Intentions via Complex Logical Reasoning,0.154441,"Understanding user intentions is essential for improving product
recommendations, navigation suggestions, and query reformulations. However,
user intentions can be intricate, involving multiple sessions and attribute
requirements connected by logical operators such as And, Or, and Not. For
instance, a user may search for Nike or Adidas running shoes across various
sessions, with a preference for purple. In another example, a user may have
purchased a mattress in a previous session and is now looking for a matching
bed frame without intending to buy another mattress. Existing research on
session understanding has not adequately addressed making product or attribute
recommendations for such complex intentions. In this paper, we present the task
of logical session complex query answering (LS-CQA), where sessions are treated
as hyperedges of items, and we frame the problem of complex intention
understanding as an LS-CQA task on an aggregated hypergraph of sessions, items,
and attributes. This is a unique complex query answering task with sessions as
ordered hyperedges. We also introduce a new model, the Logical Session Graph
Transformer (LSGT), which captures interactions among items across different
sessions and their logical connections using a transformer structure. We
analyze the expressiveness of LSGT and prove the permutation invariance of the
inputs for the logical operators. By evaluating LSGT on three datasets, we
demonstrate that it achieves state-of-the-art results.",None,-1
28ab206a-67ec-47e6-a126-e7033b37818d,How Efficient Are Today's Continual Learning Algorithms?,0.566887,"Supervised Continual learning involves updating a deep neural network (DNN)
from an ever-growing stream of labeled data. While most work has focused on
overcoming catastrophic forgetting, one of the major motivations behind
continual learning is being able to efficiently update a network with new
information, rather than retraining from scratch on the training dataset as it
grows over time. Despite recent continual learning methods largely solving the
catastrophic forgetting problem, there has been little attention paid to the
efficiency of these algorithms. Here, we study recent methods for incremental
class learning and illustrate that many are highly inefficient in terms of
compute, memory, and storage. Some methods even require more compute than
training from scratch! We argue that for continual learning to have real-world
applicability, the research community cannot ignore the resources used by these
algorithms. There is more to continual learning than mitigating catastrophic
forgetting.",None,-1
a076f502-fae9-45da-a78d-1d398030f506,Toolformer: Language Models Can Teach Themselves to Use Tools,1.0,"Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic functionality, such as arithmetic or factual
lookup, where much simpler and smaller models excel. In this paper, we show
that LMs can teach themselves to use external tools via simple APIs and achieve
the best of both worlds. We introduce Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and how to best
incorporate the results into future token prediction. This is done in a
self-supervised way, requiring nothing more than a handful of demonstrations
for each API. We incorporate a range of tools, including a calculator, a Q\&A
system, two different search engines, a translation system, and a calendar.
Toolformer achieves substantially improved zero-shot performance across a
variety of downstream tasks, often competitive with much larger models, without
sacrificing its core language modeling abilities.",None,-1
2613e5fa-e5d5-43a2-b21f-f2f0f028e773,TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective,0.649053,"Vision Transformers (ViTs) have demonstrated powerful representation ability
in various visual tasks thanks to their intrinsic data-hungry nature. However,
we unexpectedly find that ViTs perform vulnerably when applied to face
recognition (FR) scenarios with extremely large datasets. We investigate the
reasons for this phenomenon and discover that the existing data augmentation
approach and hard sample mining strategy are incompatible with ViTs-based FR
backbone due to the lack of tailored consideration on preserving face
structural information and leveraging each local token information. To remedy
these problems, this paper proposes a superior FR model called TransFace, which
employs a patch-level data augmentation strategy named DPAP and a hard sample
mining strategy named EHSM. Specially, DPAP randomly perturbs the amplitude
information of dominant patches to expand sample diversity, which effectively
alleviates the overfitting problem in ViTs. EHSM utilizes the information
entropy in the local tokens to dynamically adjust the importance weight of easy
and hard samples during training, leading to a more stable prediction.
Experiments on several benchmarks demonstrate the superiority of our TransFace.
Code and models are available at https://github.com/DanJun6737/TransFace.",None,-1
0d36a969-0075-4a52-8a4f-bc4d708aec6b,From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization,0.307679,"Key Point Analysis (KPA) has been recently proposed for deriving fine-grained
insights from collections of textual comments. KPA extracts the main points in
the data as a list of concise sentences or phrases, termed key points, and
quantifies their prevalence. While key points are more expressive than word
clouds and key phrases, making sense of a long, flat list of key points, which
often express related ideas in varying levels of granularity, may still be
challenging. To address this limitation of KPA, we introduce the task of
organizing a given set of key points into a hierarchy, according to their
specificity. Such hierarchies may be viewed as a novel type of Textual
Entailment Graph. We develop ThinkP, a high quality benchmark dataset of key
point hierarchies for business and product reviews, obtained by consolidating
multiple annotations. We compare different methods for predicting pairwise
relations between key points, and for inferring a hierarchy from these pairwise
predictions. In particular, for the task of computing pairwise key point
relations, we achieve significant gains over existing strong baselines by
applying directional distributional similarity methods to a novel
distributional representation of key points, and further boost performance via
weak supervision.",None,-1
9745b14b-17a6-4e46-980f-5e55b39899a9,AutoRecon: Automated 3D Object Discovery and Reconstruction,0.920092,"A fully automated object reconstruction pipeline is crucial for digital
content creation. While the area of 3D reconstruction has witnessed profound
developments, the removal of background to obtain a clean object model still
relies on different forms of manual labor, such as bounding box labeling, mask
annotations, and mesh manipulations. In this paper, we propose a novel
framework named AutoRecon for the automated discovery and reconstruction of an
object from multi-view images. We demonstrate that foreground objects can be
robustly located and segmented from SfM point clouds by leveraging
self-supervised 2D vision transformer features. Then, we reconstruct decomposed
neural scene representations with dense supervision provided by the decomposed
point clouds, resulting in accurate object reconstruction and segmentation.
Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the
effectiveness and robustness of AutoRecon.",None,-1
638f9bb1-0c44-4eae-bb47-4feb6bef9915,Ancient Chinese Word Segmentation and Part-of-Speech Tagging Using Distant Supervision,0.871603,"Ancient Chinese word segmentation (WSG) and part-of-speech tagging (POS) are
important to study ancient Chinese, but the amount of ancient Chinese WSG and
POS tagging data is still rare. In this paper, we propose a novel augmentation
method of ancient Chinese WSG and POS tagging data using distant supervision
over parallel corpus. However, there are still mislabeled and unlabeled ancient
Chinese words inevitably in distant supervision. To address this problem, we
take advantage of the memorization effects of deep neural networks and a small
amount of annotated data to get a model with much knowledge and a little noise,
and then we use this model to relabel the ancient Chinese sentences in parallel
corpus. Experiments show that the model trained over the relabeled data
outperforms the model trained over the data generated from distant supervision
and the annotated data. Our code is available at
https://github.com/farlit/ACDS.",None,-1
1ba520ef-8482-4c2c-886d-c3603f998cb6,Humanoid Agents: Platform for Simulating Human-like Generative Agents,0.697962,"Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents",None,-1
66ba3158-a9d7-44f6-94ed-5fa7ea4c50a6,How to Choose Pretrained Handwriting Recognition Models for Single Writer Fine-Tuning,0.379607,"Recent advancements in Deep Learning-based Handwritten Text Recognition (HTR)
have led to models with remarkable performance on both modern and historical
manuscripts in large benchmark datasets. Nonetheless, those models struggle to
obtain the same performance when applied to manuscripts with peculiar
characteristics, such as language, paper support, ink, and author handwriting.
This issue is very relevant for valuable but small collections of documents
preserved in historical archives, for which obtaining sufficient annotated
training data is costly or, in some cases, unfeasible. To overcome this
challenge, a possible solution is to pretrain HTR models on large datasets and
then fine-tune them on small single-author collections. In this paper, we take
into account large, real benchmark datasets and synthetic ones obtained with a
styled Handwritten Text Generation model. Through extensive experimental
analysis, also considering the amount of fine-tuning lines, we give a
quantitative indication of the most relevant characteristics of such data for
obtaining an HTR model able to effectively transcribe manuscripts in small
collections with as little as five real fine-tuning lines.",None,-1
fb2a96cb-8777-4d22-9aff-8f573991d40c,UGAE: A Novel Approach to Non-exponential Discounting,0.0291728,"The discounting mechanism in Reinforcement Learning determines the relative
importance of future and present rewards. While exponential discounting is
widely used in practice, non-exponential discounting methods that align with
human behavior are often desirable for creating human-like agents. However,
non-exponential discounting methods cannot be directly applied in modern
on-policy actor-critic algorithms. To address this issue, we propose Universal
Generalized Advantage Estimation (UGAE), which allows for the computation of
GAE advantage values with arbitrary discounting. Additionally, we introduce
Beta-weighted discounting, a continuous interpolation between exponential and
hyperbolic discounting, to increase flexibility in choosing a discounting
method. To showcase the utility of UGAE, we provide an analysis of the
properties of various discounting methods. We also show experimentally that
agents with non-exponential discounting trained via UGAE outperform variants
trained with Monte Carlo advantage estimation. Through analysis of various
discounting methods and experiments, we demonstrate the superior performance of
UGAE with Beta-weighted discounting over the Monte Carlo baseline on standard
RL benchmarks. UGAE is simple and easily integrated into any advantage-based
algorithm as a replacement for the standard recursive GAE.",None,-1
e6091415-94d8-4dab-8ed9-d075b3a6dbcb,T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation,0.410935,"Warning: This paper contains several contents that may be toxic, harmful, or
offensive.
  In the last few years, text-to-image generative models have gained remarkable
success in generating images with unprecedented quality accompanied by a
breakthrough of inference speed. Despite their rapid progress, human biases
that manifest in the training examples, particularly with regard to common
stereotypical biases, like gender and skin tone, still have been found in these
generative models. In this work, we seek to measure more complex human biases
exist in the task of text-to-image generations. Inspired by the well-known
Implicit Association Test (IAT) from social psychology, we propose a novel
Text-to-Image Association Test (T2IAT) framework that quantifies the implicit
stereotypes between concepts and valence, and those in the images. We replicate
the previously documented bias tests on generative models, including morally
neutral tests on flowers and insects as well as demographic stereotypical tests
on diverse social attributes. The results of these experiments demonstrate the
presence of complex stereotypical behaviors in image generations.",None,-1
09fb1d19-8400-4056-b228-fdbcbd0b59ad,Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?,0.807754,"Large Language Models (LLMs) are advancing at a rapid pace, with significant
improvements at natural language processing and coding tasks. Yet, their
ability to work with formal languages representing data, specifically within
the realm of knowledge graph engineering, remains under-investigated. To
evaluate the proficiency of various LLMs, we created a set of five tasks that
probe their ability to parse, understand, analyze, and create knowledge graphs
serialized in Turtle syntax. These tasks, each embodying distinct degrees of
complexity and being able to scale with the size of the problem, have been
integrated into our automated evaluation system, the LLM-KG-Bench. The
evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,
Claude 1.3, and Claude 2.0, as well as two freely accessible offline models,
GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth
understanding of the strengths and shortcomings of LLMs in relation to their
application within RDF knowledge graph engineering workflows utilizing Turtle
representation. While our findings show that the latest commercial models
outperform their forerunners in terms of proficiency with the Turtle language,
they also reveal an apparent weakness. These models fall short when it comes to
adhering strictly to the output formatting constraints, a crucial requirement
in this context.",None,-1
f3b3efca-d816-458c-887d-3f4c3c9bd67d,A Data Fusion Framework for Multi-Domain Morality Learning,0.774198,"Language models can be trained to recognize the moral sentiment of text,
creating new opportunities to study the role of morality in human life. As
interest in language and morality has grown, several ground truth datasets with
moral annotations have been released. However, these datasets vary in the
method of data collection, domain, topics, instructions for annotators, etc.
Simply aggregating such heterogeneous datasets during training can yield models
that fail to generalize well. We describe a data fusion framework for training
on multiple heterogeneous datasets that improve performance and
generalizability. The model uses domain adversarial training to align the
datasets in feature space and a weighted loss function to deal with label
shift. We show that the proposed framework achieves state-of-the-art
performance in different datasets compared to prior works in morality
inference.",None,-1
64312b62-af73-4426-943f-5a55edc62415,Multimodal Machine Unlearning,0.731273,"Machine Unlearning is the process of removing specific training data samples
and their corresponding effects from an already trained model. It has
significant practical benefits, such as purging private, inaccurate, or
outdated information from trained models without the need for complete
re-training. Unlearning within a multimodal setting presents unique challenges
due to the intrinsic dependencies between different data modalities and the
expensive cost of training on large multimodal datasets and architectures.
Current approaches to machine unlearning have not fully addressed these
challenges. To bridge this gap, we introduce MMUL, a machine unlearning
approach specifically designed for multimodal data and models. MMUL formulates
the multimodal unlearning task by focusing on three key properties: (a):
modality decoupling, which effectively decouples the association between
individual unimodal data points within multimodal inputs marked for deletion,
rendering them as unrelated data points within the model's context, (b):
unimodal knowledge retention, which retains the unimodal representation
capability of the model post-unlearning, and (c): multimodal knowledge
retention, which retains the multimodal representation capability of the model
post-unlearning. MMUL is efficient to train and is not constrained by the
requirement of using a strongly convex loss. Experiments on two multimodal
models and four multimodal benchmark datasets, including vision-language and
graph-language datasets, show that MMUL outperforms existing baselines, gaining
an average improvement of +17.6 points against the best-performing unimodal
baseline in distinguishing between deleted and remaining data. In addition,
MMUL can largely maintain pre-existing knowledge of the original model post
unlearning, with a performance gap of only 0.3 points compared to retraining a
new model from scratch.",None,-1
054509f3-8821-427e-8bc0-bc428d4455dc,Geometric Ultrasound Localization Microscopy,0.718299,"Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.",None,-1
e359c51d-c515-462c-9fa0-0d3e867573f3,Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports,0.907538,"With the increasing number of clinical trial reports generated every day, it
is becoming hard to keep up with novel discoveries that inform evidence-based
healthcare recommendations. To help automate this process and assist medical
experts, NLP solutions are being developed. This motivated the SemEval-2023
Task 7, where the goal was to develop an NLP system for two tasks: evidence
retrieval and natural language inference from clinical trial data. In this
paper, we describe our two developed systems. The first one is a pipeline
system that models the two tasks separately, while the second one is a joint
system that learns the two tasks simultaneously with a shared representation
and a multi-task learning approach. The final system combines their outputs in
an ensemble system. We formalize the models, present their characteristics and
challenges, and provide an analysis of achieved results. Our system ranked 3rd
out of 40 participants with a final submission.",None,-1
7d3dcc83-f134-4095-8083-de722a581753,HeGeL: A Novel Dataset for Geo-Location from Hebrew Text,0.620955,"The task of textual geolocation - retrieving the coordinates of a place based
on a free-form language description - calls for not only grounding but also
natural language understanding and geospatial reasoning. Even though there are
quite a few datasets in English used for geolocation, they are currently based
on open-source data (Wikipedia and Twitter), where the location of the
described place is mostly implicit, such that the location retrieval resolution
is limited. Furthermore, there are no datasets available for addressing the
problem of textual geolocation in morphologically rich and resource-poor
languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location
(HeGeL) corpus, designed to collect literal place descriptions and analyze
lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place
descriptions of various place types in three cities in Israel. Qualitative and
empirical analysis show that the data exhibits abundant use of geospatial
reasoning and requires a novel environmental representation.",None,-1
fb71780a-4521-45ae-812a-3db016eff8a3,A Strategy-Oriented Bayesian Soft Actor-Critic Model,0.163176,"Adopting reasonable strategies is challenging but crucial for an intelligent
agent with limited resources working in hazardous, unstructured, and dynamic
environments to improve the system's utility, decrease the overall cost, and
increase mission success probability. This paper proposes a novel hierarchical
strategy decomposition approach based on the Bayesian chain rule to separate an
intricate policy into several simple sub-policies and organize their
relationships as Bayesian strategy networks (BSN). We integrate this approach
into the state-of-the-art DRL method -- soft actor-critic (SAC) and build the
corresponding Bayesian soft actor-critic (BSAC) model by organizing several
sub-policies as a joint policy. We compare the proposed BSAC method with the
SAC and other state-of-the-art approaches such as TD3, DDPG, and PPO on the
standard continuous control benchmarks -- Hopper-v2, Walker2d-v2, and
Humanoid-v2 -- in MuJoCo with the OpenAI Gym environment. The results
demonstrate that the promising potential of the BSAC method significantly
improves training efficiency.",None,-1
b24da6c0-62fa-4227-8c82-8aec08efcd79,MemeFier: Dual-stage Modality Fusion for Image Meme Classification,0.34106,"Hate speech is a societal problem that has significantly grown through the
Internet. New forms of digital content such as image memes have given rise to
spread of hate using multimodal means, being far more difficult to analyse and
detect compared to the unimodal case. Accurate automatic processing, analysis
and understanding of this kind of content will facilitate the endeavor of
hindering hate speech proliferation through the digital world. To this end, we
propose MemeFier, a deep learning-based architecture for fine-grained
classification of Internet image memes, utilizing a dual-stage modality fusion
module. The first fusion stage produces feature vectors containing modality
alignment information that captures non-trivial connections between the text
and image of a meme. The second fusion stage leverages the power of a
Transformer encoder to learn inter-modality correlations at the token level and
yield an informative representation. Additionally, we consider external
knowledge as an additional input, and background image caption supervision as a
regularizing component. Extensive experiments on three widely adopted
benchmarks, i.e., Facebook Hateful Memes, Memotion7k and MultiOFF, indicate
that our approach competes and in some cases surpasses state-of-the-art. Our
code is available on https://github.com/ckoutlis/memefier.",None,-1
042363d0-e7e4-44fd-ae11-dad988b18e3d,Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction,0.172451,"Real-time, accurate prediction of human steering behaviors has wide
applications, from developing intelligent traffic systems to deploying
autonomous driving systems in both real and simulated worlds. In this paper, we
present ContextVAE, a context-aware approach for multi-modal vehicle trajectory
prediction. Built upon the backbone architecture of a timewise variational
autoencoder, ContextVAE observation encoding employs a dual attention mechanism
that accounts for the environmental context and the dynamic agents' states, in
a unified way. By utilizing features extracted from semantic maps during agent
state encoding, our approach takes into account both the social features
exhibited by agents on the scene and the physical environment constraints to
generate map-compliant and socially-aware trajectories. We perform extensive
testing on the nuScenes prediction challenge, Lyft Level 5 dataset and Waymo
Open Motion Dataset to show the effectiveness of our approach and its
state-of-the-art performance. In all tested datasets, ContextVAE models are
fast to train and provide high-quality multi-modal predictions in real-time.
Our code is available at: https://github.com/xupei0610/ContextVAE.",None,-1
abf5c4ee-f5fa-403c-9892-c505cfb15f94,"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",0.973256,"Humor is a central aspect of human communication that has not been solved for
artificial agents so far. Large language models (LLMs) are increasingly able to
capture implicit and contextual information. Especially, OpenAI's ChatGPT
recently gained immense public attention. The GPT3-based model almost seems to
communicate on a human level and can even tell jokes. Humor is an essential
component of human communication. But is ChatGPT really funny? We put ChatGPT's
sense of humor to the test. In a series of exploratory experiments around
jokes, i.e., generation, explanation, and detection, we seek to understand
ChatGPT's capability to grasp and reproduce human humor. Since the model itself
is not accessible, we applied prompt-based experiments. Our empirical evidence
indicates that jokes are not hard-coded but mostly also not newly generated by
the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system
accurately explains valid jokes but also comes up with fictional explanations
for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the
classification of jokes. ChatGPT has not solved computational humor yet but it
can be a big leap toward ""funny"" machines.",None,-1
eedbedfb-0336-4df6-8f3f-65e917adaf9f,Flows: Building Blocks of Reasoning and Collaborating AI,0.807317,"Recent advances in artificial intelligence (AI) have produced highly capable
and controllable systems. This creates unprecedented opportunities for
structured reasoning as well as collaboration among multiple AI systems and
humans. To fully realize this potential, it is essential to develop a
principled way of designing and studying such structured interactions. For this
purpose, we introduce the conceptual framework Flows. Flows are self-contained
building blocks of computation, with an isolated state, communicating through a
standardized message-based interface. This modular design simplifies the
process of creating Flows by allowing them to be recursively composed into
arbitrarily nested interactions and is inherently concurrency-friendly.
Crucially, any interaction can be implemented using this framework, including
prior work on AI-AI and human-AI interactions, prompt engineering schemes, and
tool augmentation. We demonstrate the potential of Flows on competitive coding,
a challenging task on which even GPT-4 struggles. Our results suggest that
structured reasoning and collaboration substantially improve generalization,
with AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in
terms of solve rate. To support rapid and rigorous research, we introduce the
aiFlows library embodying Flows. The aiFlows library is available at
https://github.com/epfl-dlab/aiflows. Data and Flows for reproducing our
experiments are available at https://github.com/epfl-dlab/cc_flows.",None,-1
2730fd68-6de0-41ff-81da-3eed8c364417,Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations,0.314135,"Multimodal embeddings aim to enrich the semantic information in neural
representations of language compared to text-only models. While different
embeddings exhibit different applicability and performance on downstream tasks,
little is known about the systematic representation differences attributed to
the visual modality. Our paper compares word embeddings from three
vision-and-language models (CLIP, OpenCLIP and Multilingual CLIP) and three
text-only models, with static (FastText) as well as contextual representations
(multilingual BERT; XLM-RoBERTa). This is the first large-scale study of the
effect of visual grounding on language representations, including 46 semantic
parameters. We identify meaning properties and relations that characterize
words whose embeddings are most affected by the inclusion of visual modality in
the training data; that is, points where visual grounding turns out most
important. We find that the effect of visual modality correlates most with
denotational semantic properties related to concreteness, but is also detected
for several specific semantic classes, as well as for valence, a
sentiment-related connotational property of linguistic expressions.",None,-1
67a26473-05ec-4183-b191-cc4fca8c9c6d,DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition,0.989553,"Implicit Discourse Relation Recognition (IDRR) is a sophisticated and
challenging task to recognize the discourse relations between the arguments
with the absence of discourse connectives. The sense labels for each discourse
relation follow a hierarchical classification scheme in the annotation process
(Prasad et al., 2008), forming a hierarchy structure. Most existing works do
not well incorporate the hierarchy structure but focus on the syntax features
and the prior knowledge of connectives in the manner of pure text
classification. We argue that it is more effective to predict the paths inside
the hierarchical tree (e.g., ""Comparison -> Contrast -> however"") rather than
flat labels (e.g., Contrast) or connectives (e.g., however). We propose a
prompt-based path prediction method to utilize the interactive information and
intrinsic senses among the hierarchy in IDRR. This is the first work that
injects such structure information into pre-trained language models via prompt
tuning, and the performance of our solution shows significant and consistent
improvement against competitive baselines.",None,-1
24b4fab7-84d2-4e22-9d2c-38d0eb377e6b,Text-to-Image Diffusion Models are Zero-Shot Classifiers,0.60156,"The excellent generative capabilities of text-to-image diffusion models
suggest they learn informative representations of image-text data. However,
what knowledge their representations capture is not fully understood, and they
have not been thoroughly explored on downstream tasks. We investigate diffusion
models by proposing a method for evaluating them as zero-shot classifiers. The
key idea is using a diffusion model's ability to denoise a noised image given a
text description of a label as a proxy for that label's likelihood. We apply
our method to Stable Diffusion and Imagen, using it to probe fine-grained
aspects of the models' knowledge and comparing them with CLIP's zero-shot
abilities. They perform competitively with CLIP on a wide range of zero-shot
image classification datasets. Additionally, they achieve state-of-the-art
results on shape/texture bias tests and can successfully perform attribute
binding while CLIP cannot. Although generative pre-training is prevalent in
NLP, visual foundation models often use other methods such as contrastive
learning. Based on our findings, we argue that generative pre-training should
be explored as a compelling alternative for vision-language tasks.",None,-1
ff502610-eaf8-43a4-bc39-33c8a0dab485,Cooperative Open-ended Learning Framework for Zero-shot Coordination,0.735513,"Zero-shot coordination in cooperative artificial intelligence (AI) remains a
significant challenge, which means effectively coordinating with a wide range
of unseen partners. Previous algorithms have attempted to address this
challenge by optimizing fixed objectives within a population to improve
strategy or behaviour diversity. However, these approaches can result in a loss
of learning and an inability to cooperate with certain strategies within the
population, known as cooperative incompatibility. To address this issue, we
propose the Cooperative Open-ended LEarning (COLE) framework, which constructs
open-ended objectives in cooperative games with two players from the
perspective of graph theory to assess and identify the cooperative ability of
each strategy. We further specify the framework and propose a practical
algorithm that leverages knowledge from game theory and graph theory.
Furthermore, an analysis of the learning process of the algorithm shows that it
can efficiently overcome cooperative incompatibility. The experimental results
in the Overcooked game environment demonstrate that our method outperforms
current state-of-the-art methods when coordinating with different-level
partners. Our demo is available at https://sites.google.com/view/cole-2023.",None,-1
ea5e0f18-00c8-4b3d-81e2-3b433f1fd26a,Spatiotemporal Deformation Perception for Fisheye Video Rectification,0.340046,"Although the distortion correction of fisheye images has been extensively
studied, the correction of fisheye videos is still an elusive challenge. For
different frames of the fisheye video, the existing image correction methods
ignore the correlation of sequences, resulting in temporal jitter in the
corrected video. To solve this problem, we propose a temporal weighting scheme
to get a plausible global optical flow, which mitigates the jitter effect by
progressively reducing the weight of frames. Subsequently, we observe that the
inter-frame optical flow of the video is facilitated to perceive the local
spatial deformation of the fisheye video. Therefore, we derive the spatial
deformation through the flows of fisheye and distorted-free videos, thereby
enhancing the local accuracy of the predicted result. However, the independent
correction for each frame disrupts the temporal correlation. Due to the
property of fisheye video, a distorted moving object may be able to find its
distorted-free pattern at another moment. To this end, a temporal deformation
aggregator is designed to reconstruct the deformation correlation between
frames and provide a reliable global feature. Our method achieves an end-to-end
correction and demonstrates superiority in correction quality and stability
compared with the SOTA correction methods.",None,-1
b7cb28c8-e240-4233-a1ef-c3485fed2970,IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for multilayer summarization of clinical conversations?,0.164741,"Clinical conversation summarization has become an important application of
Natural language Processing. In this work, we intend to analyze summarization
model ensembling approaches, that can be utilized to improve the overall
accuracy of the generated medical report called chart note. The work starts
with a single summarization model creating the baseline. Then leads to an
ensemble of summarization models trained on a separate section of the chart
note. This leads to the final approach of passing the generated results to
another summarization model in a multi-layer/stage fashion for better coherency
of the generated text. Our results indicate that although an ensemble of models
specialized in each section produces better results, the multi-layer/stage
approach does not improve accuracy. The code for the above paper is available
at https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git",None,-1
3789d65d-3df6-4ff6-bc2e-0c75f013798a,Provable Data Subset Selection For Efficient Neural Network Training,0.166961,"Radial basis function neural networks (\emph{RBFNN}) are {well-known} for
their capability to approximate any continuous function on a closed bounded set
with arbitrary precision given enough hidden neurons. In this paper, we
introduce the first algorithm to construct coresets for \emph{RBFNNs}, i.e.,
small weighted subsets that approximate the loss of the input data on any
radial basis function network and thus approximate any function defined by an
\emph{RBFNN} on the larger input data. In particular, we construct coresets for
radial basis and Laplacian loss functions. We then use our coresets to obtain a
provable data subset selection algorithm for training deep neural networks.
Since our coresets approximate every function, they also approximate the
gradient of each weight in a neural network, which is a particular function on
the input. We then perform empirical evaluations on function approximation and
dataset subset selection on popular network architectures and data sets,
demonstrating the efficacy and accuracy of our coreset construction.",None,-1
1341bf11-5c77-462d-aa17-c1e473a091bb,SPEED: Speculative Pipelined Execution for Efficient Decoding,0.4657,"Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.",None,-1
90089784-0c0b-4a1d-9980-70ee10d8cf0f,Continual Source-Free Unsupervised Domain Adaptation,0.33108,"Existing Source-free Unsupervised Domain Adaptation (SUDA) approaches
inherently exhibit catastrophic forgetting. Typically, models trained on a
labeled source domain and adapted to unlabeled target data improve performance
on the target while dropping performance on the source, which is not available
during adaptation. In this study, our goal is to cope with the challenging
problem of SUDA in a continual learning setting, i.e., adapting to the
target(s) with varying distributional shifts while maintaining performance on
the source. The proposed framework consists of two main stages: i) a SUDA model
yielding cleaner target labels -- favoring good performance on target, and ii)
a novel method for synthesizing class-conditioned source-style images by
leveraging only the source model and pseudo-labeled target data as a prior. An
extensive pool of experiments on major benchmarks, e.g., PACS, Visda-C, and
DomainNet demonstrates that the proposed Continual SUDA (C-SUDA) framework
enables preserving satisfactory performance on the source domain without
exploiting the source data at all.",None,-1
2ee6c656-8717-4888-b2be-e6f75a26ee5b,An Extensible Multimodal Multi-task Object Dataset with Materials,0.0397434,"We present EMMa, an Extensible, Multimodal dataset of Amazon product listings
that contains rich Material annotations. It contains more than 2.8 million
objects, each with image(s), listing text, mass, price, product ratings, and
position in Amazon's product-category taxonomy. We also design a comprehensive
taxonomy of 182 physical materials (e.g., Plastic $\rightarrow$ Thermoplastic
$\rightarrow$ Acrylic). Objects are annotated with one or more materials from
this taxonomy. With the numerous attributes available for each object, we
develop a Smart Labeling framework to quickly add new binary labels to all
objects with very little manual labeling effort, making the dataset extensible.
Each object attribute in our dataset can be included in either the model inputs
or outputs, leading to combinatorial possibilities in task configurations. For
example, we can train a model to predict the object category from the listing
text, or the mass and price from the product listing image. EMMa offers a new
benchmark for multi-task learning in computer vision and NLP, and allows
practitioners to efficiently add new tasks and object attributes at scale.",None,-1
1d19662d-3923-4fcc-9510-1052d3411abd,Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios,0.346223,"The abilities to understand the social interaction behaviors between a
vehicle and its surroundings while predicting its trajectory in an urban
environment are critical for road safety in autonomous driving. Social
interactions are hard to explain because of their uncertainty. In recent years,
neural network-based methods have been widely used for trajectory prediction
and have been shown to outperform hand-crafted methods. However, these methods
suffer from their lack of interpretability. In order to overcome this
limitation, we combine the interpretability of a discrete choice model with the
high accuracy of a neural network-based model for the task of vehicle
trajectory prediction in an interactive environment. We implement and evaluate
our model using the INTERACTION dataset and demonstrate the effectiveness of
our proposed architecture to explain its predictions without compromising the
accuracy.",None,-1
f830e094-7c7d-443d-a4b6-d3fd73d07353,Methodology for generating synthetic labeled datasets for visual container inspection,0.235579,"Nowadays, containerized freight transport is one of the most important
transportation systems that is undergoing an automation process due to the Deep
Learning success. However, it suffers from a lack of annotated data in order to
incorporate state-of-the-art neural network models to its systems. In this
paper we present an innovative methodology to generate a realistic, varied,
balanced, and labelled dataset for visual inspection task of containers in a
dock environment. In addition, we validate this methodology with multiple
visual tasks recurrently found in the state of the art. We prove that the
generated synthetic labelled dataset allows to train a deep neural network that
can be used in a real world scenario. On the other side, using this methodology
we provide the first open synthetic labelled dataset called SeaFront available
in: https://datasets.vicomtech.org/di21-seafront/readme.txt.",None,-1
5242f5eb-a48a-41e0-bb9f-2ebca9463424,Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge,0.487077,"In this paper, we study the problem of knowledge-intensive text-to-SQL, in
which domain knowledge is necessary to parse expert questions into SQL queries
over domain-specific tables. We formalize this scenario by building a new
Chinese benchmark KnowSQL consisting of domain-specific questions covering
various domains. We then address this problem by presenting formulaic
knowledge, rather than by annotating additional data examples. More concretely,
we construct a formulaic knowledge bank as a domain knowledge base and propose
a framework (ReGrouP) to leverage this formulaic knowledge during parsing.
Experiments using ReGrouP demonstrate a significant 28.2% improvement overall
on KnowSQL.",None,-1
e26c09d0-8079-47eb-b1eb-d3637ba1d8c7,Ultra Sharp : Study of Single Image Super Resolution using Residual Dense Network,0.363748,"For years, Single Image Super Resolution (SISR) has been an interesting and
ill-posed problem in computer vision. The traditional super-resolution (SR)
imaging approaches involve interpolation, reconstruction, and learning-based
methods. Interpolation methods are fast and uncomplicated to compute, but they
are not so accurate and reliable. Reconstruction-based methods are better
compared with interpolation methods, but they are time-consuming and the
quality degrades as the scaling increases. Even though learning-based methods
like Markov random chains are far better than all the previous ones, they are
unable to match the performance of deep learning models for SISR. This study
examines the Residual Dense Networks architecture proposed by Yhang et al. [17]
and analyzes the importance of its components. By leveraging hierarchical
features from original low-resolution (LR) images, this architecture achieves
superior performance, with a network structure comprising four main blocks,
including the residual dense block (RDB) as the core. Through investigations of
each block and analyses using various loss metrics, the study evaluates the
effectiveness of the architecture and compares it to other state-of-the-art
models that differ in both architecture and components.",None,-1
076ff4eb-a896-468a-a582-4a553875f390,Predicting Hateful Discussions on Reddit using Graph Transformer Networks and Communal Context,0.655788,"We propose a system to predict harmful discussions on social media platforms.
Our solution uses contextual deep language models and proposes the novel idea
of integrating state-of-the-art Graph Transformer Networks to analyze all
conversations that follow an initial post. This framework also supports
adapting to future comments as the conversation unfolds. In addition, we study
whether a community-specific analysis of hate speech leads to more effective
detection of hateful discussions. We evaluate our approach on 333,487 Reddit
discussions from various communities. We find that community-specific modeling
improves performance two-fold and that models which capture wider-discussion
context improve accuracy by 28\% (35\% for the most hateful content) compared
to limited context models.",None,-1
2150016b-ce0a-43d6-bea7-7b81a4562bcc,Theory of Mind for Multi-Agent Collaboration via Large Language Models,0.855026,"While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.",None,-1
39c02478-24c2-4b79-87c1-d0036af71e8a,Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark,0.912546,"With the increasing interest and rapid development of methods for Ultra-High
Resolution (UHR) segmentation, a large-scale benchmark covering a wide range of
scenes with full fine-grained dense annotations is urgently needed to
facilitate the field. To this end, the URUR dataset is introduced, in the
meaning of Ultra-High Resolution dataset with Ultra-Rich Context. As the name
suggests, URUR contains amounts of images with high enough resolution (3,008
images of size 5,120x5,120), a wide range of complex scenes (from 63 cities),
rich-enough context (1 million instances with 8 categories) and fine-grained
annotations (about 80 billion manually annotated pixels), which is far superior
to all the existing UHR datasets including DeepGlobe, Inria Aerial, UDD, etc..
Moreover, we also propose WSDNet, a more efficient and effective framework for
UHR segmentation especially with ultra-rich context. Specifically, multi-level
Discrete Wavelet Transform (DWT) is naturally integrated to release computation
burden while preserve more spatial details, along with a Wavelet Smooth Loss
(WSL) to reconstruct original structured context and texture with a smooth
constrain. Experiments on several UHR datasets demonstrate its state-of-the-art
performance. The dataset is available at https://github.com/jankyee/URUR.",None,-1
4b101753-ce22-4a12-a17c-d3bb8dfc7548,Wildfire Smoke Detection with Computer Vision,0.258688,"Wildfires are becoming more frequent and their effects more devastating every
day. Climate change has directly and indirectly affected the occurrence of
these, as well as social phenomena have increased the vulnerability of people.
Consequently, and given the inevitable occurrence of these, it is important to
have early warning systems that allow a timely and effective response.
Artificial intelligence, machine learning and Computer Vision offer an
effective and achievable alternative for opportune detection of wildfires and
thus reduce the risk of disasters. YOLOv7 offers a simple, fast, and efficient
algorithm for training object detection models which can be used in early
detection of smoke columns in the initial stage wildfires. The developed model
showed promising results, achieving a score of 0.74 in the F1 curve when the
confidence level is 0.298, that is, a higher score at lower confidence levels
was obtained. This means when the conditions are favorable for false positives.
The metrics demonstrates the resilience and effectiveness of the model in
detecting smoke columns.",None,-1
4398f376-e5bb-429b-bdb2-66f3cb8f104d,Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs,0.0286899,"A moderately detailed consideration of interactive LLMs as cognitive systems
is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama,
etc.. Cognitive strengths of these systems are reviewed, and then careful
attention is paid to the substantial differences between the sort of cognitive
system these LLMs are, and the sort of cognitive systems human beings are. It
is found that many of the practical weaknesses of these AI systems can be tied
specifically to lacks in the basic cognitive architectures according to which
these systems are built. It is argued that incremental improvement of such LLMs
is not a viable approach to working toward human-level AGI, in practical terms
given realizable amounts of compute resources. This does not imply there is
nothing to learn about human-level AGI from studying and experimenting with
LLMs, nor that LLMs cannot form significant parts of human-level AGI
architectures that also incorporate other ideas. Social and ethical matters
regarding LLMs are very briefly touched from this perspective, which implies
that while care should be taken regarding misinformation and other issues, and
economic upheavals will need their own social remedies based on their
unpredictable course as with any powerfully impactful technology, overall the
sort of policy needed as regards modern LLMs is quite different than would be
the case if a more credible approximation to human-level AGI were at hand.",None,-1
a7c52823-4a78-496e-881e-0c2235639f23,Knowledge Graph Embedding: An Overview,0.272004,"Many mathematical models have been leveraged to design embeddings for
representing Knowledge Graph (KG) entities and relations for link prediction
and many downstream tasks. These mathematically-inspired models are not only
highly scalable for inference in large KGs, but also have many explainable
advantages in modeling different relation patterns that can be validated
through both formal proofs and empirical results. In this paper, we make a
comprehensive overview of the current state of research in KG completion. In
particular, we focus on two main branches of KG embedding (KGE) design: 1)
distance-based methods and 2) semantic matching-based methods. We discover the
connections between recently proposed models and present an underlying trend
that might help researchers invent novel and more effective models. Next, we
delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D
affine operations, respectively. They encompass a broad spectrum of techniques
including distance-based and semantic-based methods. We will also discuss an
emerging approach for KG completion which leverages pre-trained language models
(PLMs) and textual descriptions of entities and relations and offer insights
into the integration of KGE embedding methods with PLMs for KG completion.",None,-1
b3367868-3fa2-4c79-afa6-4ccb1723077b,Investigation of Architectures and Receptive Fields for Appearance-based Gaze Estimation,0.0898404,"With the rapid development of deep learning technology in the past decade,
appearance-based gaze estimation has attracted great attention from both
computer vision and human-computer interaction research communities.
Fascinating methods were proposed with variant mechanisms including soft
attention, hard attention, two-eye asymmetry, feature disentanglement, rotation
consistency, and contrastive learning. Most of these methods take the
single-face or multi-region as input, yet the basic architecture of gaze
estimation has not been fully explored. In this paper, we reveal the fact that
tuning a few simple parameters of a ResNet architecture can outperform most of
the existing state-of-the-art methods for the gaze estimation task on three
popular datasets. With our extensive experiments, we conclude that the stride
number, input image resolution, and multi-region architecture are critical for
the gaze estimation performance while their effectiveness dependent on the
quality of the input face image. We obtain the state-of-the-art performances on
three datasets with 3.64 on ETH-XGaze, 4.50 on MPIIFaceGaze, and 9.13 on
Gaze360 degrees gaze estimation error by taking ResNet-50 as the backbone.",None,-1
f97f9e4a-443c-4452-b848-1da0e7800b0b,CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement,0.731583,"Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN's potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.",None,-1
4591dfd1-d604-47ed-be27-71a42c7fcdfa,Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning,0.0999024,"Recent advanced methods in Natural Language Understanding for Task-oriented
Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a
large amount of annotated data to achieve competitive performance. In reality,
token-level annotations (slot labels) are time-consuming and difficult to
acquire. In this work, we study the Slot Induction (SI) task whose objective is
to induce slot boundaries without explicit knowledge of token-level slot
annotations. We propose leveraging Unsupervised Pre-trained Language Model
(PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised
semantic knowledge extracted from PLM, and (2) additional sentence-level intent
label signals available from TOD. Our approach is shown to be effective in SI
task and capable of bridging the gaps with token-level supervised models on two
NLU benchmark datasets. When generalized to emerging intents, our SI objectives
also provide enhanced slot label representations, leading to improved
performance on the Slot Filling tasks.",None,-1
27eb526a-3904-42dd-b62d-15c154153e6d,CSED: A Chinese Semantic Error Diagnosis Corpus,0.876224,"Recently, much Chinese text error correction work has focused on Chinese
Spelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In
contrast, little attention has been paid to the complicated problem of Chinese
Semantic Error Diagnosis (CSED), which lacks relevant datasets. The study of
semantic errors is important because they are very common and may lead to
syntactic irregularities or even problems of comprehension. To investigate
this, we build the CSED corpus, which includes two datasets. The one is for the
CSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C)
task. Our annotation guarantees high-quality data through quality assurance
mechanisms. Our experiments show that powerful pre-trained models perform
poorly on this corpus. We also find that the CSED task is challenging, as
evidenced by the fact that even humans receive a low score. This paper proposes
syntax-aware models to specifically adapt to the CSED task. The experimental
results show that the introduction of the syntax-aware approach is meaningful.",None,-1
516aa7d3-23cd-4b9f-8c5a-4879338323d0,Skip-Attention: Improving Vision Transformers by Paying Less Attention,0.21812,"This work aims to improve the efficiency of vision transformers (ViT). While
ViTs use computationally expensive self-attention operations in every layer, we
identify that these operations are highly correlated across layers -- a key
redundancy that causes unnecessary computations. Based on this observation, we
propose SkipAt, a method to reuse self-attention computation from preceding
layers to approximate attention at one or more subsequent layers. To ensure
that reusing self-attention blocks across layers does not degrade the
performance, we introduce a simple parametric function, which outperforms the
baseline transformer's performance while running computationally faster. We
show the effectiveness of our method in image classification and
self-supervised learning on ImageNet-1K, semantic segmentation on ADE20K, image
denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput
at the same-or-higher accuracy levels in all these tasks.",None,-1
b604675a-2684-43ef-8dce-1a761169e590,Multilingual Alzheimer's Dementia Recognition through Spontaneous Speech: a Signal Processing Grand Challenge,0.97266,"This Signal Processing Grand Challenge (SPGC) targets a difficult automatic
prediction problem of societal and medical relevance, namely, the detection of
Alzheimer's Dementia (AD). Participants were invited to employ signal
processing and machine learning methods to create predictive models based on
spontaneous speech data. The Challenge has been designed to assess the extent
to which predictive models built based on speech in one language (English)
generalise to another language (Greek). To the best of our knowledge no work
has investigated acoustic features of the speech signal in multilingual AD
detection. Our baseline system used conventional machine learning algorithms
with Active Data Representation of acoustic features, achieving accuracy of
73.91% on AD detection, and 4.95 root mean squared error on cognitive score
prediction.",None,-1
12a0a136-83bb-4d98-9bf2-552c4a80fd45,CD-CTFM: A Lightweight CNN-Transformer Network for Remote Sensing Cloud Detection Fusing Multiscale Features,0.456315,"Clouds in remote sensing images inevitably affect information extraction,
which hinder the following analysis of satellite images. Hence, cloud detection
is a necessary preprocessing procedure. However, the existing methods have
numerous calculations and parameters. In this letter, a lightweight
CNN-Transformer network, CD-CTFM, is proposed to solve the problem. CD-CTFM is
based on encoder-decoder architecture and incorporates the attention mechanism.
In the decoder part, we utilize a lightweight network combing CNN and
Transformer as backbone, which is conducive to extract local and global
features simultaneously. Moreover, a lightweight feature pyramid module is
designed to fuse multiscale features with contextual information. In the
decoder part, we integrate a lightweight channel-spatial attention module into
each skip connection between encoder and decoder, extracting low-level features
while suppressing irrelevant information without introducing many parameters.
Finally, the proposed model is evaluated on two cloud datasets, 38-Cloud and
MODIS. The results demonstrate that CD-CTFM achieves comparable accuracy as the
state-of-art methods. At the same time, CD-CTFM outperforms state-of-art
methods in terms of efficiency.",None,-1
bf11a7bc-9fff-4dab-bf38-1e765cc86efd,MedMine: Examining Pre-trained Language Models on Medication Mining,0.474115,"Automatic medication mining from clinical and biomedical text has become a
popular topic due to its real impact on healthcare applications and the recent
development of powerful language models (LMs). However, fully-automatic
extraction models still face obstacles to be overcome such that they can be
deployed directly into clinical practice for better impacts. Such obstacles
include their imbalanced performances on different entity types and clinical
events. In this work, we examine current state-of-the-art pre-trained language
models (PLMs) on such tasks, via fine-tuning including the monolingual model
Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their
advantages and drawbacks using historical medication mining shared task data
sets from n2c2-2018 challenges. We report the findings we get from these
fine-tuning experiments such that they can facilitate future research on
addressing them, for instance, how to combine their outputs, merge such models,
or improve their overall accuracy by ensemble learning and data augmentation.
MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}",None,-1
88248d21-e12d-4ce8-b73f-0bd7ce28e17a,MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain,0.113972,"Keeping track of all relevant recent publications and experimental results
for a research area is a challenging task. Prior work has demonstrated the
efficacy of information extraction models in various scientific areas.
Recently, several datasets have been released for the yet understudied
materials science domain. However, these datasets focus on sub-problems such as
parsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells.
In this resource paper, we present MuLMS, a new dataset of 50 open-access
articles, spanning seven sub-domains of materials science. The corpus has been
annotated by domain experts with several layers ranging from named entities
over relations to frame structures. We present competitive neural models for
all tasks and demonstrate that multi-task training with existing related
resources leads to benefits.",None,-1
e3000bc6-7925-4f62-a3bc-1c0ca9c8d1ca,Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning,0.071494,"As the size of the pre-trained language model (PLM) continues to increase,
numerous parameter-efficient transfer learning methods have been proposed
recently to compensate for the tremendous cost of fine-tuning. Despite the
impressive results achieved by large pre-trained language models (PLMs) and
various parameter-efficient transfer learning (PETL) methods on sundry
benchmarks, it remains unclear if they can handle inputs that have been
distributionally shifted effectively. In this study, we systematically explore
how the ability to detect out-of-distribution (OOD) changes as the size of the
PLM grows or the transfer methods are altered. Specifically, we evaluated
various PETL techniques, including fine-tuning, Adapter, LoRA, and
prefix-tuning, on three different intention classification tasks, each
utilizing various language models with different scales.",None,-1
474ff3bd-81c3-407c-a20c-48d672c516bd,Affordance Grounding from Demonstration Video to Target Image,0.574253,"Humans excel at learning from expert demonstrations and solving their own
problems. To equip intelligent robots and assistants, such as AR glasses, with
this ability, it is essential to ground human hand interactions (i.e.,
affordances) from demonstration videos and apply them to a target image like a
user's AR glass view. The video-to-image affordance grounding task is
challenging due to (1) the need to predict fine-grained affordances, and (2)
the limited training data, which inadequately covers video-image discrepancies
and negatively impacts grounding. To tackle them, we propose Affordance
Transformer (Afformer), which has a fine-grained transformer-based decoder that
gradually refines affordance grounding. Moreover, we introduce Mask Affordance
Hand (MaskAHand), a self-supervised pre-training technique for synthesizing
video-image data and simulating context changes, enhancing affordance grounding
across video-image discrepancies. Afformer with MaskAHand pre-training achieves
state-of-the-art performance on multiple benchmarks, including a substantial
37% improvement on the OPRA dataset. Code is made available at
https://github.com/showlab/afformer.",None,-1
1303169d-fe39-45f2-a15f-d1af2569c922,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,0.689559,"Crowd analysis from drones has attracted increasing attention in recent times
due to the ease of use and affordable cost of these devices. However, how this
technology can provide a solution to crowd flow detection is still an
unexplored research question. To this end, we propose a crowd flow detection
method for video sequences shot by a drone. The method is based on a
fully-convolutional network that learns to perform crowd clustering in order to
detect the centroids of crowd-dense areas and track their movement in
consecutive frames. The proposed method proved effective and efficient when
tested on the Crowd Counting datasets of the VisDrone challenge, characterized
by video sequences rather than still images. The encouraging results show that
the proposed method could open up new ways of analyzing high-level crowd
behavior from drones.",None,-1
fbc5e1df-d440-4138-9c6e-ee5b53abe471,LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding,0.616426,"Visually-rich Document Understanding (VrDU) has attracted much research
attention over the past years. Pre-trained models on a large number of document
images with transformer-based backbones have led to significant performance
gains in this field. The major challenge is how to fusion the different
modalities (text, layout, and image) of the documents in a unified model with
different pre-training tasks. This paper focuses on improving text-layout
interactions and proposes a novel multi-modal pre-training model, LayoutMask.
LayoutMask uses local 1D position, instead of global 1D position, as layout
input and has two pre-training objectives: (1) Masked Language Modeling:
predicting masked tokens with two novel masking strategies; (2) Masked Position
Modeling: predicting masked 2D positions to improve layout representation
learning. LayoutMask can enhance the interactions between text and layout
modalities in a unified model and produce adaptive and robust multi-modal
representations for downstream tasks. Experimental results show that our
proposed method can achieve state-of-the-art results on a wide variety of VrDU
problems, including form understanding, receipt understanding, and document
image classification.",None,-1
15181a35-4c11-4528-b395-d7cc513e0a64,Toward Unified Controllable Text Generation via Regular Expression Instruction,0.0651716,"Controllable text generation is a fundamental aspect of natural language
generation, with numerous methods proposed for different constraint types.
However, these approaches often require significant architectural or decoding
modifications, making them challenging to apply to additional constraints or
resolve different constraint combinations. To address this, our paper
introduces Regular Expression Instruction (REI), which utilizes an
instruction-based mechanism to fully exploit regular expressions' advantages to
uniformly model diverse constraints. Specifically, our REI supports all popular
fine-grained controllable generation constraints, i.e., lexical, positional,
and length, as well as their complex combinations, via regular expression-style
instructions. Our method only requires fine-tuning on medium-scale language
models or few-shot, in-context learning on large language models, and requires
no further adjustment when applied to various constraint combinations.
Experiments demonstrate that our straightforward approach yields high success
rates and adaptability to various constraints while maintaining competitiveness
in automatic metrics and outperforming most previous baselines.",None,-1
92bb5b59-eb71-4d11-83f1-07adb9cc100a,AttentionMix: Data augmentation method that relies on BERT attention mechanism,0.0957452,"The Mixup method has proven to be a powerful data augmentation technique in
Computer Vision, with many successors that perform image mixing in a guided
manner. One of the interesting research directions is transferring the
underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP).
Even though there already exist several methods that apply Mixup to textual
data, there is still room for new, improved approaches. In this work, we
introduce AttentionMix, a novel mixing method that relies on attention-based
information. While the paper focuses on the BERT attention mechanism, the
proposed approach can be applied to generally any attention-based model.
AttentionMix is evaluated on 3 standard sentiment classification datasets and
in all three cases outperforms two benchmark approaches that utilize Mixup
mechanism, as well as the vanilla BERT method. The results confirm that the
attention-based information can be effectively used for data augmentation in
the NLP domain.",None,-1
2f3ccafd-db8a-4724-bef9-ad1c649e403d,"Meta-Diversity Search in Complex Systems, A Recipe for Artificial Open-Endedness ?",0.471738,"Can we build an artificial system that would be able to generate endless
surprises if ran ""forever"" in Minecraft? While there is not a single path
toward solving that grand challenge, this article presents what we believe to
be some working ingredients for the endless generation of novel increasingly
complex artifacts in Minecraft. Our framework for an open-ended system includes
two components: a complex system used to recursively grow and complexify
artifacts over time, and a discovery algorithm that leverages the concept of
meta-diversity search. Since complex systems have shown to enable the emergence
of considerable complexity from set of simple rules, we believe them to be
great candidates to generate all sort of artifacts in Minecraft. Yet, the space
of possible artifacts that can be generated by these systems is often unknown,
challenging to characterize and explore. Therefore automating the long-term
discovery of novel and increasingly complex artifacts in these systems is an
exciting research field. To approach these challenges, we formulate the problem
of meta-diversity search where an artificial ""discovery assistant""
incrementally learns a diverse set of representations to characterize behaviors
and searches to discover diverse patterns within each of them. A successful
discovery assistant should continuously seek for novel sources of diversities
while being able to quickly specialize the search toward a new unknown type of
diversity. To implement those ideas in the Minecraft environment, we simulate
an artificial ""chemistry"" system based on Lenia continuous cellular automaton
for generating artifacts, as well as an artificial ""discovery assistant""
(called Holmes) for the artifact-discovery process. Holmes incrementally learns
a hierarchy of modular representations to characterize divergent sources of
diversity and uses a goal-based intrinsically-motivated exploration as the
diversity search strategy.",None,-1
5d156219-2e5b-42ba-aacb-33bbd4fc3452,Asynchronous training of quantum reinforcement learning,0.612224,"The development of quantum machine learning (QML) has received a lot of
interest recently thanks to developments in both quantum computing (QC) and
machine learning (ML). One of the ML paradigms that can be utilized to address
challenging sequential decision-making issues is reinforcement learning (RL).
It has been demonstrated that classical RL can successfully complete many
difficult tasks. A leading method of building quantum RL agents relies on the
variational quantum circuits (VQC). However, training QRL algorithms with VQCs
requires significant amount of computational resources. This issue hurdles the
exploration of various QRL applications. In this paper, we approach this
challenge through asynchronous training QRL agents. Specifically, we choose the
asynchronous training of advantage actor-critic variational quantum policies.
We demonstrate the results via numerical simulations that within the tasks
considered, the asynchronous training of QRL agents can reach performance
comparable to or superior than classical agents with similar model sizes and
architectures.",None,-1
478f438f-a589-4f35-898e-cfff59471ce9,ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations,0.39776,"As generative AI becomes more prevalent, it is important to study how human
users interact with such models. In this work, we investigate how people use
text-to-image models to generate desired target images. To study this
interaction, we created ArtWhisperer, an online game where users are given a
target image and are tasked with iteratively finding a prompt that creates a
similar-looking image as the target. Through this game, we recorded over 50,000
human-AI interactions; each interaction corresponds to one text prompt created
by a user and the corresponding generated image. The majority of these are
repeated interactions where a user iterates to find the best prompt for their
target image, making this a unique sequential dataset for studying human-AI
collaborations. In an initial analysis of this dataset, we identify several
characteristics of prompt interactions and user strategies. People submit
diverse prompts and are able to discover a variety of text descriptions that
generate similar images. Interestingly, prompt diversity does not decrease as
users find better prompts. We further propose a new metric to quantify the
steerability of AI using our dataset. We define steerability as the expected
number of interactions required to adequately complete a task. We estimate this
value by fitting a Markov chain for each target task and calculating the
expected time to reach an adequate score in the Markov chain. We quantify and
compare AI steerability across different types of target images and two
different models, finding that images of cities and natural world images are
more steerable than artistic and fantasy images. These findings provide
insights into human-AI interaction behavior, present a concrete method of
assessing AI steerability, and demonstrate the general utility of the
ArtWhisperer dataset.",None,-1
c83accc0-0233-4c11-9112-e5528138c5c9,What does BERT learn about prosody?,0.51502,"Language models have become nearly ubiquitous in natural language processing
applications achieving state-of-the-art results in many tasks including
prosody. As the model design does not define predetermined linguistic targets
during training but rather aims at learning generalized representations of the
language, analyzing and interpreting the representations that models implicitly
capture is important in bridging the gap between interpretability and model
performance. Several studies have explored the linguistic information that
models capture providing some insights on their representational capacity.
However, the current studies have not explored whether prosody is part of the
structural information of the language that models learn. In this work, we
perform a series of experiments on BERT probing the representations captured at
different layers. Our results show that information about prosodic prominence
spans across many layers but is mostly focused in middle layers suggesting that
BERT relies mostly on syntactic and semantic information.",None,-1
6713b485-2d2a-4470-9586-11338a0fc088,Heuristic Algorithms for the Approximation of Mutual Coherence,0.127092,"Mutual coherence is a measure of similarity between two opinions. Although
the notion comes from philosophy, it is essential for a wide range of
technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters
to find candidates that are the closest to their political preferences. The
exact computation of mutual coherence is highly time-consuming due to the
iteration over all subsets of an opinion. Moreover, for every subset, an
instance of the SAT model counting problem has to be solved which is known to
be a hard problem in computer science. This work is the first study to
accelerate this computation. We model the distribution of the so-called
confirmation values as a mixture of three Gaussians and present efficient
heuristics to estimate its model parameters. The mutual coherence is then
approximated with the expected value of the distribution. Some of the presented
algorithms are fully polynomial-time, others only require solving a small
number of instances of the SAT model counting problem. The average squared
error of our best algorithm lies below 0.0035 which is insignificant if the
efficiency is taken into account. Furthermore, the accuracy is precise enough
to be used in Wahl-O-Mat-like systems.",None,-1
022a5efa-ac5c-47d1-9513-4ded5197a5bd,Dense Text-to-Image Generation with Attention Modulation,0.843161,"Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.",None,-1
22b79ce0-8e7c-400a-96af-0deeb429fc47,Artificial General Intelligence for Medical Imaging,0.727174,"In this review, we explore the potential applications of Artificial General
Intelligence (AGI) models in healthcare, focusing on foundational Large
Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We
emphasize the importance of integrating clinical expertise, domain knowledge,
and multimodal capabilities into AGI models. In addition, we lay out key
roadmaps that guide the development and deployment of healthcare AGI models.
Throughout the review, we provide critical perspectives on the potential
challenges and pitfalls associated with deploying large-scale AGI models in the
medical field. This comprehensive review aims to offer insights into the future
implications of AGI in medical imaging, healthcare and beyond.",None,-1
21275d64-27f6-4f7e-9ad2-968da43c067d,Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning,0.126052,"The disruptive potential of AI systems roots in the emergence of big data.
Yet, a significant portion is scattered and locked in data silos, leaving its
potential untapped. Federated Machine Learning is a novel AI paradigm enabling
the creation of AI models from decentralized, potentially siloed data. Hence,
Federated Machine Learning could technically open data silos and therefore
unlock economic potential. However, this requires collaboration between
multiple parties owning data silos. Setting up collaborative business models is
complex and often a reason for failure. Current literature lacks guidelines on
which aspects must be considered to successfully realize collaborative AI
projects. This research investigates the challenges of prevailing collaborative
business models and distinct aspects of Federated Machine Learning. Through a
systematic literature review, focus group, and expert interviews, we provide a
systemized collection of socio-technical challenges and an extended Business
Model Canvas for the initial viability assessment of collaborative AI projects.",None,-1
8e0a0b69-84ba-43b3-9f26-0604b89cf5e7,PuoBERTa: Training and evaluation of a curated language model for Setswana,0.235551,"Natural language processing (NLP) has made significant progress for
well-resourced languages such as English but lagged behind for low-resource
languages like Setswana. This paper addresses this gap by presenting PuoBERTa,
a customised masked language model trained specifically for Setswana. We cover
how we collected, curated, and prepared diverse monolingual texts to generate a
high-quality corpus for PuoBERTa's training. Building upon previous efforts in
creating monolingual resources for Setswana, we evaluated PuoBERTa across
several NLP tasks, including part-of-speech (POS) tagging, named entity
recognition (NER), and news categorisation. Additionally, we introduced a new
Setswana news categorisation dataset and provided the initial benchmarks using
PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP
capabilities for understudied languages like Setswana and paves the way for
future research directions.",None,-1
851f6105-273a-4cce-a2f3-c64bcac67bae,A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation,0.611043,"The task of semantic segmentation requires a model to assign semantic labels
to each pixel of an image. However, the performance of such models degrades
when deployed in an unseen domain with different data distributions compared to
the training domain. We present a new augmentation-driven approach to domain
generalization for semantic segmentation using a re-parameterized vision
transformer (ReVT) with weight averaging of multiple models after training. We
evaluate our approach on several benchmark datasets and achieve
state-of-the-art mIoU performance of 47.3% (prior art: 46.3%) for small models
and of 50.1% (prior art: 47.8%) for midsized models on commonly used benchmark
datasets. At the same time, our method requires fewer parameters and reaches a
higher frame rate than the best prior art. It is also easy to implement and,
unlike network ensembles, does not add any computational complexity during
inference.",None,-1
a1d0d589-bd25-4b14-9726-cbfa8cf25a2b,Multimodal Color Recommendation in Vector Graphic Documents,0.269372,"Color selection plays a critical role in graphic document design and requires
sufficient consideration of various contexts. However, recommending appropriate
colors which harmonize with the other colors and textual contexts in documents
is a challenging task, even for experienced designers. In this study, we
propose a multimodal masked color model that integrates both color and textual
contexts to provide text-aware color recommendation for graphic documents. Our
proposed model comprises self-attention networks to capture the relationships
between colors in multiple palettes, and cross-attention networks that
incorporate both color and CLIP-based text representations. Our proposed method
primarily focuses on color palette completion, which recommends colors based on
the given colors and text. Additionally, it is applicable for another color
recommendation task, full palette generation, which generates a complete color
palette corresponding to the given text. Experimental results demonstrate that
our proposed approach surpasses previous color palette completion methods on
accuracy, color distribution, and user experience, as well as full palette
generation methods concerning color diversity and similarity to the ground
truth palettes.",None,-1
bf886a07-f82b-4602-9e96-abd0e09ca62f,Conversational Health Agents: A Personalized LLM-Powered Agent Framework,0.999578,"Conversational Health Agents (CHAs) are interactive systems that provide
healthcare services, such as assistance and diagnosis. Current CHAs, especially
those utilizing Large Language Models (LLMs), primarily focus on conversation
aspects. However, they offer limited agent capabilities, specifically lacking
multi-step problem-solving, personalized conversations, and multimodal data
analysis. Our aim is to overcome these limitations. We propose openCHA, an
open-source LLM-powered framework, to empower conversational agents to generate
a personalized response for users' healthcare queries. This framework enables
developers to integrate external sources including data sources, knowledge
bases, and analysis models, into their LLM-based solutions. openCHA includes an
orchestrator to plan and execute actions for gathering information from
external sources, essential for formulating responses to user inquiries. It
facilitates knowledge acquisition, problem-solving capabilities, multilingual
and multimodal conversations, and fosters interaction with various AI
platforms. We illustrate the framework's proficiency in handling complex
healthcare tasks via three demonstrations. Moreover, we release openCHA as open
source available to the community via GitHub.",None,-1
26964895-c532-4dd5-9a92-cb17f5a6db94,Shape of You: Precise 3D shape estimations for diverse body types,0.11275,"This paper presents Shape of You (SoY), an approach to improve the accuracy
of 3D body shape estimation for vision-based clothing recommendation systems.
While existing methods have successfully estimated 3D poses, there remains a
lack of work in precise shape estimation, particularly for diverse human
bodies. To address this gap, we propose two loss functions that can be readily
integrated into parametric 3D human reconstruction pipelines. Additionally, we
propose a test-time optimization routine that further improves quality. Our
method improves over the recent SHAPY method by 17.7% on the challenging SSP-3D
dataset. We consider our work to be a step towards a more accurate 3D shape
estimation system that works reliably on diverse body types and holds promise
for practical applications in the fashion industry.",None,-1
a47ea070-f163-4df8-b6e9-0cb602a9c256,ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics,0.844938,"We introduce ProofNet, a benchmark for autoformalization and formal proving
of undergraduate-level mathematics. The ProofNet benchmarks consists of 371
examples, each consisting of a formal theorem statement in Lean 3, a natural
language theorem statement, and a natural language proof. The problems are
primarily drawn from popular undergraduate pure mathematics textbooks and cover
topics such as real and complex analysis, linear algebra, abstract algebra, and
topology. We intend for ProofNet to be a challenging benchmark that will drive
progress in autoformalization and automatic theorem proving. We report baseline
results on statement autoformalization via in-context learning. Moreover, we
introduce two novel statement autoformalization methods: prompt retrieval and
distilled backtranslation.",None,-1
454c2e5d-35d6-4ea7-9f00-7c9643a4f63d,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,0.582375,"Underwater images typically experience mixed degradations of brightness and
structure caused by the absorption and scattering of light by suspended
particles. To address this issue, we propose a Real-time Spatial and Frequency
Domains Modulation Network (RSFDM-Net) for the efficient enhancement of colors
and details in underwater images. Specifically, our proposed conditional
network is designed with Adaptive Fourier Gating Mechanism (AFGM) and
Multiscale Convolutional Attention Module (MCAM) to generate vectors carrying
low-frequency background information and high-frequency detail features, which
effectively promote the network to model global background information and
local texture details. To more precisely correct the color cast and low
saturation of the image, we introduce a Three-branch Feature Extraction (TFE)
block in the primary net that processes images pixel by pixel to integrate the
color information extended by the same channel (R, G, or B). This block
consists of three small branches, each of which has its own weights. Extensive
experiments demonstrate that our network significantly outperforms over
state-of-the-art methods in both visual quality and quantitative metrics.",None,-1
73759fe6-3afa-4738-bcb1-1d508b01db10,Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling,0.145055,"Diffusion models have recently demonstrated an impressive ability to address
inverse problems in an unsupervised manner. While existing methods primarily
focus on modifying the posterior sampling process, the potential of the forward
process remains largely unexplored. In this work, we propose Shortcut Sampling
for Diffusion(SSD), a novel approach for solving inverse problems in a
zero-shot manner. Instead of initiating from random noise, the core concept of
SSD is to find a specific transitional state that bridges the measurement image
y and the restored image x. By utilizing the shortcut path of ""input -
transitional state - output"", SSD can achieve precise restoration with fewer
steps. To derive the transitional state during the forward process, we
introduce Distortion Adaptive Inversion. Moreover, we apply back projection as
additional consistency constraints during the generation process.
Experimentally, we demonstrate SSD's effectiveness on multiple representative
IR tasks. Our method achieves competitive results with only 30 NFEs compared to
state-of-the-art zero-shot methods(100 NFEs) and outperforms them with 100 NFEs
in certain tasks. Code is available at https://github.com/GongyeLiu/SSD",None,-1
6b84f318-7a41-4990-bedd-ad95192f18da,BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?,0.0899171,"Language models have seen significant growth in the size of their corpus,
leading to notable performance improvements. Yet, there has been limited
progress in developing models that handle smaller, more human-like datasets. As
part of the BabyLM shared task, this study explores the impact of reinforcement
learning from human feedback (RLHF) on language models pretrained from scratch
with a limited training corpus. Comparing two GPT-2 variants, the larger model
performs better in storytelling tasks after RLHF fine-tuning. These findings
suggest that RLHF techniques may be more advantageous for larger models due to
their higher learning and adaptation capacity, though more experiments are
needed to confirm this finding. These insights highlight the potential benefits
of RLHF fine-tuning for language models within limited data, enhancing their
ability to maintain narrative focus and coherence while adhering better to
initial instructions in storytelling tasks. The code for this work is publicly
at https://github.com/Zephyr1022/BabyStories-UTSA.",None,-1
154a5be7-4dc6-40de-8956-868baa5a2f75,ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency,0.69659,"We present ShapeClipper, a novel method that reconstructs 3D object shapes
from real-world single-view RGB images. Instead of relying on laborious 3D,
multi-view or camera pose annotation, ShapeClipper learns shape reconstruction
from a set of single-view segmented images. The key idea is to facilitate shape
learning via CLIP-based shape consistency, where we encourage objects with
similar CLIP encodings to share similar shapes. We also leverage off-the-shelf
normals as an additional geometric constraint so the model can learn better
bottom-up reasoning of detailed surface geometry. These two novel consistency
constraints, when used to regularize our model, improve its ability to learn
both global shape structure and local geometric details. We evaluate our method
over three challenging real-world datasets, Pix3D, Pascal3D+, and OpenImages,
where we achieve superior performance over state-of-the-art methods.",None,-1
75d3bf59-a72d-4444-9e8c-6123ea253870,Recovering from Privacy-Preserving Masking with Large Language Models,0.497833,"Model adaptation is crucial to handle the discrepancy between proxy training
data and actual users data received. To effectively perform adaptation, textual
data of users is typically stored on servers or their local devices, where
downstream natural language processing (NLP) models can be directly trained
using such in-domain data. However, this might raise privacy and security
concerns due to the extra risks of exposing user information to adversaries.
Replacing identifying information in textual data with a generic marker has
been recently explored. In this work, we leverage large language models (LLMs)
to suggest substitutes of masked tokens and have their effectiveness evaluated
on downstream language modeling tasks. Specifically, we propose multiple
pre-trained and fine-tuned LLM-based approaches and perform empirical studies
on various datasets for the comparison of these methods. Experimental results
show that models trained on the obfuscation corpora are able to achieve
comparable performance with the ones trained on the original data without
privacy-preserving token masking.",None,-1
f5b31e46-b470-46c6-bd20-7e20a34e79e3,AMR Parsing with Causal Hierarchical Attention and Pointers,0.236382,"Translation-based AMR parsers have recently gained popularity due to their
simplicity and effectiveness. They predict linearized graphs as free texts,
avoiding explicit structure modeling. However, this simplicity neglects
structural locality in AMR graphs and introduces unnecessary tokens to
represent coreferences. In this paper, we introduce new target forms of AMR
parsing and a novel model, CHAP, which is equipped with causal hierarchical
attention and the pointer mechanism, enabling the integration of structures
into the Transformer decoder. We empirically explore various alternative
modeling options. Experiments show that our model outperforms baseline models
on four out of five benchmarks in the setting of no additional data.",None,-1
26431290-085b-4cdb-9973-dfcfd6ea0ccd,Towards a Unifying Model of Rationality in Multiagent Systems,0.0548495,"Multiagent systems deployed in the real world need to cooperate with other
agents (including humans) nearly as effectively as these agents cooperate with
one another. To design such AI, and provide guarantees of its effectiveness, we
need to clearly specify what types of agents our AI must be able to cooperate
with. In this work we propose a generic model of socially intelligent agents,
which are individually rational learners that are also able to cooperate with
one another (in the sense that their joint behavior is Pareto efficient). We
define rationality in terms of the regret incurred by each agent over its
lifetime, and show how we can construct socially intelligent agents for
different forms of regret. We then discuss the implications of this model for
the development of ""robust"" MAS that can cooperate with a wide variety of
socially intelligent agents.",None,-1
c39f1264-97fd-4eb1-a301-e39b35f75a87,Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation,0.588244,"Zero-shot instance segmentation aims to detect and precisely segment objects
of unseen categories without any training samples. Since the model is trained
on seen categories, there is a strong bias that the model tends to classify all
the objects into seen categories. Besides, there is a natural confusion between
background and novel objects that have never shown up in training. These two
challenges make novel objects hard to be raised in the final instance
segmentation results. It is desired to rescue novel objects from background and
dominated seen categories. To this end, we propose D$^2$Zero with
Semantic-Promoted Debiasing and Background Disambiguation to enhance the
performance of Zero-shot instance segmentation. Semantic-promoted debiasing
utilizes inter-class semantic relationships to involve unseen categories in
visual feature training and learns an input-conditional classifier to conduct
dynamical classification based on the input image. Background disambiguation
produces image-adaptive background representation to avoid mistaking novel
objects for background. Extensive experiments show that we significantly
outperform previous state-of-the-art methods by a large margin, e.g., 16.86%
improvement on COCO. Project page: https://henghuiding.github.io/D2Zero/",None,-1
8930d5a9-ee43-4b45-83bd-29a8de820564,Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data,0.699403,"Chain-of-thought (CoT) advances the reasoning abilities of large language
models (LLMs) and achieves superior performance in complex reasoning tasks.
However, most CoT studies rely on carefully designed human-annotated rational
chains to prompt LLMs, posing challenges for real-world applications where
labeled data is available without rational chains. This paper proposes a new
strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with
Chain-of-Thought), that can bypass human engineering of CoT by automatically
augmenting rational chains from a small labeled dataset, and then pruning
low-quality chains to construct a candidate pool of machine-generated rationale
chains based on the labels. Finally, it selects the optimal combination of
several rationale chains from the pool for CoT prompting by employing a
variance-reduced policy gradient strategy to estimate the significance of each
example. Automate-CoT enables a quick adaptation of the CoT technique to
different tasks. Experimental results demonstrate the effectiveness of our
method, where competitive results are achieved on arithmetic reasoning (+2.7%),
commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning
tasks (+2.5%). The code is available at
https://github.com/SHUMKASHUN/Automate-CoT.",None,-1
eed31fc4-9ab0-47aa-9dbb-2ca258e74f8a,DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning,0.310214,"Pragmatic reasoning plays a pivotal role in deciphering implicit meanings
that frequently arise in real-life conversations and is essential for the
development of communicative social agents. In this paper, we introduce a novel
challenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic
reasoning and situated conversational understanding. Compared with previous
works that treat different figurative expressions (e.g. metaphor, sarcasm) as
individual tasks, DiPlomat provides a cohesive framework towards general
pragmatic understanding. Our dataset is created through the utilization of
Amazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn
dialogues. In conjunction with the dataset, we propose two tasks, Pragmatic
Identification and Reasoning (PIR) and Conversational Question Answering (CQA).
Experimental results with state-of-the-art (SOTA) neural architectures reveal
several significant findings: 1) large language models ( LLMs) exhibit poor
performance in tackling this subjective domain; 2) comprehensive comprehension
of context emerges as a critical factor for establishing benign human-machine
interactions; 3) current models defect in the application of pragmatic
reasoning. As a result, we call on more attention to improve the ability of
context understanding, reasoning, and implied meaning modeling.",None,-1
4a0c18b8-f246-4f82-ab54-6d36b088aa12,Syllable Subword Tokens for Open Vocabulary Speech Recognition in Malayalam,0.0783221,"In a hybrid automatic speech recognition (ASR) system, a pronunciation
lexicon (PL) and a language model (LM) are essential to correctly retrieve
spoken word sequences. Being a morphologically complex language, the vocabulary
of Malayalam is so huge and it is impossible to build a PL and an LM that cover
all diverse word forms. Usage of subword tokens to build PL and LM, and
combining them to form words after decoding, enables the recovery of many out
of vocabulary words. In this work we investigate the impact of using syllables
as subword tokens instead of words in Malayalam ASR, and evaluate the relative
improvement in lexicon size, model memory requirement and word error rate.",None,-1
311049aa-3781-4874-904f-be780d312fa1,ScandEval: A Benchmark for Scandinavian Natural Language Processing,0.205702,"This paper introduces a Scandinavian benchmarking platform, ScandEval, which
can benchmark any pretrained model on four different tasks in the Scandinavian
languages. The datasets used in two of the tasks, linguistic acceptability and
question answering, are new. We develop and release a Python package and
command-line interface, scandeval, which can benchmark any model that has been
uploaded to the Hugging Face Hub, with reproducible results. Using this
package, we benchmark more than 100 Scandinavian or multilingual models and
present the results of these in an interactive online leaderboard, as well as
provide an analysis of the results. The analysis shows that there is
substantial cross-lingual transfer among the Mainland Scandinavian languages
(Danish, Swedish and Norwegian), with limited cross-lingual transfer between
the group of Mainland Scandinavian languages and the group of Insular
Scandinavian languages (Icelandic and Faroese). The benchmarking results also
show that the investment in language technology in Norway, Sweden and Denmark
has led to language models that outperform massively multilingual models such
as XLM-RoBERTa and mDeBERTaV3. We release the source code for both the package
and leaderboard.",None,-1
3bf424f3-888b-45f3-a3b1-41e3ccc16fd4,Incorporating Graph Information in Transformer-based AMR Parsing,0.767792,"Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that
aims at providing a semantic graph abstraction representing a given text.
Current approaches are based on autoregressive language models such as BART or
T5, fine-tuned through Teacher Forcing to obtain a linearized version of the
AMR graph from a sentence. In this paper, we present LeakDistill, a model and
method that explores a modification to the Transformer architecture, using
structural adapters to explicitly incorporate graph information into the
learned representations and improve AMR parsing performance. Our experiments
show how, by employing word-to-node alignment to embed graph structural
information into the encoder at training time, we can obtain state-of-the-art
AMR parsing through self-knowledge distillation, even without the use of
additional data. We release the code at
\url{http://www.github.com/sapienzanlp/LeakDistill}.",None,-1
335c656e-e484-4cf2-afe6-286a76d0c4d7,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,0.520159,"The large number of ReLU non-linearity operations in existing deep neural
networks makes them ill-suited for latency-efficient private inference (PI).
Existing techniques to reduce ReLU operations often involve manual effort and
sacrifice significant accuracy. In this paper, we first present a novel measure
of non-linearity layers' ReLU sensitivity, enabling mitigation of the
time-consuming manual efforts in identifying the same. Based on this
sensitivity, we then present SENet, a three-stage training method that for a
given ReLU budget, automatically assigns per-layer ReLU counts, decides the
ReLU locations for each layer's activation map, and trains a model with
significantly fewer ReLUs to potentially yield latency and communication
efficient PI. Experimental evaluations with multiple models on various datasets
show SENet's superior performance both in terms of reduced ReLUs and improved
classification accuracy compared to existing alternatives. In particular, SENet
can yield models that require up to ~2x fewer ReLUs while yielding similar
accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved
classification accuracy, evaluated on CIFAR-100.",None,-1
b972cd54-1815-4ecf-8db2-86393628fd6d,Mimicking the Thinking Process for Emotion Recognition in Conversation with Prompts and Paraphrasing,0.797573,"Emotion recognition in conversation, which aims to predict the emotion for
all utterances, has attracted considerable research attention in recent years.
It is a challenging task since the recognition of the emotion in one utterance
involves many complex factors, such as the conversational context, the
speaker's background, and the subtle difference between emotion labels. In this
paper, we propose a novel framework which mimics the thinking process when
modeling these factors. Specifically, we first comprehend the conversational
context with a history-oriented prompt to selectively gather information from
predecessors of the target utterance. We then model the speaker's background
with an experience-oriented prompt to retrieve the similar utterances from all
conversations. We finally differentiate the subtle label semantics with a
paraphrasing mechanism to elicit the intrinsic label related knowledge. We
conducted extensive experiments on three benchmarks. The empirical results
demonstrate the superiority of our proposed framework over the state-of-the-art
baselines.",None,-1
4c5c48ab-aad2-4d80-8c22-60280633b8d8,Future Lens: Anticipating Subsequent Tokens from a Single Hidden State,0.901025,"We conjecture that hidden state vectors corresponding to individual input
tokens encode information sufficient to accurately predict several tokens
ahead. More concretely, in this paper we ask: Given a hidden (internal)
representation of a single token at position $t$ in an input, can we reliably
anticipate the tokens that will appear at positions $\geq t + 2$? To test this,
we measure linear approximation and causal intervention methods in GPT-J-6B to
evaluate the degree to which individual hidden states in the network contain
signal rich enough to predict future hidden states and, ultimately, token
outputs. We find that, at some layers, we can approximate a model's output with
more than 48% accuracy with respect to its prediction of subsequent tokens
through a single hidden state. Finally we present a ""Future Lens"" visualization
that uses these methods to create a new view of transformer states.",None,-1
c89f16d1-be26-42c1-aae6-25fbe0a0a404,Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs,0.864471,"The performance of large language models (LLMs) has recently improved to the
point where the models can perform well on many language tasks. We show here
that for the first time, the models can also generate coherent and valid formal
analyses of linguistic data and illustrate the vast potential of large language
models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their
metalinguistic abilities improves our understanding of their general
capabilities and sheds new light on theoretical models in linguistics. In this
paper, we probe into GPT-4's metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a
research program for metalinguistic analyses of large language models, propose
experimental designs, provide general guidelines, discuss limitations, and
offer future directions for this line of research. This line of inquiry also
exemplifies behavioral interpretability of deep learning, where models'
representations are accessed by explicit prompting rather than internal
representations.",None,-1
0b8a569d-f855-4558-a116-e8ffcc763c26,Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings,0.369776,"Languages are dynamic entities, where the meanings associated with words
constantly change with time. Detecting the semantic variation of words is an
important task for various NLP applications that must make time-sensitive
predictions. Existing work on semantic variation prediction have predominantly
focused on comparing some form of an averaged contextualised representation of
a target word computed from a given corpus. However, some of the previously
associated meanings of a target word can become obsolete over time (e.g.
meaning of gay as happy), while novel usages of existing words are observed
(e.g. meaning of cell as a mobile phone). We argue that mean representations
alone cannot accurately capture such semantic variations and propose a method
that uses the entire cohort of the contextualised embeddings of the target
word, which we refer to as the sibling distribution. Experimental results on
SemEval-2020 Task 1 benchmark dataset for semantic variation prediction show
that our method outperforms prior work that consider only the mean embeddings,
and is comparable to the current state-of-the-art. Moreover, a qualitative
analysis shows that our method detects important semantic changes in words that
are not captured by the existing methods. Source code is available at
https://github.com/a1da4/svp-gauss .",None,-1
c20c54c2-009c-40fa-b01f-44f56f442e18,Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention,0.542167,"Traditional multi-agent reinforcement learning algorithms are difficultly
applied in a large-scale multi-agent environment. The introduction of mean
field theory has enhanced the scalability of multi-agent reinforcement learning
in recent years. This paper considers partially observable multi-agent
reinforcement learning (MARL), where each agent can only observe other agents
within a fixed range. This partial observability affects the agent's ability to
assess the quality of the actions of surrounding agents. This paper focuses on
developing a method to capture more effective information from local
observations in order to select more effective actions. Previous work in this
field employs probability distributions or weighted mean field to update the
average actions of neighborhood agents, but it does not fully consider the
feature information of surrounding neighbors and leads to a local optimum. In
this paper, we propose a novel multi-agent reinforcement learning algorithm,
Partially Observable Mean Field Multi-Agent Reinforcement Learning based on
Graph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention
module and a mean field module to describe how an agent is influenced by the
actions of other agents at each time step. This graph attention module consists
of a graph attention encoder and a differentiable attention mechanism, and this
mechanism outputs a dynamic graph to represent the effectiveness of
neighborhood agents against central agents. The mean--field module approximates
the effect of a neighborhood agent on a central agent as the average effect of
effective neighborhood agents. We evaluate GAMFQ on three challenging tasks in
the MAgents framework. Experiments show that GAMFQ outperforms baselines
including the state-of-the-art partially observable mean-field reinforcement
learning algorithms.",None,-1
7ecc0782-66f7-497a-bd57-892102b24267,Follow-on Question Suggestion via Voice Hints for Voice Assistants,0.600456,"The adoption of voice assistants like Alexa or Siri has grown rapidly,
allowing users to instantly access information via voice search. Query
suggestion is a standard feature of screen-based search experiences, allowing
users to explore additional topics. However, this is not trivial to implement
in voice-based settings. To enable this, we tackle the novel task of suggesting
questions with compact and natural voice hints to allow users to ask follow-up
questions.
  We define the task, ground it in syntactic theory and outline linguistic
desiderata for spoken hints. We propose baselines and an approach using
sequence-to-sequence Transformers to generate spoken hints from a list of
questions. Using a new dataset of 6681 input questions and human written hints,
we evaluated the models with automatic metrics and human evaluation. Results
show that a naive approach of concatenating suggested questions creates poor
voice hints. Our approach, which applies a linguistically-motivated pretraining
task was strongly preferred by humans for producing the most natural hints.",None,-1
ae9f07af-3e08-4693-9e9e-6e98396bbe5e,Fair Decision-making Under Uncertainty,0.756516,"There has been concern within the artificial intelligence (AI) community and
the broader society regarding the potential lack of fairness of AI-based
decision-making systems. Surprisingly, there is little work quantifying and
guaranteeing fairness in the presence of uncertainty which is prevalent in many
socially sensitive applications, ranging from marketing analytics to actuarial
analysis and recidivism prediction instruments. To this end, we study a
longitudinal censored learning problem subject to fairness constraints, where
we require that algorithmic decisions made do not affect certain individuals or
social groups negatively in the presence of uncertainty on class label due to
censorship. We argue that this formulation has a broader applicability to
practical scenarios concerning fairness. We show how the newly devised fairness
notions involving censored information and the general framework for fair
predictions in the presence of censorship allow us to measure and mitigate
discrimination under uncertainty that bridges the gap with real-world
applications. Empirical evaluations on real-world discriminated datasets with
censorship demonstrate the practicality of our approach.",None,-1
e79e8333-bc54-4c68-b140-26157bc2b616,Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions,0.737271,"We propose a method for synthesizing edited photo-realistic digital avatars
with text instructions. Given a short monocular RGB video and text
instructions, our method uses an image-conditioned diffusion model to edit one
head image and uses the video stylization method to accomplish the editing of
other head images. Through iterative training and update (three times or more),
our method synthesizes edited photo-realistic animatable 3D neural head avatars
with a deformable neural radiance field head synthesis method. In quantitative
and qualitative studies on various subjects, our method outperforms
state-of-the-art methods.",None,-1
300aab74-9b60-4cc7-aa10-a670d569a226,Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs,0.438003,"Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency
multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.
However, MLPs rely exclusively on the node features and fail to capture the
graph structural information. Previous methods address this issue by processing
graph edges into extra inputs for MLPs, but such graph structures may be
unavailable for various scenarios. To this end, we propose a Prototype-Guided
Knowledge Distillation~(PGKD) method, which does not require graph
edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the
graph structural information in GNN teachers, and distill such information from
GNNs to MLPs via prototypes in an edge-free setting. Experimental results on
popular graph benchmarks demonstrate the effectiveness and robustness of the
proposed PGKD.",None,-1
ac3731fc-66aa-4e06-a274-0d2fe52766d7,Structured Dialogue Discourse Parsing,0.799105,"Dialogue discourse parsing aims to uncover the internal structure of a
multi-participant conversation by finding all the discourse~\emph{links} and
corresponding~\emph{relations}. Previous work either treats this task as a
series of independent multiple-choice problems, in which the link existence and
relations are decoded separately, or the encoding is restricted to only local
interaction, ignoring the holistic structural information. In contrast, we
propose a principled method that improves upon previous work from two
perspectives: encoding and decoding. From the encoding side, we perform
structured encoding on the adjacency matrix followed by the matrix-tree
learning algorithm, where all discourse links and relations in the dialogue are
jointly optimized based on latent tree-level distribution. From the decoding
side, we perform structured inference using the modified Chiu-Liu-Edmonds
algorithm, which explicitly generates the labeled multi-root non-projective
spanning tree that best captures the discourse structure. In addition, unlike
in previous work, we do not rely on hand-crafted features; this improves the
model's robustness. Experiments show that our method achieves new
state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on
Molweni (F1 scores). \footnote{Code released
at~\url{https://github.com/chijames/structured_dialogue_discourse_parsing}.}",None,-1
dcee5c3c-c99c-4599-99ba-3f63af28042d,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,0.974691,"Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.",None,-1
528246c4-3b7f-4d29-9450-d80d21d1a61a,Multistage Spatial Context Models for Learned Image Compression,0.621367,"Recent state-of-the-art Learned Image Compression methods feature spatial
context models, achieving great rate-distortion improvements over hyperprior
methods. However, the autoregressive context model requires serial decoding,
limiting runtime performance. The Checkerboard context model allows parallel
decoding at a cost of reduced RD performance. We present a series of multistage
spatial context models allowing both fast decoding and better RD performance.
We split the latent space into square patches and decode serially within each
patch while different patches are decoded in parallel. The proposed method
features a comparable decoding speed to Checkerboard while reaching the RD
performance of Autoregressive and even also outperforming Autoregressive.
Inside each patch, the decoding order must be carefully decided as a bad order
negatively impacts performance; therefore, we also propose a decoding order
optimization algorithm.",None,-1
03560753-7c04-46d9-b2b5-fd5d3b1e08ed,World Models for Math Story Problems,0.310181,"Solving math story problems is a complex task for students and NLP models
alike, requiring them to understand the world as described in the story and
reason over it to compute an answer. Recent years have seen impressive
performance on automatically solving these problems with large pre-trained
language models and innovative techniques to prompt them. However, it remains
unclear if these models possess accurate representations of mathematical
concepts. This leads to lack of interpretability and trustworthiness which
impedes their usefulness in various applications. In this paper, we consolidate
previous work on categorizing and representing math story problems and develop
MathWorld, which is a graph-based semantic formalism specific for the domain of
math story problems. With MathWorld, we can assign world models to math story
problems which represent the situations and actions introduced in the text and
their mathematical relationships. We combine math story problems from several
existing datasets and annotate a corpus of 1,019 problems and 3,204 logical
forms with MathWorld. Using this data, we demonstrate the following use cases
of MathWorld: (1) prompting language models with synthetically generated
question-answer pairs to probe their reasoning and world modeling abilities,
and (2) generating new problems by using the world models as a design space.",None,-1
baf227fb-0cec-413a-9575-8260d6de821b,Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media,0.264947,"Stance detection predicts attitudes towards targets in texts and has gained
attention with the rise of social media. Traditional approaches include
conventional machine learning, early deep neural networks, and pre-trained
fine-tuning models. However, with the evolution of very large pre-trained
language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face
deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not
requiring backpropagation training, has emerged as a promising alternative.
This paper examines CoT's effectiveness in stance detection tasks,
demonstrating its superior accuracy and discussing associated challenges.",None,-1
77f680fa-650f-4cd0-b540-b9d4d25c5484,A Simple and Flexible Modeling for Mental Disorder Detection by Learning from Clinical Questionnaires,0.407836,"Social media is one of the most highly sought resources for analyzing
characteristics of the language by its users. In particular, many researchers
utilized various linguistic features of mental health problems from social
media. However, existing approaches to detecting mental disorders face critical
challenges, such as the scarcity of high-quality data or the trade-off between
addressing the complexity of models and presenting interpretable results
grounded in expert domain knowledge. To address these challenges, we design a
simple but flexible model that preserves domain-based interpretability. We
propose a novel approach that captures the semantic meanings directly from the
text and compares them to symptom-related descriptions. Experimental results
demonstrate that our model outperforms relevant baselines on various mental
disorder detection tasks. Our detailed analysis shows that the proposed model
is effective at leveraging domain knowledge, transferable to other mental
disorders, and providing interpretable detection results.",None,-1
86e960e6-2228-40c6-8715-0e4518647f05,No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference,0.0664817,"Natural Language Inference (NLI) has been a cornerstone task in evaluating
language models' inferential reasoning capabilities. However, the standard
three-way classification scheme used in NLI has well-known shortcomings in
evaluating models' ability to capture the nuances of natural human reasoning.
In this paper, we argue that the operationalization of the neutral label in
current NLI datasets has low validity, is interpreted inconsistently, and that
at least one important sense of neutrality is often ignored. We uncover the
detrimental impact of these shortcomings, which in some cases leads to
annotation datasets that actually decrease performance on downstream tasks. We
compare approaches of handling annotator disagreement and identify flaws in a
recent NLI dataset that designs an annotator study based on a problematic
operationalization. Our findings highlight the need for a more refined
evaluation framework for NLI, and we hope to spark further discussion and
action in the NLP community.",None,-1
f50c414a-922d-4668-9ee6-27eac91d4839,Revisiting Large Language Models as Zero-shot Relation Extractors,0.882441,"Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.",None,-1
718d98fb-9d2d-41e0-be4d-0ddd77c6e059,Adaptive Planning Search Algorithm for Analog Circuit Verification,0.291826,"Integrated circuit verification has gathered considerable interest in recent
times. Since these circuits keep growing in complexity year by year,
pre-Silicon (pre-SI) verification becomes ever more important, in order to
ensure proper functionality. Thus, in order to reduce the time needed for
manually verifying ICs, we propose a machine learning (ML) approach, which uses
less simulations. This method relies on an initial evaluation set of operating
condition configurations (OCCs), in order to train Gaussian process (GP)
surrogate models. By using surrogate models, we can propose further, more
difficult OCCs. Repeating this procedure for several iterations has shown
better GP estimation of the circuit's responses, on both synthetic and real
circuits, resulting in a better chance of finding the worst case, or even
failures, for certain circuit responses. Thus, we show that the proposed
approach is able to provide OCCs closer to the specifications for all circuits
and identify a failure (specification violation) for one of the responses of a
real circuit.",None,-1
7d1dd196-9eca-4ebc-a9c3-a9c7d885091e,EVA-CLIP: Improved Training Techniques for CLIP at Scale,0.999963,"Contrastive language-image pre-training, CLIP for short, has gained
increasing attention for its potential in various scenarios. In this paper, we
propose EVA-CLIP, a series of models that significantly improve the efficiency
and effectiveness of CLIP training. Our approach incorporates new techniques
for representation learning, optimization, and augmentation, enabling EVA-CLIP
to achieve superior performance compared to previous CLIP models with the same
number of parameters but significantly smaller training costs. Notably, our
largest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples
achieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller
EVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples
achieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open
access and open research, we release the complete suite of EVA-CLIP to the
community at https://github.com/baaivision/EVA/tree/master/EVA-CLIP.",None,-1
2a10a22f-fe98-43b4-a5c2-91000c66bba1,Generative Plug and Play: Posterior Sampling for Inverse Problems,0.747906,"Over the past decade, Plug-and-Play (PnP) has become a popular method for
reconstructing images using a modular framework consisting of a forward and
prior model. The great strength of PnP is that an image denoiser can be used as
a prior model while the forward model can be implemented using more traditional
physics-based approaches. However, a limitation of PnP is that it reconstructs
only a single deterministic image.
  In this paper, we introduce Generative Plug-and-Play (GPnP), a generalization
of PnP to sample from the posterior distribution. As with PnP, GPnP has a
modular framework using a physics-based forward model and an image denoising
prior model. However, in GPnP these models are extended to become proximal
generators, which sample from associated distributions. GPnP applies these
proximal generators in alternation to produce samples from the posterior. We
present experimental simulations using the well-known BM3D denoiser. Our
results demonstrate that the GPnP method is robust, easy to implement, and
produces intuitively reasonable samples from the posterior for sparse
interpolation and tomographic reconstruction. Code to accompany this paper is
available at https://github.com/gbuzzard/generative-pnp-allerton .",None,-1
e9a0bb42-97fa-4349-acc8-b570088d3e9f,Far3D: Expanding the Horizon for Surround-view 3D Object Detection,0.967395,"Recently 3D object detection from surround-view images has made notable
advancements with its low deployment cost. However, most works have primarily
focused on close perception range while leaving long-range detection less
explored. Expanding existing methods directly to cover long distances poses
challenges such as heavy computation costs and unstable convergence. To address
these limitations, this paper proposes a novel sparse query-based framework,
dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D
adaptive queries that complement the 3D global queries. To efficiently capture
discriminative features across different views and scales for long-range
objects, we introduce a perspective-aware aggregation module. Additionally, we
propose a range-modulated 3D denoising approach to address query error
propagation and mitigate convergence issues in long-range tasks. Significantly,
Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset,
covering a wide range of 150 meters, surpassing several LiDAR-based approaches.
Meanwhile, Far3D exhibits superior performance compared to previous methods on
the nuScenes dataset. The code is available at
https://github.com/megvii-research/Far3D.",None,-1
890d503c-d843-4ab2-b9ca-86e85010d9f0,TUTORING: Instruction-Grounded Conversational Agent for Language Learners,0.0620104,"In this paper, we propose Tutoring bot, a generative chatbot trained on a
large scale of tutor-student conversations for English-language learning. To
mimic a human tutor's behavior in language education, the tutor bot leverages
diverse educational instructions and grounds to each instruction as additional
input context for the tutor response generation. As a single instruction
generally involves multiple dialogue turns to give the student sufficient
speaking practice, the tutor bot is required to monitor and capture when the
current instruction should be kept or switched to the next instruction. For
that, the tutor bot is learned to not only generate responses but also infer
its teaching action and progress on the current conversation simultaneously by
a multi-task learning scheme. Our Tutoring bot is deployed under a
non-commercial use license at https://tutoringai.com.",None,-1
8d1c10c0-1f52-4180-b9e8-f99a82d35a3c,Mediapipe and CNNs for Real-Time ASL Gesture Recognition,0.726769,"This research paper describes a realtime system for identifying American Sign
Language (ASL) movements that employs modern computer vision and machine
learning approaches. The suggested method makes use of the Mediapipe library
for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture
classification. The testing results show that the suggested system can detect
all ASL alphabets with an accuracy of 99.95%, indicating its potential for use
in communication devices for people with hearing impairments. The proposed
approach can also be applied to additional sign languages with similar hand
motions, potentially increasing the quality of life for people with hearing
loss. Overall, the study demonstrates the effectiveness of using Mediapipe and
CNN for real-time sign language recognition, making a significant contribution
to the field of computer vision and machine learning.",None,-1
7e1fbc47-82a0-4ed0-8194-00ed52cc440a,CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus,0.0958277,"This article presents a sentence-level sentiment dataset for the Croatian
news domain. In addition to the 3K annotated texts already present, our dataset
contains 14.5K annotated sentence occurrences that have been tagged with 5
classes. We provide baseline scores in addition to the annotation process and
inter-annotator agreement.",None,-1
1357c45d-056e-4c1f-91e4-7a4cc30916d2,Classification of Cross-cultural News Events,0.819908,"We present a methodology to support the analysis of culture from text such as
news events and demonstrate its usefulness on categorizing news events from
different categories (society, business, health, recreation, science, shopping,
sports, arts, computers, games and home) across different geographical
locations (different places in 117 countries). We group countries based on the
culture that they follow and then filter the news events based on their content
category. The news events are automatically labelled with the help of Hofstedes
cultural dimensions. We present combinations of events across different
categories and check the performances of different classification methods. We
also presents experimental comparison of different number of features in order
to find a suitable set to represent the culture.",None,-1
7ff4e1aa-0db1-46b5-a388-f316a0244d3f,ExpertPrompting: Instructing Large Language Models to be Distinguished Experts,0.39323,"The answering quality of an aligned large language model (LLM) can be
drastically improved if treated with proper crafting of prompts. In this paper,
we propose ExpertPrompting to elicit the potential of LLMs to answer as
distinguished experts. We first utilize In-Context Learning to automatically
synthesize detailed and customized descriptions of the expert identity for each
specific instruction, and then ask LLMs to provide answer conditioned on such
agent background. Based on this augmented prompting strategy, we produce a new
set of instruction-following data using GPT-3.5, and train a competitive
open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation
to show that 1) the expert data is of significantly higher quality than vanilla
answers, and 2) ExpertLLaMA outperforms existing open-source opponents and
achieves 96\% of the original ChatGPT's capability. All data and the
ExpertLLaMA model will be made publicly available at
\url{https://github.com/OFA-Sys/ExpertLLaMA}.",None,-1
48b590cf-9439-4e8a-b55e-c67204259cfa,Attack Named Entity Recognition by Entity Boundary Interference,0.375781,"Named Entity Recognition (NER) is a cornerstone NLP task while its robustness
has been given little attention. This paper rethinks the principles of NER
attacks derived from sentence classification, as they can easily violate the
label consistency between the original and adversarial NER examples. This is
due to the fine-grained nature of NER, as even minor word changes in the
sentence can result in the emergence or mutation of any entities, resulting in
invalid adversarial examples. To this end, we propose a novel one-word
modification NER attack based on a key insight, NER models are always
vulnerable to the boundary position of an entity to make their decision. We
thus strategically insert a new boundary into the sentence and trigger the
Entity Boundary Interference that the victim model makes the wrong prediction
either on this boundary word or on other words in the sentence. We call this
attack Virtual Boundary Attack (ViBA), which is shown to be remarkably
effective when attacking both English and Chinese models with a 70%-90% attack
success rate on state-of-the-art language models (e.g. RoBERTa, DeBERTa) and
also significantly faster than previous methods.",None,-1
f29018c2-05a5-4940-83ce-f5bba204f254,DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers,0.444631,"We propose a novel talking head synthesis pipeline called ""DiT-Head"", which
is based on diffusion transformers and uses audio as a condition to drive the
denoising process of a diffusion model. Our method is scalable and can
generalise to multiple identities while producing high-quality results. We
train and evaluate our proposed approach and compare it against existing
methods of talking head synthesis. We show that our model can compete with
these methods in terms of visual quality and lip-sync accuracy. Our results
highlight the potential of our proposed approach to be used for a wide range of
applications, including virtual assistants, entertainment, and education. For a
video demonstration of the results and our user study, please refer to our
supplementary material.",None,-1
a7bd7c3e-af1e-4411-84d6-92d742e717b2,Learning by Self-Explaining,0.0603785,"Current AI research mainly treats explanations as a means for model
inspection. Yet, this neglects findings from human psychology that describe the
benefit of self-explanations in an agent's learning process. Motivated by this,
we introduce a novel approach in the context of image classification, termed
Learning by Self-Explaining (LSX). LSX utilizes aspects of self-refining AI and
human-guided explanatory machine learning. The underlying idea is that a
learner model, in addition to optimizing for the original predictive task, is
further optimized based on explanatory feedback from an internal critic model.
Intuitively, a learner's explanations are considered ""useful"" if the internal
critic can perform the same task given these explanations. We provide an
overview of important components of LSX and, based on this, perform extensive
experimental evaluations via three different example instantiations. Our
results indicate improvements via Learning by Self-Explaining on several
levels: in terms of model generalization, reducing the influence of confounding
factors, and providing more task-relevant and faithful model explanations.
Overall, our work provides evidence for the potential of self-explaining within
the learning phase of an AI model.",None,-1
35be039a-2560-4016-a2a6-09e9a14b3855,Can Contextual Biasing Remain Effective with Whisper and GPT-2?,0.848173,"End-to-end automatic speech recognition (ASR) and large language models, such
as Whisper and GPT-2, have recently been scaled to use vast amounts of training
data. Despite the large amount of training data, infrequent content words that
occur in a particular task may still exhibit poor ASR performance, with
contextual biasing a possible remedy. This paper investigates the effectiveness
of neural contextual biasing for Whisper combined with GPT-2. Specifically,
this paper proposes integrating an adapted tree-constrained pointer generator
(TCPGen) component for Whisper and a dedicated training scheme to dynamically
adjust the final output without modifying any Whisper model parameters.
Experiments across three datasets show a considerable reduction in errors on
biasing words with a biasing list of 1000 words. Contextual biasing was more
effective when applied to domain-specific data and can boost the performance of
Whisper and GPT-2 without losing their generality.",None,-1
3f947129-6a97-46d2-920c-4c27773335f9,SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks,0.983921,"SALMA, the first Arabic sense-annotated corpus, consists of ~34K tokens,
which are all sense-annotated. The corpus is annotated using two different
sense inventories simultaneously (Modern and Ghani). SALMA novelty lies in how
tokens and senses are associated. Instead of linking a token to only one
intended sense, SALMA links a token to multiple senses and provides a score to
each sense. A smart web-based annotation tool was developed to support scoring
multiple senses against a given word. In addition to sense annotations, we also
annotated the corpus using six types of named entities. The quality of our
annotations was assessed using various metrics (Kappa, Linear Weighted Kappa,
Quadratic Weighted Kappa, Mean Average Error, and Root Mean Square Error),
which show very high inter-annotator agreement. To establish a Word Sense
Disambiguation baseline using our SALMA corpus, we developed an end-to-end Word
Sense Disambiguation system using Target Sense Verification. We used this
system to evaluate three Target Sense Verification models available in the
literature. Our best model achieved an accuracy with 84.2% using Modern and
78.7% using Ghani. The full corpus and the annotation tool are open-source and
publicly available at https://sina.birzeit.edu/salma/.",None,-1
eb26b3e1-30b4-4518-9c64-2a3966b0bfe6,Noise-Robust Dense Retrieval via Contrastive Alignment Post Training,0.103225,"The success of contextual word representations and advances in neural
information retrieval have made dense vector-based retrieval a standard
approach for passage and document ranking. While effective and efficient,
dual-encoders are brittle to variations in query distributions and noisy
queries. Data augmentation can make models more robust but introduces overhead
to training set generation and requires retraining and index regeneration. We
present Contrastive Alignment POst Training (CAPOT), a highly efficient
finetuning method that improves model robustness without requiring index
regeneration, the training set optimization, or alteration. CAPOT enables
robust retrieval by freezing the document encoder while the query encoder
learns to align noisy queries with their unaltered root. We evaluate CAPOT
noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,
finding CAPOT has a similar impact as data augmentation with none of its
overhead.",None,-1
0c32c653-bb50-4c49-af44-b9db08cf5128,Feature-Rich Audio Model Inversion for Data-Free Knowledge Distillation Towards General Sound Classification,0.376754,"Data-Free Knowledge Distillation (DFKD) has recently attracted growing
attention in the academic community, especially with major breakthroughs in
computer vision. Despite promising results, the technique has not been well
applied to audio and signal processing. Due to the variable duration of audio
signals, it has its own unique way of modeling. In this work, we propose
feature-rich audio model inversion (FRAMI), a data-free knowledge distillation
framework for general sound classification tasks. It first generates
high-quality and feature-rich Mel-spectrograms through a feature-invariant
contrastive loss. Then, the hidden states before and after the statistics
pooling layer are reused when knowledge distillation is performed on these
feature-rich samples. Experimental results on the Urbansound8k, ESC-50, and
audioMNIST datasets demonstrate that FRAMI can generate feature-rich samples.
Meanwhile, the accuracy of the student model is further improved by reusing the
hidden state and significantly outperforms the baseline method.",None,-1
8f55781f-c960-4b9e-8416-fe58fb757667,A Coreset Learning Reality Check,0.108591,"Subsampling algorithms are a natural approach to reduce data size before
fitting models on massive datasets. In recent years, several works have
proposed methods for subsampling rows from a data matrix while maintaining
relevant information for classification. While these works are supported by
theory and limited experiments, to date there has not been a comprehensive
evaluation of these methods. In our work, we directly compare multiple methods
for logistic regression drawn from the coreset and optimal subsampling
literature and discover inconsistencies in their effectiveness. In many cases,
methods do not outperform simple uniform subsampling.",None,-1
2d65e8bf-1915-444e-9f30-9e45506b1f50,The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks,0.997611,"In the realm of Computational Social Science (CSS), practitioners often
navigate complex, low-resource domains and face the costly and time-intensive
challenges of acquiring and annotating data. We aim to establish a set of
guidelines to address such challenges, comparing the use of human-labeled data
with synthetically generated data from GPT-4 and Llama-2 in ten distinct CSS
classification tasks of varying complexity. Additionally, we examine the impact
of training data sizes on performance. Our findings reveal that models trained
on human-labeled data consistently exhibit superior or comparable performance
compared to their synthetically augmented counterparts. Nevertheless, synthetic
augmentation proves beneficial, particularly in improving performance on rare
classes within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2
for zero-shot classification and find that, while they generally display strong
performance, they often fall short when compared to specialized classifiers
trained on moderately sized training sets.",None,-1
3523b203-40d0-4ccf-bc02-727adabef32f,Going faster to see further: GPU-accelerated value iteration and simulation for perishable inventory control using JAX,0.396407,"Value iteration can find the optimal replenishment policy for a perishable
inventory problem, but is computationally demanding due to the large state
spaces that are required to represent the age profile of stock. The parallel
processing capabilities of modern GPUs can reduce the wall time required to run
value iteration by updating many states simultaneously. The adoption of
GPU-accelerated approaches has been limited in operational research relative to
other fields like machine learning, in which new software frameworks have made
GPU programming widely accessible. We used the Python library JAX to implement
value iteration and simulators of the underlying Markov decision processes in a
high-level API, and relied on this library's function transformations and
compiler to efficiently utilize GPU hardware. Our method can extend use of
value iteration to settings that were previously considered infeasible or
impractical. We demonstrate this on example scenarios from three recent studies
which include problems with over 16 million states and additional problem
features, such as substitution between products, that increase computational
complexity. We compare the performance of the optimal replenishment policies to
heuristic policies, fitted using simulation optimization in JAX which allowed
the parallel evaluation of multiple candidate policy parameters on thousands of
simulated years. The heuristic policies gave a maximum optimality gap of 2.49%.
Our general approach may be applicable to a wide range of problems in
operational research that would benefit from large-scale parallel computation
on consumer-grade GPU hardware.",None,-1
357e1acc-c4b0-49d6-a539-fa6d1c7d053e,Thinker: Learning to Plan and Act,0.0349272,"We propose the Thinker algorithm, a novel approach that enables reinforcement
learning agents to autonomously interact with and utilize a learned world
model. The Thinker algorithm wraps the environment with a world model and
introduces new actions designed for interacting with the world model. These
model-interaction actions enable agents to perform planning by proposing
alternative plans to the world model before selecting a final action to execute
in the environment. This approach eliminates the need for handcrafted planning
algorithms by enabling the agent to learn how to plan autonomously and allows
for easy interpretation of the agent's plan with visualization. We demonstrate
the algorithm's effectiveness through experimental results in the game of
Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves
state-of-the-art performance and competitive results, respectively.
Visualizations of agents trained with the Thinker algorithm demonstrate that
they have learned to plan effectively with the world model to select better
actions. Thinker is the first work showing that an RL agent can learn to plan
with a learned world model in complex environments.",None,-1
d0b6de57-42d4-413b-aafb-f83795876a60,Limits for Learning with Language Models,0.211659,"With the advent of large language models (LLMs), the trend in NLP has been to
train LLMs on vast amounts of data to solve diverse language understanding and
generation tasks. The list of LLM successes is long and varied. Nevertheless,
several recent papers provide empirical evidence that LLMs fail to capture
important aspects of linguistic meaning. Focusing on universal quantification,
we provide a theoretical foundation for these empirical findings by proving
that LLMs cannot learn certain fundamental semantic properties including
semantic entailment and consistency as they are defined in formal semantics.
More generally, we show that LLMs are unable to learn concepts beyond the first
level of the Borel Hierarchy, which imposes severe limits on the ability of
LMs, both large and small, to capture many aspects of linguistic meaning. This
means that LLMs will continue to operate without formal guarantees on tasks
that require entailments and deep linguistic understanding.",None,-1
54661847-613f-4fe3-8e72-175336262b79,Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance,0.980599,"Recent advances in many domains require more and more complicated experiment
design. Such complicated experiments often have many parameters, which
necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a
Bayesian optimization method, is widely used in recent parameter tuning
frameworks. Despite its popularity, the roles of each control parameter and the
algorithm intuition have not been discussed so far. In this tutorial, we will
identify the roles of each control parameter and their impacts on
hyperparameter optimization using a diverse set of benchmarks. We compare our
recommended setting drawn from the ablation study with baseline methods and
demonstrate that our recommended setting improves the performance of TPE. Our
TPE implementation is available at
https://github.com/nabenabe0928/tpe/tree/single-opt.",None,-1
948b3212-ba64-46fe-8767-c2008c841125,DQNAS: Neural Architecture Search using Reinforcement Learning,0.151683,"Convolutional Neural Networks have been used in a variety of image related
applications after their rise in popularity due to ImageNet competition.
Convolutional Neural Networks have shown remarkable results in applications
including face recognition, moving target detection and tracking,
classification of food based on the calorie content and many more. Designing of
Convolutional Neural Networks requires experts having a cross domain knowledge
and it is laborious, which requires a lot of time for testing different values
for different hyperparameter along with the consideration of different
configurations of existing architectures. Neural Architecture Search is an
automated way of generating Neural Network architectures which saves
researchers from all the brute-force testing trouble, but with the drawback of
consuming a lot of computational resources for a prolonged period. In this
paper, we propose an automated Neural Architecture Search framework DQNAS,
guided by the principles of Reinforcement Learning along with One-shot Training
which aims to generate neural network architectures that show superior
performance and have minimum scalability problem.",None,-1
ddf30d03-6964-4f03-b5cc-cea043f82971,Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction,0.782246,"Relation extraction (RE) aims to extract potential relations according to the
context of two entities, thus, deriving rational contexts from sentences plays
an important role. Previous works either focus on how to leverage the entity
information (e.g., entity types, entity verbalization) to inference relations,
but ignore context-focused content, or use counterfactual thinking to remove
the model's bias of potential relations in entities, but the relation reasoning
process will still be hindered by irrelevant content. Therefore, how to
preserve relevant content and remove noisy segments from sentences is a crucial
task. In addition, retained content needs to be fluent enough to maintain
semantic coherence and interpretability. In this work, we propose a novel
rationale extraction framework named RE2, which leverages two continuity and
sparsity factors to obtain relevant and coherent rationales from sentences. To
solve the problem that the gold rationales are not labeled, RE2 applies an
optimizable binary mask to each token in the sentence, and adjust the
rationales that need to be selected according to the relation label.
Experiments on four datasets show that RE2 surpasses baselines.",None,-1
1f3552ae-840b-4102-9459-630a7870f216,Language Conditioned Traffic Generation,0.827904,"Simulation forms the backbone of modern self-driving development. Simulators
help develop, test, and improve driving systems without putting humans,
vehicles, or their environment at risk. However, simulators face a major
challenge: They rely on realistic, scalable, yet interesting content. While
recent advances in rendering and scene reconstruction make great strides in
creating static scene assets, modeling their layout, dynamics, and behaviors
remains challenging. In this work, we turn to language as a source of
supervision for dynamic traffic scene generation. Our model, LCTGen, combines a
large language model with a transformer-based decoder architecture that selects
likely map locations from a dataset of maps, and produces an initial traffic
distribution, as well as the dynamics of each vehicle. LCTGen outperforms prior
work in both unconditional and conditional traffic scene generation in terms of
realism and fidelity. Code and video will be available at
https://ariostgx.github.io/lctgen.",None,-1
20ca83a5-6f8b-4347-982a-4a6c7c7c9b31,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,0.999979,"Multi-modal large language models are regarded as a crucial step towards
Artificial General Intelligence (AGI) and have garnered significant interest
with the emergence of ChatGPT. However, current speech-language models
typically adopt the cascade paradigm, preventing inter-modal knowledge
transfer. In this paper, we propose SpeechGPT, a large language model with
intrinsic cross-modal conversational abilities, capable of perceiving and
generating multi-model content. With discrete speech representations, we first
construct SpeechInstruct, a large-scale cross-modal speech instruction dataset.
Additionally, we employ a three-stage training strategy that includes
modality-adaptation pre-training, cross-modal instruction fine-tuning, and
chain-of-modality instruction fine-tuning. The experimental results demonstrate
that SpeechGPT has an impressive capacity to follow multi-modal human
instructions and highlight the potential of handling multiple modalities with
one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",None,-1
b99065e6-7848-4ee7-81de-a6543b20b833,Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints,0.636655,"Local feature extraction is a standard approach in computer vision for
tackling important tasks such as image matching and retrieval. The core
assumption of most methods is that images undergo affine transformations,
disregarding more complicated effects such as non-rigid deformations.
Furthermore, incipient works tailored for non-rigid correspondence still rely
on keypoint detectors designed for rigid transformations, hindering performance
due to the limitations of the detector. We propose DALF (Deformation-Aware
Local Features), a novel deformation-aware network for jointly detecting and
describing keypoints, to handle the challenging problem of matching deformable
surfaces. All network components work cooperatively through a feature fusion
approach that enforces the descriptors' distinctiveness and invariance.
Experiments using real deforming objects showcase the superiority of our
method, where it delivers 8% improvement in matching scores compared to the
previous best results. Our approach also enhances the performance of two
real-world applications: deformable object retrieval and non-rigid 3D surface
registration. Code for training, inference, and applications are publicly
available at https://verlab.dcc.ufmg.br/descriptors/dalf_cvpr23.",None,-1
88d8384c-95f0-4fc8-bde8-bf160ad31bdb,Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints,0.355432,"Learning robust local image feature matching is a fundamental low-level
vision task, which has been widely explored in the past few years. Recently,
detector-free local feature matchers based on transformers have shown promising
results, which largely outperform pure Convolutional Neural Network (CNN) based
ones. But correlations produced by transformer-based methods are spatially
limited to the center of source views' coarse patches, because of the costly
attention learning. In this work, we rethink this issue and find that such
matching formulation degrades pose estimation, especially for low-resolution
images. So we propose a transformer-based cascade matching model -- Cascade
feature Matching TRansformer (CasMTR), to efficiently learn dense feature
correlations, which allows us to choose more reliable matching pairs for the
relative pose estimation. Instead of re-training a new detector, we use a
simple yet effective Non-Maximum Suppression (NMS) post-process to filter
keypoints through the confidence map, and largely improve the matching
precision. CasMTR achieves state-of-the-art performance in indoor and outdoor
pose estimation as well as visual localization. Moreover, thorough ablations
show the efficacy of the proposed components and techniques.",None,-1
04e9baa7-8772-43aa-b3e0-a093731c322b,HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation,0.107469,"To fully leverage the advantages of large-scale pre-trained language models
(PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to
fine-tune the entire parameters of PLMs. However, this paradigm poses issues of
inefficient updating and resource over-consuming for fine-tuning in data-scarce
and resource-limited scenarios, because of the large scale of parameters in
PLMs. To alleviate these concerns, in this paper, we propose a
parameter-efficient fine-tuning method HiFi, that is, only the highly
informative and strongly correlated attention heads for the specific task are
fine-tuned. To search for those significant attention heads, we develop a novel
framework to analyze the effectiveness of heads. Specifically, we first model
the relationship between heads into a graph from two perspectives of
information richness and correlation, and then apply PageRank algorithm to
determine the relative importance of each head. Extensive experiments on the
GLUE benchmark demonstrate the effectiveness of our method, and show that HiFi
obtains state-of-the-art performance over the prior baselines.",None,-1
a2bc17b4-dec0-4613-811d-b9b6b6713012,Reinforcement Learning with Human Feedback for Realistic Traffic Simulation,0.50789,"In light of the challenges and costs of real-world testing, autonomous
vehicle developers often rely on testing in simulation for the creation of
reliable systems. A key element of effective simulation is the incorporation of
realistic traffic models that align with human knowledge, an aspect that has
proven challenging due to the need to balance realism and diversity. This works
aims to address this by developing a framework that employs reinforcement
learning with human preference (RLHF) to enhance the realism of existing
traffic models. This study also identifies two main challenges: capturing the
nuances of human preferences on realism and the unification of diverse traffic
simulation models. To tackle these issues, we propose using human feedback for
alignment and employ RLHF due to its sample efficiency. We also introduce the
first dataset for realism alignment in traffic modeling to support such
research. Our framework, named TrafficRLHF, demonstrates its proficiency in
generating realistic traffic scenarios that are well-aligned with human
preferences, as corroborated by comprehensive evaluations on the nuScenes
dataset.",None,-1
8809e893-6b9f-4044-96a2-ddd64189b999,Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis,0.0996835,"Generative Engineering Design approaches driven by Deep Generative Models
(DGM) have been proposed to facilitate industrial engineering processes. In
such processes, designs often come in the form of images, such as blueprints,
engineering drawings, and CAD models depending on the level of detail. DGMs
have been successfully employed for synthesis of natural images, e.g.,
displaying animals, human faces and landscapes. However, industrial design
images are fundamentally different from natural scenes in that they contain
rich structural patterns and long-range dependencies, which are challenging for
convolution-based DGMs to generate. Moreover, DGM-driven generation process is
typically triggered based on random noisy inputs, which outputs unpredictable
samples and thus cannot perform an efficient industrial design exploration. We
tackle these challenges by proposing a novel model Self-Attention Adversarial
Latent Autoencoder (SA-ALAE), which allows generating feasible design images of
complex engineering parts. With SA-ALAE, users can not only explore novel
variants of an existing design, but also control the generation process by
operating in latent space. The potential of SA-ALAE is shown by generating
engineering blueprints in a real automotive design task.",None,-1
d92b324a-5ec3-4c59-8309-ad48f428ce94,Event Camera Data Pre-training,0.879907,"This paper proposes a pre-trained neural network for handling event camera
data. Our model is a self-supervised learning framework, and uses paired event
camera data and natural RGB images for training.
  Our method contains three modules connected in a sequence: i) a family of
event data augmentations, generating meaningful event images for
self-supervised training; ii) a conditional masking strategy to sample
informative event patches from event images, encouraging our model to capture
the spatial layout of a scene and accelerating training; iii) a contrastive
learning approach, enforcing the similarity of embeddings between matching
event images, and between paired event and RGB images. An embedding projection
loss is proposed to avoid the model collapse when enforcing the event image
embedding similarities. A probability distribution alignment loss is proposed
to encourage the event image to be consistent with its paired RGB image in the
feature space.
  Transfer learning performance on downstream tasks shows the superiority of
our method over state-of-the-art methods. For example, we achieve top-1
accuracy at 64.83% on the N-ImageNet dataset.",None,-1
d6ae0953-05af-43ad-9e83-cbbd54535ce5,SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation,0.556332,"In this paper, we investigate the vulnerability of MDE to adversarial
patches. We propose a novel \underline{S}tealthy \underline{A}dversarial
\underline{A}ttacks on \underline{M}DE (SAAM) that compromises MDE by either
corrupting the estimated distance or causing an object to seamlessly blend into
its surroundings. Our experiments, demonstrate that the designed stealthy patch
successfully causes a DNN-based MDE to misestimate the depth of objects. In
fact, our proposed adversarial patch achieves a significant 60\% depth error
with 99\% ratio of the affected region. Importantly, despite its adversarial
nature, the patch maintains a naturalistic appearance, making it inconspicuous
to human observers. We believe that this work sheds light on the threat of
adversarial attacks in the context of MDE on edge devices. We hope it raises
awareness within the community about the potential real-life harm of such
attacks and encourages further research into developing more robust and
adaptive defense mechanisms.",None,-1
69c42f4a-bfda-471c-991b-54f7619da819,Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task,0.337034,"The evolution of Generative Pre-trained Transformer (GPT) models has led to
significant advancements in various natural language processing applications,
particularly in legal textual entailment. We present an analysis of GPT-3.5
(ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominent
benchmark in this domain. The study encompasses data from Heisei 18 (2006) to
Reiwa 3 (2021), exploring the models' abilities to discern entailment
relationships within Japanese statute law across different periods. Our
preliminary experimental results unveil intriguing insights into the models'
strengths and weaknesses in handling legal textual entailment tasks, as well as
the patterns observed in model performance. In the context of proprietary
models with undisclosed architectures and weights, black-box analysis becomes
crucial for evaluating their capabilities. We discuss the influence of training
data distribution and the implications on the models' generalizability. This
analysis serves as a foundation for future research, aiming to optimize
GPT-based models and enable their successful adoption in legal information
extraction and entailment applications.",None,-1
0b3be4a4-49ff-45ea-aae6-1583f45b3e81,RQAT-INR: Improved Implicit Neural Image Compression,0.620297,"Deep variational autoencoders for image and video compression have gained
significant attraction in the recent years, due to their potential to offer
competitive or better compression rates compared to the decades long
traditional codecs such as AVC, HEVC or VVC. However, because of complexity and
energy consumption, these approaches are still far away from practical usage in
industry. More recently, implicit neural representation (INR) based codecs have
emerged, and have lower complexity and energy usage to classical approaches at
decoding. However, their performances are not in par at the moment with
state-of-the-art methods. In this research, we first show that INR based image
codec has a lower complexity than VAE based approaches, then we propose several
improvements for INR-based image codec and outperformed baseline model by a
large margin.",None,-1
e6b3ffbf-6cf9-41ed-8ed5-dccf80307971,Multilingual End to End Entity Linking,0.41676,"Entity Linking is one of the most common Natural Language Processing tasks in
practical applications, but so far efficient end-to-end solutions with
multilingual coverage have been lacking, leading to complex model stacks. To
fill this gap, we release and open source BELA, the first fully end-to-end
multilingual entity linking model that efficiently detects and links entities
in texts in any of 97 languages. We provide here a detailed description of the
model and report BELA's performance on four entity linking datasets covering
high- and low-resource languages.",None,-1
b050a7dc-28de-4d98-a85d-5f90b6108d6a,On the Planning Abilities of Large Language Models : A Critical Investigation,0.984913,"Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating
plans autonomously in commonsense planning tasks and (2) the potential of LLMs
in LLM-Modulo settings where they act as a source of heuristic guidance for
external planners and verifiers. We conduct a systematic study by generating a
suite of instances on domains similar to the ones employed in the International
Planning Competition and evaluate LLMs in two distinct modes: autonomous and
heuristic. Our findings reveal that LLMs' ability to generate executable plans
autonomously is rather limited, with the best model (GPT-4) having an average
success rate of ~12% across the domains. However, the results in the LLM-Modulo
setting show more promise. In the LLM-Modulo setting, we demonstrate that
LLM-generated plans can improve the search process for underlying sound
planners and additionally show that external verifiers can help provide
feedback on the generated plans and back-prompt the LLM for better plan
generation.",None,-1
1906b87f-d3b0-45f0-95ea-24f5dd642451,Transformer-based World Models Are Happy With 100k Interactions,0.685862,"Deep neural networks have been successful in many reinforcement learning
settings. However, compared to human learners they are overly data hungry. To
build a sample-efficient world model, we apply a transformer to real-world
episodes in an autoregressive manner: not only the compact latent states and
the taken actions but also the experienced or predicted rewards are fed into
the transformer, so that it can attend flexibly to all three modalities at
different time steps. The transformer allows our world model to access previous
states directly, instead of viewing them through a compressed recurrent state.
By utilizing the Transformer-XL architecture, it is able to learn long-term
dependencies while staying computationally efficient. Our transformer-based
world model (TWM) generates meaningful, new experience, which is used to train
a policy that outperforms previous model-free and model-based reinforcement
learning algorithms on the Atari 100k benchmark.",None,-1
3569c34c-daa3-4f96-b6b0-0660028eeaac,Adversarial Capsule Networks for Romanian Satire Detection and Sentiment Analysis,0.0515573,"Satire detection and sentiment analysis are intensively explored natural
language processing (NLP) tasks that study the identification of the satirical
tone from texts and extracting sentiments in relationship with their targets.
In languages with fewer research resources, an alternative is to produce
artificial examples based on character-level adversarial processes to overcome
dataset size limitations. Such samples are proven to act as a regularization
method, thus improving the robustness of models. In this work, we improve the
well-known NLP models (i.e., Convolutional Neural Networks, Long Short-Term
Memory (LSTM), Bidirectional LSTM, Gated Recurrent Units (GRUs), and
Bidirectional GRUs) with adversarial training and capsule networks. The
fine-tuned models are used for satire detection and sentiment analysis tasks in
the Romanian language. The proposed framework outperforms the existing methods
for the two tasks, achieving up to 99.08% accuracy, thus confirming the
improvements added by the capsule layers and the adversarial training in NLP
approaches.",None,-1
74ec965a-f516-4bb3-ae22-f32728a7858d,Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval,0.304856,"Recently multi-lingual pre-trained language models (PLM) such as mBERT and
XLM-R have achieved impressive strides in cross-lingual dense retrieval.
Despite its successes, they are general-purpose PLM while the multilingual PLM
tailored for cross-lingual retrieval is still unexplored. Motivated by an
observation that the sentences in parallel documents are approximately in the
same order, which is universal across languages, we propose to model this
sequential sentence relation to facilitate cross-lingual representation
learning. Specifically, we propose a multilingual PLM called masked sentence
model (MSM), which consists of a sentence encoder to generate the sentence
representations, and a document encoder applied to a sequence of sentence
vectors from a document. The document encoder is shared for all languages to
model the universal sequential sentence relation across languages. To train the
model, we propose a masked sentence prediction task, which masks and predicts
the sentence vector via a hierarchical contrastive loss with sampled negatives.
Comprehensive experiments on four cross-lingual retrieval tasks show MSM
significantly outperforms existing advanced pre-training models, demonstrating
the effectiveness and stronger cross-lingual retrieval capabilities of our
approach. Code and model will be available.",None,-1
83c06d41-c31a-4461-b563-4873c61eb9d0,Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization,0.65134,"Multi-document summarization (MDS) aims to generate a summary for a number of
related documents. We propose HGSUM, an MDS model that extends an
encoder-decoder architecture, to incorporate a heterogeneous graph to represent
different semantic units (e.g., words and sentences) of the documents. This
contrasts with existing MDS models which do not consider different edge types
of graphs and as such do not capture the diversity of relationships in the
documents. To preserve only key information and relationships of the documents
in the heterogeneous graph, HGSUM uses graph pooling to compress the input
graph. And to guide HGSUM to learn compression, we introduce an additional
objective that maximizes the similarity between the compressed graph and the
graph constructed from the ground-truth summary during training. HGSUM is
trained end-to-end with graph similarity and standard cross-entropy objectives.
Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM
outperforms state-of-the-art MDS models. The code for our model and experiments
is available at: https://github.com/oaimli/HGSum.",None,-1
35c5ece9-7ce0-44d9-9cf4-68535ee2c790,Learning Empirical Bregman Divergence for Uncertain Distance Representation,0.0917093,"Deep metric learning techniques have been used for visual representation in
various supervised and unsupervised learning tasks through learning embeddings
of samples with deep networks. However, classic approaches, which employ a
fixed distance metric as a similarity function between two embeddings, may lead
to suboptimal performance for capturing the complex data distribution. The
Bregman divergence generalizes measures of various distance metrics and arises
throughout many fields of deep metric learning. In this paper, we first show
how deep metric learning loss can arise from the Bregman divergence. We then
introduce a novel method for learning empirical Bregman divergence directly
from data based on parameterizing the convex function underlying the Bregman
divergence with a deep learning setting. We further experimentally show that
our approach performs effectively on five popular public datasets compared to
other SOTA deep metric learning methods, particularly for pattern recognition
problems.",None,-1
be866603-f4e1-469b-baae-e8b68b9bf8d3,NOPE: Novel Object Pose Estimation from a Single Image,0.633687,"The practicality of 3D object pose estimation remains limited for many
applications due to the need for prior knowledge of a 3D model and a training
period for new objects. To address this limitation, we propose an approach that
takes a single image of a new object as input and predicts the relative pose of
this object in new images without prior knowledge of the object's 3D model and
without requiring training time for new objects and categories. We achieve this
by training a model to directly predict discriminative embeddings for
viewpoints surrounding the object. This prediction is done using a simple U-Net
architecture with attention and conditioned on the desired pose, which yields
extremely fast inference. We compare our approach to state-of-the-art methods
and show it outperforms them both in terms of accuracy and robustness. Our
source code is publicly available at https://github.com/nv-nguyen/nope",None,-1
05af65aa-96ff-4cd7-97d3-2794a8b4fc87,Adversarial Training For Low-Resource Disfluency Correction,0.443974,"Disfluencies commonly occur in conversational speech. Speech with
disfluencies can result in noisy Automatic Speech Recognition (ASR)
transcripts, which affects downstream tasks like machine translation. In this
paper, we propose an adversarially-trained sequence-tagging model for
Disfluency Correction (DC) that utilizes a small amount of labeled real
disfluent data in conjunction with a large amount of unlabeled data. We show
the benefit of our proposed technique, which crucially depends on synthetically
generated disfluent data, by evaluating it for DC in three Indian languages-
Bengali, Hindi, and Marathi (all from the Indo-Aryan family). Our technique
also performs well in removing stuttering disfluencies in ASR transcripts
introduced by speech impairments. We achieve an average 6.15 points improvement
in F1-score over competitive baselines across all three languages mentioned. To
the best of our knowledge, we are the first to utilize adversarial training for
DC and use it to correct stuttering disfluencies in English, establishing a new
benchmark for this task.",None,-1
053617b5-4e95-4d65-bcd2-cb44d2de2507,Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning,0.339732,"Dialogue state tracking (DST) is an important step in dialogue management to
keep track of users' beliefs. Existing works fine-tune all language model (LM)
parameters to tackle the DST task, which requires significant data and
computing resources for training and hosting. The cost grows exponentially in
the real-world deployment where dozens of fine-tuned LM are used for different
domains and tasks. To reduce parameter size and better utilize cross-task
shared information, we propose to use soft prompt token embeddings to learn
task properties. Without tuning LM parameters, our method drastically reduces
the number of parameters needed to less than 0.5% of prior works while achieves
better low-resource DST performance.",None,-1
aa5a34bc-14e1-4101-ac30-200c3942f933,JCoLA: Japanese Corpus of Linguistic Acceptability,0.156283,"Neural language models have exhibited outstanding performance in a range of
downstream tasks. However, there is limited understanding regarding the extent
to which these models internalize syntactic knowledge, so that various datasets
have recently been constructed to facilitate syntactic evaluation of language
models across languages. In this paper, we introduce JCoLA (Japanese Corpus of
Linguistic Acceptability), which consists of 10,020 sentences annotated with
binary acceptability judgments. Specifically, those sentences are manually
extracted from linguistics textbooks, handbooks and journal articles, and split
into in-domain data (86 %; relatively simple acceptability judgments extracted
from textbooks and handbooks) and out-of-domain data (14 %; theoretically
significant acceptability judgments extracted from journal articles), the
latter of which is categorized by 12 linguistic phenomena. We then evaluate the
syntactic knowledge of 9 different types of Japanese language models on JCoLA.
The results demonstrated that several models could surpass human performance
for the in-domain data, while no models were able to exceed human performance
for the out-of-domain data. Error analyses by linguistic phenomena further
revealed that although neural language models are adept at handling local
syntactic dependencies like argument structure, their performance wanes when
confronted with long-distance syntactic dependencies like verbal agreement and
NPI licensing.",None,-1
323d74eb-edcb-4574-a437-834bb1b1d1c3,Memory Encoding Model,0.380623,"We explore a new class of brain encoding model by adding memory-related
information as input. Memory is an essential brain mechanism that works
alongside visual stimuli. During a vision-memory cognitive task, we found the
non-visual brain is largely predictable using previously seen images. Our
Memory Encoding Model (Mem) won the Algonauts 2023 visual brain competition
even without model ensemble (single model score 66.8, ensemble score 70.8). Our
ensemble model without memory input (61.4) can also stand a 3rd place.
Furthermore, we observe periodic delayed brain response correlated to 6th-7th
prior image, and hippocampus also showed correlated activity timed with this
periodicity. We conjuncture that the periodic replay could be related to memory
mechanism to enhance the working memory.",None,-1
6860e7ff-3310-4d6a-902f-d2a94e225f82,Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition,0.610264,"The development of robust, generalized models in human activity recognition
(HAR) has been hindered by the scarcity of large-scale, labeled data sets.
Recent work has shown that virtual IMU data extracted from videos using
computer vision techniques can lead to substantial performance improvements
when training HAR models combined with small portions of real IMU data.
Inspired by recent advances in motion synthesis from textual descriptions and
connecting Large Language Models (LLMs) to various AI models, we introduce an
automated pipeline that first uses ChatGPT to generate diverse textual
descriptions of activities. These textual descriptions are then used to
generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and
later converted to streams of virtual IMU data. We benchmarked our approach on
three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the
use of virtual IMU training data generated using our new approach leads to
significantly improved HAR model performance compared to only using real IMU
data. Our approach contributes to the growing field of cross-modality transfer
methods and illustrate how HAR models can be improved through the generation of
virtual training data that do not require any manual effort.",None,-1
341a20c7-1db8-41bc-a799-c8e5702d294b,VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs,0.522479,"Deep Learning (DL) acceleration support in CPUs has recently gained a lot of
traction, with several companies (Arm, Intel, IBM) announcing products with
specialized matrix engines accessible via GEMM instructions. CPUs are pervasive
and need to handle diverse requirements across DL workloads running in
edge/HPC/cloud platforms. Therefore, as DL workloads embrace sparsity to reduce
the computations and memory size of models, it is also imperative for CPUs to
add support for sparsity to avoid under-utilization of the dense matrix engine
and inefficient usage of the caches and registers. This work presents VEGETA, a
set of ISA and microarchitecture extensions over dense matrix engines to
support flexible structured sparsity for CPUs, enabling programmable support
for diverse DL models with varying degrees of sparsity. Compared to the
state-of-the-art (SOTA) dense matrix engine in CPUs, a VEGETA engine provides
1.09x, 2.20x, 3.74x, and 3.28x speed-ups when running 4:4 (dense), 2:4, 1:4,
and unstructured (95%) sparse DNN layers.",None,-1
46881e9b-d3ab-4e68-a49c-051ad0dbe69e,GlyphControl: Glyph Conditional Control for Visual Text Generation,0.355567,"Recently, there has been an increasing interest in developing diffusion-based
text-to-image generative models capable of generating coherent and well-formed
visual text. In this paper, we propose a novel and efficient approach called
GlyphControl to address this task. Unlike existing methods that rely on
character-aware text encoders like ByT5 and require retraining of text-to-image
models, our approach leverages additional glyph conditional information to
enhance the performance of the off-the-shelf Stable-Diffusion model in
generating accurate visual text. By incorporating glyph instructions, users can
customize the content, location, and size of the generated text according to
their specific requirements. To facilitate further research in visual text
generation, we construct a training benchmark dataset called LAION-Glyph. We
evaluate the effectiveness of our approach by measuring OCR-based metrics, CLIP
score, and FID of the generated visual text. Our empirical evaluations
demonstrate that GlyphControl outperforms the recent DeepFloyd IF approach in
terms of OCR accuracy, CLIP score, and FID, highlighting the efficacy of our
method.",None,-1
5a120acf-fbee-443b-925e-298528f0ef1c,Compositional Probabilistic and Causal Inference using Tractable Circuit Models,0.461829,"Probabilistic circuits (PCs) are a class of tractable probabilistic models,
which admit efficient inference routines depending on their structural
properties. In this paper, we introduce md-vtrees, a novel structural
formulation of (marginal) determinism in structured decomposable PCs, which
generalizes previously proposed classes such as probabilistic sentential
decision diagrams. Crucially, we show how mdvtrees can be used to derive
tractability conditions and efficient algorithms for advanced inference queries
expressed as arbitrary compositions of basic probabilistic operations, such as
marginalization, multiplication and reciprocals, in a sound and generalizable
manner. In particular, we derive the first polytime algorithms for causal
inference queries such as backdoor adjustment on PCs. As a practical
instantiation of the framework, we propose MDNets, a novel PC architecture
using md-vtrees, and empirically demonstrate their application to causal
inference.",None,-1
8406cf74-e6a9-48a8-b31b-b26fd0fe4aa7,LED: A Dataset for Life Event Extraction from Dialogs,0.422646,"Lifelogging has gained more attention due to its wide applications, such as
personalized recommendations or memory assistance. The issues of collecting and
extracting personal life events have emerged. People often share their life
experiences with others through conversations. However, extracting life events
from conversations is rarely explored. In this paper, we present Life Event
Dialog, a dataset containing fine-grained life event annotations on
conversational data. In addition, we initiate a novel conversational life event
extraction task and differentiate the task from the public event extraction or
the life event extraction from other sources like microblogs. We explore three
information extraction (IE) frameworks to address the conversational life event
extraction task: OpenIE, relation extraction, and event extraction. A
comprehensive empirical analysis of the three baselines is established. The
results suggest that the current event extraction model still struggles with
extracting life events from human daily conversations. Our proposed life event
dialog dataset and in-depth analysis of IE frameworks will facilitate future
research on life event extraction from conversations.",None,-1
1822d83f-ae20-47d0-b7a8-4c311be45f23,BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects,1.0,"We present a near real-time method for 6-DoF tracking of an unknown object
from a monocular RGBD video sequence, while simultaneously performing neural 3D
reconstruction of the object. Our method works for arbitrary rigid objects,
even when visual texture is largely absent. The object is assumed to be
segmented in the first frame only. No additional information is required, and
no assumption is made about the interaction agent. Key to our method is a
Neural Object Field that is learned concurrently with a pose graph optimization
process in order to robustly accumulate information into a consistent 3D
representation capturing both geometry and appearance. A dynamic pool of posed
memory frames is automatically maintained to facilitate communication between
these threads. Our approach handles challenging sequences with large pose
changes, partial and full occlusion, untextured surfaces, and specular
highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets,
demonstrating that our method significantly outperforms existing approaches.
Project page: https://bundlesdf.github.io",None,-1
60fe92ad-c5d6-4350-8c89-2c215ce996e6,Interpretable Image Quality Assessment via CLIP with Multiple Antonym-Prompt Pairs,0.428382,"No reference image quality assessment (NR-IQA) is a task to estimate the
perceptual quality of an image without its corresponding original image. It is
even more difficult to perform this task in a zero-shot manner, i.e., without
task-specific training. In this paper, we propose a new zero-shot and
interpretable NRIQA method that exploits the ability of a pre-trained
visionlanguage model to estimate the correlation between an image and a textual
prompt. The proposed method employs a prompt pairing strategy and multiple
antonym-prompt pairs corresponding to carefully selected descriptive features
corresponding to the perceptual image quality. Thus, the proposed method is
able to identify not only the perceptual quality evaluation of the image, but
also the cause on which the quality evaluation is based. Experimental results
show that the proposed method outperforms existing zero-shot NR-IQA methods in
terms of accuracy and can evaluate the causes of perceptual quality
degradation.",None,-1
715326f7-d2a9-44da-97a8-13cdb0521880,Rsum Parsing as Hierarchical Sequence Labeling: An Empirical Study,0.210938,"Extracting information from r\'esum\'es is typically formulated as a
two-stage problem, where the document is first segmented into sections and then
each section is processed individually to extract the target entities. Instead,
we cast the whole problem as sequence labeling in two levels -- lines and
tokens -- and study model architectures for solving both tasks simultaneously.
We build high-quality r\'esum\'e parsing corpora in English, French, Chinese,
Spanish, German, Portuguese, and Swedish. Based on these corpora, we present
experimental results that demonstrate the effectiveness of the proposed models
for the information extraction task, outperforming approaches introduced in
previous work. We conduct an ablation study of the proposed architectures. We
also analyze both model performance and resource efficiency, and describe the
trade-offs for model deployment in the context of a production environment.",None,-1
d4bd5b39-0604-41a6-9ee5-00860c1a3be2,Towards a Better Understanding of Learning with Multiagent Teams,0.113212,"While it has long been recognized that a team of individual learning agents
can be greater than the sum of its parts, recent work has shown that larger
teams are not necessarily more effective than smaller ones. In this paper, we
study why and under which conditions certain team structures promote effective
learning for a population of individual learning agents. We show that,
depending on the environment, some team structures help agents learn to
specialize into specific roles, resulting in more favorable global results.
However, large teams create credit assignment challenges that reduce
coordination, leading to large teams performing poorly compared to smaller
ones. We support our conclusions with both theoretical analysis and empirical
results.",None,-1
8c7ce85f-c271-4716-96f2-6ab8ecfea59b,Depth-Relative Self Attention for Monocular Depth Estimation,0.512263,"Monocular depth estimation is very challenging because clues to the exact
depth are incomplete in a single RGB image. To overcome the limitation, deep
neural networks rely on various visual hints such as size, shade, and texture
extracted from RGB information. However, we observe that if such hints are
overly exploited, the network can be biased on RGB information without
considering the comprehensive view. We propose a novel depth estimation model
named RElative Depth Transformer (RED-T) that uses relative depth as guidance
in self-attention. Specifically, the model assigns high attention weights to
pixels of close depth and low attention weights to pixels of distant depth. As
a result, the features of similar depth can become more likely to each other
and thus less prone to misused visual hints. We show that the proposed model
achieves competitive results in monocular depth estimation benchmarks and is
less biased to RGB information. In addition, we propose a novel monocular depth
estimation benchmark that limits the observable depth range during training in
order to evaluate the robustness of the model for unseen depths.",None,-1
b227ef49-5967-4331-8e89-65f71b81f009,Overcoming Prior Misspecification in Online Learning to Rank,0.122727,"The recent literature on online learning to rank (LTR) has established the
utility of prior knowledge to Bayesian ranking bandit algorithms. However, a
major limitation of existing work is the requirement for the prior used by the
algorithm to match the true prior. In this paper, we propose and analyze
adaptive algorithms that address this issue and additionally extend these
results to the linear and generalized linear models. We also consider scalar
relevance feedback on top of click feedback. Moreover, we demonstrate the
efficacy of our algorithms using both synthetic and real-world experiments.",None,-1
56d735c1-d95a-48ed-9060-17725e72ccf3,CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation,0.605424,"Conversational search provides a natural interface for information retrieval
(IR). Recent approaches have demonstrated promising results in applying dense
retrieval to conversational IR. However, training dense retrievers requires
large amounts of in-domain paired data. This hinders the development of
conversational dense retrievers, as abundant in-domain conversations are
expensive to collect. In this paper, we propose CONVERSER, a framework for
training conversational dense retrievers with at most 6 examples of in-domain
dialogues. Specifically, we utilize the in-context learning capability of large
language models to generate conversational queries given a passage in the
retrieval corpus. Experimental results on conversational retrieval benchmarks
OR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable
performance to fully-supervised models, demonstrating the effectiveness of our
proposed framework in few-shot conversational dense retrieval. All source code
and generated datasets are available at https://github.com/MiuLab/CONVERSER",None,-1
2fa5d557-7797-4499-a74d-a95bac6af931,Fused Classification For Differential Face Morphing Detection,0.232718,"Face morphing, a sophisticated presentation attack technique, poses
significant security risks to face recognition systems. Traditional methods
struggle to detect morphing attacks, which involve blending multiple face
images to create a synthetic image that can match different individuals. In
this paper, we focus on the differential detection of face morphing and propose
an extended approach based on fused classification method for no-reference
scenario. We introduce a public face morphing detection benchmark for the
differential scenario and utilize a specific data mining technique to enhance
the performance of our approach. Experimental results demonstrate the
effectiveness of our method in detecting morphing attacks.",None,-1
819aee21-50bc-47f2-8f7a-f3abb4e65047,Exploring Large Language Models for Ontology Alignment,0.341839,"This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.",None,-1
9c78b029-29ed-4b56-b079-fb7abbb42e64,PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering,0.964689,"In this paper, we focus on the problem of Medical Visual Question Answering
(MedVQA), which is crucial in efficiently interpreting medical images with
vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as
a generation task that naturally follows the human-machine interaction, we
propose a generative-based model for medical visual understanding by aligning
visual information from a pre-trained vision encoder with a large language
model. Secondly, we establish a scalable pipeline to construct a large-scale
medical visual question-answering dataset, named PMC-VQA, which contains 227k
VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we
pre-train our proposed model on PMC-VQA and then fine-tune it on multiple
public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a
large margin. Additionally, we propose a test set that has undergone manual
verification, which is significantly more challenging, even the best models
struggle to solve.",None,-1
844a9b74-5385-49e2-b81b-917b4cf50048,Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters,0.79919,"Identifying logical errors in complex, incomplete or even contradictory and
overall heterogeneous data like students' experimentation protocols is
challenging. Recognizing the limitations of current evaluation methods, we
investigate the potential of Large Language Models (LLMs) for automatically
identifying student errors and streamlining teacher assessments. Our aim is to
provide a foundation for productive, personalized feedback. Using a dataset of
65 student protocols, an Artificial Intelligence (AI) system based on the
GPT-3.5 and GPT-4 series was developed and tested against human raters. Our
results indicate varying levels of accuracy in error detection between the AI
system and human raters. The AI system can accurately identify many fundamental
student errors, for instance, the AI system identifies when a student is
focusing the hypothesis not on the dependent variable but solely on an expected
observation (acc. = 0.90), when a student modifies the trials in an ongoing
investigation (acc. = 1), and whether a student is conducting valid test trials
(acc. = 0.82) reliably. The identification of other, usually more complex
errors, like whether a student conducts a valid control trial (acc. = .60),
poses a greater challenge. This research explores not only the utility of AI in
educational settings, but also contributes to the understanding of the
capabilities of LLMs in error detection in inquiry-based learning like
experimentation.",None,-1
511aa900-05fa-4bf1-8574-e7a50e580aa3,MarioGPT: Open-Ended Text2Level Generation through Large Language Models,0.664068,"Procedural Content Generation (PCG) is a technique to generate complex and
diverse environments in an automated way. However, while generating content
with PCG methods is often straightforward, generating meaningful content that
reflects specific intentions and constraints remains challenging. Furthermore,
many PCG algorithms lack the ability to generate content in an open-ended
manner. Recently, Large Language Models (LLMs) have shown to be incredibly
effective in many diverse domains. These trained LLMs can be fine-tuned,
re-using information and accelerating training for new tasks. Here, we
introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game
levels, in our case Super Mario Bros levels. MarioGPT can not only generate
diverse levels, but can be text-prompted for controllable level generation,
addressing one of the key challenges of current PCG techniques. As far as we
know, MarioGPT is the first text-to-level model and combined with novelty
search it enables the generation of diverse levels with varying play-style
dynamics (i.e. player paths) and the open-ended discovery of an increasingly
diverse range of content. Code available at
https://github.com/shyamsn97/mario-gpt.",None,-1
7a08087e-993b-4bea-a412-1de25c9188e5,Semidefinite Relaxations for Robust Multiview Triangulation,0.431987,"We propose an approach based on convex relaxations for certifiably optimal
robust multiview triangulation. To this end, we extend existing relaxation
approaches to non-robust multiview triangulation by incorporating a truncated
least squares cost function. We propose two formulations, one based on epipolar
constraints and one based on fractional reprojection constraints. The first is
lower dimensional and remains tight under moderate noise and outlier levels,
while the second is higher dimensional and therefore slower but remains tight
even under extreme noise and outlier levels. We demonstrate through extensive
experiments that the proposed approaches allow us to compute provably optimal
reconstructions even under significant noise and a large percentage of
outliers.",None,-1
51342b4a-2330-4dd7-bf7a-4c0d65b97c3a,Evaluating Large Language Models at Evaluating Instruction Following,0.505608,"As research in large language models (LLMs) continues to accelerate,
LLM-based evaluation has emerged as a scalable and cost-effective alternative
to human evaluations for comparing the ever increasing list of models. This
paper investigates the efficacy of these ``LLM evaluators'', particularly in
using them to assess instruction following, a metric that gauges how closely
generated text adheres to the given instruction. We introduce a challenging
meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM
evaluator in discerning instruction-following outputs. The authors manually
curated 419 pairs of outputs, one adhering to instructions while the other
diverging, yet may possess deceptive qualities that mislead an LLM evaluator,
e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover
that different evaluators (i.e., combinations of LLMs and prompts) exhibit
distinct performance on LLMBar and even the highest-scoring ones have
substantial room for improvement. We also present a novel suite of prompting
strategies that further close the gap between LLM and human evaluators. With
LLMBar, we hope to offer more insight into LLM evaluators and foster future
research in developing better instruction-following models.",None,-1
ea20b59e-64f5-4009-b6cf-5c73e86096fa,Optimization-Inspired Cross-Attention Transformer for Compressive Sensing,0.852979,"By integrating certain optimization solvers with deep neural networks, deep
unfolding network (DUN) with good interpretability and high performance has
attracted growing attention in compressive sensing (CS). However, existing DUNs
often improve the visual quality at the price of a large number of parameters
and have the problem of feature information loss during iteration. In this
paper, we propose an Optimization-inspired Cross-attention Transformer (OCT)
module as an iterative process, leading to a lightweight OCT-based Unfolding
Framework (OCTUF) for image CS. Specifically, we design a novel Dual Cross
Attention (Dual-CA) sub-module, which consists of an Inertia-Supplied Cross
Attention (ISCA) block and a Projection-Guided Cross Attention (PGCA) block.
ISCA block introduces multi-channel inertia forces and increases the memory
effect by a cross attention mechanism between adjacent iterations. And, PGCA
block achieves an enhanced information interaction, which introduces the
inertia force into the gradient descent step through a cross attention block.
Extensive CS experiments manifest that our OCTUF achieves superior performance
compared to state-of-the-art methods while training lower complexity. Codes are
available at https://github.com/songjiechong/OCTUF.",None,-1
4507ca5c-16bd-477f-aff8-62f34434deee,The Cambridge Law Corpus: A Dataset for Legal AI Research,0.360857,"We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research.
It consists of over 250 000 court cases from the UK. Most cases are from the
21st century, but the corpus includes cases as old as the 16th century. This
paper presents the first release of the corpus, containing the raw text and
meta-data. Together with the corpus, we provide annotations on case outcomes
for 638 cases, done by legal experts. Using our annotated data, we have trained
and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to
provide benchmarks. We include an extensive legal and ethical discussion to
address the potentially sensitive nature of this material. As a consequence,
the corpus will only be released for research purposes under certain
restrictions.",None,-1
9566d392-36c3-43fd-94c2-76048e7e3b1a,Scene-Generalizable Interactive Segmentation of Radiance Fields,0.176217,"Existing methods for interactive segmentation in radiance fields entail
scene-specific optimization and thus cannot generalize across different scenes,
which greatly limits their applicability. In this work we make the first
attempt at Scene-Generalizable Interactive Segmentation in Radiance Fields
(SGISRF) and propose a novel SGISRF method, which can perform 3D object
segmentation for novel (unseen) scenes represented by radiance fields, guided
by only a few interactive user clicks in a given set of multi-view 2D images.
In particular, the proposed SGISRF focuses on addressing three crucial
challenges with three specially designed techniques. First, we devise the
Cross-Dimension Guidance Propagation to encode the scarce 2D user clicks into
informative 3D guidance representations. Second, the Uncertainty-Eliminated 3D
Segmentation module is designed to achieve efficient yet effective 3D
segmentation. Third, Concealment-Revealed Supervised Learning scheme is
proposed to reveal and correct the concealed 3D segmentation errors resulted
from the supervision in 2D space with only 2D mask annotations. Extensive
experiments on two real-world challenging benchmarks covering diverse scenes
demonstrate 1) effectiveness and scene-generalizability of the proposed method,
2) favorable performance compared to classical method requiring scene-specific
optimization.",None,-1
ea61e51f-38ae-4288-82dc-ea047176d0d3,Measuring Progress in Fine-grained Vision-and-Language Understanding,0.686153,"While pretraining on large-scale image-text data from the Web has facilitated
rapid progress on many vision-and-language (V&L) tasks, recent work has
demonstrated that pretrained models lack ""fine-grained"" understanding, such as
the ability to recognise relationships, verbs, and numbers in images. This has
resulted in an increased interest in the community to either develop new
benchmarks or models for such capabilities. To better understand and quantify
progress in this direction, we investigate four competitive V&L models on four
fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al.,
2022) consistently outperforms other baselines, and that modelling innovations
can impact performance more than scaling Web data, which even degrades
performance sometimes. Through a deeper investigation of X-VLM, we highlight
the importance of both novel losses and rich data sources for learning
fine-grained skills. Finally, we inspect training dynamics, and discover that
for some tasks, performance peaks early in training or significantly
fluctuates, never converging.",None,-1
dfd1501e-09d7-4700-abdf-69c61fd4d6e9,Have it your way: Individualized Privacy Assignment for DP-SGD,0.714111,"When training a machine learning model with differential privacy, one sets a
privacy budget. This budget represents a maximal privacy violation that any
user is willing to face by contributing their data to the training set. We
argue that this approach is limited because different users may have different
privacy expectations. Thus, setting a uniform privacy budget across all points
may be overly conservative for some users or, conversely, not sufficiently
protective for others. In this paper, we capture these preferences through
individualized privacy budgets. To demonstrate their practicality, we introduce
a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which
supports such individualized budgets. DP-SGD is the canonical approach to
training models with differential privacy. We modify its data sampling and
gradient noising mechanisms to arrive at our approach, which we call
Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees
tailored to the preferences of individual users and their data points, we find
it empirically improves privacy-utility trade-offs.",None,-1
9d4745a0-84eb-4966-93d0-9290fe2e9f13,A Cross-Linguistic Pressure for Uniform Information Density in Word Order,0.540743,"While natural languages differ widely in both canonical word order and word
order flexibility, their word orders still follow shared cross-linguistic
statistical patterns, often attributed to functional pressures. In the effort
to identify these pressures, prior work has compared real and counterfactual
word orders. Yet one functional pressure has been overlooked in such
investigations: the uniform information density (UID) hypothesis, which holds
that information should be spread evenly throughout an utterance. Here, we ask
whether a pressure for UID may have influenced word order patterns
cross-linguistically. To this end, we use computational models to test whether
real orders lead to greater information uniformity than counterfactual orders.
In our empirical study of 10 typologically diverse languages, we find that: (i)
among SVO languages, real word orders consistently have greater uniformity than
reverse word orders, and (ii) only linguistically implausible counterfactual
orders consistently exceed the uniformity of real orders. These findings are
compatible with a pressure for information uniformity in the development and
usage of natural languages.",None,-1
433f791c-11cb-4560-90ee-2af91cbfdf17,Image Hash Minimization for Tamper Detection,0.0150208,"Tamper detection using image hash is a very common problem of modern days.
Several research and advancements have already been done to address this
problem. However, most of the existing methods lack the accuracy of tamper
detection when the tampered area is low, as well as requiring long image
hashes. In this paper, we propose a novel method objectively to minimize the
hash length while enhancing the performance at low tampered area.",None,-1
1b319471-4b23-4306-9d96-3f67a3745630,Benchmarking Algorithms for Submodular Optimization Problems Using IOHProfiler,0.0541898,"Submodular functions play a key role in the area of optimization as they
allow to model many real-world problems that face diminishing returns.
Evolutionary algorithms have been shown to obtain strong theoretical
performance guarantees for a wide class of submodular problems under various
types of constraints while clearly outperforming standard greedy approximation
algorithms. This paper introduces a setup for benchmarking algorithms for
submodular optimization problems with the aim to provide researchers with a
framework to enhance and compare the performance of new algorithms for
submodular problems. The focus is on the development of iterative search
algorithms such as evolutionary algorithms with the implementation provided and
integrated into IOHprofiler which allows for tracking and comparing the
progress and performance of iterative search algorithms. We present a range of
submodular optimization problems that have been integrated into IOHprofiler and
show how the setup can be used for analyzing and comparing iterative search
algorithms in various settings.",None,-1
90ee18cb-3ca2-45f6-a758-b38618fe5a6d,kNN-BOX: A Unified Framework for Nearest Neighbor Generation,0.358282,"Augmenting the base neural model with a token-level symbolic datastore is a
novel generation paradigm and has achieved promising results in machine
translation (MT). In this paper, we introduce a unified framework kNN-BOX,
which enables quick development and interactive analysis for this novel
paradigm. kNN-BOX decomposes the datastore-augmentation approach into three
modules: datastore, retriever and combiner, thus putting diverse kNN generation
methods into a unified way. Currently, kNN-BOX has provided implementation of
seven popular kNN-MT variants, covering research from performance enhancement
to efficiency optimization. It is easy for users to reproduce these existing
works or customize their own models. Besides, users can interact with their kNN
generation systems with kNN-BOX to better understand the underlying inference
process in a visualized way. In the experiment section, we apply kNN-BOX for
machine translation and three other seq2seq generation tasks, namely, text
simplification, paraphrase generation and question generation. Experiment
results show that augmenting the base neural model with kNN-BOX leads to a
large performance improvement in all these tasks. The code and document of
kNN-BOX is available at https://github.com/NJUNLP/knn-box.",None,-1
593656a4-bac5-4d92-99e7-6535aa60303f,Learning in Cooperative Multiagent Systems Using Cognitive and Machine Models,0.538971,"Developing effective Multi-Agent Systems (MAS) is critical for many
applications requiring collaboration and coordination with humans. Despite the
rapid advance of Multi-Agent Deep Reinforcement Learning (MADRL) in cooperative
MAS, one major challenge is the simultaneous learning and interaction of
independent agents in dynamic environments in the presence of stochastic
rewards. State-of-the-art MADRL models struggle to perform well in Coordinated
Multi-agent Object Transportation Problems (CMOTPs), wherein agents must
coordinate with each other and learn from stochastic rewards. In contrast,
humans often learn rapidly to adapt to nonstationary environments that require
coordination among people. In this paper, motivated by the demonstrated ability
of cognitive models based on Instance-Based Learning Theory (IBLT) to capture
human decisions in many dynamic decision making tasks, we propose three
variants of Multi-Agent IBL models (MAIBL). The idea of these MAIBL algorithms
is to combine the cognitive mechanisms of IBLT and the techniques of MADRL
models to deal with coordination MAS in stochastic environments from the
perspective of independent learners. We demonstrate that the MAIBL models
exhibit faster learning and achieve better coordination in a dynamic CMOTP task
with various settings of stochastic rewards compared to current MADRL models.
We discuss the benefits of integrating cognitive insights into MADRL models.",None,-1
1b16ab9d-dba3-46ad-b100-59850c90b29c,VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking,0.0771237,"The lack of interpretability of the Vision Transformer may hinder its use in
critical real-world applications despite its effectiveness. To overcome this
issue, we propose a post-hoc interpretability method called VISION DIFFMASK,
which uses the activations of the model's hidden layers to predict the relevant
parts of the input that contribute to its final predictions. Our approach uses
a gating mechanism to identify the minimal subset of the original input that
preserves the predicted distribution over classes. We demonstrate the
faithfulness of our method, by introducing a faithfulness task, and comparing
it to other state-of-the-art attribution methods on CIFAR-10 and ImageNet-1K,
achieving compelling results. To aid reproducibility and further extension of
our work, we open source our implementation:
https://github.com/AngelosNal/Vision-DiffMask",None,-1
6b08d18c-da6a-4413-bea4-d5bd2d069f72,SMATCH++: Standardized and Extended Evaluation of Semantic Graphs,0.741204,"The Smatch metric is a popular method for evaluating graph distances, as is
necessary, for instance, to assess the performance of semantic graph parsing
systems. However, we observe some issues in the metric that jeopardize
meaningful evaluation. E.g., opaque pre-processing choices can affect results,
and current graph-alignment solvers do not provide us with upper-bounds.
Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,
adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)
are spread out, and lack a unifying framework.
  For better inspection, we divide the metric into three modules:
pre-processing, alignment, and scoring. Examining each module, we specify its
goals and diagnose potential issues, for which we discuss and test mitigation
strategies. For pre-processing, we show how to fully conform to annotation
guidelines that allow structurally deviating but valid graphs. For safer and
enhanced alignment, we show the feasibility of optimal alignment in a standard
evaluation setup, and develop a lossless graph compression method that shrinks
the search space and significantly increases efficiency. For improved scoring,
we propose standardized and extended metric calculation of fine-grained
sub-graph meaning aspects. Our code is available at
https://github.com/flipz357/smatchpp",None,-1
56917941-094a-4cdc-a5aa-7d05ce1e8021,On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion,0.747131,"Generalizing deep learning models to unknown target domain distribution with
low latency has motivated research into test-time training/adaptation
(TTT/TTA). Existing approaches often focus on improving test-time training
performance under well-curated target domain data. As figured out in this work,
many state-of-the-art methods fail to maintain the performance when the target
domain is contaminated with strong out-of-distribution (OOD) data, a.k.a.
open-world test-time training (OWTTT). The failure is mainly due to the
inability to distinguish strong OOD samples from regular weak OOD samples. To
improve the robustness of OWTTT we first develop an adaptive strong OOD pruning
which improves the efficacy of the self-training TTT method. We further propose
a way to dynamically expand the prototypes to represent strong OOD samples for
an improved weak/strong OOD data separation. Finally, we regularize
self-training with distribution alignment and the combination yields the
state-of-the-art performance on 5 OWTTT benchmarks. The code is available at
https://github.com/Yushu-Li/OWTTT.",None,-1
cbf71df2-8383-4b2c-b900-be61847351d3,Towards Agile Text Classifiers for Everyone,0.377731,"Text-based safety classifiers are widely used for content moderation and
increasingly to tune generative language model behavior - a topic of growing
concern for the safety of digital assistants and chatbots. However, different
policies require different classifiers, and safety policies themselves improve
from iteration and adaptation. This paper introduces and evaluates methods for
agile text classification, whereby classifiers are trained using small,
targeted datasets that can be quickly developed for a particular policy.
Experimenting with 7 datasets from three safety-related domains, comprising 15
annotation schemes, led to our key finding: prompt-tuning large language
models, like PaLM 62B, with a labeled dataset of as few as 80 examples can
achieve state-of-the-art performance. We argue that this enables a paradigm
shift for text classification, especially for models supporting safer online
discourse. Instead of collecting millions of examples to attempt to create
universal safety classifiers over months or years, classifiers could be tuned
using small datasets, created by individuals or small organizations, tailored
for specific use cases, and iterated on and adapted in the time-span of a day.",None,-1
17a2797d-400f-4beb-a0d1-ba41cffcd07a,Temporal Action Localization with Enhanced Instant Discriminability,0.615406,"Temporal action detection (TAD) aims to detect all action boundaries and
their corresponding categories in an untrimmed video. The unclear boundaries of
actions in videos often result in imprecise predictions of action boundaries by
existing methods. To resolve this issue, we propose a one-stage framework named
TriDet. First, we propose a Trident-head to model the action boundary via an
estimated relative probability distribution around the boundary. Then, we
analyze the rank-loss problem (i.e. instant discriminability deterioration) in
transformer-based methods and propose an efficient scalable-granularity
perception (SGP) layer to mitigate this issue. To further push the limit of
instant discriminability in the video backbone, we leverage the strong
representation capability of pretrained large models and investigate their
performance on TAD. Last, considering the adequate spatial-temporal context for
classification, we design a decoupled feature pyramid network with separate
feature pyramids to incorporate rich spatial context from the large model for
localization. Experimental results demonstrate the robustness of TriDet and its
state-of-the-art performance on multiple TAD datasets, including hierarchical
(multilabel) TAD datasets.",None,-1
143ff011-7e94-4b05-b141-2f39357cffcb,Does Visual Pretraining Help End-to-End Reasoning?,0.163669,"We aim to investigate whether end-to-end learning of visual reasoning can be
achieved with general-purpose neural networks, with the help of visual
pretraining. A positive result would refute the common belief that explicit
visual abstraction (e.g. object detection) is essential for compositional
generalization on visual reasoning, and confirm the feasibility of a neural
network ""generalist"" to solve visual recognition and reasoning tasks. We
propose a simple and general self-supervised framework which ""compresses"" each
video frame into a small set of tokens with a transformer network, and
reconstructs the remaining frames based on the compressed temporal context. To
minimize the reconstruction loss, the network must learn a compact
representation for each image, as well as capture temporal dynamics and object
permanence from temporal context. We perform evaluation on two visual reasoning
benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve
compositional generalization for end-to-end visual reasoning. Our proposed
framework outperforms traditional supervised pretraining, including image
classification and explicit object detection, by large margins.",None,-1
882c9198-e454-429c-96fd-001c59d48f00,Instance Segmentation of Dislocations in TEM Images,0.243798,"Quantitative Transmission Electron Microscopy (TEM) during in-situ straining
experiment is able to reveal the motion of dislocations -- linear defects in
the crystal lattice of metals. In the domain of materials science, the
knowledge about the location and movement of dislocations is important for
creating novel materials with superior properties. A long-standing problem,
however, is to identify the position and extract the shape of dislocations,
which would ultimately help to create a digital twin of such materials. In this
work, we quantitatively compare state-of-the-art instance segmentation methods,
including Mask R-CNN and YOLOv8. The dislocation masks as the results of the
instance segmentation are converted to mathematical lines, enabling
quantitative analysis of dislocation length and geometry -- important
information for the domain scientist, which we then propose to include as a
novel length-aware quality metric for estimating the network performance. Our
segmentation pipeline shows a high accuracy suitable for all domain-specific,
further post-processing. Additionally, our physics-based metric turns out to
perform much more consistently than typically used pixel-wise metrics.",None,-1
eb36e54c-754d-48f5-82e6-b1339d304976,Text2Tex: Text-driven Texture Synthesis via Diffusion Models,0.953282,"We present Text2Tex, a novel method for generating high-quality textures for
3D meshes from the given text prompts. Our method incorporates inpainting into
a pre-trained depth-aware image diffusion model to progressively synthesize
high resolution partial textures from multiple viewpoints. To avoid
accumulating inconsistent and stretched artifacts across views, we dynamically
segment the rendered view into a generation mask, which represents the
generation status of each visible texel. This partitioned view representation
guides the depth-aware inpainting model to generate and update partial textures
for the corresponding regions. Furthermore, we propose an automatic view
sequence generation scheme to determine the next best view for updating the
partial texture. Extensive experiments demonstrate that our method
significantly outperforms the existing text-driven approaches and GAN-based
methods.",None,-1
336e12f8-646e-4825-8136-71fa7f885438,Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning,0.268384,"We propose a model-free reinforcement learning architecture, called
distributed attentional actor architecture after conditional attention (DA6-X),
to provide better interpretability of conditional coordinated behaviors. The
underlying principle involves reusing the saliency vector, which represents the
conditional states of the environment, such as the global position of agents.
Hence, agents with DA6-X flexibility built into their policy exhibit superior
performance by considering the additional information in the conditional states
during the decision-making process. The effectiveness of the proposed method
was experimentally evaluated by comparing it with conventional methods in an
objects collection game. By visualizing the attention weights from DA6-X, we
confirmed that agents successfully learn situation-dependent coordinated
behaviors by correctly identifying various conditional states, leading to
improved interpretability of agents along with superior performance.",None,-1
eacb7ff8-ab2b-4ffa-a410-65ff46639c02,Prompt Guided Transformer for Multi-Task Dense Prediction,0.329734,"Task-conditional architecture offers advantage in parameter efficiency but
falls short in performance compared to state-of-the-art multi-decoder methods.
How to trade off performance and model parameters is an important and difficult
problem. In this paper, we introduce a simple and lightweight task-conditional
model called Prompt Guided Transformer (PGT) to optimize this challenge. Our
approach designs a Prompt-conditioned Transformer block, which incorporates
task-specific prompts in the self-attention mechanism to achieve global
dependency modeling and parameter-efficient feature adaptation across multiple
tasks. This block is integrated into both the shared encoder and decoder,
enhancing the capture of intra- and inter-task features. Moreover, we design a
lightweight decoder to further reduce parameter usage, which accounts for only
2.7% of the total model parameters. Extensive experiments on two multi-task
dense prediction benchmarks, PASCAL-Context and NYUD-v2, demonstrate that our
approach achieves state-of-the-art results among task-conditional methods while
using fewer parameters, and maintains a significant balance between performance
and parameter size.",None,-1
e9771d16-33e9-4822-855f-174cf5493368,Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges,0.0477987,"The growing popularity of generative language models has amplified interest
in interactive methods to guide model outputs. Prompt refinement is considered
one of the most effective means to influence output among these methods. We
identify several challenges associated with prompting large language models,
categorized into data- and model-specific, linguistic, and socio-linguistic
challenges. A comprehensive examination of model outputs, including runner-up
candidates and their corresponding probabilities, is needed to address these
issues. The beam search tree, the prevalent algorithm to sample model outputs,
can inherently supply this information. Consequently, we introduce an
interactive visual method for investigating the beam search tree, facilitating
analysis of the decisions made by the model during generation. We
quantitatively show the value of exposing the beam search tree and present five
detailed analysis scenarios addressing the identified challenges. Our
methodology validates existing results and offers additional insights.",None,-1
884d764b-cb4a-4953-930c-bd31e4eaf484,"Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",0.98814,"Graph Neural Networks (GNNs) have become the leading paradigm for learning on
(static) graph-structured data. However, many real-world systems are dynamic in
nature, since the graph and node/edge attributes change over time. In recent
years, GNN-based models for temporal graphs have emerged as a promising area of
research to extend the capabilities of GNNs. In this work, we provide the first
comprehensive overview of the current state-of-the-art of temporal GNN,
introducing a rigorous formalization of learning settings and tasks and a novel
taxonomy categorizing existing approaches in terms of how the temporal aspect
is represented and processed. We conclude the survey with a discussion of the
most relevant open challenges for the field, from both research and application
perspectives.",None,-1
503ed85a-9c1f-4881-a625-5c4c3a8bddc1,Fact-Checking Generative AI: Ontology-Driven Biological Graphs for Disease-Gene Link Verification,0.37116,"Since the launch of various generative AI tools, scientists have been
striving to evaluate their capabilities and contents, in the hope of
establishing trust in their generative abilities. Regulations and guidelines
are emerging to verify generated contents and identify novel uses. we aspire to
demonstrate how ChatGPT claims are checked computationally using the rigor of
network models. We aim to achieve fact-checking of the knowledge embedded in
biological graphs that were contrived from ChatGPT contents at the aggregate
level. We adopted a biological networks approach that enables the systematic
interrogation of ChatGPT's linked entities. We designed an ontology-driven
fact-checking algorithm that compares biological graphs constructed from
approximately 200,000 PubMed abstracts with counterparts constructed from a
dataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250
randomly selected records a ChatGPT dataset of 1000 ""simulated"" articles , the
fact-checking link accuracy ranged from 70% to 86%. This study demonstrated
high accuracy of aggregate disease-gene links relationships found in
ChatGPT-generated texts.",None,-1
175cfd7e-97aa-4792-ba66-aa74981f44d6,Complex Claim Verification with Evidence Retrieved in the Wild,0.901963,"Evidence retrieval is a core part of automatic fact-checking. Prior work
makes simplifying assumptions in retrieval that depart from real-world use
cases: either no access to evidence, access to evidence curated by a human
fact-checker, or access to evidence available long after the claim has been
made. In this work, we present the first fully automated pipeline to check
real-world claims by retrieving raw evidence from the web. We restrict our
retriever to only search documents available prior to the claim's making,
modeling the realistic scenario where an emerging claim needs to be checked.
Our pipeline includes five components: claim decomposition, raw document
retrieval, fine-grained evidence retrieval, claim-focused summarization, and
veracity judgment. We conduct experiments on complex political claims in the
ClaimDecomp dataset and show that the aggregated evidence produced by our
pipeline improves veracity judgments. Human evaluation finds the evidence
summary produced by our system is reliable (it does not hallucinate
information) and relevant to answering key questions about a claim, suggesting
that it can assist fact-checkers even when it cannot surface a complete
evidence set.",None,-1
7cc35a0d-5120-4bab-acd6-bac873eb079d,DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars,0.904779,"We present DINAR, an approach for creating realistic rigged fullbody avatars
from single RGB images. Similarly to previous works, our method uses neural
textures combined with the SMPL-X body model to achieve photo-realistic quality
of avatars while keeping them easy to animate and fast to infer. To restore the
texture, we use a latent diffusion model and show how such model can be trained
in the neural texture space. The use of the diffusion model allows us to
realistically reconstruct large unseen regions such as the back of a person
given the frontal view. The models in our pipeline are trained using 2D images
and videos only. In the experiments, our approach achieves state-of-the-art
rendering quality and good generalization to new poses and viewpoints. In
particular, the approach improves state-of-the-art on the SnapshotPeople public
benchmark.",None,-1
2b63866c-2dd2-483e-9e8d-01b1096a6027,PV2TEA: Patching Visual Modality to Textual-Established Information Extraction,0.147802,"Information extraction, e.g., attribute value extraction, has been
extensively studied and formulated based only on text. However, many attributes
can benefit from image-based extraction, like color, shape, pattern, among
others. The visual modality has long been underutilized, mainly due to
multimodal annotation difficulty. In this paper, we aim to patch the visual
modality to the textual-established attribute information extractor. The
cross-modality integration faces several unique challenges: (C1) images and
textual descriptions are loosely paired intra-sample and inter-samples; (C2)
images usually contain rich backgrounds that can mislead the prediction; (C3)
weakly supervised labels from textual-established extractors are biased for
multimodal training. We present PV2TEA, an encoder-decoder architecture
equipped with three bias reduction schemes: (S1) Augmented label-smoothed
contrast to improve the cross-modality alignment for loosely-paired image and
text; (S2) Attention-pruning that adaptively distinguishes the visual
foreground; (S3) Two-level neighborhood regularization that mitigates the label
textual bias via reliability estimation. Empirical results on real-world
e-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1
increase over unimodal baselines.",None,-1
dd9dcd8e-4c87-4cb6-bf1b-b41853fcba24,Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models,0.443735,"After the inception of emotion recognition or affective computing, it has
increasingly become an active research topic due to its broad applications.
Over the past couple of decades, emotion recognition models have gradually
migrated from statistically shallow models to neural network-based deep models,
which can significantly boost the performance of emotion recognition models and
consistently achieve the best results on different benchmarks. Therefore, in
recent years, deep models have always been considered the first option for
emotion recognition. However, the debut of large language models (LLMs), such
as ChatGPT, has remarkably astonished the world due to their emerged
capabilities of zero/few-shot learning, in-context learning, chain-of-thought,
and others that are never shown in previous deep models. In the present paper,
we comprehensively investigate how the LLMs perform in emotion recognition in
terms of diverse aspects, including in-context learning, few-short learning,
accuracy, generalisation, and explanation. Moreover, we offer some insights and
pose other potential challenges, hoping to ignite broader discussions about
enhancing emotion recognition in the new era of advanced and generalised large
models.",None,-1
410b2df9-a3c3-4572-8942-48fbab07e4b0,Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling,0.664819,"We introduce Reprompting, an iterative sampling algorithm that automatically
learns the Chain-of-Thought (CoT) recipes for a given task without human
intervention. Through Gibbs sampling, Reprompting infers the CoT recipes that
work consistently well for a set of training samples by iteratively sampling
new recipes using previously sampled recipes as parent prompts to solve other
training problems. We conduct extensive experiments on 20 challenging reasoning
tasks. Results show that Reprompting outperforms human-written CoT prompts
substantially by +9.4 points on average. It also achieves consistently better
performance than the state-of-the-art prompt optimization and decoding
algorithms.",None,-1
0ef1d2de-2c85-4f5c-afb7-f5dbab947aa0,Semi-Supervised SAR ATR Framework with Transductive Auxiliary Segmentation,0.999866,"Convolutional neural networks (CNNs) have achieved high performance in
synthetic aperture radar (SAR) automatic target recognition (ATR). However, the
performance of CNNs depends heavily on a large amount of training data. The
insufficiency of labeled training SAR images limits the recognition performance
and even invalidates some ATR methods. Furthermore, under few labeled training
data, many existing CNNs are even ineffective. To address these challenges, we
propose a Semi-supervised SAR ATR Framework with transductive Auxiliary
Segmentation (SFAS). The proposed framework focuses on exploiting the
transductive generalization on available unlabeled samples with an auxiliary
loss serving as a regularizer. Through auxiliary segmentation of unlabeled SAR
samples and information residue loss (IRL) in training, the framework can
employ the proposed training loop process and gradually exploit the information
compilation of recognition and segmentation to construct a helpful inductive
bias and achieve high performance. Experiments conducted on the MSTAR dataset
have shown the effectiveness of our proposed SFAS for few-shot learning. The
recognition performance of 94.18\% can be achieved under 20 training samples in
each class with simultaneous accurate segmentation results. Facing variances of
EOCs, the recognition ratios are higher than 88.00\% when 10 training samples
each class.",None,-1
ab672c10-a25a-4563-9c46-d7602f10c4c0,Online POMDP Planning with Anytime Deterministic Guarantees,0.814804,"Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.",None,-1
b0df0ef9-0f1b-471e-a8b6-df8e457a136b,Conformal Prediction with Large Language Models for Multi-Choice Question Answering,0.915801,"As large language models continue to be widely developed, robust uncertainty
quantification techniques will become crucial for their safe deployment in
high-stakes scenarios. In this work, we explore how conformal prediction can be
used to provide uncertainty quantification in language models for the specific
task of multiple-choice question-answering. We find that the uncertainty
estimates from conformal prediction are tightly correlated with prediction
accuracy. This observation can be useful for downstream applications such as
selective classification and filtering out low-quality predictions. We also
investigate the exchangeability assumption required by conformal prediction to
out-of-subject questions, which may be a more realistic scenario for many
practical applications. Our work contributes towards more trustworthy and
reliable usage of large language models in safety-critical situations, where
robust guarantees of error rate are required.",None,-1
8c0861a1-b42a-44de-86c3-c98ac42b028c,A Reparameterized Discrete Diffusion Model for Text Generation,0.363538,"This work studies discrete diffusion probabilistic models with applications
to natural language generation. We derive an alternative yet equivalent
formulation of the sampling from discrete diffusion processes and leverage this
insight to develop a family of reparameterized discrete diffusion models. The
derived generic framework is highly flexible, offers a fresh perspective of the
generation process in discrete diffusion models, and features more effective
training and decoding techniques. We conduct extensive experiments to evaluate
the text generation capability of our model, demonstrating significant
improvements over existing diffusion models.",None,-1
28172fd6-5e5c-4a94-a384-981918e13e0f,Jointly Optimizing Translations and Speech Timing to Improve Isochrony in Automatic Dubbing,0.110815,"Automatic dubbing (AD) is the task of translating the original speech in a
video into target language speech. The new target language speech should
satisfy isochrony; that is, the new speech should be time aligned with the
original video, including mouth movements, pauses, hand gestures, etc. In this
paper, we propose training a model that directly optimizes both the translation
as well as the speech duration of the generated translations. We show that this
system generates speech that better matches the timing of the original speech,
compared to prior work, while simplifying the system architecture.",None,-1
d70442cf-a4aa-4c3e-9698-8d7d23dc87f7,A Simple and Effective Pruning Approach for Large Language Models,0.981832,"As their size increases, Large Languages Models (LLMs) are natural candidates
for network pruning methods: approaches that drop a subset of network weights
while striving to preserve performance. Existing methods, however, require
either retraining, which is rarely affordable for billion-scale LLMs, or
solving a weight reconstruction problem reliant on second-order information,
which may also be computationally expensive. In this paper, we introduce a
novel, straightforward yet effective pruning method, termed Wanda (Pruning by
Weights and activations), designed to induce sparsity in pretrained LLMs.
Motivated by the recent observation of emergent large magnitude features in
LLMs, our approach prunes weights with the smallest magnitudes multiplied by
the corresponding input activations, on a per-output basis. Notably, Wanda
requires no retraining or weight update, and the pruned LLM can be used as is.
We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2
across various language benchmarks. Wanda significantly outperforms the
established baseline of magnitude pruning and performs competitively against
recent method involving intensive weight update. Code is available at
https://github.com/locuslab/wanda.",None,-1
111f7146-c3c3-40be-9c6d-135feca19dae,Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware Sound Separation,0.867137,"The audio-visual sound separation field assumes visible sources in videos,
but this excludes invisible sounds beyond the camera's view. Current methods
struggle with such sounds lacking visible cues. This paper introduces a novel
""Audio-Visual Scene-Aware Separation"" (AVSA-Sep) framework. It includes a
semantic parser for visible and invisible sounds and a separator for
scene-informed separation. AVSA-Sep successfully separates both sound types,
with joint training and cross-modal alignment enhancing effectiveness.",None,-1
22a60b51-6859-4ee7-8e7a-27225a194758,AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks,0.0944174,"Sparse training is emerging as a promising avenue for reducing the
computational cost of training neural networks. Several recent studies have
proposed pruning methods using learnable thresholds to efficiently explore the
non-uniform distribution of sparsity inherent within the models. In this paper,
we propose Gradient Annealing (GA), where gradients of masked weights are
scaled down in a non-linear manner. GA provides an elegant trade-off between
sparsity and accuracy without the need for additional sparsity-inducing
regularization. We integrated GA with the latest learnable pruning methods to
create an automated sparse training algorithm called AutoSparse, which achieves
better accuracy and/or training/inference FLOPS reduction than existing
learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K:
AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for
ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms
sparse-to-sparse SotA method MEST (uniform sparsity) for 80% sparse ResNet50
with similar accuracy, where MEST uses 12% more training FLOPS and 50% more
inference FLOPS.",None,-1
5ad841be-044e-4edd-90ad-b7fbb27bdf11,"Milestones in Autonomous Driving and Intelligent Vehicles Part I: Control, Computing System Design, Communication, HD Map, Testing, and Human Behaviors",0.867843,"Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing
at a rapid pace due to the convenience, safety, and economic benefits. Although
a number of surveys have reviewed research achievements in this field, they are
still limited in specific tasks and lack systematic summaries and research
directions in the future. Our work is divided into 3 independent articles and
the first part is a Survey of Surveys (SoS) for total technologies of AD and
IVs that involves the history, summarizes the milestones, and provides the
perspectives, ethics, and future research directions. This is the second part
(Part I for this technical survey) to review the development of control,
computing system design, communication, High Definition map (HD map), testing,
and human behaviors in IVs. In addition, the third part (Part II for this
technical survey) is to review the perception and planning sections. The
objective of this paper is to involve all the sections of AD, summarize the
latest technical milestones, and guide abecedarians to quickly understand the
development of AD and IVs. Combining the SoS and Part II, we anticipate that
this work will bring novel and diverse insights to researchers and
abecedarians, and serve as a bridge between past and future.",None,-1
3280b031-108f-4d17-9f97-7c4ed509a652,Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping,0.317187,"Clinical practice in psychiatry is burdened with the increased demand for
healthcare services and the scarce resources available. New paradigms of health
data powered with machine learning techniques could open the possibility to
improve clinical workflow in critical stages of clinical assessment and
treatment in psychiatry. In this work, we propose a machine learning system
capable of predicting, detecting, and explaining individual changes in symptoms
of patients with Schizophrenia by using behavioral digital phenotyping data. We
forecast symptoms of patients with an error rate below 10%. The system detects
decreases in symptoms using changepoint algorithms and uses counterfactual
explanations as a recourse in a simulated continuous monitoring scenario in
healthcare. Overall, this study offers valuable insights into the performance
and potential of counterfactual explanations, predictive models, and
change-point detection within a simulated clinical workflow. These findings lay
the foundation for further research to explore additional facets of the
workflow, aiming to enhance its effectiveness and applicability in real-world
healthcare settings. By leveraging these components, the goal is to develop an
actionable, interpretable, and trustworthy integrative decision support system
that combines real-time clinical assessments with sensor-based inputs.",None,-1
8791300b-26ba-4719-98bd-fe729f98b90a,Improving Generalization for Multimodal Fake News Detection,0.153966,"The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for fake news
detection. However, state-of-the-art approaches are usually trained on datasets
of smaller size or with a limited set of specific topics. As a consequence,
these models lack generalization capabilities and are not applicable to
real-world data. In this paper, we propose three models that adopt and
fine-tune state-of-the-art multimodal transformers for multimodal fake news
detection. We conduct an in-depth analysis by manipulating the input data aimed
to explore models performance in realistic use cases on social media. Our study
across multiple models demonstrates that these systems suffer significant
performance drops against manipulated data. To reduce the bias and improve
model generalization, we suggest training data augmentation to conduct more
meaningful experiments for fake news detection on social media. The proposed
data augmentation techniques enable models to generalize better and yield
improved state-of-the-art results.",None,-1
37647640-828a-42b8-af8b-6e8a78cb211f,Reflective Artificial Intelligence,0.0821224,"Artificial Intelligence (AI) is about making computers that do the sorts of
things that minds can do, and as we progress towards this goal, we tend to
increasingly delegate human tasks to machines. However, AI systems usually do
these tasks with an unusual imbalance of insight and understanding: new, deeper
insights are present, yet many important qualities that a human mind would have
previously brought to the activity are utterly absent. Therefore, it is crucial
to ask which features of minds have we replicated, which are missing, and if
that matters. One core feature that humans bring to tasks, when dealing with
the ambiguity, emergent knowledge, and social context presented by the world,
is reflection. Yet this capability is utterly missing from current mainstream
AI. In this paper we ask what reflective AI might look like. Then, drawing on
notions of reflection in complex systems, cognitive science, and agents, we
sketch an architecture for reflective AI agents, and highlight ways forward.",None,-1
51af7c13-d1fd-4e74-b92a-d4c198e813b6,Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling,0.417397,"Convolutional neural networks encode images through a sequence of
convolutions, normalizations and non-linearities as well as downsampling
operations into potentially strong semantic embeddings. Yet, previous work
showed that even slight mistakes during sampling, leading to aliasing, can be
directly attributed to the networks' lack in robustness. To address such issues
and facilitate simpler and faster adversarial training, [12] recently proposed
FLC pooling, a method for provably alias-free downsampling - in theory. In this
work, we conduct a further analysis through the lens of signal processing and
find that such current pooling methods, which address aliasing in the frequency
domain, are still prone to spectral leakage artifacts. Hence, we propose
aliasing and spectral artifact-free pooling, short ASAP. While only introducing
a few modifications to FLC pooling, networks using ASAP as downsampling method
exhibit higher native robustness against common corruptions, a property that
FLC pooling was missing. ASAP also increases native robustness against
adversarial attacks on high and low resolution data while maintaining similar
clean accuracy or even outperforming the baseline.",None,-1
1d51748e-0434-4ca8-a69e-c5c461584ea8,Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost,0.437157,"Medical artificial general intelligence (AGI) is an emerging field that aims
to develop systems specifically designed for medical applications that possess
the ability to understand, learn, and apply knowledge across a wide range of
tasks and domains. Large language models (LLMs) represent a significant step
towards AGI. However, training cross-domain LLMs in the medical field poses
significant challenges primarily attributed to the requirement of collecting
data from diverse domains. This task becomes particularly difficult due to
privacy restrictions and the scarcity of publicly available medical datasets.
Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific
medical LLMs with the lowest cost, and suggest a possible path to achieve
medical AGI. With an increasing number of domain-specific professional
multimodal LLMs in the medical field being developed, MedAGI is designed to
automatically select appropriate medical models by analyzing users' questions
with our novel adaptive expert selection algorithm. It offers a unified
approach to existing LLMs in the medical field, eliminating the need for
retraining regardless of the introduction of new models. This characteristic
renders it a future-proof solution in the dynamically advancing medical domain.
To showcase the resilience of MedAGI, we conducted an evaluation across three
distinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis
of pathology pictures. The results demonstrated that MedAGI exhibited
remarkable versatility and scalability, delivering exceptional performance
across diverse domains. Our code is publicly available to facilitate further
research at https://github.com/JoshuaChou2018/MedAGI.",None,-1
0a7f5d7a-203b-48ca-81c9-eda30dae52a3,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,0.872206,"While multi-modal foundation models pre-trained on large-scale data have been
successful in natural language understanding and vision recognition, their use
in medical domains is still limited due to the fine-grained nature of medical
tasks and the high demand for domain knowledge. To address this challenge, we
propose a novel approach called Knowledge-enhanced Auto Diagnosis (KAD) which
leverages existing medical domain knowledge to guide vision-language
pre-training using paired chest X-rays and radiology reports. We evaluate KAD
on {four} external X-ray datasets and demonstrate that its zero-shot
performance is not only comparable to that of fully-supervised models, but also
superior to the average of three expert radiologists for three (out of five)
pathologies with statistical significance. Moreover, when few-shot annotation
is available, KAD outperforms all existing approaches in fine-tuning settings,
demonstrating its potential for application in different clinical scenarios.",None,-1
5c9b235a-166a-475f-b618-91bc305cf382,Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment,0.913031,"The multi-modal entity alignment (MMEA) aims to find all equivalent entity
pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and
neighboring entities are valuable for the alignment task, but existing works
ignore contextual gap problems that the aligned entities have different numbers
of attributes on specific modality when learning entity representations. In
this paper, we propose a novel attribute-consistent knowledge graph
representation learning framework for MMEA (ACK-MMEA) to compensate the
contextual gaps through incorporating consistent alignment knowledge.
Attribute-consistent KGs (ACKGs) are first constructed via multi-modal
attribute uniformization with merge and generate operators so that each entity
has one and only one uniform feature in each modality. The ACKGs are then fed
into a relation-aware graph neural network with random dropouts, to obtain
aggregated relation representations and robust entity representations. In order
to evaluate the ACK-MMEA facilitated for entity alignment, we specially design
a joint alignment loss for both entity and attribute evaluation. Extensive
experiments conducted on two benchmark datasets show that our approach achieves
excellent performance compared to its competitors.",None,-1
8215d7cf-8c15-4391-bdce-ea41c1c6f3ea,Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance,0.893414,"Robust obstacle avoidance is one of the critical steps for successful
goal-driven indoor navigation tasks.Due to the obstacle missing in the visual
image and the possible missed detection issue, visual image-based obstacle
avoidance techniques still suffer from unsatisfactory robustness. To mitigate
it, in this paper, we propose a novel implicit obstacle map-driven indoor
navigation framework for robust obstacle avoidance, where an implicit obstacle
map is learned based on the historical trial-and-error experience rather than
the visual image. In order to further improve the navigation efficiency, a
non-local target memory aggregation module is designed to leverage a non-local
network to model the intrinsic relationship between the target semantic and the
target orientation clues during the navigation process so as to mine the most
target-correlated object clues for the navigation decision. Extensive
experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent
obstacle avoidance and navigation efficiency of our proposed method. The core
source code is available at https://github.com/xwaiyy123/object-navigation.",None,-1
05930784-8204-4107-b710-53a03902c038,Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation,0.843233,"Understanding and manipulating deformable objects (e.g., ropes and fabrics)
is an essential yet challenging task with broad applications. Difficulties come
from complex states and dynamics, diverse configurations and high-dimensional
action space of deformable objects. Besides, the manipulation tasks usually
require multiple steps to accomplish, and greedy policies may easily lead to
local optimal states. Existing studies usually tackle this problem using
reinforcement learning or imitating expert demonstrations, with limitations in
modeling complex states or requiring hand-crafted expert policies. In this
paper, we study deformable object manipulation using dense visual affordance,
with generalization towards diverse states, and propose a novel kind of
foresightful dense affordance, which avoids local optima by estimating states'
values for long-term manipulation. We propose a framework for learning this
representation, with novel designs such as multi-stage stable learning and
efficient self-supervised data collection without experts. Experiments
demonstrate the superiority of our proposed foresightful dense affordance.
Project page: https://hyperplane-lab.github.io/DeformableAffordance",None,-1
703c0341-cb93-464e-a684-cc5ef0151ab6,Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints,0.522158,"In safety-critical robotic tasks, potential failures must be reduced, and
multiple constraints must be met, such as avoiding collisions, limiting energy
consumption, and maintaining balance. Thus, applying safe reinforcement
learning (RL) in such robotic tasks requires to handle multiple constraints and
use risk-averse constraints rather than risk-neutral constraints. To this end,
we propose a trust region-based safe RL algorithm for multiple constraints
called a safe distributional actor-critic (SDAC). Our main contributions are as
follows: 1) introducing a gradient integration method to manage infeasibility
issues in multi-constrained problems, ensuring theoretical convergence, and 2)
developing a TD($\lambda$) target distribution to estimate risk-averse
constraints with low biases. We evaluate SDAC through extensive experiments
involving multi- and single-constrained robotic tasks. While maintaining high
scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in
multi-constrained tasks and 1.78 times fewer constraint violations in
single-constrained tasks compared to safe RL baselines. Code is available at:
https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.",None,-1
8649af88-798a-4150-bf29-a777f25ba370,Recursive Joint Attention for Audio-Visual Fusion in Regression based Emotion Recognition,0.39664,"In video-based emotion recognition (ER), it is important to effectively
leverage the complementary relationship among audio (A) and visual (V)
modalities, while retaining the intra-modal characteristics of individual
modalities. In this paper, a recursive joint attention model is proposed along
with long short-term memory (LSTM) modules for the fusion of vocal and facial
expressions in regression-based ER. Specifically, we investigated the
possibility of exploiting the complementary nature of A and V modalities using
a joint cross-attention model in a recursive fashion with LSTMs to capture the
intra-modal temporal dependencies within the same modalities as well as among
the A-V feature representations. By integrating LSTMs with recursive joint
cross-attention, our model can efficiently leverage both intra- and inter-modal
relationships for the fusion of A and V modalities. The results of extensive
experiments performed on the challenging Affwild2 and Fatigue (private)
datasets indicate that the proposed A-V fusion model can significantly
outperform state-of-art-methods.",None,-1
780a5648-0f77-4fcc-8caf-a7920c550d17,Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models,0.936578,"Large-scale text-to-image diffusion models achieve unprecedented success in
image generation and editing. However, how to extend such success to video
editing is unclear. Recent initial attempts at video editing require
significant text-to-video data and computation resources for training, which is
often not accessible. In this work, we propose vid2vid-zero, a simple yet
effective method for zero-shot video editing. Our vid2vid-zero leverages
off-the-shelf image diffusion models, and doesn't require training on any
video. At the core of our method is a null-text inversion module for
text-to-video alignment, a cross-frame modeling module for temporal
consistency, and a spatial regularization module for fidelity to the original
video. Without any training, we leverage the dynamic nature of the attention
mechanism to enable bi-directional temporal modeling at test time. Experiments
and analyses show promising results in editing attributes, subjects, places,
etc., in real-world videos. Code is made available at
\url{https://github.com/baaivision/vid2vid-zero}.",None,-1
219b41b3-3d64-485b-ab55-dc531b4f8f57,How Fragile is Relation Extraction under Entity Replacements?,0.246303,"Relation extraction (RE) aims to extract the relations between entity names
from the textual context. In principle, textual context determines the
ground-truth relation and the RE models should be able to correctly identify
the relations reflected by the textual context. However, existing work has
found that the RE models memorize the entity name patterns to make RE
predictions while ignoring the textual context. This motivates us to raise the
question: ``are RE models robust to the entity replacements?'' In this work, we
operate the random and type-constrained entity replacements over the RE
instances in TACRED and evaluate the state-of-the-art RE models under the
entity replacements. We observe the 30\% - 50\% F1 score drops on the
state-of-the-art RE models under entity replacements. These results suggest
that we need more efforts to develop effective RE models robust to entity
replacements. We release the source code at
https://github.com/wangywUST/RobustRE.",None,-1
f65c5823-c106-4512-811a-c3057e6c5df2,Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning,0.541887,"Adapting to regularities of the environment is critical for biological
organisms to anticipate events and plan. A prominent example is the circadian
rhythm corresponding to the internalization by organisms of the $24$-hour
period of the Earth's rotation. In this work, we study the emergence of
circadian-like rhythms in deep reinforcement learning agents. In particular, we
deployed agents in an environment with a reliable periodic variation while
solving a foraging task. We systematically characterize the agent's behavior
during learning and demonstrate the emergence of a rhythm that is endogenous
and entrainable. Interestingly, the internal rhythm adapts to shifts in the
phase of the environmental signal without any re-training. Furthermore, we show
via bifurcation and phase response curve analyses how artificial neurons
develop dynamics to support the internalization of the environmental rhythm.
From a dynamical systems view, we demonstrate that the adaptation proceeds by
the emergence of a stable periodic orbit in the neuron dynamics with a phase
response that allows an optimal phase synchronisation between the agent's
dynamics and the environmental rhythm.",None,-1
fff648e4-f4fb-4d5d-adac-3447a1cd5757,Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution,0.47626,"This paper presents a novel Diffusion-Wavelet (DiWa) approach for
Single-Image Super-Resolution (SISR). It leverages the strengths of Denoising
Diffusion Probabilistic Models (DDPMs) and Discrete Wavelet Transformation
(DWT). By enabling DDPMs to operate in the DWT domain, our DDPM models
effectively hallucinate high-frequency information for super-resolved images on
the wavelet spectrum, resulting in high-quality and detailed reconstructions in
image space. Quantitatively, we outperform state-of-the-art diffusion-based
SISR methods, namely SR3 and SRDiff, regarding PSNR, SSIM, and LPIPS on both
face (8x scaling) and general (4x scaling) SR benchmarks. Meanwhile, using DWT
enabled us to use fewer parameters than the compared models: 92M parameters
instead of 550M compared to SR3 and 9.3M instead of 12M compared to SRDiff.
Additionally, our method outperforms other state-of-the-art generative methods
on classical general SR datasets while saving inference time. Finally, our work
highlights its potential for various applications.",None,-1
37cb3538-c301-417d-a43b-1bfbbbeff671,Data-Free Distillation of Language Model by Text-to-Text Transfer,0.252638,"Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the
model when original training data is unavailable. Previous works for DFKD in
NLP mainly focus on distilling encoder-only structures like BERT on
classification tasks, which overlook the notable progress of generative
language modeling. In this work, we propose a novel DFKD framework, namely
DFKD-T$^{3}$, where the pretrained generative language model can also serve as
a controllable data generator for model compression. This novel framework
DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to
transform the general domain corpus to compression-friendly task data,
targeting to improve both the \textit{specificity} and \textit{diversity}.
Extensive experiments show that our method can boost the distillation
performance in various downstream tasks such as sentiment analysis, linguistic
acceptability, and information extraction. Furthermore, we show that the
generated texts can be directly used for distilling other language models and
outperform the SOTA methods, making our method more appealing in a general DFKD
setting. Our code is available at
https://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\_T3.",None,-1
b2e7570b-ba6e-42a4-a5fa-ae57f88f9990,Inferring Fluid Dynamics via Inverse Rendering,0.414274,"Humans have a strong intuitive understanding of physical processes such as
fluid falling by just a glimpse of such a scene picture, i.e., quickly derived
from our immersive visual experiences in memory. This work achieves such a
photo-to-fluid-dynamics reconstruction functionality learned from unannotated
videos, without any supervision of ground-truth fluid dynamics. In a nutshell,
a differentiable Euler simulator modeled with a ConvNet-based pressure
projection solver, is integrated with a volumetric renderer, supporting
end-to-end/coherent differentiable dynamic simulation and rendering. By
endowing each sampled point with a fluid volume value, we derive a NeRF-like
differentiable renderer dedicated from fluid data; and thanks to this
volume-augmented representation, fluid dynamics could be inversely inferred
from the error signal between the rendered result and ground-truth video frame
(i.e., inverse rendering). Experiments on our generated Fluid Fall datasets and
DPI Dam Break dataset are conducted to demonstrate both effectiveness and
generalization ability of our method.",None,-1
8f871cc8-f3fb-40f3-a477-9bd75da858cd,XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words,0.416717,"Due to the absence of explicit word boundaries in the speech stream, the task
of segmenting spoken sentences into word units without text supervision is
particularly challenging. In this work, we leverage the most recent
self-supervised speech models that have proved to quickly adapt to new tasks
through fine-tuning, even in low resource conditions. Taking inspiration from
semi-supervised learning, we fine-tune an XLS-R model to predict word
boundaries themselves produced by top-tier speech segmentation systems: DPDP,
VG-HuBERT, GradSeg and DP-Parse. Once XLS-R is fine-tuned, it is used to infer
new word boundary labels that are used in turn for another fine-tuning step.
Our method consistently improves the performance of each system and sets a new
state-of-the-art that is, on average 130% higher than the previous one as
measured by the F1 score on correctly discovered word tokens on five corpora
featuring different languages. Finally, our system can segment speech from
languages unseen during fine-tuning in a zero-shot fashion.",None,-1
43883dcd-a617-467b-ae70-9a6f8fbdbf94,Descriptive Knowledge Graph in Biomedical Domain,0.275798,"We present a novel system that automatically extracts and generates
informative and descriptive sentences from the biomedical corpus and
facilitates the efficient search for relational knowledge. Unlike previous
search engines or exploration systems that retrieve unconnected passages, our
system organizes descriptive sentences as a relational graph, enabling
researchers to explore closely related biomedical entities (e.g., diseases
treated by a chemical) or indirectly connected entities (e.g., potential drugs
for treating a disease). Our system also uses ChatGPT and a fine-tuned relation
synthesis model to generate concise and reliable descriptive sentences from
retrieved information, reducing the need for extensive human reading effort.
With our system, researchers can easily obtain both high-level knowledge and
detailed references and interactively steer to the information of interest. We
spotlight the application of our system in COVID-19 research, illustrating its
utility in areas such as drug repurposing and literature curation.",None,-1
bc993636-9c57-40db-84bd-e7235235fbec,CCGen: Explainable Complementary Concept Generation in E-Commerce,0.565221,"We propose and study Complementary Concept Generation (CCGen): given a
concept of interest, e.g., ""Digital Cameras"", generating a list of
complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)
Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications
like query suggestion and item recommendation, especially in the e-commerce
domain. To solve CCGen, we propose to train language models to generate ranked
lists of concepts with a two-step training strategy. We also teach the models
to generate explanations by incorporating explanations distilled from large
teacher models. Extensive experiments and analysis demonstrate that our model
can generate high-quality concepts complementary to the input concept while
producing explanations to justify the predictions.",None,-1
9246d1d9-d27d-43d6-b59d-c2f5d95032ad,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,0.284027,"This paper focuses on Winograd transformation in 3D convolutional neural
networks (CNNs) that are more over-parameterized compared with the 2D version.
The over-increasing Winograd parameters not only exacerbate training complexity
but also barricade the practical speedups due simply to the volume of
element-wise products in the Winograd domain. We attempt to reduce trainable
parameters by introducing a low-rank Winograd transformation, a novel training
paradigm that decouples the original large tensor into two less
storage-required trainable tensors, leading to a significant complexity
reduction. Built upon our low-rank Winograd transformation, we take one step
ahead by proposing a low-rank oriented sparse granularity that measures
column-wise parameter importance. By simply involving the non-zero columns in
the element-wise product, our sparse granularity is empowered with the ability
to produce a very regular sparse pattern to acquire effectual Winograd
speedups. To better understand the efficacy of our method, we perform extensive
experiments on 3D CNNs. Results manifest that our low-rank Winograd
transformation well outperforms the vanilla Winograd transformation. We also
show that our proposed low-rank oriented sparse granularity permits practical
Winograd acceleration compared with the vanilla counterpart.",None,-1
29f809e8-6d63-4b55-bbab-593119111299,Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning,0.577294,"Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph
(KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial
task that aims to predict future facts based on historical occurrences. The key
challenge lies in uncovering structural dependencies within historical
subgraphs and temporal patterns. Most existing approaches model TKGs relying on
entity modeling, as nodes in the graph play a crucial role in knowledge
representation. However, the real-world scenario often involves an extensive
number of entities, with new entities emerging over time. This makes it
challenging for entity-dependent methods to cope with extensive volumes of
entities, and effectively handling newly emerging entities also becomes a
significant challenge. Therefore, we propose Temporal Inductive Path Neural
Network (TiPNN), which models historical information in an entity-independent
perspective. Specifically, TiPNN adopts a unified graph, namely history
temporal graph, to comprehensively capture and encapsulate information from
history. Subsequently, we utilize the defined query-aware temporal paths on a
history temporal graph to model historical path information related to queries
for reasoning. Extensive experiments illustrate that the proposed model not
only attains significant performance enhancements but also handles inductive
settings, while additionally facilitating the provision of reasoning evidence
through history temporal graphs.",None,-1
30fd59f0-2174-4c0b-b6d2-ddc6ae1d00c6,Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities,0.304948,"Recent advances in artificial general intelligence (AGI), particularly large
language models and creative image generation systems have demonstrated
impressive capabilities on diverse tasks spanning the arts and humanities.
However, the swift evolution of AGI has also raised critical questions about
its responsible deployment in these culturally significant domains
traditionally seen as profoundly human. This paper provides a comprehensive
analysis of the applications and implications of AGI for text, graphics, audio,
and video pertaining to arts and the humanities. We survey cutting-edge systems
and their usage in areas ranging from poetry to history, marketing to film, and
communication to classical art. We outline substantial concerns pertaining to
factuality, toxicity, biases, and public safety in AGI systems, and propose
mitigation strategies. The paper argues for multi-stakeholder collaboration to
ensure AGI promotes creativity, knowledge, and cultural values without
undermining truth or human dignity. Our timely contribution summarizes a
rapidly developing field, highlighting promising directions while advocating
for responsible progress centering on human flourishing. The analysis lays the
groundwork for further research on aligning AGI's technological capacities with
enduring social goods.",None,-1
c51a4485-bf57-4205-b49e-f2d7fa5afbe9,High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage,0.147475,"Visual inspection is predominantly used to evaluate the state of civil
structures, but recent developments in unmanned aerial vehicles (UAVs) and
artificial intelligence have increased the speed, safety, and reliability of
the inspection process. In this study, we develop a semantic segmentation
network based on vision transformers and Laplacian pyramids scaling networks
for efficiently parsing high-resolution visual inspection images. The massive
amounts of collected high-resolution images during inspections can slow down
the investigation efforts. And while there have been extensive studies
dedicated to the use of deep learning models for damage segmentation,
processing high-resolution visual data can pose major computational
difficulties. Traditionally, images are either uniformly downsampled or
partitioned to cope with computational demands. However, the input is at risk
of losing local fine details, such as thin cracks, or global contextual
information. Inspired by super-resolution architectures, our vision transformer
model learns to resize high-resolution images and masks to retain both the
valuable local features and the global semantics without sacrificing
computational efficiency. The proposed framework has been evaluated through
comprehensive experiments on a dataset of bridge inspection report images using
multiple metrics for pixel-wise materials detection.",None,-1
64d191f1-7481-459e-8468-065342716ff9,Leveraging Summary Guidance on Medical Report Summarization,0.204942,"This study presents three deidentified large medical text datasets, named
DISCHARGE, ECHO and RADIOLOGY, which contain 50K, 16K and 378K pairs of report
and summary that are derived from MIMIC-III, respectively. We implement
convincing baselines of automated abstractive summarization on the proposed
datasets with pre-trained encoder-decoder language models, including BERT2BERT,
T5-large and BART. Further, based on the BART model, we leverage the sampled
summaries from the train set as prior knowledge guidance, for encoding
additional contextual representations of the guidance with the encoder and
enhancing the decoding representations in the decoder. The experimental results
confirm the improvement of ROUGE scores and BERTScore made by the proposed
method, outperforming the larger model T5-large.",None,-1
6e955b31-6eb2-4e49-8de1-cf80f357b697,Linguistic ambiguity analysis in ChatGPT,0.335841,"Linguistic ambiguity is and has always been one of the main challenges in
Natural Language Processing (NLP) systems. Modern Transformer architectures
like BERT, T5 or more recently InstructGPT have achieved some impressive
improvements in many NLP fields, but there is still plenty of work to do.
Motivated by the uproar caused by ChatGPT, in this paper we provide an
introduction to linguistic ambiguity, its varieties and their relevance in
modern NLP, and perform an extensive empiric analysis. ChatGPT strengths and
weaknesses are revealed, as well as strategies to get the most of this model.",None,-1
c2661a27-4339-4539-86df-e07c9068c6d8,Efficient Real Time Recurrent Learning through combined activity and parameter sparsity,0.0846928,"Backpropagation through time (BPTT) is the standard algorithm for training
recurrent neural networks (RNNs), which requires separate simulation phases for
the forward and backward passes for inference and learning, respectively.
Moreover, BPTT requires storing the complete history of network states between
phases, with memory consumption growing proportional to the input sequence
length. This makes BPTT unsuited for online learning and presents a challenge
for implementation on low-resource real-time systems. Real-Time Recurrent
Learning (RTRL) allows online learning, and the growth of required memory is
independent of sequence length. However, RTRL suffers from exceptionally high
computational costs that grow proportional to the fourth power of the state
size, making RTRL computationally intractable for all but the smallest of
networks. In this work, we show that recurrent networks exhibiting high
activity sparsity can reduce the computational cost of RTRL. Moreover,
combining activity and parameter sparsity can lead to significant enough
savings in computational and memory costs to make RTRL practical. Unlike
previous work, this improvement in the efficiency of RTRL can be achieved
without using any approximations for the learning process.",None,-1
39239bb3-6269-49c3-90d1-23cede34f3b6,IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction,0.845477,"Reliable multi-agent trajectory prediction is crucial for the safe planning
and control of autonomous systems. Compared with single-agent cases, the major
challenge in simultaneously processing multiple agents lies in modeling complex
social interactions caused by various driving intentions and road conditions.
Previous methods typically leverage graph-based message propagation or
attention mechanism to encapsulate such interactions in the format of marginal
probabilistic distributions. However, it is inherently sub-optimal. In this
paper, we propose IPCC-TP, a novel relevance-aware module based on Incremental
Pearson Correlation Coefficient to improve multi-agent interaction modeling.
IPCC-TP learns pairwise joint Gaussian Distributions through the
tightly-coupled estimation of the means and covariances according to
interactive incremental movements. Our module can be conveniently embedded into
existing multi-agent prediction methods to extend original motion distribution
decoders. Extensive experiments on nuScenes and Argoverse 2 datasets
demonstrate that IPCC-TP improves the performance of baselines by a large
margin.",None,-1
03764a62-e430-40f6-b73b-dca0d9201883,UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View,0.894304,"In the field of 3D object detection for autonomous driving, the sensor
portfolio including multi-modality and single-modality is diverse and complex.
Since the multi-modal methods have system complexity while the accuracy of
single-modal ones is relatively low, how to make a tradeoff between them is
difficult. In this work, we propose a universal cross-modality knowledge
distillation framework (UniDistill) to improve the performance of
single-modality detectors. Specifically, during training, UniDistill projects
the features of both the teacher and the student detector into Bird's-Eye-View
(BEV), which is a friendly representation for different modalities. Then, three
distillation losses are calculated to sparsely align the foreground features,
helping the student learn from the teacher without introducing additional cost
during inference. Taking advantage of the similar detection paradigm of
different detectors in BEV, UniDistill easily supports LiDAR-to-camera,
camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths.
Furthermore, the three distillation losses can filter the effect of misaligned
background information and balance between objects of different sizes,
improving the distillation effectiveness. Extensive experiments on nuScenes
demonstrate that UniDistill effectively improves the mAP and NDS of student
detectors by 2.0%~3.2%.",None,-1
b981f944-df27-4a2d-ad1d-3adfffc1154e,Exploiting Language Relatedness in Machine Translation Through Domain Adaptation Techniques,0.0799428,"One of the significant challenges of Machine Translation (MT) is the scarcity
of large amounts of data, mainly parallel sentence aligned corpora. If the
evaluation is as rigorous as resource-rich languages, both Neural Machine
Translation (NMT) and Statistical Machine Translation (SMT) can produce good
results with such large amounts of data. However, it is challenging to improve
the quality of MT output for low resource languages, especially in NMT and SMT.
In order to tackle the challenges faced by MT, we present a novel approach of
using a scaled similarity score of sentences, especially for related languages
based on a 5-gram KenLM language model with Kneser-ney smoothing technique for
filtering in-domain data from out-of-domain corpora that boost the translation
quality of MT. Furthermore, we employ other domain adaptation techniques such
as multi-domain, fine-tuning and iterative back-translation approach to compare
our novel approach on the Hindi-Nepali language pair for NMT and SMT. Our
approach succeeds in increasing ~2 BLEU point on multi-domain approach, ~3 BLEU
point on fine-tuning for NMT and ~2 BLEU point on iterative back-translation
approach.",None,-1
20eab739-7cdb-4e9e-b9cb-fe19605683fb,"Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",0.583092,"Online education platforms, leveraging the internet to distribute education
resources, seek to provide convenient education but often fall short in
real-time communication with students. They often struggle to address the
diverse obstacles students encounter throughout their learning journey. Solving
the problems encountered by students poses a significant challenge for
traditional deep learning models, as it requires not only a broad spectrum of
subject knowledge but also the ability to understand what constitutes a
student's individual difficulties. It's challenging for traditional machine
learning models, as they lack the capacity to comprehend students' personalized
needs. Recently, the emergence of large language models (LLMs) offers the
possibility for resolving this issue by comprehending individual requests.
Although LLMs have been successful in various fields, creating an LLM-based
education system is still challenging for the wide range of educational skills
required. This paper reviews the recently emerged LLM research related to
educational capabilities, including mathematics, writing, programming,
reasoning, and knowledge-based question answering, with the aim to explore
their potential in constructing the next-generation intelligent education
system. Specifically, for each capability, we focus on investigating two
aspects. Firstly, we examine the current state of LLMs regarding this
capability: how advanced they have become, whether they surpass human
abilities, and what deficiencies might exist. Secondly, we evaluate whether the
development methods for LLMs in this area are generalizable, that is, whether
these methods can be applied to construct a comprehensive educational
supermodel with strengths across various capabilities, rather than being
effective in only a singular aspect.",None,-1
9dd79f3e-487a-40c8-8b32-36858bb7088c,"TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",0.723464,"Tables are prevalent in real-world databases, requiring significant time and
effort for humans to analyze and manipulate. The advancements in large language
models (LLMs) have made it possible to interact with tables using natural
language input, bringing this capability closer to reality. In this paper, we
present TableGPT, a unified fine-tuned framework that enables LLMs to
understand and operate on tables using external functional commands. It
introduces the capability to seamlessly interact with tables, enabling a wide
range of functionalities such as question answering, data manipulation (e.g.,
insert, delete, query, and modify operations), data visualization, analysis
report generation, and automated prediction. TableGPT aims to provide
convenience and accessibility to users by empowering them to effortlessly
leverage tabular data. At the core of TableGPT lies the novel concept of global
tabular representations, which empowers LLMs to gain a comprehensive
understanding of the entire table beyond meta-information. By jointly training
LLMs on both table and text modalities, TableGPT achieves a deep understanding
of tabular data and the ability to perform complex operations on tables through
chain-of-command instructions. Importantly, TableGPT offers the advantage of
being a self-contained system rather than relying on external API interfaces.
Moreover, it supports efficient data process flow, query rejection (when
appropriate) and private deployment, enabling faster domain data fine-tuning
and ensuring data privacy, which enhances the framework's adaptability to
specific use cases.",None,-1
b09148f1-e81f-4144-8e8d-f76a79b1bd6e,Robust Robot Planning for Human-Robot Collaboration,0.324454,"In human-robot collaboration, the objectives of the human are often unknown
to the robot. Moreover, even assuming a known objective, the human behavior is
also uncertain. In order to plan a robust robot behavior, a key preliminary
question is then: How to derive realistic human behaviors given a known
objective? A major issue is that such a human behavior should itself account
for the robot behavior, otherwise collaboration cannot happen. In this paper,
we rely on Markov decision models, representing the uncertainty over the human
objective as a probability distribution over a finite set of objective
functions (inducing a distribution over human behaviors). Based on this, we
propose two contributions: 1) an approach to automatically generate an
uncertain human behavior (a policy) for each given objective function while
accounting for possible robot behaviors; and 2) a robot planning algorithm that
is robust to the above-mentioned uncertainties and relies on solving a
partially observable Markov decision process (POMDP) obtained by reasoning on a
distribution over human behaviors. A co-working scenario allows conducting
experiments and presenting qualitative and quantitative results to evaluate our
approach.",None,-1
f89595fa-9933-44d3-b78e-58679b95ce82,Use neural networks to recognize students' handwritten letters and incorrect symbols,0.500889,"Correcting students' multiple-choice answers is a repetitive and mechanical
task that can be considered an image multi-classification task. Assuming
possible options are 'abcd' and the correct option is one of the four, some
students may write incorrect symbols or options that do not exist. In this
paper, five classifications were set up - four for possible correct options and
one for other incorrect writing. This approach takes into account the
possibility of non-standard writing options.",None,-1
99bc7bb9-70b3-4479-b66c-f4250b1a7665,SimDA: Simple Diffusion Adapter for Efficient Video Generation,0.947908,"The recent wave of AI-generated content has witnessed the great development
and success of Text-to-Image (T2I) technologies. By contrast, Text-to-Video
(T2V) still falls short of expectations though attracting increasing interests.
Existing works either train from scratch or adapt large T2I model to videos,
both of which are computation and resource expensive. In this work, we propose
a Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1B
parameters of a strong T2I model, adapting it to video generation in a
parameter-efficient way. In particular, we turn the T2I model for T2V by
designing light-weight spatial and temporal adapters for transfer learning.
Besides, we change the original spatial attention to the proposed Latent-Shift
Attention (LSA) for temporal consistency. With similar model architecture, we
further train a video super-resolution model to generate high-definition
(1024x1024) videos. In addition to T2V generation in the wild, SimDA could also
be utilized in one-shot video editing with only 2 minutes tuning. Doing so, our
method could minimize the training effort with extremely few tunable parameters
for model adaptation.",None,-1
c53acaf8-9af4-4899-983d-d156efaa111d,Benchmarking bias: Expanding clinical AI model card to incorporate bias reporting of social and non-social factors,0.303575,"Clinical AI model reporting cards should be expanded to incorporate a broad
bias reporting of both social and non-social factors. Non-social factors
consider the role of other factors, such as disease dependent, anatomic, or
instrument factors on AI model bias, which are essential to ensure safe
deployment.",None,-1
7ac7b534-67dc-4b0b-968c-5c784d17775f,Tracking Progress in Multi-Agent Path Finding,0.89821,"Multi-Agent Path Finding (MAPF) is an important core problem for many new and
emerging industrial applications. Many works appear on this topic each year,
and a large number of substantial advancements and performance improvements
have been reported. Yet measuring overall progress in MAPF is difficult: there
are many potential competitors, and the computational burden for comprehensive
experimentation is prohibitively large. Moreover, detailed data from past
experimentation is usually unavailable. In this work, we introduce a set of
methodological and visualisation tools which can help the community establish
clear indicators for state-of-the-art MAPF performance and which can facilitate
large-scale comparisons between MAPF solvers. Our objectives are to lower the
barrier of entry for new researchers and to further promote the study of MAPF,
since progress in the area and the main challenges are made much clearer.",None,-1
5f3a6a8b-2b84-4a2d-9394-5ef5365bbf9d,Modernizing Old Photos Using Multiple References via Photorealistic Style Transfer,0.487612,"This paper firstly presents old photo modernization using multiple references
by performing stylization and enhancement in a unified manner. In order to
modernize old photos, we propose a novel multi-reference-based old photo
modernization (MROPM) framework consisting of a network MROPM-Net and a novel
synthetic data generation scheme. MROPM-Net stylizes old photos using multiple
references via photorealistic style transfer (PST) and further enhances the
results to produce modern-looking images. Meanwhile, the synthetic data
generation scheme trains the network to effectively utilize multiple references
to perform modernization. To evaluate the performance, we propose a new old
photos benchmark dataset (CHD) consisting of diverse natural indoor and outdoor
scenes. Extensive experiments show that the proposed method outperforms other
baselines in performing modernization on real old photos, even though no old
photos were used during training. Moreover, our method can appropriately select
styles from multiple references for each semantic region in the old photo to
further improve the modernization performance.",None,-1
34c435e1-6e69-41e1-9133-6630d03ff45c,Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information,0.819576,"Visual Word Sense Disambiguation (VWSD) is a task to find the image that most
accurately depicts the correct sense of the target word for the given context.
Previously, image-text matching models often suffered from recognizing
polysemous words. This paper introduces an unsupervised VWSD approach that uses
gloss information of an external lexical knowledge-base, especially the sense
definitions. Specifically, we suggest employing Bayesian inference to
incorporate the sense definitions when sense information of the answer is not
provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we
propose a context-aware definition generation with GPT-3. Experimental results
show that the VWSD performance significantly increased with our Bayesian
inference-based approach. In addition, our context-aware definition generation
achieved prominent performance improvement in OOD examples exhibiting better
performance than the existing definition generation method.",None,-1
e66e6d24-3c96-48f9-9a81-ad9e57d041fc,AutoAD: Movie Description in Context,0.447745,"The objective of this paper is an automatic Audio Description (AD) model that
ingests movies and outputs AD in text form. Generating high-quality movie AD is
challenging due to the dependency of the descriptions on context, and the
limited amount of training data available. In this work, we leverage the power
of pretrained foundation models, such as GPT and CLIP, and only train a mapping
network that bridges the two models for visually-conditioned text generation.
In order to obtain high-quality AD, we make the following four contributions:
(i) we incorporate context from the movie clip, AD from previous clips, as well
as the subtitles; (ii) we address the lack of training data by pretraining on
large-scale datasets, where visual or contextual information is unavailable,
e.g. text-only AD without movies or visual captioning datasets without context;
(iii) we improve on the currently available AD datasets, by removing label
noise in the MAD dataset, and adding character naming information; and (iv) we
obtain strong results on the movie AD task compared with previous methods.",None,-1
e1cd1960-b5cc-407d-ad8a-d44cb8d8feb3,TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications,0.67874,"We introduce TabRepo, a new dataset of tabular model evaluations and
predictions. TabRepo contains the predictions and metrics of 1310 models
evaluated on 200 classification and regression datasets. We illustrate the
benefit of our dataset in multiple ways. First, we show that it allows to
perform analysis such as comparing Hyperparameter Optimization against current
AutoML systems while also considering ensembling at marginal cost by using
precomputed model predictions. Second, we show that our dataset can be readily
leveraged to perform transfer-learning. In particular, we show that applying
standard transfer-learning techniques allows to outperform current
state-of-the-art tabular systems in accuracy, runtime and latency.",None,-1
43dc8c4a-e87d-4620-a711-1c1ee3ba87ab,Analysis of Recent Trends in Face Recognition Systems,0.406701,"With the tremendous advancements in face recognition technology, face
modality has been widely recognized as a significant biometric identifier in
establishing a person's identity rather than any other biometric trait like
fingerprints that require contact sensors. However, due to inter-class
similarities and intra-class variations, face recognition systems generate
false match and false non-match errors respectively. Recent research focuses on
improving the robustness of extracted features and the pre-processing
algorithms to enhance recognition accuracy. Since face recognition has been
extensively used for several applications ranging from law enforcement to
surveillance systems, the accuracy and performance of face recognition must be
the finest. In this paper various face recognition systems are discussed and
analysed like RPRV, LWKPCA, SVM Model, LTrP based SPM and a deep learning
framework for recognising images from CCTV. All these face recognition methods,
their implementations and performance evaluations are compared to derive the
best outcome for future developmental works.",None,-1
e3e54e61-4122-4b5f-bda5-b3a33d8fc455,TryOnDiffusion: A Tale of Two UNets,0.859621,"Given two images depicting a person and a garment worn by another person, our
goal is to generate a visualization of how the garment might look on the input
person. A key challenge is to synthesize a photorealistic detail-preserving
visualization of the garment, while warping the garment to accommodate a
significant body pose and shape change across the subjects. Previous methods
either focus on garment detail preservation without effective pose and shape
variation, or allow try-on with the desired shape and pose but lack garment
details. In this paper, we propose a diffusion-based architecture that unifies
two UNets (referred to as Parallel-UNet), which allows us to preserve garment
details and warp the garment for significant pose and body change in a single
network. The key ideas behind Parallel-UNet include: 1) garment is warped
implicitly via a cross attention mechanism, 2) garment warp and person blend
happen as part of a unified process as opposed to a sequence of two separate
tasks. Experimental results indicate that TryOnDiffusion achieves
state-of-the-art performance both qualitatively and quantitatively.",None,-1
f7850965-402e-4ce9-8250-2d505c337918,EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding,0.379022,"With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI),
large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.
However, most current research is built on resources derived from third-person
video action recognition. This inherent domain gap between first- and
third-person action videos, which have not been adequately addressed before,
makes current Ego-HOI suboptimal. This paper rethinks and proposes a new
framework as an infrastructure to advance Ego-HOI recognition by Probing,
Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets,
balanced test sets and a new baseline, which are complete with a
training-finetuning strategy. With our new framework, we not only achieve
state-of-the-art performance on Ego-HOI benchmarks but also build several new
and effective mechanisms and settings to advance further research. We believe
our data and the findings will pave a new way for Ego-HOI understanding. Code
and data are available at https://mvig-rhos.com/ego_pca",None,-1
b66f8a23-a223-4694-9c5a-6593538ccd7f,SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples,0.092815,"Detecting negatives (such as non-entailment relationships, unanswerable
questions, and false claims) is an important and challenging aspect of many
natural language understanding tasks. Though manually collecting challenging
negative examples can help models detect them, it is both costly and
domain-specific. In this work, we propose Self-labeled Counterfactuals for
Extrapolating to Negative Examples (SCENE), an automatic method for
synthesizing training data that greatly improves models' ability to detect
challenging negative examples. In contrast with standard data augmentation,
which synthesizes new examples for existing labels, SCENE can synthesize
negative examples zero-shot from only positive ones. Given a positive example,
SCENE perturbs it with a mask infilling model, then determines whether the
resulting example is negative based on a self-training heuristic. With access
to only answerable training examples, SCENE can close 69.6% of the performance
gap on SQuAD 2.0, a dataset where half of the evaluation examples are
unanswerable, compared to a model trained on SQuAD 2.0. Our method also extends
to boolean question answering and recognizing textual entailment, and improves
generalization from SQuAD to ACE-whQA, an out-of-domain extractive QA
benchmark.",None,-1
7198d1ea-6167-4441-8ca5-f97fc5422920,KGTrust: Evaluating Trustworthiness of SIoT via Knowledge Enhanced Graph Neural Networks,0.506885,"Social Internet of Things (SIoT), a promising and emerging paradigm that
injects the notion of social networking into smart objects (i.e., things),
paving the way for the next generation of Internet of Things. However, due to
the risks and uncertainty, a crucial and urgent problem to be settled is
establishing reliable relationships within SIoT, that is, trust evaluation.
Graph neural networks for trust evaluation typically adopt a straightforward
way such as one-hot or node2vec to comprehend node characteristics, which
ignores the valuable semantic knowledge attached to nodes. Moreover, the
underlying structure of SIoT is usually complex, including both the
heterogeneous graph structure and pairwise trust relationships, which renders
hard to preserve the properties of SIoT trust during information propagation.
To address these aforementioned problems, we propose a novel knowledge-enhanced
graph neural network (KGTrust) for better trust evaluation in SIoT.
Specifically, we first extract useful knowledge from users' comment behaviors
and external structured triples related to object descriptions, in order to
gain a deeper insight into the semantics of users and objects. Furthermore, we
introduce a discriminative convolutional layer that utilizes heterogeneous
graph structure, node semantics, and augmented trust relationships to learn
node embeddings from the perspective of a user as a trustor or a trustee,
effectively capturing multi-aspect properties of SIoT trust during information
propagation. Finally, a trust prediction layer is developed to estimate the
trust relationships between pairwise nodes. Extensive experiments on three
public datasets illustrate the superior performance of KGTrust over
state-of-the-art methods.",None,-1
1ea90055-1227-41ed-b1cd-b1116f901dd9,AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator,0.884432,"Designing robust machine learning systems remains an open problem, and there
is a need for benchmark problems that cover both environmental changes and
evaluation on a downstream task. In this work, we introduce AVOIDDS, a
realistic object detection benchmark for the vision-based aircraft
detect-and-avoid problem. We provide a labeled dataset consisting of 72,000
photorealistic images of intruder aircraft with various lighting conditions,
weather conditions, relative geometries, and geographic locations. We also
provide an interface that evaluates trained models on slices of this dataset to
identify changes in performance with respect to changing environmental
conditions. Finally, we implement a fully-integrated, closed-loop simulator of
the vision-based detect-and-avoid problem to evaluate trained models with
respect to the downstream collision avoidance task. This benchmark will enable
further research in the design of robust machine learning systems for use in
safety-critical applications. The AVOIDDS dataset and code are publicly
available at https://purl.stanford.edu/hj293cv5980 and
https://github.com/sisl/VisionBasedAircraftDAA respectively.",None,-1
54e2f563-6667-417e-9a45-dae62f46834a,Privacy-Preserving Prompt Tuning for Large Language Model Services,0.491618,"Prompt tuning provides an efficient way for users to customize Large Language
Models (LLMs) with their private data in the emerging LLM service scenario.
However, the sensitive nature of private data brings the need for privacy
preservation in LLM service customization. Based on prompt tuning, we propose
Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy
guarantees for LLM services. \textsc{rapt} adopts a local privacy setting,
allowing users to privatize their data locally with local differential privacy.
As prompt tuning performs poorly when directly trained on privatized data, we
introduce a novel privatized token reconstruction task that is trained jointly
with the downstream task, allowing LLMs to learn better task-dependent
representations. Despite the simplicity of our framework, experiments show that
RAPT achieves competitive performance across tasks while providing privacy
guarantees against adversaries.",None,-1
abe6d20c-e676-4c8b-aec1-15802cc3c608,You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis,0.317934,"Expressing attitude or stance toward entities and concepts is an integral
part of human behavior and personality. Recently, evaluative language data has
become more accessible with social media's rapid growth, enabling large-scale
opinion analysis. However, surprisingly little research examines the
relationship between personality and evaluative language. To bridge this gap,
we introduce the notion of evaluative topics, obtained by applying topic models
to pre-filtered evaluative text from social media. We then link evaluative
topics to individual text authors to build their evaluative profiles. We apply
evaluative profiling to Reddit comments labeled with personality scores and
conduct an exploratory study on the relationship between evaluative topics and
Big Five personality facets, aiming for a more interpretable, facet-level
analysis. Finally, we validate our approach by observing correlations
consistent with prior research in personality psychology.",None,-1
d91dd95a-cee8-41ee-a139-bcee9c6f3721,Controllable Mind Visual Diffusion Model,0.827786,"Brain signal visualization has emerged as an active research area, serving as
a critical interface between the human visual system and computer vision
models. Although diffusion models have shown promise in analyzing functional
magnetic resonance imaging (fMRI) data, including reconstructing high-quality
images consistent with original visual stimuli, their accuracy in extracting
semantic and silhouette information from brain signals remains limited. In this
regard, we propose a novel approach, referred to as Controllable Mind Visual
Diffusion Model (CMVDM). CMVDM extracts semantic and silhouette information
from fMRI data using attribute alignment and assistant networks. Additionally,
a residual block is incorporated to capture information beyond semantic and
silhouette features. We then leverage a control model to fully exploit the
extracted information for image synthesis, resulting in generated images that
closely resemble the visual stimuli in terms of semantics and silhouette.
Through extensive experimentation, we demonstrate that CMVDM outperforms
existing state-of-the-art methods both qualitatively and quantitatively.",None,-1
7b56e67f-89d3-4ecb-9bf5-383f3f9ea3a5,ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT,0.879127,"In this paper, we investigate the use of data obtained from prompting a large
generative language model, ChatGPT, to generate synthetic training data with
the aim of augmenting data in low resource scenarios. We show that with
appropriate task-specific ChatGPT prompts, we outperform the most popular
existing approaches for such data augmentation. Furthermore, we investigate
methodologies for evaluating the similarity of the augmented data generated
from ChatGPT with the aim of validating and assessing the quality of the data
generated.",None,-1
63c5ef8b-d909-4b99-a7e5-239f85b360eb,STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection,0.993273,"Recently, deep learning-based facial landmark detection has achieved
significant improvement. However, the semantic ambiguity problem degrades
detection performance. Specifically, the semantic ambiguity causes inconsistent
annotation and negatively affects the model's convergence, leading to worse
accuracy and instability prediction. To solve this problem, we propose a
Self-adapTive Ambiguity Reduction (STAR) loss by exploiting the properties of
semantic ambiguity. We find that semantic ambiguity results in the anisotropic
predicted distribution, which inspires us to use predicted distribution to
represent semantic ambiguity. Based on this, we design the STAR loss that
measures the anisotropism of the predicted distribution. Compared with the
standard regression loss, STAR loss is encouraged to be small when the
predicted distribution is anisotropic and thus adaptively mitigates the impact
of semantic ambiguity. Moreover, we propose two kinds of eigenvalue restriction
methods that could avoid both distribution's abnormal change and the model's
premature convergence. Finally, the comprehensive experiments demonstrate that
STAR loss outperforms the state-of-the-art methods on three benchmarks, i.e.,
COFW, 300W, and WFLW, with negligible computation overhead. Code is at
https://github.com/ZhenglinZhou/STAR.",None,-1
dbf6a794-510f-4c13-ad05-7f0f2d023edd,Active Neural Mapping,0.582136,"We address the problem of active mapping with a continually-learned neural
scene representation, namely Active Neural Mapping. The key lies in actively
finding the target space to be explored with efficient agent movement, thus
minimizing the map uncertainty on-the-fly within a previously unseen
environment. In this paper, we examine the weight space of the
continually-learned neural field, and show empirically that the neural
variability, the prediction robustness against random weight perturbation, can
be directly utilized to measure the instant uncertainty of the neural map.
Together with the continuous geometric information inherited in the neural map,
the agent can be guided to find a traversable path to gradually gain knowledge
of the environment. We present for the first time an active mapping system with
a coordinate-based implicit neural representation for online scene
reconstruction. Experiments in the visually-realistic Gibson and Matterport3D
environment demonstrate the efficacy of the proposed method.",None,-1
5e9d6639-4056-4ce6-8575-f3e6f6e6c93c,Response: Emergent analogical reasoning in large language models,0.0788329,"In their recent Nature Human Behaviour paper, ""Emergent analogical reasoning
in large language models,"" (Webb, Holyoak, and Lu, 2023) the authors argue that
""large language models such as GPT-3 have acquired an emergent ability to find
zero-shot solutions to a broad range of analogy problems."" In this response, we
provide counterexamples of the letter string analogies. In our tests, GPT-3
fails to solve simplest variations of the original tasks, whereas human
performance remains consistently high across all modified versions. Zero-shot
reasoning is an extraordinary claim that requires extraordinary evidence. We do
not see that evidence in our experiments. To strengthen claims of humanlike
reasoning such as zero-shot reasoning, it is important that the field develop
approaches that rule out data memorization.",None,-1
65e807d1-bdc7-45e5-9d2c-5f6a0bd46df5,SemDeDup: Data-efficient learning at web-scale through semantic deduplication,0.951353,"Progress in machine learning has been driven in large part by massive
increases in data. However, large web-scale datasets such as LAION are largely
uncurated beyond searches for exact duplicates, potentially leaving much
redundancy. Here, we introduce SemDeDup, a method which leverages embeddings
from pre-trained models to identify and remove semantic duplicates: data pairs
which are semantically similar, but not exactly identical. Removing semantic
duplicates preserves performance and speeds up learning. Analyzing a subset of
LAION, we show that SemDeDup can remove 50% of the data with minimal
performance loss, effectively halving training time. Moreover, performance
increases out of distribution. Also, analyzing language models trained on C4, a
partially curated dataset, we show that SemDeDup improves over prior approaches
while providing efficiency gains. SemDeDup provides an example of how simple
ways of leveraging quality embeddings can be used to make models learn faster
with less data.",None,-1
c42c1860-7a79-4af4-97d8-914b3dedfcba,CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection,0.612356,"Task driven object detection aims to detect object instances suitable for
affording a task in an image. Its challenge lies in object categories available
for the task being too diverse to be limited to a closed set of object
vocabulary for traditional object detection. Simply mapping categories and
visual features of common objects to the task cannot address the challenge. In
this paper, we propose to explore fundamental affordances rather than object
categories, i.e., common attributes that enable different objects to accomplish
the same task. Moreover, we propose a novel multi-level chain-of-thought
prompting (MLCoT) to extract the affordance knowledge from large language
models, which contains multi-level reasoning steps from task to object examples
to essential visual attributes with rationales. Furthermore, to fully exploit
knowledge to benefit object recognition and localization, we propose a
knowledge-conditional detection framework, namely CoTDet. It conditions the
detector from the knowledge to generate object queries and regress boxes.
Experimental results demonstrate that our CoTDet outperforms state-of-the-art
methods consistently and significantly (+15.6 box AP and +14.8 mask AP) and can
generate rationales for why objects are detected to afford the task.",None,-1
a06aa00b-bb86-418d-b53e-7087d4e7d493,Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding,0.678847,"To tackle the high inference latency exhibited by autoregressive language
models, previous studies have proposed an early-exiting framework that
allocates adaptive computation paths for each token based on the complexity of
generating the subsequent token. However, we observed several shortcomings,
including performance degradation caused by a state copying mechanism or
numerous exit paths, and sensitivity to exit confidence thresholds.
Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework,
which incorporates a shallow-deep module and a synchronized parallel decoding.
Our framework enables faster inference by synchronizing the decoding process of
the current token with previously stacked early-exited tokens. Furthermore, as
parallel decoding allows us to observe predictions from both shallow and deep
models, we present a novel adaptive threshold estimator that exploits a Beta
mixture model to determine suitable confidence thresholds. We empirically
demonstrated the superiority of our proposed framework on extensive generation
tasks.",None,-1
ee2384f0-de01-4030-81fd-8d2b89017c36,Let's reward step by step: Step-Level reward model as the Navigators for Reasoning,0.50468,"Recent years have seen considerable advancements in multi-step reasoning with
Large Language Models (LLMs). The previous studies have elucidated the merits
of integrating feedback or search mechanisms during model inference to improve
the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically
furnishes LLMs with step-by-step feedback during the training phase, akin to
Proximal Policy Optimization (PPO) or reject sampling. Our objective is to
examine the efficacy of PRM in the inference phase to help discern the optimal
solution paths for multi-step tasks such as mathematical reasoning and code
generation. To this end, we propose a heuristic greedy search algorithm that
employs the step-level feedback from PRM to optimize the reasoning pathways
explored by LLMs. This tailored PRM demonstrated enhanced results compared to
the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH.
Additionally, to explore the versatility of our approach, we develop a novel
method to automatically generate step-level reward dataset for coding tasks and
observed similar improved performance in the code generation tasks. Thus
highlighting the robust nature of our reward-model-based approach to inference
for reasoning tasks.",None,-1
2173792c-1a69-4e23-9b6e-baa4d8ae180d,Exploring Challenges and Opportunities to Support Designers in Learning to Co-create with AI-based Manufacturing Design Tools,0.9895,"AI-based design tools are proliferating in professional software to assist
engineering and industrial designers in complex manufacturing and design tasks.
These tools take on more agentic roles than traditional computer-aided design
tools and are often portrayed as ""co-creators."" Yet, working effectively with
such systems requires different skills than working with complex CAD tools
alone. To date, we know little about how engineering designers learn to work
with AI-based design tools. In this study, we observed trained designers as
they learned to work with two AI-based tools on a realistic design task. We
find that designers face many challenges in learning to effectively co-create
with current systems, including challenges in understanding and adjusting AI
outputs and in communicating their design goals. Based on our findings, we
highlight several design opportunities to better support designer-AI
co-creation.",None,-1
349de56b-1a6e-4ec8-94d6-d8858fb11f7e,Generalized Lightness Adaptation with Channel Selective Normalization,0.192666,"Lightness adaptation is vital to the success of image processing to avoid
unexpected visual deterioration, which covers multiple aspects, e.g., low-light
image enhancement, image retouching, and inverse tone mapping. Existing methods
typically work well on their trained lightness conditions but perform poorly in
unknown ones due to their limited generalization ability. To address this
limitation, we propose a novel generalized lightness adaptation algorithm that
extends conventional normalization techniques through a channel filtering
design, dubbed Channel Selective Normalization (CSNorm). The proposed CSNorm
purposely normalizes the statistics of lightness-relevant channels and keeps
other channels unchanged, so as to improve feature generalization and
discrimination. To optimize CSNorm, we propose an alternating training strategy
that effectively identifies lightness-relevant channels. The model equipped
with our CSNorm only needs to be trained on one lightness condition and can be
well generalized to unknown lightness conditions. Experimental results on
multiple benchmark datasets demonstrate the effectiveness of CSNorm in
enhancing the generalization ability for the existing lightness adaptation
methods. Code is available at https://github.com/mdyao/CSNorm.",None,-1
202ae4d4-bfb7-4114-bf8d-7254bddf06b5,Online Safety Property Collection and Refinement for Safe Deep Reinforcement Learning in Mapless Navigation,0.315506,"Safety is essential for deploying Deep Reinforcement Learning (DRL)
algorithms in real-world scenarios. Recently, verification approaches have been
proposed to allow quantifying the number of violations of a DRL policy over
input-output relationships, called properties. However, such properties are
hard-coded and require task-level knowledge, making their application
intractable in challenging safety-critical tasks. To this end, we introduce the
Collection and Refinement of Online Properties (CROP) framework to design
properties at training time. CROP employs a cost signal to identify unsafe
interactions and use them to shape safety properties. Hence, we propose a
refinement strategy to combine properties that model similar unsafe
interactions. Our evaluation compares the benefits of computing the number of
violations using standard hard-coded properties and the ones generated with
CROP. We evaluate our approach in several robotic mapless navigation tasks and
demonstrate that the violation metric computed with CROP allows higher returns
and lower violations over previous Safe DRL approaches.",None,-1
e3f2e86a-cec3-4396-a6ab-a79b22785aef,Language Independent Neuro-Symbolic Semantic Parsing for Form Understanding,0.215552,"Recent works on form understanding mostly employ multimodal transformers or
large-scale pre-trained language models. These models need ample data for
pre-training. In contrast, humans can usually identify key-value pairings from
a form only by looking at layouts, even if they don't comprehend the language
used. No prior research has been conducted to investigate how helpful layout
information alone is for form understanding. Hence, we propose a unique
entity-relation graph parsing method for scanned forms called LAGNN, a
language-independent Graph Neural Network model. Our model parses a form into a
word-relation graph in order to identify entities and relations jointly and
reduce the time complexity of inference. This graph is then transformed by
deterministic rules into a fully connected entity-relation graph. Our model
simply takes into account relative spacing between bounding boxes from layout
information to facilitate easy transfer across languages. To further improve
the performance of LAGNN, and achieve isomorphism between entity-relation
graphs and word-relation graphs, we use integer linear programming (ILP) based
inference. Code is publicly available at https://github.com/Bhanu068/LAGNN",None,-1
8b01c0b7-46d3-475a-873a-ba599ab4a22b,QCQP-Tunneling: Ellipsoidal Constrained Agent Navigation,0.238172,"This paper presents a convex-QCQP based novel path planning algorithm named
ellipsoidal constrained agent navigation (ECAN), for a challenging problem of
online path planning in completely unknown and unseen continuous environments.
ECAN plans path for the agent by making a tunnel of overlapping ellipsoids, in
an online fashion, through the environment. Convex constraints in the
ellipsoid-formation step circumvent collision with the obstacles. The problem
of online-tunneling is solved as a convex-QCQP. This paper assumes no
constraints on shape of the agent and the obstacles. However, to make the
approach clearer, this paper first introduces the framework for a point-mass
agent with point-size obstacles. After explaining the underlying principle in
drawing an ellipsoid tunnel, the framework is extended to the agent and
obstacles having finite area (2d space) and finite-volume (3d-space).",None,-1
c0c12f07-5de0-4d9f-9a47-c42b54c03594,Interactive Segment Anything NeRF with Feature Imitation,0.64827,"This paper investigates the potential of enhancing Neural Radiance Fields
(NeRF) with semantics to expand their applications. Although NeRF has been
proven useful in real-world applications like VR and digital creation, the lack
of semantics hinders interaction with objects in complex scenes. We propose to
imitate the backbone feature of off-the-shelf perception models to achieve
zero-shot semantic segmentation with NeRF. Our framework reformulates the
segmentation process by directly rendering semantic features and only applying
the decoder from perception models. This eliminates the need for expensive
backbones and benefits 3D consistency. Furthermore, we can project the learned
semantics onto extracted mesh surfaces for real-time interaction. With the
state-of-the-art Segment Anything Model (SAM), our framework accelerates
segmentation by 16 times with comparable mask quality. The experimental results
demonstrate the efficacy and computational advantages of our approach. Project
page: \url{https://me.kiui.moe/san/}.",None,-1
53cd14e9-3227-453e-b5cf-3acf16bd47bd,SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation,0.479121,"This paper studies referring video object segmentation (RVOS) by boosting
video-level visual-linguistic alignment. Recent approaches model the RVOS task
as a sequence prediction problem and perform multi-modal interaction as well as
segmentation for each frame separately. However, the lack of a global view of
video content leads to difficulties in effectively utilizing inter-frame
relationships and understanding textual descriptions of object temporal
variations. To address this issue, we propose Semantic-assisted Object Cluster
(SOC), which aggregates video content and textual guidance for unified temporal
modeling and cross-modal alignment. By associating a group of frame-level
object embeddings with language tokens, SOC facilitates joint space learning
across modalities and time steps. Moreover, we present multi-modal contrastive
supervision to help construct well-aligned joint space at the video level. We
conduct extensive experiments on popular RVOS benchmarks, and our method
outperforms state-of-the-art competitors on all benchmarks by a remarkable
margin. Besides, the emphasis on temporal coherence enhances the segmentation
stability and adaptability of our method in processing text expressions with
temporal variations. Code will be available.",None,-1
5b618f4d-d5d5-4aad-bb22-920c92dc09a6,Cryptocurrency Price Prediction using Twitter Sentiment Analysis,0.525252,"The cryptocurrency ecosystem has been the centre of discussion on many social
media platforms, following its noted volatility and varied opinions. Twitter is
rapidly being utilised as a news source and a medium for bitcoin discussion.
Our algorithm seeks to use historical prices and sentiment of tweets to
forecast the price of Bitcoin. In this study, we develop an end-to-end model
that can forecast the sentiment of a set of tweets (using a Bidirectional
Encoder Representations from Transformers - based Neural Network Model) and
forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted
sentiment and other metrics like historical cryptocurrency price data, tweet
volume, a user's following, and whether or not a user is verified. The
sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average
of real-time data, and test data. The mean absolute percent error for the price
prediction was 3.6%.",None,-1
6f3b8128-95cd-4edf-b599-fead9f0ac3d6,SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation,0.54981,"Recent advances in deep learning-based medical image segmentation studies
achieve nearly human-level performance in fully supervised manner. However,
acquiring pixel-level expert annotations is extremely expensive and laborious
in medical imaging fields. Unsupervised domain adaptation (UDA) can alleviate
this problem, which makes it possible to use annotated data in one imaging
modality to train a network that can successfully perform segmentation on
target imaging modality with no labels. In this work, we propose SDC-UDA, a
simple yet effective volumetric UDA framework for slice-direction continuous
cross-modality medical image segmentation which combines intra- and inter-slice
self-attentive image translation, uncertainty-constrained pseudo-label
refinement, and volumetric self-training. Our method is distinguished from
previous methods on UDA for medical image segmentation in that it can obtain
continuous segmentation in the slice direction, thereby ensuring higher
accuracy and potential in clinical practice. We validate SDC-UDA with multiple
publicly available cross-modality medical image segmentation datasets and
achieve state-of-the-art segmentation performance, not to mention the superior
slice-direction continuity of prediction compared to previous studies.",None,-1
715dbad1-35e0-4448-b787-1857519bdedd,Learning Unseen Modality Interaction,0.182589,"Multimodal learning assumes all modality combinations of interest are
available during training to learn cross-modal correspondences. In this paper,
we challenge this modality-complete assumption for multimodal learning and
instead strive for generalization to unseen modality combinations during
inference. We pose the problem of unseen modality interaction and introduce a
first solution. It exploits a module that projects the multidimensional
features of different modalities into a common space with rich information
preserved. This allows the information to be accumulated with a simple
summation operation across available modalities. To reduce overfitting to less
discriminative modality combinations during training, we further improve the
model learning with pseudo-supervision indicating the reliability of a
modality's prediction. We demonstrate that our approach is effective for
diverse tasks and modalities by evaluating it for multimodal video
classification, robot state regression, and multimedia retrieval. Project
website: https://xiaobai1217.github.io/Unseen-Modality-Interaction/.",None,-1
536761d1-d909-43c2-abdb-6bcb17a22f9c,A Mathematical Guide to Operator Learning,0.871258,"Operator learning aims to discover properties of an underlying dynamical
system or partial differential equation (PDE) from data. Here, we present a
step-by-step guide to operator learning. We explain the types of problems and
PDEs amenable to operator learning, discuss various neural network
architectures, and explain how to employ numerical PDE solvers effectively. We
also give advice on how to create and manage training data and conduct
optimization. We offer intuition behind the various neural network
architectures employed in operator learning by motivating them from the
point-of-view of numerical linear algebra.",None,-1
44ea66f9-85cc-4de7-a0ab-9294570f7e19,"If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection",0.247606,"Despite their impressive capabilities, diffusion-based text-to-image (T2I)
models can lack faithfulness to the text prompt, where generated images may not
contain all the mentioned objects, attributes or relations. To alleviate these
issues, recent works proposed post-hoc methods to improve model faithfulness
without costly retraining, by modifying how the model utilizes the input
prompt. In this work, we take a step back and show that large T2I diffusion
models are more faithful than usually assumed, and can generate images faithful
to even complex prompts without the need to manipulate the generative process.
Based on that, we show how faithfulness can be simply treated as a candidate
selection problem instead, and introduce a straightforward pipeline that
generates candidate images for a text prompt and picks the best one according
to an automatic scoring system that can leverage already existing T2I
evaluation metrics. Quantitative comparisons alongside user studies on diverse
benchmarks show consistently improved faithfulness over post-hoc enhancement
methods, with comparable or lower computational cost. Code is available at
\url{https://github.com/ExplainableML/ImageSelect}.",None,-1
ab028c50-52d1-477f-a4bc-9f29c53d68ea,DevelSet: Deep Neural Level Set for Instant Mask Optimization,0.917004,"With the feature size continuously shrinking in advanced technology nodes,
mask optimization is increasingly crucial in the conventional design flow,
accompanied by an explosive growth in prohibitive computational overhead in
optical proximity correction (OPC) methods. Recently, inverse lithography
technique (ILT) has drawn significant attention and is becoming prevalent in
emerging OPC solutions. However, ILT methods are either time-consuming or in
weak performance of mask printability and manufacturability. In this paper, we
present DevelSet, a GPU and deep neural network (DNN) accelerated level set OPC
framework for metal layer. We first improve the conventional level set-based
ILT algorithm by introducing the curvature term to reduce mask complexity and
applying GPU acceleration to overcome computational bottlenecks. To further
enhance printability and fast iterative convergence, we propose a novel deep
neural network delicately designed with level set intrinsic principles to
facilitate the joint optimization of DNN and GPU accelerated level set
optimizer. Experimental results show that DevelSet framework surpasses the
state-of-the-art methods in printability and boost the runtime performance
achieving instant level (around 1 second).",None,-1
f4b68abf-0949-424e-bf90-d885dc36868f,Large Language Models Perform Diagnostic Reasoning,0.396936,"We explore the extension of chain-of-thought (CoT) prompting to medical
reasoning for the task of automatic diagnosis. Motivated by doctors' underlying
reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical
results demonstrate that by simply prompting large language models trained only
on general text corpus with two DR-CoT exemplars, the diagnostic accuracy
improves by 15% comparing to standard prompting. Moreover, the gap reaches a
pronounced 18% in out-domain settings. Our findings suggest expert-knowledge
reasoning in large language models can be elicited through proper promptings.",None,-1
a3b23d96-3372-422f-9c0e-c0ce45643a7d,Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features,0.831646,"Recent studies have demonstrated the susceptibility of deep neural networks
to backdoor attacks. Given a backdoored model, its prediction of a poisoned
sample with trigger will be dominated by the trigger information, though
trigger information and benign information coexist. Inspired by the mechanism
of the optical polarizer that a polarizer could pass light waves with
particular polarizations while filtering light waves with other polarizations,
we propose a novel backdoor defense method by inserting a learnable neural
polarizer into the backdoored model as an intermediate layer, in order to
purify the poisoned sample via filtering trigger information while maintaining
benign information. The neural polarizer is instantiated as one lightweight
linear transformation layer, which is learned through solving a well designed
bi-level optimization problem, based on a limited clean dataset. Compared to
other fine-tuning-based defense methods which often adjust all parameters of
the backdoored model, the proposed method only needs to learn one additional
layer, such that it is more efficient and requires less clean data. Extensive
experiments demonstrate the effectiveness and efficiency of our method in
removing backdoors across various neural network architectures and datasets,
especially in the case of very limited clean data.",None,-1
27c2d2ad-b15f-4e55-910e-134412b005b6,Longformer: Longitudinal Transformer for Alzheimer's Disease Classification with Structural MRIs,0.383848,"Structural magnetic resonance imaging (sMRI) is widely used for brain
neurological disease diagnosis; while longitudinal MRIs are often collected to
monitor and capture disease progression, as clinically used in diagnosing
Alzheimer's disease (AD). However, most current methods neglect AD's
progressive nature and only take a single sMRI for recognizing AD. In this
paper, we consider the problem of leveraging the longitudinal MRIs of a subject
for AD identification. To capture longitudinal changes in sMRIs, we propose a
novel model Longformer, a spatiotemporal transformer network that performs
attention mechanisms spatially on sMRIs at each time point and integrates brain
region features over time to obtain longitudinal embeddings for classification.
Our Longformer achieves state-of-the-art performance on two binary
classification tasks of separating different stages of AD using the ADNI
dataset. Our source code is available at https://github.com/Qybc/LongFormer.",None,-1
77b94abc-0121-4448-b0aa-26e56badc9d7,Enhancing Translation for Indigenous Languages: Experiments with Multilingual Models,0.239799,"This paper describes CIC NLP's submission to the AmericasNLP 2023 Shared Task
on machine translation systems for indigenous languages of the Americas. We
present the system descriptions for three methods. We used two multilingual
models, namely M2M-100 and mBART50, and one bilingual (one-to-one) -- Helsinki
NLP Spanish-English translation model, and experimented with different transfer
learning setups. We experimented with 11 languages from America and report the
setups we used as well as the results we achieved. Overall, the mBART setup was
able to improve upon the baseline for three out of the eleven languages.",None,-1
6e7375b4-b9c3-4307-96ec-f403f7cec6a3,Context-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model,0.210739,"Despite the success of deep neural network (DNN) on sequential data (i.e.,
scene text and speech) recognition, it suffers from the over-confidence problem
mainly due to overfitting in training with the cross-entropy loss, which may
make the decision-making less reliable. Confidence calibration has been
recently proposed as one effective solution to this problem. Nevertheless, the
majority of existing confidence calibration methods aims at non-sequential
data, which is limited if directly applied to sequential data since the
intrinsic contextual dependency in sequences or the class-specific statistical
prior is seldom exploited. To the end, we propose a Context-Aware Selective
Label Smoothing (CASLS) method for calibrating sequential data. The proposed
CASLS fully leverages the contextual dependency in sequences to construct
confusion matrices of contextual prediction statistics over different classes.
Class-specific error rates are then used to adjust the weights of smoothing
strength in order to achieve adaptive calibration. Experimental results on
sequence recognition tasks, including scene text recognition and speech
recognition, demonstrate that our method can achieve the state-of-the-art
performance.",None,-1
5f9ecdbd-fadd-49e9-a2a9-5022b8ac0ae5,"InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",0.187426,"While large models such as GPT-3 demonstrate exceptional performance in
zeroshot and fewshot summarization tasks, their extensive serving and
fine-tuning costs hinder their utilization in various applications. Conversely,
previous studies have found that although automatic metrics tend to favor
smaller fine-tuned models, the quality of the summaries they generate is
inferior to that of larger models like GPT-3 when assessed by human evaluators.
To address this issue, we propose InheritSumm, a versatile and compact
summarization model derived from GPT-3.5 through distillation. InheritSumm not
only exhibits comparable zeroshot and fewshot summarization capabilities to
GPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental
results demonstrate that InheritSumm achieves similar or superior performance
to GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the
previously established best small models in both prefix-tuning and full-data
fine-tuning scenarios.",None,-1
49f87702-b72c-4737-9221-728d117d88a3,Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique,0.381496,"This paper presents a novel benchmarking framework Dyport for evaluating
biomedical hypothesis generation systems. Utilizing curated datasets, our
approach tests these systems under realistic conditions, enhancing the
relevance of our evaluations. We integrate knowledge from the curated databases
into a dynamic graph, accompanied by a method to quantify discovery importance.
This not only assesses hypothesis accuracy but also their potential impact in
biomedical research which significantly extends traditional link prediction
benchmarks. Applicability of our benchmarking process is demonstrated on
several link prediction systems applied on biomedical semantic knowledge
graphs. Being flexible, our benchmarking system is designed for broad
application in hypothesis generation quality verification, aiming to expand the
scope of scientific discovery within the biomedical research community.
Availability and implementation: Dyport framework is fully open-source. All
code and datasets are available at: https://github.com/IlyaTyagin/Dyport",None,-1
5fc23ef5-4610-4284-bf96-6baa293527ae,Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization,0.473286,"Prompt tuning is one of the successful approaches for parameter-efficient
tuning of pre-trained language models. Despite being arguably the most
parameter-efficient (tuned soft prompts constitute <0.1% of total parameters),
it typically performs worse than other efficient tuning methods and is quite
sensitive to hyper-parameters. In this work, we introduce Residual Prompt
Tuning - a simple and efficient method that significantly improves the
performance and stability of prompt tuning. We propose to reparameterize soft
prompt embeddings using a shallow network with a residual connection. Our
experiments show that Residual Prompt Tuning significantly outperforms prompt
tuning on SuperGLUE benchmark. Notably, our method reaches +7 points
improvement over prompt tuning with T5-Base and allows to reduce the prompt
length by 10x without hurting performance. In addition, we show that our
approach is robust to the choice of learning rate and prompt initialization,
and is effective in few-shot settings.",None,-1
d19064a9-2e32-4147-a854-7617a298c4b0,A Divide-Align-Conquer Strategy for Program Synthesis,0.321416,"A major bottleneck in search-based program synthesis is the exponentially
growing search space which makes learning large programs intractable. Humans
mitigate this problem by leveraging the compositional nature of the real world:
In structured domains, a logical specification can often be decomposed into
smaller, complementary solution programs. We show that compositional
segmentation can be applied in the programming by examples setting to divide
the search for large programs across multiple smaller program synthesis
problems. For each example, we search for a decomposition into smaller units
which maximizes the reconstruction accuracy in the output under a latent task
program. A structural alignment of the constituent parts in the input and
output leads to pairwise correspondences used to guide the program synthesis
search. In order to align the input/output structures, we make use of the
Structure-Mapping Theory (SMT), a formal model of human analogical reasoning
which originated in the cognitive sciences. We show that decomposition-driven
program synthesis with structural alignment outperforms Inductive Logic
Programming (ILP) baselines on string transformation tasks even with minimal
knowledge priors. Unlike existing methods, the predictive accuracy of our agent
monotonically increases for additional examples and achieves an average time
complexity of $\mathcal{O}(m)$ in the number $m$ of partial programs for highly
structured domains such as strings. We extend this method to the complex
setting of visual reasoning in the Abstraction and Reasoning Corpus (ARC) for
which ILP methods were previously infeasible.",None,-1
41f41089-eb69-4c1f-b712-f6fff0689e88,SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark,0.411183,"Large language models (LLMs) have shown the potential to be integrated into
human daily lives. Therefore, user preference is the most critical criterion
for assessing LLMs' performance in real-world scenarios. However, existing
benchmarks mainly focus on measuring models' accuracy using multi-choice
questions, which limits the understanding of their capabilities in real
applications. We fill this gap by proposing a comprehensive Chinese benchmark
SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE
encompasses three sub-tasks: actual users' queries and ratings derived from an
LLM battle platform (CArena), open-ended questions with single and
multiple-turn dialogues (OPEN), and closed-ended questions with the same stems
as open-ended single-turn ones (CLOSE). Our study shows that accuracy on
closed-ended questions is insufficient to reflect human preferences achieved on
open-ended ones. At the same time, they can complement each other to predict
actual user preferences. We also demonstrate that GPT-4 is a reliable judge to
automatically evaluate human preferences on open-ended questions in a Chinese
context. Our benchmark will be released at https://www.CLUEbenchmarks.com",None,-1
15a5dac8-0374-42c0-aac5-55e37df5cf5b,Automatic Number Plate Recognition using Random Forest Classifier,0.59768,"Automatic Number Plate Recognition System (ANPRS) is a mass surveillance
embedded system that recognizes the number plate of the vehicle. This system is
generally used for traffic management applications. It should be very efficient
in detecting the number plate in noisy as well as in low illumination and also
within required time frame. This paper proposes a number plate recognition
method by processing vehicle's rear or front image. After image is captured,
processing is divided into four steps which are Pre-Processing, Number plate
localization, Character segmentation and Character recognition. Pre-Processing
enhances the image for further processing, number plate localization extracts
the number plate region from the image, character segmentation separates the
individual characters from the extracted number plate and character recognition
identifies the optical characters by using random forest classification
algorithm. Experimental results reveal that the accuracy of this method is
90.9%.",None,-1
0301a27f-6c96-4083-98b3-d6bdda7efe13,Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting,0.394309,"Most existing stylistic text rewriting methods and evaluation metrics operate
on a sentence level, but ignoring the broader context of the text can lead to
preferring generic, ambiguous, and incoherent rewrites. In this paper, we
investigate integrating the preceding textual context into both the
$\textit{rewriting}$ and $\textit{evaluation}$ stages of stylistic text
rewriting, and introduce a new composite contextual evaluation metric
$\texttt{CtxSimFit}$ that combines similarity to the original sentence with
contextual cohesiveness. We comparatively evaluate non-contextual and
contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our
experiments show that humans significantly prefer contextual rewrites as more
fitting and natural over non-contextual ones, yet existing sentence-level
automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences
($\rho$=0--0.3). In contrast, human preferences are much better reflected by
both our novel $\texttt{CtxSimFit}$ ($\rho$=0.7--0.9) as well as proposed
context-infused versions of common metrics ($\rho$=0.4--0.7). Overall, our
findings highlight the importance of integrating context into the generation
and especially the evaluation stages of stylistic text rewriting.",None,-1
9553bb85-3f31-4d3a-a5f9-795e796f1fbc,GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters,0.169866,"This report describes GMU's sentiment analysis system for the SemEval-2023
shared task AfriSenti-SemEval. We participated in all three sub-tasks:
Monolingual, Multilingual, and Zero-Shot. Our approach uses models initialized
with AfroXLMR-large, a pre-trained multilingual language model trained on
African languages and fine-tuned correspondingly. We also introduce augmented
training data along with original training data. Alongside finetuning, we
perform phylogeny-based adapter tuning to create several models and ensemble
the best models for the final submission. Our system achieves the best F1-score
on track 5: Amharic, with 6.2 points higher F1-score than the second-best
performing system on this track. Overall, our system ranks 5th among the 10
systems participating in all 15 tracks.",None,-1
fe1b2f5a-2103-4fd1-9871-7c8e4861d1ea,Targeted Adversarial Attacks against Neural Machine Translation,0.579601,"Neural Machine Translation (NMT) systems are used in various applications.
However, it has been shown that they are vulnerable to very small perturbations
of their inputs, known as adversarial attacks. In this paper, we propose a new
targeted adversarial attack against NMT models. In particular, our goal is to
insert a predefined target keyword into the translation of the adversarial
sentence while maintaining similarity between the original sentence and the
perturbed one in the source domain. To this aim, we propose an optimization
problem, including an adversarial loss term and a similarity term. We use
gradient projection in the embedding space to craft an adversarial sentence.
Experimental results show that our attack outperforms Seq2Sick, the other
targeted adversarial attack against NMT models, in terms of success rate and
decrease in translation quality. Our attack succeeds in inserting a keyword
into the translation for more than 75% of sentences while similarity with the
original sentence stays preserved.",None,-1
5b13000a-8097-460e-911e-725786f8b68d,Learning to Paraphrase Sentences to Different Complexity Levels,0.32436,"While sentence simplification is an active research topic in NLP, its
adjacent tasks of sentence complexification and same-level paraphrasing are
not. To train models on all three tasks, we present two new unsupervised
datasets. We compare these datasets, one labeled by a weak classifier and the
other by a rule-based approach, with a single supervised dataset. Using these
three datasets for training, we perform extensive experiments on both
multitasking and prompting strategies. Compared to other systems trained on
unsupervised parallel data, models trained on our weak classifier labeled
dataset achieve state-of-the-art performance on the ASSET simplification
benchmark. Our models also outperform previous work on sentence level
targeting. Finally, we establish how a handful of Large Language Models perform
on these tasks under a zero-shot setting.",None,-1
11b8238d-ba49-4ab4-b777-38934625aa00,Zero-1-to-3: Zero-shot One Image to 3D Object,1.0,"We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an
object given just a single RGB image. To perform novel view synthesis in this
under-constrained setting, we capitalize on the geometric priors that
large-scale diffusion models learn about natural images. Our conditional
diffusion model uses a synthetic dataset to learn controls of the relative
camera viewpoint, which allow new images to be generated of the same object
under a specified camera transformation. Even though it is trained on a
synthetic dataset, our model retains a strong zero-shot generalization ability
to out-of-distribution datasets as well as in-the-wild images, including
impressionist paintings. Our viewpoint-conditioned diffusion approach can
further be used for the task of 3D reconstruction from a single image.
Qualitative and quantitative experiments show that our method significantly
outperforms state-of-the-art single-view 3D reconstruction and novel view
synthesis models by leveraging Internet-scale pre-training.",None,-1
1fa9f9f1-1592-48d5-9d0f-b1c85f4e7fd9,Improved Differential-neural Cryptanalysis for Round-reduced Simeck32/64,0.172362,"In CRYPTO 2019, Gohr presented differential-neural cryptanalysis by building
the differential distinguisher with a neural network, achieving practical 11-,
and 12-round key recovery attack for Speck32/64. Inspired by this framework, we
develop the Inception neural network that is compatible with the round function
of Simeck to improve the accuracy of the neural distinguishers, thus improving
the accuracy of (9-12)-round neural distinguishers for Simeck32/64. To provide
solid baselines for neural distinguishers, we compute the full distribution of
differences induced by one specific input difference up to 13-round
Simeck32/64. Moreover, the performance of the DDT-based distinguishers in
multiple ciphertext pairs is evaluated. Compared with the DDT-based
distinguishers, the 9-, and 10-round neural distinguishers achieve better
accuracy. Also, an in-depth analysis of the wrong key response profile revealed
that the 12-th and 13-th bits of the subkey have little effect on the score of
the neural distinguisher, thereby accelerating key recovery attacks. Finally,
an enhanced 15-round and the first practical 16-, and 17-round attacks are
implemented for Simeck32/64, and the success rate of both the 15-, and 16-round
attacks is almost 100%.",None,-1
59ec0ded-a19b-4a7b-ba3c-97e4cf23c64f,QVRF: A Quantization-error-aware Variable Rate Framework for Learned Image Compression,0.303119,"Learned image compression has exhibited promising compression performance,
but variable bitrates over a wide range remain a challenge. State-of-the-art
variable rate methods compromise the loss of model performance and require
numerous additional parameters. In this paper, we present a
Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a
univariate quantization regulator a to achieve wide-range variable rates within
a single model. Specifically, QVRF defines a quantization regulator vector
coupled with predefined Lagrange multipliers to control quantization error of
all latent representation for discrete variable rates. Additionally, the
reparameterization method makes QVRF compatible with a round quantizer.
Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods
equipped with QVRF can achieve wide-range continuous variable rates within a
single model without significant performance degradation. Furthermore, QVRF
outperforms contemporary variable-rate methods in rate-distortion performance
with minimal additional parameters.",None,-1
39459497-dd37-4086-80d7-748f7ea4475a,Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything,0.907193,"The research on extrinsic calibration between Light Detection and
Ranging(LiDAR) and camera are being promoted to a more accurate, automatic and
generic manner. Since deep learning has been employed in calibration, the
restrictions on the scene are greatly reduced. However, data driven method has
the drawback of low transfer-ability. It cannot adapt to dataset variations
unless additional training is taken. With the advent of foundation model, this
problem can be significantly mitigated. By using the Segment Anything
Model(SAM), we propose a novel LiDAR-camera calibration method, which requires
zero extra training and adapts to common scenes. With an initial guess, we
opimize the extrinsic parameter by maximizing the consistency of points that
are projected inside each image mask. The consistency includes three properties
of the point cloud: the intensity, normal vector and categories derived from
some segmentation methods. The experiments on different dataset have
demonstrated the generality and comparable accuracy of our method. The code is
available at https://github.com/OpenCalib/CalibAnything.",None,-1
b13a6a10-82e4-40ba-8321-f7d30d71fc89,Design by Contract Framework for Quantum Software,0.185047,"To realize reliable quantum software, techniques to automatically ensure the
quantum software's correctness have recently been investigated. However, they
primarily focus on fixed quantum circuits rather than the procedure of building
quantum circuits. Despite being a common approach, the correctness of building
circuits using different parameters following the same procedure is not
guaranteed. To this end, we propose a design-by-contract framework for quantum
software. Our framework provides a python-embedded language to write assertions
on the input and output states of all quantum circuits built by certain
procedures. Additionally, it provides a method to write assertions about the
statistical processing of measurement results to ensure the procedure's
correctness for obtaining the final result. These assertions are automatically
checked using a quantum computer simulator. For evaluation, we implemented our
framework and wrote assertions for some widely used quantum algorithms.
Consequently, we found that our framework has sufficient expressive power to
verify the whole procedure of quantum software.",None,-1
6d0b76d9-3261-4762-be2e-7e1a793cf13e,CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning,0.454876,"Commonsense reasoning, aiming at endowing machines with a human-like ability
to make situational presumptions, is extremely challenging to generalize. For
someone who barely knows about ""meditation,"" while is knowledgeable about
""singing,"" he can still infer that ""meditation makes people relaxed"" from the
existing knowledge that ""singing makes people relaxed"" by first conceptualizing
""singing"" as a ""relaxing event"" and then instantiating that event to
""meditation."" This process, known as conceptual induction and deduction, is
fundamental to commonsense reasoning while lacking both labeled data and
methodologies to enhance commonsense modeling. To fill such a research gap, we
propose CAT (Contextualized ConceptuAlization and InsTantiation), a
semi-supervised learning framework that integrates event conceptualization and
instantiation to conceptualize commonsense knowledge bases at scale. Extensive
experiments show that our framework achieves state-of-the-art performances on
two conceptualization tasks, and the acquired abstract commonsense knowledge
can significantly improve commonsense inference modeling. Our code, data, and
fine-tuned models are publicly available at
https://github.com/HKUST-KnowComp/CAT.",None,-1
27633444-711d-4da7-b4fe-294a58345d6a,AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction,0.518131,"In this work, we present a multimodal solution to the problem of 4D face
reconstruction from monocular videos. 3D face reconstruction from 2D images is
an under-constrained problem due to the ambiguity of depth. State-of-the-art
methods try to solve this problem by leveraging visual information from a
single image or video, whereas 3D mesh animation approaches rely more on audio.
However, in most cases (e.g. AR/VR applications), videos include both visual
and speech information. We propose AVFace that incorporates both modalities and
accurately reconstructs the 4D facial and lip motion of any speaker, without
requiring any 3D ground truth for training. A coarse stage estimates the
per-frame parameters of a 3D morphable model, followed by a lip refinement, and
then a fine stage recovers facial geometric details. Due to the temporal audio
and video information captured by transformer-based modules, our method is
robust in cases when either modality is insufficient (e.g. face occlusions).
Extensive qualitative and quantitative evaluation demonstrates the superiority
of our method over the current state-of-the-art.",None,-1
f1454ebd-89e0-4022-96d3-136c926ac949,Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking,0.842551,"Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.",None,-1
053c47e4-158e-4cf3-842a-6bd758edd277,Exploring Continual Learning of Diffusion Models,0.316374,"Diffusion models have achieved remarkable success in generating high-quality
images thanks to their novel training procedures applied to unprecedented
amounts of data. However, training a diffusion model from scratch is
computationally expensive. This highlights the need to investigate the
possibility of training these models iteratively, reusing computation while the
data distribution changes. In this study, we take the first step in this
direction and evaluate the continual learning (CL) properties of diffusion
models. We begin by benchmarking the most common CL methods applied to
Denoising Diffusion Probabilistic Models (DDPMs), where we note the strong
performance of the experience replay with the reduced rehearsal coefficient.
Furthermore, we provide insights into the dynamics of forgetting, which exhibit
diverse behavior across diffusion timesteps. We also uncover certain pitfalls
of using the bits-per-dimension metric for evaluating CL.",None,-1
35a53513-a4c7-424e-b26d-f6f0aa209533,"ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition",0.278155,"Video Action Recognition (VAR) is a challenging task due to its inherent
complexities. Though different approaches have been explored in the literature,
designing a unified framework to recognize a large number of human actions is
still a challenging problem. Recently, Multi-Modal Learning (MML) has
demonstrated promising results in this domain. In literature, 2D skeleton or
pose modality has often been used for this task, either independently or in
conjunction with the visual information (RGB modality) present in videos.
However, the combination of pose, visual information, and text attributes has
not been explored yet, though text and pose attributes independently have been
proven to be effective in numerous computer vision tasks. In this paper, we
present the first pose augmented Vision-language model (VLM) for VAR. Notably,
our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video
action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even
without any video data pre-training, and an accuracy of 96.11% and 75.75% after
kinetics pre-training.",None,-1
c4fbe97b-2cb4-4010-9602-a2bb19e7bf10,GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction,0.979605,"Large Language Models (LLMs) combined with instruction tuning have made
significant progress when generalizing to unseen tasks. However, they have been
less successful in Information Extraction (IE), lagging behind task-specific
models. Typically, IE tasks are characterized by complex annotation guidelines
that describe the task and give examples to humans. Previous attempts to
leverage such information have failed, even with the largest models, as they
are not able to follow the guidelines out of the box. In this paper, we propose
GoLLIE (Guideline-following Large Language Model for IE), a model able to
improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to
comply with annotation guidelines. Comprehensive evaluation empirically
demonstrates that GoLLIE is able to generalize to and follow unseen guidelines,
outperforming previous attempts at zero-shot information extraction. The
ablation study shows that detailed guidelines are key for good results.",None,-1
3668d4e0-eabb-4d20-a24e-f654367e42d4,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,0.101113,"This paper investigates the potential usage of large text-to-image (LTI)
models for the automated diagnosis of a few skin conditions with rarity or a
serious lack of annotated datasets. As the input to the LTI model, we provide
the targeted instantiation of a generic but succinct prompt structure designed
upon careful observations of the conditional narratives from the standard
medical textbooks. In this regard, we pave the path to utilizing accessible
textbook descriptions for automated diagnosis of conditions with data scarcity
through the lens of LTI models. Experiments show the efficacy of the proposed
framework, including much better localization of the infected regions.
Moreover, it has the immense possibility for generalization across the medical
sub-domains, not only to mitigate the data scarcity issue but also to debias
automated diagnostics from the all-pervasive racial biases.",None,-1
778789cc-3f6b-4e6a-aad6-2698dae7590b,Toward Open-ended Embodied Tasks Solving,0.640492,"Empowering embodied agents, such as robots, with Artificial Intelligence (AI)
has become increasingly important in recent years. A major challenge is task
open-endedness. In practice, robots often need to perform tasks with novel
goals that are multifaceted, dynamic, lack a definitive ""end-state"", and were
not encountered during training. To tackle this problem, this paper introduces
\textit{Diffusion for Open-ended Goals} (DOG), a novel framework designed to
enable embodied AI to plan and act flexibly and dynamically for open-ended task
goals. DOG synergizes the generative prowess of diffusion models with
state-of-the-art, training-free guidance techniques to adaptively perform
online planning and control. Our evaluations demonstrate that DOG can handle
various kinds of novel task goals not seen during training, in both maze
navigation and robot control problems. Our work sheds light on enhancing
embodied AI's adaptability and competency in tackling open-ended goals.",None,-1
1287d3c0-3979-4eee-9d25-56d8cc7e43d2,FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow,0.648888,"Reconstruction of 3D neural fields from posed images has emerged as a
promising method for self-supervised representation learning. The key challenge
preventing the deployment of these 3D scene learners on large-scale video data
is their dependence on precise camera poses from structure-from-motion, which
is prohibitively expensive to run at scale. We propose a method that jointly
reconstructs camera poses and 3D neural scene representations online and in a
single forward pass. We estimate poses by first lifting frame-to-frame optical
flow to 3D scene flow via differentiable rendering, preserving locality and
shift-equivariance of the image processing backbone. SE(3) camera pose
estimation is then performed via a weighted least-squares fit to the scene flow
field. This formulation enables us to jointly supervise pose estimation and a
generalizable neural scene representation via re-rendering the input video, and
thus, train end-to-end and fully self-supervised on real-world video datasets.
We demonstrate that our method performs robustly on diverse, real-world video,
notably on sequences traditionally challenging to optimization-based pose
estimation techniques.",None,-1
48d2a9ff-63bd-428b-8b3f-a19166efbaf4,Data Acquisition: A New Frontier in Data-centric AI,0.801211,"As Machine Learning (ML) systems continue to grow, the demand for relevant
and comprehensive datasets becomes imperative. There is limited study on the
challenges of data acquisition due to ad-hoc processes and lack of consistent
methodologies. We first present an investigation of current data marketplaces,
revealing lack of platforms offering detailed information about datasets,
transparent pricing, standardized data formats. With the objective of inciting
participation from the data-centric AI community, we then introduce the DAM
challenge, a benchmark to model the interaction between the data providers and
acquirers. The benchmark was released as a part of DataPerf. Our evaluation of
the submitted strategies underlines the need for effective data acquisition
strategies in ML.",None,-1
5a8b9f48-0114-43d8-9cab-ae257fd3de88,Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling,0.0634513,"With their increasing size, large language models (LLMs) are becoming
increasingly good at language understanding tasks. But even with high
performance on specific downstream task, LLMs fail at simple linguistic tests
for negation or quantifier understanding. Previous work on quantifier
understanding in LLMs show inverse scaling in understanding few-type
quantifiers. In this paper, we question the claims of of previous work and show
that it is a result of inappropriate testing methodology. We also present
alternate methods to measure quantifier comprehension in LLMs and show that
LLMs are able to better understand the difference between the meaning of
few-type and most-type quantifiers as their size increases, although they are
not particularly good at it. We also observe inverse scaling for most-type
quantifier understanding, which is contrary to human psycho-linguistic
experiments and previous work, where the model's understanding of most-type
quantifier gets worse as the model size increases. We do this evaluation on
models ranging from 125M-175B parameters, which suggests that LLMs do not do as
well as expected with quantifiers. We also discuss the possible reasons for
this and the relevance of quantifier understanding in evaluating language
understanding in LLMs.",None,-1
4fe2e60b-c59a-4575-b534-1442a81b11d1,An Extended Sequence Tagging Vocabulary for Grammatical Error Correction,0.381122,"We extend a current sequence-tagging approach to Grammatical Error Correction
(GEC) by introducing specialised tags for spelling correction and morphological
inflection using the SymSpell and LemmInflect algorithms. Our approach improves
generalisation: the proposed new tagset allows a smaller number of tags to
correct a larger range of errors. Our results show a performance improvement
both overall and in the targeted error categories. We further show that
ensembles trained with our new tagset outperform those trained with the
baseline tagset on the public BEA benchmark.",None,-1
7f44238e-236f-4a4c-9794-cf20a8254435,Multi3DRefer: Grounding Text Description to Multiple 3D Objects,0.728358,"We introduce the task of localizing a flexible number of objects in
real-world 3D scenes using natural language descriptions. Existing 3D visual
grounding tasks focus on localizing a unique object given a text description.
However, such a strict setting is unnatural as localizing potentially multiple
objects is a common need in real-world scenarios and robotic tasks (e.g.,
visual navigation and object rearrangement). To address this setting we propose
Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains
61926 descriptions of 11609 objects, where zero, single or multiple target
objects are referenced by each description. We also introduce a new evaluation
metric and benchmark methods from prior work to enable further investigation of
multi-modal 3D scene understanding. Furthermore, we develop a better baseline
leveraging 2D features from CLIP by rendering object proposals online with
contrastive learning, which outperforms the state of the art on the ScanRefer
benchmark.",None,-1
31ffca2e-bd16-4b2a-8182-39a4c2d853f9,Negative Sampling with Adaptive Denoising Mixup for Knowledge Graph Embedding,0.490492,"Knowledge graph embedding (KGE) aims to map entities and relations of a
knowledge graph (KG) into a low-dimensional and dense vector space via
contrasting the positive and negative triples. In the training process of KGEs,
negative sampling is essential to find high-quality negative triples since KGs
only contain positive triples. Most existing negative sampling methods assume
that non-existent triples with high scores are high-quality negative triples.
However, negative triples sampled by these methods are likely to contain noise.
Specifically, they ignore that non-existent triples with high scores might also
be true facts due to the incompleteness of KGs, which are usually called false
negative triples. To alleviate the above issue, we propose an easily pluggable
denoising mixup method called DeMix, which generates high-quality triples by
refining sampled negative triples in a self-supervised manner. Given a sampled
unlabeled triple, DeMix firstly classifies it into a marginal pseudo-negative
triple or a negative triple based on the judgment of the KGE model itself.
Secondly, it selects an appropriate mixup partner for the current triple to
synthesize a partially positive or a harder negative triple. Experimental
results on the knowledge graph completion task show that the proposed DeMix is
superior to other negative sampling techniques, ensuring corresponding KGEs a
faster convergence and better link prediction results.",None,-1
62ee2dbe-a051-429d-b914-b256e031fed2,I-PHYRE: Interactive Physical Reasoning,0.528876,"Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.",None,-1
51176e62-0814-41f0-96d6-8359c4b46d50,MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval,0.264747,"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to
specialized tasks becomes increasingly challenging due to high computational
and memory demands. Indeed, traditional fine-tuning methods are costly, due to
the need for extensive, task-specific training. While efficient adaptation
methods exist that aim to reduce these costs, in practice they suffer from
shallow inter-modal alignment, which severely hurts model effectiveness. To
tackle these computational challenges and improve inter-modal alignment, we
introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment
Enhancer'. This enhancer deepens inter-modal alignment, enabling high
transferability with minimal tuning effort. Our experiments show that unlike
prior efficient tuning approaches, MWA maintains model effectiveness, while
reducing training time by up-to 57%. MWA is also lightweight, increasing model
size by only 2-3% (in terms of parameters) for state-of-the-art foundation
models like BEiT-3 Large. These results demonstrate that MWA provides an
efficient and effective adaptation method for MLLMs, significantly broadening
their applicability.",None,-1
6580493a-2a34-4c91-b0c4-33f33a7d6330,Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition,0.781288,"We tackle the data scarcity challenge in few-shot point cloud recognition of
3D objects by using a joint prediction from a conventional 3D model and a
well-trained 2D model. Surprisingly, such an ensemble, though seems trivial,
has hardly been shown effective in recent 2D-3D models. We find out the crux is
the less effective training for the ''joint hard samples'', which have high
confidence prediction on different wrong labels, implying that the 2D and 3D
models do not collaborate well. To this end, our proposed invariant training
strategy, called InvJoint, does not only emphasize the training more on the
hard samples, but also seeks the invariance between the conflicting 2D and 3D
ambiguous predictions. InvJoint can learn more collaborative 2D and 3D
representations for better ensemble. Extensive experiments on 3D shape
classification with widely adopted ModelNet10/40, ScanObjectNN and Toys4K, and
shape retrieval with ShapeNet-Core validate the superiority of our InvJoint.",None,-1
32d32ef7-f3eb-43db-9d12-aa0bb16e7d08,ADaPT: As-Needed Decomposition and Planning with Language Models,0.263209,"Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.",None,-1
c8503cac-2b49-4126-9b97-1d7601ad0c16,Weighted Notions of Fairness with Binary Supermodular Chores,0.112335,"We study the problem of allocating indivisible chores among agents with
binary supermodular cost functions. In other words, each chore has a marginal
cost of $0$ or $1$ and chores exhibit increasing marginal costs (or decreasing
marginal utilities). In this note, we combine the techniques of Viswanathan and
Zick (2022) and Barman et al. (2023) to present a general framework for fair
allocation with this class of valuation functions. Our framework allows us to
generalize the results of Barman et al. (2023) and efficiently compute
allocations which satisfy weighted notions of fairness like weighted leximin or
min weighted $p$-mean malfare for any $p \ge 1$.",None,-1
7ded54c5-7c86-41ff-91ff-ece85b186ce1,Rethinking Voice-Face Correlation: A Geometry View,0.835534,"Previous works on voice-face matching and voice-guided face synthesis
demonstrate strong correlations between voice and face, but mainly rely on
coarse semantic cues such as gender, age, and emotion. In this paper, we aim to
investigate the capability of reconstructing the 3D facial shape from voice
from a geometry perspective without any semantic information. We propose a
voice-anthropometric measurement (AM)-face paradigm, which identifies
predictable facial AMs from the voice and uses them to guide 3D face
reconstruction. By leveraging AMs as a proxy to link the voice and face
geometry, we can eliminate the influence of unpredictable AMs and make the face
geometry tractable. Our approach is evaluated on our proposed dataset with
ground-truth 3D face scans and corresponding voice recordings, and we find
significant correlations between voice and specific parts of the face geometry,
such as the nasal cavity and cranium. Our work offers a new perspective on
voice-face correlation and can serve as a good empirical study for
anthropometry science.",None,-1
397010b4-c360-40e6-b9fe-caa496836360,DETR-based Layered Clothing Segmentation and Fine-Grained Attribute Recognition,0.0345007,"Clothing segmentation and fine-grained attribute recognition are challenging
tasks at the crossing of computer vision and fashion, which segment the entire
ensemble clothing instances as well as recognize detailed attributes of the
clothing products from any input human images. Many new models have been
developed for the tasks in recent years, nevertheless the segmentation accuracy
is less than satisfactory in case of layered clothing or fashion products in
different scales. In this paper, a new DEtection TRansformer (DETR) based
method is proposed to segment and recognize fine-grained attributes of ensemble
clothing instances with high accuracy. In this model, we propose a
\textbf{multi-layered attention module} by aggregating features of different
scales, determining the various scale components of a single instance, and
merging them together. We train our model on the Fashionpedia dataset and
demonstrate our method surpasses SOTA models in tasks of layered clothing
segmentation and fine-grained attribute recognition.",None,-1
2ea12f41-25e7-4be6-a2c7-9d8754dc99b7,Japanese Lexical Complexity for Non-Native Readers: A New Dataset,0.694203,"Lexical complexity prediction (LCP) is the task of predicting the complexity
of words in a text on a continuous scale. It plays a vital role in simplifying
or annotating complex words to assist readers. To study lexical complexity in
Japanese, we construct the first Japanese LCP dataset. Our dataset provides
separate complexity scores for Chinese/Korean annotators and others to address
the readers' L1-specific needs. In the baseline experiment, we demonstrate the
effectiveness of a BERT-based system for Japanese LCP.",None,-1
2ecd0e5f-d6bf-4b20-b297-8d6f1671a5f6,Less is More for Long Document Summary Evaluation by LLMs,0.804249,"Large Language Models (LLMs) have shown promising performance in summary
evaluation tasks, yet they face challenges such as high computational costs and
the Lost-in-the-Middle problem where important information in the middle of
long documents is often overlooked. To address these issues, this paper
introduces a novel approach, Extract-then-Evaluate, which involves extracting
key sentences from a long source document and then evaluating the summary by
prompting LLMs. The results reveal that the proposed method not only
significantly reduces evaluation costs but also exhibits a higher correlation
with human evaluations. Furthermore, we provide practical recommendations for
optimal document length and sentence extraction methods, contributing to the
development of cost-effective yet more accurate methods for LLM-based text
generation evaluation.",None,-1
94267be8-9c9f-49ce-addb-e92227548448,Adaptive Multi-Teacher Knowledge Distillation with Meta-Learning,0.318919,"Multi-Teacher knowledge distillation provides students with additional
supervision from multiple pre-trained teachers with diverse information
sources. Most existing methods explore different weighting strategies to obtain
a powerful ensemble teacher, while ignoring the student with poor learning
ability may not benefit from such specialized integrated knowledge. To address
this problem, we propose Adaptive Multi-teacher Knowledge Distillation with
Meta-Learning (MMKD) to supervise student with appropriate knowledge from a
tailored ensemble teacher. With the help of a meta-weight network, the diverse
yet compatible teacher knowledge in the output layer and intermediate layers is
jointly leveraged to enhance the student performance. Extensive experiments on
multiple benchmark datasets validate the effectiveness and flexibility of our
methods. Code is available: https://github.com/Rorozhl/MMKD.",None,-1
d39174b5-764d-4af2-8d05-aefece9c7c5e,MFT: Long-Term Tracking of Every Pixel,0.990078,"We propose MFT -- Multi-Flow dense Tracker -- a novel method for dense,
pixel-level, long-term tracking. The approach exploits optical flows estimated
not only between consecutive frames, but also for pairs of frames at
logarithmically spaced intervals. It selects the most reliable sequence of
flows on the basis of estimates of its geometric accuracy and the probability
of occlusion, both provided by a pre-trained CNN. We show that MFT achieves
competitive performance on the TAP-Vid benchmark, outperforming baselines by a
significant margin, and tracking densely orders of magnitude faster than the
state-of-the-art point-tracking methods. The method is insensitive to
medium-length occlusions and it is robustified by estimating flow with respect
to the reference frame, which reduces drift.",None,-1
0b27debb-a6fc-4263-8919-131109cc33a0,Two Kinds of Recall,0.0805737,"It is an established assumption that pattern-based models are good at
precision, while learning based models are better at recall. But is that really
the case? I argue that there are two kinds of recall: d-recall, reflecting
diversity, and e-recall, reflecting exhaustiveness. I demonstrate through
experiments that while neural methods are indeed significantly better at
d-recall, it is sometimes the case that pattern-based methods are still
substantially better at e-recall. Ideal methods should aim for both kinds, and
this ideal should in turn be reflected in our evaluations.",None,-1
9a09b71e-5973-499a-9cb2-9b914198d5b8,DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition,0.594126,"Graph Convolutional Networks (GCNs) have been widely used in skeleton-based
human action recognition. In GCN-based methods, the spatio-temporal graph is
fundamental for capturing motion patterns. However, existing approaches ignore
the physical dependency and synchronized spatio-temporal correlations between
joints, which limits the representation capability of GCNs. To solve these
problems, we construct the directed diffusion graph for action modeling and
introduce the activity partition strategy to optimize the weight sharing
mechanism of graph convolution kernels. In addition, we present the
spatio-temporal synchronization encoder to embed synchronized spatio-temporal
semantics. Finally, we propose Directed Diffusion Graph Convolutional Network
(DD-GCN) for action recognition, and the experiments on three public datasets:
NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art
performance of our method.",None,-1
276dae41-9277-46c4-8f39-4361f350f5c0,The Evolutionary Computation Methods No One Should Use,0.346538,"The center-bias (or zero-bias) operator has recently been identified as one
of the problems plaguing the benchmarking of evolutionary computation methods.
This operator lets the methods that utilize it easily optimize functions that
have their respective optima in the center of the feasible set. In this paper,
we describe a simple procedure that can be used to identify methods that
incorporate a center-bias operator and use it to investigate 90 evolutionary
computation methods that were published between 1987 and 2022. We show that
more than half (47 out of the 90) of the considered methods have the
center-bias problem. We also show that the center-bias is a relatively new
phenomenon (with the first identified method being from 2012), but its
inclusion has become extremely prevalent in the last few years. Lastly, we
briefly discuss the possible root causes of this issue.",None,-1
115f1c8a-8f6f-4dbd-8890-16b29811589b,"KGConv, a Conversational Corpus grounded in Wikidata",0.110941,"We present KGConv, a large, conversational corpus of 71k conversations where
each question-answer pair is grounded in a Wikidata fact. Conversations contain
on average 8.6 questions and for each Wikidata fact, we provide multiple
variants (12 on average) of the corresponding question using templates, human
annotations, hand-crafted rules and a question rewriting neural model. We
provide baselines for the task of Knowledge-Based, Conversational Question
Generation. KGConv can further be used for other generation and analysis tasks
such as single-turn question generation from Wikidata triples, question
rewriting, question answering from conversation or from knowledge graphs and
quiz generation.",None,-1
7266c801-5759-4103-930b-b645eb3dfc37,Multi-Granularity Prompts for Topic Shift Detection in Dialogue,0.637737,"The goal of dialogue topic shift detection is to identify whether the current
topic in a conversation has changed or needs to change. Previous work focused
on detecting topic shifts using pre-trained models to encode the utterance,
failing to delve into the various levels of topic granularity in the dialogue
and understand dialogue contents. To address the above issues, we take a
prompt-based approach to fully extract topic information from dialogues at
multiple-granularity, i.e., label, turn, and topic. Experimental results on our
annotated Chinese Natural Topic Dialogue dataset CNTD and the publicly
available English TIAGE dataset show that the proposed model outperforms the
baselines. Further experiments show that the information extracted at different
levels of granularity effectively helps the model comprehend the conversation
topics.",None,-1
3ecbf59d-f8ad-4e71-b7e6-c12ab3cda1ad,Construction of Knowledge Graphs: State and Challenges,0.925324,"With knowledge graphs (KGs) at the center of numerous applications such as
recommender systems and question answering, the need for generalized pipelines
to construct and continuously update such KGs is increasing. While the
individual steps that are necessary to create KGs from unstructured (e.g. text)
and structured data sources (e.g. databases) are mostly well-researched for
their one-shot execution, their adoption for incremental KG updates and the
interplay of the individual steps have hardly been investigated in a systematic
manner so far. In this work, we first discuss the main graph models for KGs and
introduce the major requirement for future KG construction pipelines. Next, we
provide an overview of the necessary steps to build high-quality KGs, including
cross-cutting topics such as metadata management, ontology development, and
quality assurance. We then evaluate the state of the art of KG construction
w.r.t the introduced requirements for specific popular KGs as well as some
recent tools and strategies for KG construction. Finally, we identify areas in
need of further research and improvement.",None,-1
5a9f702a-7672-4277-8876-d187497aa86a,Tiny and Efficient Model for the Edge Detection Generalization,0.372618,"Most high-level computer vision tasks rely on low-level image operations as
their initial processes. Operations such as edge detection, image enhancement,
and super-resolution, provide the foundations for higher level image analysis.
In this work we address the edge detection considering three main objectives:
simplicity, efficiency, and generalization since current state-of-the-art
(SOTA) edge detection models are increased in complexity for better accuracy.
To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light
convolutional neural network with only $58K$ parameters, less than $0.2$% of
the state-of-the-art models. Training on the BIPED dataset takes $less than 30
minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model
is easy to train and it quickly converges within very first few epochs, while
the predicted edge-maps are crisp and of high quality. Additionally, we propose
a new dataset to test the generalization of edge detection, which comprises
samples from popular images used in edge detection and image segmentation. The
source code is available in https://github.com/xavysp/TEED.",None,-1
59f3b25d-60a2-4bf3-bda4-6fd1948e9859,Automatic Aspect Extraction from Scientific Texts,0.164286,"Being able to extract from scientific papers their main points, key insights,
and other important information, referred to here as aspects, might facilitate
the process of conducting a scientific literature review. Therefore, the aim of
our research is to create a tool for automatic aspect extraction from
Russian-language scientific texts of any domain. In this paper, we present a
cross-domain dataset of scientific texts in Russian, annotated with such
aspects as Task, Contribution, Method, and Conclusion, as well as a baseline
algorithm for aspect extraction, based on the multilingual BERT model
fine-tuned on our data. We show that there are some differences in aspect
representation in different domains, but even though our model was trained on a
limited number of scientific domains, it is still able to generalize to new
domains, as was proved by cross-domain experiments. The code and the dataset
are available at
\url{https://github.com/anna-marshalova/automatic-aspect-extraction-from-scientific-texts}.",None,-1
85c06942-18b1-4d5b-99bb-22cd468db51c,TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation,0.855161,"Test-time adaptation methods have been gaining attention recently as a
practical solution for addressing source-to-target domain gaps by gradually
updating the model without requiring labels on the target data. In this paper,
we propose a method of test-time adaptation for category-level object pose
estimation called TTA-COPE. We design a pose ensemble approach with a
self-training loss using pose-aware confidence. Unlike previous unsupervised
domain adaptation methods for category-level object pose estimation, our
approach processes the test data in a sequential, online manner, and it does
not require access to the source domain at runtime. Extensive experimental
results demonstrate that the proposed pose ensemble and the self-training loss
improve category-level object pose performance during test time under both
semi-supervised and unsupervised settings. Project page:
https://taeyeop.com/ttacope",None,-1
facdecb2-b11b-4564-a317-ebccd93b2d10,Adaptive Control of Resource Flow to Optimize Construction Work and Cash Flow via Online Deep Reinforcement Learning,0.981726,"Due to complexity and dynamics of construction work, resource, and cash
flows, poor management of them usually leads to time and cost overruns,
bankruptcy, even project failure. Existing approaches in construction failed to
achieve optimal control of resource flow in a dynamic environment with
uncertainty. Therefore, this paper introducess a model and method to adaptive
control the resource flows to optimize the work and cash flows of construction
projects. First, a mathematical model based on a partially observable Markov
decision process is established to formulate the complex interactions of
construction work, resource, and cash flows as well as uncertainty and
variability of diverse influence factors. Meanwhile, to efficiently find the
optimal solutions, a deep reinforcement learning (DRL) based method is
introduced to realize the continuous adaptive optimal control of labor and
material flows, thereby optimizing the work and cash flows. To assist the
training process of DRL, a simulator based on discrete event simulation is also
developed to mimic the dynamic features and external environments of a project.
Experiments in simulated scenarios illustrate that our method outperforms the
vanilla empirical method and genetic algorithm, possesses remarkable capability
in diverse projects and external environments, and a hybrid agent of DRL and
empirical method leads to the best result. This paper contributes to adaptive
control and optimization of coupled work, resource, and cash flows, and may
serve as a step stone for adopting DRL technology in construction project
management.",None,-1
fc2e9f72-01af-43d4-8c0f-956c320a5d5e,UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity,0.738807,"Image reconstruction and captioning from brain activity evoked by visual
stimuli allow researchers to further understand the connection between the
human brain and the visual perception system. While deep generative models have
recently been employed in this field, reconstructing realistic captions and
images with both low-level details and high semantic fidelity is still a
challenging problem. In this work, we propose UniBrain: Unify Image
Reconstruction and Captioning All in One Diffusion Model from Human Brain
Activity. For the first time, we unify image reconstruction and captioning from
visual-evoked functional magnetic resonance imaging (fMRI) through a latent
diffusion model termed Versatile Diffusion. Specifically, we transform fMRI
voxels into text and image latent for low-level information and guide the
backward diffusion process through fMRI-based image and text conditions derived
from CLIP to generate realistic captions and images. UniBrain outperforms
current methods both qualitatively and quantitatively in terms of image
reconstruction and reports image captioning results for the first time on the
Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and
functional region-of-interest (ROI) analysis further exhibit the superiority of
UniBrain and provide comprehensive insight for visual-evoked brain decoding.",None,-1
b71b226d-1538-4680-8da6-7cf7bb6ae032,Jambu: A historical linguistic database for South Asian languages,0.409659,"We introduce Jambu, a cognate database of South Asian languages which unifies
dozens of previous sources in a structured and accessible format. The database
includes 287k lemmata from 602 lects, grouped together in 23k sets of cognates.
We outline the data wrangling necessary to compile the dataset and train neural
models for reflex prediction on the Indo-Aryan subset of the data. We hope that
Jambu is an invaluable resource for all historical linguists and Indologists,
and look towards further improvement and expansion of the database.",None,-1
090f6a24-5732-44ae-b16f-516f7930f849,CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities,0.609976,"Detecting norm violations in online communities is critical to maintaining
healthy and safe spaces for online discussions. Existing machine learning
approaches often struggle to adapt to the diverse rules and interpretations
across different communities due to the inherent challenges of fine-tuning
models for such context-specific tasks. In this paper, we introduce
Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a
novel method that employs prompt-based learning to detect norm violations
across various types of rules. CPL-NoViD outperforms the baseline by
incorporating context through natural language prompts and demonstrates
improved performance across different rule types. Significantly, it not only
excels in cross-rule-type and cross-community norm violation detection but also
exhibits adaptability in few-shot learning scenarios. Most notably, it
establishes a new state-of-the-art in norm violation detection, surpassing
existing benchmarks. Our work highlights the potential of prompt-based learning
for context-sensitive norm violation detection and paves the way for future
research on more adaptable, context-aware models to better support online
community moderators.",None,-1
623d928f-f0c7-4f96-badb-ce2179257bbf,Data-Driven Bilateral Generalized Two-Dimensional Quaternion Principal Component Analysis with Application to Color Face Recognition,0.213221,"A new data-driven bilateral generalized two-dimensional quaternion principal
component analysis (BiG2DQPCA) is presented to extract the features of matrix
samples from both row and column directions. This general framework directly
works on the 2D color images without vectorizing and well preserves the spatial
and color information, which makes it flexible to fit various real-world
applications. A generalized ridge regression model of BiG2DQPCA is firstly
proposed with orthogonality constrains on aimed features. Applying the
deflation technique and the framework of minorization-maximization, a new
quaternion optimization algorithm is proposed to compute the optimal features
of BiG2DQPCA and a closed-form solution is obtained at each iteration. A new
approach based on BiG2DQPCA is presented for color face recognition and image
reconstruction with a new data-driven weighting technique. Sufficient numerical
experiments are implemented on practical color face databases and indicate the
superiority of BiG2DQPCA over the state-of-the-art methods in terms of
recognition accuracies and rates of image reconstruction.",None,-1
3cef9d68-b846-44b4-b3cf-e593becc458c,Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation,0.592189,"Cross-modal contrastive learning in vision language pretraining (VLP) faces
the challenge of (partial) false negatives. In this paper, we study this
problem from the perspective of Mutual Information (MI) optimization. It is
common sense that InfoNCE loss used in contrastive learning will maximize the
lower bound of MI between anchors and their positives, while we theoretically
prove that MI involving negatives also matters when noises commonly exist.
Guided by a more general lower bound form for optimization, we propose a
contrastive learning strategy regulated by progressively refined cross-modal
similarity, to more accurately optimize MI between an image/text anchor and its
negative texts/images instead of improperly minimizing it. Our method performs
competitively on four downstream cross-modal tasks and systematically balances
the beneficial and harmful effects of (partial) false negative samples under
theoretical guidance.",None,-1
cd7b1ead-cb6c-4eb9-b852-41af0f58fe03,CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval,0.102759,"Passage retrieval aims to retrieve relevant passages from large collections
of the open-domain corpus. Contextual Masked Auto-Encoding has been proven
effective in representation bottleneck pre-training of a monolithic
dual-encoder for passage retrieval. Siamese or fully separated dual-encoders
are often adopted as basic retrieval architecture in the pre-training and
fine-tuning stages for encoding queries and passages into their latent
embedding spaces. However, simply sharing or separating the parameters of the
dual-encoder results in an imbalanced discrimination of the embedding spaces.
In this work, we propose to pre-train Contextual Masked Auto-Encoder with
Mixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate
textual-specific experts for individually encoding the distinct properties of
queries and passages. Meanwhile, a shared self-attention layer is still kept
for unified attention modeling. Results on large-scale passage retrieval
benchmarks show steady improvement in retrieval performances. The quantitive
analysis also shows a more balanced discrimination of the latent embedding
spaces.",None,-1
0706c55d-d31e-44ff-94ec-f91b847907ea,Generative Knowledge Selection for Knowledge-Grounded Dialogues,0.679619,"Knowledge selection is the key in knowledge-grounded dialogues (KGD), which
aims to select an appropriate knowledge snippet to be used in the utterance
based on dialogue history. Previous studies mainly employ the classification
approach to classify each candidate snippet as ""relevant"" or ""irrelevant""
independently. However, such approaches neglect the interactions between
snippets, leading to difficulties in inferring the meaning of snippets.
Moreover, they lack modeling of the discourse structure of dialogue-knowledge
interactions. We propose a simple yet effective generative approach for
knowledge selection, called GenKS. GenKS learns to select snippets by
generating their identifiers with a sequence-to-sequence model. GenKS therefore
captures intra-knowledge interaction inherently through attention mechanisms.
Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge
interactions explicitly. We conduct experiments on three benchmark datasets,
and verify GenKS achieves the best results on both knowledge selection and
response generation.",None,-1
3185c662-662a-4625-9f52-f9781cab3fe6,ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer,0.643781,"Large-scale language models, like ChatGPT, have garnered significant media
attention and stunned the public with their remarkable capacity for generating
coherent text from short natural language prompts. In this paper, we aim to
conduct a systematic inspection of ChatGPT's performance in two controllable
generation tasks, with respect to ChatGPT's ability to adapt its output to
different target audiences (expert vs. layman) and writing styles (formal vs.
informal). Additionally, we evaluate the faithfulness of the generated text,
and compare the model's performance with human-authored texts. Our findings
indicate that the stylistic variations produced by humans are considerably
larger than those demonstrated by ChatGPT, and the generated texts diverge from
human samples in several characteristics, such as the distribution of word
types. Moreover, we observe that ChatGPT sometimes incorporates factual errors
or hallucinations when adapting the text to suit a specific style.",None,-1
1f9f64fd-4d60-41ff-8aa8-c35e176d75e8,Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models,0.55578,"In this study, we present an efficient and effective approach for achieving
temporally consistent synthetic-to-real video translation in videos of varying
lengths. Our method leverages off-the-shelf conditional image diffusion models,
allowing us to perform multiple synthetic-to-real image generations in
parallel. By utilizing the available optical flow information from the
synthetic videos, our approach seamlessly enforces temporal consistency among
corresponding pixels across frames. This is achieved through joint noise
optimization, effectively minimizing spatial and temporal discrepancies. To the
best of our knowledge, our proposed method is the first to accomplish diverse
and temporally consistent synthetic-to-real video translation using conditional
image diffusion models. Furthermore, our approach does not require any training
or fine-tuning of the diffusion models. Extensive experiments conducted on
various benchmarks for synthetic-to-real video translation demonstrate the
effectiveness of our approach, both quantitatively and qualitatively. Finally,
we show that our method outperforms other baseline methods in terms of both
temporal consistency and visual quality.",None,-1
f3d3ca86-7637-490c-867e-bdfd0f955fc4,Learning the greatest common divisor: explaining transformer predictions,0.361206,"The predictions of small transformers, trained to calculate the greatest
common divisor (GCD) of two positive integers, can be fully characterized by
looking at model inputs and outputs. As training proceeds, the model learns a
list $\mathcal D$ of integers, products of divisors of the base used to
represent integers and small primes, and predicts the largest element of
$\mathcal D$ that divides both inputs. Training distributions impact
performance. Models trained from uniform operands only learn a handful of GCD
(up to $38$ GCD $\leq100$). Log-uniform operands boost performance to $73$ GCD
$\leq 100$, and a log-uniform distribution of outcomes (i.e. GCD) to $91$.
However, training from uniform (balanced) GCD breaks explainability.",None,-1
e99073e4-5802-49b2-ba86-0d95108fb611,PyReason: Software for Open World Temporal Logic,0.520276,"The growing popularity of neuro symbolic reasoning has led to the adoption of
various forms of differentiable (i.e., fuzzy) first order logic. We introduce
PyReason, a software framework based on generalized annotated logic that both
captures the current cohort of differentiable logics and temporal extensions to
support inference over finite periods of time with capabilities for open world
reasoning. Further, PyReason is implemented to directly support reasoning over
graphical structures (e.g., knowledge graphs, social networks, biological
networks, etc.), produces fully explainable traces of inference, and includes
various practical features such as type checking and a memory-efficient
implementation. This paper reviews various extensions of generalized annotated
logic integrated into our implementation, our modern, efficient Python-based
implementation that conducts exact yet scalable deductive inference, and a
suite of experiments. PyReason is available at: github.com/lab-v2/pyreason.",None,-1
6642ba50-051b-4210-8056-2b4dd38ff868,Language Models can be Logical Solvers,0.435382,"Logical reasoning is a fundamental aspect of human intelligence and a key
component of tasks like problem-solving and decision-making. Recent
advancements have enabled Large Language Models (LLMs) to potentially exhibit
reasoning capabilities, but complex logical reasoning remains a challenge. The
state-of-the-art, solver-augmented language models, use LLMs to parse natural
language logical questions into symbolic representations first and then adopt
external logical solvers to take in the symbolic representations and output the
answers. Despite their impressive performance, any parsing errors will
inevitably result in the failure of the execution of the external logical
solver and no answer to the logical questions. In this paper, we introduce
LoGiPT, a novel language model that directly emulates the reasoning processes
of logical solvers and bypasses the parsing errors by learning to strict
adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly
constructed instruction-tuning dataset derived from revealing and refining the
invisible reasoning process of deductive solvers. Experimental results on two
public deductive reasoning datasets demonstrate that LoGiPT outperforms
state-of-the-art solver-augmented LMs and few-shot prompting methods on
competitive LLMs like ChatGPT or GPT-4.",None,-1
2b771b09-3b8b-4274-b839-84b2971b4a8a,Neuro-symbolic Meta Reinforcement Learning for Trading,0.146773,"We model short-duration (e.g. day) trading in financial markets as a
sequential decision-making problem under uncertainty, with the added
complication of continual concept-drift. We, therefore, employ meta
reinforcement learning via the RL2 algorithm. It is also known that human
traders often rely on frequently occurring symbolic patterns in price series.
We employ logical program induction to discover symbolic patterns that occur
frequently as well as recently, and explore whether using such features
improves the performance of our meta reinforcement learning algorithm. We
report experiments on real data indicating that meta-RL is better than vanilla
RL and also benefits from learned symbolic features.",None,-1
d96d3c2f-ae33-4b33-91bf-83e7533d1dd4,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,0.800991,"Layout-to-image generation refers to the task of synthesizing photo-realistic
images based on semantic layouts. In this paper, we propose LayoutDiffuse that
adapts a foundational diffusion model pretrained on large-scale image or
text-image datasets for layout-to-image generation. By adopting a novel neural
adaptor based on layout attention and task-aware prompts, our method trains
efficiently, generates images with both high perceptual quality and layout
alignment, and needs less data. Experiments on three datasets show that our
method significantly outperforms other 10 generative models based on GANs,
VQ-VAE, and diffusion models.",None,-1
436027b0-0332-469a-a3a0-adfceed4853f,Is More Always Better? The Effects of Personal Characteristics and Level of Detail on the Perception of Explanations in a Recommender System,0.879326,"Despite the acknowledgment that the perception of explanations may vary
considerably between end-users, explainable recommender systems (RS) have
traditionally followed a one-size-fits-all model, whereby the same explanation
level of detail is provided to each user, without taking into consideration
individual user's context, i.e., goals and personal characteristics. To fill
this research gap, we aim in this paper at a shift from a one-size-fits-all to
a personalized approach to explainable recommendation by giving users agency in
deciding which explanation they would like to see. We developed a transparent
Recommendation and Interest Modeling Application (RIMA) that provides on-demand
personalized explanations of the recommendations, with three levels of detail
(basic, intermediate, advanced) to meet the demands of different types of
end-users. We conducted a within-subject study (N=31) to investigate the
relationship between user's personal characteristics and the explanation level
of detail, and the effects of these two variables on the perception of the
explainable RS with regard to different explanation goals. Our results show
that the perception of explainable RS with different levels of detail is
affected to different degrees by the explanation goal and user type.
Consequently, we suggested some theoretical and design guidelines to support
the systematic design of explanatory interfaces in RS tailored to the user's
context.",None,-1
07ca5dec-3cb6-444e-9f06-326dcf926d53,Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition,0.780015,"Human emotion recognition plays an important role in human-computer
interaction. In this paper, we present our approach to the Valence-Arousal (VA)
Estimation Challenge, Expression (Expr) Classification Challenge, and Action
Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective
Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel
multi-modal fusion model that leverages Temporal Convolutional Networks (TCN)
and Transformer to enhance the performance of continuous emotion recognition.
Our model aims to effectively integrate visual and audio information for
improved accuracy in recognizing emotions. Our model outperforms the baseline
and ranks 3 in the Expression Classification challenge.",None,-1
53d22f47-2f92-4a88-9bfa-356eceff48d0,A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets,0.562199,"Low-quality data can cause downstream problems in high-stakes applications.
Data-centric approach emphasizes on improving dataset quality to enhance model
performance. High-quality datasets are needed for general-purpose Large
Language Models (LLMs) training, as well as for domain-specific models, which
are usually small in size as it is costly to engage a large number of domain
experts for their creation. Thus, it is vital to ensure high-quality
domain-specific training data. In this paper, we propose a framework for
enhancing the data quality of original datasets. We applied the proposed
framework to four biomedical datasets and showed relative improvement of up to
33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when
using back translation to enhance the original dataset quality.",None,-1
917e4a7e-0ad3-4663-a856-235530071f8f,Turning large language models into cognitive models,0.841132,"Large language models are powerful systems that excel at many tasks, ranging
from translation to mathematical reasoning. Yet, at the same time, these models
often show unhuman-like characteristics. In the present paper, we address this
gap and ask whether large language models can be turned into cognitive models.
We find that -- after finetuning them on data from psychological experiments --
these models offer accurate representations of human behavior, even
outperforming traditional cognitive models in two decision-making domains. In
addition, we show that their representations contain the information necessary
to model behavior on the level of individual subjects. Finally, we demonstrate
that finetuning on multiple tasks enables large language models to predict
human behavior in a previously unseen task. Taken together, these results
suggest that large, pre-trained models can be adapted to become generalist
cognitive models, thereby opening up new research directions that could
transform cognitive psychology and the behavioral sciences as a whole.",None,-1
4755f031-e5bd-4f7e-9e50-589c038cc580,Self-Detoxifying Language Models via Toxification Reversal,0.118408,"Language model detoxification aims to minimize the risk of generating
offensive or harmful content in pretrained language models (PLMs) for safer
deployment. Existing methods can be roughly categorized as finetuning-based and
decoding-based. However, the former is often resource-intensive, while the
latter relies on additional components and potentially compromises the
generation fluency. In this paper, we propose a more lightweight approach that
enables the PLM itself to achieve ""self-detoxification"". Our method is built
upon the observation that prepending a negative steering prompt can effectively
induce PLMs to generate toxic content. At the same time, we are inspired by the
recent research in the interpretability field, which formulates the evolving
contextualized representations within the PLM as an information stream
facilitated by the attention layers. Drawing on this idea, we devise a method
to identify the toxification direction from the normal generation process to
the one prompted with the negative prefix, and then steer the generation to the
reversed direction by manipulating the information movement within the
attention layers. Experimental results show that our approach, without any
fine-tuning or extra components, can achieve comparable performance with
state-of-the-art methods.",None,-1
06f762bf-06ff-4a16-8df8-4ceced1271e7,Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities,0.649518,"One of the challenges of studying common neurological disorders is disease
heterogeneity including differences in causes, neuroimaging characteristics,
comorbidities, or genetic variation. Normative modelling has become a popular
method for studying such cohorts where the 'normal' behaviour of a
physiological system is modelled and can be used at subject level to detect
deviations relating to disease pathology. For many heterogeneous diseases, we
expect to observe abnormalities across a range of neuroimaging and biological
variables. However, thus far, normative models have largely been developed for
studying a single imaging modality. We aim to develop a multi-modal normative
modelling framework where abnormality is aggregated across variables of
multiple modalities and is better able to detect deviations than uni-modal
baselines. We propose two multi-modal VAE normative models to detect subject
level deviations across T1 and DTI data. Our proposed models were better able
to detect diseased individuals, capture disease severity, and correlate with
patient cognition than baseline approaches. We also propose a multivariate
latent deviation metric, measuring deviations from the joint latent space,
which outperformed feature-based metrics.",None,-1
2203a033-7a7d-454a-807f-4bb4b49d0b2b,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,0.878276,"Few-shot semantic segmentation is the task of learning to locate each pixel
of the novel class in the query image with only a few annotated support images.
The current correlation-based methods construct pair-wise feature correlations
to establish the many-to-many matching because the typical prototype-based
approaches cannot learn fine-grained correspondence relations. However, the
existing methods still suffer from the noise contained in naive correlations
and the lack of context semantic information in correlations. To alleviate
these problems mentioned above, we propose a Feature-Enhanced Context-Aware
Network (FECANet). Specifically, a feature enhancement module is proposed to
suppress the matching noise caused by inter-class local similarity and enhance
the intra-class relevance in the naive correlation. In addition, we propose a
novel correlation reconstruction module that encodes extra correspondence
relations between foreground and background and multi-scale context semantic
features, significantly boosting the encoder to capture a reliable matching
pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that
our proposed FECANet leads to remarkable improvement compared to previous
state-of-the-arts, demonstrating its effectiveness.",None,-1
47b0ef04-6b18-435a-9406-4aa5b441af7c,Towards Learning Rubik's Cube with N-tuple-based Reinforcement Learning,0.497613,"This work describes in detail how to learn and solve the Rubik's cube game
(or puzzle) in the General Board Game (GBG) learning and playing framework. We
cover the cube sizes 2x2x2 and 3x3x3. We describe in detail the cube's state
representation, how to transform it with twists, whole-cube rotations and color
transformations and explain the use of symmetries in Rubik's cube. Next, we
discuss different n-tuple representations for the cube, how we train the agents
by reinforcement learning and how we improve the trained agents during
evaluation by MCTS wrapping. We present results for agents that learn Rubik's
cube from scratch, with and without MCTS wrapping, with and without symmetries
and show that both, MCTS wrapping and symmetries, increase computational costs,
but lead at the same time to much better results. We can solve the 2x2x2 cube
completely, and the 3x3x3 cube in the majority of the cases for scrambled cubes
up to p = 15 (QTM). We cannot yet reliably solve 3x3x3 cubes with more than 15
scrambling twists. Although our computational costs are higher with MCTS
wrapping and with symmetries than without, they are still considerably lower
than in the approaches of McAleer et al. (2018, 2019) and Agostinelli et al.
(2019) who provide the best Rubik's cube learning agents so far.",None,-1
2cbdc5a8-fcb7-48ba-8064-618fd923cfc7,Scale Guided Hypernetwork for Blind Super-Resolution Image Quality Assessment,0.40159,"With the emergence of image super-resolution (SR) algorithm, how to blindly
evaluate the quality of super-resolution images has become an urgent task.
However, existing blind SR image quality assessment (IQA) metrics merely focus
on visual characteristics of super-resolution images, ignoring the available
scale information. In this paper, we reveal that the scale factor has a
statistically significant impact on subjective quality scores of SR images,
indicating that the scale information can be used to guide the task of blind SR
IQA. Motivated by this, we propose a scale guided hypernetwork framework that
evaluates SR image quality in a scale-adaptive manner. Specifically, the blind
SR IQA procedure is divided into three stages, i.e., content perception,
evaluation rule generation, and quality prediction. After content perception, a
hypernetwork generates the evaluation rule used in quality prediction based on
the scale factor of the SR image. We apply the proposed scale guided
hypernetwork framework to existing representative blind IQA metrics, and
experimental results show that the proposed framework not only boosts the
performance of these IQA metrics but also enhances their generalization
abilities. Source code will be available at https://github.com/JunFu1995/SGH.",None,-1
4dd39af8-3761-48e3-b212-ef53c28bf9be,Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction,0.999988,"Modern methods for vision-centric autonomous driving perception widely adopt
the bird's-eye-view (BEV) representation to describe a 3D scene. Despite its
better efficiency than voxel representation, it has difficulty describing the
fine-grained 3D structure of a scene with a single plane. To address this, we
propose a tri-perspective view (TPV) representation which accompanies BEV with
two additional perpendicular planes. We model each point in the 3D space by
summing its projected features on the three planes. To lift image features to
the 3D TPV space, we further propose a transformer-based TPV encoder
(TPVFormer) to obtain the TPV features effectively. We employ the attention
mechanism to aggregate the image features corresponding to each query in each
TPV plane. Experiments show that our model trained with sparse supervision
effectively predicts the semantic occupancy for all voxels. We demonstrate for
the first time that using only camera inputs can achieve comparable performance
with LiDAR-based methods on the LiDAR segmentation task on nuScenes. Code:
https://github.com/wzzheng/TPVFormer.",None,-1
071d4962-3656-4bf7-afe3-d0d7d3e6b09d,Generative Models for 3D Point Clouds,0.0737171,"Point clouds are rich geometric data structures, where their three
dimensional structure offers an excellent domain for understanding the
representation learning and generative modeling in 3D space. In this work, we
aim to improve the performance of point cloud latent-space generative models by
experimenting with transformer encoders, latent-space flow models, and
autoregressive decoders. We analyze and compare both generation and
reconstruction performance of these models on various object types.",None,-1
ca35bc5c-473a-441d-ae42-0e272d7f4712,Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer,0.472168,"We propose a novel solution for predicting future trajectories of
pedestrians. Our method uses a multimodal encoder-decoder transformer
architecture, which takes as input both pedestrian locations and ego-vehicle
speeds. Notably, our decoder predicts the entire future trajectory in a
single-pass and does not perform one-step-ahead prediction, which makes the
method effective for embedded edge deployment. We perform detailed experiments
and evaluate our method on two popular datasets, PIE and JAAD. Quantitative
results demonstrate the superiority of our proposed model over the current
state-of-the-art, which consistently achieves the lowest error for 3 time
horizons of 0.5, 1.0 and 1.5 seconds. Moreover, the proposed method is
significantly faster than the state-of-the-art for the two datasets of PIE and
JAAD. Lastly, ablation experiments demonstrate the impact of the key multimodal
configuration of our method.",None,-1
9fb1b49f-e44b-4673-a464-471a599a7fed,MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine,0.134135,"METHODS: First, a set of evaluation criteria is designed based on a
comprehensive literature review. Second, existing candidate criteria are
optimized for using a Delphi method by five experts in medicine and
engineering. Third, three clinical experts design a set of medical datasets to
interact with LLMs. Finally, benchmarking experiments are conducted on the
datasets. The responses generated by chatbots based on LLMs are recorded for
blind evaluations by five licensed medical experts. RESULTS: The obtained
evaluation criteria cover medical professional capabilities, social
comprehensive capabilities, contextual capabilities, and computational
robustness, with sixteen detailed indicators. The medical datasets include
twenty-seven medical dialogues and seven case reports in Chinese. Three
chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor
PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental
results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both
multiple-turn medical dialogue and case report scenarios.",None,-1
fada50da-e180-4d92-8cd7-4eb751760167,Discovering Universal Geometry in Embeddings with ICA,0.760512,"This study utilizes Independent Component Analysis (ICA) to unveil a
consistent semantic structure within embeddings of words or images. Our
approach extracts independent semantic components from the embeddings of a
pre-trained model by leveraging anisotropic information that remains after the
whitening process in Principal Component Analysis (PCA). We demonstrate that
each embedding can be expressed as a composition of a few intrinsic
interpretable axes and that these semantic axes remain consistent across
different languages, algorithms, and modalities. The discovery of a universal
semantic structure in the geometric patterns of embeddings enhances our
understanding of the representations in embeddings.",None,-1
a418702e-9771-4cb1-8efd-12ae06f58176,A Case Study for Compliance as Code with Graphs and Language Models: Public release of the Regulatory Knowledge Graph,0.13081,"The paper presents a study on using language models to automate the
construction of executable Knowledge Graph (KG) for compliance. The paper
focuses on Abu Dhabi Global Market regulations and taxonomy, involves manual
tagging a portion of the regulations, training BERT-based models, which are
then applied to the rest of the corpus. Coreference resolution and syntax
analysis were used to parse the relationships between the tagged entities and
to form KG stored in a Neo4j database. The paper states that the use of machine
learning models released by regulators to automate the interpretation of rules
is a vital step towards compliance automation, demonstrates the concept
querying with Cypher, and states that the produced sub-graphs combined with
Graph Neural Networks (GNN) will achieve expandability in judgment automation
systems. The graph is open sourced on GitHub to provide structured data for
future advancements in the field.",None,-1
c0e30eb6-01bf-4142-8f3c-81bfb54bf970,Improving Contrastive Learning of Sentence Embeddings from AI Feedback,0.73365,"Contrastive learning has become a popular approach in natural language
processing, particularly for the learning of sentence embeddings. However, the
discrete nature of natural language makes it difficult to ensure the quality of
positive and negative sample pairs generated through data augmentation methods.
Although supervised contrastive learning can produce more accurate sample pairs
with human feedback labels, it still lacks fine-grained training signals. In
this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of
sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our
method utilizes AI feedback from large pre-trained language models (LLMs) to
construct sample pairs with fine-grained sample similarity scores to improve
contrastive learning. Besides, we combine human feedback and AI feedback to
provide better supervision signals for supervised contrastive learning of
sentence embeddings. Experimental results show that our method achieves
state-of-the-art performance on several semantic textual similarity (STS) and
transfer learning tasks compared to other unsupervised and supervised
contrastive learning methods.",None,-1
516f64c1-6ba8-4c77-a87e-6bfe58c77374,Causal Document-Grounded Dialogue Pre-training,0.426382,"The goal of document-grounded dialogue (DocGD) is to generate a response by
grounding the evidence in a supporting document in accordance with the dialogue
context. This process involves four variables that are causally connected.
Recently, task-specific pre-training has greatly boosted performances on many
downstream tasks. Existing DocGD methods, however, continue to rely on general
pre-trained language models without a specifically tailored pre-training
approach that explicitly captures the causal relationships. To tackle this
issue, we are the first to present a causally-complete dataset construction
strategy for building million-level DocGD pre-training corpora. To better
capture causality, we further propose a causally-perturbed pre-training
strategy, which introduces causal perturbations on the variables and optimizes
the overall causal effect. Experiments on three benchmark datasets demonstrate
that our causal pre-training achieves considerable and consistent improvements
under fully-supervised, low-resource, few-shot, and zero-shot settings.",None,-1
9f5a01aa-3512-400e-b0be-05b71926e985,Enriching language models with graph-based context information to better understand textual data,0.080906,"A considerable number of texts encountered daily are somehow connected with
each other. For example, Wikipedia articles refer to other articles via
hyperlinks, scientific papers relate to others via citations or (co)authors,
while tweets relate via users that follow each other or reshare content. Hence,
a graph-like structure can represent existing connections and be seen as
capturing the ""context"" of the texts. The question thus arises if extracting
and integrating such context information into a language model might help
facilitate a better automated understanding of the text. In this study, we
experimentally demonstrate that incorporating graph-based contextualization
into BERT model enhances its performance on an example of a classification
task. Specifically, on Pubmed dataset, we observed a reduction in error from
8.51% to 7.96%, while increasing the number of parameters just by 1.6%.
  Our source code: https://github.com/tryptofanik/gc-bert",None,-1
82d3c84f-5c90-4349-84c4-b00d74ee512e,Multi-Dimensional Evaluation of Text Summarization with In-Context Learning,0.842946,"Evaluation of natural language generation (NLG) is complex and
multi-dimensional. Generated text can be evaluated for fluency, coherence,
factuality, or any other dimensions of interest. Most frameworks that perform
such multi-dimensional evaluation require training on large manually or
synthetically generated datasets. In this paper, we study the efficacy of large
language models as multi-dimensional evaluators using in-context learning,
obviating the need for large training datasets. Our experiments show that
in-context learning-based evaluators are competitive with learned evaluation
frameworks for the task of text summarization, establishing state-of-the-art on
dimensions such as relevance and factual consistency. We then analyze the
effects of factors such as the selection and number of in-context examples on
performance. Finally, we study the efficacy of in-context learning based
evaluators in evaluating zero-shot summaries written by large language models
such as GPT-3.",None,-1
915c0d6b-10f4-4100-b0cd-598a4a68c4ec,SLiC-HF: Sequence Likelihood Calibration with Human Feedback,1.0,"Learning from human feedback has been shown to be effective at aligning
language models with human preferences. Past work has often relied on
Reinforcement Learning from Human Feedback (RLHF), which optimizes the language
model using reward scores assigned from a reward model trained on human
preference data. In this work we show how the recently introduced Sequence
Likelihood Calibration (SLiC), can also be used to effectively learn from human
preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human
feedback data collected for a different model, similar to off-policy, offline
RL data. Automatic and human evaluation experiments on the TL;DR summarization
task show that SLiC-HF significantly improves supervised fine-tuning baselines.
Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF
implementation used in past work while being much simpler to implement, easier
to tune and more computationally efficient in practice.",None,-1
2bfc4078-18fa-438e-9385-e217b61c4205,Rethinking the Localization in Weakly Supervised Object Localization,0.466265,"Weakly supervised object localization (WSOL) is one of the most popular and
challenging tasks in computer vision. This task is to localize the objects in
the images given only the image-level supervision. Recently, dividing WSOL into
two parts (class-agnostic object localization and object classification) has
become the state-of-the-art pipeline for this task. However, existing solutions
under this pipeline usually suffer from the following drawbacks: 1) they are
not flexible since they can only localize one object for each image due to the
adopted single-class regression (SCR) for localization; 2) the generated pseudo
bounding boxes may be noisy, but the negative impact of such noise is not well
addressed. To remedy these drawbacks, we first propose to replace SCR with a
binary-class detector (BCD) for localizing multiple objects, where the detector
is trained by discriminating the foreground and background. Then we design a
weighted entropy (WE) loss using the unlabeled data to reduce the negative
impact of noisy bounding boxes. Extensive experiments on the popular
CUB-200-2011 and ImageNet-1K datasets demonstrate the effectiveness of our
method.",None,-1
0b8243b7-14ea-4066-a4b9-19e5f89e140d,Detecting and Mitigating Hallucinations in Multilingual Summarisation,0.859355,"Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.",None,-1
6b5998e2-d7c8-4655-b4e6-6634f087390a,SceneCalib: Automatic Targetless Calibration of Cameras and Lidars in Autonomous Driving,0.439047,"Accurate camera-to-lidar calibration is a requirement for sensor data fusion
in many 3D perception tasks. In this paper, we present SceneCalib, a novel
method for simultaneous self-calibration of extrinsic and intrinsic parameters
in a system containing multiple cameras and a lidar sensor. Existing methods
typically require specially designed calibration targets and human operators,
or they only attempt to solve for a subset of calibration parameters. We
resolve these issues with a fully automatic method that requires no explicit
correspondences between camera images and lidar point clouds, allowing for
robustness to many outdoor environments. Furthermore, the full system is
jointly calibrated with explicit cross-camera constraints to ensure that
camera-to-camera and camera-to-lidar extrinsic parameters are consistent.",None,-1
3f58e23e-1d7b-4dc5-a823-b3cefc198880,"Industrial Segment Anything -- a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul",0.538005,"Deploying deep learning-based applications in specialized domains like the
aircraft production industry typically suffers from the training data
availability problem. Only a few datasets represent non-everyday objects,
situations, and tasks. Recent advantages in research around Vision Foundation
Models (VFM) opened a new area of tasks and models with high generalization
capabilities in non-semantic and semantic predictions. As recently demonstrated
by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a
promising direction in tackling the boundaries spanned by data, context, and
sensor variety. Although, investigating its application within specific domains
is subject to ongoing research. This paper contributes here by surveying
applications of the SAM in aircraft production-specific use cases. We include
manufacturing, intralogistics, as well as maintenance, repair, and overhaul
processes, also representing a variety of other neighboring industrial domains.
Besides presenting the various use cases, we further discuss the injection of
domain knowledge.",None,-1
d89b7240-85a7-4e28-ba92-b79187cd8905,Estimation of the qualification and behavior of a contributor and aggregation of his answers in a crowdsourcing context,0.747399,"Crowdsourcing is the outsourcing of tasks to a crowd of contributors on a
dedicated platform. The crowd on these platforms is very diversified and
includes various profiles of contributors which generates data of uneven
quality. However, majority voting, which is the aggregating method commonly
used in platforms, gives equal weight to each contribution. To overcome this
problem, we propose a method, MONITOR, which estimates the contributor's
profile and aggregates the collected data by taking into account their possible
imperfections thanks to the theory of belief functions. To do so, MONITOR
starts by estimating the profile of the contributor through his qualification
for the task and his behavior.Crowdsourcing campaigns have been carried out to
collect the necessary data to test MONITOR on real data in order to compare it
to existing approaches. The results of the experiments show that thanks to the
use of the MONITOR method, we obtain a better rate of correct answer after
aggregation of the contributions compared to the majority voting. Our
contributions in this article are for the first time the proposal of a model
that takes into account both the qualification of the contributor and his
behavior in the estimation of his profile. For the second one, the weakening
and the aggregation of the answers according to the estimated profiles.",None,-1
c9671fcc-3a82-4398-8424-ab159cb41aaa,Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems,0.415197,"We study the multi-objective minimum weight base problem, an abstraction of
classical NP-hard combinatorial problems such as the multi-objective minimum
spanning tree problem. We prove some important properties of the convex hull of
the non-dominated front, such as its approximation quality and an upper bound
on the number of extreme points. Using these properties, we give the first
run-time analysis of the MOEA/D algorithm for this problem, an evolutionary
algorithm that effectively optimizes by decomposing the objectives into
single-objective components. We show that the MOEA/D, given an appropriate
decomposition setting, finds all extreme points within expected fixed-parameter
polynomial time in the oracle model, the parameter being the number of
objectives. Experiments are conducted on random bi-objective minimum spanning
tree instances, and the results agree with our theoretical findings.
Furthermore, compared with a previously studied evolutionary algorithm for the
problem GSEMO, MOEA/D finds all extreme points much faster across all
instances.",None,-1
a2b06362-ba52-4ae2-8f4e-f68e76d9b1dc,Decomposing Complex Queries for Tip-of-the-tongue Retrieval,0.418106,"When re-finding items, users who forget or are uncertain about identifying
details often rely on creative strategies for expressing their information
needs -- complex queries that describe content elements (e.g., book characters
or events), information beyond the document text (e.g., descriptions of book
covers), or personal context (e.g., when they read a book). This retrieval
setting, called tip of the tongue (TOT), is especially challenging for models
heavily reliant on lexical and semantic overlap between query and document
text. In this work, we introduce a simple yet effective framework for handling
such complex queries by decomposing the query into individual clues, routing
those as sub-queries to specialized retrievers, and ensembling the results.
This approach allows us to take advantage of off-the-shelf retrievers (e.g.,
CLIP for retrieving images of book covers) or incorporate retriever-specific
logic (e.g., date constraints). We show that our framework incorportating query
decompositions into retrievers can improve gold book recall up to 7% relative
again for Recall@5 on a new collection of 14,441 real-world query-book pairs
from an online community for resolving TOT inquiries.",None,-1
5d971ddc-ad69-490a-ac94-1b9deea5d1c1,RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model,1.0,"Leveraging the extensive training data from SA-1B, the Segment Anything Model
(SAM) demonstrates remarkable generalization and zero-shot capabilities.
However, as a category-agnostic instance segmentation method, SAM heavily
relies on prior manual guidance, including points, boxes, and coarse-grained
masks. Furthermore, its performance in remote sensing image segmentation tasks
remains largely unexplored and unproven. In this paper, we aim to develop an
automated instance segmentation approach for remote sensing images, based on
the foundational SAM model and incorporating semantic category information.
Drawing inspiration from prompt learning, we propose a method to learn the
generation of appropriate prompts for SAM. This enables SAM to produce
semantically discernible segmentation results for remote sensing images, a
concept we have termed RSPrompter. We also propose several ongoing derivatives
for instance segmentation tasks, drawing on recent advancements within the SAM
community, and compare their performance with RSPrompter. Extensive
experimental results, derived from the WHU building, NWPU VHR-10, and SSDD
datasets, validate the effectiveness of our proposed method. The code for our
method is publicly available at kychen.me/RSPrompter.",None,-1
10f1783b-3664-42c0-a865-921eaa5e1e26,Diversity-Measurable Anomaly Detection,0.802379,"Reconstruction-based anomaly detection models achieve their purpose by
suppressing the generalization ability for anomaly. However, diverse normal
patterns are consequently not well reconstructed as well. Although some efforts
have been made to alleviate this problem by modeling sample diversity, they
suffer from shortcut learning due to undesired transmission of abnormal
information. In this paper, to better handle the tradeoff problem, we propose
Diversity-Measurable Anomaly Detection (DMAD) framework to enhance
reconstruction diversity while avoid the undesired generalization on anomalies.
To this end, we design Pyramid Deformation Module (PDM), which models diverse
normals and measures the severity of anomaly by estimating multi-scale
deformation fields from reconstructed reference to original input. Integrated
with an information compression module, PDM essentially decouples deformation
from prototypical embedding and makes the final anomaly score more reliable.
Experimental results on both surveillance videos and industrial images
demonstrate the effectiveness of our method. In addition, DMAD works equally
well in front of contaminated data and anomaly-like normal samples.",None,-1
e788283b-285c-4112-aca6-d0095f90dd6c,FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning,0.891416,"In this paper, we propose FrameBERT, a RoBERTa-based model that can
explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor
detection. FrameBERT not only achieves better or comparable performance to the
state-of-the-art, but also is more explainable and interpretable compared to
existing models, attributing to its ability of accounting for external
knowledge of FrameNet.",None,-1
1cfcd07d-9942-4999-b632-3dead73b399b,Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations,0.234687,"In recent years, discriminative self-supervised methods have made significant
strides in advancing various visual tasks. The central idea of learning a data
encoder that is robust to data distortions/augmentations is straightforward yet
highly effective. Although many studies have demonstrated the empirical success
of various learning methods, the resulting learned representations can exhibit
instability and hinder downstream performance. In this study, we analyze
discriminative self-supervised methods from a causal perspective to explain
these unstable behaviors and propose solutions to overcome them. Our approach
draws inspiration from prior works that empirically demonstrate the ability of
discriminative self-supervised methods to demix ground truth causal sources to
some extent. Unlike previous work on causality-empowered representation
learning, we do not apply our solutions during the training process but rather
during the inference process to improve time efficiency. Through experiments on
both controlled image datasets and realistic image datasets, we show that our
proposed solutions, which involve tempering a linear transformation with
controlled synthetic data, are effective in addressing these issues.",None,-1
4f1de7db-9af9-4bdd-af20-077b4d2d01fe,AlbNER: A Corpus for Named Entity Recognition in Albanian,0.310662,"Scarcity of resources such as annotated text corpora for under-resourced
languages like Albanian is a serious impediment in computational linguistics
and natural language processing research. This paper presents AlbNER, a corpus
of 900 sentences with labeled named entities, collected from Albanian Wikipedia
articles. Preliminary results with BERT and RoBERTa variants fine-tuned and
tested with AlbNER data indicate that model size has slight impact on NER
performance, whereas language transfer has a significant one. AlbNER corpus and
these obtained results should serve as baselines for future experiments.",None,-1
28ff21b0-3dae-487f-a9cf-c8c1ef3a04df,Optimization-Based Eye Tracking using Deflectometric Information,0.766444,"Eye tracking is an important tool with a wide range of applications in
Virtual, Augmented, and Mixed Reality (VR/AR/MR) technologies. State-of-the-art
eye tracking methods are either reflection-based and track reflections of
sparse point light sources, or image-based and exploit 2D features of the
acquired eye image. In this work, we attempt to significantly improve
reflection-based methods by utilizing pixel-dense deflectometric surface
measurements in combination with optimization-based inverse rendering
algorithms. Utilizing the known geometry of our deflectometric setup, we
develop a differentiable rendering pipeline based on PyTorch3D that simulates a
virtual eye under screen illumination. Eventually, we exploit the
image-screen-correspondence information from the captured measurements to find
the eye's rotation, translation, and shape parameters with our renderer via
gradient descent. In general, our method does not require a specific pattern
and can work with ordinary video frames of the main VR/AR/MR screen itself. We
demonstrate real-world experiments with evaluated mean relative gaze errors
below 0.45 degrees at a precision better than 0.11 degrees. Moreover, we show
an improvement of 6X over a representative reflection-based state-of-the-art
method in simulation.",None,-1
1d3a267e-45f9-4692-ab7f-6e1106002638,Robust Wind Turbine Blade Segmentation from RGB Images in the Wild,0.780248,"With the relentless growth of the wind industry, there is an imperious need
to design automatic data-driven solutions for wind turbine maintenance. As
structural health monitoring mainly relies on visual inspections, the first
stage in any automatic solution is to identify the blade region on the image.
Thus, we propose a novel segmentation algorithm that strengthens the U-Net
results by a tailored loss, which pools the focal loss with a contiguity
regularization term. To attain top performing results, a set of additional
steps are proposed to ensure a reliable, generic, robust and efficient
algorithm. First, we leverage our prior knowledge on the images by filling the
holes enclosed by temporarily-classified blade pixels and by the image
boundaries. Subsequently, the mislead classified pixels are successfully
amended by training an on-the-fly random forest. Our algorithm demonstrates its
effectiveness reaching a non-trivial 97.39% of accuracy.",None,-1
9b555769-aaf6-4456-8d72-43d5f6ec83d9,HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation,0.628845,"In this paper, we study Text-to-3D content generation leveraging 2D diffusion
priors to enhance the quality and detail of the generated 3D models. Recent
progress (Magic3D) in text-to-3D has shown that employing high-resolution
(e.g., 512 x 512) renderings can lead to the production of high-quality 3D
models using latent diffusion priors. To enable rendering at even higher
resolutions, which has the potential to further augment the quality and detail
of the models, we propose a novel approach that combines multiple noise
estimation processes with a pretrained 2D diffusion prior. Distinct from the
Bar-Tal et al.s' study which binds multiple denoised results to generate images
from texts, our approach integrates the computation of scoring distillation
losses such as SDS loss and VSD loss which are essential techniques for the 3D
content generation with 2D diffusion priors. We experimentally evaluated the
proposed approach. The results show that the proposed approach can generate
high-quality details compared to the baselines.",None,-1
17725416-2576-42d6-84f6-be3f559bf958,A Systematic Analysis of Vocabulary and BPE Settings for Optimal Fine-tuning of NMT: A Case Study of In-domain Translation,0.575362,"The effectiveness of Neural Machine Translation (NMT) models largely depends
on the vocabulary used at training; small vocabularies can lead to
out-of-vocabulary problems -- large ones, to memory issues. Subword (SW)
tokenization has been successfully employed to mitigate these issues. The
choice of vocabulary and SW tokenization has a significant impact on both
training and fine-tuning an NMT model. Fine-tuning is a common practice in
optimizing an MT model with respect to new data. However, new data potentially
introduces new words (or tokens), which, if not taken into consideration, may
lead to suboptimal performance. In addition, the distribution of tokens in the
new data can differ from the distribution of the original data. As such, the
original SW tokenization model could be less suitable for the new data. Through
a systematic empirical evaluation, in this work we compare different strategies
for SW tokenization and vocabulary generation with the ultimate goal to uncover
an optimal setting for fine-tuning a domain-specific model. Furthermore, we
developed several (in-domain) models, the best of which achieves 6 BLEU points
improvement over the baseline.",None,-1
fc16488a-9d8d-41e8-a6e2-3969e5c0303a,CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models,0.611815,"We propose CHiLL (Crafting High-Level Latents), an approach for
natural-language specification of features for linear models. CHiLL prompts
LLMs with expert-crafted queries to generate interpretable features from health
records. The resulting noisy labels are then used to train a simple linear
classifier. Generating features based on queries to an LLM can empower
physicians to use their domain expertise to craft features that are clinically
meaningful for a downstream task of interest, without having to manually
extract these from raw EHR. We are motivated by a real-world risk prediction
task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and
standard predictive tasks (e.g., 30-day readmission) to evaluate this approach.
We find that linear models using automatically extracted features are
comparably performant to models using reference features, and provide greater
interpretability than linear models using ""Bag-of-Words"" features. We verify
that learned feature weights align well with clinical expectations.",None,-1
839a269c-9dde-4d46-93f0-a47e1800433c,Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search,0.733832,"Text-based Person Search (TPS), is targeted on retrieving pedestrians to
match text descriptions instead of query images. Recent Vision-Language
Pre-training (VLP) models can bring transferable knowledge to downstream TPS
tasks, resulting in more efficient performance gains. However, existing TPS
methods improved by VLP only utilize pre-trained visual encoders, neglecting
the corresponding textual representation and breaking the significant modality
alignment learned from large-scale pre-training. In this paper, we explore the
full utilization of textual potential from VLP in TPS tasks. We build on the
proposed VLP-TPS baseline model, which is the first TPS model with both
pre-trained modalities. We propose the Multi-Integrity Description Constraints
(MIDC) to enhance the robustness of the textual modality by incorporating
different components of fine-grained corpus during training. Inspired by the
prompt approach for zero-shot classification with VLP models, we propose the
Dynamic Attribute Prompt (DAP) to provide a unified corpus of fine-grained
attributes as language hints for the image modality. Extensive experiments show
that our proposed TPS framework achieves state-of-the-art performance,
exceeding the previous best method by a margin.",None,-1
d247c2c5-c135-4623-a0be-acf1ac4ab7c9,Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method,0.541608,"Theorem proving is a fundamental task in mathematics. With the advent of
large language models (LLMs) and interactive theorem provers (ITPs) like Lean,
there has been growing interest in integrating LLMs and ITPs to automate
theorem proving. In this approach, the LLM generates proof steps (tactics), and
the ITP checks the applicability of the tactics at the current goal. The two
systems work together to complete the proof. In this paper, we introduce
DS-Prover, a novel dynamic sampling method for theorem proving. This method
dynamically determines the number of tactics to apply to expand the current
goal, taking into account the remaining time compared to the total allocated
time for proving a theorem. This makes the proof search process more efficient
by adjusting the balance between exploration and exploitation as time passes.
We also augment the training dataset by decomposing simplification and rewrite
tactics with multiple premises into tactics with single premises. This gives
the model more examples to learn from and helps it to predict the tactics with
premises more accurately. We perform our experiments using the Mathlib dataset
of the Lean theorem prover and report the performance on two standard datasets,
MiniF2F and ProofNet. Our methods achieve significant performance gains on both
datasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the
ProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the
best-reported Pass@1 of 29.6% using Lean.",None,-1
107783fb-1ced-4244-80af-3b011f32c507,Don't Lie to Me: Avoiding Malicious Explanations with STEALTH,0.113649,"STEALTH is a method for using some AI-generated model, without suffering from
malicious attacks (i.e. lying) or associated unfairness issues. After
recursively bi-clustering the data, STEALTH system asks the AI model a limited
number of queries about class labels. STEALTH asks so few queries (1 per data
cluster) that malicious algorithms (a) cannot detect its operation, nor (b)
know when to lie.",None,-1
5ffb6005-0194-46f9-8387-eb92fa897003,Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes,0.946003,"Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0",None,-1
e09eb647-2a83-4db2-9ee0-9a32dd6a451d,ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing,0.998579,"This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing.",None,-1
479349b3-ce5c-4c2f-a298-c79d6285873d,NeuroX Library for Neuron Analysis of Deep NLP Models,0.741346,"Neuron analysis provides insights into how knowledge is structured in
representations and discovers the role of neurons in the network. In addition
to developing an understanding of our models, neuron analysis enables various
applications such as debiasing, domain adaptation and architectural search. We
present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis
of natural language processing models. It implements various interpretation
methods under a unified API, and provides a framework for data processing and
evaluation, thus making it easier for researchers and practitioners to perform
neuron analysis. The Python toolkit is available at
https://www.github.com/fdalvi/NeuroX. Demo Video available at
https://youtu.be/mLhs2YMx4u8.",None,-1
882a24ea-8b67-4c96-bcc7-207b9e013d7c,AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR Parsing,0.598511,"In this paper, we examine the current state-of-the-art in AMR parsing, which
relies on ensemble strategies by merging multiple graph predictions. Our
analysis reveals that the present models often violate AMR structural
constraints. To address this issue, we develop a validation method, and show
how ensemble models can exploit SMATCH metric weaknesses to obtain higher
scores, but sometimes result in corrupted graphs. Additionally, we highlight
the demanding need to compute the SMATCH score among all possible predictions.
To overcome these challenges, we propose two novel ensemble strategies based on
Transformer models, improving robustness to structural constraints, while also
reducing the computational time. Our methods provide new insights for enhancing
AMR parsers and metrics. Our code is available at
\href{https://www.github.com/babelscape/AMRs-Assemble}{github.com/babelscape/AMRs-Assemble}.",None,-1
67c97e1c-dcca-47db-8eb3-4b9b42289ab0,SegGPT: Segmenting Everything In Context,0.982141,"We present SegGPT, a generalist model for segmenting everything in context.
We unify various segmentation tasks into a generalist in-context learning
framework that accommodates different kinds of segmentation data by
transforming them into the same format of images. The training of SegGPT is
formulated as an in-context coloring problem with random color mapping for each
data sample. The objective is to accomplish diverse tasks according to the
context, rather than relying on specific colors. After training, SegGPT can
perform arbitrary segmentation tasks in images or videos via in-context
inference, such as object instance, stuff, part, contour, and text. SegGPT is
evaluated on a broad range of tasks, including few-shot semantic segmentation,
video object segmentation, semantic segmentation, and panoptic segmentation.
Our results show strong capabilities in segmenting in-domain and out-of-domain
targets, either qualitatively or quantitatively.",None,-1
474255ad-daf9-4ab9-84d8-72a26ec31b8e,A Huber Loss Minimization Approach to Byzantine Robust Federated Learning,0.284568,"Federated learning systems are susceptible to adversarial attacks. To combat
this, we introduce a novel aggregator based on Huber loss minimization, and
provide a comprehensive theoretical analysis. Under independent and identically
distributed (i.i.d) assumption, our approach has several advantages compared to
existing methods. Firstly, it has optimal dependence on $\epsilon$, which
stands for the ratio of attacked clients. Secondly, our approach does not need
precise knowledge of $\epsilon$. Thirdly, it allows different clients to have
unequal data sizes. We then broaden our analysis to include non-i.i.d data,
such that clients have slightly different distributions.",None,-1
afef3cd8-661e-4710-91c2-e22bc8d8770a,D2NT: A High-Performing Depth-to-Normal Translator,0.396987,"Surface normal holds significant importance in visual environmental
perception, serving as a source of rich geometric information. However, the
state-of-the-art (SoTA) surface normal estimators (SNEs) generally suffer from
an unsatisfactory trade-off between efficiency and accuracy. To resolve this
dilemma, this paper first presents a superfast depth-to-normal translator
(D2NT), which can directly translate depth images into surface normal maps
without calculating 3D coordinates. We then propose a discontinuity-aware
gradient (DAG) filter, which adaptively generates gradient convolution kernels
to improve depth gradient estimation. Finally, we propose a surface normal
refinement module that can easily be integrated into any depth-to-normal SNEs,
substantially improving the surface normal estimation accuracy. Our proposed
algorithm demonstrates the best accuracy among all other existing real-time
SNEs and achieves the SoTA trade-off between efficiency and accuracy.",None,-1
bd606608-4810-4c42-8caf-a68ccfde1706,Learning Expressive Prompting With Residuals for Vision Transformers,0.303767,"Prompt learning is an efficient approach to adapt transformers by inserting
learnable set of parameters into the input and intermediate representations of
a pre-trained model. In this work, we present Expressive Prompts with Residuals
(EXPRES) which modifies the prompt learning paradigm specifically for effective
adaptation of vision transformers (ViT). Out method constructs downstream
representations via learnable ``output'' tokens, that are akin to the learned
class tokens of the ViT. Further for better steering of the downstream
representation processed by the frozen transformer, we introduce residual
learnable tokens that are added to the output of various computations. We apply
EXPRES for image classification, few shot learning, and semantic segmentation,
and show our method is capable of achieving state of the art prompt tuning on
3/3 categories of the VTAB benchmark. In addition to strong performance, we
observe that our approach is an order of magnitude more prompt efficient than
existing visual prompting baselines. We analytically show the computational
benefits of our approach over weight space adaptation techniques like
finetuning. Lastly we systematically corroborate the architectural design of
our method via a series of ablation experiments.",None,-1
edef9ae1-2ef1-4f1b-8d71-9e3b6dbe4d51,Deep Unsupervised Learning Using Spike-Timing-Dependent Plasticity,0.648952,"Spike-Timing-Dependent Plasticity (STDP) is an unsupervised learning
mechanism for Spiking Neural Networks (SNNs) that has received significant
attention from the neuromorphic hardware community. However, scaling such local
learning techniques to deeper networks and large-scale tasks has remained
elusive. In this work, we investigate a Deep-STDP framework where a rate-based
convolutional network, that can be deployed in a neuromorphic setting, is
trained in tandem with pseudo-labels generated by the STDP clustering process
on the network outputs. We achieve $24.56\%$ higher accuracy and $3.5\times$
faster convergence speed at iso-accuracy on a 10-class subset of the Tiny
ImageNet dataset in contrast to a $k$-means clustering approach.",None,-1
7663c54c-d45a-4ca4-b784-d9a121349cb3,Joint Skeletal and Semantic Embedding Loss for Micro-gesture Classification,0.649246,"In this paper, we briefly introduce the solution of our team HFUT-VUT for the
Micros-gesture Classification in the MiGA challenge at IJCAI 2023. The
micro-gesture classification task aims at recognizing the action category of a
given video based on the skeleton data. For this task, we propose a
3D-CNNs-based micro-gesture recognition network, which incorporates a skeletal
and semantic embedding loss to improve action classification performance.
Finally, we rank 1st in the Micro-gesture Classification Challenge, surpassing
the second-place team in terms of Top-1 accuracy by 1.10%.",None,-1
219b365f-1d60-40a4-8916-2d25891eecdf,Incremental Generalized Category Discovery,0.787969,"We explore the problem of Incremental Generalized Category Discovery (IGCD).
This is a challenging category incremental learning setting where the goal is
to develop models that can correctly categorize images from previously seen
categories, in addition to discovering novel ones. Learning is performed over a
series of time steps where the model obtains new labeled and unlabeled data,
and discards old data, at each iteration. The difficulty of the problem is
compounded in our generalized setting as the unlabeled data can contain images
from categories that may or may not have been observed before. We present a new
method for IGCD which combines non-parametric categorization with efficient
image sampling to mitigate catastrophic forgetting. To quantify performance, we
propose a new benchmark dataset named iNatIGCD that is motivated by a
real-world fine-grained visual categorization task. In our experiments we
outperform existing related methods",None,-1
4f269702-1795-4605-adb9-23829c8cde9d,Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification,0.565402,"This article aims to classify diabetic retinopathy (DR) disease into five
different classes using an ensemble approach based on two popular pre-trained
convolutional neural networks: VGG16 and Inception V3. The proposed model aims
to leverage the strengths of the two individual nets to enhance the
classification performance for diabetic retinopathy. The ensemble model
architecture involves freezing a portion of the layers in each pre-trained
model to utilize their learned representations effectively. Global average
pooling layers are added to transform the output feature maps into fixed-length
vectors. These vectors are then concatenated to form a consolidated
representation of the input image. The ensemble model is trained using a
dataset of diabetic retinopathy images (APTOS), divided into training and
validation sets. During the training process, the model learns to classify the
retinal images into the corresponding diabetic retinopathy classes.
Experimental results on the test set demonstrate the efficacy of the proposed
ensemble model for DR classification achieving an accuracy of 96.4%.",None,-1
a6e2995d-339f-4b07-9e45-82155e4380cb,Accelerating exploration and representation learning with offline pre-training,0.11198,"Sequential decision-making agents struggle with long horizon tasks, since
solving them requires multi-step reasoning. Most reinforcement learning (RL)
algorithms address this challenge by improved credit assignment, introducing
memory capability, altering the agent's intrinsic motivation (i.e. exploration)
or its worldview (i.e. knowledge representation). Many of these components
could be learned from offline data. In this work, we follow the hypothesis that
exploration and representation learning can be improved by separately learning
two different models from a single offline dataset. We show that learning a
state representation using noise-contrastive estimation and a model of
auxiliary reward separately from a single collection of human demonstrations
can significantly improve the sample efficiency on the challenging NetHack
benchmark. We also ablate various components of our experimental setting and
highlight crucial insights.",None,-1
36d01dd3-646e-4abf-8884-c13cdbacf708,HIORE: Leveraging High-order Interactions for Unified Entity Relation Extraction,0.343354,"Entity relation extraction consists of two sub-tasks: entity recognition and
relation extraction. Existing methods either tackle these two tasks separately
or unify them with word-by-word interactions. In this paper, we propose HIORE,
a new method for unified entity relation extraction. The key insight is to
leverage the high-order interactions, i.e., the complex association among word
pairs, which contains richer information than the first-order word-by-word
interactions. For this purpose, we first devise a W-shape DNN (WNet) to capture
coarse-level high-order connections. Then, we build a heuristic high-order
graph and further calibrate the representations with a graph neural network
(GNN). Experiments on three benchmarks (ACE04, ACE05, SciERC) show that HIORE
achieves the state-of-the-art performance on relation extraction and an
improvement of 1.1~1.8 F1 points over the prior best unified model.",None,-1
36c0fb15-90b5-4e82-a7d6-66c8a39a0050,Inspecting and Editing Knowledge Representations in Language Models,0.977738,"Neural language models (LMs) represent facts about the world described by
text. Sometimes these facts derive from training data (in most LMs, a
representation of the word ""banana"" encodes the fact that bananas are fruits).
Sometimes facts derive from input text itself (a representation of the sentence
""I poured out the bottle"" encodes the fact that the bottle became empty). We
describe REMEDI, a method for learning to map statements in natural language to
fact encodings in an LM's internal representation system. REMEDI encodings can
be used as knowledge editors: when added to LM hidden representations, they
modify downstream generation to be consistent with new facts. REMEDI encodings
may also be used as probes: when compared to LM representations, they reveal
which properties LMs already attribute to mentioned entities, in some cases
making it possible to predict when LMs will generate outputs that conflict with
background knowledge or input text. REMEDI thus links work on probing,
prompting, and LM editing, and offers steps toward general tools for
fine-grained inspection and control of knowledge in LMs.",None,-1
40cded90-19cf-44e5-90be-bbcd0e7d3870,Towards Cognitive Bots: Architectural Research Challenges,0.194119,"Software bots operating in multiple virtual digital platforms must understand
the platforms' affordances and behave like human users. Platform affordances or
features differ from one application platform to another or through a life
cycle, requiring such bots to be adaptable. Moreover, bots in such platforms
could cooperate with humans or other software agents for work or to learn
specific behavior patterns. However, present-day bots, particularly chatbots,
other than language processing and prediction, are far from reaching a human
user's behavior level within complex business information systems. They lack
the cognitive capabilities to sense and act in such virtual environments,
rendering their development a challenge to artificial general intelligence
research. In this study, we problematize and investigate assumptions in
conceptualizing software bot architecture by directing attention to significant
architectural research challenges in developing cognitive bots endowed with
complex behavior for operation on information systems. As an outlook, we
propose alternate architectural assumptions to consider in future bot design
and bot development frameworks.",None,-1
2370e40c-125b-4db0-9ed3-d093727a6145,Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction,0.546302,"In this study, we investigated the potential of GPT-3 for the anti-cancer
drug sensitivity prediction task using structured pharmacogenomics data across
five tissue types and evaluated its performance with zero-shot prompting and
fine-tuning paradigms. The drug's smile representation and cell line's genomic
mutation features were predictive of the drug response. The results from this
study have the potential to pave the way for designing more efficient treatment
protocols in precision oncology.",None,-1
c664c994-cd7b-46d2-a966-079f4121c793,Weighted First Order Model Counting with Directed Acyclic Graph Axioms,0.142891,"Statistical Relational Learning (SRL) integrates First-Order Logic (FOL) and
probability theory for learning and inference over relational data.
Probabilistic inference and learning in many SRL models can be reduced to
Weighted First Order Model Counting (WFOMC). However, WFOMC is known to be
intractable ($\mathrm{\#P_1-}$ complete). Hence, logical fragments that admit
polynomial time WFOMC are of significant interest. Such fragments are called
domain liftable. Recent line of works have shown the two-variable fragment of
FOL, extended with counting quantifiers ($\mathrm{C^2}$) to be domain-liftable.
However, many properties of real-world data can not be modelled in
$\mathrm{C^2}$. In fact many ubiquitous properties of real-world data are
inexressible in FOL. Acyclicity is one such property, found in citation
networks, genealogy data, temporal data e.t.c. In this paper we aim to address
this problem by investigating the domain liftability of directed acyclicity
constraints. We show that the fragment $\mathrm{C^2}$ with a Directed Acyclic
Graph (DAG) axiom, i.e., a predicate in the language is axiomatized to
represent a DAG, is domain-liftable. We present a method based on principle of
inclusion-exclusion for WFOMC of $\mathrm{C^2}$ formulas extended with DAG
axioms.",None,-1
45dc8ba3-ed91-46aa-b76f-ea63c7607c3d,"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",0.994193,"Conversational systems based on Large Language Models (LLMs), such as
ChatGPT, show exceptional proficiency in context understanding and response
generation. However, despite their impressive capabilities, they still possess
limitations, such as providing randomly-guessed answers to ambiguous queries or
failing to refuse users' requests, both of which are considered aspects of a
conversational agent's proactivity. This raises the question of whether
LLM-based conversational systems are equipped to handle proactive dialogue
problems. In this work, we conduct a comprehensive analysis of LLM-based
conversational systems, specifically focusing on three aspects of proactive
dialogue systems: clarification, target-guided, and non-collaborative
dialogues. To trigger the proactivity of LLMs, we propose the Proactive
Chain-of-Thought prompting scheme, which augments LLMs with the goal planning
capability over descriptive reasoning chains. Empirical findings are discussed
to promote future studies on LLM-based proactive dialogue systems.",None,-1
7ffc5e8d-aed2-4b7c-b52c-96997de4e139,Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models,0.286181,"Clinical natural language processing requires methods that can address
domain-specific challenges, such as complex medical terminology and clinical
contexts. Recently, large language models (LLMs) have shown promise in this
domain. Yet, their direct deployment can lead to privacy issues and are
constrained by resources. To address this challenge, we delve into synthetic
clinical text generation using LLMs for clinical NLP tasks. We propose an
innovative, resource-efficient approach, ClinGen, which infuses knowledge into
the process. Our model involves clinical knowledge extraction and
context-informed LLM prompting. Both clinical topics and writing styles are
drawn from external domain-specific knowledge graphs and LLMs to guide data
generation. Our extensive empirical study across 7 clinical NLP tasks and 16
datasets reveals that ClinGen consistently enhances performance across various
tasks, effectively aligning the distribution of real datasets and significantly
enriching the diversity of generated training instances. We will publish our
code and all the generated data in \url{https://github.com/ritaranx/ClinGen}.",None,-1
33915ddc-36a4-42fc-91ae-12cd27899556,MedChatZH: a Better Medical Adviser Learns from Better Instructions,0.753541,"Generative large language models (LLMs) have shown great success in various
applications, including question-answering (QA) and dialogue systems. However,
in specialized domains like traditional Chinese medical QA, these models may
perform unsatisfactorily without fine-tuning on domain-specific datasets. To
address this, we introduce MedChatZH, a dialogue model designed specifically
for traditional Chinese medical QA. Our model is pre-trained on Chinese
traditional medical books and fine-tuned with a carefully curated medical
instruction dataset. It outperforms several solid baselines on a real-world
medical dialogue dataset. We release our model, code, and dataset on
https://github.com/tyang816/MedChatZH to facilitate further research in the
domain of traditional Chinese medicine and LLMs.",None,-1
671938ec-677e-4f77-ba36-d9606e4d10b1,Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models,0.275597,"Federated learning (FL) is an emerging technique that trains massive and
geographically distributed edge data while maintaining privacy. However, FL has
inherent challenges in terms of fairness and computational efficiency due to
the rising heterogeneity of edges, and thus usually results in sub-optimal
performance in recent state-of-the-art (SOTA) solutions. In this paper, we
propose a Customized Federated Learning (CFL) system to eliminate FL
heterogeneity from multiple dimensions. Specifically, CFL tailors personalized
models from the specially designed global model for each client jointly guided
by an online trained model-search helper and a novel aggregation algorithm.
Extensive experiments demonstrate that CFL has full-stack advantages for both
FL training and edge reasoning and significantly improves the SOTA performance
w.r.t. model accuracy (up to 7.2% in the non-heterogeneous environment and up
to 21.8% in the heterogeneous environment), efficiency, and FL fairness.",None,-1
a0e93122-2c05-4e8c-b8a2-b4d72f885c95,Assessing the Interpretability of Programmatic Policies with Large Language Models,0.329396,"Although the synthesis of programs encoding policies often carries the
promise of interpretability, systematic evaluations were never performed to
assess the interpretability of these policies, likely because of the complexity
of such an evaluation. In this paper, we introduce a novel metric that uses
large-language models (LLM) to assess the interpretability of programmatic
policies. For our metric, an LLM is given both a program and a description of
its associated programming language. The LLM then formulates a natural language
explanation of the program. This explanation is subsequently fed into a second
LLM, which tries to reconstruct the program from the natural-language
explanation. Our metric then measures the behavioral similarity between the
reconstructed program and the original. We validate our approach with
synthesized and human-crafted programmatic policies for playing a real-time
strategy game, comparing the interpretability scores of these programmatic
policies to obfuscated versions of the same programs. Our LLM-based
interpretability score consistently ranks less interpretable programs lower and
more interpretable ones higher. These findings suggest that our metric could
serve as a reliable and inexpensive tool for evaluating the interpretability of
programmatic policies.",None,-1
54ca2cae-70f0-48f7-91ef-88205789ea38,Morphological Inflection: A Reality Check,0.265067,"Morphological inflection is a popular task in sub-word NLP with both
practical and cognitive applications. For years now, state-of-the-art systems
have reported high, but also highly variable, performance across data sets and
languages. We investigate the causes of this high performance and high
variability; we find several aspects of data set creation and evaluation which
systematically inflate performance and obfuscate differences between languages.
To improve generalizability and reliability of results, we propose new data
sampling and evaluation strategies that better reflect likely use-cases. Using
these new strategies, we make new observations on the generalization abilities
of current inflection systems.",None,-1
b456ed17-e4a4-4f77-bfe0-9361e76147f9,ViT2EEG: Leveraging Hybrid Pretrained Vision Transformers for EEG Data,0.53499,"In this study, we demonstrate the application of a hybrid Vision Transformer
(ViT) model, pretrained on ImageNet, on an electroencephalogram (EEG)
regression task. Despite being originally trained for image classification
tasks, when fine-tuned on EEG data, this model shows a notable increase in
performance compared to other models, including an identical architecture ViT
trained without the ImageNet weights. This discovery challenges the traditional
understanding of model generalization, suggesting that Transformer models
pretrained on seemingly unrelated image data can provide valuable priors for
EEG regression tasks with an appropriate fine-tuning pipeline.
  The success of this approach suggests that the features extracted by ViT
models in the context of visual tasks can be readily transformed for the
purpose of EEG predictive modeling. We recommend utilizing this methodology not
only in neuroscience and related fields, but generally for any task where data
collection is limited by practical, financial, or ethical constraints. Our
results illuminate the potential of pretrained models on tasks that are clearly
distinct from their original purpose.",None,-1
f93d0405-41cc-4aba-8101-7e5f893b0164,AutoKary2022: A Large-Scale Densely Annotated Dataset for Chromosome Instance Segmentation,0.303514,"Automated chromosome instance segmentation from metaphase cell microscopic
images is critical for the diagnosis of chromosomal disorders (i.e., karyotype
analysis). However, it is still a challenging task due to lacking of densely
annotated datasets and the complicated morphologies of chromosomes, e.g., dense
distribution, arbitrary orientations, and wide range of lengths. To facilitate
the development of this area, we take a big step forward and manually construct
a large-scale densely annotated dataset named AutoKary2022, which contains over
27,000 chromosome instances in 612 microscopic images from 50 patients.
Specifically, each instance is annotated with a polygonal mask and a class
label to assist in precise chromosome detection and segmentation. On top of it,
we systematically investigate representative methods on this dataset and obtain
a number of interesting findings, which helps us have a deeper understanding of
the fundamental problems in chromosome instance segmentation. We hope this
dataset could advance research towards medical understanding. The dataset can
be available at:
https://github.com/wangjuncongyu/chromosome-instance-segmentation-dataset.",None,-1
ba48f096-ae45-449f-81ea-deff0884cdeb,Chain-of-Verification Reduces Hallucination in Large Language Models,0.502733,"Generation of plausible yet incorrect factual information, termed
hallucination, is an unsolved issue in large language models. We study the
ability of language models to deliberate on the responses they give in order to
correct their mistakes. We develop the Chain-of-Verification (CoVe) method
whereby the model first (i) drafts an initial response; then (ii) plans
verification questions to fact-check its draft; (iii) answers those questions
independently so the answers are not biased by other responses; and (iv)
generates its final verified response. In experiments, we show CoVe decreases
hallucinations across a variety of tasks, from list-based questions from
Wikidata, closed book MultiSpanQA and longform text generation.",None,-1
6afa39f3-032d-4d1b-a88e-5561485dc5c1,SETI: Systematicity Evaluation of Textual Inference,0.052131,"We propose SETI (Systematicity Evaluation of Textual Inference), a novel and
comprehensive benchmark designed for evaluating pre-trained language models
(PLMs) for their systematicity capabilities in the domain of textual inference.
Specifically, SETI offers three different NLI tasks and corresponding datasets
to evaluate various types of systematicity in reasoning processes. In order to
solve these tasks, models are required to perform compositional inference based
on known primitive constituents. We conduct experiments of SETI on six widely
used PLMs. Results show that various PLMs are able to solve unseen
compositional inferences when having encountered the knowledge of how to
combine primitives, with good performance. However, they are considerably
limited when this knowledge is unknown to the model (40-100% points decrease).
Furthermore, we find that PLMs can improve drastically once exposed to crucial
compositional knowledge in minimalistic shots. These findings position SETI as
the first benchmark for measuring the future progress of PLMs in achieving
systematicity generalization in the textual inference.",None,-1
55c1a611-0e71-4b8d-ab0e-818f67b82586,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,0.97303,"Neural radiance fields enable state-of-the-art photorealistic view synthesis.
However, existing radiance field representations are either too
compute-intensive for real-time rendering or require too much memory to scale
to large scenes. We present a Memory-Efficient Radiance Field (MERF)
representation that achieves real-time rendering of large-scale scenes in a
browser. MERF reduces the memory consumption of prior sparse volumetric
radiance fields using a combination of a sparse feature grid and
high-resolution 2D feature planes. To support large-scale unbounded scenes, we
introduce a novel contraction function that maps scene coordinates into a
bounded volume while still allowing for efficient ray-box intersection. We
design a lossless procedure for baking the parameterization used during
training into a model that achieves real-time rendering while still preserving
the photorealistic view synthesis quality of a volumetric radiance field.",None,-1
fb0bdbe2-95c7-4428-89cf-b317fb432f93,An Empirical Analysis of Range for 3D Object Detection,0.755755,"LiDAR-based 3D detection plays a vital role in autonomous navigation.
Surprisingly, although autonomous vehicles (AVs) must detect both near-field
objects (for collision avoidance) and far-field objects (for longer-term
planning), contemporary benchmarks focus only on near-field 3D detection.
However, AVs must detect far-field objects for safe navigation. In this paper,
we present an empirical analysis of far-field 3D detection using the long-range
detection dataset Argoverse 2.0 to better understand the problem, and share the
following insight: near-field LiDAR measurements are dense and optimally
encoded by small voxels, while far-field measurements are sparse and are better
encoded with large voxels. We exploit this observation to build a collection of
range experts tuned for near-vs-far field detection, and propose simple
techniques to efficiently ensemble models for long-range detection that improve
efficiency by 33% and boost accuracy by 3.2% CDS.",None,-1
77ac5eea-9e69-4dfe-9242-89677c4641c2,HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird's Eye View,0.514041,"Vision-based Bird's Eye View (BEV) representation is an emerging perception
formulation for autonomous driving. The core challenge is to construct BEV
space with multi-camera features, which is a one-to-many ill-posed problem.
Diving into all previous BEV representation generation methods, we found that
most of them fall into two types: modeling depths in image views or modeling
heights in the BEV space, mostly in an implicit way. In this work, we propose
to explicitly model heights in the BEV space, which needs no extra data like
LiDAR and can fit arbitrary camera rigs and types compared to modeling depths.
Theoretically, we give proof of the equivalence between height-based methods
and depth-based methods. Considering the equivalence and some advantages of
modeling heights, we propose HeightFormer, which models heights and
uncertainties in a self-recursive way. Without any extra data, the proposed
HeightFormer could estimate heights in BEV accurately. Benchmark results show
that the performance of HeightFormer achieves SOTA compared with those
camera-only methods.",None,-1
d54eebfb-0275-435e-8b8b-acb42bc598ee,GART: Gaussian Articulated Template Models,0.682926,"We introduce Gaussian Articulated Template Model GART, an explicit,
efficient, and expressive representation for non-rigid articulated subject
capturing and rendering from monocular videos. GART utilizes a mixture of
moving 3D Gaussians to explicitly approximate a deformable subject's geometry
and appearance. It takes advantage of a categorical template model prior (SMPL,
SMAL, etc.) with learnable forward skinning while further generalizing to more
complex non-rigid deformations with novel latent bones. GART can be
reconstructed via differentiable rendering from monocular videos in seconds or
minutes and rendered in novel poses faster than 150fps.",None,-1
7d3bf0e1-bdce-49da-9c26-2a94fbd34bfa,Code Models are Zero-shot Precondition Reasoners,0.452807,"One of the fundamental skills required for an agent acting in an environment
to complete tasks is the ability to understand what actions are plausible at
any given point. This work explores a novel use of code representations to
reason about action preconditions for sequential decision making tasks. Code
representations offer the flexibility to model procedural activities and
associated constraints as well as the ability to execute and verify constraint
satisfaction. Leveraging code representations, we extract action preconditions
from demonstration trajectories in a zero-shot manner using pre-trained code
models. Given these extracted preconditions, we propose a precondition-aware
action sampling strategy that ensures actions predicted by a policy are
consistent with preconditions. We demonstrate that the proposed approach
enhances the performance of few-shot policy learning approaches across
task-oriented dialog and embodied textworld benchmarks.",None,-1
4c91ffd5-0b32-41f1-934e-e1537be7d8cb,MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation,0.999803,"We release MiDaS v3.1 for monocular depth estimation, offering a variety of
new models based on different encoder backbones. This release is motivated by
the success of transformers in computer vision, with a large variety of
pretrained vision transformers now available. We explore how using the most
promising vision transformers as image encoders impacts depth estimation
quality and runtime of the MiDaS architecture. Our investigation also includes
recent convolutional approaches that achieve comparable quality to vision
transformers in image classification tasks. While the previous release MiDaS
v3.0 solely leverages the vanilla vision transformer ViT, MiDaS v3.1 offers
additional models based on BEiT, Swin, SwinV2, Next-ViT and LeViT. These models
offer different performance-runtime tradeoffs. The best model improves the
depth estimation quality by 28% while efficient models enable downstream tasks
requiring high frame rates. We also describe the general process for
integrating new backbones. A video summarizing the work can be found at
https://youtu.be/UjaeNNFf9sE and the code is available at
https://github.com/isl-org/MiDaS.",None,-1
1823c64c-36ac-45b9-b22e-66524fd57687,Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives,0.548665,"Cognitive diagnosis seeks to estimate the cognitive states of students by
exploring their logged practice quiz data. It plays a pivotal role in
personalized learning guidance within intelligent education systems. In this
paper, we focus on an important, practical, yet often underexplored task:
domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the
absence of student practice logs in newly launched domains. Recent cross-domain
diagnostic models have been demonstrated to be a promising strategy for DZCD.
These methods primarily focus on how to transfer student states across domains.
However, they might inadvertently incorporate non-transferable information into
student representations, thereby limiting the efficacy of knowledge transfer.
To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive
diagnosis framework via one batch of early-bird students towards three
diagnostic objectives. Our approach initiates with pre-training a diagnosis
model with dual regularizers, which decouples student states into domain-shared
and domain-specific parts. The shared cognitive signals can be transferred to
the target domain, enriching the cognitive priors for the new domain, which
ensures the cognitive state propagation objective. Subsequently, we devise a
strategy to generate simulated practice logs for cold-start students through
analyzing the behavioral patterns from early-bird students, fulfilling the
domain-adaption goal. Consequently, we refine the cognitive states of
cold-start students as diagnostic outcomes via virtual data, aligning with the
diagnosis-oriented goal. Finally, extensive experiments on six real-world
datasets highlight the efficacy of our model for DZCD and its practical
application in question recommendation. The code is publicly available at
https://github.com/bigdata-ustc/Zero-1-to-3.",None,-1
597c84ec-21b9-4a65-8f8c-a6db029d9ad6,Inferring Capabilities from Task Performance with Bayesian Triangulation,0.905794,"As machine learning models become more general, we need to characterise them
in richer, more meaningful ways. We describe a method to infer the cognitive
profile of a system from diverse experimental data. To do so, we introduce
measurement layouts that model how task-instance features interact with system
capabilities to affect performance. These features must be triangulated in
complex ways to be able to infer capabilities from non-populational data -- a
challenge for traditional psychometric and inferential tools. Using the
Bayesian probabilistic programming library PyMC, we infer different cognitive
profiles for agents in two scenarios: 68 actual contestants in the AnimalAI
Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery.
We showcase the potential for capability-oriented evaluation.",None,-1
eb145cb7-7353-413c-baba-b089fb59885b,STRONG -- Structure Controllable Legal Opinion Summary Generation,0.111323,"We propose an approach for the structure controllable summarization of long
legal opinions that considers the argument structure of the document. Our
approach involves using predicted argument role information to guide the model
in generating coherent summaries that follow a provided structure pattern. We
demonstrate the effectiveness of our approach on a dataset of legal opinions
and show that it outperforms several strong baselines with respect to ROUGE,
BERTScore, and structure similarity.",None,-1
27a4cdbe-96f3-4b43-8720-923edbbeb946,"DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering",0.649429,"Uncalibrated photometric stereo (UPS) is challenging due to the inherent
ambiguity brought by the unknown light. Although the ambiguity is alleviated on
non-Lambertian objects, the problem is still difficult to solve for more
general objects with complex shapes introducing irregular shadows and general
materials with complex reflectance like anisotropic reflectance. To exploit
cues from shadow and reflectance to solve UPS and improve performance on
general materials, we propose DANI-Net, an inverse rendering framework with
differentiable shadow handling and anisotropic reflectance modeling. Unlike
most previous methods that use non-differentiable shadow maps and assume
isotropic material, our network benefits from cues of shadow and anisotropic
reflectance through two differentiable paths. Experiments on multiple
real-world datasets demonstrate our superior and robust performance.",None,-1
8c9c5fd7-033a-4e17-ac09-cb699f4e659b,Formal concept analysis for evaluating intrinsic dimension of a natural language,0.357809,"Some results of a computational experiment for determining the intrinsic
dimension of linguistic varieties for the Bengali and Russian languages are
presented. At the same time, both sets of words and sets of bigrams in these
languages were considered separately. The method used to solve this problem was
based on formal concept analysis algorithms. It was found that the intrinsic
dimensions of these languages are significantly less than the dimensions used
in popular neural network models in natural language processing.",None,-1
5b86dc0a-8411-4ebd-8f40-d627b479dc41,Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles,0.866629,"Modern hierarchical vision transformers have added several vision-specific
components in the pursuit of supervised classification performance. While these
components lead to effective accuracies and attractive FLOP counts, the added
complexity actually makes these transformers slower than their vanilla ViT
counterparts. In this paper, we argue that this additional bulk is unnecessary.
By pretraining with a strong visual pretext task (MAE), we can strip out all
the bells-and-whistles from a state-of-the-art multi-stage vision transformer
without losing accuracy. In the process, we create Hiera, an extremely simple
hierarchical vision transformer that is more accurate than previous models
while being significantly faster both at inference and during training. We
evaluate Hiera on a variety of tasks for image and video recognition. Our code
and models are available at https://github.com/facebookresearch/hiera.",None,-1
97c0606f-3d4e-4649-96ec-80f09a47416b,Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap,0.459294,"Have you ever imagined how it would look if we placed new objects into
paintings? For example, what would it look like if we placed a basketball into
Claude Monet's ``Water Lilies, Evening Effect''? We propose Reference-based
Painterly Inpainting, a novel task that crosses the wild reference domain gap
and implants novel objects into artworks. Although previous works have examined
reference-based inpainting, they are not designed for large domain
discrepancies between the target and the reference, such as inpainting an
artistic image using a photorealistic reference. This paper proposes a novel
diffusion framework, dubbed RefPaint, to ``inpaint more wildly'' by taking such
references with large domain gaps. Built with an image-conditioned diffusion
model, we introduce a ladder-side branch and a masked fusion mechanism to work
with the inpainting mask. By decomposing the CLIP image embeddings at inference
time, one can manipulate the strength of semantic and style information with
ease. Experiments demonstrate that our proposed RefPaint framework produces
significantly better results than existing methods. Our method enables creative
painterly image inpainting with reference objects that would otherwise be
difficult to achieve. Project page: https://vita-group.github.io/RefPaint/",None,-1
3049f080-f3bb-4aa6-833d-a7cd76854853,XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models,0.906285,"Large multilingual language models typically rely on a single vocabulary
shared across 100+ languages. As these models have increased in parameter count
and depth, vocabulary size has remained largely unchanged. This
\textit{vocabulary bottleneck} limits the representational capabilities of
multilingual models like XLM-R. In this paper, we introduce a new approach for
scaling to very large multilingual vocabularies by de-emphasizing token sharing
between languages with little lexical overlap and assigning vocabulary capacity
to achieve sufficient coverage for each individual language. Tokenizations
using our vocabulary are typically more semantically meaningful and shorter
compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a
multilingual language model with a one million token vocabulary. XLM-V
outperforms XLM-R on every task we tested on ranging from natural language
inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity
recognition (WikiAnn). XLM-V is particularly effective on low-resource language
tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and
Americas NLI, respectively.",None,-1
9cca61c4-9766-44fd-bdb5-6bceca8f7b83,Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models,0.0373369,"In this work, we propose a method that combines two popular research areas by
injecting linguistic structures into pre-trained language models in the
parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel
adapter modules encoding different linguistic structures are combined using a
novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates
are used to determine the importance of these modules at each layer of the
model. To reduce the number of parameters, we first train the model for a fixed
small number of steps before pruning the experts based on their importance
scores. Our experiment results with three different pre-trained models show
that our approach can outperform state-of-the-art PEFT methods with a
comparable number of parameters. In addition, we provide additional analysis to
examine the experts selected by each model at each layer to provide insights
for future studies.",None,-1
75e4e5ad-6b92-42e6-a2b7-940e0ee79adb,CPopQA: Ranking Cultural Concept Popularity by LLMs,0.878456,"Prior work has demonstrated large language models' (LLMs) potential to
discern statistical tendencies within their pre-training corpora. Despite that,
many examinations of LLMs' knowledge capacity focus on knowledge explicitly
appearing in the training data or implicitly inferable from similar contexts.
How well an LLM captures the corpus-level statistical trends of concepts for
reasoning, especially long-tail ones, is still underexplored. In this study, we
introduce a novel few-shot question-answering task (CPopQA) that examines LLMs'
statistical ranking abilities for long-tail cultural concepts (e.g., holidays),
with a specific focus on these concepts' popularity in the United States and
the United Kingdom, respectively. We curate a dataset containing 459 holidays
across 58 countries, generating a total of 6,000 QA testing pairs. Experiments
on four strong LLMs show that large models are capable of ranking long-tail
cultural concepts regarding their statistical tendency. Notably, GPT-3.5
displayed superior performance and exhibited its potential to identify
geo-cultural proximity across continents.",None,-1
b674f653-2060-4516-aab0-63129d2087be,SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,0.996609,"Building end-to-end task bots and maintaining their integration with new
functionalities using minimal human efforts is a long-standing challenge in
dialog research. Recently large language models (LLMs) have demonstrated
exceptional proficiency in conversational engagement and adherence to
instructions across various downstream tasks. In this work, we introduce
SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems
effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we
instruct fixed LLMs to generate appropriate responses on novel tasks,
circumventing the need for training data. Specifically, SGP-TOD comprises three
components: a LLM for engaging with users, a DST Prompter to aid the LLM with
dialog state tracking, which is then used to retrieve database items, and a
Policy Prompter to elicit proper responses adhering to the provided dialog
policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that
our training-free strategy SGP-TOD, without any task-specific data, yields
state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot
approaches. In a domain-extension setting, SGP-TOD aptly adapts to new
functionalities by merely adding supplementary schema rules. We make our code
and data publicly available.",None,-1
5107cfa5-73e0-444f-a6f3-94f4ae6b7d56,MatFuse: Controllable Material Generation with Diffusion Models,0.877781,"Creating high-quality materials in computer graphics is a challenging and
time-consuming task, which requires great expertise. To simplify this process,
we introduce MatFuse, a unified approach that harnesses the generative power of
diffusion models for creation and editing of 3D materials. Our method
integrates multiple sources of conditioning, including color palettes,
sketches, text, and pictures, enhancing creative possibilities and granting
fine-grained control over material synthesis. Additionally, MatFuse enables
map-level material editing capabilities through latent manipulation by means of
a multi-encoder compression model which learns a disentangled latent
representation for each map. We demonstrate the effectiveness of MatFuse under
multiple conditioning settings and explore the potential of material editing.
Finally, we assess the quality of the generated materials both quantitatively
in terms of CLIP-IQA and FID scores and qualitatively by conducting a user
study. Source code for training MatFuse and supplemental materials are publicly
available at https://gvecchio.com/matfuse.",None,-1
a40b8b6e-936a-46bd-9ade-754982c7f855,PoseRAC: Pose Saliency Transformer for Repetitive Action Counting,0.978583,"This paper presents a significant contribution to the field of repetitive
action counting through the introduction of a new approach called Pose Saliency
Representation. The proposed method efficiently represents each action using
only two salient poses instead of redundant frames, which significantly reduces
the computational cost while improving the performance. Moreover, we introduce
a pose-level method, PoseRAC, which is based on this representation and
achieves state-of-the-art performance on two new version datasets by using Pose
Saliency Annotation to annotate salient poses for training. Our lightweight
model is highly efficient, requiring only 20 minutes for training on a GPU, and
infers nearly 10x faster compared to previous methods. In addition, our
approach achieves a substantial improvement over the previous state-of-the-art
TransRAC, achieving an OBO metric of 0.56 compared to 0.29 of TransRAC. The
code and new dataset are available at https://github.com/MiracleDance/PoseRAC
for further research and experimentation, making our proposed approach highly
accessible to the research community.",None,-1
1d82465f-e5b6-4401-b6f3-ab29310d151d,Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics,0.905356,"The Japanese notion of ""kawaii"" or expressions of cuteness, vulnerability,
and/or charm is a global cultural export. Work has explored kawaii-ness as a
design feature and factor of user experience in the visual appearance,
nonverbal behaviour, and sound of robots and virtual characters. In this
initial work, we consider whether voices can be kawaii by exploring the vocal
qualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an
age-inclusive model of kawaii, we ran a user perceptions study on the
kawaii-ness of younger- and older-sounding Japanese computer voices. We found
that kawaii-ness intersected with perceptions of gender and age, i.e., gender
ambiguous and girlish, as well as VA features, i.e., fluency and artificiality.
We propose an initial model of kawaii vocalics to be validated through the
identification and study of vocal qualities, cognitive appraisals, behavioural
responses, and affective reports.",None,-1
8144242d-986b-41e8-b2cb-f30515a04c8f,Weakly Supervised Reasoning by Neuro-Symbolic Approaches,0.623636,"Deep learning has largely improved the performance of various natural
language processing (NLP) tasks. However, most deep learning models are
black-box machinery, and lack explicit interpretation. In this chapter, we will
introduce our recent progress on neuro-symbolic approaches to NLP, which
combines different schools of AI, namely, symbolism and connectionism.
Generally, we will design a neural system with symbolic latent structures for
an NLP task, and apply reinforcement learning or its relaxation to perform
weakly supervised reasoning in the downstream task. Our framework has been
successfully applied to various tasks, including table query reasoning,
syntactic structure reasoning, information extraction reasoning, and rule
reasoning. For each application, we will introduce the background, our
approach, and experimental results.",None,-1
4414a9fe-1c77-4925-b3c4-8a7cedf71da1,Ensemble Distillation for Unsupervised Constituency Parsing,0.69139,"We investigate the unsupervised constituency parsing task, which organizes
words and phrases of a sentence into a hierarchical structure without using
linguistically annotated data. We observe that existing unsupervised parsers
capture differing aspects of parsing structures, which can be leveraged to
enhance unsupervised parsing performance. To this end, we propose a notion of
""tree averaging,"" based on which we further propose a novel ensemble method for
unsupervised parsing. To improve inference efficiency, we further distill the
ensemble knowledge into a student model; such an ensemble-then-distill process
is an effective approach to mitigate the over-smoothing problem existing in
common multi-teacher distilling methods. Experiments show that our method
surpasses all previous approaches, consistently demonstrating its effectiveness
and robustness across various runs, with different ensemble components, and
under domain-shift conditions.",None,-1
f9d4a823-335d-4ecb-8a03-4eaa93f2d323,Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution,0.819605,"Entity Resolution (ER) is the problem of semi-automatically determining when
two entities refer to the same underlying entity, with applications ranging
from healthcare to e-commerce. Traditional ER solutions required considerable
manual expertise, including domain-specific feature engineering, as well as
identification and curation of training data. Recently released large language
models (LLMs) provide an opportunity to make ER more seamless and
domain-independent. However, it is also well known that LLMs can pose risks,
and that the quality of their outputs can depend on how prompts are engineered.
Unfortunately, a systematic experimental study on the effects of different
prompting methods for addressing unsupervised ER, using LLMs like ChatGPT, has
been lacking thus far. This paper aims to address this gap by conducting such a
study. We consider some relatively simple and cost-efficient ER prompt
engineering methods and apply them to ER on two real-world datasets widely used
in the community. We use an extensive set of experimental results to show that
an LLM like GPT3.5 is viable for high-performing unsupervised ER, and
interestingly, that more complicated and detailed (and hence, expensive)
prompting methods do not necessarily outperform simpler approaches. We provide
brief discussions on qualitative and error analysis, including a study of the
inter-consistency of different prompting methods to determine whether they
yield stable outputs. Finally, we consider some limitations of LLMs when
applied to ER.",None,-1
8f14831b-1653-4aef-8b12-a365079cecdf,RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters,0.069541,"Novel view synthesis (NVS) is a challenging task in computer vision that
involves synthesizing new views of a scene from a limited set of input images.
Neural Radiance Fields (NeRF) have emerged as a powerful approach to address
this problem, but they require accurate knowledge of camera \textit{intrinsic}
and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM)
and multi-view stereo (MVS) approaches have been used to extract camera
parameters, but these methods can be unreliable and may fail in certain cases.
In this paper, we propose a novel technique that leverages unposed images from
dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera
parameters directly from data. Our approach is highly extensible and can be
integrated into existing NeRF architectures with minimal modifications. We
demonstrate the effectiveness of our method on a variety of static and dynamic
scenes and show that it outperforms traditional SfM and MVS approaches. The
code for our method is publicly available at
\href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}.
Our approach offers a promising new direction for improving the accuracy and
robustness of NVS using NeRF, and we anticipate that it will be a valuable tool
for a wide range of applications in computer vision and graphics.",None,-1
51390cab-2b87-4694-941c-f16d7cf4f0e7,LEGO-Prover: Neural Theorem Proving with Growing Libraries,0.999444,"Despite the success of large language models (LLMs), the task of theorem
proving still remains one of the hardest reasoning tasks that is far from being
fully solved. Prior methods using language models have demonstrated promising
results, but they still struggle to prove even middle school level theorems.
One common limitation of these methods is that they assume a fixed theorem
library during the whole theorem proving process. However, as we all know,
creating new useful theorems or even new theories is not only helpful but
crucial and necessary for advancing mathematics and proving harder and deeper
results. In this work, we present LEGO-Prover, which employs a growing skill
library containing verified lemmas as skills to augment the capability of LLMs
used in theorem proving. By constructing the proof modularly, LEGO-Prover
enables LLMs to utilize existing skills retrieved from the library and to
create new skills during the proving process. These skills are further evolved
(by prompting an LLM) to enrich the library on another scale. Modular and
reusable skills are constantly added to the library to enable tackling
increasingly intricate mathematical problems. Moreover, the learned library
further bridges the gap between human proofs and formal proofs by making it
easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass
rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).
During the proving process, LEGO-Prover also manages to generate over 20,000
skills (theorems/lemmas) and adds them to the growing library. Our ablation
study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.",None,-1
54133f17-a4ce-48c5-accb-9965a7ff4f4f,Towards Integration of Discriminability and Robustness for Document-Level Relation Extraction,0.775232,"Document-level relation extraction (DocRE) predicts relations for entity
pairs that rely on long-range context-dependent reasoning in a document. As a
typical multi-label classification problem, DocRE faces the challenge of
effectively distinguishing a small set of positive relations from the majority
of negative ones. This challenge becomes even more difficult to overcome when
there exists a significant number of annotation errors in the dataset. In this
work, we aim to achieve better integration of both the discriminability and
robustness for the DocRE problem. Specifically, we first design an effective
loss function to endow high discriminability to both probabilistic outputs and
internal representations. We innovatively customize entropy minimization and
supervised contrastive learning for the challenging multi-label and long-tailed
learning problems. To ameliorate the impact of label errors, we equipped our
method with a novel negative label sampling strategy to strengthen the model
robustness. In addition, we introduce two new data regimes to mimic more
realistic scenarios with annotation errors and evaluate our sampling strategy.
Experimental results verify the effectiveness of each component and show that
our method achieves new state-of-the-art results on the DocRED dataset, its
recently cleaned version, Re-DocRED, and the proposed data regimes.",None,-1
949a95e1-6b90-4d67-99bd-5c88d80b4ad9,Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages,0.553307,"Sentiment analysis (SA) systems are used in many products and hundreds of
languages. Gender and racial biases are well-studied in English SA systems, but
understudied in other languages, with few resources for such studies. To remedy
this, we build a counterfactual evaluation corpus for gender and racial/migrant
bias in four languages. We demonstrate its usefulness by answering a simple but
important question that an engineer might need to answer when deploying a
system: What biases do systems import from pre-trained models when compared to
a baseline with no pre-training? Our evaluation corpus, by virtue of being
counterfactual, not only reveals which models have less bias, but also
pinpoints changes in model bias behaviour, which enables more targeted
mitigation strategies. We release our code and evaluation corpora to facilitate
future research.",None,-1
160ebf9f-7ae2-4a67-93d9-10f2777c305e,Emergent Communication with Attention,0.437692,"To develop computational agents that better communicate using their own
emergent language, we endow the agents with an ability to focus their attention
on particular concepts in the environment. Humans often understand an object or
scene as a composite of concepts and those concepts are further mapped onto
words. We implement this intuition as cross-modal attention mechanisms in
Speaker and Listener agents in a referential game and show attention leads to
more compositional and interpretable emergent language. We also demonstrate how
attention aids in understanding the learned communication protocol by
investigating the attention weights associated with each message symbol and the
alignment of attention weights between Speaker and Listener agents. Overall,
our results suggest that attention is a promising mechanism for developing more
human-like emergent language.",None,-1
a22216e8-4a95-4b60-9934-05b6a31873e1,gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction,0.714349,"Signed distance functions (SDFs) is an attractive framework that has recently
shown promising results for 3D shape reconstruction from images. SDFs
seamlessly generalize to different shape resolutions and topologies but lack
explicit modelling of the underlying 3D geometry. In this work, we exploit the
hand structure and use it as guidance for SDF-based shape reconstruction. In
particular, we address reconstruction of hands and manipulated objects from
monocular RGB images. To this end, we estimate poses of hands and objects and
use them to guide 3D reconstruction. More specifically, we predict kinematic
chains of pose transformations and align SDFs with highly-articulated hand
poses. We improve the visual features of 3D points with geometry alignment and
further leverage temporal information to enhance the robustness to occlusion
and motion blurs. We conduct extensive experiments on the challenging ObMan and
DexYCB benchmarks and demonstrate significant improvements of the proposed
method over the state of the art.",None,-1
85d1bbb5-5d4a-40b2-b701-114a67e2b0d6,Control Risk for Potential Misuse of Artificial Intelligence in Science,0.885085,"The expanding application of Artificial Intelligence (AI) in scientific
fields presents unprecedented opportunities for discovery and innovation.
However, this growth is not without risks. AI models in science, if misused,
can amplify risks like creation of harmful substances, or circumvention of
established regulations. In this study, we aim to raise awareness of the
dangers of AI misuse in science, and call for responsible AI development and
use in this domain. We first itemize the risks posed by AI in scientific
contexts, then demonstrate the risks by highlighting real-world examples of
misuse in chemical science. These instances underscore the need for effective
risk management strategies. In response, we propose a system called SciGuard to
control misuse risks for AI models in science. We also propose a red-teaming
benchmark SciMT-Safety to assess the safety of different systems. Our proposed
SciGuard shows the least harmful impact in the assessment without compromising
performance in benign tests. Finally, we highlight the need for a
multidisciplinary and collaborative effort to ensure the safe and ethical use
of AI models in science. We hope that our study can spark productive
discussions on using AI ethically in science among researchers, practitioners,
policymakers, and the public, to maximize benefits and minimize the risks of
misuse.",None,-1
419292c6-b9b6-4c30-a730-a7ca61f18f2e,Tools for Landscape Analysis of Optimisation Problems in Procedural Content Generation for Games,0.122696,"The term Procedural Content Generation (PCG) refers to the (semi-)automatic
generation of game content by algorithmic means, and its methods are becoming
increasingly popular in game-oriented research and industry. A special class of
these methods, which is commonly known as search-based PCG, treats the given
task as an optimisation problem. Such problems are predominantly tackled by
evolutionary algorithms.
  We will demonstrate in this paper that obtaining more information about the
defined optimisation problem can substantially improve our understanding of how
to approach the generation of content. To do so, we present and discuss three
efficient analysis tools, namely diagonal walks, the estimation of high-level
properties, as well as problem similarity measures. We discuss the purpose of
each of the considered methods in the context of PCG and provide guidelines for
the interpretation of the results received. This way we aim to provide methods
for the comparison of PCG approaches and eventually, increase the quality and
practicality of generated content in industry.",None,-1
c4cbb718-d44c-4b94-9665-7b4702aeff5e,SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool,0.174892,"Large Language Model (LLM) based Generative AI systems have seen significant
progress in recent years. Integrating a knowledge retrieval architecture allows
for seamless integration of private data into publicly available Generative AI
systems using pre-trained LLM without requiring additional model fine-tuning.
Moreover, Retrieval-Centric Generation (RCG) approach, a promising future
research direction that explicitly separates roles of LLMs and retrievers in
context interpretation and knowledge memorization, potentially leads to more
efficient implementation. SimplyRetrieve is an open-source tool with the goal
of providing a localized, lightweight, and user-friendly interface to these
sophisticated advancements to the machine learning community. SimplyRetrieve
features a GUI and API based RCG platform, assisted by a Private Knowledge Base
Constructor and a Retrieval Tuning Module. By leveraging these capabilities,
users can explore the potential of RCG for improving generative AI performance
while maintaining privacy standards. The tool is available at
https://github.com/RCGAI/SimplyRetrieve with an MIT license.",None,-1
088bdfe2-17cf-4b61-b4b7-99710147192f,EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design,0.406039,"We present a hardware-efficient architecture of convolutional neural network,
which has a repvgg-like architecture. Flops or parameters are traditional
metrics to evaluate the efficiency of networks which are not sensitive to
hardware including computing ability and memory bandwidth. Thus, how to design
a neural network to efficiently use the computing ability and memory bandwidth
of hardware is a critical problem. This paper proposes a method how to design
hardware-aware neural network. Based on this method, we designed EfficientRep
series convolutional networks, which are high-computation hardware(e.g. GPU)
friendly and applied in YOLOv6 object detection framework. YOLOv6 has published
YOLOv6N/YOLOv6S/YOLOv6M/YOLOv6L models in v1 and v2 versions.",None,-1
2f642455-cdc5-4fea-9fde-2bbe98461bf1,"Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts, Datasets and Metrics",0.590647,"One of the main paths towards the reduction of traffic accidents is the
increase in vehicle safety through driver assistance systems or even systems
with a complete level of autonomy. In these types of systems, tasks such as
obstacle detection and segmentation, especially the Deep Learning-based ones,
play a fundamental role in scene understanding for correct and safe navigation.
Besides that, the wide variety of sensors in vehicles nowadays provides a rich
set of alternatives for improvement in the robustness of perception in
challenging situations, such as navigation under lighting and weather adverse
conditions. Despite the current focus given to the subject, the literature
lacks studies on radar-based and radar-camera fusion-based perception. Hence,
this work aims to carry out a study on the current scenario of camera and
radar-based perception for ADAS and autonomous vehicles. Concepts and
characteristics related to both sensors, as well as to their fusion, are
presented. Additionally, we give an overview of the Deep Learning-based
detection and segmentation tasks, and the main datasets, metrics, challenges,
and open questions in vehicle perception.",None,-1
c12cc9e8-f223-4d0d-8fcd-991b9b8394cf,MVDream: Multi-view Diffusion for 3D Generation,1.0,"We introduce MVDream, a diffusion model that is able to generate consistent
multi-view images from a given text prompt. Learning from both 2D and 3D data,
a multi-view diffusion model can achieve the generalizability of 2D diffusion
models and the consistency of 3D renderings. We demonstrate that such a
multi-view diffusion model is implicitly a generalizable 3D prior agnostic to
3D representations. It can be applied to 3D generation via Score Distillation
Sampling, significantly enhancing the consistency and stability of existing
2D-lifting methods. It can also learn new concepts from a few 2D examples, akin
to DreamBooth, but for 3D generation.",None,-1
540a720f-ab49-4ceb-9266-2f7a5eb61c63,Software Vulnerability Prediction Knowledge Transferring Between Programming Languages,0.215,"Developing automated and smart software vulnerability detection models has
been receiving great attention from both research and development communities.
One of the biggest challenges in this area is the lack of code samples for all
different programming languages. In this study, we address this issue by
proposing a transfer learning technique to leverage available datasets and
generate a model to detect common vulnerabilities in different programming
languages. We use C source code samples to train a Convolutional Neural Network
(CNN) model, then, we use Java source code samples to adopt and evaluate the
learned model. We use code samples from two benchmark datasets: NIST Software
Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show
that proposed model detects vulnerabilities in both C and Java codes with
average recall of 72\%. Additionally, we employ explainable AI to investigate
how much each feature contributes to the knowledge transfer mechanisms between
C and Java in the proposed model.",None,-1
353795b3-3961-466b-941c-542f7115b335,Contrastive Decoding Improves Reasoning in Large Language Models,0.255835,"We demonstrate that Contrastive Decoding -- a simple, computationally light,
and training-free text generation method proposed by Li et al 2022 -- achieves
large out-of-the-box improvements over greedy decoding on a variety of
reasoning tasks. Originally shown to improve the perceived quality of long-form
text generation, Contrastive Decoding searches for strings that maximize a
weighted difference in likelihood between strong and weak models. We show that
Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM
2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA
2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in
addition to improvements on a collection of other tasks. Analysis suggests that
Contrastive Decoding improves over existing methods by preventing some abstract
reasoning errors, as well as by avoiding simpler modes such as copying sections
of the input during chain-of-thought. Overall, Contrastive Decoding outperforms
nucleus sampling for long-form generation and greedy decoding for reasoning
tasks, making it a powerful general purpose method for generating text from
language models.",None,-1
befb16f6-4ba0-429f-8b93-508938588e03,Re-thinking Federated Active Learning based on Inter-class Diversity,0.290416,"Although federated learning has made awe-inspiring advances, most studies
have assumed that the client's data are fully labeled. However, in a real-world
scenario, every client may have a significant amount of unlabeled instances.
Among the various approaches to utilizing unlabeled data, a federated active
learning framework has emerged as a promising solution. In the decentralized
setting, there are two types of available query selector models, namely
'global' and 'local-only' models, but little literature discusses their
performance dominance and its causes. In this work, we first demonstrate that
the superiority of two selector models depends on the global and local
inter-class diversity. Furthermore, we observe that the global and local-only
models are the keys to resolving the imbalance of each side. Based on our
findings, we propose LoGo, a FAL sampling strategy robust to varying local
heterogeneity levels and global imbalance ratio, that integrates both models by
two steps of active selection scheme. LoGo consistently outperforms six active
learning strategies in the total number of 38 experimental settings.",None,-1
2b9a9eb9-c96a-4d14-adbe-1545435074cd,Towards Detecting Harmful Agendas in News Articles,0.0634929,"Manipulated news online is a growing problem which necessitates the use of
automated systems to curtail its spread. We argue that while misinformation and
disinformation detection have been studied, there has been a lack of investment
in the important open challenge of detecting harmful agendas in news articles;
identifying harmful agendas is critical to flag news campaigns with the
greatest potential for real world harm. Moreover, due to real concerns around
censorship, harmful agenda detectors must be interpretable to be effective. In
this work, we propose this new task and release a dataset, NewsAgendas, of
annotated news articles for agenda identification. We show how interpretable
systems can be effective on this task and demonstrate that they can perform
comparably to black-box models.",None,-1
a0e11955-ab0c-4e80-8d1f-6785fd434117,Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,0.881903,"Prompt tuning, in which a base pretrained model is adapted to each task via
conditioning on learned prompt vectors, has emerged as a promising approach for
efficiently adapting large language models to multiple downstream tasks.
However, existing methods typically learn soft prompt vectors from scratch, and
it has not been clear how to exploit the rich cross-task knowledge with prompt
vectors in a multitask learning setting. We propose multitask prompt tuning
(MPT), which first learns a single transferable prompt by distilling knowledge
from multiple task-specific source prompts. We then learn multiplicative low
rank updates to this shared prompt to efficiently adapt it to each downstream
target task. Extensive experiments on 23 NLP datasets demonstrate that our
proposed approach outperforms the state-of-the-art methods, including the full
finetuning baseline in some cases, despite only tuning 0.035% as many
task-specific parameters.",None,-1
da27f42a-8148-4c88-b79f-901969445b51,Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation,0.173135,"Language Models today provide a high accuracy across a large number of
downstream tasks. However, they remain susceptible to adversarial attacks,
particularly against those where the adversarial examples maintain considerable
similarity to the original text. Given the multilingual nature of text, the
effectiveness of adversarial examples across translations and how machine
translations can improve the robustness of adversarial examples remain largely
unexplored. In this paper, we present a comprehensive study on the robustness
of current text adversarial attacks to round-trip translation. We demonstrate
that 6 state-of-the-art text-based adversarial attacks do not maintain their
efficacy after round-trip translation. Furthermore, we introduce an
intervention-based solution to this problem, by integrating Machine Translation
into the process of adversarial example generation and demonstrating increased
robustness to round-trip translation. Our results indicate that finding
adversarial examples robust to translation can help identify the insufficiency
of language models that is common across languages, and motivate further
research into multilingual adversarial attacks.",None,-1
186abe1b-f67a-4454-8483-2da55b16e892,Generative Novel View Synthesis with 3D-Aware Diffusion Models,0.996175,"We present a diffusion-based model for 3D-aware generative novel view
synthesis from as few as a single input image. Our model samples from the
distribution of possible renderings consistent with the input and, even in the
presence of ambiguity, is capable of rendering diverse and plausible novel
views. To achieve this, our method makes use of existing 2D diffusion backbones
but, crucially, incorporates geometry priors in the form of a 3D feature
volume. This latent feature field captures the distribution over possible scene
representations and improves our method's ability to generate view-consistent
novel renderings. In addition to generating novel views, our method has the
ability to autoregressively synthesize 3D-consistent sequences. We demonstrate
state-of-the-art results on synthetic renderings and room-scale scenes; we also
show compelling results for challenging, real-world objects.",None,-1
fa340f2e-1e27-46c8-b960-15b1cf50c84f,ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models,0.0456665,"As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.",None,-1
31e81c8f-86b1-4d6b-8956-74986a688e7b,A Neural Span-Based Continual Named Entity Recognition Model,0.442146,"Named Entity Recognition (NER) models capable of Continual Learning (CL) are
realistically valuable in areas where entity types continuously increase (e.g.,
personal assistants). Meanwhile the learning paradigm of NER advances to new
patterns such as the span-based methods. However, its potential to CL has not
been fully explored. In this paper, we propose SpanKL, a simple yet effective
Span-based model with Knowledge distillation (KD) to preserve memories and
multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence
labeling approaches, the inherently independent modeling in span and entity
level with the designed coherent optimization on SpanKL promotes its learning
at each incremental step and mitigates the forgetting. Experiments on synthetic
CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly
outperforms previous SoTA in many aspects, and obtains the smallest gap from CL
to the upper bound revealing its high practiced value. The code is available at
https://github.com/Qznan/SpanKL.",None,-1
96da574c-610a-488e-977c-f5dcbaab0f0a,Diffusion Self-Guidance for Controllable Image Generation,0.999628,"Large-scale generative models are capable of producing high-quality images
from detailed text descriptions. However, many aspects of an image are
difficult or impossible to convey through text. We introduce self-guidance, a
method that provides greater control over generated images by guiding the
internal representations of diffusion models. We demonstrate that properties
such as the shape, location, and appearance of objects can be extracted from
these representations and used to steer sampling. Self-guidance works similarly
to classifier guidance, but uses signals present in the pretrained model
itself, requiring no additional models or training. We show how a simple set of
properties can be composed to perform challenging image manipulations, such as
modifying the position or size of objects, merging the appearance of objects in
one image with the layout of another, composing objects from many images into
one, and more. We also show that self-guidance can be used to edit real images.
For results and an interactive demo, see our project page at
https://dave.ml/selfguidance/",None,-1
d8c2b909-28f5-4faa-90ed-32744088f4ee,Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling,0.24785,"We study speech intent classification and slot filling (SICSF) by proposing
to use an encoder pretrained on speech recognition (ASR) to initialize an
end-to-end (E2E) Conformer-Transformer model, which achieves the new
state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and
82.27% SLURP-F1. We compare our model with encoders pretrained on
self-supervised learning (SSL), and show that ASR pretraining is much more
effective than SSL for SICSF. To explore parameter efficiency, we freeze the
encoder and add Adapter modules, and show that parameter efficiency is only
achievable with an ASR-pretrained encoder, while the SSL encoder needs full
finetuning to achieve comparable results. In addition, we provide an in-depth
comparison on end-to-end models versus cascading models (ASR+NLU), and show
that E2E models are better than cascaded models unless an oracle ASR model is
provided. Last but not least, our model is the first E2E model that achieves
the same performance as cascading models with oracle ASR. Code, checkpoints and
configs are available.",None,-1
50b9b174-9574-41fe-a63a-e154449b62ba,Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations,0.257591,"Adversarial training (AT) methods have been found to be effective against
adversarial attacks on deep neural networks. Many variants of AT have been
proposed to improve its performance. Pang et al. [1] have recently shown that
incorporating hypersphere embedding (HE) into the existing AT procedures
enhances robustness. We observe that the existing AT procedures are not
designed for the HE framework, and thus fail to adequately learn the angular
discriminative information available in the HE framework. In this paper, we
propose integrating HE into AT with regularization terms that exploit the rich
angular information available in the HE framework. Specifically, our method,
termed angular-AT, adds regularization terms to AT that explicitly enforce
weight-feature compactness and inter-class separation; all expressed in terms
of angular features. Experimental results show that angular-AT further improves
adversarial robustness.",None,-1
4c5d6f6d-0d93-4084-8a7c-b8f55ed72493,nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla Sentiment Analysis,0.559276,"In this paper, we discuss the nlpBDpatriots entry to the shared task on
Sentiment Analysis of Bangla Social Media Posts organized at the first workshop
on Bangla Language Processing (BLP) co-located with EMNLP. The main objective
of this task is to identify the polarity of social media content using a Bangla
dataset annotated with positive, neutral, and negative labels provided by the
shared task organizers. Our best system for this task is a transfer learning
approach with data augmentation which achieved a micro F1 score of 0.71. Our
best system ranked 12th among 30 teams that participated in the competition.",None,-1
fbe5531f-4378-4c03-83c3-4200ed2fe6f4,Holy Grail 2.0: From Natural Language to Constraint Models,0.5314,"Twenty-seven years ago, E. Freuder highlighted that ""Constraint programming
represents one of the closest approaches computer science has yet made to the
Holy Grail of programming: the user states the problem, the computer solves
it"". Nowadays, CP users have great modeling tools available (like Minizinc and
CPMpy), allowing them to formulate the problem and then let a solver do the
rest of the job, getting closer to the stated goal. However, this still
requires the CP user to know the formalism and respect it. Another significant
challenge lies in the expertise required to effectively model combinatorial
problems. All this limits the wider adoption of CP. In this position paper, we
investigate a possible approach to leverage pre-trained Large Language Models
to extract models from textual problem descriptions. More specifically, we take
inspiration from the Natural Language Processing for Optimization (NL4OPT)
challenge and present early results with a decomposition-based prompting
approach to GPT Models.",None,-1
48eab504-7d89-4933-9662-e1006a2fb82d,Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality,0.412156,"Improving factual consistency of abstractive summarization has been a widely
studied topic. However, most of the prior works on training factuality-aware
models have ignored the negative effect it has on summary quality. We propose
EFACTSUM (i.e., Effective Factual Summarization), a candidate summary
generation and ranking technique to improve summary factuality without
sacrificing summary quality. We show that using a contrastive learning
framework with our refined candidate summaries leads to significant gains on
both factuality and similarity-based metrics. Specifically, we propose a
ranking strategy in which we effectively combine two metrics, thereby
preventing any conflict during training. Models trained using our approach show
up to 6 points of absolute improvement over the base model with respect to
FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either
similarity-based metrics or absractiveness.",None,-1
1aa149d8-6a0b-4808-8d86-3fc9d1ed42cd,ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations,0.181515,"3D instance segmentation methods often require fully-annotated dense labels
for training, which are costly to obtain. In this paper, we present ClickSeg, a
novel click-level weakly supervised 3D instance segmentation method that
requires one point per instance annotation merely. Such a problem is very
challenging due to the extremely limited labels, which has rarely been solved
before. We first develop a baseline weakly-supervised training method, which
generates pseudo labels for unlabeled data by the model itself. To utilize the
property of click-level annotation setting, we further propose a new training
framework. Instead of directly using the model inference way, i.e., mean-shift
clustering, to generate the pseudo labels, we propose to use k-means with fixed
initial seeds: the annotated points. New similarity metrics are further
designed for clustering. Experiments on ScanNetV2 and S3DIS datasets show that
the proposed ClickSeg surpasses the previous best weakly supervised instance
segmentation result by a large margin (e.g., +9.4% mAP on ScanNetV2). Using
0.02% supervision signals merely, ClickSeg achieves $\sim$90% of the accuracy
of the fully-supervised counterpart. Meanwhile, it also achieves
state-of-the-art semantic segmentation results among weakly supervised methods
that use the same annotation settings.",None,-1
2f0dd0e5-cab8-41ac-9edc-97d2a998dd2e,Sig-Splines: universal approximation and convex calibration of time series generative models,0.0645763,"We propose a novel generative model for multivariate discrete-time time
series data. Drawing inspiration from the construction of neural spline flows,
our algorithm incorporates linear transformations and the signature transform
as a seamless substitution for traditional neural networks. This approach
enables us to achieve not only the universality property inherent in neural
networks but also introduces convexity in the model's parameters.",None,-1
fd3297ef-bf60-4373-a0e1-2d2129c6f6df,The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models,0.272042,"Partially Observable Markov Decision Processes (POMDPs) are used to model
environments where the full state cannot be perceived by an agent. As such the
agent needs to reason taking into account the past observations and actions.
However, simply remembering the full history is generally intractable due to
the exponential growth in the history space. Maintaining a probability
distribution that models the belief over what the true state is can be used as
a sufficient statistic of the history, but its computation requires access to
the model of the environment and is often intractable. While SOTA algorithms
use Recurrent Neural Networks to compress the observation-action history aiming
to learn a sufficient statistic, they lack guarantees of success and can lead
to sub-optimal policies. To overcome this, we propose the Wasserstein Belief
Updater, an RL algorithm that learns a latent model of the POMDP and an
approximation of the belief update. Our approach comes with theoretical
guarantees on the quality of our approximation ensuring that our outputted
beliefs allow for learning the optimal value function.",None,-1
6559fc97-e3eb-4b47-b1dc-12616404fb8f,Optimized Custom Dataset for Efficient Detection of Underwater Trash,0.580041,"Accurately quantifying and removing submerged underwater waste plays a
crucial role in safeguarding marine life and preserving the environment. While
detecting floating and surface debris is relatively straightforward,
quantifying submerged waste presents significant challenges due to factors like
light refraction, absorption, suspended particles, and color distortion. This
paper addresses these challenges by proposing the development of a custom
dataset and an efficient detection approach for submerged marine debris. The
dataset encompasses diverse underwater environments and incorporates
annotations for precise labeling of debris instances. Ultimately, the primary
objective of this custom dataset is to enhance the diversity of litter
instances and improve their detection accuracy in deep submerged environments
by leveraging state-of-the-art deep learning architectures.",None,-1
a7bd2828-24d0-4f86-a22a-8472e9fee1ac,Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences,0.477925,"As a natural language assistant, ChatGPT is capable of performing various
tasks, including but not limited to article generation, code completion, and
data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable
level of accuracy and reliability in terms of content evaluation, exhibiting
the capability of mimicking human preferences. To further explore ChatGPT's
potential in this regard, a study is conducted to assess its ability to rank
content. In order to do so, a test set consisting of prompts is created,
covering a wide range of use cases, and five models are utilized to generate
corresponding responses. ChatGPT is then instructed to rank the responses
generated by these models. The results on the test set show that ChatGPT's
ranking preferences are consistent with human to a certain extent. This
preliminary experimental finding implies that ChatGPT's zero-shot ranking
capability could be used to reduce annotation pressure in a number of ranking
tasks.",None,-1
d522ebba-4d74-4cad-99e2-63c0f12b3cc7,Anatomy-Driven Pathology Detection on Chest X-rays,0.924691,"Pathology detection and delineation enables the automatic interpretation of
medical scans such as chest X-rays while providing a high level of
explainability to support radiologists in making informed decisions. However,
annotating pathology bounding boxes is a time-consuming task such that large
public datasets for this purpose are scarce. Current approaches thus use weakly
supervised object detection to learn the (rough) localization of pathologies
from image-level annotations, which is however limited in performance due to
the lack of bounding box supervision. We therefore propose anatomy-driven
pathology detection (ADPD), which uses easy-to-annotate bounding boxes of
anatomical regions as proxies for pathologies. We study two training
approaches: supervised training using anatomy-level pathology labels and
multiple instance learning (MIL) with image-level pathology labels. Our results
show that our anatomy-level training approach outperforms weakly supervised
methods and fully supervised detection with limited training samples, and our
MIL approach is competitive with both baseline approaches, therefore
demonstrating the potential of our approach.",None,-1
35726a7b-18f2-4b2b-8bd7-2365f5229608,Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table,0.269592,"Despite recent interest in open domain question answering (ODQA) over tables,
many studies still rely on datasets that are not truly optimal for the task
with respect to utilizing structural nature of table. These datasets assume
answers reside as a single cell value and do not necessitate exploring over
multiple cells such as aggregation, comparison, and sorting. Thus, we release
Open-WikiTable, the first ODQA dataset that requires complex reasoning over
tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be
applicable in the open-domain setting. As each question is coupled with both
textual answers and SQL queries, Open-WikiTable opens up a wide range of
possibilities for future research, as both reader and parser methods can be
applied. The dataset and code are publicly available.",None,-1
fe7246af-5505-4b40-b3b1-c2816c746b05,A Boosted Model Ensembling Approach to Ball Action Spotting in Videos: The Runner-Up Solution to CVPR'23 SoccerNet Challenge,0.761796,"This technical report presents our solution to Ball Action Spotting in
videos. Our method reached second place in the CVPR'23 SoccerNet Challenge.
Details of this challenge can be found at
https://www.soccer-net.org/tasks/ball-action-spotting. Our approach is
developed based on a baseline model termed E2E-Spot, which was provided by the
organizer of this competition. We first generated several variants of the
E2E-Spot model, resulting in a candidate model set. We then proposed a strategy
for selecting appropriate model members from this set and assigning an
appropriate weight to each model. The aim of this strategy is to boost the
performance of the resulting model ensemble. Therefore, we call our approach
Boosted Model Ensembling (BME). Our code is available at
https://github.com/ZJLAB-AMMI/E2E-Spot-MBS.",None,-1
31702d26-ca0a-492f-96fc-bfa3274d7e4e,A New Dataset and Comparative Study for Aphid Cluster Detection,0.520463,"Aphids are one of the main threats to crops, rural families, and global food
security. Chemical pest control is a necessary component of crop production for
maximizing yields, however, it is unnecessary to apply the chemical approaches
to the entire fields in consideration of the environmental pollution and the
cost. Thus, accurately localizing the aphid and estimating the infestation
level is crucial to the precise local application of pesticides. Aphid
detection is very challenging as each individual aphid is really small and all
aphids are crowded together as clusters. In this paper, we propose to estimate
the infection level by detecting aphid clusters. We have taken millions of
images in the sorghum fields, manually selected 5,447 images that contain
aphids, and annotated each aphid cluster in the image. To use these images for
machine learning models, we crop the images into patches and created a labeled
dataset with over 151,000 image patches. Then, we implement and compare the
performance of four state-of-the-art object detection models.",None,-1
0038c8b0-72fc-4fbc-a59b-852a65a552d4,Low Complexity Approaches for End-to-End Latency Prediction,0.124951,"Software Defined Networks have opened the door to statistical and AI-based
techniques to improve efficiency of networking. Especially to ensure a certain
Quality of Service (QoS) for specific applications by routing packets with
awareness on content nature (VoIP, video, files, etc.) and its needs (latency,
bandwidth, etc.) to use efficiently resources of a network. Predicting various
Key Performance Indicators (KPIs) at any level may handle such problems while
preserving network bandwidth. The question addressed in this work is the design
of efficient and low-cost algorithms for KPI prediction, implementable at the
local level. We focus on end-to-end latency prediction, for which we illustrate
our approaches and results on a public dataset from the recent international
challenge on GNN [1]. We propose several low complexity, locally implementable
approaches, achieving significantly lower wall time both for training and
inference, with marginally worse prediction accuracy compared to
state-of-the-art global GNN solutions.",None,-1
5d18f9ee-b185-4774-8f34-40037d2d77d2,Exploiting Neighborhood Structural Features for Change Detection,0.226176,"In this letter, a novel method for change detection is proposed using
neighborhood structure correlation. Because structure features are insensitive
to the intensity differences between bi-temporal images, we perform the
correlation analysis on structure features rather than intensity information.
First, we extract the structure feature maps by using multi-orientated gradient
information. Then, the structure feature maps are used to obtain the
Neighborhood Structural Correlation Image (NSCI), which can represent the
context structure information. In addition, we introduce a measure named
matching error which can be used to improve neighborhood information.
Subsequently, a change detection model based on the random forest is
constructed. The NSCI feature and matching error are used as the model inputs
for training and prediction. Finally, the decision tree voting is used to
produce the change detection result. To evaluate the performance of the
proposed method, it was compared with three state-of-the-art change detection
methods. The experimental results on two datasets demonstrated the
effectiveness and robustness of the proposed method.",None,-1
2bafb246-7256-4cf2-91a4-41c1b94a8fbc,Reference-based Image Composition with Sketch via Structure-aware Diffusion Model,0.302267,"Recent remarkable improvements in large-scale text-to-image generative models
have shown promising results in generating high-fidelity images. To further
enhance editability and enable fine-grained generation, we introduce a
multi-input-conditioned image composition model that incorporates a sketch as a
novel modal, alongside a reference image. Thanks to the edge-level
controllability using sketches, our method enables a user to edit or complete
an image sub-part with a desired structure (i.e., sketch) and content (i.e.,
reference image). Our framework fine-tunes a pre-trained diffusion model to
complete missing regions using the reference image while maintaining sketch
guidance. Albeit simple, this leads to wide opportunities to fulfill user needs
for obtaining the in-demand images. Through extensive experiments, we
demonstrate that our proposed method offers unique use cases for image
manipulation, enabling user-driven modifications of arbitrary scenes.",None,-1
1a551ffe-c880-42c4-9ee7-e3197f987a6d,Spatially Covariant Lesion Segmentation,0.360431,"Compared to natural images, medical images usually show stronger visual
patterns and therefore this adds flexibility and elasticity to resource-limited
clinical applications by injecting proper priors into neural networks. In this
paper, we propose spatially covariant pixel-aligned classifier (SCP) to improve
the computational efficiency and meantime maintain or increase accuracy for
lesion segmentation. SCP relaxes the spatial invariance constraint imposed by
convolutional operations and optimizes an underlying implicit function that
maps image coordinates to network weights, the parameters of which are obtained
along with the backbone network training and later used for generating network
weights to capture spatially covariant contextual information. We demonstrate
the effectiveness and efficiency of the proposed SCP using two lesion
segmentation tasks from different imaging modalities: white matter
hyperintensity segmentation in magnetic resonance imaging and liver tumor
segmentation in contrast-enhanced abdominal computerized tomography. The
network using SCP has achieved 23.8%, 64.9% and 74.7% reduction in GPU memory
usage, FLOPs, and network size with similar or better accuracy for lesion
segmentation.",None,-1
48c7f7af-2684-4aad-ba5b-15f8711e14bf,Large-scale Ridesharing DARP Instances Based on Real Travel Demand,0.632121,"Accurately predicting the real-life performance of algorithms solving the
Dial-a-Ride Problem (DARP) in the context of Mobility on Demand (MoD) systems
with ridesharing requires evaluating them on representative instances. However,
the benchmarking of state-of-the-art DARP solution methods has been limited to
small, artificial instances or outdated non-public instances, hindering direct
comparisons. With the rise of large MoD systems and the availability of open
travel demand datasets for many US cities, there is now an opportunity to
evaluate these algorithms on standardized, realistic, and representative
instances. Despite the significant challenges involved in processing obfuscated
and diverse datasets, we have developed a methodology using which we have
created a comprehensive set of large-scale demand instances based on real-world
data. These instances cover diverse use cases, one of which is demonstrated in
an evaluation of two established DARP methods: the insertion heuristic and
optimal vehicle-group assignment method. We publish the full results of both
methods in a standardized format. The results show significant differences
between areas in all measured quantities, emphasizing the importance of
evaluating methods across different cities.",None,-1
262b0461-29ed-4d7a-903d-2654c8788594,LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents,0.0460174,"Recent advancements in reasoning abilities of Large Language Models (LLM) has
promoted their usage in problems that require high-level planning for robots
and artificial agents. However, current techniques that utilize LLMs for such
planning tasks make certain key assumptions such as, access to datasets that
permit finetuning, meticulously engineered prompts that only provide relevant
and essential information to the LLM, and most importantly, a deterministic
approach to allow execution of the LLM responses either in the form of existing
policies or plan operators. In this work, we propose LgTS (LLM-guided
Teacher-Student learning), a novel approach that explores the planning
abilities of LLMs to provide a graphical representation of the sub-goals to a
reinforcement learning (RL) agent that does not have access to the transition
dynamics of the environment. The RL agent uses Teacher-Student learning
algorithm to learn a set of successful policies for reaching the goal state
from the start state while simultaneously minimizing the number of
environmental interactions. Unlike previous methods that utilize LLMs, our
approach does not assume access to a propreitary or a fine-tuned LLM, nor does
it require pre-trained policies that achieve the sub-goals proposed by the LLM.
Through experiments on a gridworld based DoorKey domain and a search-and-rescue
inspired domain, we show that generating a graphical structure of sub-goals
helps in learning policies for the LLM proposed sub-goals and the
Teacher-Student learning algorithm minimizes the number of environment
interactions when the transition dynamics are unknown.",None,-1
187b03ef-b9f5-4661-8e30-0f25e4afe117,Learning Disentangled Prompts for Compositional Image Synthesis,0.149421,"We study domain-adaptive image synthesis, the problem of teaching pretrained
image generative models a new style or concept from as few as one image to
synthesize novel images, to better understand the compositional image
synthesis. We present a framework that leverages a pretrained class-conditional
generation model and visual prompt tuning. Specifically, we propose a novel
source class distilled visual prompt that learns disentangled prompts of
semantic (e.g., class) and domain (e.g., style) from a few images. Learned
domain prompt is then used to synthesize images of any classes in the style of
target domain. We conduct studies on various target domains with the number of
images ranging from one to a few to many, and show qualitative results which
show the compositional generalization of our method. Moreover, we show that our
method can help improve zero-shot domain adaptation classification accuracy.",None,-1
3ffab0dd-6f14-4426-9ed4-f5b6d4983386,Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection,0.432017,"This paper presents a novel framework, named Global-Local Correspondence
Framework (GLCF), for visual anomaly detection with logical constraints. Visual
anomaly detection has become an active research area in various real-world
applications, such as industrial anomaly detection and medical disease
diagnosis. However, most existing methods focus on identifying local structural
degeneration anomalies and often fail to detect high-level functional anomalies
that involve logical constraints. To address this issue, we propose a
two-branch approach that consists of a local branch for detecting structural
anomalies and a global branch for detecting logical anomalies. To facilitate
local-global feature correspondence, we introduce a novel semantic bottleneck
enabled by the visual Transformer. Moreover, we develop feature estimation
networks for each branch separately to detect anomalies. Our proposed framework
is validated using various benchmarks, including industrial datasets, Mvtec AD,
Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show
that our method outperforms existing methods, particularly in detecting logical
anomalies.",None,-1
703d0dbe-a6c1-40ce-912e-b5a84ce8fe51,ColdNAS: Search to Modulate for User Cold-Start Recommendation,0.160512,"Making personalized recommendation for cold-start users, who only have a few
interaction histories, is a challenging problem in recommendation systems.
Recent works leverage hypernetworks to directly map user interaction histories
to user-specific parameters, which are then used to modulate predictor by
feature-wise linear modulation function. These works obtain the
state-of-the-art performance. However, the physical meaning of scaling and
shifting in recommendation data is unclear. Instead of using a fixed modulation
function and deciding modulation position by expertise, we propose a modulation
framework called ColdNAS for user cold-start problem, where we look for proper
modulation structure, including function and position, via neural architecture
search. We design a search space which covers broad models and theoretically
prove that this search space can be transformed to a much smaller space,
enabling an efficient and robust one-shot search algorithm. Extensive
experimental results on benchmark datasets show that ColdNAS consistently
performs the best. We observe that different modulation functions lead to the
best performance on different datasets, which validates the necessity of
designing a searching-based method.",None,-1
599f3d15-1f02-4045-9364-2058c295cb35,Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement,0.712303,"Conversational agents participating in multi-party interactions face
significant challenges in dialogue state tracking, since the identity of the
speaker adds significant contextual meaning. It is common to utilise
diarisation models to identify the speaker. However, it is not clear if these
are accurate enough to correctly identify specific conversational events such
as agreement or disagreement during a real-time interaction. This study uses a
cooperative quiz, where the conversational agent acts as quiz-show host, to
determine whether diarisation or a frequency-and-proximity-based method is more
accurate at determining agreement, and whether this translates to feelings of
engagement from the players. Experimental results show that our procedural
system was more engaging to players, and was more accurate at detecting
agreement, reaching an average accuracy of 0.44 compared to 0.28 for the
diarised system.",None,-1
90fa03b1-0d2a-4864-b3b6-4d5ccb90ddda,AFPN: Asymptotic Feature Pyramid Network for Object Detection,0.829378,"Multi-scale features are of great importance in encoding objects with scale
variance in object detection tasks. A common strategy for multi-scale feature
extraction is adopting the classic top-down and bottom-up feature pyramid
networks. However, these approaches suffer from the loss or degradation of
feature information, impairing the fusion effect of non-adjacent levels. This
paper proposes an asymptotic feature pyramid network (AFPN) to support direct
interaction at non-adjacent levels. AFPN is initiated by fusing two adjacent
low-level features and asymptotically incorporates higher-level features into
the fusion process. In this way, the larger semantic gap between non-adjacent
levels can be avoided. Given the potential for multi-object information
conflicts to arise during feature fusion at each spatial location, adaptive
spatial fusion operation is further utilized to mitigate these inconsistencies.
We incorporate the proposed AFPN into both two-stage and one-stage object
detection frameworks and evaluate with the MS-COCO 2017 validation and test
datasets. Experimental evaluation shows that our method achieves more
competitive results than other state-of-the-art feature pyramid networks. The
code is available at
\href{https://github.com/gyyang23/AFPN}{https://github.com/gyyang23/AFPN}.",None,-1
068e8788-6e6b-4340-9e26-8ad4a067a023,Pluggable Neural Machine Translation Models via Memory-augmented Adapters,0.205754,"Although neural machine translation (NMT) models perform well in the general
domain, it remains rather challenging to control their generation behavior to
satisfy the requirement of different users. Given the expensive training cost
and the data scarcity challenge of learning a new model from scratch for each
user requirement, we propose a memory-augmented adapter to steer pretrained NMT
models in a pluggable manner. Specifically, we construct a multi-granular
memory based on the user-provided text samples and propose a new adapter
architecture to combine the model representations and the retrieved results. We
also propose a training strategy using memory dropout to reduce spurious
dependencies between the NMT model and the memory. We validate our approach on
both style- and domain-specific experiments and the results indicate that our
method can outperform several representative pluggable baselines.",None,-1
2bef8cf3-9145-4ac5-9628-a8c107431d71,Ladder Fine-tuning approach for SAM integrating complementary network,0.806241,"Recently, foundation models have been introduced demonstrating various tasks
in the field of computer vision. These models such as Segment Anything Model
(SAM) are generalized models trained using huge datasets. Currently, ongoing
research focuses on exploring the effective utilization of these generalized
models for specific domains, such as medical imaging. However, in medical
imaging, the lack of training samples due to privacy concerns and other factors
presents a major challenge for applying these generalized models to medical
image segmentation task. To address this issue, the effective fine tuning of
these models is crucial to ensure their optimal utilization. In this study, we
propose to combine a complementary Convolutional Neural Network (CNN) along
with the standard SAM network for medical image segmentation. To reduce the
burden of fine tuning large foundation model and implement cost-efficient
trainnig scheme, we focus only on fine-tuning the additional CNN network and
SAM decoder part. This strategy significantly reduces trainnig time and
achieves competitive results on publicly available dataset. The code is
available at https://github.com/11yxk/SAM-LST.",None,-1
a298e444-5b8f-4204-9490-d54908fc4eb0,StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model,0.614412,"Despite the progress made in the style transfer task, most previous work
focus on transferring only relatively simple features like color or texture,
while missing more abstract concepts such as overall art expression or
painter-specific traits. However, these abstract semantics can be captured by
models like DALL-E or CLIP, which have been trained using huge datasets of
images and textual documents. In this paper, we propose StylerDALLE, a style
transfer method that exploits both of these models and uses natural language to
describe abstract art styles. Specifically, we formulate the language-guided
style transfer task as a non-autoregressive token sequence translation, i.e.,
from input content image to output stylized image, in the discrete latent space
of a large-scale pretrained vector-quantized tokenizer, e.g., the discrete
variational auto-encoder (dVAE) of DALL-E. To incorporate style information, we
propose a Reinforcement Learning strategy with CLIP-based language supervision
that ensures stylization and content preservation simultaneously. Experimental
results demonstrate the superiority of our method, which can effectively
transfer art styles using language instructions at different granularities.
Code is available at https://github.com/zipengxuc/StylerDALLE.",None,-1
602358fb-2e02-4517-aa62-e42cd59f3ffe,Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping,0.556607,"This paper presents a novel visual-LiDAR odometry and mapping method with
low-drift characteristics. The proposed method is based on two popular
approaches, ORB-SLAM and A-LOAM, with monocular scale correction and
visual-bootstrapped LiDAR poses initialization modifications. The scale
corrector calculates the proportion between the depth of image keypoints
recovered by triangulation and that provided by LiDAR, using an outlier
rejection process for accuracy improvement. Concerning LiDAR poses
initialization, the visual odometry approach gives the initial guesses of LiDAR
motions for better performance. This methodology is not only applicable to
high-resolution LiDAR but can also adapt to low-resolution LiDAR. To evaluate
the proposed SLAM system's robustness and accuracy, we conducted experiments on
the KITTI Odometry and S3E datasets. Experimental results illustrate that our
method significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore,
regarding the accuracy of visual odometry with scale correction, our method
performs similarly to the stereo-mode ORB-SLAM2.",None,-1
2c3ffb9e-3614-4592-9615-706255401e01,Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play,0.999662,"Teaching dexterity to multi-fingered robots has been a longstanding challenge
in robotics. Most prominent work in this area focuses on learning controllers
or policies that either operate on visual observations or state estimates
derived from vision. However, such methods perform poorly on fine-grained
manipulation tasks that require reasoning about contact forces or about objects
occluded by the hand itself. In this work, we present T-Dex, a new approach for
tactile-based dexterity, that operates in two phases. In the first phase, we
collect 2.5 hours of play data, which is used to train self-supervised tactile
encoders. This is necessary to bring high-dimensional tactile readings to a
lower-dimensional embedding. In the second phase, given a handful of
demonstrations for a dexterous task, we learn non-parametric policies that
combine the tactile observations with visual ones. Across five challenging
dexterous tasks, we show that our tactile-based dexterity models outperform
purely vision and torque-based models by an average of 1.7X. Finally, we
provide a detailed analysis on factors critical to T-Dex including the
importance of play data, architectures, and representation learning.",None,-1
11b7aa5c-6b18-4fc4-a812-a217e2a2fac6,Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation,0.335981,"Most modern image-based 6D object pose estimation methods learn to predict
2D-3D correspondences, from which the pose can be obtained using a PnP solver.
Because of the non-differentiable nature of common PnP solvers, these methods
are supervised via the individual correspondences. To address this, several
methods have designed differentiable PnP strategies, thus imposing supervision
on the pose obtained after the PnP step. Here, we argue that this conflicts
with the averaging nature of the PnP problem, leading to gradients that may
encourage the network to degrade the accuracy of individual correspondences. To
address this, we derive a loss function that exploits the ground truth pose
before solving the PnP problem. Specifically, we linearize the PnP solver
around the ground-truth pose and compute the covariance of the resulting pose
distribution. We then define our loss based on the diagonal covariance
elements, which entails considering the final pose estimate yet not suffering
from the PnP averaging issue. Our experiments show that our loss consistently
improves the pose estimation accuracy for both dense and sparse correspondence
based methods, achieving state-of-the-art results on both Linemod-Occluded and
YCB-Video.",None,-1
33070bce-9179-47cd-ae23-386b69a32ee9,Prompting Large Language Models for Topic Modeling,0.613611,"Topic modeling is a widely used technique for revealing underlying thematic
structures within textual data. However, existing models have certain
limitations, particularly when dealing with short text datasets that lack
co-occurring words. Moreover, these models often neglect sentence-level
semantics, focusing primarily on token-level semantics. In this paper, we
propose PromptTopic, a novel topic modeling approach that harnesses the
advanced language understanding of large language models (LLMs) to address
these challenges. It involves extracting topics at the sentence level from
individual documents, then aggregating and condensing these topics into a
predefined quantity, ultimately providing coherent topics for texts of varying
lengths. This approach eliminates the need for manual parameter tuning and
improves the quality of extracted topics. We benchmark PromptTopic against the
state-of-the-art baselines on three vastly diverse datasets, establishing its
proficiency in discovering meaningful topics. Furthermore, qualitative analysis
showcases PromptTopic's ability to uncover relevant topics in multiple
datasets.",None,-1
b2f28bc9-8cf8-481f-a1bf-e86b3cbe09ea,Your Day in Your Pocket: Complex Activity Recognition from Smartphone Accelerometers,0.468659,"Human Activity Recognition (HAR) enables context-aware user experiences where
mobile apps can alter content and interactions depending on user activities.
Hence, smartphones have become valuable for HAR as they allow large, and
diversified data collection. Although previous work in HAR managed to detect
simple activities (i.e., sitting, walking, running) with good accuracy using
inertial sensors (i.e., accelerometer), the recognition of complex daily
activities remains an open problem, specially in remote work/study settings
when people are more sedentary. Moreover, understanding the everyday activities
of a person can support the creation of applications that aim to support their
well-being. This paper investigates the recognition of complex activities
exclusively using smartphone accelerometer data. We used a large smartphone
sensing dataset collected from over 600 users in five countries during the
pandemic and showed that deep learning-based, binary classification of eight
complex activities (sleeping, eating, watching videos, online communication,
attending a lecture, sports, shopping, studying) can be achieved with AUROC
scores up to 0.76 with partially personalized models. This shows encouraging
signs toward assessing complex activities only using phone accelerometer data
in the post-pandemic world.",None,-1
8262fbbd-7eb3-4a38-b7a3-4649a4d5c90c,Unbiased organism-agnostic and highly sensitive signal peptide predictor with deep protein language model,0.572113,"Signal peptide (SP) is a short peptide located in the N-terminus of proteins.
It is essential to target and transfer transmembrane and secreted proteins to
correct positions. Compared with traditional experimental methods to identify
signal peptides, computational methods are faster and more efficient, which are
more practical for analyzing thousands or even millions of protein sequences,
especially for metagenomic data. Here we present Unbiased Organism-agnostic
Signal Peptide Network (USPNet), a signal peptide classification and cleavage
site prediction deep learning method that takes advantage of protein language
models. We propose to apply label distribution-aware margin loss to handle data
imbalance problems and use evolutionary information of protein to enrich
representation and overcome species information dependence.",None,-1
4e383edd-a464-410f-84de-4f698355ddf0,"The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising ""Alignment"" in Large Language Models",0.0217728,"In this paper, we address the concept of ""alignment"" in large language models
(LLMs) through the lens of post-structuralist socio-political theory,
specifically examining its parallels to empty signifiers. To establish a shared
vocabulary around how abstract concepts of alignment are operationalised in
empirical datasets, we propose a framework that demarcates: 1) which dimensions
of model behaviour are considered important, then 2) how meanings and
definitions are ascribed to these dimensions, and by whom. We situate existing
empirical literature and provide guidance on deciding which paradigm to follow.
Through this framework, we aim to foster a culture of transparency and critical
evaluation, aiding the community in navigating the complexities of aligning
LLMs with human populations.",None,-1
aafb3da2-fcb2-427a-9c2d-2f4079ef4d21,"Mini-Giants: ""Small"" Language Models and Open Source Win-Win",0.0381123,"ChatGPT is phenomenal. However, it is prohibitively expensive to train and
refine such giant models. Fortunately, small language models are flourishing
and becoming more and more competent. We call them ""mini-giants"". We argue that
open source community like Kaggle and mini-giants will win-win in many ways,
technically, ethically and socially. In this article, we present a brief yet
rich background, discuss how to attain small language models, present a
comparative study of small language models and a brief discussion of evaluation
methods, discuss the application scenarios where small language models are most
needed in the real world, and conclude with discussion and outlook.",None,-1
094988b7-a1e0-42ad-93ff-63d2d3b8bbed,Named entity recognition using GPT for identifying comparable companies,0.41859,"For both public and private firms, comparable companies' analysis is widely
used as a method for company valuation. In particular, the method is of great
value for valuation of private equity companies. The several approaches to the
comparable companies' method usually rely on a qualitative approach to
identifying similar peer companies, which tend to use established industry
classification schemes and/or analyst intuition and knowledge. However, more
quantitative methods have started being used in the literature and in the
private equity industry, in particular, machine learning clustering, and
natural language processing (NLP). For NLP methods, the process consists of
extracting product entities from e.g., the company's website or company
descriptions from some financial database system and then to perform similarity
analysis. Here, using companies' descriptions/summaries from publicly available
companies' Wikipedia websites, we show that using large language models (LLMs),
such as GPT from OpenAI, has a much higher precision and success rate than
using the standard named entity recognition (NER) methods which use manual
annotation. We demonstrate quantitatively a higher precision rate, and show
that, qualitatively, it can be used to create appropriate comparable companies
peer groups which could then be used for equity valuation.",None,-1
13f4d823-91d2-4ae9-a0f1-6c33d7decc66,Document-Level Language Models for Machine Translation,0.610523,"Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.",None,-1
3bacff92-3bbe-4a9c-b2ed-f5305dac6c68,Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve Sentence Embedding,0.191545,"Learning multi-lingual sentence embeddings is a fundamental task in natural
language processing. Recent trends in learning both mono-lingual and
multi-lingual sentence embeddings are mainly based on contrastive learning (CL)
among an anchor, one positive, and multiple negative instances. In this work,
we argue that leveraging multiple positives should be considered for
multi-lingual sentence embeddings because (1) positives in a diverse set of
languages can benefit cross-lingual learning, and (2) transitive similarity
across multiple positives can provide reliable structural information for
learning. In order to investigate the impact of multiple positives in CL, we
propose a novel approach, named MPCL, to effectively utilize multiple positive
instances to improve the learning of multi-lingual sentence embeddings.
Experimental results on various backbone models and downstream tasks
demonstrate that MPCL leads to better retrieval, semantic similarity, and
classification performances compared to conventional CL. We also observe that
in unseen languages, sentence embedding models trained on multiple positives
show better cross-lingual transfer performance than models trained on a single
positive instance.",None,-1
ab8af330-a810-473a-a0cb-380c7542519e,Diffusion-based Document Layout Generation,0.637846,"We develop a diffusion-based approach for various document layout sequence
generation. Layout sequences specify the contents of a document design in an
explicit format. Our novel diffusion-based approach works in the sequence
domain rather than the image domain in order to permit more complex and
realistic layouts. We also introduce a new metric, Document Earth Mover's
Distance (Doc-EMD). By considering similarity between heterogeneous categories
document designs, we handle the shortcomings of prior document metrics that
only evaluate the same category of layouts. Our empirical analysis shows that
our diffusion-based approach is comparable to or outperforming other previous
methods for layout generation across various document datasets. Moreover, our
metric is capable of differentiating documents better than previous metrics for
specific cases.",None,-1
e3a44528-11d6-47bb-b232-8d5158bf629d,ClothCombo: Modeling Inter-Cloth Interaction for Draping Multi-Layered Clothes,0.172302,"We present ClothCombo, a pipeline to drape arbitrary combinations of clothes
on 3D human models with varying body shapes and poses. While existing
learning-based approaches for draping clothes have shown promising results,
multi-layered clothing remains challenging as it is non-trivial to model
inter-cloth interaction. To this end, our method utilizes a GNN-based network
to efficiently model the interaction between clothes in different layers, thus
enabling multi-layered clothing. Specifically, we first create feature
embedding for each cloth using a topology-agnostic network. Then, the draping
network deforms all clothes to fit the target body shape and pose without
considering inter-cloth interaction. Lastly, the untangling network predicts
the per-vertex displacements in a way that resolves interpenetration between
clothes. In experiments, the proposed model demonstrates strong performance in
complex multi-layered scenarios. Being agnostic to cloth topology, our method
can be readily used for layered virtual try-on of real clothes in diverse poses
and combinations of clothes.",None,-1
eb112eb9-1531-4aee-8098-b03f3303fabe,Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters,0.363696,"Speech representations learned in a self-supervised fashion from massive
unlabeled speech corpora have been adapted successfully toward several
downstream tasks. However, such representations may be skewed toward canonical
data characteristics of such corpora and perform poorly on atypical, non-native
accented speaker populations. With the state-of-the-art HuBERT model as a
baseline, we propose and investigate self-supervised adaptation of speech
representations to such populations in a parameter-efficient way via training
accent-specific residual adapters. We experiment with 4 accents and choose
automatic speech recognition (ASR) as the downstream task of interest. We
obtain strong word error rate reductions (WERR) over HuBERT-large for all 4
accents, with a mean WERR of 22.7% with accent-specific adapters and a mean
WERR of 25.1% if the entire encoder is accent-adapted. While our experiments
utilize HuBERT and ASR as the downstream task, our proposed approach is both
model and task-agnostic.",None,-1
f692af78-9dd3-454e-a560-41bba6ee4f52,Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering,0.0884352,"We train a language model (LM) to robustly answer multistep questions by
generating and answering sub-questions. We propose Chain-of-Questions, a
framework that trains a model to generate sub-questions and sub-answers one at
a time by leveraging human annotated question decomposition meaning
representation (QDMR). The key technical challenge is that QDMR only contains
sub-questions but not answers to those sub-questions, so we treat sub-answers
as latent variables and optimize them using a novel dynamic mixture of Hard-EM
and MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods
by 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA
adversarial set, thus demonstrating the effectiveness and robustness of our
framework.",None,-1
784a6845-5eb7-4ec8-bd9d-99ee07fe03a2,Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra,0.769243,"To improve the accuracy of color image completion with missing entries, we
present a recovery method based on generalized higher-order scalars. We extend
the traditional second-order matrix model to a more comprehensive higher-order
matrix equivalent, called the ""t-matrix"" model, which incorporates a pixel
neighborhood expansion strategy to characterize the local pixel constraints.
This ""t-matrix"" model is then used to extend some commonly used matrix and
tensor completion algorithms to their higher-order versions. We perform
extensive experiments on various algorithms using simulated data and algorithms
on simulated data and publicly available images and compare their performance.
The results show that our generalized matrix completion model and the
corresponding algorithm compare favorably with their lower-order tensor and
conventional matrix counterparts.",None,-1
53402105-b075-48db-aecd-fdae1b6a5e1a,A Global Model Approach to Robust Few-Shot SAR Automatic Target Recognition,0.875866,"In real-world scenarios, it may not always be possible to collect hundreds of
labeled samples per class for training deep learning-based SAR Automatic Target
Recognition (ATR) models. This work specifically tackles the few-shot SAR ATR
problem, where only a handful of labeled samples may be available to support
the task of interest. Our approach is composed of two stages. In the first, a
global representation model is trained via self-supervised learning on a large
pool of diverse and unlabeled SAR data. In the second stage, the global model
is used as a fixed feature extractor and a classifier is trained to partition
the feature space given the few-shot support samples, while simultaneously
being calibrated to detect anomalous inputs. Unlike competing approaches which
require a pristine labeled dataset for pretraining via meta-learning, our
approach learns highly transferable features from unlabeled data that have
little-to-no relation to the downstream task. We evaluate our method in
standard and extended MSTAR operating conditions and find it to achieve high
accuracy and robust out-of-distribution detection in many different few-shot
settings. Our results are particularly significant because they show the merit
of a global model approach to SAR ATR, which makes minimal assumptions, and
provides many axes for extendability.",None,-1
2f8f56fb-619c-4d53-bc06-73f07b0a6d20,A novel approach to generate datasets with XAI ground truth to evaluate image models,0.200777,"With the increased usage of artificial intelligence (AI), it is imperative to
understand how these models work internally. These needs have led to the
development of a new field called eXplainable artificial intelligence (XAI).
This field consists of on a set of techniques that allows us to theoretically
determine the cause of the AI decisions. One main issue of XAI is how to verify
the works on this field, taking into consideration the lack of ground truth
(GT). In this study, we propose a new method to generate datasets with GT. We
conducted a set of experiments that compared our GT with real model
explanations and obtained excellent results confirming that our proposed method
is correct.",None,-1
7165044c-9738-4b53-b0f4-874651d871b8,Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators,0.692428,"The recent success of Large Language Models (LLMs) signifies an impressive
stride towards artificial general intelligence. They have shown a promising
prospect in automatically completing tasks upon user instructions, functioning
as brain-like coordinators. The associated risks will be revealed as we
delegate an increasing number of tasks to machines for automated completion. A
big question emerges: how can we make machines behave responsibly when helping
humans automate tasks as personal copilots? In this paper, we explore this
question in depth from the perspectives of feasibility, completeness and
security. In specific, we present Responsible Task Automation (ResponsibleTA)
as a fundamental framework to facilitate responsible collaboration between
LLM-based coordinators and executors for task automation with three empowered
capabilities: 1) predicting the feasibility of the commands for executors; 2)
verifying the completeness of executors; 3) enhancing the security (e.g., the
protection of users' privacy). We further propose and compare two paradigms for
implementing the first two capabilities. One is to leverage the generic
knowledge of LLMs themselves via prompt engineering while the other is to adopt
domain-specific learnable models. Moreover, we introduce a local memory
mechanism for achieving the third capability. We evaluate our proposed
ResponsibleTA on UI task automation and hope it could bring more attentions to
ensuring LLMs more responsible in diverse scenarios.",None,-1
45dad6c7-cf35-4fd4-8855-68a72e925204,A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI),0.808963,"Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.",None,-1
2c8537cd-8106-425a-8b37-e268af0b113c,Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain,0.523763,"Many NLP tasks, although well-resolved for general English, face challenges
in specific domains like fantasy literature. This is evident in Named Entity
Recognition (NER), which detects and categorizes entities in text. We analyzed
10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess
domain-specific performance. Using open-source Large Language Models, we
annotated named entities in these books and evaluated each model's precision.
Our findings indicate that, without modifications, Flair, Trankit, and Spacy
outperform others in identifying named entities in the D&D context.",None,-1
1a372e6e-eca0-4594-8276-001de39cb736,A computational framework of human values for ethical AI,0.21032,"In the diverse array of work investigating the nature of human values from
psychology, philosophy and social sciences, there is a clear consensus that
values guide behaviour. More recently, a recognition that values provide a
means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed
shifting AI's focus away from simply ``intelligence'' towards intelligence
``provably aligned with human values''. This challenge -- the value alignment
problem -- with others including an AI's learning of human values, aggregating
individual values to groups, and designing computational mechanisms to reason
over values, has energised a sustained research effort. Despite this, no
formal, computational definition of values has yet been proposed. We address
this through a formal conceptual framework rooted in the social sciences, that
provides a foundation for the systematic, integrated and interdisciplinary
investigation into how human values can support designing ethical AI.",None,-1
3f488bda-df2c-40b7-8417-2fd0c062f5a2,Learning Efficient Representations for Image-Based Patent Retrieval,0.264303,"Patent retrieval has been attracting tremendous interest from researchers in
intellectual property and information retrieval communities in the past
decades. However, most existing approaches rely on textual and metadata
information of the patent, and content-based image-based patent retrieval is
rarely investigated. Based on traits of patent drawing images, we present a
simple and lightweight model for this task. Without bells and whistles, this
approach significantly outperforms other counterparts on a large-scale
benchmark and noticeably improves the state-of-the-art by 33.5% with the mean
average precision (mAP) score. Further experiments reveal that this model can
be elaborately scaled up to achieve a surprisingly high mAP of 93.5%. Our
method ranks first in the ECCV 2022 Patent Diagram Image Retrieval Challenge.",None,-1
0225f91b-2cd1-456e-bfd0-7bb9efa5032b,A Multiagent CyberBattleSim for RL Cyber Operation Agents,0.687528,"Hardening cyber physical assets is both crucial and labor-intensive.
Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more
specifically has shown great promise to automate tasks that otherwise would
require significant human insight/intelligence. The development of autonomous
RL agents requires a suitable training environment that allows us to quickly
evaluate various alternatives, in particular how to arrange training scenarios
that pit attackers and defenders against each other. CyberBattleSim is a
training environment that supports the training of red agents, i.e., attackers.
We added the capability to train blue agents, i.e., defenders. The paper
describes our changes and reports on the results we obtained when training blue
agents, either in isolation or jointly with red agents. Our results show that
training a blue agent does lead to stronger defenses against attacks. In
particular, training a blue agent jointly with a red agent increases the blue
agent's capability to thwart sophisticated red agents.",None,-1
c97620f7-de25-4a41-8711-43b6bcd54b06,"Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",0.699482,"The use of large language models (LLMs) in healthcare is gaining popularity,
but their practicality and safety in clinical settings have not been thoroughly
assessed. In high-stakes environments like medical settings, trust and safety
are critical issues for LLMs. To address these concerns, we present an approach
to evaluate the performance and trustworthiness of a GPT3.5 model for medical
image protocol assignment. We compare it with a fine-tuned BERT model and a
radiologist. In addition, we have a radiologist review the GPT3.5 output to
evaluate its decision-making process. Our evaluation dataset consists of 4,700
physician entries across 11 imaging protocol classes spanning the entire head.
Our findings suggest that the GPT3.5 performance falls behind BERT and a
radiologist. However, GPT3.5 outperforms BERT in its ability to explain its
decision, detect relevant word indicators, and model calibration. Furthermore,
by analyzing the explanations of GPT3.5 for misclassifications, we reveal
systematic errors that need to be resolved to enhance its safety and
suitability for clinical use.",None,-1
74df6861-504a-4178-a27d-7f1e99515462,Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data,0.612433,"Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., ""an image of a <category>"", and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.",None,-1
310f5bd6-2f2a-4eb7-92a8-fde964f73a2a,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,0.749927,"Although large language models (LLMs) are often pre-trained on large-scale
multilingual texts, their reasoning abilities and real-world knowledge are
mainly evaluated based on English datasets. Assessing LLM capabilities beyond
English is increasingly vital but hindered due to the lack of suitable
datasets. In this work, we introduce IndoMMLU, the first multi-task language
understanding benchmark for Indonesian culture and languages, which consists of
questions from primary school to university entrance exams in Indonesia. By
employing professional teachers, we obtain 14,981 questions across 64 tasks and
education levels, with 46% of the questions focusing on assessing proficiency
in the Indonesian language and knowledge of nine local languages and cultures
in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass
the Indonesian primary school level, with limited knowledge of local Indonesian
languages and culture. Other smaller models such as BLOOMZ and Falcon perform
at even lower levels.",None,-1
9b8b91ac-2511-4207-9553-06abcc48c3d3,CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer,0.760784,"Content affinity loss including feature and pixel affinity is a main problem
which leads to artifacts in photorealistic and video style transfer. This paper
proposes a new framework named CAP-VSTNet, which consists of a new reversible
residual network and an unbiased linear transform module, for versatile style
transfer. This reversible residual network can not only preserve content
affinity but not introduce redundant information as traditional reversible
networks, and hence facilitate better stylization. Empowered by Matting
Laplacian training loss which can address the pixel affinity loss problem led
by the linear transform, the proposed framework is applicable and effective on
versatile style transfer. Extensive experiments show that CAP-VSTNet can
produce better qualitative and quantitative results in comparison with the
state-of-the-art methods.",None,-1
8afcbf8a-7209-4770-9fd6-8f3d17de9dc2,Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,0.58271,"Large language models (LLMs) have demonstrated their potential in social
science research by emulating human perceptions and behaviors, a concept
referred to as algorithmic fidelity. This study assesses the algorithmic
fidelity and bias of LLMs by utilizing two nationally representative climate
change surveys. The LLMs were conditioned on demographics and/or psychological
covariates to simulate survey responses. The findings indicate that LLMs can
effectively capture presidential voting behaviors but encounter challenges in
accurately representing global warming perspectives when relevant covariates
are not included. GPT-4 exhibits improved performance when conditioned on both
demographics and covariates. However, disparities emerge in LLM estimations of
the views of certain groups, with LLMs tending to underestimate worry about
global warming among Black Americans. While highlighting the potential of LLMs
to aid social science research, these results underscore the importance of
meticulous conditioning, model selection, survey question format, and bias
assessment when employing LLMs for survey simulation. Further investigation
into prompt engineering and algorithm auditing is essential to harness the
power of LLMs while addressing their inherent limitations.",None,-1
e2d19006-1050-447f-837a-1c2be2d62c4f,Triplet Knowledge Distillation,0.291629,"In Knowledge Distillation, the teacher is generally much larger than the
student, making the solution of the teacher likely to be difficult for the
student to learn. To ease the mimicking difficulty, we introduce a triplet
knowledge distillation mechanism named TriKD. Besides teacher and student,
TriKD employs a third role called anchor model. Before distillation begins, the
pre-trained anchor model delimits a subspace within the full solution space of
the target problem. Solutions within the subspace are expected to be easy
targets that the student could mimic well. Distillation then begins in an
online manner, and the teacher is only allowed to express solutions within the
aforementioned subspace. Surprisingly, benefiting from accurate but
easy-to-mimic hints, the student can finally perform well. After the student is
well trained, it can be used as the new anchor for new students, forming a
curriculum learning strategy. Our experiments on image classification and face
recognition with various models clearly demonstrate the effectiveness of our
method. Furthermore, the proposed TriKD is also effective in dealing with the
overfitting issue. Moreover, our theoretical analysis supports the rationality
of our triplet distillation.",None,-1
49ff3b86-c16b-4ed0-9e01-80fd83c26485,Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment,0.612607,"Open Information Extraction (OIE) aims to extract objective structured
knowledge from natural texts, which has attracted growing attention to build
dedicated models with human experience. As the large language models (LLMs)
have exhibited remarkable in-context learning capabilities, a question arises
as to whether the task of OIE can be effectively tackled with this paradigm? In
this paper, we explore solving the OIE problem by constructing an appropriate
reasoning environment for LLMs. Specifically, we first propose a method to
effectively estimate the discrepancy of syntactic distribution between a LLM
and test samples, which can serve as correlation evidence for preparing
positive demonstrations. Upon the evidence, we introduce a simple yet effective
mechanism to establish the reasoning environment for LLMs on specific tasks.
Without bells and whistles, experimental results on the standard CaRB benchmark
demonstrate that our $6$-shot approach outperforms state-of-the-art supervised
method, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and
ACE05 show that our method can naturally generalize to other information
extraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,
respectively.",None,-1
e295caf9-2f38-481e-b650-b04c774889b4,Logarithm-transform aided Gaussian Sampling for Few-Shot Learning,0.0855107,"Few-shot image classification has recently witnessed the rise of
representation learning being utilised for models to adapt to new classes using
only a few training examples. Therefore, the properties of the representations,
such as their underlying probability distributions, assume vital importance.
Representations sampled from Gaussian distributions have been used in recent
works, [19] to train classifiers for few-shot classification. These methods
rely on transforming the distributions of experimental data to approximate
Gaussian distributions for their functioning. In this paper, I propose a novel
Gaussian transform, that outperforms existing methods on transforming
experimental data into Gaussian-like distributions. I then utilise this novel
transformation for few-shot image classification and show significant gains in
performance, while sampling lesser data.",None,-1
1b158085-e057-4c27-bc94-97bc1b33a800,Orthogonal Annotation Benefits Barely-supervised Medical Image Segmentation,0.540742,"Recent trends in semi-supervised learning have significantly boosted the
performance of 3D semi-supervised medical image segmentation. Compared with 2D
images, 3D medical volumes involve information from different directions, e.g.,
transverse, sagittal, and coronal planes, so as to naturally provide
complementary views. These complementary views and the intrinsic similarity
among adjacent 3D slices inspire us to develop a novel annotation way and its
corresponding semi-supervised model for effective segmentation. Specifically,
we firstly propose the orthogonal annotation by only labeling two orthogonal
slices in a labeled volume, which significantly relieves the burden of
annotation. Then, we perform registration to obtain the initial pseudo labels
for sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,
we propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that
exploits dense pseudo labels in early stage and sparse labels in later stage
and meanwhile forces consistent output of two networks. Experimental results on
three benchmark datasets validated our effectiveness in performance and
efficiency in annotation. For example, with only 10 annotated slices, our
method reaches a Dice up to 86.93% on KiTS19 dataset.",None,-1
a547e9b1-ce4b-493c-9683-70d13a955f12,ArcGPT: A Large Language Model Tailored for Real-world Archival Applications,0.378363,"Archives play a crucial role in preserving information and knowledge, and the
exponential growth of such data necessitates efficient and automated tools for
managing and utilizing archive information resources. Archival applications
involve managing massive data that are challenging to process and analyze.
Although LLMs have made remarkable progress in diverse domains, there are no
publicly available archives tailored LLM. Addressing this gap, we introduce
ArcGPT, to our knowledge, the first general-purpose LLM tailored to the
archival field. To enhance model performance on real-world archival tasks,
ArcGPT has been pre-trained on massive and extensive archival domain data.
Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world
archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing
state-of-the-art models, marking a substantial step forward in effective
archival data management. Ultimately, ArcGPT aims to better serve the archival
community, aiding archivists in their crucial role of preserving and harnessing
our collective information and knowledge.",None,-1
db66ead5-d18e-432f-b7fc-8be39b61f506,Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models,0.688304,"Fine-tuning large models is highly effective, however, inference can be
expensive and produces carbon emissions. Knowledge distillation has been shown
to be a practical solution to reduce inference costs, but the distillation
process itself requires significant computational resources. Rather than buying
or renting GPUs to fine-tune, then distill a large model, an NLP practitioner
might instead choose to allocate the available budget to hire annotators and
manually label additional fine-tuning data. In this paper, we investigate how
to most efficiently use a fixed budget to build a compact model. Through
extensive experiments on six diverse tasks, we show that distilling from T5-XXL
(11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to
annotating more data to directly train a compact model (T5-Small). We further
investigate how the optimal budget allocated towards computation varies across
scenarios. We will make our code, datasets, annotation cost estimates, and
baseline models available as a benchmark to support further work on
cost-efficient training of compact models.",None,-1
daf17e00-a7dc-4db4-a39c-60032c5a47c6,DANES: Deep Neural Network Ensemble Architecture for Social and Textual Context-aware Fake News Detection,0.567826,"The growing popularity of social media platforms has simplified the creation
and distribution of news articles but also creates a conduit for spreading fake
news. In consequence, the need arises for effective context-aware fake news
detection mechanisms, where the contextual information can be built either from
the textual content of posts or from available social data (e.g., information
about the users, reactions to posts, or the social network). In this paper, we
propose DANES, a Deep Neural Network Ensemble Architecture for Social and
Textual Context-aware Fake News Detection. DANES comprises a Text Branch for a
textual content-based context and a Social Branch for the social context. These
two branches are used to create a novel Network Embedding. Preliminary ablation
results on 3 real-world datasets, i.e., BuzzFace, Twitter15, and Twitter16, are
promising, with an accuracy that outperforms state-of-the-art solutions when
employing both social and textual content features.",None,-1
dd9f3104-8d50-4ec6-a34a-f490b68d293c,Interpretable Deep Learning for Forecasting Online Advertising Costs: Insights from the Competitive Bidding Landscape,0.317826,"As advertisers increasingly shift their budgets toward digital advertising,
forecasting advertising costs is essential for making budget plans to optimize
marketing campaign returns. In this paper, we perform a comprehensive study
using a variety of time-series forecasting methods to predict daily average
cost-per-click (CPC) in the online advertising market. We show that forecasting
advertising costs would benefit from multivariate models using covariates from
competitors' CPC development identified through time-series clustering. We
further interpret the results by analyzing feature importance and temporal
attention. Finally, we show that our approach has several advantages over
models that individual advertisers might build based solely on their collected
data.",None,-1
b69e889e-4831-4bfb-9b76-5fc37bd011d7,QuAVF: Quality-aware Audio-Visual Fusion for Ego4D Talking to Me Challenge,0.113046,"This technical report describes our QuAVF@NTU-NVIDIA submission to the Ego4D
Talking to Me (TTM) Challenge 2023. Based on the observation from the TTM task
and the provided dataset, we propose to use two separate models to process the
input videos and audio. By doing so, we can utilize all the labeled training
data, including those without bounding box labels. Furthermore, we leverage the
face quality score from a facial landmark prediction model for filtering noisy
face input data. The face quality score is also employed in our proposed
quality-aware fusion for integrating the results from two branches. With the
simple architecture design, our model achieves 67.4% mean average precision
(mAP) on the test set, which ranks first on the leaderboard and outperforms the
baseline method by a large margin. Code is available at:
https://github.com/hsi-che-lin/Ego4D-QuAVF-TTM-CVPR23",None,-1
704eed5e-907d-48ec-b5b7-19b65e33dbf9,Does progress on ImageNet transfer to real-world datasets?,0.459414,"Does progress on ImageNet transfer to real-world datasets? We investigate
this question by evaluating ImageNet pre-trained models with varying accuracy
(57% - 83%) on six practical image classification datasets. In particular, we
study datasets collected with the goal of solving real-world tasks (e.g.,
classifying images from camera traps or satellites), as opposed to web-scraped
benchmarks collected for comparing models. On multiple datasets, models with
higher ImageNet accuracy do not consistently yield performance improvements.
For certain tasks, interventions such as data augmentation improve performance
even when architectures do not. We hope that future benchmarks will include
more diverse datasets to encourage a more comprehensive approach to improving
learning algorithms.",None,-1
7b0a2700-c8be-46d7-8801-625be12007f3,Are UD Treebanks Getting More Consistent? A Report Card for English UD,0.207507,"Recent efforts to consolidate guidelines and treebanks in the Universal
Dependencies project raise the expectation that joint training and dataset
comparison is increasingly possible for high-resource languages such as
English, which have multiple corpora. Focusing on the two largest UD English
treebanks, we examine progress in data consolidation and answer several
questions: Are UD English treebanks becoming more internally consistent? Are
they becoming more like each other and to what extent? Is joint training a good
idea, and if so, since which UD version? Our results indicate that while
consolidation has made progress, joint models may still suffer from
inconsistencies, which hamper their ability to leverage a larger pool of
training data.",None,-1
6b807e64-e4ae-4c16-b4c2-f45650877e10,Dynamic Residual Classifier for Class Incremental Learning,0.704187,"The rehearsal strategy is widely used to alleviate the catastrophic
forgetting problem in class incremental learning (CIL) by preserving limited
exemplars from previous tasks. With imbalanced sample numbers between old and
new classes, the classifier learning can be biased. Existing CIL methods
exploit the long-tailed (LT) recognition techniques, e.g., the adjusted losses
and the data re-sampling methods, to handle the data imbalance issue within
each increment task. In this work, the dynamic nature of data imbalance in CIL
is shown and a novel Dynamic Residual Classifier (DRC) is proposed to handle
this challenging scenario. Specifically, DRC is built upon a recent advance
residual classifier with the branch layer merging to handle the model-growing
problem. Moreover, DRC is compatible with different CIL pipelines and
substantially improves them. Combining DRC with the model adaptation and fusion
(MAF) pipeline, this method achieves state-of-the-art results on both the
conventional CIL and the LT-CIL benchmarks. Extensive experiments are also
conducted for a detailed analysis. The code is publicly available.",None,-1
7cb061ed-0012-4c41-abb9-925dad666b9e,Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization,0.593385,"End-to-end speech summarization (E2E SSum) directly summarizes input speech
into easy-to-read short sentences with a single model. This approach is
promising because it, in contrast to the conventional cascade approach, can
utilize full acoustical information and mitigate to the propagation of
transcription errors. However, due to the high cost of collecting
speech-summary pairs, an E2E SSum model tends to suffer from training data
scarcity and output unnatural sentences. To overcome this drawback, we propose
for the first time to integrate a pre-trained language model (LM), which is
highly capable of generating natural sentences, into the E2E SSum decoder via
transfer learning. In addition, to reduce the gap between the independently
pre-trained encoder and decoder, we also propose to transfer the baseline E2E
SSum encoder instead of the commonly used automatic speech recognition encoder.
Experimental results show that the proposed model outperforms baseline and data
augmented models.",None,-1
6dfd82e2-02c9-4437-a324-37acb489e243,LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following,0.661069,"E-commerce authoring entails creating engaging, diverse, and targeted content
to enhance preference elicitation and retrieval experience. While Large
Language Models (LLMs) have revolutionized content generation, they often fall
short in e-commerce applications due to their limited memorization of
domain-specific features. This paper proposes LLaMA-E, the unified e-commerce
authoring models that address the contextual preferences of customers, sellers,
and platforms, the essential objects in e-commerce operation. We design the
instruction set derived from tasks of ads generation, query-enhanced product
title rewriting, product classification, purchase intent speculation, and
general e-commerce Q&A. The instruction formulation ensures the interleaved
cover of the presented and required object features, allowing the alignment of
base models to parameterise e-commerce knowledge comprehensively. The proposed
LLaMA-E models achieve state-of-the-art evaluation performance and exhibit the
advantage in zero-shot practical applications. To our knowledge, this is the
first LLM tailored to empower authoring applications with comprehensive
scenario understanding by integrating features focused on participated objects.",None,-1
e9e82884-f3d3-42fa-a5c4-80fdb03e572c,Do Differences in Values Influence Disagreements in Online Discussions?,0.896703,"Disagreements are common in online discussions. Disagreement may foster
collaboration and improve the quality of a discussion under some conditions.
Although there exist methods for recognizing disagreement, a deeper
understanding of factors that influence disagreement is lacking in the
literature. We investigate a hypothesis that differences in personal values are
indicative of disagreement in online discussions. We show how state-of-the-art
models can be used for estimating values in online discussions and how the
estimated values can be aggregated into value profiles. We evaluate the
estimated value profiles based on human-annotated agreement labels. We find
that the dissimilarity of value profiles correlates with disagreement in
specific cases. We also find that including value information in agreement
prediction improves performance.",None,-1
805e66ca-9717-4007-8636-de49ede021c0,LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for Real Time Semantic Grid Prediction,0.282022,"Semantic grids can be useful representations of the scene around an
autonomous system. By having information about the layout of the space around
itself, a robot can leverage this type of representation for crucial tasks such
as navigation or tracking. By fusing information from multiple sensors,
robustness can be increased and the computational load for the task can be
lowered, achieving real time performance. Our multi-scale LiDAR-Aided
Perspective Transform network uses information available in point clouds to
guide the projection of image features to a top-view representation, resulting
in a relative improvement in the state of the art for semantic grid generation
for human (+8.67%) and movable object (+49.07%) classes in the nuScenes
dataset, as well as achieving results close to the state of the art for the
vehicle, drivable area and walkway classes, while performing inference at 25
FPS.",None,-1
c87fc242-a18a-4ee2-a93b-72e73b394429,Learning a Depth Covariance Function,0.248531,"We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.",None,-1
b88f11c9-094e-466a-a4cc-889727b053e9,Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature,0.824431,"Large language models (LLMs) have shown the ability to produce fluent and
cogent content, presenting both productivity opportunities and societal risks.
To build trustworthy AI systems, it is imperative to distinguish between
machine-generated and human-authored content. The leading zero-shot detector,
DetectGPT, showcases commendable performance but is marred by its intensive
computational costs. In this paper, we introduce the concept of conditional
probability curvature to elucidate discrepancies in word choices between LLMs
and humans within a given context. Utilizing this curvature as a foundational
metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which
substitutes DetectGPT's perturbation step with a more efficient sampling step.
Our evaluations on various datasets, source models, and test conditions
indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around
75% in both the white-box and black-box settings but also accelerates the
detection process by a factor of 340, as detailed in Table 1. See
\url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and
results.",None,-1
0590a8e3-987f-42f1-853a-8fb4e7a30106,Toward Mesh-Invariant 3D Generative Deep Learning with Geometric Measures,0.250501,"3D generative modeling is accelerating as the technology allowing the capture
of geometric data is developing. However, the acquired data is often
inconsistent, resulting in unregistered meshes or point clouds. Many generative
learning algorithms require correspondence between each point when comparing
the predicted shape and the target shape. We propose an architecture able to
cope with different parameterizations, even during the training phase. In
particular, our loss function is built upon a kernel-based metric over a
representation of meshes using geometric measures such as currents and
varifolds. The latter allows to implement an efficient dissimilarity measure
with many desirable properties such as robustness to resampling of the mesh or
point cloud. We demonstrate the efficiency and resilience of our model with a
generative learning task of human faces.",None,-1
10483fb6-f8f6-46db-a34f-e683a98f72c6,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,0.753719,"Visual question answering on document images that contain textual, visual,
and layout information, called document VQA, has received much attention
recently. Although many datasets have been proposed for developing document VQA
systems, most of the existing datasets focus on understanding the content
relationships within a single image and not across multiple images. In this
study, we propose a new multi-image document VQA dataset, SlideVQA, containing
2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a
slide deck. SlideVQA requires complex reasoning, including single-hop,
multi-hop, and numerical reasoning, and also provides annotated arithmetic
expressions of numerical answers for enhancing the ability of numerical
reasoning. Moreover, we developed a new end-to-end document VQA model that
treats evidence selection and question answering in a unified
sequence-to-sequence format. Experiments on SlideVQA show that our model
outperformed existing state-of-the-art QA models, but that it still has a large
gap behind human performance. We believe that our dataset will facilitate
research on document VQA.",None,-1
0695110b-4093-484f-b87f-f6268e78781c,Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation,0.789349,"Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a ""Game-of-Thoughts"". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.",None,-1
ee64ff08-ac47-4cbc-a9cb-770b4245b996,Active hypothesis testing in unknown environments using recurrent neural networks and model free reinforcement learning,0.143515,"A combination of deep reinforcement learning and supervised learning is
proposed for the problem of active sequential hypothesis testing in completely
unknown environments. We make no assumptions about the prior probability, the
action and observation sets, and the observation generating process. Our method
can be used in any environment even if it has continuous observations or
actions, and performs competitively and sometimes better than the Chernoff
test, in both finite and infinite horizon problems, despite not having access
to the environment dynamics.",None,-1
a6be6fe8-bfca-4fe2-955e-1fb543a1a907,CONTAIN: A Community-based Algorithm for Network Immunization,0.482194,"Network immunization is an automated task in the field of network analysis
that involves protecting a network (modeled as a graph) from being infected by
an undesired arbitrary diffusion. In this article, we consider the spread of
harmful content in social networks, and we propose CONTAIN, a novel
COmmuNiTy-based Algorithm for network ImmuNization. Our solution uses the
network information to (1) detect harmful content spreaders, and (2) generate
partitions and rank them for immunization using the subgraphs induced by each
spreader, i.e., employing CONTAIN. The experimental results obtained on
real-world datasets show that CONTAIN outperforms state-of-the-art solutions,
i.e., NetShield and SparseShield, by immunizing the network in fewer
iterations, thus, converging significantly faster than the state-of-the-art
algorithms. We also compared our solution in terms of scalability with the
state-of-the-art tree-based mitigation algorithm MCWDST, as well as with
NetShield and SparseShield. We can conclude that our solution outperforms
MCWDST and NetShield.",None,-1
1aeaa12d-7982-4763-9396-7f48ae25b1ce,ACC-UNet: A Completely Convolutional UNet model for the 2020s,0.726884,"This decade is marked by the introduction of Vision Transformer, a radical
paradigm shift in broad computer vision. A similar trend is followed in medical
imaging, UNet, one of the most influential architectures, has been redesigned
with transformers. Recently, the efficacy of convolutional models in vision is
being reinvestigated by seminal works such as ConvNext, which elevates a ResNet
to Swin Transformer level. Deriving inspiration from this, we aim to improve a
purely convolutional UNet model so that it can be on par with the
transformer-based models, e.g, Swin-Unet or UCTransNet. We examined several
advantages of the transformer-based UNet models, primarily long-range
dependencies and cross-level skip connections. We attempted to emulate them
through convolution operations and thus propose, ACC-UNet, a completely
convolutional UNet model that brings the best of both worlds, the inherent
inductive biases of convnets with the design decisions of transformers.
ACC-UNet was evaluated on 5 different medical image segmentation benchmarks and
consistently outperformed convnets, transformers, and their hybrids. Notably,
ACC-UNet outperforms state-of-the-art models Swin-Unet and UCTransNet by $2.64
\pm 2.54\%$ and $0.45 \pm 1.61\%$ in terms of dice score, respectively, while
using a fraction of their parameters ($59.26\%$ and $24.24\%$). Our codes are
available at https://github.com/kiharalab/ACC-UNet.",None,-1
548c7c1a-f9e7-48ed-a6eb-96148c1a19a2,Hyperbolic Audio-visual Zero-shot Learning,0.239753,"Audio-visual zero-shot learning aims to classify samples consisting of a pair
of corresponding audio and video sequences from classes that are not present
during training. An analysis of the audio-visual data reveals a large degree of
hyperbolicity, indicating the potential benefit of using a hyperbolic
transformation to achieve curvature-aware geometric learning, with the aim of
exploring more complex hierarchical data structures for this task. The proposed
approach employs a novel loss function that incorporates cross-modality
alignment between video and audio features in the hyperbolic space.
Additionally, we explore the use of multiple adaptive curvatures for hyperbolic
projections. The experimental results on this very challenging task demonstrate
that our proposed hyperbolic approach for zero-shot learning outperforms the
SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL
achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,
respectively.",None,-1
262143b3-20d1-42da-9952-38e120ee3a38,Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning,0.764459,"A general framework of unsupervised learning for combinatorial optimization
(CO) is to train a neural network (NN) whose output gives a problem solution by
directly optimizing the CO objective. Albeit with some advantages over
traditional solvers, the current framework optimizes an averaged performance
over the distribution of historical problem instances, which misaligns with the
actual goal of CO that looks for a good solution to every future encountered
instance. With this observation, we propose a new objective of unsupervised
learning for CO where the goal of learning is to search for good initialization
for future problem instances rather than give direct solutions. We propose a
meta-learning-based training pipeline for this new objective. Our method
achieves good empirical performance. We observe that even just the initial
solution given by our model before fine-tuning can significantly outperform the
baselines under various evaluation settings including evaluation across
multiple datasets, and the case with big shifts in the problem scale. The
reason we conjecture is that meta-learning-based training lets the model be
loosely tied to each local optima for a training instance while being more
adaptive to the changes of optimization landscapes across instances.",None,-1
0af657c5-3b3e-41ad-aedf-151718c94d97,Harnessing Large Language Models' Empathetic Response Generation Capabilities for Online Mental Health Counselling Support,0.978513,"Large Language Models (LLMs) have demonstrated remarkable performance across
various information-seeking and reasoning tasks. These computational systems
drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also
carry substantial promise in meeting the growing demands of mental health care,
albeit relatively unexplored. As such, this study sought to examine LLMs'
capability to generate empathetic responses in conversations that emulate those
in a mental health counselling setting. We selected five LLMs: version 3.5 and
version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways
Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple
instructional prompt, these models responded to utterances derived from the
EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we
compared their responses to those from traditional response generation dialogue
systems, which were fine-tuned on the ED dataset, along with human-generated
responses. Notably, we discovered that responses from the LLMs were remarkably
more empathetic in most scenarios. We position our findings in light of
catapulting advancements in creating empathetic conversational systems.",None,-1
6d23c438-a719-47b3-8d39-c9870412f1eb,Parameter-efficient Modularised Bias Mitigation via AdapterFusion,0.857569,"Large pre-trained language models contain societal biases and carry along
these biases to downstream tasks. Current in-processing bias mitigation
approaches (like adversarial training) impose debiasing by updating a model's
parameters, effectively transferring the model to a new, irreversible debiased
state. In this work, we propose a novel approach to develop stand-alone
debiasing functionalities separate from the model, which can be integrated into
the model on-demand, while keeping the core model untouched. Drawing from the
concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing
with Adapter Modules) - a debiasing approach to first encapsulate arbitrary
bias mitigation functionalities into separate adapters, and then add them to
the model on-demand in order to deliver fairness qualities. We conduct a large
set of experiments on three classification tasks with gender, race, and age as
protected attributes. Our results show that DAM improves or maintains the
effectiveness of bias mitigation, avoids catastrophic forgetting in a
multi-attribute scenario, and maintains on-par task performance, while granting
parameter-efficiency and easy switching between the original and debiased
models.",None,-1
e7c145c1-5a7a-49de-af3f-a9a9a39917a5,Incentives to Offer Algorithmic Recourse,0.159465,"Due to the importance of artificial intelligence (AI) in a variety of
high-stakes decisions, such as loan approval, job hiring, and criminal bail,
researchers in Explainable AI (XAI) have developed algorithms to provide users
with recourse for an unfavorable outcome. We analyze the incentives for a
decision-maker to offer recourse to a set of applicants. Does the
decision-maker have the incentive to offer recourse to all rejected applicants?
We show that the decision-maker only offers recourse to all applicants in
extreme cases, such as when the recourse process is impossible to manipulate.
Some applicants may be worse off when the decision-maker can offer recourse.",None,-1
d85e4c06-145e-4d70-9d2b-1ed799b85c2e,Can We Edit Multimodal Large Language Models?,0.378592,"In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.",None,-1
6484a738-ac6a-4fa5-81c2-f45cbb4c25a2,Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization,0.0979567,"Anomaly detection has a wide range of applications and is especially
important in industrial quality inspection. Currently, many top-performing
anomaly-detection models rely on feature-embedding methods. However, these
methods do not perform well on datasets with large variations in object
locations. Reconstruction-based methods use reconstruction errors to detect
anomalies without considering positional differences between samples. In this
study, a reconstruction-based method using the noise-to-norm paradigm is
proposed, which avoids the invariant reconstruction of anomalous regions. Our
reconstruction network is based on M-net and incorporates multiscale fusion and
residual attention modules to enable end-to-end anomaly detection and
localization. Experiments demonstrate that the method is effective in
reconstructing anomalous regions into normal patterns and achieving accurate
anomaly detection and localization. On the MPDD and VisA datasets, our proposed
method achieved more competitive results than the latest methods, and it set a
new state-of-the-art standard on the MPDD dataset.",None,-1
7f4cc4fa-a305-486e-be2d-b4d1cd8d5cd9,Non-autoregressive Machine Translation with Probabilistic Context-free Grammar,0.420422,"Non-autoregressive Transformer(NAT) significantly accelerates the inference
of neural machine translation. However, conventional NAT models suffer from
limited expression power and performance degradation compared to autoregressive
(AT) models due to the assumption of conditional independence among target
tokens. To address these limitations, we propose a novel approach called
PCFG-NAT, which leverages a specially designed Probabilistic Context-Free
Grammar (PCFG) to enhance the ability of NAT models to capture complex
dependencies among output tokens. Experimental results on major machine
translation benchmarks demonstrate that PCFG-NAT further narrows the gap in
translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a
deeper understanding of the generated sentences, addressing the lack of
satisfactory explainability in neural machine translation.Code is publicly
available at https://github.com/ictnlp/PCFG-NAT.",None,-1
d434d0a2-0327-4abe-9a1b-53ca354218c0,Adversarial Ink: Componentwise Backward Error Attacks on Deep Learning,0.435374,"Deep neural networks are capable of state-of-the-art performance in many
classification tasks. However, they are known to be vulnerable to adversarial
attacks -- small perturbations to the input that lead to a change in
classification. We address this issue from the perspective of backward error
and condition number, concepts that have proved useful in numerical analysis.
To do this, we build on the work of Beuzeville et al. (2021). In particular, we
develop a new class of attack algorithms that use componentwise relative
perturbations. Such attacks are highly relevant in the case of handwritten
documents or printed texts where, for example, the classification of
signatures, postcodes, dates or numerical quantities may be altered by changing
only the ink consistency and not the background. This makes the perturbed
images look natural to the naked eye. Such ``adversarial ink'' attacks
therefore reveal a weakness that can have a serious impact on safety and
security. We illustrate the new attacks on real data and contrast them with
existing algorithms. We also study the use of a componentwise condition number
to quantify vulnerability.",None,-1
215e015e-b2b5-4177-b18b-6d9ea6a75d86,Multimodal Question Answering for Unified Information Extraction,0.0823407,"Multimodal information extraction (MIE) aims to extract structured
information from unstructured multimedia content. Due to the diversity of tasks
and settings, most current MIE models are task-specific and data-intensive,
which limits their generalization to real-world scenarios with diverse task
requirements and limited labeled data. To address these issues, we propose a
novel multimodal question answering (MQA) framework to unify three MIE tasks by
reformulating them into a unified span extraction and multi-choice QA pipeline.
Extensive experiments on six datasets show that: 1) Our MQA framework
consistently and significantly improves the performances of various
off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla
prompting. 2) In the zero-shot setting, MQA outperforms previous
state-of-the-art baselines by a large margin. In addition, the effectiveness of
our framework can successfully transfer to the few-shot setting, enhancing LMMs
on a scale of 10B parameters to be competitive or outperform much larger
language models such as ChatGPT and GPT-4. Our MQA framework can serve as a
general principle of utilizing LMMs to better solve MIE and potentially other
downstream multimodal tasks.",None,-1
ec28b16e-8fba-4dbf-8ee6-9fc22c934876,Human Pose as Compositional Tokens,0.892381,"Human pose is typically represented by a coordinate vector of body joints or
their heatmap embeddings. While easy for data processing, unrealistic pose
estimates are admitted due to the lack of dependency modeling between the body
joints. In this paper, we present a structured representation, named Pose as
Compositional Tokens (PCT), to explore the joint dependency. It represents a
pose by M discrete tokens with each characterizing a sub-structure with several
interdependent joints. The compositional design enables it to achieve a small
reconstruction error at a low cost. Then we cast pose estimation as a
classification task. In particular, we learn a classifier to predict the
categories of the M tokens from an image. A pre-learned decoder network is used
to recover the pose from the tokens without further post-processing. We show
that it achieves better or comparable pose estimation results as the existing
methods in general scenarios, yet continues to work well when occlusion occurs,
which is ubiquitous in practice. The code and models are publicly available at
https://github.com/Gengzigang/PCT.",None,-1
9804930a-5f5f-4821-90a8-97c856fd6b39,Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT,0.613742,"Foundation models exhibit significant capabilities in decision-making and
logical deductions. Nonetheless, a continuing discourse persists regarding
their genuine understanding of the world as opposed to mere stochastic mimicry.
This paper meticulously examines a simple transformer trained for Othello,
extending prior research to enhance comprehension of the emergent world model
of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a
linear representation of opposing pieces, a factor that causally steers its
decision-making process. This paper further elucidates the interplay between
the linear world representation and causal decision-making, and their
dependence on layer depth and model complexity. We have made the code public.",None,-1
6dc11160-4e2b-4be1-bc94-f971ecea63ef,"Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation",0.7539,"Interpreting ancient Chinese has been the key to comprehending vast Chinese
literature, tradition, and civilization. In this paper, we propose Erya for
ancient Chinese translation. From a dataset perspective, we collect, clean, and
classify ancient Chinese materials from various sources, forming the most
extensive ancient Chinese resource to date. From a model perspective, we devise
Erya training method oriented towards ancient Chinese. We design two
jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked
language model (DMLM). From an evaluation perspective, we build a benchmark to
judge ancient Chinese translation quality in different scenarios and evaluate
the ancient Chinese translation capacities of various existing models. Our
model exhibits remarkable zero-shot performance across five domains, with over
+12.0 BLEU against GPT-3.5 models and better human evaluation results than
ERNIE Bot. Subsequent fine-tuning further shows the superior transfer
capability of Erya model with +6.2 BLEU gain. We release all the
above-mentioned resources at https://github.com/RUCAIBox/Erya.",None,-1
4e76c5f2-5554-45cf-a2ab-ab4703c6b914,Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents,0.333257,"Temporal credit assignment is crucial for learning and skill development in
natural and artificial intelligence. While computational methods like the TD
approach in reinforcement learning have been proposed, it's unclear if they
accurately represent how humans handle feedback delays. Cognitive models intend
to represent the mental steps by which humans solve problems and perform a
number of tasks, but limited research in cognitive science has addressed the
credit assignment problem in humans and cognitive models. Our research uses a
cognitive model based on a theory of decisions from experience, Instance-Based
Learning Theory (IBLT), to test different credit assignment mechanisms in a
goal-seeking navigation task with varying levels of decision complexity.
Instance-Based Learning (IBL) models simulate the process of making sequential
choices with different credit assignment mechanisms, including a new IBL-TD
model that combines the IBL decision mechanism with the TD approach. We found
that (1) An IBL model that gives equal credit assignment to all decisions is
able to match human performance better than other models, including IBL-TD and
Q-learning; (2) IBL-TD and Q-learning models underperform compared to humans
initially, but eventually, they outperform humans; (3) humans are influenced by
decision complexity, while models are not. Our study provides insights into the
challenges of capturing human behavior and the potential opportunities to use
these models in future AI systems to support human activities.",None,-1
a193db28-3691-4c42-89ad-692c1f53d4d8,Multi-lingual and Multi-cultural Figurative Language Understanding,0.468803,"Figurative language permeates human communication, but at the same time is
relatively understudied in NLP. Datasets have been created in English to
accelerate progress towards measuring and improving figurative language
processing in language models (LMs). However, the use of figurative language is
an expression of our cultural and societal experiences, making it difficult for
these phrases to be universally applicable. In this work, we create a
figurative language inference dataset, \datasetname, for seven diverse
languages associated with a variety of cultures: Hindi, Indonesian, Javanese,
Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language
relies on cultural and regional concepts for figurative expressions, with the
highest overlap between languages originating from the same region. We assess
multilingual LMs' abilities to interpret figurative language in zero-shot and
few-shot settings. All languages exhibit a significant deficiency compared to
English, with variations in performance reflecting the availability of
pre-training and fine-tuning data, emphasizing the need for LMs to be exposed
to a broader range of linguistic and cultural variation during training.",None,-1
0dcf6af4-e93a-4b9b-9106-9508233fe30f,Learning in a Single Domain for Non-Stationary Multi-Texture Synthesis,0.0892953,"This paper aims for a new generation task: non-stationary multi-texture
synthesis, which unifies synthesizing multiple non-stationary textures in a
single model. Most non-stationary textures have large scale variance and can
hardly be synthesized through one model. To combat this, we propose a
multi-scale generator to capture structural patterns of various scales and
effectively synthesize textures with a minor cost. However, it is still hard to
handle textures of different categories with different texture patterns.
Therefore, we present a category-specific training strategy to focus on
learning texture pattern of a specific domain. Interestingly, once trained, our
model is able to produce multi-pattern generations with dynamic variations
without the need to finetune the model for different styles. Moreover, an
objective evaluation metric is designed for evaluating the quality of texture
expansion and global structure consistency. To our knowledge, ours is the first
scheme for this challenging task, including model, training, and evaluation.
Experimental results demonstrate the proposed method achieves superior
performance and time efficiency. The code will be available after the
publication.",None,-1
b9a76a28-2e3b-4c60-8529-88847a064cc3,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,0.309948,"Self-driving vehicles rely on urban street maps for autonomous navigation. In
this paper, we introduce Pix2Map, a method for inferring urban street map
topology directly from ego-view images, as needed to continually update and
expand existing maps. This is a challenging task, as we need to infer a complex
urban road topology directly from raw image data. The main insight of this
paper is that this problem can be posed as cross-modal retrieval by learning a
joint, cross-modal embedding space for images and existing maps, represented as
discrete graphs that encode the topological layout of the visual surroundings.
We conduct our experimental evaluation using the Argoverse dataset and show
that it is indeed possible to accurately retrieve street maps corresponding to
both seen and unseen roads solely from image data. Moreover, we show that our
retrieved maps can be used to update or expand existing maps and even show
proof-of-concept results for visual localization and image retrieval from
spatial graphs.",None,-1
830bbf8a-1612-4289-b2be-3a78ddd06876,ProAgent: Building Proactive Cooperative Agents with Large Language Models,0.999988,"Building agents with adaptive behavior in cooperative tasks stands as a
paramount goal in the realm of multi-agent systems. Current approaches to
developing cooperative agents rely primarily on learning-based methods, whose
policy generalization depends heavily on the diversity of teammates they
interact with during the training phase. Such reliance, however, constrains the
agents' capacity for strategic adaptation when cooperating with unfamiliar
teammates, which becomes a significant challenge in zero-shot coordination
scenarios. To address this challenge, we propose ProAgent, a novel framework
that harnesses large language models (LLMs) to create proactive agents capable
of dynamically adapting their behavior to enhance cooperation with teammates.
ProAgent can analyze the present state, and infer the intentions of teammates
from observations. It then updates its beliefs in alignment with the teammates'
subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of
modularity and interpretability, making it easily integrated into various of
coordination scenarios. Experimental evaluations conducted within the
Overcooked-AI environment unveil the remarkable performance superiority of
ProAgent, outperforming five methods based on self-play and population-based
training when cooperating with AI agents. Furthermore, in partnered with human
proxy models, its performance exhibits an average improvement exceeding 10%
compared to the current state-of-the-art method. For more information about our
project, please visit~\url{https://pku-proagent.github.io}.",None,-1
9e92a83b-bc18-42f3-b012-c3f3dd2ab39c,Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability,0.0535544,"Reinforcement learning (RL) is an area of significant research interest, and
safe RL in particular is attracting attention due to its ability to handle
safety-driven constraints that are crucial for real-world applications of RL
algorithms. This work proposes a novel approach to RL training, called control
invariant set (CIS) enhanced RL, which leverages the benefits of CIS to improve
stability guarantees and sampling efficiency. The approach consists of two
learning stages: offline and online. In the offline stage, CIS is incorporated
into the reward design, initial state sampling, and state reset procedures. In
the online stage, RL is retrained whenever the state is outside of CIS, which
serves as a stability criterion. A backup table that utilizes the explicit form
of CIS is obtained to ensure the online stability. To evaluate the proposed
approach, we apply it to a simulated chemical reactor. The results show a
significant improvement in sampling efficiency during offline training and
closed-loop stability in the online implementation.",None,-1
79c24eb3-d114-4eb3-965e-6ae25b503b33,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,0.996578,"Volumetric scene representations enable photorealistic view synthesis for
static scenes and form the basis of several existing 6-DoF video techniques.
However, the volume rendering procedures that drive these representations
necessitate careful trade-offs in terms of quality, rendering speed, and memory
efficiency. In particular, existing methods fail to simultaneously achieve
real-time performance, small memory footprint, and high-quality rendering for
challenging real-world scenes. To address these issues, we present HyperReel --
a novel 6-DoF video representation. The two core components of HyperReel are:
(1) a ray-conditioned sample prediction network that enables high-fidelity,
high frame rate rendering at high resolutions and (2) a compact and
memory-efficient dynamic volume representation. Our 6-DoF video pipeline
achieves the best performance compared to prior and contemporary approaches in
terms of visual quality with small memory requirements, while also rendering at
up to 18 frames-per-second at megapixel resolution without any custom CUDA
code.",None,-1
43364561-f169-49fb-9d3c-73b134274f82,Self-supervised representations in speech-based depression detection,0.972758,"This paper proposes handling training data sparsity in speech-based automatic
depression detection (SDD) using foundation models pre-trained with
self-supervised learning (SSL). An analysis of SSL representations derived from
different layers of pre-trained foundation models is first presented for SDD,
which provides insight to suitable indicator for depression detection.
Knowledge transfer is then performed from automatic speech recognition (ASR)
and emotion recognition to SDD by fine-tuning the foundation models. Results
show that the uses of oracle and ASR transcriptions yield similar SDD
performance when the hidden representations of the ASR model is incorporated
along with the ASR textual information. By integrating representations from
multiple foundation models, state-of-the-art SDD results based on real ASR were
achieved on the DAIC-WOZ dataset.",None,-1
207e16e2-b9aa-438b-9d6e-32b9c39c7a63,MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation,0.430773,"This paper addresses the issue of modifying the visual appearance of videos
while preserving their motion. A novel framework, named MagicProp, is proposed,
which disentangles the video editing process into two stages: appearance
editing and motion-aware appearance propagation. In the first stage, MagicProp
selects a single frame from the input video and applies image-editing
techniques to modify the content and/or style of the frame. The flexibility of
these techniques enables the editing of arbitrary regions within the frame. In
the second stage, MagicProp employs the edited frame as an appearance reference
and generates the remaining frames using an autoregressive rendering approach.
To achieve this, a diffusion-based conditional generation model, called
PropDPM, is developed, which synthesizes the target frame by conditioning on
the reference appearance, the target motion, and its previous appearance. The
autoregressive editing approach ensures temporal consistency in the resulting
videos. Overall, MagicProp combines the flexibility of image-editing techniques
with the superior temporal consistency of autoregressive modeling, enabling
flexible editing of object types and aesthetic styles in arbitrary regions of
input videos while maintaining good temporal consistency across frames.
Extensive experiments in various video editing scenarios demonstrate the
effectiveness of MagicProp.",None,-1
46c7d706-f164-4986-bae7-fb4a27be18d8,Deception Detection with Feature-Augmentation by soft Domain Transfer,0.107104,"In this era of information explosion, deceivers use different domains or
mediums of information to exploit the users, such as News, Emails, and Tweets.
Although numerous research has been done to detect deception in all these
domains, information shortage in a new event necessitates these domains to
associate with each other to battle deception. To form this association, we
propose a feature augmentation method by harnessing the intermediate layer
representation of neural models. Our approaches provide an improvement over the
self-domain baseline models by up to 6.60%. We find Tweets to be the most
helpful information provider for Fake News and Phishing Email detection,
whereas News helps most in Tweet Rumor detection. Our analysis provides a
useful insight for domain knowledge transfer which can help build a stronger
deception detection system than the existing literature.",None,-1
10af9ed5-c5e3-4d6b-a6e8-a95b35267337,Unsupervised Skin Lesion Segmentation via Structural Entropy Minimization on Multi-Scale Superpixel Graphs,0.866018,"Skin lesion segmentation is a fundamental task in dermoscopic image analysis.
The complex features of pixels in the lesion region impede the lesion
segmentation accuracy, and existing deep learning-based methods often lack
interpretability to this problem. In this work, we propose a novel unsupervised
Skin Lesion sEgmentation framework based on structural entropy and isolation
forest outlier Detection, namely SLED. Specifically, skin lesions are segmented
by minimizing the structural entropy of a superpixel graph constructed from the
dermoscopic image. Then, we characterize the consistency of healthy skin
features and devise a novel multi-scale segmentation mechanism by outlier
detection, which enhances the segmentation accuracy by leveraging the
superpixel features from multiple scales. We conduct experiments on four skin
lesion benchmarks and compare SLED with nine representative unsupervised
segmentation methods. Experimental results demonstrate the superiority of the
proposed framework. Additionally, some case studies are analyzed to demonstrate
the effectiveness of SLED.",None,-1
9074aa60-fae4-4087-a49b-3afe3343a63b,A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment,0.665795,"When communicating with elders with cognitive impairment, cognitive
stimulation (CS) help to maintain the cognitive health of elders. Data sparsity
is the main challenge in building CS-based dialogue systems, particularly in
the Chinese language. To fill this gap, we construct a Chinese CS conversation
(CSConv) dataset, which contains about 2.6K groups of dialogues with CS
principles and emotional support strategy labels. Making chit chat while
providing emotional support is overlooked by the majority of existing cognitive
dialogue systems. In this paper, we propose a multi-source knowledge fusion
method for CS dialogue (CSD), to generate open-ended responses guided by the CS
principle and emotional support strategy. We first use a progressive mask
method based on external knowledge to learn encoders as effective classifiers,
which is the prerequisite to predict the CS principle and emotional support
strategy of the target response. Then a decoder interacts with the perceived CS
principle and emotional support strategy to generate responses. Extensive
experiments conducted on the CSConv dataset demonstrate the effectiveness of
the proposed method, while there is still a large space for improvement
compared to human performance.",None,-1
74158941-9df7-469d-8ee3-c380fd24449c,Fairness in Visual Clustering: A Novel Transformer Clustering Approach,0.854114,"Promoting fairness for deep clustering models in unsupervised clustering
settings to reduce demographic bias is a challenging goal. This is because of
the limitation of large-scale balanced data with well-annotated labels for
sensitive or protected attributes. In this paper, we first evaluate demographic
bias in deep clustering models from the perspective of cluster purity, which is
measured by the ratio of positive samples within a cluster to their correlation
degree. This measurement is adopted as an indication of demographic bias. Then,
a novel loss function is introduced to encourage a purity consistency for all
clusters to maintain the fairness aspect of the learned clustering model.
Moreover, we present a novel attention mechanism, Cross-attention, to measure
correlations between multiple clusters, strengthening faraway positive samples
and improving the purity of clusters during the learning process. Experimental
results on a large-scale dataset with numerous attribute settings have
demonstrated the effectiveness of the proposed approach on both clustering
accuracy and fairness enhancement on several sensitive attributes.",None,-1
efc55a3c-4587-487f-9d11-f3382c1223dd,Lifted Sequential Planning with Lazy Constraint Generation Solvers,0.0583941,"This paper studies the possibilities made open by the use of Lazy Clause
Generation (LCG) based approaches to Constraint Programming (CP) for tackling
sequential classical planning. We propose a novel CP model based on seminal
ideas on so-called lifted causal encodings for planning as satisfiability, that
does not require grounding, as choosing groundings for functions and action
schemas becomes an integral part of the problem of designing valid plans. This
encoding does not require encoding frame axioms, and does not explicitly
represent states as decision variables for every plan step. We also present a
propagator procedure that illustrates the possibilities of LCG to widen the
kind of inference methods considered to be feasible in planning as (iterated)
CSP solving. We test encodings and propagators over classic IPC and recently
proposed benchmarks for lifted planning, and report that for planning problem
instances requiring fewer plan steps our methods compare very well with the
state-of-the-art in optimal sequential planning.",None,-1
a1c32ee9-81d8-42e6-9120-6d44929bf5e0,Probabilistic Uncertainty-Aware Risk Spot Detector for Naturalistic Driving,0.15119,"Risk assessment is a central element for the development and validation of
Autonomous Vehicles (AV). It comprises a combination of occurrence probability
and severity of future critical events. Time Headway (TH) as well as
Time-To-Contact (TTC) are commonly used risk metrics and have qualitative
relations to occurrence probability. However, they lack theoretical derivations
and additionally they are designed to only cover special types of traffic
scenarios (e.g. following between single car pairs). In this paper, we present
a probabilistic situation risk model based on survival analysis considerations
and extend it to naturally incorporate sensory, temporal and behavioral
uncertainties as they arise in real-world scenarios. The resulting Risk Spot
Detector (RSD) is applied and tested on naturalistic driving data of a
multi-lane boulevard with several intersections, enabling the visualization of
road criticality maps. Compared to TH and TTC, our approach is more selective
and specific in predicting risk. RSD concentrates on driving sections of high
vehicle density where large accelerations and decelerations or approaches with
high velocity occur.",None,-1
b760e741-4f33-4980-b884-f9ddd6362131,Understand the Dynamic World: An End-to-End Knowledge Informed Framework for Open Domain Entity State Tracking,0.436153,"Open domain entity state tracking aims to predict reasonable state changes of
entities (i.e., [attribute] of [entity] was [before_state] and [after_state]
afterwards) given the action descriptions. It's important to many reasoning
tasks to support human everyday activities. However, it's challenging as the
model needs to predict an arbitrary number of entity state changes caused by
the action while most of the entities are implicitly relevant to the actions
and their attributes as well as states are from open vocabularies. To tackle
these challenges, we propose a novel end-to-end Knowledge Informed framework
for open domain Entity State Tracking, namely KIEST, which explicitly retrieves
the relevant entities and attributes from external knowledge graph (i.e.,
ConceptNet) and incorporates them to autoregressively generate all the entity
state changes with a novel dynamic knowledge grained encoder-decoder framework.
To enforce the logical coherence among the predicted entities, attributes, and
states, we design a new constraint decoding strategy and employ a coherence
reward to improve the decoding process. Experimental results show that our
proposed KIEST framework significantly outperforms the strong baselines on the
public benchmark dataset OpenPI.",None,-1
1a352dad-3f5e-48ed-af85-902e260ce4b6,MagicEdit: High-Fidelity and Temporally Coherent Video Editing,0.810554,"In this report, we present MagicEdit, a surprisingly simple yet effective
solution to the text-guided video editing task. We found that high-fidelity and
temporally coherent video-to-video translation can be achieved by explicitly
disentangling the learning of content, structure and motion signals during
training. This is in contradict to most existing methods which attempt to
jointly model both the appearance and temporal representation within a single
framework, which we argue, would lead to degradation in per-frame quality.
Despite its simplicity, we show that MagicEdit supports various downstream
video editing tasks, including video stylization, local editing, video-MagicMix
and video outpainting.",None,-1
bc5ccdec-85f9-4f75-b6ba-8d615b436ef2,HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition,0.445753,"State-of-the-art ASR systems have achieved promising results by modeling
local and global interactions separately. While the former can be computed
efficiently, global interactions are usually modeled via attention mechanisms,
which are expensive for long input sequences. Here, we address this by
extending HyperMixer, an efficient alternative to attention exhibiting linear
complexity, to the Conformer architecture for speech recognition, leading to
HyperConformer. In particular, multi-head HyperConformer achieves comparable or
higher recognition performance while being more efficient than Conformer in
terms of inference speed, memory, parameter count, and available training data.
HyperConformer achieves a word error rate of 2.9% on Librispeech test-clean
with less than 8M neural parameters and a peak memory during training of 5.7GB,
hence trainable with accessible hardware. Encoder speed is between 38% on
mid-length speech and 56% on long speech faster than an equivalent Conformer.
(The HyperConformer recipe is publicly available in:
https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech/ASR/transformer/)",None,-1
96f32f7b-a356-4557-9a86-494e76a76eb5,Attributable and Scalable Opinion Summarization,0.18234,"We propose a method for unsupervised opinion summarization that encodes
sentences from customer reviews into a hierarchical discrete latent space, then
identifies common opinions based on the frequency of their encodings. We are
able to generate both abstractive summaries by decoding these frequent
encodings, and extractive summaries by selecting the sentences assigned to the
same frequent encodings. Our method is attributable, because the model
identifies sentences used to generate the summary as part of the summarization
process. It scales easily to many hundreds of input reviews, because
aggregation is performed in the latent space rather than over long sequences of
tokens. We also demonstrate that our appraoch enables a degree of control,
generating aspect-specific summaries by restricting the model to parts of the
encoding space that correspond to desired aspects (e.g., location or food).
Automatic and human evaluation on two datasets from different domains
demonstrates that our method generates summaries that are more informative than
prior work and better grounded in the input reviews.",None,-1
4ffafd04-3947-4eb9-83eb-7a9ae6327abc,Assessing the potential of AI-assisted pragmatic annotation: The case of apologies,0.882681,"Certain forms of linguistic annotation, like part of speech and semantic
tagging, can be automated with high accuracy. However, manual annotation is
still necessary for complex pragmatic and discursive features that lack a
direct mapping to lexical forms. This manual process is time-consuming and
error-prone, limiting the scalability of function-to-form approaches in corpus
linguistics. To address this, our study explores automating pragma-discursive
corpus annotation using large language models (LLMs). We compare ChatGPT, the
Bing chatbot, and a human coder in annotating apology components in English
based on the local grammar framework. We find that the Bing chatbot
outperformed ChatGPT, with accuracy approaching that of a human coder. These
results suggest that AI can be successfully deployed to aid pragma-discursive
corpus annotation, making the process more efficient and scalable. Keywords:
linguistic annotation, function-to-form approaches, large language models,
local grammar analysis, Bing chatbot, ChatGPT",None,-1
31334cd0-68e3-45fa-8663-52d0564ef927,Adaptive manifold for imbalanced transductive few-shot learning,0.20804,"Transductive few-shot learning algorithms have showed substantially superior
performance over their inductive counterparts by leveraging the unlabeled
queries. However, the vast majority of such methods are evaluated on perfectly
class-balanced benchmarks. It has been shown that they undergo remarkable drop
in performance under a more realistic, imbalanced setting. To this end, we
propose a novel algorithm to address imbalanced transductive few-shot learning,
named Adaptive Manifold. Our method exploits the underlying manifold of the
labeled support examples and unlabeled queries by using manifold similarity to
predict the class probability distribution per query. It is parameterized by
one centroid per class as well as a set of graph-specific parameters that
determine the manifold. All parameters are optimized through a loss function
that can be tuned towards class-balanced or imbalanced distributions. The
manifold similarity shows substantial improvement over Euclidean distance,
especially in the 1-shot setting. Our algorithm outperforms or is on par with
other state of the art methods in three benchmark datasets, namely
miniImageNet, tieredImageNet and CUB, and three different backbones, namely
ResNet-18, WideResNet-28-10 and DenseNet-121. In certain cases, our algorithm
outperforms the previous state of the art by as much as 4.2%.",None,-1
ba6fa6a1-dd46-43a1-9ed6-b867b9665fee,Monocular Depth Estimation using Diffusion Models,0.99565,"We formulate monocular depth estimation using denoising diffusion models,
inspired by their recent successes in high fidelity image generation. To that
end, we introduce innovations to address problems arising due to noisy,
incomplete depth maps in training data, including step-unrolled denoising
diffusion, an $L_1$ loss, and depth infilling during training. To cope with the
limited availability of data for supervised training, we leverage pre-training
on self-supervised image-to-image translation tasks. Despite the simplicity of
the approach, with a generic loss and architecture, our DepthGen model achieves
SOTA performance on the indoor NYU dataset, and near SOTA results on the
outdoor KITTI dataset. Further, with a multimodal posterior, DepthGen naturally
represents depth ambiguity (e.g., from transparent surfaces), and its zero-shot
performance combined with depth imputation, enable a simple but effective
text-to-3D pipeline. Project page: https://depth-gen.github.io",None,-1
b687a32c-6686-49d4-be6f-a30e04e2d023,AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing,0.710102,"The intensive care unit (ICU) is a specialized hospital space where
critically ill patients receive intensive care and monitoring. Comprehensive
monitoring is imperative in assessing patients conditions, in particular
acuity, and ultimately the quality of care. However, the extent of patient
monitoring in the ICU is limited due to time constraints and the workload on
healthcare providers. Currently, visual assessments for acuity, including fine
details such as facial expressions, posture, and mobility, are sporadically
captured, or not captured at all. These manual observations are subjective to
the individual, prone to documentation errors, and overburden care providers
with the additional workload. Artificial Intelligence (AI) enabled systems has
the potential to augment the patient visual monitoring and assessment due to
their exceptional learning capabilities. Such systems require robust annotated
data to train. To this end, we have developed pervasive sensing and data
processing system which collects data from multiple modalities depth images,
color RGB images, accelerometry, electromyography, sound pressure, and light
levels in ICU for developing intelligent monitoring systems for continuous and
granular acuity, delirium risk, pain, and mobility assessment. This paper
presents the Intelligent Intensive Care Unit (I2CU) system architecture we
developed for real-time patient monitoring and visual assessment.",None,-1
f267ccd1-fc93-4821-893e-5b4710c90830,Parkinson gait modelling from an anomaly deep representation,0.217655,"Parkinson's Disease (PD) is associated with gait movement disorders, such as
bradykinesia, stiffness, tremors and postural instability, caused by
progressive dopamine deficiency. Today, some approaches have implemented
learning representations to quantify kinematic patterns during locomotion,
supporting clinical procedures such as diagnosis and treatment planning. These
approaches assumes a large amount of stratified and labeled data to optimize
discriminative representations. Nonetheless these considerations may restrict
the approaches to be operable in real scenarios during clinical practice. This
work introduces a self-supervised generative representation to learn
gait-motion-related patterns, under the pretext of video reconstruction and an
anomaly detection framework. This architecture is trained following a one-class
weakly supervised learning to avoid inter-class variance and approach the
multiple relationships that represent locomotion. The proposed approach was
validated with 14 PD patients and 23 control subjects, and trained with the
control population only, achieving an AUC of 95%, homocedasticity level of 70%
and shapeness level of 70% in the classification task considering its
generalization.",None,-1
a5bbbb29-267a-4a30-a087-88f1efd4e927,TempT: Temporal consistency for Test-time adaptation,0.5716,"We introduce Temporal consistency for Test-time adaptation (TempT) a novel
method for test-time adaptation on videos through the use of temporal coherence
of predictions across sequential frames as a self-supervision signal. TempT is
an approach with broad potential applications in computer vision tasks
including facial expression recognition (FER) in videos. We evaluate TempT
performance on the AffWild2 dataset. Our approach focuses solely on the
unimodal visual aspect of the data and utilizes a popular 2D CNN backbone in
contrast to larger sequential or attention-based models used in other
approaches. Our preliminary experimental results demonstrate that TempT has
competitive performance compared to the previous years reported performances
and its efficacy provides a compelling proof-of-concept for its use in various
real-world applications.",None,-1
e189df86-186b-4285-a48c-9e74b6e83be3,Audio-Visual Contrastive Learning with Temporal Self-Supervision,0.471514,"We propose a self-supervised learning approach for videos that learns
representations of both the RGB frames and the accompanying audio without human
supervision. In contrast to images that capture the static scene appearance,
videos also contain sound and temporal scene dynamics. To leverage the temporal
and aural dimension inherent to videos, our method extends temporal
self-supervision to the audio-visual setting and integrates it with multi-modal
contrastive objectives. As temporal self-supervision, we pose playback speed
and direction recognition in both modalities and propose intra- and inter-modal
temporal ordering tasks. Furthermore, we design a novel contrastive objective
in which the usual pairs are supplemented with additional sample-dependent
positives and negatives sampled from the evolving feature space. In our model,
we apply such losses among video clips and between videos and their temporally
corresponding audio clips. We verify our model design in extensive ablation
experiments and evaluate the video and audio representations in transfer
experiments to action recognition and retrieval on UCF101 and HMBD51, audio
classification on ESC50, and robust video fingerprinting on VGG-Sound, with
state-of-the-art results.",None,-1
bc8f1e38-8bd1-4530-94f3-913e16059596,Hierarchically Gated Recurrent Neural Network for Sequence Modeling,0.821246,"Transformers have surpassed RNNs in popularity due to their superior
abilities in parallel training and long-term dependency modeling. Recently,
there has been a renewed interest in using linear RNNs for efficient sequence
modeling. These linear RNNs often employ gating mechanisms in the output of the
linear recurrence layer while ignoring the significance of using forget gates
within the recurrence. In this paper, we propose a gated linear RNN model
dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes
forget gates that are lower bounded by a learnable value. The lower bound
increases monotonically when moving up layers. This allows the upper layers to
model long-term dependencies and the lower layers to model more local,
short-term dependencies. Experiments on language modeling, image
classification, and long-range arena benchmarks showcase the efficiency and
effectiveness of our proposed model. The source code is available at
https://github.com/OpenNLPLab/HGRN.",None,-1
3373057b-7ea9-4dd2-9534-eeb99a33e508,AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,0.419645,"AutoGen is an open-source framework that allows developers to build LLM
applications via multiple agents that can converse with each other to
accomplish tasks. AutoGen agents are customizable, conversable, and can operate
in various modes that employ combinations of LLMs, human inputs, and tools.
Using AutoGen, developers can also flexibly define agent interaction behaviors.
Both natural language and computer code can be used to program flexible
conversation patterns for different applications. AutoGen serves as a generic
infrastructure to build diverse applications of various complexities and LLM
capacities. Empirical studies demonstrate the effectiveness of the framework in
many example applications, with domains ranging from mathematics, coding,
question answering, operations research, online decision-making, entertainment,
etc.",None,-1
542a3a1b-ad7a-4881-91e5-1b2aa7511666,You Only Segment Once: Towards Real-Time Panoptic Segmentation,0.674976,"In this paper, we propose YOSO, a real-time panoptic segmentation framework.
YOSO predicts masks via dynamic convolutions between panoptic kernels and image
feature maps, in which you only need to segment once for both instance and
semantic segmentation tasks. To reduce the computational overhead, we design a
feature pyramid aggregator for the feature map extraction, and a separable
dynamic decoder for the panoptic kernel generation. The aggregator
re-parameterizes interpolation-first modules in a convolution-first way, which
significantly speeds up the pipeline without any additional costs. The decoder
performs multi-head cross-attention via separable dynamic convolution for
better efficiency and accuracy. To the best of our knowledge, YOSO is the first
real-time panoptic segmentation framework that delivers competitive performance
compared to state-of-the-art models. Specifically, YOSO achieves 46.4 PQ, 45.6
FPS on COCO; 52.5 PQ, 22.6 FPS on Cityscapes; 38.0 PQ, 35.4 FPS on ADE20K; and
34.1 PQ, 7.1 FPS on Mapillary Vistas. Code is available at
https://github.com/hujiecpp/YOSO.",None,-1
831fdd80-16b7-4362-b4bc-59622c03bdae,Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory,0.286248,"We address a fundamental challenge in Natural Language Generation (NLG) model
evaluation -- the design and evaluation of evaluation metrics. Recognizing the
limitations of existing automatic metrics and noises from how current human
evaluation was conducted, we propose MetricEval, a framework informed by
measurement theory, the foundation of educational test design, for
conceptualizing and evaluating the reliability and validity of NLG evaluation
metrics. The framework formalizes the source of measurement error and offers
statistical tools for evaluating evaluation metrics based on empirical data.
With our framework, one can quantify the uncertainty of the metrics to better
interpret the result. To exemplify the use of our framework in practice, we
analyzed a set of evaluation metrics for summarization and identified issues
related to conflated validity structure in human-eval and reliability in
LLM-based metrics. Through MetricEval, we aim to promote the design,
evaluation, and interpretation of valid and reliable metrics to advance robust
and effective NLG models.",None,-1
7586da71-542f-4f31-a239-54aab8b2ed12,Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents,0.999977,"In this paper, we present a novel framework for enhancing the capabilities of
large language models (LLMs) by leveraging the power of multi-agent systems.
Our framework introduces a collaborative environment where multiple intelligent
agent components, each with distinctive attributes and roles, work together to
handle complex tasks more efficiently and effectively. We demonstrate the
practicality and versatility of our framework through case studies in
artificial general intelligence (AGI), specifically focusing on the Auto-GPT
and BabyAGI models. We also examine the ""Gorilla"" model, which integrates
external APIs into the LLM. Our framework addresses limitations and challenges
such as looping issues, security risks, scalability, system evaluation, and
ethical considerations. By modeling various domains such as courtroom
simulations and software development scenarios, we showcase the potential
applications and benefits of our proposed multi-agent system. Our framework
provides an avenue for advancing the capabilities and performance of LLMs
through collaboration and knowledge exchange among intelligent agents.",None,-1
96ab2208-18fa-4e58-9134-97a5884b4c85,"(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions",0.671933,"The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents.",None,-1
91686bac-7552-45af-983b-11077fa8ca9e,The moral authority of ChatGPT,0.909972,"ChatGPT is not only fun to chat with, but it also searches information,
answers questions, and gives advice. With consistent moral advice, it might
improve the moral judgment and decisions of users, who often hold contradictory
moral beliefs. Unfortunately, ChatGPT turns out highly inconsistent as a moral
advisor. Nonetheless, it influences users' moral judgment, we find in an
experiment, even if they know they are advised by a chatting bot, and they
underestimate how much they are influenced. Thus, ChatGPT threatens to corrupt
rather than improves users' judgment. These findings raise the question of how
to ensure the responsible use of ChatGPT and similar AI. Transparency is often
touted but seems ineffective. We propose training to improve digital literacy.",None,-1
dbcd6b69-7adb-4771-9df4-6b9f414a8294,ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals,0.974765,"Motion forecasting is a key module in an autonomous driving system. Due to
the heterogeneous nature of multi-sourced input, multimodality in agent
behavior, and low latency required by onboard deployment, this task is
notoriously challenging. To cope with these difficulties, this paper proposes a
novel agent-centric model with anchor-informed proposals for efficient
multimodal motion prediction. We design a modality-agnostic strategy to
concisely encode the complex input in a unified manner. We generate diverse
proposals, fused with anchors bearing goal-oriented scene context, to induce
multimodal prediction that covers a wide range of future trajectories. Our
network architecture is highly uniform and succinct, leading to an efficient
model amenable for real-world driving deployment. Experiments reveal that our
agent-centric network compares favorably with the state-of-the-art methods in
prediction accuracy, while achieving scene-centric level inference latency.",None,-1
3b9afdcb-4fd4-45fa-ba99-fc8fa9917780,SA6D: Self-Adaptive Few-Shot 6D Pose Estimator for Novel and Occluded Objects,0.381692,"To enable meaningful robotic manipulation of objects in the real-world, 6D
pose estimation is one of the critical aspects. Most existing approaches have
difficulties to extend predictions to scenarios where novel object instances
are continuously introduced, especially with heavy occlusions. In this work, we
propose a few-shot pose estimation (FSPE) approach called SA6D, which uses a
self-adaptive segmentation module to identify the novel target object and
construct a point cloud model of the target object using only a small number of
cluttered reference images. Unlike existing methods, SA6D does not require
object-centric reference images or any additional object information, making it
a more generalizable and scalable solution across categories. We evaluate SA6D
on real-world tabletop object datasets and demonstrate that SA6D outperforms
existing FSPE methods, particularly in cluttered scenes with occlusions, while
requiring fewer reference images.",None,-1
72f4a065-d146-4ea6-a7e4-731e6f7c88c0,Using ChatGPT as a CAT tool in Easy Language translation,0.326791,"This study sets out to investigate the feasibility of using ChatGPT to
translate citizen-oriented administrative texts into German Easy Language, a
simplified, controlled language variety that is adapted to the needs of people
with reading impairments. We use ChatGPT to translate selected texts from
websites of German public authorities using two strategies, i.e. linguistic and
holistic. We analyse the quality of the generated texts based on different
criteria, such as correctness, readability, and syntactic complexity. The
results indicated that the generated texts are easier than the standard texts,
but that they still do not fully meet the established Easy Language standards.
Additionally, the content is not always rendered correctly.",None,-1
926247f7-30a1-48bc-97a9-446aa8471384,Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection,0.791661,"Real-world deployment of reliable object detectors is crucial for
applications such as autonomous driving. However, general-purpose object
detectors like Faster R-CNN are prone to providing overconfident predictions
for outlier objects. Recent outlier-aware object detection approaches estimate
the density of instance-wide features with class-conditional Gaussians and
train on synthesized outlier features from their low-likelihood regions.
However, this strategy does not guarantee that the synthesized outlier features
will have a low likelihood according to the other class-conditional Gaussians.
We propose a novel outlier-aware object detection framework that distinguishes
outliers from inlier objects by learning the joint data distribution of all
inlier classes with an invertible normalizing flow. The appropriate sampling of
the flow model ensures that the synthesized outliers have a lower likelihood
than inliers of all object classes, thereby modeling a better decision boundary
between inlier and outlier objects. Our approach significantly outperforms the
state-of-the-art for outlier-aware object detection on both image and video
datasets. Code available at https://github.com/nish03/FFS",None,-1
033fe709-1ba5-4a13-b2ac-e3c5dd28f680,PanelNet: Understanding 360 Indoor Environment via Panel Representation,0.756741,"Indoor 360 panoramas have two essential properties. (1) The panoramas are
continuous and seamless in the horizontal direction. (2) Gravity plays an
important role in indoor environment design. By leveraging these properties, we
present PanelNet, a framework that understands indoor environments using a
novel panel representation of 360 images. We represent an equirectangular
projection (ERP) as consecutive vertical panels with corresponding 3D panel
geometry. To reduce the negative impact of panoramic distortion, we incorporate
a panel geometry embedding network that encodes both the local and global
geometric features of a panel. To capture the geometric context in room design,
we introduce Local2Global Transformer, which aggregates local information
within a panel and panel-wise global context. It greatly improves the model
performance with low training overhead. Our method outperforms existing methods
on indoor 360 depth estimation and shows competitive results against
state-of-the-art approaches on the task of indoor layout estimation and
semantic segmentation.",None,-1
46171730-04ce-4498-acdf-87ac19ac37a0,Exploration and Exploitation of Unlabeled Data for Open-Set Semi-Supervised Learning,0.152051,"In this paper, we address a complex but practical scenario in semi-supervised
learning (SSL) named open-set SSL, where unlabeled data contain both
in-distribution (ID) and out-of-distribution (OOD) samples. Unlike previous
methods that only consider ID samples to be useful and aim to filter out OOD
ones completely during training, we argue that the exploration and exploitation
of both ID and OOD samples can benefit SSL. To support our claim, i) we propose
a prototype-based clustering and identification algorithm that explores the
inherent similarity and difference among samples at feature level and
effectively cluster them around several predefined ID and OOD prototypes,
thereby enhancing feature learning and facilitating ID/OOD identification; ii)
we propose an importance-based sampling method that exploits the difference in
importance of each ID and OOD sample to SSL, thereby reducing the sampling bias
and improving the training. Our proposed method achieves state-of-the-art in
several challenging benchmarks, and improves upon existing SSL methods even
when ID samples are totally absent in unlabeled data.",None,-1
b8af34ef-3619-4e15-8c76-443f98068aa9,Prototype Knowledge Distillation for Medical Segmentation with Missing Modality,0.646528,"Multi-modality medical imaging is crucial in clinical treatment as it can
provide complementary information for medical image segmentation. However,
collecting multi-modal data in clinical is difficult due to the limitation of
the scan time and other clinical situations. As such, it is clinically
meaningful to develop an image segmentation paradigm to handle this missing
modality problem. In this paper, we propose a prototype knowledge distillation
(ProtoKD) method to tackle the challenging problem, especially for the toughest
scenario when only single modal data can be accessed. Specifically, our ProtoKD
can not only distillate the pixel-wise knowledge of multi-modality data to
single-modality data but also transfer intra-class and inter-class feature
variations, such that the student model could learn more robust feature
representation from the teacher model and inference with only one single
modality data. Our method achieves state-of-the-art performance on BraTS
benchmark. The code is available at
\url{https://github.com/SakurajimaMaiii/ProtoKD}.",None,-1
9abbe065-9188-4c43-ac30-aa521b0d87cf,Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning,0.733599,"We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a
novel self-supervised learning framework for learning accurate and dense vision
representation. To extract spatial-sensitive information, ADCLR introduces
query patches for contrasting in addition with global contrasting. Compared
with previous dense contrasting methods, ADCLR mainly enjoys three merits: i)
achieving both global-discriminative and spatial-sensitive representation, ii)
model-efficient (no extra parameters in addition to the global contrasting
baseline), and iii) correspondence-free and thus simpler to implement. Our
approach achieves new state-of-the-art performance for contrastive methods. On
classification tasks, for ViT-S, ADCLR achieves 77.5% top-1 accuracy on
ImageNet with linear probing, outperforming our baseline (DINO) without our
devised techniques as plug-in, by 0.5%. For ViT-B, ADCLR achieves 79.8%, 84.0%
accuracy on ImageNet by linear probing and finetune, outperforming iBOT by
0.3%, 0.2% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant
improvements of 44.3% AP on object detection, 39.7% AP on instance
segmentation, outperforming previous SOTA method SelfPatch by 2.2% and 1.2%,
respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0% mIoU, 1.2% mAcc on
the segme",None,-1
127b26d0-be2c-4596-87cc-792c0393a85b,Why think step by step? Reasoning emerges from the locality of experience,0.489774,"Humans have a powerful and mysterious capacity to reason. Working through a
set of mental steps enables us to make inferences we would not be capable of
making directly even though we get no additional data from the world.
Similarly, when large language models generate intermediate steps (a chain of
thought) before answering a question, they often produce better answers than
they would directly. We investigate why and how chain-of-thought reasoning is
useful in language models, testing the hypothesis that reasoning is effective
when training data consists of overlapping local clusters of variables that
influence each other strongly. These training conditions enable the chaining of
accurate local inferences to estimate relationships between variables that were
not seen together in training. We prove that there will exist a ""reasoning
gap"", where reasoning through intermediate variables reduces bias, for the
simple case of an autoregressive density estimator trained on local samples
from a chain-structured probabilistic model. We then test our hypothesis
experimentally in more complex models, training an autoregressive language
model on samples from Bayes nets but only including a subset of variables in
each sample. We test language models' ability to match conditional
probabilities with and without intermediate reasoning steps, finding that
intermediate steps are only helpful when the training data is locally
structured with respect to dependencies between variables. The combination of
locally structured observations and reasoning is much more data-efficient than
training on all variables. Our results illustrate how the effectiveness of
reasoning step by step is rooted in the local statistical structure of the
training data.",None,-1
20a106f1-8b2b-4883-bda2-8122ac9c6762,ACLS: Adaptive and Conditional Label Smoothing for Network Calibration,0.265347,"We address the problem of network calibration adjusting miscalibrated
confidences of deep neural networks. Many approaches to network calibration
adopt a regularization-based method that exploits a regularization term to
smooth the miscalibrated confidences. Although these approaches have shown the
effectiveness on calibrating the networks, there is still a lack of
understanding on the underlying principles of regularization in terms of
network calibration. We present in this paper an in-depth analysis of existing
regularization-based methods, providing a better understanding on how they
affect to network calibration. Specifically, we have observed that 1) the
regularization-based methods can be interpreted as variants of label smoothing,
and 2) they do not always behave desirably. Based on the analysis, we introduce
a novel loss function, dubbed ACLS, that unifies the merits of existing
regularization methods, while avoiding the limitations. We show extensive
experimental results for image classification and semantic segmentation on
standard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCAL
VOC, demonstrating the effectiveness of our loss function.",None,-1
eaaa3fc9-3717-4ccb-bdfa-7e407e2017cb,SFD2: Semantic-guided Feature Detection and Description,0.90005,"Visual localization is a fundamental task for various applications including
autonomous driving and robotics. Prior methods focus on extracting large
amounts of often redundant locally reliable features, resulting in limited
efficiency and accuracy, especially in large-scale environments under
challenging conditions. Instead, we propose to extract globally reliable
features by implicitly embedding high-level semantics into both the detection
and description processes. Specifically, our semantic-aware detector is able to
detect keypoints from reliable regions (e.g. building, traffic lane) and
suppress unreliable areas (e.g. sky, car) implicitly instead of relying on
explicit semantic labels. This boosts the accuracy of keypoint matching by
reducing the number of features sensitive to appearance changes and avoiding
the need of additional segmentation networks at test time. Moreover, our
descriptors are augmented with semantics and have stronger discriminative
ability, providing more inliers at test time. Particularly, experiments on
long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons
datasets demonstrate that our model outperforms previous local features and
gives competitive accuracy to advanced matchers but is about 2 and 3 times
faster when using 2k and 4k keypoints, respectively.",None,-1
186913b0-c9e7-46ca-962b-3564f287b8dc,Towards Designing a ChatGPT Conversational Companion for Elderly People,0.783826,"Loneliness and social isolation are serious and widespread problems among
older people, affecting their physical and mental health, quality of life, and
longevity. In this paper, we propose a ChatGPT-based conversational companion
system for elderly people. The system is designed to provide companionship and
help reduce feelings of loneliness and social isolation. The system was
evaluated with a preliminary study. The results showed that the system was able
to generate responses that were relevant to the created elderly personas.
However, it is essential to acknowledge the limitations of ChatGPT, such as
potential biases and misinformation, and to consider the ethical implications
of using AI-based companionship for the elderly, including privacy concerns.",None,-1
1b40282a-ffe7-4ea6-91b9-21307fcf28ac,DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer,0.0977081,"Generating visual layouts is an essential ingredient of graphic design. The
ability to condition layout generation on a partial subset of component
attributes is critical to real-world applications that involve user
interaction. Recently, diffusion models have demonstrated high-quality
generative performances in various domains. However, it is unclear how to apply
diffusion models to the natural representation of layouts which consists of a
mix of discrete (class) and continuous (location, size) attributes. To address
the conditioning layout generation problem, we introduce DLT, a joint
discrete-continuous diffusion model. DLT is a transformer-based model which has
a flexible conditioning mechanism that allows for conditioning on any given
subset of all the layout component classes, locations, and sizes. Our method
outperforms state-of-the-art generative models on various layout generation
datasets with respect to different metrics and conditioning settings.
Additionally, we validate the effectiveness of our proposed conditioning
mechanism and the joint continuous-diffusion process. This joint process can be
incorporated into a wide range of mixed discrete-continuous generative tasks.",None,-1
c5e33c28-82fb-4a69-97a1-0500578e5c87,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,0.77426,"The language ability of Large Language Models (LLMs) is often unbalanced
towards English because of the imbalance in the distribution of the
pre-training data. This disparity is demanded in further fine-tuning and
affecting the cross-lingual abilities of LLMs. In this paper, we propose to
empower Instructiontuned LLMs (It-LLMs) in languages other than English by
building semantic alignment between them. Hence, we propose CrossAlpaca, an
It-LLM with cross-lingual instruction-following and Translation-following
demonstrations to improve semantic alignment between languages. We validate our
approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA
and adapted versions of MMLU and BBH. Our models, tested over six different
languages, outperform the It-LLMs tuned on monolingual data. The final results
show that instruction tuning on non-English data is not enough and that
semantic alignment can be further improved by Translation-following
demonstrations.",None,-1
116cdab6-768d-49d7-9878-eb6a83cca9b4,DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models,0.413005,"Despite their impressive capabilities, large language models (LLMs) are prone
to hallucinations, i.e., generating content that deviates from facts seen
during pretraining. We propose a simple decoding strategy for reducing
hallucinations with pretrained LLMs that does not require conditioning on
retrieved external knowledge nor additional fine-tuning. Our approach obtains
the next-token distribution by contrasting the differences in logits obtained
from projecting the later layers versus earlier layers to the vocabulary space,
exploiting the fact that factual knowledge in an LLMs has generally been shown
to be localized to particular transformer layers. We find that this Decoding by
Contrasting Layers (DoLa) approach is able to better surface factual knowledge
and reduce the generation of incorrect facts. DoLa consistently improves the
truthfulness across multiple choices tasks and open-ended generation tasks, for
example improving the performance of LLaMA family models on TruthfulQA by
12-17% absolute points, demonstrating its potential in making LLMs reliably
generate truthful facts.",None,-1
9b511877-c20f-462f-bde0-5599d4b33c27,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,0.761132,"Position embeddings, encoding the positional relationships among tokens in
text sequences, make great contributions to modeling local context features in
Transformer-based pre-trained language models. However, in Extractive Question
Answering, position embeddings trained with instances of varied context lengths
may not perform well as we expect. Since the embeddings of rear positions are
updated fewer times than the front position embeddings, the rear ones may not
be properly trained. In this paper, we propose a simple but effective strategy,
Random Padding, without any modifications to architectures of existing
pre-trained language models. We adjust the token order of input sequences when
fine-tuning, to balance the number of updating times of every position
embedding. Experiments show that Random Padding can significantly improve model
performance on the instances whose answers are located at rear positions,
especially when models are trained on short contexts but evaluated on long
contexts. Our code and data will be released for future research.",None,-1
4536bee3-cbd1-401a-a103-a894b8ebd814,FedDrive v2: an Analysis of the Impact of Label Skewness in Federated Semantic Segmentation for Autonomous Driving,0.256498,"We propose FedDrive v2, an extension of the Federated Learning benchmark for
Semantic Segmentation in Autonomous Driving. While the first version aims at
studying the effect of domain shift of the visual features across clients, in
this work, we focus on the distribution skewness of the labels. We propose six
new federated scenarios to investigate how label skewness affects the
performance of segmentation models and compare it with the effect of domain
shift. Finally, we study the impact of using the domain information during
testing. Official website: https://feddrive.github.io",None,-1
29e2ac0a-c477-46e8-b7f3-9d3409ef5fef,Glocal Energy-based Learning for Few-Shot Open-Set Recognition,0.245677,"Few-shot open-set recognition (FSOR) is a challenging task of great practical
value. It aims to categorize a sample to one of the pre-defined, closed-set
classes illustrated by few examples while being able to reject the sample from
unknown classes. In this work, we approach the FSOR task by proposing a novel
energy-based hybrid model. The model is composed of two branches, where a
classification branch learns a metric to classify a sample to one of closed-set
classes and the energy branch explicitly estimates the open-set probability. To
achieve holistic detection of open-set samples, our model leverages both
class-wise and pixel-wise features to learn a glocal energy-based score, in
which a global energy score is learned using the class-wise features, while a
local energy score is learned using the pixel-wise features. The model is
enforced to assign large energy scores to samples that are deviated from the
few-shot examples in either the class-wise features or the pixel-wise features,
and to assign small energy scores otherwise. Experiments on three standard FSOR
datasets show the superior performance of our model.",None,-1
20101c9a-5563-40f3-bb4a-4e051f23def4,Scenario-Agnostic Zero-Trust Defense with Explainable Threshold Policy: A Meta-Learning Approach,0.965205,"The increasing connectivity and intricate remote access environment have made
traditional perimeter-based network defense vulnerable. Zero trust becomes a
promising approach to provide defense policies based on agent-centric trust
evaluation. However, the limited observations of the agent's trace bring
information asymmetry in the decision-making. To facilitate the human
understanding of the policy and the technology adoption, one needs to create a
zero-trust defense that is explainable to humans and adaptable to different
attack scenarios. To this end, we propose a scenario-agnostic zero-trust
defense based on Partially Observable Markov Decision Processes (POMDP) and
first-order Meta-Learning using only a handful of sample scenarios. The
framework leads to an explainable and generalizable trust-threshold defense
policy. To address the distribution shift between empirical security datasets
and reality, we extend the model to a robust zero-trust defense minimizing the
worst-case loss. We use case studies and real-world attacks to corroborate the
results.",None,-1
1906acd9-98ce-4e6d-a684-c05a95ce2743,Generalisation Through Negation and Predicate Invention,0.411443,"The ability to generalise from a small number of examples is a fundamental
challenge in machine learning. To tackle this challenge, we introduce an
inductive logic programming (ILP) approach that combines negation and predicate
invention. Combining these two features allows an ILP system to generalise
better by learning rules with universally quantified body-only variables. We
implement our idea in NOPI, which can learn normal logic programs with
predicate invention, including Datalog programs with stratified negation. Our
experimental results on multiple domains show that our approach can improve
predictive accuracies and learning times.",None,-1
7cfc220a-df4f-4fd1-9e8a-785606028692,Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment,0.109551,"Human communication often involves information gaps between the
interlocutors. For example, in an educational dialogue, a student often
provides an answer that is incomplete, and there is a gap between this answer
and the perfect one expected by the teacher. Successful dialogue then hinges on
the teacher asking about this gap in an effective manner, thus creating a rich
and interactive educational experience. We focus on the problem of generating
such gap-focused questions (GFQs) automatically. We define the task, highlight
key desired aspects of a good GFQ, and propose a model that satisfies these.
Finally, we provide an evaluation by human annotators of our generated
questions compared against human generated ones, demonstrating competitive
performance.",None,-1
cbf2a8a9-4a86-4c72-b509-6fd405fed49f,Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs,0.710906,"Knowledge graph embeddings (KGE) have been extensively studied to embed
large-scale relational data for many real-world applications. Existing methods
have long ignored the fact many KGs contain two fundamentally different views:
high-level ontology-view concepts and fine-grained instance-view entities. They
usually embed all nodes as vectors in one latent space. However, a single
geometric representation fails to capture the structural differences between
two views and lacks probabilistic semantics towards concepts' granularity. We
propose Concept2Box, a novel approach that jointly embeds the two views of a KG
using dual geometric representations. We model concepts with box embeddings,
which learn the hierarchy structure and complex relations such as overlap and
disjoint among them. Box volumes can be interpreted as concepts' granularity.
Different from concepts, we model entities as vectors. To bridge the gap
between concept box embeddings and entity vector embeddings, we propose a novel
vector-to-box distance metric and learn both embeddings jointly. Experiments on
both the public DBpedia KG and a newly-created industrial KG showed the
effectiveness of Concept2Box.",None,-1
111d8c19-5e98-41bd-9d60-14b8b3dd5f78,Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning,0.356751,"In Federated Learning, a global model is learned by aggregating model updates
computed at a set of independent client nodes, to reduce communication costs
multiple gradient steps are performed at each node prior to aggregation. A key
challenge in this setting is data heterogeneity across clients resulting in
differing local objectives which can lead clients to overly minimize their own
local objective, diverging from the global solution. We demonstrate that
individual client models experience a catastrophic forgetting with respect to
data from other clients and propose an efficient approach that modifies the
cross-entropy objective on a per-client basis by re-weighting the softmax
logits prior to computing the loss. This approach shields classes outside a
client's label set from abrupt representation change and we empirically
demonstrate it can alleviate client forgetting and provide consistent
improvements to standard federated learning algorithms. Our method is
particularly beneficial under the most challenging federated learning settings
where data heterogeneity is high and client participation in each round is low.",None,-1
32db4a12-dec0-42e9-95c2-a425c7fd272a,Differentially Private Image Classification by Learning Priors from Random Processes,0.950005,"In privacy-preserving machine learning, differentially private stochastic
gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient
clipping and noise addition. A recent focus in private learning research is
improving the performance of DP-SGD on private data by incorporating priors
that are learned on real-world public data. In this work, we explore how we can
improve the privacy-utility tradeoff of DP-SGD by learning priors from images
generated by random processes and transferring these priors to private data. We
propose DP-RandP, a three-phase approach. We attain new state-of-the-art
accuracy when training from scratch on CIFAR10, CIFAR100, MedMNIST and ImageNet
for a range of privacy budgets $\varepsilon \in [1, 8]$. In particular, we
improve the previous best reported accuracy on CIFAR10 from $60.6 \%$ to $72.3
\%$ for $\varepsilon=1$.",None,-1
3112a03b-3b5c-49bb-8973-fa027d679bb5,"Mephisto: A Framework for Portable, Reproducible, and Iterative Crowdsourcing",0.7561,"We introduce Mephisto, a framework to make crowdsourcing for research more
reproducible, transparent, and collaborative. Mephisto provides abstractions
that cover a broad set of task designs and data collection workflows, and
provides a simple user experience to make best-practices easy defaults. In this
whitepaper we discuss the current state of data collection and annotation in ML
research, establish the motivation for building a shared framework to enable
researchers to create and open-source data collection and annotation tools as
part of their publication, and outline a set of suggested requirements for a
system to facilitate these goals. We then step through our resolution in
Mephisto, explaining the abstractions we use, our design decisions around the
user experience, and share implementation details and where they align with the
original motivations. We also discuss current limitations, as well as future
work towards continuing to deliver on the framework's initial goals. Mephisto
is available as an open source project, and its documentation can be found at
www.mephisto.ai.",None,-1
7a03f6cb-9907-4804-8143-392e0a1745e9,"Learning, Fast and Slow: A Goal-Directed Memory-Based Approach for Dynamic Environments",0.0291728,"Model-based next state prediction and state value prediction are slow to
converge. To address these challenges, we do the following: i) Instead of a
neural network, we do model-based planning using a parallel memory retrieval
system (which we term the slow mechanism); ii) Instead of learning state
values, we guide the agent's actions using goal-directed exploration, by using
a neural network to choose the next action given the current state and the goal
state (which we term the fast mechanism). The goal-directed exploration is
trained online using hippocampal replay of visited states and future imagined
states every single time step, leading to fast and efficient training.
Empirical studies show that our proposed method has a 92% solve rate across 100
episodes in a dynamically changing grid world, significantly outperforming
state-of-the-art actor critic mechanisms such as PPO (54%), TRPO (50%) and A2C
(24%). Ablation studies demonstrate that both mechanisms are crucial. We posit
that the future of Reinforcement Learning (RL) will be to model goals and
sub-goals for various tasks, and plan it out in a goal-directed memory-based
approach.",None,-1
64975ba4-b9b9-461d-8c58-072ee326bc0a,Improving CNN-based Person Re-identification using score Normalization,0.976372,"Person re-identification (PRe-ID) is a crucial task in security,
surveillance, and retail analysis, which involves identifying an individual
across multiple cameras and views. However, it is a challenging task due to
changes in illumination, background, and viewpoint. Efficient feature
extraction and metric learning algorithms are essential for a successful PRe-ID
system. This paper proposes a novel approach for PRe-ID, which combines a
Convolutional Neural Network (CNN) based feature extraction method with
Cross-view Quadratic Discriminant Analysis (XQDA) for metric learning.
Additionally, a matching algorithm that employs Mahalanobis distance and a
score normalization process to address inconsistencies between camera scores is
implemented. The proposed approach is tested on four challenging datasets,
including VIPeR, GRID, CUHK01, and PRID450S, and promising results are
obtained. For example, without normalization, the rank-20 rate accuracies of
the GRID, CUHK01, VIPeR and PRID450S datasets were 61.92%, 83.90%, 92.03%,
96.22%; however, after score normalization, they have increased to 64.64%,
89.30%, 92.78%, and 98.76%, respectively. Accordingly, the promising results on
four challenging datasets indicate the effectiveness of the proposed approach.",None,-1
6fc246ee-d5b8-4b54-9c38-8cc7b13165bf,IC3: Image Captioning by Committee Consensus,0.335542,"If you ask a human to describe an image, they might do so in a thousand
different ways. Traditionally, image captioning models are trained to generate
a single ""best"" (most like a reference) image caption. Unfortunately, doing so
encourages captions that are ""informationally impoverished,"" and focus on only
a subset of the possible details, while ignoring other potentially useful
information in the scene. In this work, we introduce a simple, yet novel,
method: ""Image Captioning by Committee Consensus"" (IC3), designed to generate a
single caption that captures high-level details from several annotator
viewpoints. Humans rate captions produced by IC3 at least as helpful as
baseline SOTA models more than two thirds of the time, and IC3 can improve the
performance of SOTA automated recall systems by up to 84%, outperforming single
human-generated reference captions, and indicating significant improvements
over SOTA approaches for visual description. Code is available at
https://davidmchan.github.io/caption-by-committee/",None,-1
118ac28b-3b74-42ad-ac17-c6c7afbee96a,Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs,0.0931086,"Answering first-order logical (FOL) queries over knowledge graphs (KG)
remains a challenging task mainly due to KG incompleteness. Query embedding
approaches this problem by computing the low-dimensional vector representations
of entities, relations, and logical queries. KGs exhibit relational patterns
such as symmetry and composition and modeling the patterns can further enhance
the performance of query embedding models. However, the role of such patterns
in answering FOL queries by query embedding models has not been yet studied in
the literature. In this paper, we fill in this research gap and empower FOL
queries reasoning with pattern inference by introducing an inductive bias that
allows for learning relation patterns. To this end, we develop a novel query
embedding method, RoConE, that defines query regions as geometric cones and
algebraic query operators by rotations in complex space. RoConE combines the
advantages of Cone as a well-specified geometric representation for query
embedding, and also the rotation operator as a powerful algebraic operation for
pattern inference. Our experimental results on several benchmark datasets
confirm the advantage of relational patterns for enhancing logical query
answering task.",None,-1
ba5d99af-e6b1-40eb-b48f-1d79c9b4c0fd,Error Detection for Text-to-SQL Semantic Parsing,0.493814,"Despite remarkable progress in text-to-SQL semantic parsing in recent years,
the performance of existing parsers is still far from perfect. Specifically,
modern text-to-SQL parsers based on deep learning are often over-confident,
thus casting doubt on their trustworthiness when deployed for real use. In this
paper, we propose a parser-independent error detection model for text-to-SQL
semantic parsing. Using a language model of code as its bedrock, we enhance our
error detection model with graph neural networks that learn structural features
of both natural language questions and SQL queries. We train our model on
realistic parsing errors collected from a cross-domain setting, which leads to
stronger generalization ability. Experiments with three strong text-to-SQL
parsers featuring different decoding mechanisms show that our approach
outperforms parser-dependent uncertainty metrics. Our model could also
effectively improve the performance and usability of text-to-SQL semantic
parsers regardless of their architectures. (Our implementation is available at
https://github.com/OSU-NLP-Group/Text2SQL-Error-Detection)",None,-1
0a9c07da-872a-4ab2-be42-79ec9ce84a76,Semantic 3D-aware Portrait Synthesis and Manipulation Based on Compositional Neural Radiance Field,0.276333,"Recently 3D-aware GAN methods with neural radiance field have developed
rapidly. However, current methods model the whole image as an overall neural
radiance field, which limits the partial semantic editability of synthetic
results. Since NeRF renders an image pixel by pixel, it is possible to split
NeRF in the spatial dimension. We propose a Compositional Neural Radiance Field
(CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRF
divides the image by semantic regions and learns an independent neural radiance
field for each region, and finally fuses them and renders the complete image.
Thus we can manipulate the synthesized semantic regions independently, while
fixing the other parts unchanged. Furthermore, CNeRF is also designed to
decouple shape and texture within each semantic region. Compared to
state-of-the-art 3D-aware GAN methods, our approach enables fine-grained
semantic region manipulation, while maintaining high-quality 3D-consistent
synthesis. The ablation studies show the effectiveness of the structure and
loss function used by our method. In addition real image inversion and cartoon
portrait 3D editing experiments demonstrate the application potential of our
method.",None,-1
71f0bda8-abd1-4fe2-a26c-2aed9767da4c,Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models,0.992221,"Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating textual inputs into numerical representations,
capturing the semantics of the text. These models excel in applications like
dense retrieval and semantic textual similarity. This paper details the
development of Jina Embeddings, starting with the creation of high-quality
pairwise and triplet datasets. It underlines the crucial role of data cleaning
in dataset preparation, offers in-depth insights into the model training
process, and concludes with a comprehensive performance evaluation using the
Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model's
awareness of grammatical negation, we construct a novel training and evaluation
dataset of negated and non-negated statements, which we make publicly available
to the community.",None,-1
2e0ed06a-8517-46d9-a279-26339be3f78f,M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,0.892155,"Medical vision-language models enable co-learning and integrating features
from medical imaging and clinical text. However, these models are not easy to
train and the latent representation space can be complex. Here we propose a
novel way for pre-training and regularising medical vision-language models. The
proposed method, named Medical vision-language pre-training with Frozen
language models and Latent spAce Geometry optimization (M-FLAG), leverages a
frozen language model for training stability and efficiency and introduces a
novel orthogonality loss to harmonize the latent space geometry. We demonstrate
the potential of the pre-trained model on three downstream tasks: medical image
classification, segmentation, and object detection. Extensive experiments
across five public datasets demonstrate that M-FLAG significantly outperforms
existing medical vision-language pre-training approaches and reduces the number
of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the
segmentation task while using only 1\% of the RSNA dataset, even outperforming
ImageNet pre-trained models that have been fine-tuned using 100\% of the data.",None,-1
2bc8ad9e-6db6-4e2f-a60f-1c463a4fb8b4,Evolutionary approaches to explainable machine learning,0.202049,"Machine learning models are increasingly being used in critical sectors, but
their black-box nature has raised concerns about accountability and trust. The
field of explainable artificial intelligence (XAI) or explainable machine
learning (XML) has emerged in response to the need for human understanding of
these models. Evolutionary computing, as a family of powerful optimization and
learning tools, has significant potential to contribute to XAI/XML. In this
chapter, we provide a brief introduction to XAI/XML and review various
techniques in current use for explaining machine learning models. We then focus
on how evolutionary computing can be used in XAI/XML, and review some
approaches which incorporate EC techniques. We also discuss some open
challenges in XAI/XML and opportunities for future research in this field using
EC. Our aim is to demonstrate that evolutionary computing is well-suited for
addressing current problems in explainability, and to encourage further
exploration of these methods to contribute to the development of more
transparent, trustworthy and accountable machine learning models.",None,-1
985ccc4c-4416-41e5-8075-e03d4b31ee2f,Rule-based detection of access to education and training in Germany,0.28884,"As a result of transformation processes, the German labor market is highly
dependent on vocational training, retraining and continuing education. To match
training seekers and offers, we present a novel approach towards the automated
detection of access to education and training in German training offers and
advertisements. We will in particular focus on (a) general school and education
degrees and schoolleaving certificates, (b) professional experience, (c) a
previous apprenticeship and (d) a list of skills provided by the German Federal
Employment Agency. This novel approach combines several methods: First, we
provide a mapping of synonyms in education combining different qualifications
and adding deprecated terms. Second, we provide a rule-based matching to
identify the need for professional experience or apprenticeship. However, not
all access requirements can be matched due to incompatible data schemata or
non-standardizes requirements, e.g initial tests or interviews. While we can
identify several shortcomings, the presented approach offers promising results
for two data sets: training and re-training advertisements.",None,-1
c00a8fbf-0db4-4c72-8116-5ba51b036bb3,NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,0.274832,"Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs' performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.",None,-1
2f9a65f1-821c-4127-95b1-383336c9fc09,Deep Equilibrium Object Detection,0.234913,"Query-based object detectors directly decode image features into object
instances with a set of learnable queries. These query vectors are
progressively refined to stable meaningful representations through a sequence
of decoder layers, and then used to directly predict object locations and
categories with simple FFN heads. In this paper, we present a new query-based
object detector (DEQDet) by designing a deep equilibrium decoder. Our DEQ
decoder models the query vector refinement as the fixed point solving of an
{implicit} layer and is equivalent to applying {infinite} steps of refinement.
To be more specific to object decoding, we use a two-step unrolled equilibrium
equation to explicitly capture the query vector refinement. Accordingly, we are
able to incorporate refinement awareness into the DEQ training with the inexact
gradient back-propagation (RAG). In addition, to stabilize the training of our
DEQDet and improve its generalization ability, we devise the deep supervision
scheme on the optimization path of DEQ with refinement-aware
perturbation~(RAP). Our experiments demonstrate DEQDet converges faster,
consumes less memory, and achieves better results than the baseline counterpart
(AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queries
achieves the $49.5$ mAP and $33.0$ AP$_s$ on the MS COCO benchmark under
$2\times$ training scheme (24 epochs).",None,-1
924c78cb-1847-4463-adac-de9bc5323e34,DiffVoice: Text-to-Speech with Latent Diffusion,0.768762,"In this work, we present DiffVoice, a novel text-to-speech model based on
latent diffusion. We propose to first encode speech signals into a phoneme-rate
latent representation with a variational autoencoder enhanced by adversarial
training, and then jointly model the duration and the latent representation
with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS
datasets demonstrate that our method beats the best publicly available systems
in naturalness. By adopting recent generative inverse problem solving
algorithms for diffusion models, DiffVoice achieves the state-of-the-art
performance in text-based speech editing, and zero-shot adaptation.",None,-1
3dacf429-7269-4fe0-ad8b-f9705d335426,On the Foundations of Cycles in Bayesian Networks,0.207411,"Bayesian networks (BNs) are a probabilistic graphical model widely used for
representing expert knowledge and reasoning under uncertainty. Traditionally,
they are based on directed acyclic graphs that capture dependencies between
random variables. However, directed cycles can naturally arise when
cross-dependencies between random variables exist, e.g., for modeling feedback
loops. Existing methods to deal with such cross-dependencies usually rely on
reductions to BNs without cycles. These approaches are fragile to generalize,
since their justifications are intermingled with additional knowledge about the
application context. In this paper, we present a foundational study regarding
semantics for cyclic BNs that are generic and conservatively extend the
cycle-free setting. First, we propose constraint-based semantics that specify
requirements for full joint distributions over a BN to be consistent with the
local conditional probabilities and independencies. Second, two kinds of limit
semantics that formalize infinite unfolding approaches are introduced and shown
to be computable by a Markov chain construction.",None,-1
6e556d17-5604-4d72-910d-9918d538a10d,Improving Fairness in Adaptive Social Exergames via Shapley Bandits,0.356117,"Algorithmic fairness is an essential requirement as AI becomes integrated in
society. In the case of social applications where AI distributes resources,
algorithms often must make decisions that will benefit a subset of users,
sometimes repeatedly or exclusively, while attempting to maximize specific
outcomes. How should we design such systems to serve users more fairly? This
paper explores this question in the case where a group of users works toward a
shared goal in a social exergame called Step Heroes. We identify adverse
outcomes in traditional multi-armed bandits (MABs) and formalize the Greedy
Bandit Problem. We then propose a solution based on a new type of
fairness-aware multi-armed bandit, Shapley Bandits. It uses the Shapley Value
for increasing overall player participation and intervention adherence rather
than the maximization of total group output, which is traditionally achieved by
favoring only high-performing participants. We evaluate our approach via a user
study (n=46). Our results indicate that our Shapley Bandits effectively
mediates the Greedy Bandit Problem and achieves better user retention and
motivation across the participants.",None,-1
0ad9c322-b2fb-41f5-8e29-6fe85e70e7ab,Implicit neural representations for joint decomposition and registration of gene expression images in the marmoset brain,0.605263,"We propose a novel image registration method based on implicit neural
representations that addresses the challenging problem of registering a pair of
brain images with similar anatomical structures, but where one image contains
additional features or artifacts that are not present in the other image. To
demonstrate its effectiveness, we use 2D microscopy $\textit{in situ}$
hybridization gene expression images of the marmoset brain. Accurately
quantifying gene expression requires image registration to a brain template,
which is difficult due to the diversity of patterns causing variations in
visible anatomical brain structures. Our approach uses implicit networks in
combination with an image exclusion loss to jointly perform the registration
and decompose the image into a support and residual image. The support image
aligns well with the template, while the residual image captures individual
image characteristics that diverge from the template. In experiments, our
method provided excellent results and outperformed other registration
techniques.",None,-1
941032eb-3691-4991-9b76-eb5fe58354ad,A Semantic Approach to Negation Detection and Word Disambiguation with Natural Language Processing,0.0465908,"This study aims to demonstrate the methods for detecting negations in a
sentence by uniquely evaluating the lexical structure of the text via
word-sense disambiguation. The proposed framework examines all the unique
features in the various expressions within a text to resolve the contextual
usage of all tokens and decipher the effect of negation on sentiment analysis.
The application of popular expression detectors skips this important step,
thereby neglecting the root words caught in the web of negation and making text
classification difficult for machine learning and sentiment analysis. This
study adopts the Natural Language Processing (NLP) approach to discover and
antonimize words that were negated for better accuracy in text classification
using a knowledge base provided by an NLP library called WordHoard. Early
results show that our initial analysis improved on traditional sentiment
analysis, which sometimes neglects negations or assigns an inverse polarity
score. The SentiWordNet analyzer was improved by 35%, the Vader analyzer by 20%
and the TextBlob by 6%.",None,-1
1abbd963-e043-4012-9bf4-b7ab317d3c0c,HistRED: A Historical Document-Level Relation Extraction Dataset,0.740519,"Despite the extensive applications of relation extraction (RE) tasks in
various domains, little has been explored in the historical context, which
contains promising data across hundreds and thousands of years. To promote the
historical RE research, we present HistRED constructed from Yeonhaengnok.
Yeonhaengnok is a collection of records originally written in Hanja, the
classical Chinese writing, which has later been translated into Korean. HistRED
provides bilingual annotations such that RE can be performed on Korean and
Hanja texts. In addition, HistRED supports various self-contained subtexts with
different lengths, from a sentence level to a document level, supporting
diverse context settings for researchers to evaluate the robustness of their RE
models. To demonstrate the usefulness of our dataset, we propose a bilingual RE
model that leverages both Korean and Hanja contexts to predict relations
between entities. Our model outperforms monolingual baselines on HistRED,
showing that employing multiple language contexts supplements the RE
predictions. The dataset is publicly available at:
https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.",None,-1
4aa1ea26-a43c-42c0-8188-c3c0724f4721,Development and Whole-Body Validation of Personalizable Female and Male Pedestrian SAFER Human Body Models,0.218601,"Vulnerable road users are overrepresented in the worldwide number of
road-traffic injury victims. Developing biofidelic male and female pedestrian
HBMs representing a range of anthropometries is imperative to follow through
with the efforts to increase road safety and propose intervention strategies.
In this study, a 50th percentile male and female pedestrian of the SAFER HBM
was developed via a newly developed image registration-based mesh morphing
framework for subject personalization. The HBM and its accompanied
personalization framework were evaluated by means of a set of cadaver
experiments, where subjects were struck laterally by a generic sedan buck. In
the simulated whole-body pedestrian collisions, the personalized HBMs
demonstrate a good capability of reproducing the trajectories and head
kinematics observed in lateral impacts. The presented pedestrian HBMs and
personalization framework provide robust means to thoroughly and accurately
reconstruct and evaluate pedestrian-to-vehicle collisions.",None,-1
1f372d43-c2f5-4db3-9380-20bf8087bbba,Scene-Aware Feature Matching,0.335546,"Current feature matching methods focus on point-level matching, pursuing
better representation learning of individual features, but lacking further
understanding of the scene. This results in significant performance degradation
when handling challenging scenes such as scenes with large viewpoint and
illumination changes. To tackle this problem, we propose a novel model named
SAM, which applies attentional grouping to guide Scene-Aware feature Matching.
SAM handles multi-level features, i.e., image tokens and group tokens, with
attention layers, and groups the image tokens with the proposed token grouping
module. Our model can be trained by ground-truth matches only and produce
reasonable grouping results. With the sense-aware grouping guidance, SAM is not
only more accurate and robust but also more interpretable than conventional
feature matching models. Sufficient experiments on various applications,
including homography estimation, pose estimation, and image matching,
demonstrate that our model achieves state-of-the-art performance.",None,-1
b34e937b-ac3c-470f-95c6-4e5778595921,Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs,0.924968,"Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and
can solve different tasks due to their emergent ability and generalizability.
However, LLMs sometimes lack domain-specific knowledge to perform tasks, which
would also cause hallucination during inference. In some previous works,
additional modules like graph neural networks (GNNs) are trained on retrieved
knowledge from external knowledge bases, aiming to mitigate the problem of
lacking domain-specific knowledge. However, incorporating additional modules:
1) would need retraining additional modules when encountering novel domains; 2)
would become a bottleneck since LLMs' strong abilities are not fully utilized
for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver
(KSL), to teach LLMs to search for essential knowledge from external knowledge
bases by harnessing their own strong generalizability. Specifically, we design
a simple yet effective prompt to transform retrieval into a multi-hop decision
sequence, which empowers LLMs with searching knowledge ability in zero-shot
manner. Additionally, KSL is able to provide complete retrieval paths and
therefore increase explainability of LLMs' reasoning processes. We conduct
experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and
found that our approach improves LLM baseline performance by a relatively large
margin.",None,-1
de9197a2-50dc-47dd-95fb-3661838f3378,QAID: Question Answering Inspired Few-shot Intent Detection,0.486213,"Intent detection with semantically similar fine-grained intents is a
challenging task. To address it, we reformulate intent detection as a
question-answering retrieval task by treating utterances and intent names as
questions and answers. To that end, we utilize a question-answering retrieval
architecture and adopt a two stages training schema with batch contrastive
loss. In the pre-training stage, we improve query representations through
self-supervised training. Then, in the fine-tuning stage, we increase
contextualized token-level similarity scores between queries and answers from
the same intent. Our results on three few-shot intent detection benchmarks
achieve state-of-the-art performance.",None,-1
20710eae-3940-4fb3-99c6-a2ede4a0b381,RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,0.999399,"Large Language Models (LLMs) have greatly advanced code auto-completion
systems, with a potential for substantial productivity enhancements for
developers. However, current benchmarks mainly focus on single-file tasks,
leaving an assessment gap for more complex, real-world, multi-file programming
scenarios. To fill this gap, we introduce RepoBench, a new benchmark
specifically designed for evaluating repository-level code auto-completion
systems. RepoBench supports both Python and Java and consists of three
interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code
Completion), and RepoBench-P (Pipeline). Each task respectively measures the
system's ability to retrieve the most relevant code snippets from other files
as cross-file context, predict the next line of code with cross-file and
in-file context, and handle complex tasks that require a combination of both
retrieval and next-line prediction. RepoBench aims to facilitate a more
complete comparison of performance and encouraging continuous improvement in
auto-completion systems. RepoBench is publicly available at
https://github.com/Leolty/repobench.",None,-1
436249c3-5db7-49d2-938a-84108f67e823,Integrating Audio-Visual Features for Multimodal Deepfake Detection,0.904969,"Deepfakes are AI-generated media in which an image or video has been
digitally modified. The advancements made in deepfake technology have led to
privacy and security issues. Most deepfake detection techniques rely on the
detection of a single modality. Existing methods for audio-visual detection do
not always surpass that of the analysis based on single modalities. Therefore,
this paper proposes an audio-visual-based method for deepfake detection, which
integrates fine-grained deepfake identification with binary classification. We
categorize the samples into four types by combining labels specific to each
single modality. This method enhances the detection under intra-domain and
cross-domain testing.",None,-1
25aa282d-2848-4100-9a36-bc21ec3700ea,Scalable Knowledge Graph Construction and Inference on Human Genome Variants,0.439751,"Real-world knowledge can be represented as a graph consisting of entities and
relationships between the entities. The need for efficient and scalable
solutions arises when dealing with vast genomic data, like RNA-sequencing.
Knowledge graphs offer a powerful approach for various tasks in such
large-scale genomic data, such as analysis and inference. In this work,
variant-level information extracted from the RNA-sequences of vaccine-na\""ive
COVID-19 patients have been represented as a unified, large knowledge graph.
Variant call format (VCF) files containing the variant-level information were
annotated to include further information for each variant. The data records in
the annotated files were then converted to Resource Description Framework (RDF)
triples. Each VCF file obtained had an associated CADD scores file that
contained the raw and Phred-scaled scores for each variant. An ontology was
defined for the VCF and CADD scores files. Using this ontology and the
extracted information, a large, scalable knowledge graph was created. Available
graph storage was then leveraged to query and create datasets for further
downstream tasks. We also present a case study using the knowledge graph and
perform a classification task using graph machine learning. We also draw
comparisons between different Graph Neural Networks (GNNs) for the case study.",None,-1
eb7600fc-8328-4c95-b720-4ae2d70e6b30,LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery,0.643008,"Large Language Models (LLMs) have transformed the landscape of artificial
intelligence, while their enormous size presents significant challenges in
terms of computational costs. We introduce LoRAShear, a novel efficient
approach to structurally prune LLMs and recover knowledge. Given general LLMs,
LoRAShear at first creates the dependency graphs over LoRA modules to discover
minimally removal structures and analyze the knowledge distribution. It then
proceeds progressive structured pruning on LoRA adaptors and enables inherent
knowledge transfer to better preserve the information in the redundant
structures. To recover the lost knowledge during pruning, LoRAShear
meticulously studies and proposes a dynamic fine-tuning schemes with dynamic
data adaptors to effectively narrow down the performance gap to the full
models. Numerical results demonstrate that by only using one GPU within a
couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with
only 1.0% performance degradation and significantly outperforms
state-of-the-arts. The source code will be available at
https://github.com/microsoft/lorashear.",None,-1
9978a14e-5cf6-4c0a-954d-62572350da51,Automated Reading Passage Generation with OpenAI's Large Language Model,0.462899,"The widespread usage of computer-based assessments and individualized
learning platforms has resulted in an increased demand for the rapid production
of high-quality items. Automated item generation (AIG), the process of using
item models to generate new items with the help of computer technology, was
proposed to reduce reliance on human subject experts at each step of the
process. AIG has been used in test development for some time. Still, the use of
machine learning algorithms has introduced the potential to improve the
efficiency and effectiveness of the process greatly. The approach presented in
this paper utilizes OpenAI's latest transformer-based language model, GPT-3, to
generate reading passages. Existing reading passages were used in carefully
engineered prompts to ensure the AI-generated text has similar content and
structure to a fourth-grade reading passage. For each prompt, we generated
multiple passages, the final passage was selected according to the Lexile score
agreement with the original passage. In the final round, the selected passage
went through a simple revision by a human editor to ensure the text was free of
any grammatical and factual errors. All AI-generated passages, along with
original passages were evaluated by human judges according to their coherence,
appropriateness to fourth graders, and readability.",None,-1
12e5a9f1-15c1-4127-9d66-f245e4fbf28a,Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following,0.344509,"In this paper, we present our finding that prepending a Task-Agnostic Prefix
Prompt (TAPP) to the input improves the instruction-following ability of
various Large Language Models (LLMs) during inference. TAPP is different from
canonical prompts for LLMs in that it is a fixed prompt prepended to the
beginning of every input regardless of the target task for zero-shot
generalization. We observe that both base LLMs (i.e. not fine-tuned to follow
instructions) and instruction-tuned models benefit from TAPP, resulting in
34.58% and 12.26% improvement on average, respectively. This implies that the
instruction-following ability of LLMs can be improved during inference time
with a fixed prompt constructed with simple heuristics. We hypothesize that
TAPP assists language models to better estimate the output distribution by
focusing more on the instruction of the target task during inference. In other
words, such ability does not seem to be sufficiently activated in not only base
LLMs but also many instruction-fine-tuned LLMs. All experiments are
reproducible from https://github.com/seonghyeonye/TAPP.",None,-1
626ffad7-a069-4c38-9b52-d98bd6421eba,A Game of Bundle Adjustment -- Learning Efficient Convergence,0.294678,"Bundle adjustment is the common way to solve localization and mapping. It is
an iterative process in which a system of non-linear equations is solved using
two optimization methods, weighted by a damping factor. In the classic
approach, the latter is chosen heuristically by the Levenberg-Marquardt
algorithm on each iteration. This might take many iterations, making the
process computationally expensive, which might be harmful to real-time
applications. We propose to replace this heuristic by viewing the problem in a
holistic manner, as a game, and formulating it as a reinforcement-learning
task. We set an environment which solves the non-linear equations and train an
agent to choose the damping factor in a learned manner. We demonstrate that our
approach considerably reduces the number of iterations required to reach the
bundle adjustment's convergence, on both synthetic and real-life scenarios. We
show that this reduction benefits the classic approach and can be integrated
with other bundle adjustment acceleration methods.",None,-1
48c7f5c4-7db1-46ee-9f9e-bb5c35b2fd8b,From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data,0.703824,"Large Language Models (LLMs) exhibit exceptional abilities for causal
analysis between concepts in numerous societally impactful domains, including
medicine, science, and law. Recent research on LLM performance in various
causal discovery and inference tasks has given rise to a new ladder in the
classical three-stage framework of causality. In this paper, we advance the
current research of LLM-driven causal discovery by proposing a novel framework
that combines knowledge-based LLM causal analysis with data-driven causal
structure learning. To make LLM more than a query tool and to leverage its
power in discovering natural and new laws of causality, we integrate the
valuable LLM expertise on existing causal mechanisms into statistical analysis
of objective data to build a novel and practical baseline for causal structure
learning.
  We introduce a universal set of prompts designed to extract causal graphs
from given variables and assess the influence of LLM prior causality on
recovering causal structures from data. We demonstrate the significant
enhancement of LLM expertise on the quality of recovered causal structures from
data, while also identifying critical challenges and issues, along with
potential approaches to address them. As a pioneering study, this paper aims to
emphasize the new frontier that LLMs are opening for classical causal discovery
and inference, and to encourage the widespread adoption of LLM capabilities in
data-driven causal analysis.",None,-1
0a61e500-0c8e-4225-8cc5-68c6d9e99815,Visual-Language Prompt Tuning with Knowledge-guided Context Optimization,0.864916,"Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.",None,-1
4ece9c20-d878-4b1d-9ea2-89a7c83414cd,Artificial Intelligence for Drug Discovery: Are We There Yet?,0.908088,"Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages.",None,-1
0b0a2079-fee7-44ea-8941-7d16f823ec78,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,0.294249,"Monkeypox is a rare disease that raised concern among medical specialists
following the convi-19 pandemic. It's concerning since monkeypox is difficult
to diagnose early on because of symptoms that are similar to chickenpox and
measles. Furthermore, because this is a rare condition, there is a knowledge
gap among healthcare professionals. As a result, there is an urgent need for a
novel technique to combat and anticipate the disease in the early phases of
individual virus infection. Multiple CNN-based pre-trained models, including
VGG-16, VGG-19, Restnet50, Inception-V3, Densnet, Xception, MobileNetV2,
Alexnet, Lenet, and majority Voting, were employed in classification in this
study. For this study, multiple data sets were combined, such as monkeypox vs
chickenpox, monkeypox versus measles, monkeypox versus normal, and monkeypox
versus all diseases. Majority voting performed 97% in monkeypox vs chickenpox,
Xception achieved 79% in monkeypox against measles, MobileNetV2 scored 96% in
monkeypox vs normal, and Lenet performed 80% in monkeypox versus all.",None,-1
fade1260-4289-4f16-8f1b-eed60463740b,Multi-class Categorization of Reasons behind Mental Disturbance in Long Texts,0.364936,"Motivated with recent advances in inferring users' mental state in social
media posts, we identify and formulate the problem of finding causal indicators
behind mental illness in self-reported text. In the past, we witness the
presence of rule-based studies for causal explanation analysis on curated
Facebook data. The investigation on transformer-based model for multi-class
causal categorization in Reddit posts point to a problem of using long-text
which contains as many as 4000 words. Developing end-to-end transformer-based
models subject to the limitation of maximum-length in a given instance. To
handle this problem, we use Longformer and deploy its encoding on
transformer-based classifier. The experimental results show that Longformer
achieves new state-of-the-art results on M-CAMS, a publicly available dataset
with 62\% F1-score. Cause-specific analysis and ablation study prove the
effectiveness of Longformer. We believe our work facilitates causal analysis of
depression and suicide risk on social media data, and shows potential for
application on other mental health conditions.",None,-1
fd3cec4f-7a9e-4979-ab11-0787ee68f873,Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer,0.293179,"Zero-shot cross-lingual transfer is a central task in multilingual NLP,
allowing models trained in languages with more sufficient training resources to
generalize to other low-resource languages. Earlier efforts on this task use
parallel corpora, bilingual dictionaries, or other annotated alignment data to
improve cross-lingual transferability, which are typically expensive to obtain.
In this paper, we propose a simple yet effective method, SALT, to improve the
zero-shot cross-lingual transfer of the multilingual pretrained language models
without the help of such external data. By incorporating code-switching and
embedding mixup with self-augmentation, SALT effectively distills cross-lingual
knowledge from the multilingual PLM and enhances its transferability on
downstream tasks. Experimental results on XNLI and PAWS-X show that our method
is able to improve zero-shot cross-lingual transferability without external
data. Our code is available at https://github.com/luka-group/SALT.",None,-1
691b151f-4eca-4b1a-b2f8-1bd0d72ff12d,SimHaze: game engine simulated data for real-world dehazing,0.137924,"Deep models have demonstrated recent success in single-image dehazing. Most
prior methods consider fully supervised training and learn from paired clean
and hazy images, where a hazy image is synthesized based on a clean image and
its estimated depth map. This paradigm, however, can produce low-quality hazy
images due to inaccurate depth estimation, resulting in poor generalization of
the trained models. In this paper, we explore an alternative approach for
generating paired clean-hazy images by leveraging computer graphics. Using a
modern game engine, our approach renders crisp clean images and their precise
depth maps, based on which high-quality hazy images can be synthesized for
training dehazing models. To this end, we present SimHaze: a new synthetic haze
dataset. More importantly, we show that training with SimHaze alone allows the
latest dehazing models to achieve significantly better performance in
comparison to previous dehazing datasets. Our dataset and code will be made
publicly available.",None,-1
e07ff86d-f700-449a-8e71-7a10c36911db,Retrieval-augmented Image Captioning,0.607163,"Inspired by retrieval-augmented language generation and pretrained Vision and
Language (V&L) encoders, we present a new approach to image captioning that
generates sentences given the input image and a set of captions retrieved from
a datastore, as opposed to the image alone. The encoder in our model jointly
processes the image and retrieved captions using a pretrained V&L BERT, while
the decoder attends to the multimodal encoder representations, benefiting from
the extra textual evidence from the retrieved captions. Experimental results on
the COCO dataset show that image captioning can be effectively formulated from
this new perspective. Our model, named EXTRA, benefits from using captions
retrieved from the training dataset, and it can also benefit from using an
external dataset without the need for retraining. Ablation studies show that
retrieving a sufficient number of captions (e.g., k=5) can improve captioning
quality. Our work contributes towards using pretrained V&L encoders for
generative tasks, instead of standard classification tasks.",None,-1
b36fed3f-ed3a-4795-9f69-d84e9d85babf,FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain,0.398526,"This paper introduces FrenchMedMCQA, the first publicly available
Multiple-Choice Question Answering (MCQA) dataset in French for medical domain.
It is composed of 3,105 questions taken from real exams of the French medical
specialization diploma in pharmacy, mixing single and multiple answers. Each
instance of the dataset contains an identifier, a question, five possible
answers and their manual correction(s). We also propose first baseline models
to automatically process this MCQA task in order to report on the current
performances and to highlight the difficulty of the task. A detailed analysis
of the results showed that it is necessary to have representations adapted to
the medical domain or to the MCQA task: in our case, English specialized models
yielded better results than generic French ones, even though FrenchMedMCQA is
in French. Corpus, models and tools are available online.",None,-1
c541c37d-008e-4a6f-bcd6-8b9012181b8e,Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse,0.503415,"Online violence against children has increased globally recently, demanding
urgent attention. Competent authorities manually analyze abuse complaints to
comprehend crime dynamics and identify patterns. However, the manual analysis
of these complaints presents a challenge because it exposes analysts to harmful
content during the review process. Given these challenges, we present a novel
solution, an automated tool designed to analyze children's sexual abuse reports
comprehensively. By automating the analysis process, our tool significantly
reduces the risk of exposure to harmful content by categorizing the reports on
three dimensions: Subject, Degree of Criminality, and Damage. Furthermore,
leveraging our multidisciplinary team's expertise, we introduce a novel
approach to annotate the collected data, enabling a more in-depth analysis of
the reports. This approach improves the comprehension of fundamental patterns
and trends, enabling law enforcement agencies and policymakers to create
focused strategies in the fight against children's violence.",None,-1
00c4db51-d9e5-4a0e-98e6-7c5f4dd8f2e9,UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM,0.868616,"We present an uncertainty learning framework for dense neural simultaneous
localization and mapping (SLAM). Estimating pixel-wise uncertainties for the
depth input of dense SLAM methods allows re-weighing the tracking and mapping
losses towards image regions that contain more suitable information that is
more reliable for SLAM. To this end, we propose an online framework for sensor
uncertainty estimation that can be trained in a self-supervised manner from
only 2D input data. We further discuss the advantages of the uncertainty
learning for the case of multi-sensor input. Extensive analysis,
experimentation, and ablations show that our proposed modeling paradigm
improves both mapping and tracking accuracy and often performs better than
alternatives that require ground truth depth or 3D. Our experiments show that
we achieve a 38\% and 27\% lower absolute trajectory tracking error (ATE) on
the 7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset
using two types of depth sensors, we report an 11\% F1-score improvement on
RGBD SLAM compared to the recent state-of-the-art neural implicit approaches.
Source code: https://github.com/kev-in-ta/UncLe-SLAM.",None,-1
f7af4d3b-186c-48a8-aff9-08f12f7fe1a3,Contrastive Grouping with Transformer for Referring Image Segmentation,0.867403,"Referring image segmentation aims to segment the target referent in an image
conditioning on a natural language expression. Existing one-stage methods
employ per-pixel classification frameworks, which attempt straightforwardly to
align vision and language at the pixel level, thus failing to capture critical
object-level information. In this paper, we propose a mask classification
framework, Contrastive Grouping with Transformer network (CGFormer), which
explicitly captures object-level information via token-based querying and
grouping strategy. Specifically, CGFormer first introduces learnable query
tokens to represent objects and then alternately queries linguistic features
and groups visual features into the query tokens for object-aware cross-modal
reasoning. In addition, CGFormer achieves cross-level interaction by jointly
updating the query tokens and decoding masks in every two consecutive layers.
Finally, CGFormer cooperates contrastive learning to the grouping strategy to
identify the token and its mask corresponding to the referent. Experimental
results demonstrate that CGFormer outperforms state-of-the-art methods in both
segmentation and generalization settings consistently and significantly.",None,-1
a7b92c35-ddd1-4e95-a663-c46d0a8c3466,Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation,0.0320608,"The decoding algorithm is critical for open-ended text generation,
transforming latent representations into coherent and meaningful outputs. This
paper investigates the self-reinforcement effect in text generation and the
effectiveness of a repetition penalty to mitigate it. However, determining the
optimal repetition penalty value is challenging. To tackle this, we propose a
forgetting mechanism that disregards distant tokens, reducing the burden of
penalty selection. In addition, we introduce a length penalty to address overly
short sentences caused by excessive penalties. Our penalty decoding approach
incorporating three strategies helps resolve issues with sampling methods
deviating from factual information. Experimental results demonstrate the
efficacy of our approach in generating high-quality sentences resembling human
output.",None,-1
8d48283e-9aa3-4c3e-8c89-9d19a4118cb6,BAD: BiAs Detection for Large Language Models in the context of candidate screening,0.617272,"Application Tracking Systems (ATS) have allowed talent managers, recruiters,
and college admissions committees to process large volumes of potential
candidate applications efficiently. Traditionally, this screening process was
conducted manually, creating major bottlenecks due to the quantity of
applications and introducing many instances of human bias. The advent of large
language models (LLMs) such as ChatGPT and the potential of adopting methods to
current automated application screening raises additional bias and fairness
issues that must be addressed. In this project, we wish to identify and
quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the
context of candidate screening in order to demonstrate how the use of these
models could perpetuate existing biases and inequalities in the hiring process.",None,-1
8f659afb-cba8-4d08-956e-3d6dad722ad2,A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3,0.832124,"LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art
language model GPT-3, fine-tuned for the legal domain. The system is designed
to provide legal assistance to users in a conversational manner, helping them
with tasks such as answering legal questions, generating legal documents, and
providing legal advice. In this paper, we provide a brief overview of LawGPT
1.0, its architecture, and its performance on a set of legal benchmark tasks.
Please note that the detailed information about the model is protected by a
non-disclosure agreement (NDA) and cannot be disclosed in this report.",None,-1
12adca27-977d-4bc9-852e-41f011bbb1cf,Underwater object classification combining SAS and transferred optical-to-SAS Imagery,0.0829241,"Combining synthetic aperture sonar (SAS) imagery with optical images for
underwater object classification has the potential to overcome challenges such
as water clarity, the stability of the optical image analysis platform, and
strong reflections from the seabed for sonar-based classification. In this
work, we propose this type of multi-modal combination to discriminate between
man-made targets and objects such as rocks or litter. We offer a novel
classification algorithm that overcomes the problem of intensity and object
formation differences between the two modalities. To this end, we develop a
novel set of geometrical shape descriptors that takes into account the
geometrical relation between the objects shadow and highlight. Results from
7,052 pairs of SAS and optical images collected during several sea experiments
show improved classification performance compared to the state-of-the-art for
better discrimination between different types of underwater objects. For
reproducibility, we share our database.",None,-1
e501e5a8-b951-457a-963a-ebd677b75ed4,Understanding Imbalanced Semantic Segmentation Through Neural Collapse,0.69733,"A recent study has shown a phenomenon called neural collapse in that the
within-class means of features and the classifier weight vectors converge to
the vertices of a simplex equiangular tight frame at the terminal phase of
training for classification. In this paper, we explore the corresponding
structures of the last-layer feature centers and classifiers in semantic
segmentation. Based on our empirical and theoretical analysis, we point out
that semantic segmentation naturally brings contextual correlation and
imbalanced distribution among classes, which breaks the equiangular and
maximally separated structure of neural collapse for both feature centers and
classifiers. However, such a symmetric structure is beneficial to
discrimination for the minor classes. To preserve these advantages, we
introduce a regularizer on feature centers to encourage the network to learn
features closer to the appealing structure in imbalanced semantic segmentation.
Experimental results show that our method can bring significant improvements on
both 2D and 3D semantic segmentation benchmarks. Moreover, our method ranks 1st
and sets a new record (+6.8% mIoU) on the ScanNet200 test leaderboard. Code
will be available at https://github.com/dvlab-research/Imbalanced-Learning.",None,-1
ffbd2b40-740b-4546-bae9-d94ec2327b4f,ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting,0.576065,"Recent advances in neural rendering have shown great potential for
reconstructing scenes from multiview images. However, accurately representing
objects with glossy surfaces remains a challenge for existing methods. In this
work, we introduce ENVIDR, a rendering and modeling framework for high-quality
rendering and reconstruction of surfaces with challenging specular reflections.
To achieve this, we first propose a novel neural renderer with decomposed
rendering components to learn the interaction between surface and environment
lighting. This renderer is trained using existing physically based renderers
and is decoupled from actual scene representations. We then propose an
SDF-based neural surface model that leverages this learned neural renderer to
represent general scenes. Our model additionally synthesizes indirect
illuminations caused by inter-reflections from shiny surfaces by marching
surface-reflected rays. We demonstrate that our method outperforms state-of-art
methods on challenging shiny scenes, providing high-quality rendering of
specular reflections while also enabling material editing and scene relighting.",None,-1
429225bd-c3a7-4c53-8722-bdf7daae9d30,MSdocTr-Lite: A Lite Transformer for Full Page Multi-script Handwriting Recognition,0.510608,"The Transformer has quickly become the dominant architecture for various
pattern recognition tasks due to its capacity for long-range representation.
However, transformers are data-hungry models and need large datasets for
training. In Handwritten Text Recognition (HTR), collecting a massive amount of
labeled data is a complicated and expensive task. In this paper, we propose a
lite transformer architecture for full-page multi-script handwriting
recognition. The proposed model comes with three advantages: First, to solve
the common problem of data scarcity, we propose a lite transformer model that
can be trained on a reasonable amount of data, which is the case of most HTR
public datasets, without the need for external data. Second, it can learn the
reading order at page-level thanks to a curriculum learning strategy, allowing
it to avoid line segmentation errors, exploit a larger context and reduce the
need for costly segmentation annotations. Third, it can be easily adapted to
other scripts by applying a simple transfer-learning process using only
page-level labeled images. Extensive experiments on different datasets with
different scripts (French, English, Spanish, and Arabic) show the effectiveness
of the proposed model.",None,-1
84b13cca-402d-415b-88c7-52e684a09f53,Unsupervised Domain Adaptation for Training Event-Based Networks Using Contrastive Learning and Uncorrelated Conditioning,0.626462,"Event-based cameras offer reliable measurements for preforming computer
vision tasks in high-dynamic range environments and during fast motion
maneuvers. However, adopting deep learning in event-based vision faces the
challenge of annotated data scarcity due to recency of event cameras.
Transferring the knowledge that can be obtained from conventional camera
annotated data offers a practical solution to this challenge. We develop an
unsupervised domain adaptation algorithm for training a deep network for
event-based data image classification using contrastive learning and
uncorrelated conditioning of data. Our solution outperforms the existing
algorithms for this purpose.",None,-1
49751f1c-95a8-40dc-b647-ee3c430a2a6e,Generating Data for Symbolic Language with Large Language Models,0.224437,"While large language models (LLMs) bring not only performance but also
complexity, recent work has started to turn LLMs into data generators rather
than task inferencers, where another affordable task model is trained for
efficient deployment and inference. However, such an approach has primarily
been applied to natural language tasks and has not yet been explored for
symbolic language tasks with complex structured outputs (e.g., semantic parsing
and code generation). In this paper, we propose SymGen which utilizes LLMs for
generating various annotation-expensive symbolic language data. SymGen consists
of an informative prompt to steer generation and an agreement-based verifier to
improve data correctness. We conduct extensive experiments on six symbolic
language tasks across various settings. Compared with the LLMs, we demonstrate
the 1\%-sized task model can achieve comparable or better performance, largely
cutting inference and deployment costs. We also show that generated data with
only a few human demonstrations can be as effective as over 10 times the amount
of human-annotated data when training the task model, saving a considerable
amount of annotation effort. SymGen sheds new light on data generation for
complex tasks, and we release the code at
\href{https://github.com/HKUNLP/SymGen}{https://github.com/HKUNLP/SymGen}.",None,-1
99b081f5-4a03-4d1a-ac9e-f09631548272,Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness,0.573949,"Learning from raw high dimensional data via interaction with a given
environment has been effectively achieved through the utilization of deep
neural networks. Yet the observed degradation in policy performance caused by
imperceptible worst-case policy dependent translations along high sensitivity
directions (i.e. adversarial perturbations) raises concerns on the robustness
of deep reinforcement learning policies. In our paper, we show that these high
sensitivity directions do not lie only along particular worst-case directions,
but rather are more abundant in the deep neural policy landscape and can be
found via more natural means in a black-box setting. Furthermore, we show that
vanilla training techniques intriguingly result in learning more robust
policies compared to the policies learnt via the state-of-the-art adversarial
training techniques. We believe our work lays out intriguing properties of the
deep reinforcement learning policy manifold and our results can help to build
robust and generalizable deep reinforcement learning policies.",None,-1
1b9b674f-d293-4721-b2ca-e13cf88f169f,The First Proven Performance Guarantees for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) on a Combinatorial Optimization Problem,0.989897,"The Non-dominated Sorting Genetic Algorithm-II (NSGA-II) is one of the most
prominent algorithms to solve multi-objective optimization problems. Recently,
the first mathematical runtime guarantees have been obtained for this
algorithm, however only for synthetic benchmark problems.
  In this work, we give the first proven performance guarantees for a classic
optimization problem, the NP-complete bi-objective minimum spanning tree
problem. More specifically, we show that the NSGA-II with population size $N
\ge 4((n-1) w_{\max} + 1)$ computes all extremal points of the Pareto front in
an expected number of $O(m^2 n w_{\max} \log(n w_{\max}))$ iterations, where
$n$ is the number of vertices, $m$ the number of edges, and $w_{\max}$ is the
maximum edge weight in the problem instance. This result confirms, via
mathematical means, the good performance of the NSGA-II observed empirically.
It also shows that mathematical analyses of this algorithm are not only
possible for synthetic benchmark problems, but also for more complex
combinatorial optimization problems.
  As a side result, we also obtain a new analysis of the performance of the
global SEMO algorithm on the bi-objective minimum spanning tree problem, which
improves the previous best result by a factor of $|F|$, the number of extremal
points of the Pareto front, a set that can be as large as $n w_{\max}$. The
main reason for this improvement is our observation that both multi-objective
evolutionary algorithms find the different extremal points in parallel rather
than sequentially, as assumed in the previous proofs.",None,-1
0e135730-2959-483e-ba68-5d5bad3fb575,Literal-Aware Knowledge Graph Embedding for Welding Quality Monitoring: A Bosch Case,0.317137,"Recently there has been a series of studies in knowledge graph embedding
(KGE), which attempts to learn the embeddings of the entities and relations as
numerical vectors and mathematical mappings via machine learning (ML). However,
there has been limited research that applies KGE for industrial problems in
manufacturing. This paper investigates whether and to what extent KGE can be
used for an important problem: quality monitoring for welding in manufacturing
industry, which is an impactful process accounting for production of millions
of cars annually. The work is in line with Bosch research of data-driven
solutions that intends to replace the traditional way of destroying cars, which
is extremely costly and produces waste. The paper tackles two very challenging
questions simultaneously: how large the welding spot diameter is; and to which
car body the welded spot belongs to. The problem setting is difficult for
traditional ML because there exist a high number of car bodies that should be
assigned as class labels. We formulate the problem as link prediction, and
experimented popular KGE methods on real industry data, with consideration of
literals. Our results reveal both limitations and promising aspects of adapted
KGE methods.",None,-1
8f495bef-83cc-4418-aada-9e3270135040,Language Models Trained on Media Diets Can Predict Public Opinion,0.880255,"Public opinion reflects and shapes societal behavior, but the traditional
survey-based tools to measure it are limited. We introduce a novel approach to
probe media diet models -- language models adapted to online news, TV
broadcast, or radio show content -- that can emulate the opinions of
subpopulations that have consumed a set of media. To validate this method, we
use as ground truth the opinions expressed in U.S. nationally representative
surveys on COVID-19 and consumer confidence. Our studies indicate that this
approach is (1) predictive of human judgements found in survey response
distributions and robust to phrasing and channels of media exposure, (2) more
accurate at modeling people who follow media more closely, and (3) aligned with
literature on which types of opinions are affected by media consumption.
Probing language models provides a powerful new method for investigating media
effects, has practical applications in supplementing polls and forecasting
public opinion, and suggests a need for further study of the surprising
fidelity with which neural language models can predict human responses.",None,-1
0ba6d750-4b77-4b9f-965d-ab64fbd9abe6,GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions,0.812818,"There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings",None,-1
ec004937-7c47-43d7-93cd-603736092ae1,Prediction-Powered Inference,0.871716,"Prediction-powered inference is a framework for performing valid statistical
inference when an experimental dataset is supplemented with predictions from a
machine-learning system. The framework yields simple algorithms for computing
provably valid confidence intervals for quantities such as means, quantiles,
and linear and logistic regression coefficients, without making any assumptions
on the machine-learning algorithm that supplies the predictions. Furthermore,
more accurate predictions translate to smaller confidence intervals.
Prediction-powered inference could enable researchers to draw valid and more
data-efficient conclusions using machine learning. The benefits of
prediction-powered inference are demonstrated with datasets from proteomics,
astronomy, genomics, remote sensing, census analysis, and ecology.",None,-1
95405879-1c50-4240-af7a-fc26df1673ed,DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion,0.12456,"We introduce a novel method to automatically generate an artistic typography
by stylizing one or more letter fonts to visually convey the semantics of an
input word, while ensuring that the output remains readable. To address an
assortment of challenges with our task at hand including conflicting goals
(artistic stylization vs. legibility), lack of ground truth, and immense search
space, our approach utilizes large language models to bridge texts and visual
images for stylization and build an unsupervised generative model with a
diffusion model backbone. Specifically, we employ the denoising generator in
Latent Diffusion Model (LDM), with the key addition of a CNN-based
discriminator to adapt the input style onto the input text. The discriminator
uses rasterized images of a given letter/word font as real samples and output
of the denoising generator as fake samples. Our model is coined DS-Fusion for
discriminated and stylized diffusion. We showcase the quality and versatility
of our method through numerous examples, qualitative and quantitative
evaluation, as well as ablation studies. User studies comparing to strong
baselines including CLIPDraw and DALL-E 2, as well as artist-crafted
typographies, demonstrate strong performance of DS-Fusion.",None,-1
2ca4c886-a753-41ae-961b-96c63f9fc9a8,H-TSP: Hierarchically Solving the Large-Scale Travelling Salesman Problem,0.994799,"We propose an end-to-end learning framework based on hierarchical
reinforcement learning, called H-TSP, for addressing the large-scale Travelling
Salesman Problem (TSP). The proposed H-TSP constructs a solution of a TSP
instance starting from the scratch relying on two components: the upper-level
policy chooses a small subset of nodes (up to 200 in our experiment) from all
nodes that are to be traversed, while the lower-level policy takes the chosen
nodes as input and outputs a tour connecting them to the existing partial route
(initially only containing the depot). After jointly training the upper-level
and lower-level policies, our approach can directly generate solutions for the
given TSP instances without relying on any time-consuming search procedures. To
demonstrate effectiveness of the proposed approach, we have conducted extensive
experiments on randomly generated TSP instances with different numbers of
nodes. We show that H-TSP can achieve comparable results (gap 3.42% vs. 7.32%)
as SOTA search-based approaches, and more importantly, we reduce the time
consumption up to two orders of magnitude (3.32s vs. 395.85s). To the best of
our knowledge, H-TSP is the first end-to-end deep reinforcement learning
approach that can scale to TSP instances of up to 10000 nodes. Although there
are still gaps to SOTA results with respect to solution quality, we believe
that H-TSP will be useful for practical applications, particularly those that
are time-sensitive e.g., on-call routing and ride hailing service.",None,-1
655db86e-20f8-4281-ab3e-5a9b8de4fb29,Chat Translation Error Detection for Assisting Cross-lingual Communications,0.357306,"In this paper, we describe the development of a communication support system
that detects erroneous translations to facilitate crosslingual communications
due to the limitations of current machine chat translation methods. We trained
an error detector as the baseline of the system and constructed a new
Japanese-English bilingual chat corpus, BPersona-chat, which comprises
multiturn colloquial chats augmented with crowdsourced quality ratings. The
error detector can serve as an encouraging foundation for more advanced
erroneous translation detection systems.",None,-1
c73356a8-f1b3-4a5d-be17-34ec95f3ed3e,Empirical study of the modulus as activation function in computer vision applications,0.509669,"In this work we propose a new non-monotonic activation function: the modulus.
The majority of the reported research on nonlinearities is focused on monotonic
functions. We empirically demonstrate how by using the modulus activation
function on computer vision tasks the models generalize better than with other
nonlinearities - up to a 15% accuracy increase in CIFAR100 and 4% in CIFAR10,
relative to the best of the benchmark activations tested. With the proposed
activation function the vanishing gradient and dying neurons problems
disappear, because the derivative of the activation function is always 1 or -1.
The simplicity of the proposed function and its derivative make this solution
specially suitable for TinyML and hardware applications.",None,-1
3f58438c-85e7-4a36-a5a2-017bf40141b1,Zero-Shot Image Harmonization with Generative Model Prior,0.416548,"We propose a zero-shot approach to image harmonization, aiming to overcome
the reliance on large amounts of synthetic composite images in existing
methods. These methods, while showing promising results, involve significant
training expenses and often struggle with generalization to unseen images. To
this end, we introduce a fully modularized framework inspired by human
behavior. Leveraging the reasoning capabilities of recent foundation models in
language and vision, our approach comprises three main stages. Initially, we
employ a pretrained vision-language model (VLM) to generate descriptions for
the composite image. Subsequently, these descriptions guide the foreground
harmonization direction of a text-to-image generative model (T2I). We refine
text embeddings for enhanced representation of imaging conditions and employ
self-attention and edge maps for structure preservation. Following each
harmonization iteration, an evaluator determines whether to conclude or modify
the harmonization direction. The resulting framework, mirroring human behavior,
achieves harmonious results without the need for extensive training. We present
compelling visual results across diverse scenes and objects, along with a user
study validating the effectiveness of our approach.",None,-1
0c77cd12-375b-4b57-8ba5-1c0802cd1498,Category Query Learning for Human-Object Interaction Classification,0.603884,"Unlike most previous HOI methods that focus on learning better human-object
features, we propose a novel and complementary approach called category query
learning. Such queries are explicitly associated to interaction categories,
converted to image specific category representation via a transformer decoder,
and learnt via an auxiliary image-level classification task. This idea is
motivated by an earlier multi-label image classification method, but is for the
first time applied for the challenging human-object interaction classification
task. Our method is simple, general and effective. It is validated on three
representative HOI baselines and achieves new state-of-the-art results on two
benchmarks.",None,-1
1d5b08f3-65f0-4d5a-9015-45cdda3be063,FaceChat: An Emotion-Aware Face-to-face Dialogue Framework,0.326346,"While current dialogue systems like ChatGPT have made significant
advancements in text-based interactions, they often overlook the potential of
other modalities in enhancing the overall user experience. We present FaceChat,
a web-based dialogue framework that enables emotionally-sensitive and
face-to-face conversations. By seamlessly integrating cutting-edge technologies
in natural language processing, computer vision, and speech processing,
FaceChat delivers a highly immersive and engaging user experience. FaceChat
framework has a wide range of potential applications, including counseling,
emotional support, and personalized customer service. The system is designed to
be simple and flexible as a platform for future researchers to advance the
field of multimodal dialogue systems. The code is publicly available at
https://github.com/qywu/FaceChat.",None,-1
67c0a7b1-af9d-4cb6-9d08-e70141a99012,CrossKD: Cross-Head Knowledge Distillation for Object Detection,0.174006,"Knowledge Distillation (KD) has been validated as an effective model
compression technique for learning compact object detectors. Existing
state-of-the-art KD methods for object detection are mostly based on feature
imitation. In this paper, we present a general and effective prediction
mimicking distillation scheme, called CrossKD, which delivers the intermediate
features of the student's detection head to the teacher's detection head. The
resulting cross-head predictions are then forced to mimic the teacher's
predictions. This manner relieves the student's head from receiving
contradictory supervision signals from the annotations and the teacher's
predictions, greatly improving the student's detection performance. Moreover,
as mimicking the teacher's predictions is the target of KD, CrossKD offers more
task-oriented information in contrast with feature imitation. On MS COCO, with
only prediction mimicking losses applied, our CrossKD boosts the average
precision of GFL ResNet-50 with 1x training schedule from 40.2 to 43.7,
outperforming all existing KD methods. In addition, our method also works well
when distilling detectors with heterogeneous backbones. Code is available at
https://github.com/jbwang1997/CrossKD.",None,-1
dafb59b8-e8a4-42c0-bce2-4e8094c954f6,Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation,0.563514,"Previous studies have shown that leveraging domain index can significantly
boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628).
However, such domain indices are not always available. To address this
challenge, we first provide a formal definition of domain index from the
probabilistic perspective, and then propose an adversarial variational Bayesian
framework that infers domain indices from multi-domain data, thereby providing
additional insight on domain relations and improving domain adaptation
performance. Our theoretical analysis shows that our adversarial variational
Bayesian framework finds the optimal domain index at equilibrium. Empirical
results on both synthetic and real data verify that our model can produce
interpretable domain indices which enable us to achieve superior performance
compared to state-of-the-art domain adaptation methods. Code is available at
https://github.com/Wang-ML-Lab/VDI.",None,-1
2ae37175-b6c1-4718-93f4-9e99b83699db,Analyzing the Performance of GPT-3.5 and GPT-4 in Grammatical Error Correction,0.488128,"GPT-3 and GPT-4 models are powerful, achieving high performance on a variety
of Natural Language Processing tasks. However, there is a relative lack of
detailed published analysis of their performance on the task of grammatical
error correction (GEC). To address this, we perform experiments testing the
capabilities of a GPT-3.5 model (text-davinci-003) and a GPT-4 model
(gpt-4-0314) on major GEC benchmarks. We compare the performance of different
prompts in both zero-shot and few-shot settings, analyzing intriguing or
problematic outputs encountered with different prompt formats. We report the
performance of our best prompt on the BEA-2019 and JFLEG datasets, finding that
the GPT models can perform well in a sentence-level revision setting, with
GPT-4 achieving a new high score on the JFLEG benchmark. Through human
evaluation experiments, we compare the GPT models' corrections to source, human
reference, and baseline GEC system sentences and observe differences in editing
strategies and how they are scored by human raters.",None,-1
a2f65590-2a80-431b-b1ad-5ea9afaade90,A Unified Framework of Policy Learning for Contextual Bandit with Confounding Bias and Missing Observations,0.22748,"We study the offline contextual bandit problem, where we aim to acquire an
optimal policy using observational data. However, this data usually contains
two deficiencies: (i) some variables that confound actions are not observed,
and (ii) missing observations exist in the collected data. Unobserved
confounders lead to a confounding bias and missing observations cause bias and
inefficiency problems. To overcome these challenges and learn the optimal
policy from the observed dataset, we present a new algorithm called
Causal-Adjusted Pessimistic (CAP) policy learning, which forms the reward
function as the solution of an integral equation system, builds a confidence
set, and greedily takes action with pessimism. With mild assumptions on the
data, we develop an upper bound to the suboptimality of CAP for the offline
contextual bandit problem.",None,-1
aa08e02b-6ae3-452a-9df9-f2c4d7062225,PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing,0.342516,"Diffusion models have showcased their remarkable capability to synthesize
diverse and high-quality images, sparking interest in their application for
real image editing. However, existing diffusion-based approaches for local
image editing often suffer from undesired artifacts due to the pixel-level
blending of the noised target images and diffusion latent variables, which lack
the necessary semantics for maintaining image consistency. To address these
issues, we propose PFB-Diff, a Progressive Feature Blending method for
Diffusion-based image editing. Unlike previous methods, PFB-Diff seamlessly
integrates text-guided generated content into the target image through
multi-level feature blending. The rich semantics encoded in deep features and
the progressive blending scheme from high to low levels ensure semantic
coherence and high quality in edited images. Additionally, we introduce an
attention masking mechanism in the cross-attention layers to confine the impact
of specific words to desired regions, further improving the performance of
background editing. PFB-Diff can effectively address various editing tasks,
including object/background replacement and object attribute editing. Our
method demonstrates its superior performance in terms of image fidelity,
editing accuracy, efficiency, and faithfulness to the original image, without
the need for fine-tuning or training.",None,-1
40ba5653-8614-427a-a123-05016f6f33c1,Consciousness as a logically consistent and prognostic model of reality,0.254528,"The work demonstrates that brain might reflect the external world causal
relationships in the form of a logically consistent and prognostic model of
reality, which shows up as consciousness. The paper analyses and solves the
problem of statistical ambiguity and provides a formal model of causal
relationships as probabilistic maximally specific rules. We suppose that brain
makes all possible inferences from causal relationships. We prove that the
suggested formal model has a property of an unambiguous inference: from
consistent premises we infer a consistent conclusion. It enables a set of all
inferences to form a consistent model of the perceived world. Causal
relationships may create fixed points of cyclic inter-predictable properties.
We consider the ""natural"" classification introduced by John St. Mill and
demonstrate that a variety of fixed points of the objects' attributes forms a
""natural"" classification of the external world. Then we consider notions of
""natural"" categories and causal models of categories, introduced by Eleanor
Rosch and Bob Rehder and demonstrate that fixed points of causal relationships
between objects attributes, which we perceive, formalize these notions. If the
""natural"" classification describes the objects of the external world, and
""natural"" concepts the perception of these objects, then the theory of
integrated information, introduced by G. Tononi, describes the information
processes of the brain for ""natural"" concepts formation that reflects the
""natural"" classification. We argue that integrated information provides high
accuracy of the objects identification. A computer-based experiment is provided
that illustrates fixed points formation for coded digits.",None,-1
09eff274-679b-4bdd-98ff-5edef0a6ca8e,WHC: Weighted Hybrid Criterion for Filter Pruning on Convolutional Neural Networks,0.283692,"Filter pruning has attracted increasing attention in recent years for its
capacity in compressing and accelerating convolutional neural networks. Various
data-independent criteria, including norm-based and relationship-based ones,
were proposed to prune the most unimportant filters. However, these
state-of-the-art criteria fail to fully consider the dissimilarity of filters,
and thus might lead to performance degradation. In this paper, we first analyze
the limitation of relationship-based criteria with examples, and then introduce
a new data-independent criterion, Weighted Hybrid Criterion (WHC), to tackle
the problems of both norm-based and relationship-based criteria. By taking the
magnitude of each filter and the linear dependence between filters into
consideration, WHC can robustly recognize the most redundant filters, which can
be safely pruned without introducing severe performance degradation to
networks. Extensive pruning experiments in a simple one-shot manner demonstrate
the effectiveness of the proposed WHC. In particular, WHC can prune ResNet-50
on ImageNet with more than 42% of floating point operations reduced without any
performance loss in top-5 accuracy.",None,-1
39505951-1970-4527-b7b9-6d3970625d3f,Prompt-based Learning for Text Readability Assessment,0.507873,"We propose the novel adaptation of a pre-trained seq2seq model for
readability assessment. We prove that a seq2seq model - T5 or BART - can be
adapted to discern which text is more difficult from two given texts
(pairwise). As an exploratory study to prompt-learn a neural network for text
readability in a text-to-text manner, we report useful tips for future work in
seq2seq training and ranking-based approach to readability assessment.
Specifically, we test nine input-output formats/prefixes and show that they can
significantly influence the final model performance.
  Also, we argue that the combination of text-to-text training and pairwise
ranking setup 1) enables leveraging multiple parallel text simplification data
for teaching readability and 2) trains a neural model for the general concept
of readability (therefore, better cross-domain generalization). At last, we
report a 99.6% pairwise classification accuracy on Newsela and a 98.7% for
OneStopEnglish, through a joint training approach.",None,-1
12c201ed-6ee8-441a-9331-6d9eaa070f4e,Explaining Legal Concepts with Augmented Large Language Models (GPT-4),0.999837,"Interpreting the meaning of legal open-textured terms is a key task of legal
professionals. An important source for this interpretation is how the term was
applied in previous court cases. In this paper, we evaluate the performance of
GPT-4 in generating factually accurate, clear and relevant explanations of
terms in legislation. We compare the performance of a baseline setup, where
GPT-4 is directly asked to explain a legal term, to an augmented approach,
where a legal information retrieval module is used to provide relevant context
to the model, in the form of sentences from case law. We found that the direct
application of GPT-4 yields explanations that appear to be of very high quality
on their surface. However, detailed analysis uncovered limitations in terms of
the factual accuracy of the explanations. Further, we found that the
augmentation leads to improved quality, and appears to eliminate the issue of
hallucination, where models invent incorrect statements. These findings open
the door to the building of systems that can autonomously retrieve relevant
sentences from case law and condense them into a useful explanation for legal
scholars, educators or practicing lawyers alike.",None,-1
46a9bd3f-adcb-45cc-8a97-76b4801a34d2,Face Presentation Attack Detection by Excavating Causal Clues and Adapting Embedding Statistics,0.453921,"Recent face presentation attack detection (PAD) leverages domain adaptation
(DA) and domain generalization (DG) techniques to address performance
degradation on unknown domains. However, DA-based PAD methods require access to
unlabeled target data, while most DG-based PAD solutions rely on a priori,
i.e., known domain labels. Moreover, most DA-/DG-based methods are
computationally intensive, demanding complex model architectures and/or
multi-stage training processes. This paper proposes to model face PAD as a
compound DG task from a causal perspective, linking it to model optimization.
We excavate the causal factors hidden in the high-level representation via
counterfactual intervention. Moreover, we introduce a class-guided MixStyle to
enrich feature-level data distribution within classes instead of focusing on
domain information. Both class-guided MixStyle and counterfactual intervention
components introduce no extra trainable parameters and negligible computational
resources. Extensive cross-dataset and analytic experiments demonstrate the
effectiveness and efficiency of our method compared to state-of-the-art PADs.
The implementation and the trained weights are publicly available.",None,-1
af25cc0b-8592-4fe5-97f6-ea218a39a7b9,Frustratingly Simple but Effective Zero-shot Detection and Segmentation: Analysis and a Strong Baseline,0.272519,"Methods for object detection and segmentation often require abundant
instance-level annotations for training, which are time-consuming and expensive
to collect. To address this, the task of zero-shot object detection (or
segmentation) aims at learning effective methods for identifying and localizing
object instances for the categories that have no supervision available.
Constructing architectures for these tasks requires choosing from a myriad of
design options, ranging from the form of the class encoding used to transfer
information from seen to unseen categories, to the nature of the function being
optimized for learning. In this work, we extensively study these design
choices, and carefully construct a simple yet extremely effective zero-shot
recognition method. Through extensive experiments on the MSCOCO dataset on
object detection and segmentation, we highlight that our proposed method
outperforms existing, considerably more complex, architectures. Our findings
and method, which we propose as a competitive future baseline, point towards
the need to revisit some of the recent design trends in zero-shot detection /
segmentation.",None,-1
7b37e3ed-3c7e-4afb-8bb1-f748b93839dc,Personalized Abstractive Summarization by Tri-agent Generation Pipeline,0.272962,"Tailoring outputs from large language models, like ChatGPT, to implicit user
preferences remains a challenge despite their impressive generative
capabilities. In this paper, we propose a tri-agent generation pipeline
comprising a generator, an instructor, and an editor to enhance output
personalization. The generator produces an initial output, the instructor
automatically generates editing instructions based on user preferences, and the
editor refines the output to align with those preferences. The inference-only
large language model (ChatGPT) serves as both the generator and editor, with a
smaller model acting as the instructor to guide output generation. We train the
instructor using editor-steered reinforcement learning, leveraging feedback
from a large-scale editor model to optimize instruction generation.
Experimental results on two abstractive summarization datasets demonstrate the
effectiveness of our approach in generating outputs that better meet user
expectations. Code is available at
\url{https://github.com/Wendy-Xiao/chatgpt_editing_summ}",None,-1
238e5910-8e03-4a84-81a5-f18ee5ad08e3,Prompting is not a substitute for probability measurements in large language models,0.0822862,"Prompting is now a dominant method for evaluating the linguistic knowledge of
large language models (LLMs). While other methods directly read out models'
probability distributions over strings, prompting requires models to access
this internal information by processing linguistic input, thereby implicitly
testing a new type of emergent ability: metalinguistic judgment. In this study,
we compare metalinguistic prompting and direct probability measurements as ways
of measuring models' linguistic knowledge. Broadly, we find that LLMs'
metalinguistic judgments are inferior to quantities directly derived from
representations. Furthermore, consistency gets worse as the prompt query
diverges from direct measurements of next-word probabilities. Our findings
suggest that negative results relying on metalinguistic prompts cannot be taken
as conclusive evidence that an LLM lacks a particular linguistic
generalization. Our results also highlight the value that is lost with the move
to closed APIs where access to probability distributions is limited.",None,-1
0019f1e1-08d4-480d-9558-ab9737c0d2ba,GPT4GEO: How a Language Model Sees the World's Geography,0.967532,"Large language models (LLMs) have shown remarkable capabilities across a
broad range of tasks involving question answering and the generation of
coherent text and code. Comprehensively understanding the strengths and
weaknesses of LLMs is beneficial for safety, downstream applications and
improving performance. In this work, we investigate the degree to which GPT-4
has acquired factual geographic knowledge and is capable of using this
knowledge for interpretative reasoning, which is especially important for
applications that involve geographic data, such as geospatial analysis, supply
chain management, and disaster response. To this end, we design and conduct a
series of diverse experiments, starting from factual tasks such as location,
distance and elevation estimation to more complex questions such as generating
country outlines and travel networks, route finding under constraints and
supply chain analysis. We provide a broad characterisation of what GPT-4
(without plugins or Internet access) knows about the world, highlighting both
potentially surprising capabilities but also limitations.",None,-1
bb301251-d922-4a65-94de-285ddd86bcc3,"ARDIAS: AI-Enhanced Research Management, Discovery, and Advisory System",0.0622884,"In this work, we present ARDIAS, a web-based application that aims to provide
researchers with a full suite of discovery and collaboration tools. ARDIAS
currently allows searching for authors and articles by name and gaining
insights into the research topics of a particular researcher. With the aid of
AI-based tools, ARDIAS aims to recommend potential collaborators and topics to
researchers. In the near future, we aim to add tools that allow researchers to
communicate with each other and start new projects.",None,-1
7699b192-c2f5-46e9-8528-634d66fd7e40,Inpainting borehole images using Generative Adversarial Networks,0.0755199,"In this paper, we propose a GAN-based approach for gap filling in borehole
images created by wireline microresistivity imaging tools. The proposed method
utilizes a generator, global discriminator, and local discriminator to inpaint
the missing regions of the image. The generator is based on an auto-encoder
architecture with skip-connections, and the loss function used is the
Wasserstein GAN loss. Our experiments on a dataset of borehole images
demonstrate that the proposed model can effectively deal with large-scale
missing pixels and generate realistic completion results. This approach can
improve the quantitative evaluation of reservoirs and provide an essential
basis for interpreting geological phenomena and reservoir parameters.",None,-1
ad66f9b5-181e-479d-a106-f63d0f5af6cb,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,0.293608,"The Odeuropa Challenge on Olfactory Object Recognition aims to foster the
development of object detection in the visual arts and to promote an olfactory
perspective on digital heritage. Object detection in historical artworks is
particularly challenging due to varying styles and artistic periods. Moreover,
the task is complicated due to the particularity and historical variance of
predefined target objects, which exhibit a large intra-class variance, and the
long tail distribution of the dataset labels, with some objects having only
very few training examples. These challenges should encourage participants to
create innovative approaches using domain adaptation or few-shot learning. We
provide a dataset of 2647 artworks annotated with 20 120 tightly fit bounding
boxes that are split into a training and validation set (public). A test set
containing 1140 artworks and 15 480 annotations is kept private for the
challenge evaluation.",None,-1
03f43bae-dfa6-4ecc-a186-34e4cd06e41c,Multiparticle Kalman filter for object localization in symmetric environments,0.397605,"This study considers the object localization problem and proposes a novel
multiparticle Kalman filter to solve it in complex and symmetric environments.
Two well-known classes of filtering algorithms to solve the localization
problem are Kalman filter-based methods and particle filter-based methods. We
consider these classes, demonstrate their complementary properties, and propose
a novel filtering algorithm that takes the best from two classes. We evaluate
the multiparticle Kalman filter in symmetric and noisy environments. Such
environments are especially challenging for both classes of classical methods.
We compare the proposed approach with the particle filter since only this
method is feasible if the initial state is unknown. In the considered
challenging environments, our method outperforms the particle filter in terms
of both localization error and runtime.",None,-1
c9974553-1aaa-4c42-b776-ab71f41d136d,Explainable Multi-Agent Reinforcement Learning for Temporal Queries,0.577713,"As multi-agent reinforcement learning (MARL) systems are increasingly
deployed throughout society, it is imperative yet challenging for users to
understand the emergent behaviors of MARL agents in complex environments. This
work presents an approach for generating policy-level contrastive explanations
for MARL to answer a temporal user query, which specifies a sequence of tasks
completed by agents with possible cooperation. The proposed approach encodes
the temporal query as a PCTL logic formula and checks if the query is feasible
under a given MARL policy via probabilistic model checking. Such explanations
can help reconcile discrepancies between the actual and anticipated multi-agent
behaviors. The proposed approach also generates correct and complete
explanations to pinpoint reasons that make a user query infeasible. We have
successfully applied the proposed approach to four benchmark MARL domains (up
to 9 agents in one domain). Moreover, the results of a user study show that the
generated explanations significantly improve user performance and satisfaction.",None,-1
be595275-55d3-413c-b2a8-020daee1f01f,NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource Languages through Data Enrichment,0.501738,"In recent years, natural language processing has gained significant
popularity in various sectors, including the legal domain. This paper presents
NeCo Team's solutions to the Vietnamese text processing tasks provided in the
Automated Legal Question Answering Competition 2023 (ALQAC 2023), focusing on
legal domain knowledge acquisition for low-resource languages through data
enrichment. Our methods for the legal document retrieval task employ a
combination of similarity ranking and deep learning models, while for the
second task, which requires extracting an answer from a relevant legal article
in response to a question, we propose a range of adaptive techniques to handle
different question types. Our approaches achieve outstanding results on both
tasks of the competition, demonstrating the potential benefits and
effectiveness of question answering systems in the legal field, particularly
for low-resource languages.",None,-1
9b11d2fa-9d4a-4484-8875-a596e05e77e4,"Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and Embedding",0.283827,"One of the significant barriers to the training of statistical models on
knowledge graphs is the difficulty that scientists have in finding the best
input data to address their prediction goal. In addition to this, a key
challenge is to determine how to manipulate these relational data, which are
often in the form of particular triples (i.e., subject, predicate, object), to
enable the learning process. Currently, many high-quality catalogs of knowledge
graphs, are available. However, their primary goal is the re-usability of these
resources, and their interconnection, in the context of the Semantic Web. This
paper describes the LiveSchema initiative, namely, a first version of a gateway
that has the main scope of leveraging the gold mine of data collected by many
existing catalogs collecting relational data like ontologies and knowledge
graphs. At the current state, LiveSchema contains - 1000 datasets from 4 main
sources and offers some key facilities, which allow to: i) evolving LiveSchema,
by aggregating other source catalogs and repositories as input sources; ii)
querying all the collected resources; iii) transforming each given dataset into
formal concept analysis matrices that enable analysis and visualization
services; iv) generating models and tensors from each given dataset.",None,-1
48e61cf3-6920-4317-92f7-ba7b619563f6,Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models,0.765612,"Parameter-efficient tuning (PET) methods fit pre-trained language models
(PLMs) to downstream tasks by either computing a small compressed update for a
subset of model parameters, or appending and fine-tuning a small number of new
model parameters to the pre-trained network. Hand-designed PET architectures
from the literature perform well in practice, but have the potential to be
improved via automated neural architecture search (NAS). We propose an
efficient NAS method for learning PET architectures via structured and
unstructured pruning. We present experiments on GLUE demonstrating the
effectiveness of our algorithm and discuss how PET architectural design choices
affect performance in practice.",None,-1
45aac6c4-dc4f-4535-b0ae-4303b84a0d14,Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams,0.41115,"The present study aims to explore the capabilities of Language Models (LMs)
in tackling high-stakes multiple-choice tests, represented here by the Exame
Nacional do Ensino M\'edio (ENEM), a multidisciplinary entrance examination
widely adopted by Brazilian universities. This exam poses challenging tasks for
LMs, since its questions may span into multiple fields of knowledge, requiring
understanding of information from diverse domains. For instance, a question may
require comprehension of both statistics and biology to be solved. This work
analyzed responses generated by GPT-3.5 and GPT-4 models for questions
presented in the 2009-2017 exams, as well as for questions of the 2022 exam,
which were made public after the training of the models was completed.
Furthermore, different prompt strategies were tested, including the use of
Chain-of-Thought (CoT) prompts to generate explanations for answers. On the
2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy
of 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on
experiments are available at https://github.com/piresramon/gpt-4-enem.",None,-1
4dc530dc-41f0-4a18-8653-f9afc416f86a,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",0.998601,"We introduce k-planes, a white-box model for radiance fields in arbitrary
dimensions. Our model uses d choose 2 planes to represent a d-dimensional
scene, providing a seamless way to go from static (d=3) to dynamic (d=4)
scenes. This planar factorization makes adding dimension-specific priors easy,
e.g. temporal smoothness and multi-resolution spatial structure, and induces a
natural decomposition of static and dynamic components of a scene. We use a
linear feature decoder with a learned color basis that yields similar
performance as a nonlinear black-box MLP decoder. Across a range of synthetic
and real, static and dynamic, fixed and varying appearance scenes, k-planes
yields competitive and often state-of-the-art reconstruction fidelity with low
memory usage, achieving 1000x compression over a full 4D grid, and fast
optimization with a pure PyTorch implementation. For video results and code,
please see https://sarafridov.github.io/K-Planes.",None,-1
11a66120-aa60-43e7-808c-84e8ddaa42bb,Self-Supervised Video Similarity Learning,0.342565,"We introduce S$^2$VS, a video similarity learning approach with
self-supervision. Self-Supervised Learning (SSL) is typically used to train
deep models on a proxy task so as to have strong transferability on target
tasks after fine-tuning. Here, in contrast to prior work, SSL is used to
perform video similarity learning and address multiple retrieval and detection
tasks at once with no use of labeled data. This is achieved by learning via
instance-discrimination with task-tailored augmentations and the widely used
InfoNCE loss together with an additional loss operating jointly on
self-similarity and hard-negative similarity. We benchmark our method on tasks
where video relevance is defined with varying granularity, ranging from video
copies to videos depicting the same incident or event. We learn a single
universal model that achieves state-of-the-art performance on all tasks,
surpassing previously proposed methods that use labeled data. The code and
pretrained models are publicly available at: https://github.com/gkordo/s2vs",None,-1
16bdd8c9-52cb-4e6d-94d6-ed21cc7ee306,Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation,0.662958,"Most of the speech translation models heavily rely on parallel data, which is
hard to collect especially for low-resource languages. To tackle this issue, we
propose to build a cascaded speech translation system without leveraging any
kind of paired data. We use fully unpaired data to train our unsupervised
systems and evaluate our results on CoVoST 2 and CVSS. The results show that
our work is comparable with some other early supervised methods in some
language pairs. While cascaded systems always suffer from severe error
propagation problems, we proposed denoising back-translation (DBT), a novel
approach to building robust unsupervised neural machine translation (UNMT). DBT
successfully increases the BLEU score by 0.7--0.9 in all three translation
directions. Moreover, we simplified the pipeline of our cascaded system to
reduce inference latency and conducted a comprehensive analysis of every part
of our work. We also demonstrate our unsupervised speech translation results on
the established website.",None,-1
365e10ed-70ff-4c07-96da-44e2fb926c67,Deep Image Compression Using Scene Text Quality Assessment,0.402196,"Image compression is a fundamental technology for Internet communication
engineering. However, a high compression rate with general methods may degrade
images, resulting in unreadable texts. In this paper, we propose an image
compression method for maintaining text quality. We developed a scene text
image quality assessment model to assess text quality in compressed images. The
assessment model iteratively searches for the best-compressed image holding
high-quality text. Objective and subjective results showed that the proposed
method was superior to existing methods. Furthermore, the proposed assessment
model outperformed other deep-learning regression models.",None,-1
3c9e903e-3a6c-4d67-b3c5-792da4aae2a8,Pure Monte Carlo Counterfactual Regret Minimization,0.859653,"Counterfactual Regret Minimization (CFR) and its variants are the best
algorithms so far for solving large-scale incomplete information games.
However, we believe that there are two problems with CFR: First, matrix
multiplication is required in CFR iteration, and the time complexity of one
iteration is too high; Secondly, the game characteristics in the real world are
different. Just using one CFR algorithm will not be perfectly suitable for all
game problems.
  For these two problems, this paper proposes a new algorithm called Pure CFR
(PCFR) based on CFR. PCFR can be seen as a combination of CFR and Fictitious
Play (FP), inheriting the concept of counterfactual regret (value) from CFR,
and using the best response strategy instead of the regret matching strategy
for the next iteration. This algorithm has three advantages. First, PCFR can be
combined with any CFR variant. The resulting Pure MCCFR (PMCCFR) can
significantly reduce the time and space complexity of one iteration. Secondly,
our experiments show that the convergence speed of the PMCCFR is 2$\sim$3 times
that of the MCCFR. Finally, there is a type of game that is very suitable for
PCFR. We call this type of game clear-game, which is characterized by a high
proportion of dominated strategies. Experiments show that in clear-game, the
convergence rate of PMCCFR is two orders of magnitude higher than that of
MCCFR.",None,-1
1bdda410-c25c-4301-9a06-2647c97b6e04,Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach,0.313559,"Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, it requires a significant amount
of storage space that can only be deployed on remote cloud servers.
Additionally, using commercial LLMs can be costly since they may charge based
on usage frequency. In this paper, we explore how to enable intelligent
cost-effective interactions between a down stream task oriented agent and an
LLM. We find that this problem can be naturally formulated by a Markov decision
process (MDP), and propose When2Ask, a reinforcement learning based approach
that learns when it is necessary to query LLMs for high-level instructions to
accomplish a target task. On one side, When2Ask discourages unnecessary
redundant interactions, while on the other side, it enables the agent to
identify and follow useful instructions from the LLM. This enables the agent to
halt an ongoing plan and transition to a more suitable one based on new
environmental observations. Experiments on MiniGrid and Habitat environments
that entail planning sub-goals demonstrate that When2Ask learns to solve target
tasks with only a few necessary interactions with the LLM, significantly
reducing interaction costs in testing environments compared with baseline
methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.",None,-1
a11fae81-57f2-4fb2-a20a-8aa338c33daf,Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning,0.620888,"Pretrained large language models (LLMs) are able to solve a wide variety of
tasks through transfer learning. Various explainability methods have been
developed to investigate their decision making process. TracIn (Pruthi et al.,
2020) is one such gradient-based method which explains model inferences based
on the influence of training examples. In this paper, we explore the use of
TracIn to improve model performance in the parameter-efficient tuning (PET)
setting. We develop conversational safety classifiers via the prompt-tuning PET
method and show how the unique characteristics of the PET regime enable TracIn
to identify the cause for certain misclassifications by LLMs. We develop a new
methodology for using gradient-based explainability techniques to improve model
performance, G-BAIR: gradient-based automated iterative recovery. We show that
G-BAIR can recover LLM performance on benchmarks after manually corrupting
training labels. This suggests that influence methods like TracIn can be used
to automatically perform data cleaning, and introduces the potential for
interactive debugging and relabeling for PET-based transfer learning methods.",None,-1
816b7c90-6751-40ca-9f6b-1c51257b687b,Ticket-BERT: Labeling Incident Management Tickets with Language Models,0.399739,"An essential aspect of prioritizing incident tickets for resolution is
efficiently labeling tickets with fine-grained categories. However, ticket data
is often complex and poses several unique challenges for modern machine
learning methods: (1) tickets are created and updated either by machines with
pre-defined algorithms or by engineers with domain expertise that share
different protocols, (2) tickets receive frequent revisions that update ticket
status by modifying all or parts of ticket descriptions, and (3) ticket
labeling is time-sensitive and requires knowledge updates and new labels per
the rapid software and hardware improvement lifecycle. To handle these issues,
we introduce Ticket- BERT which trains a simple yet robust language model for
labeling tickets using our proposed ticket datasets. Experiments demonstrate
the superiority of Ticket-BERT over baselines and state-of-the-art text
classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT
with an active learning cycle and deploy it on the Microsoft IcM system, which
enables the model to quickly finetune on newly-collected tickets with a few
annotations.",None,-1
d55743b4-d72e-41a3-b33a-8c28a9fe9db4,A Comparison of Decision Algorithms on Newcomblike Problems,0.115651,"When formulated using Bayesian networks, two standard decision algorithms
(Evidential Decision Theory and Causal Decision Theory) can be shown to fail
systematically when faced with aspects of the prisoner's dilemma and so-called
""Newcomblike"" problems. We describe a new form of decision algorithm, called
Timeless Decision Theory, which consistently wins on these problems.",None,-1
520b94de-ab6f-4630-8aca-3bbdcbb5eca0,AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation,0.799138,"To apply optical flow in practice, it is often necessary to resize the input
to smaller dimensions in order to reduce computational costs. However,
downsizing inputs makes the estimation more challenging because objects and
motion ranges become smaller. Even though recent approaches have demonstrated
high-quality flow estimation, they tend to fail to accurately model small
objects and precise boundaries when the input resolution is lowered,
restricting their applicability to high-resolution inputs. In this paper, we
introduce AnyFlow, a robust network that estimates accurate flow from images of
various resolutions. By representing optical flow as a continuous
coordinate-based representation, AnyFlow generates outputs at arbitrary scales
from low-resolution inputs, demonstrating superior performance over prior works
in capturing tiny objects with detail preservation on a wide range of scenes.
We establish a new state-of-the-art performance of cross-dataset generalization
on the KITTI dataset, while achieving comparable accuracy on the online
benchmarks to other SOTA methods.",None,-1
65124f42-099e-4d07-8988-a23fd41edb52,FinEntity: Entity-level Sentiment Classification for Financial Texts,0.935926,"In the financial domain, conducting entity-level sentiment analysis is
crucial for accurately assessing the sentiment directed toward a specific
financial entity. To our knowledge, no publicly available dataset currently
exists for this purpose. In this work, we introduce an entity-level sentiment
classification dataset, called \textbf{FinEntity}, that annotates financial
entity spans and their sentiment (positive, neutral, and negative) in financial
news. We document the dataset construction process in the paper. Additionally,
we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on
entity-level sentiment classification. In a case study, we demonstrate the
practical utility of using FinEntity in monitoring cryptocurrency markets. The
data and code of FinEntity is available at
\url{https://github.com/yixuantt/FinEntity}",None,-1
be5b05ea-7884-4d5a-a0b4-0557284ecd32,Graph-CoVis: GNN-based Multi-view Panorama Global Pose Estimation,0.385833,"In this paper, we address the problem of wide-baseline camera pose estimation
from a group of 360$^\circ$ panoramas under upright-camera assumption. Recent
work has demonstrated the merit of deep-learning for end-to-end direct relative
pose regression in 360$^\circ$ panorama pairs [11]. To exploit the benefits of
multi-view logic in a learning-based framework, we introduce Graph-CoVis, which
non-trivially extends CoVisPose [11] from relative two-view to global
multi-view spherical camera pose estimation. Graph-CoVis is a novel Graph
Neural Network based architecture that jointly learns the co-visible structure
and global motion in an end-to-end and fully-supervised approach. Using the
ZInD [4] dataset, which features real homes presenting wide-baselines,
occlusion, and limited visual overlap, we show that our model performs
competitively to state-of-the-art approaches.",None,-1
ef1c41f2-435c-49bf-b70d-0d1fd3bde904,Deep-Reinforcement-Learning-based Path Planning for Industrial Robots using Distance Sensors as Observation,0.051928,"Industrial robots are widely used in various manufacturing environments due
to their efficiency in doing repetitive tasks such as assembly or welding. A
common problem for these applications is to reach a destination without
colliding with obstacles or other robot arms. Commonly used sampling-based path
planning approaches such as RRT require long computation times, especially in
complex environments. Furthermore, the environment in which they are employed
needs to be known beforehand. When utilizing the approaches in new
environments, a tedious engineering effort in setting hyperparameters needs to
be conducted, which is time- and cost-intensive. On the other hand, Deep
Reinforcement Learning has shown remarkable results in dealing with unknown
environments, generalizing new problem instances, and solving motion planning
problems efficiently. On that account, this paper proposes a
Deep-Reinforcement-Learning-based motion planner for robotic manipulators. We
evaluated our model against state-of-the-art sampling-based planners in several
experiments. The results show the superiority of our planner in terms of path
length and execution time.",None,-1
1f9c9f3c-9272-4fbe-a2f3-bb5c7db9e16f,MidMed: Towards Mixed-Type Dialogues for Medical Consultation,0.654771,"Most medical dialogue systems assume that patients have clear goals (medicine
querying, surgical operation querying, etc.) before medical consultation.
However, in many real scenarios, due to the lack of medical knowledge, it is
usually difficult for patients to determine clear goals with all necessary
slots. In this paper, we identify this challenge as how to construct medical
consultation dialogue systems to help patients clarify their goals. To mitigate
this challenge, we propose a novel task and create a human-to-human mixed-type
medical consultation dialogue corpus, termed MidMed, covering five dialogue
types: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded
dialogue, QA, and chitchat. MidMed covers four departments
(otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175
dialogues. Furthermore, we build baselines on MidMed and propose an
instruction-guiding medical dialogue generation framework, termed InsMed, to
address this task. Experimental results show the effectiveness of InsMed.",None,-1
366b9f86-4da7-4ccc-add6-928aafc8306f,Infinite Photorealistic Worlds using Procedural Generation,0.343849,"We introduce Infinigen, a procedural generator of photorealistic 3D scenes of
the natural world. Infinigen is entirely procedural: every asset, from shape to
texture, is generated from scratch via randomized mathematical rules, using no
external source and allowing infinite variation and composition. Infinigen
offers broad coverage of objects and scenes in the natural world including
plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and
snow. Infinigen can be used to generate unlimited, diverse training data for a
wide range of computer vision tasks including object detection, semantic
segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a
useful resource for computer vision research and beyond. Please visit
https://infinigen.org for videos, code and pre-generated data.",None,-1
4c41589a-07be-4161-8cf0-83aff7a923db,Asymmetric Polynomial Loss For Multi-Label Classification,0.304214,"Various tasks are reformulated as multi-label classification problems, in
which the binary cross-entropy (BCE) loss is frequently utilized for optimizing
well-designed models. However, the vanilla BCE loss cannot be tailored for
diverse tasks, resulting in a suboptimal performance for different models.
Besides, the imbalance between redundant negative samples and rare positive
samples could degrade the model performance. In this paper, we propose an
effective Asymmetric Polynomial Loss (APL) to mitigate the above issues.
Specifically, we first perform Taylor expansion on BCE loss. Then we ameliorate
the coefficients of polynomial functions. We further employ the asymmetric
focusing mechanism to decouple the gradient contribution from the negative and
positive samples. Moreover, we validate that the polynomial coefficients can
recalibrate the asymmetric focusing hyperparameters. Experiments on relation
extraction, text classification, and image classification show that our APL
loss can consistently improve performance without extra training burden.",None,-1
d3e17c6f-d1ab-42f7-b203-b646bd4f8bba,CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models,0.764718,"In this paper, we consider the challenge of summarizing patients' medical
progress notes in a limited data setting. For the Problem List Summarization
(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5
fine-tuned to 765 medical clinic notes outperforms other extractive,
abstractive and zero-shot baselines, yielding reasonable baseline systems for
medical note summarization. Further, we introduce Hierarchical Ensemble of
Summarization Models (HESM), consisting of token-level ensembles of diverse
fine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.
Our HESM approach lead to a considerable summarization performance boost, and
when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which
was the best-performing system at the top of the shared task leaderboard.",None,-1
956995a8-8daa-4241-895e-9a778b2a175d,Bird's-Eye-View Scene Graph for Vision-Language Navigation,0.306659,"Vision-language navigation (VLN), which entails an agent to navigate 3D
environments following human instructions, has shown great advances. However,
current agents are built upon panoramic observations, which hinders their
ability to perceive 3D scene geometry and easily leads to ambiguous selection
of panoramic view. To address these limitations, we present a BEV Scene Graph
(BSG), which leverages multi-step BEV representations to encode scene layouts
and geometric cues of indoor environment under the supervision of 3D detection.
During navigation, BSG builds a local BEV representation at each step and
maintains a BEV-based global scene map, which stores and organizes all the
online collected local BEV representations according to their topological
relations. Based on BSG, the agent predicts a local BEV grid-level decision
score and a global graph-level decision score, combined with a sub-view
selection score on panoramic views, for more accurate action prediction. Our
approach significantly outperforms state-of-the-art methods on REVERIE, R2R,
and R4R, showing the potential of BEV perception in VLN.",None,-1
e134fabf-76b3-4c96-9222-64062c00a6c2,Instruction Mining: When Data Mining Meets Large Language Model Finetuning,0.222347,"Large language models (LLMs) are initially pretrained for broad capabilities
and then finetuned with instruction-following datasets to improve their
performance in interacting with humans. Despite advances in finetuning, a
standardized guideline for selecting high-quality datasets to optimize this
process remains elusive. In this paper, we first propose InstructMining, an
innovative method designed for automatically selecting premium
instruction-following data for finetuning LLMs. Specifically, InstructMining
utilizes natural language indicators as a measure of data quality, applying
them to evaluate unseen datasets. During experimentation, we discover that
double descent phenomenon exists in large language model finetuning. Based on
this observation, we further leverage BlendSearch to help find the best subset
among the entire dataset (i.e., 2,532 out of 100,000). Experiment results show
that InstructMining-7B achieves state-of-the-art performance on two of the most
popular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.",None,-1
780843fe-32d8-4583-9d51-4551836a0dc2,MG-GNN: Multigrid Graph Neural Networks for Learning Multilevel Domain Decomposition Methods,0.616566,"Domain decomposition methods (DDMs) are popular solvers for discretized
systems of partial differential equations (PDEs), with one-level and multilevel
variants. These solvers rely on several algorithmic and mathematical
parameters, prescribing overlap, subdomain boundary conditions, and other
properties of the DDM. While some work has been done on optimizing these
parameters, it has mostly focused on the one-level setting or special cases
such as structured-grid discretizations with regular subdomain construction. In
this paper, we propose multigrid graph neural networks (MG-GNN), a novel GNN
architecture for learning optimized parameters in two-level DDMs\@. We train
MG-GNN using a new unsupervised loss function, enabling effective training on
small problems that yields robust performance on unstructured grids that are
orders of magnitude larger than those in the training set. We show that MG-GNN
outperforms popular hierarchical graph network architectures for this
optimization and that our proposed loss function is critical to achieving this
improved performance.",None,-1
41588a50-ea3a-48bd-9f0c-eca8d2a759dc,Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods,0.998907,"Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called ""machine psychology"". The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.",None,-1
13ddb44b-e595-4055-aebe-6ffe6d9ae293,Reward Design with Language Models,0.959137,"Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we instead cheaply design rewards using
a natural language interface? This paper explores how to simplify reward design
by prompting a large language model (LLM) such as GPT-3 as a proxy reward
function, where the user provides a textual prompt containing a few examples
(few-shot) or a description (zero-shot) of the desired behavior. Our approach
leverages this proxy reward function in an RL framework. Specifically, users
specify a prompt once at the beginning of training. During training, the LLM
evaluates an RL agent's behavior against the desired behavior described by the
prompt and outputs a corresponding reward signal. The RL agent then uses this
reward to update its behavior. We evaluate whether our approach can train
agents aligned with user objectives in the Ultimatum Game, matrix games, and
the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents
trained with our framework are well-aligned with the user's objectives and
outperform RL agents trained with reward functions learned via supervised
learning",None,-1
164821d2-76c2-4bdf-81aa-1dafd2e3a57e,Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis,0.150852,"We present the first shared task for detecting and analyzing code-switching
in Guarani and Spanish, GUA-SPA at IberLEF 2023. The challenge consisted of
three tasks: identifying the language of a token, NER, and a novel task of
classifying the way a Spanish span is used in the code-switched context. We
annotated a corpus of 1500 texts extracted from news articles and tweets,
around 25 thousand tokens, with the information for the tasks. Three teams took
part in the evaluation phase, obtaining in general good results for Task 1, and
more mixed results for Tasks 2 and 3.",None,-1
ee6eb7da-5076-46f1-9304-961e942204c5,NerfAcc: Efficient Sampling Accelerates NeRFs,0.962804,"Optimizing and rendering Neural Radiance Fields is computationally expensive
due to the vast number of samples required by volume rendering. Recent works
have included alternative sampling approaches to help accelerate their methods,
however, they are often not the focus of the work. In this paper, we
investigate and compare multiple sampling approaches and demonstrate that
improved sampling is generally applicable across NeRF variants under an unified
concept of transmittance estimator. To facilitate future experiments, we
develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating
advanced sampling methods into NeRF related methods. We demonstrate its
flexibility by showing that it can reduce the training time of several recent
NeRF methods by 1.5x to 20x with minimal modifications to the existing
codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be
implemented in native PyTorch using NerfAcc.",None,-1
e9288167-a720-4b71-9225-d2d0184e1ba0,Putting People in Their Place: Affordance-Aware Human Insertion into Scenes,0.944966,"We study the problem of inferring scene affordances by presenting a method
for realistically inserting people into scenes. Given a scene image with a
marked region and an image of a person, we insert the person into the scene
while respecting the scene affordances. Our model can infer the set of
realistic poses given the scene context, re-pose the reference person, and
harmonize the composition. We set up the task in a self-supervised fashion by
learning to re-pose humans in video clips. We train a large-scale diffusion
model on a dataset of 2.4M video clips that produces diverse plausible poses
while respecting the scene context. Given the learned human-scene composition,
our model can also hallucinate realistic people and scenes when prompted
without conditioning and also enables interactive editing. A quantitative
evaluation shows that our method synthesizes more realistic human appearance
and more natural human-scene interactions than prior work.",None,-1
fb66689b-12fd-4c6f-bf0d-af77be872825,Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning,0.368393,"The Contrastive Language-Image Pre-training (CLIP) has recently shown
remarkable generalization on ""zero-shot"" training and has applied to many
downstream tasks. We explore the adaptation of CLIP to achieve a more efficient
and generalized action recognition method. We propose that the key lies in
explicitly modeling the motion cues flowing in video frames. To that end, we
design a two-stream motion modeling block to capture motion and spatial
information at the same time. And then, the obtained motion cues are utilized
to drive a dynamic prompts learner to generate motion-aware prompts, which
contain much semantic information concerning human actions. In addition, we
propose a multimodal communication block to achieve a collaborative learning
and further improve the performance. We conduct extensive experiments on
HMDB-51, UCF-101, and Kinetics-400 datasets. Our method outperforms most
existing state-of-the-art methods by a significant margin on ""few-shot"" and
""zero-shot"" training. We also achieve competitive performance on ""closed-set""
training with extremely few trainable parameters and additional computational
costs.",None,-1
9642ac59-cbb1-4bbc-ae43-0044558c89de,Carbon-Efficient Neural Architecture Search,0.220568,"This work presents a novel approach to neural architecture search (NAS) that
aims to reduce energy costs and increase carbon efficiency during the model
design process. The proposed framework, called carbon-efficient NAS (CE-NAS),
consists of NAS evaluation algorithms with different energy requirements, a
multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS
dynamically balances energy-efficient sampling and energy-consuming evaluation
tasks based on current carbon emissions. Using a recent NAS benchmark dataset
and two carbon traces, our trace-driven simulations demonstrate that CE-NAS
achieves better carbon and search efficiency than the three baselines.",None,-1
025c6f6d-68cf-4bd0-9607-8aff0ba92b61,Zero-shot Transfer of Article-aware Legal Outcome Classification for European Court of Human Rights Cases,0.911355,"In this paper, we cast Legal Judgment Prediction on European Court of Human
Rights cases into an article-aware classification task, where the case outcome
is classified from a combined input of case facts and convention articles. This
configuration facilitates the model learning some legal reasoning ability in
mapping article text to specific case fact text. It also provides an
opportunity to evaluate the model's ability to generalize to zero-shot settings
when asked to classify the case outcome with respect to articles not seen
during training. We devise zero-shot experiments and apply domain adaptation
methods based on domain discrimination and Wasserstein distance. Our results
demonstrate that the article-aware architecture outperforms straightforward
fact classification. We also find that domain adaptation methods improve
zero-shot transfer performance, with article relatedness and encoder
pre-training influencing the effect.",None,-1
f5b28c56-f72c-43ef-b375-472245927633,Efficient Transformer-based 3D Object Detection with Dynamic Token Halting,0.220714,"Balancing efficiency and accuracy is a long-standing problem for deploying
deep learning models. The trade-off is even more important for real-time
safety-critical systems like autonomous vehicles. In this paper, we propose an
effective approach for accelerating transformer-based 3D object detectors by
dynamically halting tokens at different layers depending on their contribution
to the detection task. Although halting a token is a non-differentiable
operation, our method allows for differentiable end-to-end learning by
leveraging an equivalent differentiable forward-pass. Furthermore, our
framework allows halted tokens to be reused to inform the model's predictions
through a straightforward token recycling mechanism. Our method significantly
improves the Pareto frontier of efficiency versus accuracy when compared with
the existing approaches. By halting tokens and increasing model capacity, we
are able to improve the baseline model's performance without increasing the
model's latency on the Waymo Open Dataset.",None,-1
43597e7b-b5ee-4a21-adef-1f6fd5192e85,GaitRef: Gait Recognition with Refined Sequential Skeletons,0.661168,"Identifying humans with their walking sequences, known as gait recognition,
is a useful biometric understanding task as it can be observed from a long
distance and does not require cooperation from the subject. Two common
modalities used for representing the walking sequence of a person are
silhouettes and joint skeletons. Silhouette sequences, which record the
boundary of the walking person in each frame, may suffer from the variant
appearances from carried-on objects and clothes of the person. Framewise joint
detections are noisy and introduce some jitters that are not consistent with
sequential detections. In this paper, we combine the silhouettes and skeletons
and refine the framewise joint predictions for gait recognition. With temporal
information from the silhouette sequences, we show that the refined skeletons
can improve gait recognition performance without extra annotations. We compare
our methods on four public datasets, CASIA-B, OUMVLP, Gait3D and GREW, and show
state-of-the-art performance.",None,-1
521a6657-5deb-44e7-9d19-b8e2d7e9d4d4,AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation,0.851422,"We present All-Pairs Multi-Field Transforms (AMT), a new network architecture
for video frame interpolation. It is based on two essential designs. First, we
build bidirectional correlation volumes for all pairs of pixels, and use the
predicted bilateral flows to retrieve correlations for updating both flows and
the interpolated content feature. Second, we derive multiple groups of
fine-grained flow fields from one pair of updated coarse flows for performing
backward warping on the input frames separately. Combining these two designs
enables us to generate promising task-oriented flows and reduce the
difficulties in modeling large motions and handling occluded areas during frame
interpolation. These qualities promote our model to achieve state-of-the-art
performance on various benchmarks with high efficiency. Moreover, our
convolution-based model competes favorably compared to Transformer-based models
in terms of accuracy and efficiency. Our code is available at
https://github.com/MCG-NKU/AMT.",None,-1
e4c8262d-d808-4f8d-9f4b-36b2dd6d7397,Explicit and Implicit Knowledge Distillation via Unlabeled Data,0.562731,"Data-free knowledge distillation is a challenging model lightweight task for
scenarios in which the original dataset is not available. Previous methods
require a lot of extra computational costs to update one or more generators and
their naive imitate-learning lead to lower distillation efficiency. Based on
these observations, we first propose an efficient unlabeled sample selection
method to replace high computational generators and focus on improving the
training efficiency of the selected samples. Then, a class-dropping mechanism
is designed to suppress the label noise caused by the data domain shifts.
Finally, we propose a distillation method that incorporates explicit features
and implicit structured relations to improve the effect of distillation.
Experimental results show that our method can quickly converge and obtain
higher accuracy than other state-of-the-art methods.",None,-1
0e06b5c3-584c-4a64-ad3a-d46ae3ae0830,Super-Resolution Neural Operator,0.921305,"We propose Super-resolution Neural Operator (SRNO), a deep operator learning
framework that can resolve high-resolution (HR) images at arbitrary scales from
the low-resolution (LR) counterparts. Treating the LR-HR image pairs as
continuous functions approximated with different grid sizes, SRNO learns the
mapping between the corresponding function spaces. From the perspective of
approximation theory, SRNO first embeds the LR input into a higher-dimensional
latent representation space, trying to capture sufficient basis functions, and
then iteratively approximates the implicit image function with a kernel
integral mechanism, followed by a final dimensionality reduction step to
generate the RGB representation at the target coordinates. The key
characteristics distinguishing SRNO from prior continuous SR works are: 1) the
kernel integral in each layer is efficiently implemented via the Galerkin-type
attention, which possesses non-local properties in the spatial domain and
therefore benefits the grid-free continuum; and 2) the multilayer attention
architecture allows for the dynamic latent basis update, which is crucial for
SR problems to ""hallucinate"" high-frequency information from the LR image.
Experiments show that SRNO outperforms existing continuous SR methods in terms
of both accuracy and running time. Our code is at
https://github.com/2y7c3/Super-Resolution-Neural-Operator",None,-1
287f4536-d3b5-4a8f-bf9a-4ed17f85c2bc,Are Large Language Models Robust Coreference Resolvers?,0.147302,"Recent work on extending coreference resolution across domains and languages
relies on annotated data in both the target domain and language. At the same
time, pre-trained large language models (LMs) have been reported to exhibit
strong zero- and few-shot learning abilities across a wide range of NLP tasks.
However, prior work mostly studied this ability using artificial sentence-level
datasets such as the Winograd Schema Challenge. In this paper, we assess the
feasibility of prompt-based coreference resolution by evaluating
instruction-tuned language models on difficult, linguistically-complex
coreference benchmarks (e.g., CoNLL-2012). We show that prompting for
coreference can outperform current unsupervised coreference systems, although
this approach appears to be reliant on high-quality mention detectors. Further
investigations reveal that instruction-tuned LMs generalize surprisingly well
across domains, languages, and time periods; yet continued fine-tuning of
neural models should still be preferred if small amounts of annotated examples
are available.",None,-1
6e695f43-b18a-4981-bdc2-bba772199650,Robust Detection Outcome: A Metric for Pathology Detection in Medical Images,0.190247,"Detection of pathologies is a fundamental task in medical imaging and the
evaluation of algorithms that can perform this task automatically is crucial.
However, current object detection metrics for natural images do not reflect the
specific clinical requirements in pathology detection sufficiently. To tackle
this problem, we propose Robust Detection Outcome (RoDeO); a novel metric for
evaluating algorithms for pathology detection in medical images, especially in
chest X-rays. RoDeO evaluates different errors directly and individually, and
reflects clinical needs better than current metrics. Extensive evaluation on
the ChestX-ray8 dataset shows the superiority of our metrics compared to
existing ones. We released the code at https://github.com/FeliMe/RoDeO and
published RoDeO as pip package (rodeometric).",None,-1
c26adc6e-c7ba-4776-8872-82094f12502a,A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings,0.423898,"Predicting where a person is looking is a complex task, requiring to
understand not only the person's gaze and scene content, but also the 3D scene
structure and the person's situation (are they manipulating? interacting or
observing others? attentive?) to detect obstructions in the line of sight or
apply attention priors that humans typically have when observing others. In
this paper, we hypothesize that identifying and leveraging such priors can be
better achieved through the exploitation of explicitly derived multimodal cues
such as depth and pose. We thus propose a modular multimodal architecture
allowing to combine these cues using an attention mechanism. The architecture
can naturally be exploited in privacy-sensitive situations such as surveillance
and health, where personally identifiable information cannot be released. We
perform extensive experiments on the GazeFollow and VideoAttentionTarget public
datasets, obtaining state-of-the-art performance and demonstrating very
competitive results in the privacy setting case.",None,-1
451ab7b9-f250-44ab-8339-d5b3ec9e0634,Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM,0.957834,"The NLP community recently saw the release of a new large open-access
multilingual language model, BLOOM (BigScience et al., 2022) covering 46
languages. We focus on BLOOM's multilingual ability by evaluating its machine
translation performance across several datasets (WMT, Flores-101 and DiaBLa)
and language pairs (high- and low-resourced). Our results show that 0-shot
performance suffers from overgeneration and generating in the wrong language,
but this is greatly improved in the few-shot setting, with very good results
for a number of language pairs. We study several aspects including prompt
design, model sizes, cross-lingual transfer and the use of discursive context.",None,-1
73ceb786-f7dc-4139-8feb-ba7ac608f912,SVIT: Scaling up Visual Instruction Tuning,0.834127,"Thanks to the emerging of foundation models, the large language and vision
models are integrated to acquire the multimodal ability of visual captioning,
question answering, etc. Although existing multimodal models present impressive
performance of visual understanding and reasoning, their limits are still
largely under-explored due to the scarcity of high-quality instruction tuning
data. To push the limits of multimodal capability, we Scale up Visual
Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual
instruction tuning data including 1.6M conversation question-answer (QA) pairs,
1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed
image descriptions. Besides the volume, the proposed dataset is also featured
by the high quality and rich diversity, which is generated by prompting GPT-4
with the abundant manual annotations of images. We also propose a new data
recipe to select subset with better diversity and balance, which evokes model's
superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on
the proposed dataset, outperforms state-of-the-art Multimodal Large Language
Models on popular benchmarks. The data and code are publicly available at
https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.",None,-1
7f26dde6-3957-49ac-addb-10d6053b1c43,MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems,0.736872,"MeetEval is an open-source toolkit to evaluate all kinds of meeting
transcription systems. It provides a unified interface for the computation of
commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER
along other WER definitions. We extend the cpWER computation by a temporal
constraint to ensure that only words are identified as correct when the
temporal alignment is plausible. This leads to a better quality of the matching
of the hypothesis string to the reference string that more closely resembles
the actual transcription quality, and a system is penalized if it provides poor
time annotations. Since word-level timing information is often not available,
we present a way to approximate exact word-level timings from segment-level
timings (e.g., a sentence) and show that the approximation leads to a similar
WER as a matching with exact word-level annotations. At the same time, the time
constraint leads to a speedup of the matching algorithm, which outweighs the
additional overhead caused by processing the time stamps.",None,-1
2e02f4a6-8e47-4aca-a1da-a31927395dad,When to Trust AI: Advances and Challenges for Certification of Neural Networks,0.906671,"Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.",None,-1
1cfb4514-d513-48f1-aef0-0e3d2d0657f3,Dr.ICL: Demonstration-Retrieved In-context Learning,0.647619,"In-context learning (ICL), teaching a large language model (LLM) to perform a
task with few-shot demonstrations rather than adjusting the model parameters,
has emerged as a strong paradigm for using LLMs. While early studies primarily
used a fixed or random set of demonstrations for all test queries, recent
research suggests that retrieving semantically similar demonstrations to the
input from a pool of available demonstrations results in better performance.
This work expands the applicability of retrieval-based ICL approaches by
demonstrating that even simple word-overlap similarity measures such as BM25
outperform randomly selected demonstrations. Furthermore, we extend the success
of retrieval-based ICL to instruction-finetuned LLMs as well as
Chain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that
although a model has already seen the training data at training time,
retrieving demonstrations from the training data at test time yields better
results compared to using no demonstrations or random demonstrations. Last but
not least, we train a task-specific demonstration retriever that outperforms
off-the-shelf retrievers.",None,-1
0767f03f-6807-429c-b36d-3534fc3687de,Extrinsic Factors Affecting the Accuracy of Biomedical NER,0.375919,"Biomedical named entity recognition (NER) is a critial task that aims to
identify structured information in clinical text, which is often replete with
complex, technical terms and a high degree of variability. Accurate and
reliable NER can facilitate the extraction and analysis of important biomedical
information, which can be used to improve downstream applications including the
healthcare system. However, NER in the biomedical domain is challenging due to
limited data availability, as the high expertise, time, and expenses are
required to annotate its data. In this paper, by using the limited data, we
explore various extrinsic factors including the corpus annotation scheme, data
augmentation techniques, semi-supervised learning and Brill transformation, to
improve the performance of a NER model on a clinical text dataset (i2b2 2012,
\citet{sun-rumshisky-uzuner:2013}). Our experiments demonstrate that these
approaches can significantly improve the model's F1 score from original 73.74
to 77.55. Our findings suggest that considering different extrinsic factors and
combining these techniques is a promising approach for improving NER
performance in the biomedical domain where the size of data is limited.",None,-1
1a42a218-9cfc-4f46-b66b-2033ff227b62,Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models,0.0154914,"This project focuses on enhancing open-source large language models through
instruction-tuning and providing comprehensive evaluations of their
performance. We explore how various training data factors, such as quantity,
quality, and linguistic distribution, influence the performance of
instruction-tuned models trained on publicly accessible high-quality
instruction datasets for both English and Chinese languages. Our goal is to
supplement evaluation with quantitative analyses, providing valuable insights
for the continued advancement of open-source chat models. Our model, data, and
code are publicly available for others to use and build upon.",None,-1
0ff69ad2-c100-40c9-ad2a-a62adcea563e,Disjoint Pose and Shape for 3D Face Reconstruction,0.164745,"Existing methods for 3D face reconstruction from a few casually captured
images employ deep learning based models along with a 3D Morphable Model(3DMM)
as face geometry prior. Structure From Motion(SFM), followed by Multi-View
Stereo (MVS), on the other hand, uses dozens of high-resolution images to
reconstruct accurate 3D faces.However, it produces noisy and stretched-out
results with only two views available. In this paper, taking inspiration from
both these methods, we propose an end-to-end pipeline that disjointly solves
for pose and shape to make the optimization stable and accurate. We use a face
shape prior to estimate face pose and use stereo matching followed by a 3DMM to
solve for the shape. The proposed method achieves end-to-end topological
consistency, enables iterative face pose refinement procedure, and show
remarkable improvement on both quantitative and qualitative results over
existing state-of-the-art methods.",None,-1
ba1d1f60-5bf9-46a4-873c-c87de1cf5250,MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation,0.710772,"Weakly-supervised semantic segmentation aims to reduce labeling costs by
training semantic segmentation models using weak supervision, such as
image-level class labels. However, most approaches struggle to produce accurate
localization maps and suffer from false predictions in class-related
backgrounds (i.e., biased objects), such as detecting a railroad with the train
class. Recent methods that remove biased objects require additional supervision
for manually identifying biased objects for each problematic class and
collecting their datasets by reviewing predictions, limiting their
applicability to the real-world dataset with multiple labels and complex
relationships for biasing. Following the first observation that biased features
can be separated and eliminated by matching biased objects with backgrounds in
the same dataset, we propose a fully-automatic/model-agnostic biased removal
framework called MARS (Model-Agnostic biased object Removal without additional
Supervision), which utilizes semantically consistent features of an
unsupervised technique to eliminate biased objects in pseudo labels.
Surprisingly, we show that MARS achieves new state-of-the-art results on two
popular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014
(val: 49.4%), by consistently improving the performance of various WSSS models
by at least 30% without additional supervision.",None,-1
36ab12a9-abd5-445e-9059-0dd95ace1107,Prompt Engineering a Prompt Engineer,0.637058,"Prompt engineering is a challenging yet crucial task for optimizing the
performance of large language models on customized tasks. It requires complex
reasoning to examine the model's errors, hypothesize what is missing or
misleading in the current prompt, and communicate the task with clarity. While
recent works indicate that large language models can be meta-prompted to
perform automatic prompt engineering, we argue that their potential is limited
due to insufficient guidance for complex reasoning in the meta-prompt. We fill
this gap by infusing into the meta-prompt three key components: detailed
descriptions, context specification, and a step-by-step reasoning template. The
resulting method, named PE2, showcases remarkable versatility across diverse
language tasks. It finds prompts that outperform ""let's think step by step"" by
6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on
counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted
prompt edits, rectify erroneous prompts, and induce multi-step plans for
complex tasks.",None,-1
5f90083d-71eb-40e5-a91f-8f75b333f59e,Continual Road-Scene Semantic Segmentation via Feature-Aligned Symmetric Multi-Modal Network,0.566107,"State-of-the-art multimodal semantic segmentation strategies combining LiDAR
and color data are usually designed on top of asymmetric information-sharing
schemes and assume that both modalities are always available. This strong
assumption may not hold in real-world scenarios, where sensors are prone to
failure or can face adverse conditions that make the acquired information
unreliable. This problem is exacerbated when continual learning scenarios are
considered since they have stringent data reliability constraints. In this
work, we re-frame the task of multimodal semantic segmentation by enforcing a
tightly coupled feature representation and a symmetric information-sharing
scheme, which allows our approach to work even when one of the input modalities
is missing. We also introduce an ad-hoc class-incremental continual learning
scheme, proving our approach's effectiveness and reliability even in
safety-critical settings, such as autonomous driving. We evaluate our approach
on the SemanticKITTI dataset, achieving impressive performances.",None,-1
fe0b1af2-7dc1-41ab-b69a-e2d829fee36f,Iterative Geometry Encoding Volume for Stereo Matching,0.99678,"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in
matching tasks. However, all-pairs correlations lack non-local geometry
knowledge and have difficulties tackling local ambiguities in ill-posed
regions. In this paper, we propose Iterative Geometry Encoding Volume
(IGEV-Stereo), a new deep network architecture for stereo matching. The
proposed IGEV-Stereo builds a combined geometry encoding volume that encodes
geometry and context information as well as local matching details, and
iteratively indexes it to update the disparity map. To speed up the
convergence, we exploit GEV to regress an accurate starting point for ConvGRUs
iterations. Our IGEV-Stereo ranks $1^{st}$ on KITTI 2015 and 2012 (Reflective)
among all published methods and is the fastest among the top 10 methods. In
addition, IGEV-Stereo has strong cross-dataset generalization as well as high
inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e.
IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is
available at https://github.com/gangweiX/IGEV.",None,-1
f28bc96c-e0fc-4e8b-96b7-f91e1c5a758a,EgoBlur: Responsible Innovation in Aria,0.0691899,"Project Aria pushes the frontiers of Egocentric AI with large-scale
real-world data collection using purposely designed glasses with privacy first
approach. To protect the privacy of bystanders being recorded by the glasses,
our research protocols are designed to ensure recorded video is processed by an
AI anonymization model that removes bystander faces and vehicle license plates.
Detected face and license plate regions are processed with a Gaussian blur such
that these personal identification information (PII) regions are obscured. This
process helps to ensure that anonymized versions of the video is retained for
research purposes. In Project Aria, we have developed a state-of-the-art
anonymization system EgoBlur. In this paper, we present extensive analysis of
EgoBlur on challenging datasets comparing its performance with other
state-of-the-art systems from industry and academia including extensive
Responsible AI analysis on recently released Casual Conversations V2 dataset.",None,-1
19164fef-ad1b-4a2a-a62f-f6d6f8513d1c,Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment,0.548843,"Urban environments, characterized by their complex, multi-layered networks
encompassing physical, social, economic, and environmental dimensions, face
significant challenges in the face of rapid urbanization. These challenges,
ranging from traffic congestion and pollution to social inequality, call for
advanced technological interventions. Recent developments in big data,
artificial intelligence, urban computing, and digital twins have laid the
groundwork for sophisticated city modeling and simulation. However, a gap
persists between these technological capabilities and their practical
implementation in addressing urban challenges in an systemic-intelligent way.
This paper proposes Urban Generative Intelligence (UGI), a novel foundational
platform integrating Large Language Models (LLMs) into urban systems to foster
a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model
trained on city-specific multi-source data, to create embodied agents for
various urban tasks. These agents, operating within a textual urban environment
emulated by city simulator and urban knowledge graph, interact through a
natural language interface, offering an open platform for diverse intelligent
and embodied agent development. This platform not only addresses specific urban
issues but also simulates complex urban systems, providing a multidisciplinary
approach to understand and manage urban complexity. This work signifies a
transformative step in city science and urban intelligence, harnessing the
power of LLMs to unravel and address the intricate dynamics of urban systems.
The code repository with demonstrations will soon be released here
https://github.com/tsinghua-fib-lab/UGI.",None,-1
4211b2f7-bf07-43d2-9225-9256584f3d39,Prompting Neural Machine Translation with Translation Memories,0.113071,"Improving machine translation (MT) systems with translation memories (TMs) is
of great interest to practitioners in the MT community. However, previous
approaches require either a significant update of the model architecture and/or
additional training efforts to make the models well-behaved when TMs are taken
as additional input. In this paper, we present a simple but effective method to
introduce TMs into neural machine translation (NMT) systems. Specifically, we
treat TMs as prompts to the NMT model at test time, but leave the training
process unchanged. The result is a slight update of an existing NMT system,
which can be implemented in a few hours by anyone who is familiar with NMT.
Experimental results on several datasets demonstrate that our system
significantly outperforms strong baselines.",None,-1
12ef3f0f-d03b-4c58-82be-43d5d557d501,Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature,0.234385,"We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large
language model to engage in meaningful interactions with Astronomy papers using
in-context prompting. To optimize for efficiency, we employ a distillation
technique that effectively reduces the size of the original input paper by
50\%, while maintaining the paragraph structure and overall semantic integrity.
We then explore the model's responses using a multi-document context (ten
distilled documents). Our findings indicate that GPT-4 excels in the
multi-document domain, providing detailed answers contextualized within the
framework of related research findings. Our results showcase the potential of
large language models for the astronomical community, offering a promising
avenue for further exploration, particularly the possibility of utilizing the
models for hypothesis generation.",None,-1
87a284ed-fb13-4a5d-b42d-fced32a3d34b,Semi-Oblivious Chase Termination for Linear Existential Rules: An Experimental Study,0.0389661,"The chase procedure is a fundamental algorithmic tool in databases that
allows us to reason with constraints, such as existential rules, with a
plethora of applications. It takes as input a database and a set of
constraints, and iteratively completes the database as dictated by the
constraints. A key challenge, though, is the fact that it may not terminate,
which leads to the problem of checking whether it terminates given a database
and a set of constraints. In this work, we focus on the semi-oblivious version
of the chase, which is well-suited for practical implementations, and linear
existential rules, a central class of constraints with several applications. In
this setting, there is a mature body of theoretical work that provides
syntactic characterizations of when the chase terminates, algorithms for
checking chase termination, precise complexity results, and worst-case optimal
bounds on the size of the result of the chase (whenever is finite). Our main
objective is to experimentally evaluate the existing chase termination
algorithms with the aim of understanding which input parameters affect their
performance, clarifying whether they can be used in practice, and revealing
their performance limitations.",None,-1
dcd92ed9-681c-4535-8e83-705352768e31,On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study,0.540388,"Modern deep models for summarization attains impressive benchmark
performance, but they are prone to generating miscalibrated predictive
uncertainty. This means that they assign high confidence to low-quality
predictions, leading to compromised reliability and trustworthiness in
real-world applications. Probabilistic deep learning methods are common
solutions to the miscalibration problem. However, their relative effectiveness
in complex autoregressive summarization tasks are not well-understood. In this
work, we thoroughly investigate different state-of-the-art probabilistic
methods' effectiveness in improving the uncertainty quality of the neural
summarization models, across three large-scale benchmarks with varying
difficulty. We show that the probabilistic methods consistently improve the
model's generation and uncertainty quality, leading to improved selective
generation performance (i.e., abstaining from low-quality summaries) in
practice. We also reveal notable failure patterns of probabilistic methods
widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout),
cautioning the importance of choosing appropriate method for the data setting.",None,-1
39e3af23-40b6-4e85-aec4-24757c7e26ea,InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators,0.704249,"Instruction-based language modeling has received significant attention in
pretrained language models. However, the efficiency of instruction engineering
remains low and hinders the development of instruction studies. Recent studies
have focused on automating instruction generation, but they primarily aim to
improve performance without considering other crucial objectives that impact
instruction quality, such as instruction length and perplexity. Therefore, we
propose a novel approach (i.e., InstOptima) that treats instruction generation
as an evolutionary multi-objective optimization problem. In contrast to text
edition-based methods, our approach utilizes a large language model (LLM) to
simulate instruction operators, including mutation and crossover. Furthermore,
we introduce an objective-guided mechanism for these operators, allowing the
LLM to comprehend the objectives and enhance the quality of the generated
instructions. Experimental results demonstrate improved fine-tuning performance
and the generation of a diverse set of high-quality instructions.",None,-1
fa04b7b9-b09b-4400-bfa8-f66f422c7990,Algorithm-assisted discovery of an intrinsic order among mathematical constants,0.425472,"In recent decades, a growing number of discoveries in fields of mathematics
have been assisted by computer algorithms, primarily for exploring large
parameter spaces that humans would take too long to investigate. As computers
and algorithms become more powerful, an intriguing possibility arises - the
interplay between human intuition and computer algorithms can lead to
discoveries of novel mathematical concepts that would otherwise remain elusive.
To realize this perspective, we have developed a massively parallel computer
algorithm that discovers an unprecedented number of continued fraction formulas
for fundamental mathematical constants. The sheer number of formulas discovered
by the algorithm unveils a novel mathematical structure that we call the
conservative matrix field. Such matrix fields (1) unify thousands of existing
formulas, (2) generate infinitely many new formulas, and most importantly, (3)
lead to unexpected relations between different mathematical constants,
including multiple integer values of the Riemann zeta function. Conservative
matrix fields also enable new mathematical proofs of irrationality. In
particular, we can use them to generalize the celebrated proof by Ap\'ery for
the irrationality of $\zeta(3)$. Utilizing thousands of personal computers
worldwide, our computer-supported research strategy demonstrates the power of
experimental mathematics, highlighting the prospects of large-scale
computational approaches to tackle longstanding open problems and discover
unexpected connections across diverse fields of science.",None,-1
3375dcd0-2494-4411-b52d-753ed392dc97,4D Myocardium Reconstruction with Decoupled Motion and Shape Model,0.157187,"Estimating the shape and motion state of the myocardium is essential in
diagnosing cardiovascular diseases.However, cine magnetic resonance (CMR)
imaging is dominated by 2D slices, whose large slice spacing challenges
inter-slice shape reconstruction and motion acquisition.To address this
problem, we propose a 4D reconstruction method that decouples motion and shape,
which can predict the inter-/intra- shape and motion estimation from a given
sparse point cloud sequence obtained from limited slices. Our framework
comprises a neural motion model and an end-diastolic (ED) shape model. The
implicit ED shape model can learn a continuous boundary and encourage the
motion model to predict without the supervision of ground truth deformation,
and the motion model enables canonical input of the shape model by deforming
any point from any phase to the ED phase. Additionally, the constructed
ED-space enables pre-training of the shape model, thereby guiding the motion
model and addressing the issue of data scarcity. We propose the first 4D
myocardial dataset as we know and verify our method on the proposed, public,
and cross-modal datasets, showing superior reconstruction performance and
enabling various clinical applications.",None,-1
da702e71-c7dd-40a1-8a46-1258a9307444,Autonomous Agent for Beyond Visual Range Air Combat: A Deep Reinforcement Learning Approach,0.11274,"This work contributes to developing an agent based on deep reinforcement
learning capable of acting in a beyond visual range (BVR) air combat simulation
environment. The paper presents an overview of building an agent representing a
high-performance fighter aircraft that can learn and improve its role in BVR
combat over time based on rewards calculated using operational metrics. Also,
through self-play experiments, it expects to generate new air combat tactics
never seen before. Finally, we hope to examine a real pilot's ability, using
virtual simulation, to interact in the same environment with the trained agent
and compare their performances. This research will contribute to the air combat
training context by developing agents that can interact with real pilots to
improve their performances in air defense missions.",None,-1
06e28bd5-a0d0-4495-9bf3-dbc8a37009d1,DDS3D: Dense Pseudo-Labels with Dynamic Threshold for Semi-Supervised 3D Object Detection,0.815199,"In this paper, we present a simple yet effective semi-supervised 3D object
detector named DDS3D. Our main contributions have two-fold. On the one hand,
different from previous works using Non-Maximal Suppression (NMS) or its
variants for obtaining the sparse pseudo labels, we propose a dense
pseudo-label generation strategy to get dense pseudo-labels, which can retain
more potential supervision information for the student network. On the other
hand, instead of traditional fixed thresholds, we propose a dynamic threshold
manner to generate pseudo-labels, which can guarantee the quality and quantity
of pseudo-labels during the whole training process. Benefiting from these two
components, our DDS3D outperforms the state-of-the-art semi-supervised 3d
object detection with mAP of 3.1% on the pedestrian and 2.1% on the cyclist
under the same configuration of 1% samples. Extensive ablation studies on the
KITTI dataset demonstrate the effectiveness of our DDS3D. The code and models
will be made publicly available at https://github.com/hust-jy/DDS3D",None,-1
1122b034-3d19-4b73-981f-4e9e375d8368,MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup,0.698806,"Current disfluency detection models focus on individual utterances each from
a single speaker. However, numerous discontinuity phenomena in spoken
conversational transcripts occur across multiple turns, hampering human
readability and the performance of downstream NLP tasks. This study addresses
these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken
conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We
design a data labeling schema to collect the high-quality dataset and provide
extensive data analysis. Furthermore, we leverage two modeling approaches for
experimental evaluation as benchmarks for future research.",None,-1
74399787-8afb-4c7c-8d56-b0da2a25a6f2,Pseudo-label Alignment for Semi-supervised Instance Segmentation,0.867588,"Pseudo-labeling is significant for semi-supervised instance segmentation,
which generates instance masks and classes from unannotated images for
subsequent training. However, in existing pipelines, pseudo-labels that contain
valuable information may be directly filtered out due to mismatches in class
and mask quality. To address this issue, we propose a novel framework, called
pseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we
devise a dynamic aligning loss (DALoss) that adjusts the weights of
semi-supervised loss terms with varying class and mask score pairs. Through
extensive experiments conducted on the COCO and Cityscapes datasets, we
demonstrate that PAIS is a promising framework for semi-supervised instance
segmentation, particularly in cases where labeled data is severely limited.
Notably, with just 1\% labeled data, PAIS achieves 21.2 mAP (based on
Mask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the
current state-of-the-art model, \ie, NoisyBoundary with 7.7 mAP, by a margin of
over 12 points. Code is available at: \url{https://github.com/hujiecpp/PAIS}.",None,-1
d894a3c0-f3f2-482b-b33f-906ffaaccf7a,DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase,0.238229,"In-Context Learning (ICL) combined with pre-trained large language models has
achieved promising results on various NLP tasks. However, ICL requires
high-quality annotated demonstrations which might not be available in
real-world scenarios. To overcome this limitation, we propose \textbf{D}ata
\textbf{A}ugmentation for \textbf{I}n-Context \textbf{L}earning
(\textbf{DAIL}). DAIL leverages the intuition that large language models are
more familiar with the content generated by themselves. It first utilizes the
language model to generate paraphrases of the test sample and employs majority
voting to determine the final result based on individual predictions. Our
extensive empirical evaluation shows that DAIL outperforms the standard ICL
method and other ensemble-based methods in the low-resource scenario.
Additionally, we explore the use of voting consistency as a confidence score of
the model when the logits of predictions are inaccessible. We believe our work
will stimulate further research on ICL in low-resource settings.",None,-1
31c7b081-2191-4157-8872-8c6015e46bd3,Predictive Heterogeneity: Measures and Applications,0.0716784,"As an intrinsic and fundamental property of big data, data heterogeneity
exists in a variety of real-world applications, such as precision medicine,
autonomous driving, financial applications, etc. For machine learning
algorithms, the ignorance of data heterogeneity will greatly hurt the
generalization performance and the algorithmic fairness, since the prediction
mechanisms among different sub-populations are likely to differ from each
other. In this work, we focus on the data heterogeneity that affects the
prediction of machine learning models, and firstly propose the \emph{usable
predictive heterogeneity}, which takes into account the model capacity and
computational constraints. We prove that it can be reliably estimated from
finite data with probably approximately correct (PAC) bounds. Additionally, we
design a bi-level optimization algorithm to explore the usable predictive
heterogeneity from data. Empirically, the explored heterogeneity provides
insights for sub-population divisions in income prediction, crop yield
prediction and image classification tasks, and leveraging such heterogeneity
benefits the out-of-distribution generalization performance.",None,-1
82568cbe-9c37-4a7e-a233-c1d39af48c2f,Stance Detection: A Practical Guide to Classifying Political Beliefs in Text,0.0816853,"Stance detection is identifying expressed beliefs in a document. While
researchers widely use sentiment analysis for this, recent research
demonstrates that sentiment and stance are distinct. This paper advances text
analysis methods by precisely defining stance detection and presenting three
distinct approaches: supervised classification, natural language inference, and
in-context learning with generative language models. I discuss how document
context and trade-offs between resources and workload should inform your
methods. For all three approaches I provide guidance on application and
validation techniques, as well as coding tutorials for implementation. Finally,
I demonstrate how newer classification approaches can replicate supervised
classifiers.",None,-1
308e4bf4-fe2c-4302-a6d7-c165ff7d85d5,Diffusion-based 3D Object Detection with Random Boxes,0.47813,"3D object detection is an essential task for achieving autonomous driving.
Existing anchor-based detection methods rely on empirical heuristics setting of
anchors, which makes the algorithms lack elegance. In recent years, we have
witnessed the rise of several generative models, among which diffusion models
show great potential for learning the transformation of two distributions. Our
proposed Diff3Det migrates the diffusion model to proposal generation for 3D
object detection by considering the detection boxes as generative targets.
During training, the object boxes diffuse from the ground truth boxes to the
Gaussian distribution, and the decoder learns to reverse this noise process. In
the inference stage, the model progressively refines a set of random boxes to
the prediction results. We provide detailed experiments on the KITTI benchmark
and achieve promising performance compared to classical anchor-based 3D
detection methods.",None,-1
b804cb98-3167-437e-8f1d-49dfa083e7b1,Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs,0.607936,"The latest advancements in large language models (LLMs) have revolutionized
the field of natural language processing (NLP). Inspired by the success of LLMs
in NLP tasks, some recent work has begun investigating the potential of
applying LLMs in graph learning tasks. However, most of the existing work
focuses on utilizing LLMs as powerful node feature augmenters, leaving
employing LLMs to enhance graph topological structures an understudied problem.
In this work, we explore how to leverage the information retrieval and text
generation capabilities of LLMs to refine/enhance the topological structure of
text-attributed graphs (TAGs) under the node classification setting. First, we
propose using LLMs to help remove unreliable edges and add reliable ones in the
TAG. Specifically, we first let the LLM output the semantic similarity between
node attributes through delicate prompt designs, and then perform edge deletion
and edge addition based on the similarity. Second, we propose using
pseudo-labels generated by the LLM to improve graph topology, that is, we
introduce the pseudo-label propagation as a regularization to guide the graph
neural network (GNN) in learning proper edge weights. Finally, we incorporate
the two aforementioned LLM-based methods for graph topological refinement into
the process of GNN training, and perform extensive experiments on four
real-world datasets. The experimental results demonstrate the effectiveness of
LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain
on public benchmarks).",None,-1
4b91f341-e516-4278-ae6e-29db0d1b332c,Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations,0.686332,"This paper aims to address the unsupervised video anomaly detection (VAD)
problem, which involves classifying each frame in a video as normal or
abnormal, without any access to labels. To accomplish this, the proposed method
employs conditional diffusion models, where the input data is the
spatiotemporal features extracted from a pre-trained network, and the condition
is the features extracted from compact motion representations that summarize a
given video segment in terms of its motion and appearance. Our method utilizes
a data-driven threshold and considers a high reconstruction error as an
indicator of anomalous events. This study is the first to utilize compact
motion representations for VAD and the experiments conducted on two large-scale
VAD benchmarks demonstrate that they supply relevant information to the
diffusion model, and consequently improve VAD performances w.r.t the prior art.
Importantly, our method exhibits better generalization performance across
different datasets, notably outperforming both the state-of-the-art and
baseline methods. The code of our method is available at
https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion",None,-1
9f69b4fb-e496-4e06-ba02-4672d4899903,Learning Human-Compatible Representations for Case-Based Decision Support,0.514484,"Algorithmic case-based decision support provides examples to help human make
sense of predicted labels and aid human in decision-making tasks. Despite the
promising performance of supervised learning, representations learned by
supervised models may not align well with human intuitions: what models
consider as similar examples can be perceived as distinct by humans. As a
result, they have limited effectiveness in case-based decision support. In this
work, we incorporate ideas from metric learning with supervised learning to
examine the importance of alignment for effective decision support. In addition
to instance-level labels, we use human-provided triplet judgments to learn
human-compatible decision-focused representations. Using both synthetic data
and human subject experiments in multiple classification tasks, we demonstrate
that such representation is better aligned with human perception than
representation solely optimized for classification. Human-compatible
representations identify nearest neighbors that are perceived as more similar
by humans and allow humans to make more accurate predictions, leading to
substantial improvements in human decision accuracies (17.8% in butterfly vs.
moth classification and 13.2% in pneumonia classification).",None,-1
8328bc2c-aa1d-4428-bfd0-c6f4316448bb,Not with my name! Inferring artists' names of input strings employed by Diffusion Models,0.0815472,"Diffusion Models (DM) are highly effective at generating realistic,
high-quality images. However, these models lack creativity and merely compose
outputs based on their training data, guided by a textual input provided at
creation time. Is it acceptable to generate images reminiscent of an artist,
employing his name as input? This imply that if the DM is able to replicate an
artist's work then it was trained on some or all of his artworks thus violating
copyright. In this paper, a preliminary study to infer the probability of use
of an artist's name in the input string of a generated image is presented. To
this aim we focused only on images generated by the famous DALL-E 2 and
collected images (both original and generated) of five renowned artists.
Finally, a dedicated Siamese Neural Network was employed to have a first kind
of probability. Experimental results demonstrate that our approach is an
optimal starting point and can be employed as a prior for predicting a complete
input string of an investigated image. Dataset and code are available at:
https://github.com/ictlab-unict/not-with-my-name .",None,-1
2d41818e-14b6-48b0-ab58-997e80b0e1e9,SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger,0.427302,"During the preceding biennium, vision-language pre-training has achieved
noteworthy success on several downstream tasks. Nevertheless, acquiring
high-quality image-text pairs, where the pairs are entirely exclusive of each
other, remains a challenging task, and noise exists in the commonly used
datasets. To address this issue, we propose SoftCLIP, a novel approach that
relaxes the strict one-to-one constraint and achieves a soft cross-modal
alignment by introducing a softened target, which is generated from the
fine-grained intra-modal self-similarity. The intra-modal guidance is
indicative to enable two pairs have some local similarities and model
many-to-many relationships between the two modalities. Besides, since the
positive still dominates in the softened target distribution, we disentangle
the negatives in the distribution to further boost the relation alignment with
the negatives in the cross-modal learning. Extensive experiments demonstrate
the effectiveness of SoftCLIP. In particular, on ImageNet zero-shot
classification task, using CC3M/CC12M as pre-training dataset, SoftCLIP brings
a top-1 accuracy improvement of 6.8%/7.2% over the CLIP baseline.",None,-1
631c0be2-95eb-43d5-90b7-1816d9b49c0c,Which Tokens to Use? Investigating Token Reduction in Vision Transformers,0.542913,"Since the introduction of the Vision Transformer (ViT), researchers have
sought to make ViTs more efficient by removing redundant information in the
processed tokens. While different methods have been explored to achieve this
goal, we still lack understanding of the resulting reduction patterns and how
those patterns differ across token reduction methods and datasets. To close
this gap, we set out to understand the reduction patterns of 10 different token
reduction methods using four image classification datasets. By systematically
comparing these methods on the different classification tasks, we find that the
Top-K pruning method is a surprisingly strong baseline. Through in-depth
analysis of the different methods, we determine that: the reduction patterns
are generally not consistent when varying the capacity of the backbone model,
the reduction patterns of pruning-based methods significantly differ from fixed
radial patterns, and the reduction patterns of pruning-based methods are
correlated across classification datasets. Finally we report that the
similarity of reduction patterns is a moderate-to-strong proxy for model
performance. Project page at https://vap.aau.dk/tokens.",None,-1
16ac2cce-ef0b-44d5-b1d6-ff7ecc63e3a2,Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification,0.45858,"Authorship verification (AV) is a fundamental task in natural language
processing (NLP) and computational linguistics, with applications in forensic
analysis, plagiarism detection, and identification of deceptive content.
Existing AV techniques, including traditional stylometric and deep learning
approaches, face limitations in terms of data requirements and lack of
explainability. To address these limitations, this paper proposes PromptAV, a
novel technique that leverages Large-Language Models (LLMs) for AV by providing
step-by-step stylometric explanation prompts. PromptAV outperforms
state-of-the-art baselines, operates effectively with limited training data,
and enhances interpretability through intuitive explanations, showcasing its
potential as an effective and interpretable solution for the AV task.",None,-1
acc923e0-6f74-4222-ac8a-4824cd51ec9f,Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation,0.618403,"Simultaneous speech translation is an essential communication task difficult
for humans whereby a translation is generated concurrently with oncoming speech
inputs. For such a streaming task, transformers using block processing to break
an input sequence into segments have achieved state-of-the-art performance at a
reduced cost. Current methods to allow information to propagate across
segments, including left context and memory banks, have faltered as they are
both insufficient representations and unnecessarily expensive to compute. In
this paper, we propose an Implicit Memory Transformer that implicitly retains
memory through a new left context method, removing the need to explicitly
represent memory with memory banks. We generate the left context from the
attention output of the previous segment and include it in the keys and values
of the current segment's attention calculation. Experiments on the MuST-C
dataset show that the Implicit Memory Transformer provides a substantial
speedup on the encoder forward pass with nearly identical translation quality
when compared with the state-of-the-art approach that employs both left context
and memory banks.",None,-1
4621f945-84e3-4d4e-aca6-fbc04146c665,Attention-based Part Assembly for 3D Volumetric Shape Modeling,0.0783098,"Modeling a 3D volumetric shape as an assembly of decomposed shape parts is
much more challenging, but semantically more valuable than direct
reconstruction from a full shape representation. The neural network needs to
implicitly learn part relations coherently, which is typically performed by
dedicated network layers that can generate transformation matrices for each
part. In this paper, we propose a VoxAttention network architecture for
attention-based part assembly. We further propose a variant of using
channel-wise part attention and show the advantages of this approach.
Experimental results show that our method outperforms most state-of-the-art
methods for the part relation-aware 3D shape modeling task.",None,-1
7c312df0-ccf6-4d18-9949-9b9ecafdfba0,CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation,0.544497,"Beyond novel view synthesis, Neural Radiance Fields are useful for
applications that interact with the real world. In this paper, we use them as
an implicit map of a given scene and propose a camera relocalization algorithm
tailored for this representation. The proposed method enables to compute in
real-time the precise position of a device using a single RGB camera, during
its navigation. In contrast with previous work, we do not rely on pose
regression or photometric alignment but rather use dense local features
obtained through volumetric rendering which are specialized on the scene with a
self-supervised objective. As a result, our algorithm is more accurate than
competitors, able to operate in dynamic outdoor environments with changing
lightning conditions and can be readily integrated in any volumetric neural
renderer.",None,-1
aae4e8ce-d4ed-421d-b7a0-e58a739c35db,mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection,0.512968,"This paper presents the winning system for the zero-shot Spanish framing
detection task, which also achieves competitive places in eight additional
languages. The challenge of the framing detection task lies in identifying a
set of 14 frames when only a few or zero samples are available, i.e., a
multilingual multi-label few- or zero-shot setting. Our developed solution
employs a pre-training procedure based on multilingual Transformers using a
label-aware contrastive loss function. In addition to describing the system, we
perform an embedding space analysis and ablation study to demonstrate how our
pre-training procedure supports framing detection to advance computational
framing analysis.",None,-1
588ba8b6-b9ca-4cf4-a579-f66a52aece3f,Painterly Image Harmonization using Diffusion Model,0.75003,"Painterly image harmonization aims to insert photographic objects into
paintings and obtain artistically coherent composite images. Previous methods
for this task mainly rely on inference optimization or generative adversarial
network, but they are either very time-consuming or struggling at fine control
of the foreground objects (e.g., texture and content details). To address these
issues, we propose a novel Painterly Harmonization stable Diffusion model
(PHDiffusion), which includes a lightweight adaptive encoder and a Dual Encoder
Fusion (DEF) module. Specifically, the adaptive encoder and the DEF module
first stylize foreground features within each encoder. Then, the stylized
foreground features from both encoders are combined to guide the harmonization
process. During training, besides the noise loss in diffusion model, we
additionally employ content loss and two style losses, i.e., AdaIN style loss
and contrastive style loss, aiming to balance the trade-off between style
migration and content preservation. Compared with the state-of-the-art models
from related fields, our PHDiffusion can stylize the foreground more
sufficiently and simultaneously retain finer content. Our code and model are
available at https://github.com/bcmi/PHDiffusion-Painterly-Image-Harmonization.",None,-1
30ab49ff-5ae7-4703-9d97-4733325517a1,Learning with Rejection for Abstractive Text Summarization,0.264664,"State-of-the-art abstractive summarization systems frequently hallucinate
content that is not supported by the source document, mainly due to noise in
the training dataset. Existing methods opt to drop the noisy samples or tokens
from the training set entirely, reducing the effective training set size and
creating an artificial propensity to copy words from the source. In this work,
we propose a training objective for abstractive summarization based on
rejection learning, in which the model learns whether or not to reject
potentially noisy tokens. We further propose a regularized decoding objective
that penalizes non-factual candidate summaries during inference by using the
rejection probability learned during training. We show that our method
considerably improves the factuality of generated summaries in automatic and
human evaluations when compared to five baseline models and that it does so
while increasing the abstractiveness of the generated summaries.",None,-1
d624f23d-7deb-4c45-a244-3626ef53bdc7,InstructDiffusion: A Generalist Modeling Interface for Vision Tasks,0.859959,"We present InstructDiffusion, a unifying and generic framework for aligning
computer vision tasks with human instructions. Unlike existing approaches that
integrate prior knowledge and pre-define the output space (e.g., categories and
coordinates) for each vision task, we cast diverse vision tasks into a
human-intuitive image-manipulating process whose output space is a flexible and
interactive pixel space. Concretely, the model is built upon the diffusion
process and is trained to predict pixels according to user instructions, such
as encircling the man's left shoulder in red or applying a blue mask to the
left car. InstructDiffusion could handle a variety of vision tasks, including
understanding tasks (such as segmentation and keypoint detection) and
generative tasks (such as editing and enhancement). It even exhibits the
ability to handle unseen tasks and outperforms prior methods on novel datasets.
This represents a significant step towards a generalist modeling interface for
vision tasks, advancing artificial general intelligence in the field of
computer vision.",None,-1
6637aaf7-46e6-4bf0-8b5a-84fe609143ba,Orca 2: Teaching Small Language Models How to Reason,0.846302,"Orca 1 learns from rich signals, such as explanation traces, allowing it to
outperform conventional instruction-tuned models on benchmarks like BigBench
Hard and AGIEval. In Orca 2, we continue exploring how improved training
signals can enhance smaller LMs' reasoning abilities. Research on training
small LMs has often relied on imitation learning to replicate the output of
more capable models. We contend that excessive emphasis on imitation may
restrict the potential of smaller models. We seek to teach small LMs to employ
different solution strategies for different tasks, potentially different from
the one used by the larger model. For example, while larger models might
provide a direct answer to a complex task, smaller models may not have the same
capacity. In Orca 2, we teach the model various reasoning techniques
(step-by-step, recall then generate, recall-reason-generate, direct answer,
etc.). More crucially, we aim to help the model learn to determine the most
effective solution strategy for each task. We evaluate Orca 2 using a
comprehensive set of 15 diverse benchmarks (corresponding to approximately 100
tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of
similar size and attains performance levels similar or better to those of
models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings. make Orca 2 weights publicly available at
aka.ms/orca-lm to support research on the development, evaluation, and
alignment of smaller LMs",None,-1
6ebbe8cd-02db-48b9-8f86-108ca52f3056,Diffusion-Augmented Depth Prediction with Sparse Annotations,0.570268,"Depth estimation aims to predict dense depth maps. In autonomous driving
scenes, sparsity of annotations makes the task challenging. Supervised models
produce concave objects due to insufficient structural information. They
overfit to valid pixels and fail to restore spatial structures. Self-supervised
methods are proposed for the problem. Their robustness is limited by pose
estimation, leading to erroneous results in natural scenes. In this paper, we
propose a supervised framework termed Diffusion-Augmented Depth Prediction
(DADP). We leverage the structural characteristics of diffusion model to
enforce depth structures of depth models in a plug-and-play manner. An
object-guided integrality loss is also proposed to further enhance regional
structure integrality by fetching objective information. We evaluate DADP on
three driving benchmarks and achieve significant improvements in depth
structures and robustness. Our work provides a new perspective on depth
estimation with sparse annotations in autonomous driving scenes.",None,-1
c72d756a-e5b1-4f22-8188-92df2855d989,"Towards Conceptualization of ""Fair Explanation"": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators",0.172302,"Recent research at the intersection of AI explainability and fairness has
focused on how explanations can improve human-plus-AI task performance as
assessed by fairness measures. We propose to characterize what constitutes an
explanation that is itself ""fair"" -- an explanation that does not adversely
impact specific populations. We formulate a novel evaluation method of ""fair
explanations"" using not just accuracy and label time, but also psychological
impact of explanations on different user groups across many metrics (mental
discomfort, stereotype activation, and perceived workload). We apply this
method in the context of content moderation of potential hate speech, and its
differential impact on Asian vs. non-Asian proxy moderators, across explanation
approaches (saliency map and counterfactual explanation). We find that saliency
maps generally perform better and show less evidence of disparate impact
(group) and individual unfairness than counterfactual explanations.
  Content warning: This paper contains examples of hate speech and racially
discriminatory language. The authors do not support such content. Please
consider your risk of discomfort carefully before continuing reading!",None,-1
de42b61b-2061-4eb2-bf36-feb19b59f6b7,Challenges in Domain-Specific Abstractive Summarization and How to Overcome them,0.568393,"Large Language Models work quite well with general-purpose data and many
tasks in Natural Language Processing. However, they show several limitations
when used for a task such as domain-specific abstractive text summarization.
This paper identifies three of those limitations as research problems in the
context of abstractive text summarization: 1) Quadratic complexity of
transformer-based models with respect to the input text length; 2) Model
Hallucination, which is a model's ability to generate factually incorrect text;
and 3) Domain Shift, which happens when the distribution of the model's
training and test corpus is not the same. Along with a discussion of the open
research questions, this paper also provides an assessment of existing
state-of-the-art techniques relevant to domain-specific text summarization to
address the research gaps.",None,-1
c68cb375-ef91-4985-a492-e12f594eacb2,ABAW : Facial Expression Recognition in the wild,0.520981,"The fifth Affective Behavior Analysis in-the-wild (ABAW) competition has
multiple challenges such as Valence-Arousal Estimation Challenge, Expression
Classification Challenge, Action Unit Detection Challenge, Emotional Reaction
Intensity Estimation Challenge. In this paper we have dealt only expression
classification challenge using multiple approaches such as fully supervised,
semi-supervised and noisy label approach. Our approach using noise aware model
has performed better than baseline model by 10.46% and semi supervised model
has performed better than baseline model by 9.38% and the fully supervised
model has performed better than the baseline by 9.34%",None,-1
d251f801-0c55-405d-8caa-cb63817386b4,MAP: Domain Generalization via Meta-Learning on Anatomy-Consistent Pseudo-Modalities,0.746767,"Deep models suffer from limited generalization capability to unseen domains,
which has severely hindered their clinical applicability. Specifically for the
retinal vessel segmentation task, although the model is supposed to learn the
anatomy of the target, it can be distracted by confounding factors like
intensity and contrast. We propose Meta learning on Anatomy-consistent
Pseudo-modalities (MAP), a method that improves model generalizability by
learning structural features. We first leverage a feature extraction network to
generate three distinct pseudo-modalities that share the vessel structure of
the original image. Next, we use the episodic learning paradigm by selecting
one of the pseudo-modalities as the meta-train dataset, and perform
meta-testing on a continuous augmented image space generated through Dirichlet
mixup of the remaining pseudo-modalities. Further, we introduce two loss
functions that facilitate the model's focus on shape information by clustering
the latent vectors obtained from images featuring identical vasculature. We
evaluate our model on seven public datasets of various retinal imaging
modalities and we conclude that MAP has substantially better generalizability.
Our code is publically available at https://github.com/DeweiHu/MAP.",None,-1
d8fd03b9-252e-482f-9164-8b4f7e5ab00a,Towards Diverse and Coherent Augmentation for Time-Series Forecasting,0.206409,"Time-series data augmentation mitigates the issue of insufficient training
data for deep learning models. Yet, existing augmentation methods are mainly
designed for classification, where class labels can be preserved even if
augmentation alters the temporal dynamics. We note that augmentation designed
for forecasting requires diversity as well as coherence with the original
temporal dynamics. As time-series data generated by real-life physical
processes exhibit characteristics in both the time and frequency domains, we
propose to combine Spectral and Time Augmentation (STAug) for generating more
diverse and coherent samples. Specifically, in the frequency domain, we use the
Empirical Mode Decomposition to decompose a time series and reassemble the
subcomponents with random weights. This way, we generate diverse samples while
being coherent with the original temporal relationships as they contain the
same set of base components. In the time domain, we adapt a mix-up strategy
that generates diverse as well as linearly in-between coherent samples.
Experiments on five real-world time-series datasets demonstrate that STAug
outperforms the base models without data augmentation as well as
state-of-the-art augmentation methods.",None,-1
ccfe53c5-a942-4669-beb9-161f14cd3abb,Hierarchical Semantic Tree Concept Whitening for Interpretable Image Classification,0.169916,"With the popularity of deep neural networks (DNNs), model interpretability is
becoming a critical concern. Many approaches have been developed to tackle the
problem through post-hoc analysis, such as explaining how predictions are made
or understanding the meaning of neurons in middle layers. Nevertheless, these
methods can only discover the patterns or rules that naturally exist in models.
In this work, rather than relying on post-hoc schemes, we proactively instill
knowledge to alter the representation of human-understandable concepts in
hidden layers. Specifically, we use a hierarchical tree of semantic concepts to
store the knowledge, which is leveraged to regularize the representations of
image data instances while training deep models. The axes of the latent space
are aligned with the semantic concepts, where the hierarchical relations
between concepts are also preserved. Experiments on real-world image datasets
show that our method improves model interpretability, showing better
disentanglement of semantic concepts, without negatively affecting model
classification performance.",None,-1
8acea8ca-a854-407c-a92f-f27761d0aa7d,QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing,0.607163,"Questions Under Discussion (QUD) is a versatile linguistic framework in which
discourse progresses as continuously asking questions and answering them.
Automatic parsing of a discourse to produce a QUD structure thus entails a
complex question generation task: given a document and an answer sentence,
generate a question that satisfies linguistic constraints of QUD and can be
grounded in an anchor sentence in prior context. These questions are known to
be curiosity-driven and open-ended. This work introduces the first framework
for the automatic evaluation of QUD parsing, instantiating the theoretical
constraints of QUD in a concrete protocol. We present QUDeval, a dataset of
fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned
systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD
is still challenging for modern LLMs, and that existing evaluation metrics
poorly approximate parser quality. Encouragingly, human-authored QUDs are
scored highly by our human evaluators, suggesting that there is headroom for
further progress on language modeling to improve both QUD parsing and QUD
evaluation.",None,-1
dab3c17b-5ac2-49ec-9665-310a4a2a77d8,Introducing Tales of Tribute AI Competition,0.719569,"This paper presents a new AI challenge, the Tales of Tribute AI Competition
(TOTAIC), based on a two-player deck-building card game released with the High
Isle chapter of The Elder Scrolls Online. Currently, there is no other AI
competition covering Collectible Card Games (CCG) genre, and there has never
been one that targets a deck-building game. Thus, apart from usual CCG-related
obstacles to overcome, like randomness, hidden information, and large branching
factor, the successful approach additionally requires long-term planning and
versatility. The game can be tackled with multiple approaches, including
classic adversarial search, single-player planning, and Neural Networks-based
algorithms. This paper introduces the competition framework, describes the
rules of the game, and presents the results of a tournament between sample AI
agents.",None,-1
5ee2b8cb-da9a-4d98-870f-151699d6c65f,MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System,0.865593,"Multi-modal sarcasm detection has attracted much recent attention.
Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder
the development of reliable multi-modal sarcasm detection system: (1) There are
some spurious cues in MMSD, leading to the model bias learning; (2) The
negative samples in MMSD are not always reasonable. To solve the aforementioned
issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings
of MMSD, by removing the spurious cues and re-annotating the unreasonable
samples. Meanwhile, we present a novel framework called multi-view CLIP that is
capable of leveraging multi-grained cues from multiple perspectives (i.e.,
text, image, and text-image interaction view) for multi-modal sarcasm
detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for
building reliable multi-modal sarcasm detection systems and multi-view CLIP can
significantly outperform the previous best baselines.",None,-1
38bf6e4d-f8d3-4b9c-af2a-d246590e359e,The ACL OCL Corpus: Advancing Open Science in Computational Linguistics,0.340051,"We present ACL OCL, a scholarly corpus derived from the ACL Anthology to
assist Open scientific research in the Computational Linguistics domain.
Integrating and enhancing the previous versions of the ACL Anthology, the ACL
OCL contributes metadata, PDF files, citation graphs and additional structured
full texts with sections, figures, and links to a large knowledge resource
(Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers,
alongside 210K figures.
  We spotlight how ACL OCL applies to observe trends in computational
linguistics. By detecting paper topics with a supervised neural model, we note
that interest in ""Syntax: Tagging, Chunking and Parsing"" is waning and ""Natural
Language Generation"" is resurging. Our dataset is available from HuggingFace
(https://huggingface.co/datasets/WINGNUS/ACL-OCL).",None,-1
8c9fd616-8d98-4173-831a-892f9e001bcb,Continuous Versatile Jumping Using Learned Action Residuals,0.411995,"Jumping is essential for legged robots to traverse through difficult
terrains. In this work, we propose a hierarchical framework that combines
optimal control and reinforcement learning to learn continuous jumping motions
for quadrupedal robots. The core of our framework is a stance controller, which
combines a manually designed acceleration controller with a learned residual
policy. As the acceleration controller warm starts policy for efficient
training, the trained policy overcomes the limitation of the acceleration
controller and improves the jumping stability. In addition, a low-level
whole-body controller converts the body pose command from the stance controller
to motor commands. After training in simulation, our framework can be deployed
directly to the real robot, and perform versatile, continuous jumping motions,
including omni-directional jumps at up to 50cm high, 60cm forward, and
jump-turning at up to 90 degrees. Please visit our website for more results:
https://sites.google.com/view/learning-to-jump.",None,-1
a176f869-f8f3-4b61-a6d4-7b4d372c45d2,"EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification",0.255547,"Automatic multi-hop fact verification task has gained significant attention
in recent years. Despite impressive results, these well-designed models perform
poorly on out-of-domain data. One possible solution is to augment the training
data with counterfactuals, which are generated by minimally altering the causal
features of the original data. However, current counterfactual data
augmentation techniques fail to handle multi-hop fact verification due to their
incapability to preserve the complex logical relationships within multiple
correlated texts. In this paper, we overcome this limitation by developing a
rationale-sensitive method to generate linguistically diverse and
label-flipping counterfactuals while preserving logical relationships. In
specific, the diverse and fluent counterfactuals are generated via an
Explain-Edit-Generate architecture. Moreover, the checking and filtering
modules are proposed to regularize the counterfactual data with logical
relations and flipped labels. Experimental results show that the proposed
approach outperforms the SOTA baselines and can generate linguistically diverse
counterfactual data without disrupting their logical relationships.",None,-1
39658e96-8f25-4191-b54f-93b3a98bee26,San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants,0.22203,"In this work, we develop language models for the Sanskrit language, namely
Bidirectional Encoder Representations from Transformers (BERT) and its
variants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using
Devanagari Sanskrit text corpus. Then we extracted the features for the given
text from these models. We applied the dimensional reduction and clustering
techniques on the features to generate an extractive summary for a given
Sanskrit document. Along with the extractive text summarization techniques, we
have also created and released a Sanskrit Devanagari text corpus publicly.",None,-1
4a24f2b2-13db-4a57-a435-27613e83ebed,Improving Language Models via Plug-and-Play Retrieval Feedback,0.725941,"Large language models (LLMs) exhibit remarkable performance across various
NLP tasks. However, they often generate incorrect or hallucinated information,
which hinders their practical applicability in real-world scenarios. Human
feedback has been shown to effectively enhance the factuality and quality of
generated content, addressing some of these limitations. However, this approach
is resource-intensive, involving manual input and supervision, which can be
time-consuming and expensive. Moreover, it cannot be provided during inference,
further limiting its practical utility in dynamic and interactive applications.
In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs
by providing automatic retrieval feedback in a plug-and-play framework without
the need for expensive fine-tuning. ReFeed first generates initial outputs,
then utilizes a retrieval model to acquire relevant information from large
document collections, and finally incorporates the retrieved information into
the in-context demonstration for output refinement, thereby addressing the
limitations of LLMs in a more efficient and cost-effective manner. Experiments
on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed
could improve over +6.0% under zero-shot setting and +2.5% under few-shot
setting, compared to baselines without using retrieval feedback.",None,-1
383b7e35-7a1f-4b00-afa3-c4ca7ce4d155,Causal Structure Learning Supervised by Large Language Model,0.883309,"Causal discovery from observational data is pivotal for deciphering complex
relationships. Causal Structure Learning (CSL), which focuses on deriving
causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast
DAG spaces and data sparsity. The integration of Large Language Models (LLMs),
recognized for their causal reasoning capabilities, offers a promising
direction to enhance CSL by infusing it with knowledge-based causal inferences.
However, existing approaches utilizing LLMs for CSL have encountered issues,
including unreliable constraints from imperfect LLM inferences and the
computational intensity of full pairwise variable analyses. In response, we
introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL
innovatively integrates LLM-based causal inference with CSL in an iterative
process, refining the causal DAG using feedback from LLMs. This method not only
utilizes LLM resources more efficiently but also generates more robust and
high-quality structural constraints compared to previous methodologies. Our
comprehensive evaluation across eight real-world datasets demonstrates
ILS-CSL's superior performance, setting a new standard in CSL efficacy and
showcasing its potential to significantly advance the field of causal
discovery. The codes are available at
\url{https://github.com/tyMadara/ILS-CSL}.",None,-1
0e0b41f2-da1a-45be-b216-211ce33452b7,How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives,0.499647,"Recently, various intermediate layer distillation (ILD) objectives have been
shown to improve compression of BERT models via Knowledge Distillation (KD).
However, a comprehensive evaluation of the objectives in both task-specific and
task-agnostic settings is lacking. To the best of our knowledge, this is the
first work comprehensively evaluating distillation objectives in both settings.
We show that attention transfer gives the best performance overall. We also
study the impact of layer choice when initializing the student from the teacher
layers, finding a significant impact on the performance in task-specific
distillation. For vanilla KD and hidden states transfer, initialisation with
lower layers of the teacher gives a considerable improvement over higher
layers, especially on the task of QNLI (up to an absolute percentage change of
17.8 in accuracy). Attention transfer behaves consistently under different
initialisation settings. We release our code as an efficient transformer-based
model distillation framework for further studies.",None,-1
e4b78444-9f90-4fe6-9ed0-f4d7f3a886cc,Privacy Aware Question-Answering System for Online Mental Health Risk Assessment,0.303483,"Social media platforms have enabled individuals suffering from mental
illnesses to share their lived experiences and find the online support
necessary to cope. However, many users fail to receive genuine clinical
support, thus exacerbating their symptoms. Screening users based on what they
post online can aid providers in administering targeted healthcare and minimize
false positives. Pre-trained Language Models (LMs) can assess users' social
media data and classify them in terms of their mental health risk. We propose a
Question-Answering (QA) approach to assess mental health risk using the
Unified-QA model on two large mental health datasets. To protect user data, we
extend Unified-QA by anonymizing the model training process using differential
privacy. Our results demonstrate the effectiveness of modeling risk assessment
as a QA task, specifically for mental health use cases. Furthermore, the
model's performance decreases by less than 1% with the inclusion of
differential privacy. The proposed system's performance is indicative of a
promising research direction that will lead to the development of privacy-aware
diagnostic systems.",None,-1
562269a2-096e-4f6f-a209-aff373bf7c90,Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain,0.213652,"We propose a novel approach to learn domain-specific plausible materials for
components in the vehicle repair domain by probing Pretrained Language Models
(PLMs) in a cloze task style setting to overcome the lack of annotated
datasets. We devise a new method to aggregate salient predictions from a set of
cloze query templates and show that domain-adaptation using either a small,
high-quality or a customized Wikipedia corpus boosts performance. When
exploring resource-lean alternatives, we find a distilled PLM clearly
outperforming a classic pattern-based algorithm. Further, given that 98% of our
domain-specific components are multiword expressions, we successfully exploit
the compositionality assumption as a way to address data sparsity.",None,-1
ad8ff5cf-53e6-49ee-a30b-e3e5b2b4cdaa,TEDB System Description to a Shared Task on Euphemism Detection 2022,0.0729538,"In this report, we describe our Transformers for euphemism detection baseline
(TEDB) submissions to a shared task on euphemism detection 2022. We cast the
task of predicting euphemism as text classification. We considered
Transformer-based models which are the current state-of-the-art methods for
text classification. We explored different training schemes, pretrained models,
and model architectures. Our best result of 0.816 F1-score (0.818 precision and
0.814 recall) consists of a euphemism-detection-finetuned
TweetEval/TimeLMs-pretrained RoBERTa model as a feature extractor frontend with
a KimCNN classifier backend trained end-to-end using a cosine annealing
scheduler. We observed pretrained models on sentiment analysis and
offensiveness detection to correlate with more F1-score while pretraining on
other tasks, such as sarcasm detection, produces less F1-scores. Also, putting
more word vector channels does not improve the performance in our experiments.",None,-1
55a2e85e-3028-4008-96ba-26fbe09f650c,Road Extraction with Satellite Images and Partial Road Maps,0.477884,"Road extraction is a process of automatically generating road maps mainly
from satellite images. Existing models all target to generate roads from the
scratch despite that a large quantity of road maps, though incomplete, are
publicly available (e.g. those from OpenStreetMap) and can help with road
extraction. In this paper, we propose to conduct road extraction based on
satellite images and partial road maps, which is new. We then propose a
two-branch Partial to Complete Network (P2CNet) for the task, which has two
prominent components: Gated Self-Attention Module (GSAM) and Missing Part (MP)
loss. GSAM leverages a channel-wise self-attention module and a gate module to
capture long-range semantics, filter out useless information, and better fuse
the features from two branches. MP loss is derived from the partial road maps,
trying to give more attention to the road pixels that do not exist in partial
road maps. Extensive experiments are conducted to demonstrate the effectiveness
of our model, e.g. P2CNet achieves state-of-the-art performance with the IoU
scores of 70.71% and 75.52%, respectively, on the SpaceNet and OSM datasets.",None,-1
2938c65d-906b-49dd-9306-5667f832a999,Source-Aware Embedding Training on Heterogeneous Information Networks,0.168595,"Heterogeneous information networks (HINs) have been extensively applied to
real-world tasks, such as recommendation systems, social networks, and citation
networks. While existing HIN representation learning methods can effectively
learn the semantic and structural features in the network, little awareness was
given to the distribution discrepancy of subgraphs within a single HIN.
However, we find that ignoring such distribution discrepancy among subgraphs
from multiple sources would hinder the effectiveness of graph embedding
learning algorithms. This motivates us to propose SUMSHINE (Scalable
Unsupervised Multi-Source Heterogeneous Information Network Embedding) -- a
scalable unsupervised framework to align the embedding distributions among
multiple sources of an HIN. Experimental results on real-world datasets in a
variety of downstream tasks validate the performance of our method over the
state-of-the-art heterogeneous information network embedding algorithms.",None,-1
cee00b2d-e747-4eb4-997a-7ddc991a7900,Neural Episodic Control with State Abstraction,0.346652,"Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample
inefficiency. Generally, episodic control-based approaches are solutions that
leverage highly-rewarded past experiences to improve sample efficiency of DRL
algorithms. However, previous episodic control-based approaches fail to utilize
the latent information from the historical behaviors (e.g., state transitions,
topological similarities, etc.) and lack scalability during DRL training. This
work introduces Neural Episodic Control with State Abstraction (NECSA), a
simple but effective state abstraction-based episodic control containing a more
comprehensive episodic memory, a novel state evaluation, and a multi-step state
analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym
domains. The experimental results indicate that NECSA achieves higher sample
efficiency than the state-of-the-art episodic control-based approaches. Our
data and code are available at the project
website\footnote{\url{https://sites.google.com/view/drl-necsa}}.",None,-1
4f800a83-e438-40d3-99ce-bd13e0745a48,Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport,0.508184,"Answering complex queries on knowledge graphs is important but particularly
challenging because of the data incompleteness. Query embedding methods address
this issue by learning-based models and simulating logical reasoning with set
operators. Previous works focus on specific forms of embeddings, but scoring
functions between embeddings are underexplored. In contrast to existing scoring
functions motivated by local comparison or global transport, this work
investigates the local and global trade-off with unbalanced optimal transport
theory. Specifically, we embed sets as bounded measures in $\real$ endowed with
a scoring function motivated by the Wasserstein-Fisher-Rao metric. Such a
design also facilitates closed-form set operators in the embedding space.
Moreover, we introduce a convolution-based algorithm for linear time
computation and a block-diagonal kernel to enforce the trade-off. Results show
that WFRE can outperform existing query embedding methods on standard datasets,
evaluation sets with combinatorially complex queries, and hierarchical
knowledge graphs. Ablation study shows that finding a better local and global
trade-off is essential for performance improvement.",None,-1
6226d20c-a926-4797-ac30-9dc2628a7b03,TEQ: Trainable Equivalent Transformation for Quantization of LLMs,0.142942,"As large language models (LLMs) become more prevalent, there is a growing
need for new and improved quantization methods that can meet the
computationalast layer demands of these modern architectures while maintaining
the accuracy. In this paper, we present TEQ, a trainable equivalent
transformation that preserves the FP32 precision of the model output while
taking advantage of low-precision quantization, especially 3 and 4 bits
weight-only quantization. The training process is lightweight, requiring only
1K steps and fewer than 0.1 percent of the original model's trainable
parameters. Furthermore, the transformation does not add any computational
overhead during inference. Our results are on-par with the state-of-the-art
(SOTA) methods on typical LLMs. Our approach can be combined with other methods
to achieve even better performance. The code is available at
https://github.com/intel/neural-compressor.",None,-1
03933926-306b-49a1-a85d-f1e60cc845b5,OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting,0.767675,"Seasonal-trend decomposition is one of the most fundamental concepts in time
series analysis that supports various downstream tasks, including time series
anomaly detection and forecasting. However, existing decomposition methods rely
on batch processing with a time complexity of O(W), where W is the number of
data points within a time window. Therefore, they cannot always efficiently
support real-time analysis that demands low processing delay. To address this
challenge, we propose OneShotSTL, an efficient and accurate algorithm that can
decompose time series online with an update time complexity of O(1). OneShotSTL
is more than $1,000$ times faster than the batch methods, with accuracy
comparable to the best counterparts. Extensive experiments on real-world
benchmark datasets for downstream time series anomaly detection and forecasting
tasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster than
the state-of-the-art methods, while still providing comparable or even better
accuracy.",None,-1
440b3b18-6b7b-4183-afcc-b7a97d9552a7,Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models,0.34148,"Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.",None,-1
4bf660a3-079a-477e-864b-520625f23442,Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis,1.0,"We present a method that simultaneously addresses the tasks of dynamic scene
novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense
scene elements. We follow an analysis-by-synthesis framework, inspired by
recent work that models scenes as a collection of 3D Gaussians which are
optimized to reconstruct input images via differentiable rendering. To model
dynamic scenes, we allow Gaussians to move and rotate over time while enforcing
that they have persistent color, opacity, and size. By regularizing Gaussians'
motion and rotation with local-rigidity constraints, we show that our Dynamic
3D Gaussians correctly model the same area of physical space over time,
including the rotation of that space. Dense 6-DOF tracking and dynamic
reconstruction emerges naturally from persistent dynamic view synthesis,
without requiring any correspondence or flow as input. We demonstrate a large
number of downstream applications enabled by our representation, including
first-person view synthesis, dynamic compositional scene synthesis, and 4D
video editing.",None,-1
3fe9a8c2-7df5-4e59-a3a2-5c9a63503f82,Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task,0.850962,"Large-scale language models (LLMs) has shown remarkable capability in various
of Natural Language Processing (NLP) tasks and attracted lots of attention
recently. However, some studies indicated that large language models fail to
achieve promising result beyond the state-of-the-art models in English
grammatical error correction (GEC) tasks. In this report, we aim to explore the
how large language models perform on Chinese grammatical error correction tasks
and provide guidance for future work. We conduct experiments with 3 different
LLMs of different model scale on 4 Chinese GEC dataset. Our experimental
results indicate that the performances of LLMs on automatic evaluation metrics
falls short of the previous sota models because of the problem of
over-correction. Furthermore, we also discover notable variations in the
performance of LLMs when evaluated on different data distributions. Our
findings demonstrates that further investigation is required for the
application of LLMs on Chinese GEC task.",None,-1
1f5a976c-e524-42bf-8c2e-ec754a7615cb,Uncertainty-Aware AB3DMOT by Variational 3D Object Detection,0.112084,"Autonomous driving needs to rely on high-quality 3D object detection to
ensure safe navigation in the world. Uncertainty estimation is an effective
tool to provide statistically accurate predictions, while the associated
detection uncertainty can be used to implement a more safe navigation protocol
or include the user in the loop. In this paper, we propose a Variational Neural
Network-based TANet 3D object detector to generate 3D object detections with
uncertainty and introduce these detections to an uncertainty-aware AB3DMOT
tracker. This is done by applying a linear transformation to the estimated
uncertainty matrix, which is subsequently used as a measurement noise for the
adopted Kalman filter. We implement two ways to estimate output uncertainty,
i.e., internally, by computing the variance of the CNN outputs and then
propagating the uncertainty through the post-processing, and externally, by
associating the final predictions of different samples and computing the
covariance of each predicted box. In experiments, we show that the external
uncertainty estimation leads to better results, outperforming both internal
uncertainty estimation and classical tracking approaches. Furthermore, we
propose a method to initialize the Variational 3D object detector with a
pretrained TANet model, which leads to the best performing models.",None,-1
352120a6-cf9d-44ee-8240-af3c6cdd3f14,R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces,0.264442,"This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific
self-supervision method for speaker and noise-invariant speech representations
by learning discrete acoustic units with speaker-invariant clustering (Spin).
R-Spin resolves Spin's issues and enhances content representations by learning
to predict acoustic pieces. R-Spin offers a 12X reduction in computational
resources compared to previous state-of-the-art methods while outperforming
them in severely distorted speech scenarios. This paper provides detailed
analyses to show how discrete units contribute to speech encoder training and
improving robustness in diverse acoustic environments.",None,-1
76f974e2-ba6f-445f-9786-622c6ce4e4a2,Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech,0.230109,"Modern speech synthesis systems have improved significantly, with synthetic
speech being indistinguishable from real speech. However, efficient and
holistic evaluation of synthetic speech still remains a significant challenge.
Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due
to high costs. Therefore, researchers have developed auxiliary automatic
metrics like Word Error Rate (WER) to measure intelligibility. Prior works
focus on evaluating synthetic speech based on pre-trained speech recognition
models, however, this can be limiting since this approach primarily measures
speech intelligibility. In this paper, we propose an evaluation technique
involving the training of an ASR model on synthetic speech and assessing its
performance on real speech. Our main assumption is that by training the ASR
model on the synthetic speech, the WER on real speech reflects the similarity
between distributions, a broader assessment of synthetic speech quality beyond
intelligibility. Our proposed metric demonstrates a strong correlation with
both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and
MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and
YourTTS.",None,-1
8a01fabf-d557-4df0-9079-8fdf0c936656,An Experiment in Retrofitting Competency Questions for Existing Ontologies,0.612317,"Competency Questions (CQs) are a form of ontology functional requirements
expressed as natural language questions. Inspecting CQs together with the
axioms in an ontology provides critical insights into the intended scope and
applicability of the ontology. CQs also underpin a number of tasks in the
development of ontologies e.g. ontology reuse, ontology testing, requirement
specification, and the definition of patterns that implement such requirements.
Although CQs are integral to the majority of ontology engineering
methodologies, the practice of publishing CQs alongside the ontological
artefacts is not widely observed by the community. In this context, we present
an experiment in retrofitting CQs from existing ontologies. We propose
RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using
Generative AI. In the paper we present the pipeline that facilitates the
extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its
application to a number of existing ontologies.",None,-1
2596b4f9-b18c-4715-94bb-797d37c2d8d4,Formally Explaining Neural Networks within Reactive Systems,0.761083,"Deep neural networks (DNNs) are increasingly being used as controllers in
reactive systems. However, DNNs are highly opaque, which renders it difficult
to explain and justify their actions. To mitigate this issue, there has been a
surge of interest in explainable AI (XAI) techniques, capable of pinpointing
the input features that caused the DNN to act as it did. Existing XAI
techniques typically face two limitations: (i) they are heuristic, and do not
provide formal guarantees that the explanations are correct; and (ii) they
often apply to ``one-shot'' systems, where the DNN is invoked independently of
past invocations, as opposed to reactive systems. Here, we begin bridging this
gap, and propose a formal DNN-verification-based XAI technique for reasoning
about multi-step, reactive systems. We suggest methods for efficiently
calculating succinct explanations, by exploiting the system's transition
constraints in order to curtail the search space explored by the underlying
verifier. We evaluate our approach on two popular benchmarks from the domain of
automated navigation; and observe that our methods allow the efficient
computation of minimal and minimum explanations, significantly outperforming
the state of the art. We also demonstrate that our methods produce formal
explanations that are more reliable than competing, non-verification-based XAI
techniques.",None,-1
20ad6c83-b0af-4067-b41d-5d93a005d57a,ClusterLLM: Large Language Models as a Guide for Text Clustering,0.820056,"We introduce ClusterLLM, a novel text clustering framework that leverages
feedback from an instruction-tuned large language model, such as ChatGPT.
Compared with traditional unsupervised methods that builds upon ""small""
embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the
emergent capability of LLM even if its embeddings are inaccessible; and (2) it
understands the user's preference on clustering through textual instruction
and/or a few annotated data. First, we prompt ChatGPT for insights on
clustering perspective by constructing hard triplet questions <does A better
correspond to B than C>, where A, B and C are similar data points that belong
to different clusters according to small embedder. We empirically show that
this strategy is both effective for fine-tuning small embedder and
cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on
clustering granularity by carefully designed pairwise questions <do A and B
belong to the same category>, and tune the granularity from cluster hierarchies
that is the most consistent with the ChatGPT answers. Extensive experiments on
14 datasets show that ClusterLLM consistently improves clustering quality, at
an average cost of ~$0.6 per dataset. The code will be available at
https://github.com/zhang-yu-wei/ClusterLLM.",None,-1
ce30cc5f-3e95-4a6f-b249-ad9080590435,BEVScope: Enhancing Self-Supervised Depth Estimation Leveraging Bird's-Eye-View in Dynamic Scenarios,0.431551,"Depth estimation is a cornerstone of perception in autonomous driving and
robotic systems. The considerable cost and relatively sparse data acquisition
of LiDAR systems have led to the exploration of cost-effective alternatives,
notably, self-supervised depth estimation. Nevertheless, current
self-supervised depth estimation methods grapple with several limitations: (1)
the failure to adequately leverage informative multi-camera views. (2) the
limited capacity to handle dynamic objects effectively. To address these
challenges, we present BEVScope, an innovative approach to self-supervised
depth estimation that harnesses Bird's-Eye-View (BEV) features. Concurrently,
we propose an adaptive loss function, specifically designed to mitigate the
complexities associated with moving objects. Empirical evaluations conducted on
the Nuscenes dataset validate our approach, demonstrating competitive
performance. Code will be released at https://github.com/myc634/BEVScope.",None,-1
1d5fea85-2c71-4d15-bca0-d5ee24931464,Efficiency 360: Efficient Vision Transformers,0.150911,"Transformers are widely used for solving tasks in natural language
processing, computer vision, speech, and music domains. In this paper, we talk
about the efficiency of transformers in terms of memory (the number of
parameters), computation cost (number of floating points operations), and
performance of models, including accuracy, the robustness of the model, and
fair \& bias-free features. We mainly discuss the vision transformer for the
image classification task. Our contribution is to introduce an efficient 360
framework, which includes various aspects of the vision transformer, to make it
more efficient for industrial applications. By considering those applications,
we categorize them into multiple dimensions such as privacy, robustness,
transparency, fairness, inclusiveness, continual learning, probabilistic
models, approximation, computational complexity, and spectral complexity. We
compare various vision transformer models based on their performance, the
number of parameters, and the number of floating point operations (FLOPs) on
multiple datasets.",None,-1
1d18d8a1-f40a-472f-86f2-d3b1cf79d48e,Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,0.251534,"Recent work in Natural Language Processing and Computer Vision has been using
textual information -- e.g., entity names and descriptions -- available in
knowledge graphs to ground neural models to high-quality structured data.
However, when it comes to non-English languages, the quantity and quality of
textual information are comparatively scarce. To address this issue, we
introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and
perform a thorough investigation on bridging the gap in both the quantity and
quality of textual information between English and non-English languages. More
specifically, we: i) bring to light the problem of increasing multilingual
coverage and precision of entity names and descriptions in Wikidata; ii)
demonstrate that state-of-the-art methods, namely, Machine Translation (MT),
Web Search (WS), and Large Language Models (LLMs), struggle with this task;
iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and
LLMs to generate high-quality textual information; and, iv) study the impact of
increasing multilingual coverage and precision of non-English textual
information in Entity Linking, Knowledge Graph Completion, and Question
Answering. As part of our effort towards better multilingual knowledge graphs,
we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE
approaches in 10 languages across 7 language families.",None,-1
dc8673a5-2ef0-4b6b-bec1-000a952f2dd2,UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text,0.222669,"This demo paper presents UnScientify, an interactive system designed to
detect scientific uncertainty in scholarly full text. The system utilizes a
weakly supervised technique that employs a fine-grained annotation scheme to
identify verbally formulated uncertainty at the sentence level in scientific
texts. The pipeline for the system includes a combination of pattern matching,
complex sentence checking, and authorial reference checking. Our approach
automates labeling and annotation tasks for scientific uncertainty
identification, taking into account different types of scientific uncertainty,
that can serve various applications such as information retrieval, text mining,
and scholarly document processing. Additionally, UnScientify provides
interpretable results, aiding in the comprehension of identified instances of
scientific uncertainty in text.",None,-1
81d4d4bd-8037-4708-9df9-da9c0d9829af,SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings,0.798104,"Contextual spelling correction models are an alternative to shallow fusion to
improve automatic speech recognition (ASR) quality given user vocabulary. To
deal with large user vocabularies, most of these models include candidate
retrieval mechanisms, usually based on minimum edit distance between fragments
of ASR hypothesis and user phrases. However, the edit-distance approach is
slow, non-trainable, and may have low recall as it relies only on common
letters. We propose: 1) a novel algorithm for candidate retrieval, based on
misspelled n-gram mappings, which gives up to 90% recall with just the top 10
candidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on
BERT architecture, where the initial transcript and ten candidates are combined
into one input. The experiments on Spoken Wikipedia show 21.4% word error rate
improvement compared to a baseline ASR system.",None,-1
a39d283b-641a-4c58-9b05-9b380a543b12,Hypernetworks build Implicit Neural Representations of Sounds,0.384282,"Implicit Neural Representations (INRs) are nowadays used to represent
multimedia signals across various real-life applications, including image
super-resolution, image compression, or 3D rendering. Existing methods that
leverage INRs are predominantly focused on visual data, as their application to
other modalities, such as audio, is nontrivial due to the inductive biases
present in architectural attributes of image-based INR models. To address this
limitation, we introduce HyperSound, the first meta-learning approach to
produce INRs for audio samples that leverages hypernetworks to generalize
beyond samples observed in training. Our approach reconstructs audio samples
with quality comparable to other state-of-the-art models and provides a viable
alternative to contemporary sound representations used in deep neural networks
for audio processing, such as spectrograms.",None,-1
7dcb517c-122b-427c-877e-c15c9af6fd7c,Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection,0.200003,"Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and
Google's PaLM2, have revolutionized the field of artificial intelligence. A
notable paradigm shift has been the advent of the Segment Anything Model (SAM),
which has exhibited a remarkable capability to segment real-world objects,
trained on 1 billion masks and 11 million images. Although SAM excels in
general object segmentation, it lacks the intrinsic ability to detect salient
objects, resulting in suboptimal performance in this domain. To address this
challenge, we present the Segment Salient Object Model (SSOM), an innovative
approach that adaptively fine-tunes SAM for salient object detection by
harnessing the low-rank structure inherent in deep learning. Comprehensive
qualitative and quantitative evaluations across five challenging RGB benchmark
datasets demonstrate the superior performance of our approach, surpassing
state-of-the-art methods.",None,-1
a1f9d476-9b0a-4191-b932-17298feb53ba,Tracr: Compiled Transformers as a Laboratory for Interpretability,0.746284,"We show how to ""compile"" human-readable programs into standard decoder-only
transformer models. Our compiler, Tracr, generates models with known structure.
This structure can be used to design experiments. For example, we use it to
study ""superposition"" in transformers that execute multi-step algorithms.
Additionally, the known structure of Tracr-compiled models can serve as
ground-truth for evaluating interpretability methods. Commonly, because the
""programs"" learned by transformers are unknown it is unclear whether an
interpretation succeeded. We demonstrate our approach by implementing and
examining programs including computing token frequencies, sorting, and
parenthesis checking. We provide an open-source implementation of Tracr at
https://github.com/google-deepmind/tracr.",None,-1
a62f2d49-7c2d-4f6a-8098-23b49f12eea6,"RGB-D-Based Categorical Object Pose and Shape Estimation: Methods, Datasets, and Evaluation",0.341006,"Recently, various methods for 6D pose and shape estimation of objects at a
per-category level have been proposed. This work provides an overview of the
field in terms of methods, datasets, and evaluation protocols. First, an
overview of existing works and their commonalities and differences is provided.
Second, we take a critical look at the predominant evaluation protocol,
including metrics and datasets. Based on the findings, we propose a new set of
metrics, contribute new annotations for the Redwood dataset, and evaluate
state-of-the-art methods in a fair comparison. The results indicate that
existing methods do not generalize well to unconstrained orientations and are
actually heavily biased towards objects being upright. We provide an
easy-to-use evaluation toolbox with well-defined metrics, methods, and dataset
interfaces, which allows evaluation and comparison with various
state-of-the-art approaches
(https://github.com/roym899/pose_and_shape_evaluation).",None,-1
9663ee05-e78e-4cf6-90de-425d0df5add7,Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue,0.981205,"Goal-directed dialogue systems aim to proactively reach a pre-determined
target through multi-turn conversations. The key to achieving this task lies in
planning dialogue paths that smoothly and coherently direct conversations
towards the target. However, this is a challenging and under-explored task. In
this work, we propose a coherent dialogue planning approach that uses a
stochastic process to model the temporal dynamics of dialogue paths. We define
a latent space that captures the coherence of goal-directed behavior using a
Brownian bridge process, which allows us to incorporate user feedback flexibly
in dialogue planning. Based on the derived latent trajectories, we generate
dialogue paths explicitly using pre-trained language models. We finally employ
these paths as natural language prompts to guide dialogue generation. Our
experiments show that our approach generates more coherent utterances and
achieves the goal with a higher success rate.",None,-1
4dd07f57-4d48-4c7f-8288-2aba3afdba6b,End-to-end Manipulator Calligraphy Planning via Variational Imitation Learning,0.267051,"Planning from demonstrations has shown promising results with the advances of
deep neural networks. One of the most popular real-world applications is
automated handwriting using a robotic manipulator. Classically it is simplified
as a two-dimension problem. This representation is suitable for elementary
drawings, but it is not sufficient for Japanese calligraphy or complex work of
art where the orientation of a pen is part of the user expression. In this
study, we focus on automated planning of Japanese calligraphy using a
three-dimension representation of the trajectory as well as the rotation of the
pen tip, and propose a novel deep imitation learning neural network that learns
from expert demonstrations through a combination of images and pose data. The
network consists of a combination of variational auto-encoder, bi-directional
LSTM, and Multi-Layer Perceptron (MLP). Experiments are conducted in a
progressive way, and results demonstrate that the proposed approach is
successful in completion of tasks for real-world robots, overcoming the
distribution shift problem in imitation learning. The source code and dataset
will be public.",None,-1
26bddf4d-999b-4b22-93c6-be7eaa42a702,Hierarchical and Contrastive Representation Learning for Knowledge-aware Recommendation,0.0959563,"Incorporating knowledge graph into recommendation is an effective way to
alleviate data sparsity. Most existing knowledge-aware methods usually perform
recursive embedding propagation by enumerating graph neighbors. However, the
number of nodes' neighbors grows exponentially as the hop number increases,
forcing the nodes to be aware of vast neighbors under this recursive
propagation for distilling the high-order semantic relatedness. This may induce
more harmful noise than useful information into recommendation, leading the
learned node representations to be indistinguishable from each other, that is,
the well-known over-smoothing issue. To relieve this issue, we propose a
Hierarchical and CONtrastive representation learning framework for
knowledge-aware recommendation named HiCON. Specifically, for avoiding the
exponential expansion of neighbors, we propose a hierarchical message
aggregation mechanism to interact separately with low-order neighbors and
meta-path-constrained high-order neighbors. Moreover, we also perform
cross-order contrastive learning to enforce the representations to be more
discriminative. Extensive experiments on three datasets show the remarkable
superiority of HiCON over state-of-the-art approaches.",None,-1
6260d486-cf22-4565-8a57-8b3bfed1256b,The Effect of Information Type on Human Cognitive Augmentation,0.145252,"When performing a task alone, humans achieve a certain level of performance.
When humans are assisted by a tool or automation to perform the same task,
performance is enhanced (augmented). Recently developed cognitive systems are
able to perform cognitive processing at or above the level of a human in some
domains. When humans work collaboratively with such cogs in a human/cog
ensemble, we expect augmentation of cognitive processing to be evident and
measurable. This paper shows the degree of cognitive augmentation depends on
the nature of the information the cog contributes to the ensemble. Results of
an experiment are reported showing conceptual information is the most effective
type of information resulting in increases in cognitive accuracy, cognitive
precision, and cognitive power.",None,-1
54e6f948-12ad-4b70-b4cc-a7131e75c05b,Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation,0.711215,"Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.",None,-1
101cf728-359d-4d9d-8613-aaa38cc55cc3,Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers,0.0817354,"This paper explores the effectiveness of model-generated signals in improving
zero-shot generalization of text-to-text Transformers such as T5. We study
various designs to pretrain T5 using an auxiliary model to construct more
challenging token replacements for the main model to denoise. Key aspects under
study include the decoding target, the location of the RTD head, and the
masking pattern. Based on these studies, we develop a new model, METRO-T0,
which is pretrained using the redesigned ELECTRA-Style pretraining strategies
and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all
similar-sized baselines on prompted NLP benchmarks, such as T0 Eval and MMLU,
and rivals the state-of-the-art T0-11B model with only 8% of its parameters.
Our analysis on model's neural activation and parameter sensitivity reveals
that the effectiveness of METRO-T0 stems from more balanced contribution of
parameters and better utilization of their capacity. The code and model
checkpoints are available at https://github.com/gonglinyuan/metro_t0.",None,-1
e17eb813-8abc-46c9-a54c-87f2d1afca1c,Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation,0.41364,"Causal reasoning ability is crucial for numerous NLP applications. Despite
the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear
how well ChatGPT performs in causal reasoning. In this paper, we conduct the
first comprehensive evaluation of the ChatGPT's causal reasoning capabilities.
Experiments show that ChatGPT is not a good causal reasoner, but a good causal
explainer. Besides, ChatGPT has a serious hallucination on causal reasoning,
possibly due to the reporting biases between causal and non-causal
relationships in natural language, as well as ChatGPT's upgrading processes,
such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT)
techniques can further exacerbate such causal hallucination. Additionally, the
causal reasoning ability of ChatGPT is sensitive to the words used to express
the causal concept in prompts, and close-ended prompts perform better than
open-ended prompts. For events in sentences, ChatGPT excels at capturing
explicit causality rather than implicit causality, and performs better in
sentences with lower event density and smaller lexical distance between events.
The code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .",None,-1
564f48b0-3e03-4046-bf4d-435942227500,Gloss-Free End-to-End Sign Language Translation,0.501553,"In this paper, we tackle the problem of sign language translation (SLT)
without gloss annotations. Although intermediate representation like gloss has
been proven effective, gloss annotations are hard to acquire, especially in
large quantities. This limits the domain coverage of translation datasets, thus
handicapping real-world applications. To mitigate this problem, we design the
Gloss-Free End-to-end sign language translation framework (GloFE). Our method
improves the performance of SLT in the gloss-free setting by exploiting the
shared underlying semantics of signs and the corresponding spoken translation.
Common concepts are extracted from the text and used as a weak form of
intermediate representation. The global embedding of these concepts is used as
a query for cross-attention to find the corresponding information within the
learned visual features. In a contrastive manner, we encourage the similarity
of query results between samples containing such concepts and decrease those
that do not. We obtained state-of-the-art results on large-scale datasets,
including OpenASL and How2Sign. The code and model will be available at
https://github.com/HenryLittle/GloFE.",None,-1
4ccfd000-fec0-4701-a052-a1c42e84ba3a,Text revision in Scientific Writing Assistance: An Overview,0.164946,"Writing a scientific article is a challenging task as it is a highly codified
genre. Good writing skills are essential to properly convey ideas and results
of research work. Since the majority of scientific articles are currently
written in English, this exercise is all the more difficult for non-native
English speakers as they additionally have to face language issues. This
article aims to provide an overview of text revision in writing assistance in
the scientific domain.
  We will examine the specificities of scientific writing, including the format
and conventions commonly used in research articles.
  Additionally, this overview will explore the various types of writing
assistance tools available for text revision. Despite the evolution of the
technology behind these tools through the years, from rule-based approaches to
deep neural-based ones, challenges still exist (tools' accessibility, limited
consideration of the context, inexplicit use of discursive information, etc.)",None,-1
8c3c328e-b233-4e4f-aa6c-5e9dbc33a17b,Refined Pseudo labeling for Source-free Domain Adaptive Object Detection,0.245094,"Domain adaptive object detection (DAOD) assumes that both labeled source data
and unlabeled target data are available for training, but this assumption does
not always hold in real-world scenarios. Thus, source-free DAOD is proposed to
adapt the source-trained detectors to target domains with only unlabeled target
data. Existing source-free DAOD methods typically utilize pseudo labeling,
where the performance heavily relies on the selection of confidence threshold.
However, most prior works adopt a single fixed threshold for all classes to
generate pseudo labels, which ignore the imbalanced class distribution,
resulting in biased pseudo labels. In this work, we propose a refined pseudo
labeling framework for source-free DAOD. First, to generate unbiased pseudo
labels, we present a category-aware adaptive threshold estimation module, which
adaptively provides the appropriate threshold for each category. Second, to
alleviate incorrect box regression, a localization-aware pseudo label
assignment strategy is introduced to divide labels into certain and uncertain
ones and optimize them separately. Finally, extensive experiments on four
adaptation tasks demonstrate the effectiveness of our method.",None,-1
5772ea3f-1484-445c-b791-973835991dbd,Safe POMDP Online Planning via Shielding,0.440463,"Partially observable Markov decision processes (POMDPs) have been widely used
in many robotic applications for sequential decision-making under uncertainty.
POMDP online planning algorithms such as Partially Observable Monte-Carlo
Planning (POMCP) can solve very large POMDPs with the goal of maximizing the
expected return. But the resulting policies cannot provide safety guarantees
which are imperative for real-world safety-critical tasks (e.g., autonomous
driving). In this work, we consider safety requirements represented as
almost-sure reach-avoid specifications (i.e., the probability to reach a set of
goal states is one and the probability to reach a set of unsafe states is
zero). We compute shields that restrict unsafe actions which would violate the
almost-sure reach-avoid specifications. We then integrate these shields into
the POMCP algorithm for safe POMDP online planning. We propose four distinct
shielding methods, differing in how the shields are computed and integrated,
including factored variants designed to improve scalability. Experimental
results on a set of benchmark domains demonstrate that the proposed shielding
methods successfully guarantee safety (unlike the baseline POMCP without
shielding) on large POMDPs, with negligible impact on the runtime for online
planning.",None,-1
1c301338-41a5-4a72-ae6a-387d22de37c7,The Importance of Time in Causal Algorithmic Recourse,0.332348,"The application of Algorithmic Recourse in decision-making is a promising
field that offers practical solutions to reverse unfavorable decisions.
However, the inability of these methods to consider potential dependencies
among variables poses a significant challenge due to the assumption of feature
independence. Recent advancements have incorporated knowledge of causal
dependencies, thereby enhancing the quality of the recommended recourse
actions. Despite these improvements, the inability to incorporate the temporal
dimension remains a significant limitation of these approaches. This is
particularly problematic as identifying and addressing the root causes of
undesired outcomes requires understanding time-dependent relationships between
variables. In this work, we motivate the need to integrate the temporal
dimension into causal algorithmic recourse methods to enhance recommendations'
plausibility and reliability. The experimental evaluation highlights the
significance of the role of time in this field.",None,-1
083cc60c-6a9c-450e-8fe9-6d441a6976ef,SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems,0.0618953,"Data imbalance in training data often leads to biased predictions from
trained models, which in turn causes ethical and social issues. A
straightforward solution is to carefully curate training data, but given the
enormous scale of modern neural networks, this is prohibitively labor-intensive
and thus impractical. Inspired by recent developments in generative models,
this paper explores the potential of synthetic data to address the data
imbalance problem. To be specific, our method, dubbed SYNAuG, leverages
synthetic data to equalize the unbalanced distribution of training data. Our
experiments demonstrate that, although a domain gap between real and synthetic
data exists, training with SYNAuG followed by fine-tuning with a few real
samples allows to achieve impressive performance on diverse tasks with
different data imbalance issues, surpassing existing task-specific methods for
the same purpose.",None,-1
737ef357-e6b3-4632-843b-0a5a38a40daa,Understanding Natural Language Understanding Systems. A Critical Analysis,0.275561,"The development of machines that {\guillemotleft}talk like
us{\guillemotright}, also known as Natural Language Understanding (NLU)
systems, is the Holy Grail of Artificial Intelligence (AI), since language is
the quintessence of human intelligence. The brief but intense life of NLU
research in AI and Natural Language Processing (NLP) is full of ups and downs,
with periods of high hopes that the Grail is finally within reach, typically
followed by phases of equally deep despair and disillusion. But never has the
trust that we can build {\guillemotleft}talking machines{\guillemotright} been
stronger than the one engendered by the last generation of NLU systems. But is
it gold all that glitters in AI? do state-of-the-art systems possess something
comparable to the human knowledge of language? Are we at the dawn of a new era,
in which the Grail is finally closer to us? In fact, the latest achievements of
AI systems have sparkled, or better renewed, an intense scientific debate on
their true language understanding capabilities. Some defend the idea that, yes,
we are on the right track, despite the limits that computational models still
show. Others are instead radically skeptic and even dismissal: The present
limits are not just contingent and temporary problems of NLU systems, but the
sign of the intrinsic inadequacy of the epistemological and technological
paradigm grounding them. This paper aims at contributing to such debate by
carrying out a critical analysis of the linguistic abilities of the most recent
NLU systems. I contend that they incorporate important aspects of the way
language is learnt and processed by humans, but at the same time they lack key
interpretive and inferential skills that it is unlikely they can attain unless
they are integrated with structured knowledge and the ability to exploit it for
language use.",None,-1
55a9bc3f-817d-40de-affa-08b06089839e,Sensemaking About Contraceptive Methods Across Online Platforms,0.0754004,"Selecting a birth control method is a complex healthcare decision. While
birth control methods provide important benefits, they can also cause
unpredictable side effects and be stigmatized, leading many people to seek
additional information online, where they can find reviews, advice, hypotheses,
and experiences of other birth control users. However, the relationships
between their healthcare concerns, sensemaking activities, and online settings
are not well understood. We gather texts about birth control shared on Twitter,
Reddit, and WebMD -- platforms with different affordances, moderation, and
audiences -- to study where and how birth control is discussed online. Using a
combination of topic modeling and hand annotation, we identify and characterize
the dominant sensemaking practices across these platforms, and we create
lexicons to draw comparisons across birth control methods and side effects. We
use these to measure variations from survey reports of side effect experiences
and method usage. Our findings characterize how online platforms are used to
make sense of difficult healthcare choices and highlight unmet needs of birth
control users.",None,-1
93e332f6-7389-44d9-9338-fc18d692ce88,Variation of Gender Biases in Visual Recognition Models Before and After Finetuning,0.073269,"We introduce a framework to measure how biases change before and after
fine-tuning a large scale visual recognition model for a downstream task. Deep
learning models trained on increasing amounts of data are known to encode
societal biases. Many computer vision systems today rely on models typically
pretrained on large scale datasets. While bias mitigation techniques have been
developed for tuning models for downstream tasks, it is currently unclear what
are the effects of biases already encoded in a pretrained model. Our framework
incorporates sets of canonical images representing individual and pairs of
concepts to highlight changes in biases for an array of off-the-shelf
pretrained models across model sizes, dataset sizes, and training objectives.
Through our analyses, we find that (1) supervised models trained on datasets
such as ImageNet-21k are more likely to retain their pretraining biases
regardless of the target dataset compared to self-supervised models. We also
find that (2) models finetuned on larger scale datasets are more likely to
introduce new biased associations. Our results also suggest that (3) biases can
transfer to finetuned models and the finetuning objective and dataset can
impact the extent of transferred biases.",None,-1
affbc0de-527f-4369-9bcc-a34dc1b41f88,TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition,0.146761,"High-dimensional token embeddings underpin Large Language Models (LLMs), as
they can capture subtle semantic information and significantly enhance the
modelling of complex language patterns. However, the associated high
dimensionality also introduces considerable model parameters, and a
prohibitively high model storage. To address this issue, this work proposes an
approach based on the Tensor-Train Decomposition (TTD), where each token
embedding is treated as a Matrix Product State (MPS) that can be efficiently
computed in a distributed manner. The experimental results on GPT-2 demonstrate
that, through our approach, the embedding layer can be compressed by a factor
of up to 38.40 times, and when the compression factor is 3.31 times, even
produced a better performance than the original GPT-2 model.",None,-1
415c983d-3718-49cb-9979-58cc79a6bc4d,Towards Local Visual Modeling for Image Captioning,0.738976,"In this paper, we study the local visual modeling with grid features for
image captioning, which is critical for generating accurate and detailed
captions. To achieve this target, we propose a Locality-Sensitive Transformer
Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention
(LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer
interaction in Transformer via modeling the relationship between each grid and
its neighbors. It reduces the difficulty of local object recognition during
captioning. LSF is used for inter-layer information fusion, which aggregates
the information of different encoder layers for cross-layer semantical
complementarity. With these two novel designs, the proposed LSTNet can model
the local visual information of grid features to improve the captioning
quality. To validate LSTNet, we conduct extensive experiments on the
competitive MS-COCO benchmark. The experimental results show that LSTNet is not
only capable of local visual modeling, but also outperforms a bunch of
state-of-the-art captioning models on offline and online testings, i.e., 134.8
CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is
also verified on the Flickr8k and Flickr30k datasets",None,-1
da13e15a-a23c-4cbb-b0f3-5d750d4cf939,Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint,0.627672,"Clinical trials are indispensable in developing new treatments, but they face
obstacles in patient recruitment and retention, hindering the enrollment of
necessary participants. To tackle these challenges, deep learning frameworks
have been created to match patients to trials. These frameworks calculate the
similarity between patients and clinical trial eligibility criteria,
considering the discrepancy between inclusion and exclusion criteria. Recent
studies have shown that these frameworks outperform earlier approaches.
However, deep learning models may raise fairness issues in patient-trial
matching when certain sensitive groups of individuals are underrepresented in
clinical trials, leading to incomplete or inaccurate data and potential harm.
To tackle the issue of fairness, this work proposes a fair patient-trial
matching framework by generating a patient-criterion level fairness constraint.
The proposed framework considers the inconsistency between the embedding of
inclusion and exclusion criteria among patients of different sensitive groups.
The experimental results on real-world patient-trial and patient-criterion
matching tasks demonstrate that the proposed framework can successfully
alleviate the predictions that tend to be biased.",None,-1
d20b4152-4b07-444d-af34-d1a847b41755,Boosting Diffusion Models with an Adaptive Momentum Sampler,0.0933808,"Diffusion probabilistic models (DPMs) have been shown to generate
high-quality images without the need for delicate adversarial training.
However, the current sampling process in DPMs is prone to violent shaking. In
this paper, we present a novel reverse sampler for DPMs inspired by the
widely-used Adam optimizer. Our proposed sampler can be readily applied to a
pre-trained diffusion model, utilizing momentum mechanisms and adaptive
updating to smooth the reverse sampling process and ensure stable generation,
resulting in outputs of enhanced quality. By implicitly reusing update
directions from early steps, our proposed sampler achieves a better balance
between high-level semantics and low-level details. Additionally, this sampler
is flexible and can be easily integrated into pre-trained DPMs regardless of
the sampler used during training. Our experimental results on multiple
benchmarks demonstrate that our proposed reverse sampler yields remarkable
improvements over different baselines. We will make the source code available.",None,-1
11a96912-3143-4c1c-a6cf-e8f82445a917,Guided Focal Stack Refinement Network for Light Field Salient Object Detection,0.655784,"Light field salient object detection (SOD) is an emerging research direction
attributed to the richness of light field data. However, most existing methods
lack effective handling of focal stacks, therefore making the latter involved
in a lot of interfering information and degrade the performance of SOD. To
address this limitation, we propose to utilize multi-modal features to refine
focal stacks in a guided manner, resulting in a novel guided focal stack
refinement network called GFRNet. To this end, we propose a guided refinement
and fusion module (GRFM) to refine focal stacks and aggregate multi-modal
features. In GRFM, all-in-focus (AiF) and depth modalities are utilized to
refine focal stacks separately, leading to two novel sub-modules for different
modalities, namely AiF-based refinement module (ARM) and depth-based refinement
module (DRM). Such refinement modules enhance structural and positional
information of salient objects in focal stacks, and are able to improve SOD
accuracy. Experimental results on four benchmark datasets demonstrate the
superiority of our GFRNet model against 12 state-of-the-art models.",None,-1
ec32d1ae-6645-431c-af50-de3d19bb2043,FlowText: Synthesizing Realistic Scene Text Video with Optical Flow Estimation,0.306444,"Current video text spotting methods can achieve preferable performance,
powered with sufficient labeled training data. However, labeling data manually
is time-consuming and labor-intensive. To overcome this, using low-cost
synthetic data is a promising alternative. This paper introduces a novel video
text synthesis technique called FlowText, which utilizes optical flow
estimation to synthesize a large amount of text video data at a low cost for
training robust video text spotters. Unlike existing methods that focus on
image-level synthesis, FlowText concentrates on synthesizing temporal
information of text instances across consecutive frames using optical flow.
This temporal information is crucial for accurately tracking and spotting text
in video sequences, including text movement, distortion, appearance,
disappearance, shelter, and blur. Experiments show that combining general
detectors like TransDETR with the proposed FlowText produces remarkable results
on various datasets, such as ICDAR2015video and ICDAR2013video. Code is
available at https://github.com/callsys/FlowText.",None,-1
de95f5a7-d7cf-4fa3-9745-949292701b67,ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation,0.558938,"Compositional generalization benchmarks for semantic parsing seek to assess
whether models can accurately compute meanings for novel sentences, but
operationalize this in terms of logical form (LF) prediction. This raises the
concern that semantically irrelevant details of the chosen LFs could shape
model performance. We argue that this concern is realized for the COGS
benchmark. COGS poses generalization splits that appear impossible for
present-day models, which could be taken as an indictment of those models.
However, we show that the negative results trace to incidental features of COGS
LFs. Converting these LFs to semantically equivalent ones and factoring out
capabilities unrelated to semantic interpretation, we find that even baseline
models get traction. A recent variable-free translation of COGS LFs suggests
similar conclusions, but we observe this format is not semantically equivalent;
it is incapable of accurately representing some COGS meanings. These findings
inform our proposal for ReCOGS, a modified version of COGS that comes closer to
assessing the target semantic capabilities while remaining very challenging.
Overall, our results reaffirm the importance of compositional generalization
and careful benchmark task design.",None,-1
7042cf7f-bcfa-4a9d-b8eb-4f09c560b92a,Machine Learning Recommendation System For Health Insurance Decision Making In Nigeria,0.506172,"The uptake of health insurance has been poor in Nigeria, a significant step
to improving this includes improved awareness, access to information and tools
to support decision making. Artificial intelligence (AI) based recommender
systems have gained popularity in helping individuals find movies, books,
music, and different types of products on the internet including diverse
applications in healthcare. The content-based methodology (item-based approach)
was employed in the recommender system. We applied both the K-Nearest Neighbor
(KNN) and Cosine similarity algorithm. We chose the Cosine similarity as our
chosen algorithm after several evaluations based of their outcomes in
comparison with domain knowledge. The recommender system takes into
consideration the choices entered by the user, filters the health management
organization (HMO) data by location and chosen prices. It then recommends the
top 3 HMOs with closest similarity in services offered. A recommendation tool
to help people find and select the best health insurance plan for them is
useful in reducing the barrier of accessing health insurance. Users are
empowered to easily find appropriate information on available plans, reduce
cognitive overload in dealing with over 100 options available in the market and
easily see what matches their financial capacity.",None,-1
50e34144-6ae8-489e-9812-bf7da19ac60f,Strategies for improving low resource speech to text translation relying on pre-trained ASR models,0.395068,"This paper presents techniques and findings for improving the performance of
low-resource speech to text translation (ST). We conducted experiments on both
simulated and real-low resource setups, on language pairs English - Portuguese,
and Tamasheq - French respectively. Using the encoder-decoder framework for ST,
our results show that a multilingual automatic speech recognition system acts
as a good initialization under low-resource scenarios. Furthermore, using the
CTC as an additional objective for translation during training and decoding
helps to reorder the internal representations and improves the final
translation. Through our experiments, we try to identify various factors
(initializations, objectives, and hyper-parameters) that contribute the most
for improvements in low-resource setups. With only 300 hours of pre-training
data, our model achieved 7.3 BLEU score on Tamasheq - French data,
outperforming prior published works from IWSLT 2022 by 1.6 points.",None,-1
4ca91510-ddc6-4a8f-ad92-f9dda4b537ee,Towards reducing hallucination in extracting information from financial reports using Large Language Models,0.240756,"For a financial analyst, the question and answer (Q\&A) segment of the
company financial report is a crucial piece of information for various analysis
and investment decisions. However, extracting valuable insights from the Q\&A
section has posed considerable challenges as the conventional methods such as
detailed reading and note-taking lack scalability and are susceptible to human
errors, and Optical Character Recognition (OCR) and similar techniques
encounter difficulties in accurately processing unstructured transcript text,
often missing subtle linguistic nuances that drive investor decisions. Here, we
demonstrate the utilization of Large Language Models (LLMs) to efficiently and
rapidly extract information from earnings report transcripts while ensuring
high accuracy transforming the extraction process as well as reducing
hallucination by combining retrieval-augmented generation technique as well as
metadata. We evaluate the outcomes of various LLMs with and without using our
proposed approach based on various objective metrics for evaluating Q\&A
systems, and empirically demonstrate superiority of our method.",None,-1
744d7cfc-8291-4531-a1f6-f7fbd91b7f3c,Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts,0.59686,"Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still underperforms on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.",None,-1
f639aac4-3dc7-49a2-a1dd-984a16ea7443,Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?,0.732865,"Much discussion about large language models and language-and-vision models
has focused on whether these models are intelligent agents. We present an
alternative perspective. We argue that these artificial intelligence models are
cultural technologies that enhance cultural transmission in the modern world,
and are efficient imitation engines. We explore what AI models can tell us
about imitation and innovation by evaluating their capacity to design new tools
and discover novel causal structures, and contrast their responses with those
of human children. Our work serves as a first step in determining which
particular representations and competences, as well as which kinds of knowledge
or skill, can be derived from particular learning techniques and data.
Critically, our findings suggest that machines may need more than large scale
language and images to achieve what a child can do.",None,-1
4e00127b-6c93-4b15-8876-0443df6a13d3,The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models,0.415661,"Despite the notable accomplishments of deep object detection models, a major
challenge that persists is the requirement for extensive amounts of training
data. The process of procuring such real-world data is a laborious undertaking,
which has prompted researchers to explore new avenues of research, such as
synthetic data generation techniques. This study presents a framework for the
generation of synthetic datasets by fine-tuning pretrained stable diffusion
models. The synthetic datasets are then manually annotated and employed for
training various object detection models. These detectors are evaluated on a
real-world test set of 331 images and compared against a baseline model that
was trained on real-world images. The results of this study reveal that the
object detection models trained on synthetic data perform similarly to the
baseline model. In the context of apple detection in orchards, the average
precision deviation with the baseline ranges from 0.09 to 0.12. This study
illustrates the potential of synthetic data generation techniques as a viable
alternative to the collection of extensive training data for the training of
deep models.",None,-1
84bf8053-b87d-4473-a8fb-5b3f68da6549,Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System,0.446333,"Developing an efficient retriever to retrieve knowledge from a large-scale
knowledge base (KB) is critical for task-oriented dialogue systems to
effectively handle localized and specialized tasks. However, widely used
generative models such as T5 and ChatGPT often struggle to differentiate subtle
differences among the retrieved KB records when generating responses, resulting
in suboptimal quality of generated responses. In this paper, we propose the
application of maximal marginal likelihood to train a perceptive retriever by
utilizing signals from response generation for supervision. In addition, our
approach goes beyond considering solely retrieved entities and incorporates
various meta knowledge to guide the generator, thus improving the utilization
of knowledge. We evaluate our approach on three task-oriented dialogue datasets
using T5 and ChatGPT as the backbone models. The results demonstrate that when
combined with meta knowledge, the response generator can effectively leverage
high-quality knowledge records from the retriever and enhance the quality of
generated responses. The codes and models of this paper are available at
https://github.com/shenwzh3/MK-TOD.",None,-1
a0bb1f13-aa02-418d-aaee-ac14fa09be56,How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions,1.0,"Large language models (LLMs) can ""lie"", which we define as outputting false
statements despite ""knowing"" the truth in a demonstrable sense. LLMs might
""lie"", for example, when instructed to output misinformation. Here, we develop
a simple lie detector that requires neither access to the LLM's activations
(black-box) nor ground-truth knowledge of the fact in question. The detector
works by asking a predefined set of unrelated follow-up questions after a
suspected lie, and feeding the LLM's yes/no answers into a logistic regression
classifier. Despite its simplicity, this lie detector is highly accurate and
surprisingly general. When trained on examples from a single setting --
prompting GPT-3.5 to lie about factual questions -- the detector generalises
out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,
(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as
sales. These results indicate that LLMs have distinctive lie-related
behavioural patterns, consistent across architectures and contexts, which could
enable general-purpose lie detection.",None,-1
47d7601b-6e6c-447d-9cfa-79de6cdb7a8b,Towards equilibrium molecular conformation generation with GFlowNets,0.9611,"Sampling diverse, thermodynamically feasible molecular conformations plays a
crucial role in predicting properties of a molecule. In this paper we propose
to use GFlowNet for sampling conformations of small molecules from the
Boltzmann distribution, as determined by the molecule's energy. The proposed
approach can be used in combination with energy estimation methods of different
fidelity and discovers a diverse set of low-energy conformations for highly
flexible drug-like molecules. We demonstrate that GFlowNet can reproduce
molecular potential energy surfaces by sampling proportionally to the Boltzmann
distribution.",None,-1
118469da-598e-498d-8898-b4f2bdf48d3c,Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?,0.380979,"Artificial intelligence (AI) systems will increasingly be used to cause harm
as they grow more capable. In fact, AI systems are already starting to be used
to automate fraudulent activities, violate human rights, create harmful fake
images, and identify dangerous toxins. To prevent some misuses of AI, we argue
that targeted interventions on certain capabilities will be warranted. These
restrictions may include controlling who can access certain types of AI models,
what they can be used for, whether outputs are filtered or can be traced back
to their user, and the resources needed to develop them. We also contend that
some restrictions on non-AI capabilities needed to cause harm will be required.
Though capability restrictions risk reducing use more than misuse (facing an
unfavorable Misuse-Use Tradeoff), we argue that interventions on capabilities
are warranted when other interventions are insufficient, the potential harm
from misuse is high, and there are targeted ways to intervene on capabilities.
We provide a taxonomy of interventions that can reduce AI misuse, focusing on
the specific steps required for a misuse to cause harm (the Misuse Chain), and
a framework to determine if an intervention is warranted. We apply this
reasoning to three examples: predicting novel toxins, creating harmful images,
and automating spear phishing campaigns.",None,-1
cf439585-9193-4bd4-9859-a46dddbc61ac,A Wide Evaluation of ChatGPT on Affective Computing Tasks,0.832693,"With the rise of foundation models, a new artificial intelligence paradigm
has emerged, by simply using general purpose foundation models with prompting
to solve problems instead of training a separate machine learning model for
each problem. Such models have been shown to have emergent properties of
solving problems that they were not initially trained on. The studies for the
effectiveness of such models are still quite limited. In this work, we widely
study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13
affective computing problems, namely aspect extraction, aspect polarity
classification, opinion extraction, sentiment analysis, sentiment intensity
ranking, emotions intensity ranking, suicide tendency detection, toxicity
detection, well-being assessment, engagement measurement, personality
assessment, sarcasm detection, and subjectivity detection. We introduce a
framework to evaluate the ChatGPT models on regression-based problems, such as
intensity ranking problems, by modelling them as pairwise ranking
classification. We compare ChatGPT against more traditional NLP methods, such
as end-to-end recurrent neural networks and transformers. The results
demonstrate the emergent abilities of the ChatGPT models on a wide range of
affective computing problems, where GPT-3.5 and especially GPT-4 have shown
strong performance on many problems, particularly the ones related to
sentiment, emotions, or toxicity. The ChatGPT models fell short for problems
with implicit signals, such as engagement measurement and subjectivity
detection.",None,-1
fdc5b5e3-2615-45a6-ad66-338a8d1caceb,ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots,0.650692,"The birth of ChatGPT, a cutting-edge language model-based chatbot developed
by OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its
role in rigorous scientific research is not clear yet. This paper vividly
showcases its innovative application within the field of drug discovery.
Focused specifically on developing anti-cocaine addiction drugs, the study
employs GPT-4 as a virtual guide, offering strategic and methodological
insights to researchers working on generative models for drug candidates. The
primary objective is to generate optimal drug-like molecules with desired
properties. By leveraging the capabilities of ChatGPT, the study introduces a
novel approach to the drug discovery process. This symbiotic partnership
between AI and researchers transforms how drug development is approached.
Chatbots become facilitators, steering researchers towards innovative
methodologies and productive paths for creating effective drug candidates. This
research sheds light on the collaborative synergy between human expertise and
AI assistance, wherein ChatGPT's cognitive abilities enhance the design and
development of potential pharmaceutical solutions. This paper not only explores
the integration of advanced AI in drug discovery but also reimagines the
landscape by advocating for AI-powered chatbots as trailblazers in
revolutionizing therapeutic innovation.",None,-1
c5ec1e35-ed48-44d4-a74b-bb389f1f611c,Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation,0.755187,"We address the problem of referring image segmentation that aims to generate
a mask for the object specified by a natural language expression. Many recent
works utilize Transformer to extract features for the target object by
aggregating the attended visual regions. However, the generic attention
mechanism in Transformer only uses the language input for attention weight
calculation, which does not explicitly fuse language features in its output.
Thus, its output feature is dominated by vision information, which limits the
model to comprehensively understand the multi-modal information, and brings
uncertainty for the subsequent mask decoder to extract the output mask. To
address this issue, we propose Multi-Modal Mutual Attention ($\mathrm{M^3Att}$)
and Multi-Modal Mutual Decoder ($\mathrm{M^3Dec}$) that better fuse information
from the two input modalities. Based on {$\mathrm{M^3Dec}$}, we further propose
Iterative Multi-modal Interaction ($\mathrm{IMI}$) to allow continuous and
in-depth interactions between language and vision features. Furthermore, we
introduce Language Feature Reconstruction ($\mathrm{LFR}$) to prevent the
language information from being lost or distorted in the extracted feature.
Extensive experiments show that our proposed approach significantly improves
the baseline and outperforms state-of-the-art referring image segmentation
methods on RefCOCO series datasets consistently.",None,-1
5295b821-05b4-4c44-b172-2af26fd32999,AU-aware graph convolutional network for Macro- and Micro-expression spotting,0.766672,"Automatic Micro-Expression (ME) spotting in long videos is a crucial step in
ME analysis but also a challenging task due to the short duration and low
intensity of MEs. When solving this problem, previous works generally lack in
considering the structures of human faces and the correspondence between
expressions and relevant facial muscles. To address this issue for better
performance of ME spotting, this paper seeks to extract finer spatial features
by modeling the relationships between facial Regions of Interest (ROIs).
Specifically, we propose a graph convolutional-based network, called
Action-Unit-aWare Graph Convolutional Network (AUW-GCN). Furthermore, to inject
prior information and to cope with the problem of small datasets, AU-related
statistics are encoded into the network. Comprehensive experiments show that
our results outperform baseline methods consistently and achieve new SOTA
performance in two benchmark datasets,CAS(ME)^2 and SAMM-LV. Our code is
available at https://github.com/xjtupanda/AUW-GCN.",None,-1
46b2f48a-53f3-40c6-a349-270477f6d28b,GLEN: General-Purpose Event Detection for Thousands of Types,0.772151,"The progress of event extraction research has been hindered by the absence of
wide-coverage, large-scale datasets. To make event extraction systems more
accessible, we build a general-purpose event detection dataset GLEN, which
covers 205K event mentions with 3,465 different types, making it more than 20x
larger in ontology than today's largest event dataset. GLEN is created by
utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and
PropBank rolesets. This enables us to use the abundant existing annotation for
PropBank as distant supervision. In addition, we also propose a new multi-stage
event detection model CEDAR specifically designed to handle the large ontology
size in GLEN. We show that our model exhibits superior performance compared to
a range of baselines including InstructGPT. Finally, we perform error analysis
and show that label noise is still the largest challenge for improving
performance for this new dataset. Our dataset, code, and models are released at
\url{https://github.com/ZQS1943/GLEN}.}",None,-1
2bc96f05-7ad0-4b79-b1fd-be87a301c02b,DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment,0.441575,"Text-to-audio (TTA) generation is a recent popular problem that aims to
synthesize general audio given text descriptions. Previous methods utilized
latent diffusion models to learn audio embedding in a latent space with text
embedding as the condition. However, they ignored the synchronization between
audio and visual content in the video, and tended to generate audio mismatching
from video frames. In this work, we propose a novel and personalized
text-to-sound generation approach with visual alignment based on latent
diffusion models, namely DiffAVA, that can simply fine-tune lightweight
visual-text alignment modules with frozen modality-specific encoders to update
visual-aligned text embeddings as the condition. Specifically, our DiffAVA
leverages a multi-head attention transformer to aggregate temporal information
from video features, and a dual multi-modal residual network to fuse temporal
visual representations with text embeddings. Then, a contrastive learning
objective is applied to match visual-aligned text embeddings with audio
features. Experimental results on the AudioCaps dataset demonstrate that the
proposed DiffAVA can achieve competitive performance on visual-aligned
text-to-audio generation.",None,-1
e42ce736-fe5a-4948-828a-3ed7f43ac0f3,Practical Knowledge Distillation: Using DNNs to Beat DNNs,0.0878606,"For tabular data sets, we explore data and model distillation, as well as
data denoising. These techniques improve both gradient-boosting models and a
specialized DNN architecture. While gradient boosting is known to outperform
DNNs on tabular data, we close the gap for datasets with 100K+ rows and give
DNNs an advantage on small data sets. We extend these results with input-data
distillation and optimized ensembling to help DNN performance match or exceed
that of gradient boosting. As a theoretical justification of our practical
method, we prove its equivalence to classical cross-entropy knowledge
distillation. We also qualitatively explain the superiority of DNN ensembles
over XGBoost on small data sets. For an industry end-to-end real-time ML
platform with 4M production inferences per second, we develop a model-training
workflow based on data sampling that distills ensembles of models into a single
gradient-boosting model favored for high-performance real-time inference,
without performance loss. Empirical evaluation shows that the proposed
combination of methods consistently improves model accuracy over prior best
models across several production applications deployed worldwide.",None,-1
dd5ccb19-9e31-4960-96a2-72bd1575ccf7,Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer,0.446245,"Cross-lingual natural language inference is a fundamental problem in
cross-lingual language understanding. Many recent works have used prompt
learning to address the lack of annotated parallel corpora in XNLI. However,
these methods adopt discrete prompting by simply translating the templates to
the target language and need external expert knowledge to design the templates.
Besides, discrete prompts of human-designed template words are not trainable
vectors and can not be migrated to target languages in the inference stage
flexibly. In this paper, we propose a novel Soft prompt learning framework with
the Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs
cloze-style question with soft prompts for the input sample. Then we leverage
bilingual dictionaries to generate an augmented multilingual question for the
original question. SoftMV adopts a multilingual verbalizer to align the
representations of original and augmented multilingual questions into the same
semantic space with consistency regularization. Experimental results on XNLI
demonstrate that SoftMV can achieve state-of-the-art performance and
significantly outperform the previous methods under the few-shot and full-shot
cross-lingual transfer settings.",None,-1
38c0a05a-021e-483f-a52a-0d79962788fe,Fast Matrix Multiplication Without Tears: A Constraint Programming Approach,0.178464,"It is known that the multiplication of an $N \times M$ matrix with an $M
\times P$ matrix can be performed using fewer multiplications than what the
naive $NMP$ approach suggests. The most famous instance of this is Strassen's
algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8
multiplications. This gives rise to the constraint satisfaction problem of fast
matrix multiplication, where a set of $R < NMP$ multiplication terms must be
chosen and combined such that they satisfy correctness constraints on the
output matrix. Despite its highly combinatorial nature, this problem has not
been exhaustively examined from that perspective, as evidenced for example by
the recent deep reinforcement learning approach of AlphaTensor. In this work,
we propose a simple yet novel Constraint Programming approach to find
non-commutative algorithms for fast matrix multiplication or provide proof of
infeasibility otherwise. We propose a set of symmetry-breaking constraints and
valid inequalities that are particularly helpful in proving infeasibility. On
the feasible side, we find that exploiting solver performance variability in
conjunction with a sparsity-based problem decomposition enables finding
solutions for larger (feasible) instances of fast matrix multiplication. Our
experimental results using CP Optimizer demonstrate that we can find fast
matrix multiplication algorithms for matrices up to $3\times 3$ in a short
amount of time.",None,-1
277c58ac-f44c-4531-b5f4-0fb613d9ad6b,Feature Mixing for Writer Retrieval and Identification on Papyri Fragments,0.614179,"This paper proposes a deep-learning-based approach to writer retrieval and
identification for papyri, with a focus on identifying fragments associated
with a specific writer and those corresponding to the same image. We present a
novel neural network architecture that combines a residual backbone with a
feature mixing stage to improve retrieval performance, and the final descriptor
is derived from a projection layer. The methodology is evaluated on two
benchmarks: PapyRow, where we achieve a mAP of 26.6 % and 24.9 % on writer and
page retrieval, and HisFragIR20, showing state-of-the-art performance (44.0 %
and 29.3 % mAP). Furthermore, our network has an accuracy of 28.7 % for writer
identification. Additionally, we conduct experiments on the influence of two
binarization techniques on fragments and show that binarizing does not enhance
performance. Our code and models are available to the community.",None,-1
1f1eda47-5069-45ce-853a-7e65c2c78bf9,Deficiency-Aware Masked Transformer for Video Inpainting,0.241008,"Recent video inpainting methods have made remarkable progress by utilizing
explicit guidance, such as optical flow, to propagate cross-frame pixels.
However, there are cases where cross-frame recurrence of the masked video is
not available, resulting in a deficiency. In such situation, instead of
borrowing pixels from other frames, the focus of the model shifts towards
addressing the inverse problem. In this paper, we introduce a
dual-modality-compatible inpainting framework called Deficiency-aware Masked
Transformer (DMT), which offers three key advantages. Firstly, we pretrain a
image inpainting model DMT_img serve as a prior for distilling the video model
DMT_vid, thereby benefiting the hallucination of deficiency cases. Secondly,
the self-attention module selectively incorporates spatiotemporal tokens to
accelerate inference and remove noise signals. Thirdly, a simple yet effective
Receptive Field Contextualizer is integrated into DMT, further improving
performance. Extensive experiments conducted on YouTube-VOS and DAVIS datasets
demonstrate that DMT_vid significantly outperforms previous solutions. The code
and video demonstrations can be found at github.com/yeates/DMT.",None,-1
246a78cf-6085-4da4-a9d2-7a4470648e18,Dynamic Multi-View Fusion Mechanism For Chinese Relation Extraction,0.346696,"Recently, many studies incorporate external knowledge into character-level
feature based models to improve the performance of Chinese relation extraction.
However, these methods tend to ignore the internal information of the Chinese
character and cannot filter out the noisy information of external knowledge. To
address these issues, we propose a mixture-of-view-experts framework (MoVE) to
dynamically learn multi-view features for Chinese relation extraction. With
both the internal and external knowledge of Chinese characters, our framework
can better capture the semantic information of Chinese characters. To
demonstrate the effectiveness of the proposed framework, we conduct extensive
experiments on three real-world datasets in distinct domains. Experimental
results show consistent and significant superiority and robustness of our
proposed framework. Our code and dataset will be released at:
https://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction",None,-1
5b5b5f6e-2d7e-4c9c-8a08-19e646d94bac,Poisoning Language Models During Instruction Tuning,0.791408,"Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on
datasets that contain user-submitted examples, e.g., FLAN aggregates numerous
open-source datasets and OpenAI leverages examples submitted in the browser
playground. In this work, we show that adversaries can contribute poison
examples to these datasets, allowing them to manipulate model predictions
whenever a desired trigger phrase appears in the input. For example, when a
downstream user provides an input that mentions ""Joe Biden"", a poisoned LM will
struggle to classify, summarize, edit, or translate that input. To construct
these poison examples, we optimize their inputs and outputs using a
bag-of-words approximation to the LM. We evaluate our method on open-source
instruction-tuned LMs. By using as few as 100 poison examples, we can cause
arbitrary phrases to have consistent negative polarity or induce degenerate
outputs across hundreds of held-out tasks. Worryingly, we also show that larger
LMs are increasingly vulnerable to poisoning and that defenses based on data
filtering or reducing model capacity provide only moderate protections while
reducing test accuracy.",None,-1
adcb9be0-181c-4946-99c1-f13f520b8900,LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain,0.599472,"Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.",None,-1
336ebc7a-00ba-4e91-986f-a441491bd59a,Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision,0.47314,"Credibility signals represent a wide range of heuristics that are typically
used by journalists and fact-checkers to assess the veracity of online content.
Automating the task of credibility signal extraction, however, is very
challenging as it requires high-accuracy signal-specific extractors to be
trained, while there are currently no sufficiently large datasets annotated
with all credibility signals. This paper investigates whether large language
models (LLMs) can be prompted effectively with a set of 18 credibility signals
to produce weak labels for each signal. We then aggregate these potentially
noisy labels using weak supervision in order to predict content veracity. We
demonstrate that our approach, which combines zero-shot LLM credibility signal
labeling and weak supervision, outperforms state-of-the-art classifiers on two
misinformation datasets without using any ground-truth labels for training. We
also analyse the contribution of the individual credibility signals towards
predicting content veracity, which provides new valuable insights into their
role in misinformation detection.",None,-1
13528034-c844-4af8-9f3f-7d02dc2f3522,Exploiting Diverse Feature for Multimodal Sentiment Analysis,0.102323,"In this paper, we present our solution to the MuSe-Personalisation
sub-challenge in the MuSe 2023 Multimodal Sentiment Analysis Challenge. The
task of MuSe-Personalisation aims to predict the continuous arousal and valence
values of a participant based on their audio-visual, language, and
physiological signal modalities data. Considering different people have
personal characteristics, the main challenge of this task is how to build
robustness feature presentation for sentiment prediction. To address this
issue, we propose exploiting diverse features. Specifically, we proposed a
series of feature extraction methods to build a robust representation and model
ensemble. We empirically evaluate the performance of the utilized method on the
officially provided dataset. \textbf{As a result, we achieved 3rd place in the
MuSe-Personalisation sub-challenge.} Specifically, we achieve the results of
0.8492 and 0.8439 for MuSe-Personalisation in terms of arousal and valence CCC.",None,-1
01c9c97f-99c2-447e-9ba2-33d97d1db113,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,0.30817,"The visual dimension of cities has been a fundamental subject in urban
studies, since the pioneering work of scholars such as Sitte, Lynch, Arnheim,
and Jacobs. Several decades later, big data and artificial intelligence (AI)
are revolutionizing how people move, sense, and interact with cities. This
paper reviews the literature on the appearance and function of cities to
illustrate how visual information has been used to understand them. A
conceptual framework, Urban Visual Intelligence, is introduced to
systematically elaborate on how new image data sources and AI techniques are
reshaping the way researchers perceive and measure cities, enabling the study
of the physical environment and its interactions with socioeconomic
environments at various scales. The paper argues that these new approaches
enable researchers to revisit the classic urban theories and themes, and
potentially help cities create environments that are more in line with human
behaviors and aspirations in the digital age.",None,-1
dd6eade6-9e50-4f0e-9cc9-bd23f0dc81ea,Evaluating the Robustness of Machine Reading Comprehension Models to Low Resource Entity Renaming,0.424571,"Question answering (QA) models have shown compelling results in the task of
Machine Reading Comprehension (MRC). Recently these systems have proved to
perform better than humans on held-out test sets of datasets e.g. SQuAD, but
their robustness is not guaranteed. The QA model's brittleness is exposed when
evaluated on adversarial generated examples by a performance drop. In this
study, we explore the robustness of MRC models to entity renaming, with
entities from low-resource regions such as Africa. We propose EntSwap, a method
for test-time perturbations, to create a test set whose entities have been
renamed. In particular, we rename entities of type: country, person,
nationality, location, organization, and city, to create AfriSQuAD2. Using the
perturbed test set, we evaluate the robustness of three popular MRC models. We
find that compared to base models, large models perform well comparatively on
novel entities. Furthermore, our analysis indicates that entity type person
highly challenges the MRC models' performance.",None,-1
1008d4ad-0815-4c84-bd59-a06e3112d848,For the Underrepresented in Gender Bias Research: Chinese Name Gender Prediction with Heterogeneous Graph Attention Network,0.0397292,"Achieving gender equality is an important pillar for humankind's sustainable
future. Pioneering data-driven gender bias research is based on large-scale
public records such as scientific papers, patents, and company registrations,
covering female researchers, inventors and entrepreneurs, and so on. Since
gender information is often missing in relevant datasets, studies rely on tools
to infer genders from names. However, available open-sourced Chinese
gender-guessing tools are not yet suitable for scientific purposes, which may
be partially responsible for female Chinese being underrepresented in
mainstream gender bias research and affect their universality. Specifically,
these tools focus on character-level information while overlooking the fact
that the combinations of Chinese characters in multi-character names, as well
as the components and pronunciations of characters, convey important messages.
As a first effort, we design a Chinese Heterogeneous Graph Attention (CHGAT)
model to capture the heterogeneity in component relationships and incorporate
the pronunciations of characters. Our model largely surpasses current tools and
also outperforms the state-of-the-art algorithm. Last but not least, the most
popular Chinese name-gender dataset is single-character based with far less
female coverage from an unreliable source, naturally hindering relevant
studies. We open-source a more balanced multi-character dataset from an
official source together with our code, hoping to help future research
promoting gender equality.",None,-1
a086024a-d248-40d9-91ea-6f9ce4917ace,"Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild",0.333586,"Recently, reinforcement learning has become a promising and polular solution
for robot legged locomotion. Compared to model-based control, reinforcement
learning based controllers can achieve better robustness against uncertainties
of environments through sim-to-real learning. However, the corresponding
learned gaits are in general overly conservative and unatural. In this paper,
we propose a new framework for learning robust, agile and natural legged
locomotion skills over challenging terrain. We incorporate an adversarial
training branch based on real animal locomotion data upon a teacher-student
training pipeline for robust sim-to-real transfer. Empirical results on both
simulation and real world of a quadruped robot demonstrate that our proposed
algorithm enables robustly traversing challenging terrains such as stairs,
rocky ground and slippery floor with only proprioceptive perception. Meanwhile,
the gaits are more agile, natural, and energy efficient compared to the
baselines. Both qualitative and quantitative results are presented in this
paper.",None,-1
a2f065ce-2dfd-49cd-be51-1e37130d7f63,Ontology-aware Network for Zero-shot Sketch-based Image Retrieval,0.0495931,"Zero-Shot Sketch-Based Image Retrieval (ZSSBIR) is an emerging task. The
pioneering work focused on the modal gap but ignored inter-class information.
Although recent work has begun to consider the triplet-based or contrast-based
loss to mine inter-class information, positive and negative samples need to be
carefully selected, or the model is prone to lose modality-specific
information. To respond to these issues, an Ontology-Aware Network (OAN) is
proposed. Specifically, the smooth inter-class independence learning mechanism
is put forward to maintain inter-class peculiarity. Meanwhile,
distillation-based consistency preservation is utilized to keep
modality-specific information. Extensive experiments have demonstrated the
superior performance of our algorithm on two challenging Sketchy and Tu-Berlin
datasets.",None,-1
235222e7-90ed-4f41-aa17-302130dc0a5f,Irreducible Curriculum for Language Model Pretraining,0.0722687,"Automatic data selection and curriculum design for training large language
models is challenging, with only a few existing methods showing improvements
over standard training. Furthermore, current schemes focus on domain-level
selection, overlooking the more fine-grained contributions of each individual
training point. It is difficult to apply traditional datapoint selection
methods on large language models: most online batch selection methods perform
two-times forward or backward passes, which introduces considerable extra costs
with large-scale models. To mitigate these obstacles, we propose irreducible
curriculum as a curriculum learning algorithm for language model pretraining,
which prioritizes samples with higher learnability. Specifically, to avoid
prohibitive extra computation overhead, we simulate the sample loss along the
main model's training trajectory using a small-scale proxy model. Our
experiments on the RedPajama-1B dataset demonstrate a consistent improvement on
validation perplexity across all 7 domains compared to random uniform baseline
and the anti-curriculum strategy. Our method also reduces the sharpness of the
network and illustrates a better 5-shot accuracy on MMLU benchmarks.",None,-1
c04ad999-4ceb-4969-a88f-07e206ed477b,Mimic-IV-ICD: A new benchmark for eXtreme MultiLabel Classification,0.893704,"Clinical notes are assigned ICD codes - sets of codes for diagnoses and
procedures. In the recent years, predictive machine learning models have been
built for automatic ICD coding. However, there is a lack of widely accepted
benchmarks for automated ICD coding models based on large-scale public EHR
data.
  This paper proposes a public benchmark suite for ICD-10 coding using a large
EHR dataset derived from MIMIC-IV, the most recent public EHR dataset. We
implement and compare several popular methods for ICD coding prediction tasks
to standardize data preprocessing and establish a comprehensive ICD coding
benchmark dataset. This approach fosters reproducibility and model comparison,
accelerating progress toward employing automated ICD coding in future studies.
Furthermore, we create a new ICD-9 benchmark using MIMIC-IV data, providing
more data points and a higher number of ICD codes than MIMIC-III. Our
open-source code offers easy access to data processing steps, benchmark
creation, and experiment replication for those with MIMIC-IV access, providing
insights, guidance, and protocols to efficiently develop ICD coding models.",None,-1
d94c15bf-0352-4559-911d-639bd83fdd9e,2nd Place Winning Solution for the CVPR2023 Visual Anomaly and Novelty Detection Challenge: Multimodal Prompting for Data-centric Anomaly Detection,0.097928,"This technical report introduces the winning solution of the team Segment Any
Anomaly for the CVPR2023 Visual Anomaly and Novelty Detection (VAND) challenge.
Going beyond uni-modal prompt, e.g., language prompt, we present a novel
framework, i.e., Segment Any Anomaly + (SAA$+$), for zero-shot anomaly
segmentation with multi-modal prompts for the regularization of cascaded modern
foundation models. Inspired by the great zero-shot generalization ability of
foundation models like Segment Anything, we first explore their assembly (SAA)
to leverage diverse multi-modal prior knowledge for anomaly localization.
Subsequently, we further introduce multimodal prompts (SAA$+$) derived from
domain expert knowledge and target image context to enable the non-parameter
adaptation of foundation models to anomaly segmentation. The proposed SAA$+$
model achieves state-of-the-art performance on several anomaly segmentation
benchmarks, including VisA and MVTec-AD, in the zero-shot setting. We will
release the code of our winning solution for the CVPR2023 VAN.",None,-1
395085ab-b33a-4d5a-a937-fcd6aa782f2b,Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation,0.924239,"Weakly supervised semantic segmentation with weak labels is a long-lived
ill-posed problem. Mainstream methods mainly focus on improving the quality of
pseudo labels. In this report, we attempt to explore the potential of 'prompt
to masks' from the powerful class-agnostic large segmentation model,
segment-anything. Specifically, different weak labels are used as prompts to
the segment-anything model, generating precise class masks. The class masks are
utilized to generate pseudo labels to train the segmentation networks. We have
conducted extensive experiments on PASCAL VOC 2012 dataset. Experiments
demonstrate that segment-anything can serve as a good pseudo-label generator.
The code will be made publicly available.",None,-1
536e9ac2-0bb3-456d-9d0c-08cdf1dea6c8,Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models,0.98266,"Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.",None,-1
7521f8f1-493d-4252-a329-f33c9bbaa5fe,Catch Me If You Can: Improving Adversaries in Cyber-Security With Q-Learning Algorithms,0.583138,"The ongoing rise in cyberattacks and the lack of skilled professionals in the
cybersecurity domain to combat these attacks show the need for automated tools
capable of detecting an attack with good performance. Attackers disguise their
actions and launch attacks that consist of multiple actions, which are
difficult to detect. Therefore, improving defensive tools requires their
calibration against a well-trained attacker. In this work, we propose a model
of an attacking agent and environment and evaluate its performance using basic
Q-Learning, Naive Q-learning, and DoubleQ-Learning, all of which are variants
of Q-Learning. The attacking agent is trained with the goal of exfiltrating
data whereby all the hosts in the network have a non-zero detection
probability. Results show that the DoubleQ-Learning agent has the best overall
performance rate by successfully achieving the goal in $70\%$ of the
interactions.",None,-1
c5c4b7b7-4057-4bd3-89c2-672a3b0a0e6f,Mobile User Interface Element Detection Via Adaptively Prompt Tuning,0.908859,"Recent object detection approaches rely on pretrained vision-language models
for image-text alignment. However, they fail to detect the Mobile User
Interface (MUI) element since it contains additional OCR information, which
describes its content and function but is often ignored. In this paper, we
develop a new MUI element detection dataset named MUI-zh and propose an
Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR
information. APT is a lightweight and effective module to jointly optimize
category prompts across different modalities. For every element, APT uniformly
encodes its visual features and OCR descriptions to dynamically adjust the
representation of frozen category prompts. We evaluate the effectiveness of our
plug-and-play APT upon several existing CLIP-based detectors for both standard
and open-vocabulary MUI element detection. Extensive experiments show that our
method achieves considerable improvements on two datasets. The datasets is
available at \url{github.com/antmachineintelligence/MUI-zh}.",None,-1
1fbb359d-41c8-4518-8b60-ab5f139269be,Devil's on the Edges: Selective Quad Attention for Scene Graph Generation,0.66183,"Scene graph generation aims to construct a semantic graph structure from an
image such that its nodes and edges respectively represent objects and their
relationships. One of the major challenges for the task lies in the presence of
distracting objects and relationships in images; contextual reasoning is
strongly distracted by irrelevant objects or backgrounds and, more importantly,
a vast number of irrelevant candidate relations. To tackle the issue, we
propose the Selective Quad Attention Network (SQUAT) that learns to select
relevant object pairs and disambiguate them via diverse contextual
interactions. SQUAT consists of two main components: edge selection and quad
attention. The edge selection module selects relevant object pairs, i.e., edges
in the scene graph, which helps contextual reasoning, and the quad attention
module then updates the edge features using both edge-to-node and edge-to-edge
cross-attentions to capture contextual information between objects and object
pairs. Experiments demonstrate the strong performance and robustness of SQUAT,
achieving the state of the art on the Visual Genome and Open Images v6
benchmarks.",None,-1
14c22937-7ee9-4855-91fd-1f8e0241c1c5,WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,0.987583,"We present WebGLM, a web-enhanced question-answering system based on the
General Language Model (GLM). Its goal is to augment a pre-trained large
language model (LLM) with web search and retrieval capabilities while being
efficient for real-world deployments. To achieve this, we develop WebGLM with
strategies for the LLM-augmented retriever, bootstrapped generator, and human
preference-aware scorer. Specifically, we identify and address the limitations
of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,
and cost-effectiveness advantages. In addition, we propose systematic criteria
for evaluating web-enhanced QA systems. We conduct multi-dimensional human
evaluation and quantitative ablation studies, which suggest the outperformance
of the proposed WebGLM designs over existing systems. WebGLM with the
10-billion-parameter GLM (10B) is shown to perform better than the
similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human
evaluation. The code, demo, and data are at
\url{https://github.com/THUDM/WebGLM}.",None,-1
0a06183a-b26f-46da-93eb-7370e712a66d,User Adaptive Language Learning Chatbots with a Curriculum,0.190666,"Along with the development of systems for natural language understanding and
generation, dialog systems have been widely adopted for language learning and
practicing. Many current educational dialog systems perform chitchat, where the
generated content and vocabulary are not constrained. However, for learners in
a school setting, practice through dialog is more effective if it aligns with
students' curriculum and focuses on textbook vocabulary. Therefore, we adapt
lexically constrained decoding to a dialog system, which urges the dialog
system to include curriculum-aligned words and phrases in its generated
utterances. We adopt a generative dialog system, BlenderBot3, as our backbone
model and evaluate our curriculum-based dialog system with middle school
students learning English as their second language. The constrained words and
phrases are derived from their textbooks, suggested by their English teachers.
The evaluation result demonstrates that the dialog system with curriculum
infusion improves students' understanding of target words and increases their
interest in practicing English.",None,-1
f6b9e5e6-0933-4132-98e2-9f5b002413ca,Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning,0.942213,"In-context learning (ICL) emerges as a promising capability of large language
models (LLMs) by providing them with demonstration examples to perform diverse
tasks. However, the underlying mechanism of how LLMs learn from the provided
context remains under-explored. In this paper, we investigate the working
mechanism of ICL through an information flow lens. Our findings reveal that
label words in the demonstration examples function as anchors: (1) semantic
information aggregates into label word representations during the shallow
computation layers' processing; (2) the consolidated information in label words
serves as a reference for LLMs' final predictions. Based on these insights, we
introduce an anchor re-weighting method to improve ICL performance, a
demonstration compression technique to expedite inference, and an analysis
framework for diagnosing ICL errors in GPT2-XL. The promising applications of
our findings again validate the uncovered ICL working mechanism and pave the
way for future studies.",None,-1
5edf758b-48aa-422b-afa5-b8926de15324,Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation,0.963985,"The ability to collect a large dataset of human preferences from
text-to-image users is usually limited to companies, making such datasets
inaccessible to the public. To address this issue, we create a web app that
enables text-to-image users to generate images and specify their preferences.
Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image
prompts and real users' preferences over generated images. We leverage this
dataset to train a CLIP-based scoring function, PickScore, which exhibits
superhuman performance on the task of predicting human preferences. Then, we
test PickScore's ability to perform model evaluation and observe that it
correlates better with human rankings than other automatic evaluation metrics.
Therefore, we recommend using PickScore for evaluating future text-to-image
generation models, and using Pick-a-Pic prompts as a more relevant dataset than
MS-COCO. Finally, we demonstrate how PickScore can enhance existing
text-to-image models via ranking.",None,-1
34ffe41f-b87e-4f7b-a427-95cf1e6668b8,Neural Amortized Inference for Nested Multi-agent Reasoning,0.0392152,"Multi-agent interactions, such as communication, teaching, and bluffing,
often rely on higher-order social inference, i.e., understanding how others
infer oneself. Such intricate reasoning can be effectively modeled through
nested multi-agent reasoning. Nonetheless, the computational complexity
escalates exponentially with each level of reasoning, posing a significant
challenge. However, humans effortlessly perform complex social inferences as
part of their daily lives. To bridge the gap between human-like inference
capabilities and computational limitations, we propose a novel approach:
leveraging neural networks to amortize high-order social inference, thereby
expediting nested multi-agent reasoning. We evaluate our method in two
challenging multi-agent interaction domains. The experimental results
demonstrate that our method is computationally efficient while exhibiting
minimal degradation in accuracy.",None,-1
0ca697d4-d1a5-47cb-afc7-cf6bcf482d31,Fast Neural Scene Flow,0.495673,"Neural Scene Flow Prior (NSFP) is of significant interest to the vision
community due to its inherent robustness to out-of-distribution (OOD) effects
and its ability to deal with dense lidar points. The approach utilizes a
coordinate neural network to estimate scene flow at runtime, without any
training. However, it is up to 100 times slower than current state-of-the-art
learning methods. In other applications such as image, video, and radiance
function reconstruction innovations in speeding up the runtime performance of
coordinate networks have centered upon architectural changes. In this paper, we
demonstrate that scene flow is different -- with the dominant computational
bottleneck stemming from the loss function itself (i.e., Chamfer distance).
Further, we rediscover the distance transform (DT) as an efficient,
correspondence-free loss function that dramatically speeds up the runtime
optimization. Our fast neural scene flow (FNSF) approach reports for the first
time real-time performance comparable to learning methods, without any training
or OOD bias on two of the largest open autonomous driving (AV) lidar datasets
Waymo Open and Argoverse.",None,-1
10b1916e-0100-4b9b-bf5f-8791f1038221,Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication,0.161377,"Many recent breakthroughs in machine learning have been enabled by the
pre-trained foundation models. By scaling up model parameters, training data,
and computation resources, foundation models have significantly advanced the
state-of-the-art in many applications. However, it is still an open question of
how to use these models to perform downstream tasks efficiently. Knowledge
distillation (KD) has been explored to tackle this challenge. KD transfers
knowledge from a large teacher model to a smaller student model. While KD has
been successful in improving student model performance, recent research has
discovered that a powerful teacher does not necessarily lead to a powerful
student, due to their huge capacity gap. In addition, the potential
distribution shifts between the pre-training data and downstream tasks can make
knowledge transfer in KD sub-optimal for improving downstream task performance.
In this paper, we extend KD with an interactive communication process to help
students of downstream tasks learn effectively from pre-trained foundation
models. Our design is inspired by the way humans learn from teachers who can
explain knowledge in a way that meets the students' needs. Specifically, we let
each model (i.e., student and teacher) train two components: (1) an encoder
encoding the model's hidden states to a message and (2) a decoder decoding any
messages to its own hidden states. With encoder and decoder, not only can the
teacher transfer rich information by encoding its hidden states, but also the
student can send messages with information of downstream tasks to the teacher.
Therefore, knowledge passing from teacher to student can be tailored to the
student's capacity and downstream tasks' distributions. We conducted
experiments on benchmark datasets to show that our communication mechanism
outperforms state-of-the-art distillation techniques.",None,-1
007692fb-d7b3-4298-a7b6-2d9d78dca462,SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory,0.648752,"Accurately predicting the destination of taxi trajectories can have various
benefits for intelligent location-based services. One potential method to
accomplish this prediction is by converting the taxi trajectory into a
two-dimensional grid and using computer vision techniques. While the Swin
Transformer is an innovative computer vision architecture with demonstrated
success in vision downstream tasks, it is not commonly used to solve real-world
trajectory problems. In this paper, we propose a simplified Swin Transformer
(SST) structure that does not use the shifted window idea in the traditional
Swin Transformer, as trajectory data is consecutive in nature. Our
comprehensive experiments, based on real trajectory data, demonstrate that SST
can achieve higher accuracy compared to state-of-the-art methods.",None,-1
65c85e70-187e-484f-be9c-6c4103d74d45,Multi-dimensional Signal Recovery using Low-rank Deconvolution,0.0706259,"In this work we present Low-rank Deconvolution, a powerful framework for
low-level feature-map learning for efficient signal representation with
application to signal recovery. Its formulation in multi-linear algebra
inherits properties from convolutional sparse coding and low-rank approximation
methods as in this setting signals are decomposed in a set of filters convolved
with a set of low-rank tensors. We show its advantages by learning compressed
video representations and solving image in-painting problems.",None,-1
e55dc414-7970-4c30-80cb-f0b87a64ad1d,Knowledge Enhanced Model for Live Video Comment Generation,0.422859,"Live video commenting is popular on video media platforms, as it can create a
chatting atmosphere and provide supplementary information for users while
watching videos. Automatically generating live video comments can improve user
experience and enable human-like generation for bot chatting. Existing works
mostly focus on short video datasets while ignoring other important video types
such as long videos like movies. In this work, we collect a new Movie Live
Comments (MovieLC) dataset to support research on live video comment generation
for long videos. We also propose a knowledge enhanced generation model inspired
by the divergent and informative nature of live video comments. Our model
adopts a pre-training encoder-decoder framework and incorporates external
knowledge. Extensive experiments show that both objective metrics and human
evaluation demonstrate the effectiveness of our proposed model. The MovieLC
dataset and our code will be released.",None,-1
f217bb75-965c-4394-bec4-52a02ec5a4cd,Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models,0.218649,"Emergency management urgently requires comprehensive knowledge while having a
high possibility to go beyond individuals' cognitive scope. Therefore,
artificial intelligence(AI) supported decision-making under that circumstance
is of vital importance. Recent emerging large language models (LLM) provide a
new direction for enhancing targeted machine intelligence. However, the
utilization of LLM directly would inevitably introduce unreliable output for
its inherent issue of hallucination and poor reasoning skills. In this work, we
develop a system called Enhancing Emergency decision-making with Knowledge
Graph and LLM (E-KELL), which provides evidence-based decision-making in
various emergency stages. The study constructs a structured emergency knowledge
graph and guides LLMs to reason over it via a prompt chain. In real-world
evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in
comprehensibility, accuracy, conciseness, and instructiveness from a group of
emergency commanders and firefighters, demonstrating a significant improvement
across various situations compared to baseline models. This work introduces a
novel approach to providing reliable emergency decision support.",None,-1
295b70aa-c4b0-46e8-a9f4-ef1d055ea80d,State of the Art on Diffusion Models for Visual Computing,0.830563,"The field of visual computing is rapidly advancing due to the emergence of
generative artificial intelligence (AI), which unlocks unprecedented
capabilities for the generation, editing, and reconstruction of images, videos,
and 3D scenes. In these domains, diffusion models are the generative AI
architecture of choice. Within the last year alone, the literature on
diffusion-based tools and applications has seen exponential growth and relevant
papers are published across the computer graphics, computer vision, and AI
communities with new works appearing daily on arXiv. This rapid growth of the
field makes it difficult to keep up with all recent developments. The goal of
this state-of-the-art report (STAR) is to introduce the basic mathematical
concepts of diffusion models, implementation details and design choices of the
popular Stable Diffusion model, as well as overview important aspects of these
generative AI tools, including personalization, conditioning, inversion, among
others. Moreover, we give a comprehensive overview of the rapidly growing
literature on diffusion-based generation and editing, categorized by the type
of generated medium, including 2D images, videos, 3D objects, locomotion, and
4D scenes. Finally, we discuss available datasets, metrics, open challenges,
and social implications. This STAR provides an intuitive starting point to
explore this exciting topic for researchers, artists, and practitioners alike.",None,-1
0415ef53-a091-45d2-95f6-fe34f4f6ed27,Mimetic Initialization of Self-Attention Layers,0.61913,"It is notoriously difficult to train Transformers on small datasets;
typically, large pre-trained models are instead used as the starting point. We
explore the weights of such pre-trained Transformers (particularly for vision)
to attempt to find reasons for this discrepancy. Surprisingly, we find that
simply initializing the weights of self-attention layers so that they ""look""
more like their pre-trained counterparts allows us to train vanilla
Transformers faster and to higher final accuracies, particularly on vision
tasks such as CIFAR-10 and ImageNet classification, where we see gains in
accuracy of over 5% and 4%, respectively. Our initialization scheme is closed
form, learning-free, and very simple: we set the product of the query and key
weights to be approximately the identity, and the product of the value and
projection weights to approximately the negative identity. As this mimics the
patterns we saw in pre-trained Transformers, we call the technique ""mimetic
initialization"".",None,-1
e38a3229-fb58-49cc-963c-12c904d46b24,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,0.410679,"Societal biases present in pre-trained large language models are a critical
issue as these models have been shown to propagate biases in countless
downstream applications, rendering them unfair towards specific groups of
people. Since large-scale retraining of these models from scratch is both time
and compute-expensive, a variety of approaches have been previously proposed
that de-bias a pre-trained model. While the majority of current
state-of-the-art debiasing methods focus on changes to the training regime, in
this paper, we propose data intervention strategies as a powerful yet simple
technique to reduce gender bias in pre-trained models. Specifically, we
empirically show that by fine-tuning a pre-trained model on only 10 de-biased
(intervened) training examples, the tendency to favor any gender is
significantly reduced. Since our proposed method only needs a few training
examples, our few-shot debiasing approach is highly feasible and practical.
Through extensive experimentation, we show that our debiasing technique
performs better than competitive state-of-the-art baselines with minimal loss
in language modeling ability.",None,-1
368debd8-41aa-49c8-96bb-7261baec4d25,Efficient Monotonic Multihead Attention,0.213788,"We introduce the Efficient Monotonic Multihead Attention (EMMA), a
state-of-the-art simultaneous translation model with numerically-stable and
unbiased monotonic alignment estimation. In addition, we present improved
training and inference strategies, including simultaneous fine-tuning from an
offline translation model and reduction of monotonic alignment variance. The
experimental results demonstrate that the proposed model attains
state-of-the-art performance in simultaneous speech-to-text translation on the
Spanish and English translation task.",None,-1
0b34e9f0-e956-4c52-ace5-25cbbe02fb12,Entity-Based Evaluation of Political Bias in Automatic Summarization,0.096465,"Growing literature has shown that NLP systems may encode social biases;
however, the political bias of summarization models remains relatively unknown.
In this work, we use an entity replacement method to investigate the portrayal
of politicians in automatically generated summaries of news articles. We
develop an entity-based computational framework to assess the sensitivities of
several extractive and abstractive summarizers to the politicians Donald Trump
and Joe Biden. We find consistent differences in these summaries upon entity
replacement, such as reduced emphasis of Trump's presence in the context of the
same article and a more individualistic representation of Trump with respect to
the collective US government (i.e., administration). These summary
dissimilarities are most prominent when the entity is heavily featured in the
source article. Our characterization provides a foundation for future studies
of bias in summarization and for normative discussions on the ideal qualities
of automatic summaries.",None,-1
c9fc9946-a364-4787-9e41-87dd6d276b2f,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,0.870795,"Object affordance is an important concept in hand-object interaction,
providing information on action possibilities based on human motor capacity and
objects' physical property thus benefiting tasks such as action anticipation
and robot imitation learning. However, the definition of affordance in existing
datasets often: 1) mix up affordance with object functionality; 2) confuse
affordance with goal-related action; and 3) ignore human motor capacity. This
paper proposes an efficient annotation scheme to address these issues by
combining goal-irrelevant motor actions and grasp types as affordance labels
and introducing the concept of mechanical action to represent the action
possibilities between two objects. We provide new annotations by applying this
scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as
affordance recognition, hand-object interaction hotspots prediction, and
cross-domain evaluation of affordance. The results show that models trained
with our annotation can distinguish affordance from other concepts, predict
fine-grained interaction possibilities on objects, and generalize through
different domains.",None,-1
a5095ff5-cb8e-4d67-8b8e-7f4847e3b94b,Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design,0.66509,"Most of the current hypergraph learning methodologies and benchmarking
datasets in the hypergraph realm are obtained by lifting procedures from their
graph analogs, leading to overshadowing specific characteristics of
hypergraphs. This paper attempts to confront some pending questions in that
regard: Q1 Can the concept of homophily play a crucial role in Hypergraph
Neural Networks (HNNs)? Q2 Is there room for improving current HNN
architectures by carefully addressing specific characteristics of higher-order
networks? Q3 Do existing datasets provide a meaningful benchmark for HNNs? To
address them, we first introduce a novel conceptualization of homophily in
higher-order networks based on a Message Passing (MP) scheme, unifying both the
analytical examination and the modeling of higher-order networks. Further, we
investigate some natural, yet mostly unexplored, strategies for processing
higher-order structures within HNNs such as keeping hyperedge-dependent node
representations, or performing node/hyperedge stochastic samplings, leading us
to the most general MP formulation up to date -MultiSet-, as well as to an
original architecture design, MultiSetMixer. Finally, we conduct an extensive
set of experiments that contextualize our proposals and successfully provide
insights about our inquiries.",None,-1
0c3803a5-a66d-455b-ae51-3125c278666a,AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes,0.698791,"We propose attribute-aware multimodal entity linking, where the input is a
mention described with a text and image, and the goal is to predict the
corresponding target entity from a multimodal knowledge base (KB) where each
entity is also described with a text description, a visual image and a set of
attributes and values. To support this research, we construct AMELI, a
large-scale dataset consisting of 18,472 reviews and 35,598 products. To
establish baseline performance on AMELI, we experiment with the current
state-of-the-art multimodal entity linking approaches and our enhanced
attribute-aware model and demonstrate the importance of incorporating the
attribute information into the entity linking process. To be best of our
knowledge, we are the first to build benchmark dataset and solutions for the
attribute-aware multimodal entity linking task. Datasets and codes will be made
publicly available.",None,-1
f4052ae8-3c25-4c97-8da8-7f7931b6303a,A Blackbox Approach to Best of Both Worlds in Bandits and Beyond,0.631179,"Best-of-both-worlds algorithms for online learning which achieve near-optimal
regret in both the adversarial and the stochastic regimes have received growing
attention recently. Existing techniques often require careful adaptation to
every new problem setup, including specialised potentials and careful tuning of
algorithm parameters. Yet, in domains such as linear bandits, it is still
unknown if there exists an algorithm that can simultaneously obtain
$O(\log(T))$ regret in the stochastic regime and $\tilde{O}(\sqrt{T})$ regret
in the adversarial regime. In this work, we resolve this question positively
and present a general reduction from best of both worlds to a wide family of
follow-the-regularized-leader (FTRL) and online-mirror-descent (OMD)
algorithms. We showcase the capability of this reduction by transforming
existing algorithms that are only known to achieve worst-case guarantees into
new algorithms with best-of-both-worlds guarantees in contextual bandits, graph
bandits and tabular Markov decision processes.",None,-1
509fcb42-63ae-42ea-9fe5-d44647b6fc45,Token Prediction as Implicit Classification to Identify LLM-Generated Text,0.209022,"This paper introduces a novel approach for identifying the possible large
language models (LLMs) involved in text generation. Instead of adding an
additional classification layer to a base LM, we reframe the classification
task as a next-token prediction task and directly fine-tune the base LM to
perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the
backbone for our experiments. We compared our approach to the more direct
approach of utilizing hidden states for classification. Evaluation shows the
exceptional performance of our method in the text classification task,
highlighting its simplicity and efficiency. Furthermore, interpretability
studies on the features extracted by our model reveal its ability to
differentiate distinctive writing styles among various LLMs even in the absence
of an explicit classifier. We also collected a dataset named OpenLLMText,
containing approximately 340k text samples from human and LLMs, including
GPT3.5, PaLM, LLaMA, and GPT2.",None,-1
2034b00a-eb79-4131-9ab2-6c3a4c0762de,Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features,0.704495,"A challenge towards developing NLP systems for the world's languages is
understanding how they generalize to typological differences relevant for
real-world applications. To this end, we propose M2C, a morphologically-aware
framework for behavioral testing of NLP models. We use M2C to generate tests
that probe models' behavior in light of specific linguistic features in 12
typologically diverse languages. We evaluate state-of-the-art language models
on the generated tests. While models excel at most tests in English, we
highlight generalization failures to specific typological characteristics such
as temporal expressions in Swahili and compounding possessives in Finish. Our
findings motivate the development of models that address these blind spots.",None,-1
ba8f2c3c-4b8a-4f6a-bdf5-473f728934b0,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,0.6474,"Single Domain Generalization (SDG) tackles the problem of training a model on
a single source domain so that it generalizes to any unseen target domain.
While this has been well studied for image classification, the literature on
SDG object detection remains almost non-existent. To address the challenges of
simultaneously learning robust object localization and representation, we
propose to leverage a pre-trained vision-language model to introduce semantic
domain concepts via textual prompts. We achieve this via a semantic
augmentation strategy acting on the features extracted by the detector
backbone, as well as a text-based classification loss. Our experiments evidence
the benefits of our approach, outperforming by 10% the only existing SDG object
detection method, Single-DGOD [49], on their own diverse weather-driving
benchmark.",None,-1
496e7de2-f2dd-434d-b295-f7c383ef00af,Free Lunch for Efficient Textual Commonsense Integration in Language Models,0.118557,"Recent years have witnessed the emergence of textual commonsense knowledge
bases, aimed at providing more nuanced and context-rich knowledge. The
integration of external commonsense into language models has been shown to be a
key enabler in advancing the state-of-the-art for a wide range of NLP tasks.
However, incorporating textual commonsense descriptions is computationally
expensive, as compared to encoding conventional symbolic knowledge. In this
paper, we propose a method to improve its efficiency without modifying the
model. We group training samples with similar commonsense descriptions into a
single batch, thus reusing the encoded description across multiple samples. One
key observation is that the upper bound of batch partitioning can be reduced to
the classic {\it graph k-cut problem}. Consequently, we propose a spectral
clustering-based algorithm to solve this problem. Extensive experiments
illustrate that the proposed batch partitioning approach effectively reduces
the computational cost while preserving performance. The efficiency improvement
is more pronounced on larger datasets and on devices with more memory capacity,
attesting to its practical utility for large-scale applications.",None,-1
5ae77a8a-c8b4-40c2-95b1-fcc5169cccf6,Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System,0.342832,"End-to-end task-oriented dialogue (TOD) systems have achieved promising
performance by leveraging sophisticated natural language understanding and
natural language generation capabilities of pre-trained models. This work
enables the TOD systems with more flexibility through a simple cache. The cache
provides the flexibility to dynamically update the TOD systems and handle both
existing and unseen dialogue scenarios. Towards this end, we first fine-tune a
retrieval module to effectively retrieve the most relevant information entries
from the cache. We then train end-to-end TOD models that can refer to and
ground on both dialogue history and retrieved information during TOD
generation. The cache is straightforward to construct, and the backbone models
of TOD systems are compatible with existing pre-trained generative models.
Extensive experiments demonstrate the superior performance of our framework,
with a notable improvement in non-empty joint goal accuracy by 6.7% compared to
strong baselines.",None,-1
4a6793c4-7b28-4c52-a35d-8a84253f890e,KNSE: A Knowledge-aware Natural Language Inference Framework for Dialogue Symptom Status Recognition,0.694012,"Symptom diagnosis in medical conversations aims to correctly extract both
symptom entities and their status from the doctor-patient dialogue. In this
paper, we propose a novel framework called KNSE for symptom status recognition
(SSR), where the SSR is formulated as a natural language inference (NLI) task.
For each mentioned symptom in a dialogue window, we first generate knowledge
about the symptom and hypothesis about status of the symptom, to form a
(premise, knowledge, hypothesis) triplet. The BERT model is then used to encode
the triplet, which is further processed by modules including utterance
aggregation, self-attention, cross-attention, and GRU to predict the symptom
status. Benefiting from the NLI formalization, the proposed framework can
encode more informative prior knowledge to better localize and track symptom
status, which can effectively improve the performance of symptom status
recognition. Preliminary experiments on Chinese medical dialogue datasets show
that KNSE outperforms previous competitive baselines and has advantages in
cross-disease and cross-symptom scenarios.",None,-1
3c2d17f5-fc30-45a6-b8f3-d0ce725312b8,`It is currently hodgepodge'': Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values,0.801903,"Recently, the AI/ML research community has indicated an urgent need to
establish Responsible AI (RAI) values and practices as part of the AI/ML
lifecycle. Several organizations and communities are responding to this call by
sharing RAI guidelines. However, there are gaps in awareness, deliberation, and
execution of such practices for multi-disciplinary ML practitioners. This work
contributes to the discussion by unpacking co-production challenges faced by
practitioners as they align their RAI values. We interviewed 23 individuals,
across 10 organizations, tasked to ship AI/ML based products while upholding
RAI norms and found that both top-down and bottom-up institutional structures
create burden for different roles preventing them from upholding RAI values, a
challenge that is further exacerbated when executing conflicted values. We
share multiple value levers used as strategies by the practitioners to resolve
their challenges. We end our paper with recommendations for inclusive and
equitable RAI value-practices, creating supportive organizational structures
and opportunities to further aid practitioners.",None,-1
c3418bef-99c8-41c5-aef7-216a85b2bab9,"DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion",0.547524,"We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by
integrating cutting-edge diffusion models, which have revolutionized diverse
fields, but are relatively unexplored in 3D-HPE. We show that diffusion models
enhance the accuracy, robustness, and coherence of human pose estimations. We
introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE,
and demonstrate its ability to refine standard supervised 3D-HPE. We also show
how diffusion models lead to more robust estimations in the face of occlusions,
and improve the time-coherence and the sagittal symmetry of predictions. Using
the Human\,3.6M dataset, we illustrate the effectiveness of our approach and
its superiority over existing models, even under adverse situations where the
occlusion patterns in training do not match those in inference. Our findings
indicate that while standalone diffusion models provide commendable
performance, their accuracy is even better in combination with supervised
models, opening exciting new avenues for 3D-HPE research.",None,-1
61a292b4-d198-4506-952a-ec727286a361,A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe,0.431541,"One of the problems in quantitative finance that has received the most
attention is the portfolio optimization problem. Regarding its solving, this
problem has been approached using different techniques, with those related to
quantum computing being especially prolific in recent years. In this study, we
present a system called Quantum Computing-based System for Portfolio
Optimization with Future Asset Values and Automatic Universe Reduction
(Q4FuturePOP), which deals with the Portfolio Optimization Problem considering
the following innovations: i) the developed tool is modeled for working with
future prediction of assets, instead of historical values; and ii) Q4FuturePOP
includes an automatic universe reduction module, which is conceived to
intelligently reduce the complexity of the problem. We also introduce a brief
discussion about the preliminary performance of the different modules that
compose the prototypical version of Q4FuturePOP.",None,-1
53d6774a-ba3e-4f42-893d-9c9c1bacf76d,Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature,0.176595,"Scholarly articles in mathematical fields feature mathematical statements
such as theorems, propositions, etc., as well as their proofs. Extracting them
from the PDF representation of the articles requires understanding of
scientific text along with visual and font-based indicators. We pose this
problem as a multimodal classification problem using text, font features, and
bitmap image rendering of the PDF as different modalities. In this paper we
propose a multimodal machine learning approach for extraction of theorem-like
environments and proofs, based on late fusion of features extracted by
individual unimodal classifiers, taking into account the sequential succession
of blocks in the document. For the text modality, we pretrain a new language
model on a 11 GB scientific corpus; experiments shows similar performance for
our task than a model (RoBERTa) pretrained on 160 GB, with faster convergence
while requiring much less fine-tuning data. Font-based information relies on
training a 128-cell LSTM on the sequence of font names and sizes within each
block. Bitmap renderings are dealt with using an EfficientNetv2 deep network
tuned to classify each image block. Finally, a simple CRF-based approach uses
the features of the multimodal model along with information on block sequences.
Experimental results show the benefits of using a multimodal approach vs any
single modality, as well as major performance improvements using the CRF
modeling of block sequences.",None,-1
15081636-ad1e-4f58-b667-84d1b7209949,Large Language Models can accomplish Business Process Management Tasks,0.988452,"Business Process Management (BPM) aims to improve organizational activities
and their outcomes by managing the underlying processes. To achieve this, it is
often necessary to consider information from various sources, including
unstructured textual documents. Therefore, researchers have developed several
BPM-specific solutions that extract information from textual documents using
Natural Language Processing techniques. These solutions are specific to their
respective tasks and cannot accomplish multiple process-related problems as a
general-purpose instrument. However, in light of the recent emergence of Large
Language Models (LLMs) with remarkable reasoning capabilities, such a
general-purpose instrument with multiple applications now appears attainable.
In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by
applying a specific LLM to three exemplary tasks: mining imperative process
models from textual descriptions, mining declarative process models from
textual descriptions, and assessing the suitability of process tasks from
textual descriptions for robotic process automation. We show that, without
extensive configuration or prompt engineering, LLMs perform comparably to or
better than existing solutions and discuss implications for future BPM research
as well as practical usage.",None,-1
da58a7f5-41c2-41c8-8c31-a3055f2a2d9c,C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT,0.100268,"Large language models (LLMs), such as ChatGPT, have demonstrated outstanding
performance in various fields, particularly in natural language understanding
and generation tasks. In complex application scenarios, users tend to engage in
multi-turn conversations with ChatGPT to keep contextual information and obtain
comprehensive responses. However, human forgetting and model contextual
forgetting remain prominent issues in multi-turn conversation scenarios, which
challenge the users' conversation comprehension and contextual continuity for
ChatGPT. To address these challenges, we propose an interactive conversation
visualization system called C5, which includes Global View, Topic View, and
Context-associated Q\&A View. The Global View uses the GitLog diagram metaphor
to represent the conversation structure, presenting the trend of conversation
evolution and supporting the exploration of locally salient features. The Topic
View is designed to display all the question and answer nodes and their
relationships within a topic using the structure of a knowledge graph, thereby
display the relevance and evolution of conversations. The Context-associated
Q\&A View consists of three linked views, which allow users to explore
individual conversations deeply while providing specific contextual information
when posing questions. The usefulness and effectiveness of C5 were evaluated
through a case study and a user study.",None,-1
55f9ff49-13d3-4074-9fc8-89df4f4b4e75,Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data,0.424263,"Chatbots have become popular in educational settings, revolutionizing how
students interact with material and how teachers teach. We present
Curriculum-Driven EduBot, a framework for developing a chatbot that combines
the interactive features of chatbots with the systematic material of English
textbooks to assist students in enhancing their conversational skills. We begin
by extracting pertinent topics from textbooks and then using large language
models to generate dialogues related to these topics. We then fine-tune an
open-source LLM using our generated conversational data to create our
curriculum-driven chatbot. User studies demonstrate that our chatbot
outperforms ChatGPT in leading curriculum-based dialogues and adapting its
dialogue to match the user's English proficiency level. By combining
traditional textbook methodologies with conversational AI, our approach offers
learners an interactive tool that aligns with their curriculum and provides
user-tailored conversation practice. This facilitates meaningful student-bot
dialogues and enriches the overall learning experience within the curriculum's
pedagogical framework.",None,-1
f86d11ad-7121-4535-8b55-0817b9ef05cd,High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model,0.485557,"We propose a method to fit arbitrarily accurate blendshape rig models by
solving the inverse rig problem in realistic human face animation. The method
considers blendshape models with different levels of added corrections and
solves the regularized least-squares problem using coordinate descent, i.e.,
iteratively estimating blendshape weights. Besides making the optimization
easier to solve, this approach ensures that mutually exclusive controllers will
not be activated simultaneously and improves the goodness of fit after each
iteration. We show experimentally that the proposed method yields solutions
with mesh error comparable to or lower than the state-of-the-art approaches
while significantly reducing the cardinality of the weight vector (over 20
percent), hence giving a high-fidelity reconstruction of the reference
expression that is easier to manipulate in the post-production manually. Python
scripts for the algorithm will be publicly available upon acceptance of the
paper.",None,-1
d6a92e4f-eed0-4e6f-b79e-b868a78abaf0,Mask Detection and Classification in Thermal Face Images,0.581183,"Face masks are recommended to reduce the transmission of many viruses,
especially SARS-CoV-2. Therefore, the automatic detection of whether there is a
mask on the face, what type of mask is worn, and how it is worn is an important
research topic. In this work, the use of thermal imaging was considered to
analyze the possibility of detecting (localizing) a mask on the face, as well
as to check whether it is possible to classify the type of mask on the face.
The previously proposed dataset of thermal images was extended and annotated
with the description of a type of mask and a location of a mask within a face.
Different deep learning models were adapted. The best model for face mask
detection turned out to be the Yolov5 model in the ""nano"" version, reaching mAP
higher than 97% and precision of about 95%. High accuracy was also obtained for
mask type classification. The best results were obtained for the convolutional
neural network model built on an autoencoder initially trained in the thermal
image reconstruction problem. The pretrained encoder was used to train a
classifier which achieved an accuracy of 91%.",None,-1
5ed76f81-812c-47c6-9b9c-81913a362f77,DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection,0.770375,"The increasingly pervasive facial recognition (FR) systems raise serious
concerns about personal privacy, especially for billions of users who have
publicly shared their photos on social media. Several attempts have been made
to protect individuals from being identified by unauthorized FR systems
utilizing adversarial attacks to generate encrypted face images. However,
existing methods suffer from poor visual quality or low attack success rates,
which limit their utility. Recently, diffusion models have achieved tremendous
success in image generation. In this work, we ask: can diffusion models be used
to generate adversarial examples to improve both visual quality and attack
performance? We propose DiffProtect, which utilizes a diffusion autoencoder to
generate semantically meaningful perturbations on FR systems. Extensive
experiments demonstrate that DiffProtect produces more natural-looking
encrypted images than state-of-the-art methods while achieving significantly
higher attack success rates, e.g., 24.5% and 25.1% absolute improvements on the
CelebA-HQ and FFHQ datasets.",None,-1
90da6beb-7480-412b-8f17-0f045507ad3b,Improved Trajectory Reconstruction for Markerless Pose Estimation,0.659436,"Markerless pose estimation allows reconstructing human movement from multiple
synchronized and calibrated views, and has the potential to make movement
analysis easy and quick, including gait analysis. This could enable much more
frequent and quantitative characterization of gait impairments, allowing better
monitoring of outcomes and responses to interventions. However, the impact of
different keypoint detectors and reconstruction algorithms on markerless pose
estimation accuracy has not been thoroughly evaluated. We tested these
algorithmic choices on data acquired from a multicamera system from a
heterogeneous sample of 25 individuals seen in a rehabilitation hospital. We
found that using a top-down keypoint detector and reconstructing trajectories
with an implicit function enabled accurate, smooth and anatomically plausible
trajectories, with a noise in the step width estimates compared to a GaitRite
walkway of only 8mm.",None,-1
d4fc836b-0d92-4ccc-a7dd-59b235e1aceb,Unsupervised Discovery of Semantic Latent Directions in Diffusion Models,0.206324,"Despite the success of diffusion models (DMs), we still lack a thorough
understanding of their latent space. While image editing with GANs builds upon
latent space, DMs rely on editing the conditions such as text prompts. We
present an unsupervised method to discover interpretable editing directions for
the latent variables $\mathbf{x}_t \in \mathcal{X}$ of DMs. Our method adopts
Riemannian geometry between $\mathcal{X}$ and the intermediate feature maps
$\mathcal{H}$ of the U-Nets to provide a deep understanding over the
geometrical structure of $\mathcal{X}$. The discovered semantic latent
directions mostly yield disentangled attribute changes, and they are globally
consistent across different samples. Furthermore, editing in earlier timesteps
edits coarse attributes, while ones in later timesteps focus on high-frequency
details. We define the curvedness of a line segment between samples to show
that $\mathcal{X}$ is a curved manifold. Experiments on different baselines and
datasets demonstrate the effectiveness of our method even on Stable Diffusion.
Our source code will be publicly available for the future researchers.",None,-1
bbc536b0-3c5b-4020-b915-7d7b1078a113,Contextual Object Detection with Multimodal Large Language Models,0.855345,"Recent Multimodal Large Language Models (MLLMs) are remarkable in
vision-language tasks, such as image captioning and question answering, but
lack the essential perception ability, i.e., object detection. In this work, we
address this limitation by introducing a novel research problem of contextual
object detection -- understanding visible objects within different human-AI
interactive contexts. Three representative scenarios are investigated,
including the language cloze test, visual captioning, and question answering.
Moreover, we present ContextDET, a unified multimodal model that is capable of
end-to-end differentiable modeling of visual-language contexts, so as to
locate, identify, and associate visual objects with language inputs for
human-AI interaction. Our ContextDET involves three key submodels: (i) a visual
encoder for extracting visual representations, (ii) a pre-trained LLM for
multimodal context decoding, and (iii) a visual decoder for predicting bounding
boxes given contextual object words. The new generate-then-detect framework
enables us to detect object words within human vocabulary. Extensive
experiments show the advantages of ContextDET on our proposed CODE benchmark,
open-vocabulary detection, and referring image segmentation. Github:
https://github.com/yuhangzang/ContextDET.",None,-1
113ee002-c6fe-47c0-a3d6-1132cfda40b6,Efficient neural supersampling on a novel gaming dataset,0.117614,"Real-time rendering for video games has become increasingly challenging due
to the need for higher resolutions, framerates and photorealism. Supersampling
has emerged as an effective solution to address this challenge. Our work
introduces a novel neural algorithm for supersampling rendered content that is
4 times more efficient than existing methods while maintaining the same level
of accuracy. Additionally, we introduce a new dataset which provides auxiliary
modalities such as motion vectors and depth generated using graphics rendering
features like viewport jittering and mipmap biasing at different resolutions.
We believe that this dataset fills a gap in the current dataset landscape and
can serve as a valuable resource to help measure progress in the field and
advance the state-of-the-art in super-resolution techniques for gaming content.",None,-1
4259f9e9-93e4-4ba0-9666-8473003a3f72,WYWEB: A NLP Evaluation Benchmark For Classical Chinese,0.649107,"To fully evaluate the overall performance of different NLP models in a given
domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and
CLUE. The fi eld of natural language understanding has traditionally focused on
benchmarks for various tasks in languages such as Chinese, English, and
multilingua, however, there has been a lack of attention given to the area of
classical Chinese, also known as ""wen yan wen"", which has a rich history
spanning thousands of years and holds signifi cant cultural and academic value.
For the prosperity of the NLP community, in this paper, we introduce the WYWEB
evaluation benchmark, which consists of nine NLP tasks in classical Chinese,
implementing sentence classifi cation, sequence labeling, reading
comprehension, and machine translation. We evaluate the existing pre-trained
language models, which are all struggling with this benchmark. We also
introduce a number of supplementary datasets and additional tools to help
facilitate further progress on classical Chinese NLU. The github repository is
https://github.com/baudzhou/WYWEB.",None,-1
7fd74d9e-7b6a-4a0e-b53c-8bab72038578,Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,0.500376,"Large Language Models (LLMs) have demonstrated exceptional natural language
understanding abilities and have excelled in a variety of natural language
processing (NLP)tasks in recent years. Despite the fact that most LLMs are
trained predominantly in English, multiple studies have demonstrated their
comparative performance in many other languages. However, fundamental questions
persist regarding how LLMs acquire their multi-lingual abilities and how
performance varies across different languages. These inquiries are crucial for
the study of LLMs since users and researchers often come from diverse language
backgrounds, potentially influencing their utilization and interpretation of
LLMs' results. In this work, we propose a systematic way of qualifying the
performance disparities of LLMs under multilingual settings. We investigate the
phenomenon of across-language generalizations in LLMs, wherein insufficient
multi-lingual training data leads to advanced multi-lingual capabilities. To
accomplish this, we employ a novel back-translation-based prompting method. The
results show that GPT exhibits highly translating-like behaviour in
multilingual settings.",None,-1
8b39ac8d-a365-4513-ad16-d7e127e1a6b1,Attention-based Spatial-Temporal Graph Convolutional Recurrent Networks for Traffic Forecasting,0.313696,"Traffic forecasting is one of the most fundamental problems in transportation
science and artificial intelligence. The key challenge is to effectively model
complex spatial-temporal dependencies and correlations in modern traffic data.
Existing methods, however, cannot accurately model both long-term and
short-term temporal correlations simultaneously, limiting their expressive
power on complex spatial-temporal patterns. In this paper, we propose a novel
spatial-temporal neural network framework: Attention-based Spatial-Temporal
Graph Convolutional Recurrent Network (ASTGCRN), which consists of a graph
convolutional recurrent module (GCRN) and a global attention module. In
particular, GCRN integrates gated recurrent units and adaptive graph
convolutional networks for dynamically learning graph structures and capturing
spatial dependencies and local temporal relationships. To effectively extract
global temporal dependencies, we design a temporal attention layer and
implement it as three independent modules based on multi-head self-attention,
transformer, and informer respectively. Extensive experiments on five real
traffic datasets have demonstrated the excellent predictive performance of all
our three models with all their average MAE, RMSE and MAPE across the test
datasets lower than the baseline methods.",None,-1
c1fa4d11-43af-4efd-a0be-fc6b692ac8e2,F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories,0.987084,"This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)
for novel view synthesis, which enables arbitrary input camera trajectories and
only costs a few minutes for training. Existing fast grid-based NeRF training
frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed
for bounded scenes and rely on space warping to handle unbounded scenes.
Existing two widely-used space-warping methods are only designed for the
forward-facing trajectory or the 360-degree object-centric trajectory but
cannot process arbitrary trajectories. In this paper, we delve deep into the
mechanism of space warping to handle unbounded scenes. Based on our analysis,
we further propose a novel space-warping method called perspective warping,
which allows us to handle arbitrary trajectories in the grid-based NeRF
framework. Extensive experiments demonstrate that F2-NeRF is able to use the
same perspective warping to render high-quality images on two standard datasets
and a new free trajectory dataset collected by us. Project page:
https://totoro97.github.io/projects/f2-nerf.",None,-1
54fb37cc-602e-4149-84ca-b30019b40567,Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study,0.098361,"Text classification is an area of research which has been studied over the
years in Natural Language Processing (NLP). Adapting NLP to multiple domains
has introduced many new challenges for text classification and one of them is
long document classification. While state-of-the-art transformer models provide
excellent results in text classification, most of them have limitations in the
maximum sequence length of the input sequence. The majority of the transformer
models are limited to 512 tokens, and therefore, they struggle with long
document classification problems. In this research, we explore on employing
Model Fusing for long document classification while comparing the results with
well-known BERT and Longformer architectures.",None,-1
b61936a8-de49-4e2e-9563-e4f1c16e0494,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,0.604992,"Current popular backbones in computer vision, such as Vision Transformers
(ViT) and ResNets are trained to perceive the world from 2D images. However, to
more effectively understand 3D structural priors in 2D backbones, we propose
Mask3D to leverage existing large-scale RGB-D data in a self-supervised
pre-training to embed these 3D priors into 2D learned feature representations.
In contrast to traditional 3D contrastive learning paradigms requiring 3D
reconstructions or multi-view correspondences, our approach is simple: we
formulate a pre-text reconstruction task by masking RGB and depth patches in
individual RGB-D frames. We demonstrate the Mask3D is particularly effective in
embedding 3D priors into the powerful 2D ViT backbone, enabling improved
representation learning for various scene understanding tasks, such as semantic
segmentation, instance segmentation and object detection. Experiments show that
Mask3D notably outperforms existing self-supervised 3D pre-training approaches
on ScanNet, NYUv2, and Cityscapes image understanding tasks, with an
improvement of +6.5% mIoU against the state-of-the-art Pri3D on ScanNet image
semantic segmentation.",None,-1
12e3faa6-06ce-4d2b-8701-54bd9476c8a5,Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting,0.772749,"Automatically generated reports from medical images promise to improve the
workflow of radiologists. Existing methods consider an image-to-report modeling
task by directly generating a fully-fledged report from an image. However, this
conflates the content of the report (e.g., findings and their attributes) with
its style (e.g., format and choice of words), which can lead to clinically
inaccurate reports. To address this, we propose a two-step approach for
radiology report generation. First, we extract the content from an image; then,
we verbalize the extracted content into a report that matches the style of a
specific radiologist. For this, we leverage RadGraph -- a graph representation
of reports -- together with large language models (LLMs). In our quantitative
evaluations, we find that our approach leads to beneficial performance. Our
human evaluation with clinical raters highlights that the AI-generated reports
are indistinguishably tailored to the style of individual radiologist despite
leveraging only a few examples as context.",None,-1
7c96c149-eaec-414e-80c8-a29508c98eb3,Contrastive Feature Masking Open-Vocabulary Vision Transformer,0.908116,"We present Contrastive Feature Masking Vision Transformer (CFM-ViT) - an
image-text pretraining methodology that achieves simultaneous learning of
image- and region-level representation for open-vocabulary object detection
(OVD). Our approach combines the masked autoencoder (MAE) objective into the
contrastive learning objective to improve the representation for localization
tasks. Unlike standard MAE, we perform reconstruction in the joint image-text
embedding space, rather than the pixel space as is customary with the classical
MAE method, which causes the model to better learn region-level semantics.
Moreover, we introduce Positional Embedding Dropout (PED) to address scale
variation between image-text pretraining and detection finetuning by randomly
dropping out the positional embeddings during pretraining. PED improves
detection performance and enables the use of a frozen ViT backbone as a region
classifier, preventing the forgetting of open-vocabulary knowledge during
detection finetuning. On LVIS open-vocabulary detection benchmark, CFM-ViT
achieves a state-of-the-art 33.9 AP$r$, surpassing the best approach by 7.6
points and achieves better zero-shot detection transfer. Finally, CFM-ViT
acquires strong image-level representation, outperforming the state of the art
on 8 out of 12 metrics on zero-shot image-text retrieval benchmarks.",None,-1
1ee4f7f6-ab59-4639-8cad-2794acbd709a,MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer,0.769474,"With the increasing demand for video understanding, video moment and
highlight detection (MHD) has emerged as a critical research topic. MHD aims to
localize all moments and predict clip-wise saliency scores simultaneously.
Despite progress made by existing DETR-based methods, we observe that these
methods coarsely fuse features from different modalities, which weakens the
temporal intra-modal context and results in insufficient cross-modal
interaction. To address this issue, we propose MH-DETR (Moment and Highlight
Detection Transformer) tailored for MHD. Specifically, we introduce a simple
yet efficient pooling operator within the uni-modal encoder to capture global
intra-modal context. Moreover, to obtain temporally aligned cross-modal
features, we design a plug-and-play cross-modal interaction module between the
encoder and decoder, seamlessly integrating visual and textual features.
Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and
TVSum datasets show that MH-DETR outperforms existing state-of-the-art methods,
demonstrating its effectiveness and superiority. Our code is available at
https://github.com/YoucanBaby/MH-DETR.",None,-1
bf1e83bb-842b-4bf6-ba85-a85e46585d68,Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data,0.611651,"This study proposes a novel heterogeneous graph convolutional neural network
(HGCNN) to handle complex brain fMRI data at regional and across-region levels.
We introduce a generic formulation of spectral filters on heterogeneous graphs
by introducing the $k-th$ Hodge-Laplacian (HL) operator. In particular, we
propose Laguerre polynomial approximations of HL spectral filters and prove
that their spatial localization on graphs is related to the polynomial order.
Furthermore, based on the bijection property of boundary operators on simplex
graphs, we introduce a generic topological graph pooling (TGPool) method that
can be used at any dimensional simplices. This study designs HL-node, HL-edge,
and HL-HGCNN neural networks to learn signal representation at a graph node,
edge levels, and both, respectively. Our experiments employ fMRI from the
Adolescent Brain Cognitive Development (ABCD; n=7693) to predict general
intelligence. Our results demonstrate the advantage of the HL-edge network over
the HL-node network when functional brain connectivity is considered as
features. The HL-HGCNN outperforms the state-of-the-art graph neural networks
(GNNs) approaches, such as GAT, BrainGNN, dGCN, BrainNetCNN, and Hypergraph NN.
The functional connectivity features learned from the HL-HGCNN are meaningful
in interpreting neural circuits related to general intelligence.",None,-1
6237f815-ac4f-4acb-9f3f-21a06de85a19,Message Ritual: A Posthuman Account of Living with Lamp,0.534128,"As we become increasingly entangled with digital technologies, the boundary
between human and machine is progressively blurring. Adopting a performative,
posthumanist perspective resolves this ambiguity by proposing that such
boundaries are not predetermined, rather they are enacted within a certain
material configuration. Using this approach, dubbed `Entanglement HCI', this
paper presents \emph{Message Ritual} -- a novel, integrated AI system that
encourages the re-framing of memory through machine generated poetics. Embodied
within a domestic table lamp, the system listens in on conversations occurring
within the home, drawing out key topics and phrases of the day and
reconstituting them through machine generated poetry, delivered to household
members via SMS upon waking each morning. Participants across four households
were asked to live with the lamp over a two week period. We present a
diffractive analysis exploring how the lamp \emph{becomes with} participants
and discuss the implications of this method for future HCI research.",None,-1
4d827d46-820c-4a3a-ac34-e3d3c8ad7a25,Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity,0.736049,"A widespread view is that Artificial Intelligence cannot be creative. We
tested this assumption by comparing human-generated ideas with those generated
by six Generative Artificial Intelligence (GAI) chatbots: $alpa.\!ai$,
$Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and YouChat. Humans and
a specifically trained AI independently assessed the quality and quantity of
ideas. We found no qualitative difference between AI and human-generated
creativity, although there are differences in how ideas are generated.
Interestingly, 9.4 percent of humans were more creative than the most creative
GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the
creative process. Continued research and development of GAI in creative tasks
is crucial to fully understand this technology's potential benefits and
drawbacks in shaping the future of creativity. Finally, we discuss the question
of whether GAIs are capable of being truly creative.",None,-1
80f394af-22cb-4571-90ef-98793ded3d43,JSEEGraph: Joint Structured Event Extraction as Graph Parsing,0.659386,"We propose a graph-based event extraction framework JSEEGraph that approaches
the task of event extraction as general graph parsing in the tradition of
Meaning Representation Parsing. It explicitly encodes entities and events in a
single semantic graph, and further has the flexibility to encode a wider range
of additional IE relations and jointly infer individual tasks. JSEEGraph
performs in an end-to-end manner via general graph parsing: (1) instead of flat
sequence labelling, nested structures between entities/triggers are efficiently
encoded as separate nodes in the graph, allowing for nested and overlapping
entities and triggers; (2) both entities, relations, and events can be encoded
in the same graph, where entities and event triggers are represented as nodes
and entity relations and event arguments are constructed via edges; (3) joint
inference avoids error propagation and enhances the interpolation of different
IE tasks. We experiment on two benchmark datasets of varying structural
complexities; ACE05 and Rich ERE, covering three languages: English, Chinese,
and Spanish. Experimental results show that JSEEGraph can handle nested event
structures, that it is beneficial to solve different IE tasks jointly, and that
event argument extraction in particular benefits from entity extraction. Our
code and models are released as open-source.",None,-1
64585e76-3e1d-4b5b-8831-5e04d4fde741,ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,0.278232,"We address a weakly-supervised low-shot instance segmentation, an
annotation-efficient training method to deal with novel classes effectively.
Since it is an under-explored problem, we first investigate the difficulty of
the problem and identify the performance bottleneck by conducting systematic
analyses of model components and individual sub-tasks with a simple baseline
model. Based on the analyses, we propose ENInst with sub-task enhancement
methods: instance-wise mask refinement for enhancing pixel localization quality
and novel classifier composition for improving classification accuracy. Our
proposed method lifts the overall performance by enhancing the performance of
each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in
achieving comparable performance to the existing fully-supervised few-shot
models and even outperforms them at times.",None,-1
d933e028-9874-47c6-942e-1db42d375a72,PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking,0.999102,"We introduce PointOdyssey, a large-scale synthetic dataset, and data
generation framework, for the training and evaluation of long-term fine-grained
tracking algorithms. Our goal is to advance the state-of-the-art by placing
emphasis on long videos with naturalistic motion. Toward the goal of
naturalism, we animate deformable characters using real-world motion capture
data, we build 3D scenes to match the motion capture environments, and we
render camera viewpoints using trajectories mined via structure-from-motion on
real videos. We create combinatorial diversity by randomizing character
appearance, motion profiles, materials, lighting, 3D assets, and atmospheric
effects. Our dataset currently includes 104 videos, averaging 2,000 frames
long, with orders of magnitude more correspondence annotations than prior work.
We show that existing methods can be trained from scratch in our dataset and
outperform the published variants. Finally, we introduce modifications to the
PIPs point tracking method, greatly widening its temporal receptive field,
which improves its performance on PointOdyssey as well as on two real-world
benchmarks. Our data and code are publicly available at:
https://pointodyssey.com",None,-1
1b90318a-d03b-4ef0-973d-bedb64181a69,REFinD: Relation Extraction Financial Dataset,0.722532,"A number of datasets for Relation Extraction (RE) have been created to aide
downstream tasks such as information retrieval, semantic search, question
answering and textual entailment. However, these datasets fail to capture
financial-domain specific challenges since most of these datasets are compiled
using general knowledge sources such as Wikipedia, web-based text and news
articles, hindering real-life progress and adoption within the financial world.
To address this limitation, we propose REFinD, the first large-scale annotated
dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types
of entity pairs, generated entirely over financial documents. We also provide
an empirical evaluation with various state-of-the-art models as benchmarks for
the RE task and highlight the challenges posed by our dataset. We observed that
various state-of-the-art deep learning models struggle with numeric inference,
relational and directional ambiguity.",None,-1
4145ff07-ffac-4f39-88ec-0470268844fa,UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering,0.784189,"In this technical report, we present our solution, named UniOCC, for the
Vision-Centric 3D occupancy prediction track in the nuScenes Open Dataset
Challenge at CVPR 2023. Existing methods for occupancy prediction primarily
focus on optimizing projected features on 3D volume space using 3D occupancy
labels. However, the generation process of these labels is complex and
expensive (relying on 3D semantic annotations), and limited by voxel
resolution, they cannot provide fine-grained spatial semantics. To address this
limitation, we propose a novel Unifying Occupancy (UniOcc) prediction method,
explicitly imposing spatial geometry constraint and complementing fine-grained
semantic supervision through volume ray rendering. Our method significantly
enhances model performance and demonstrates promising potential in reducing
human annotation costs. Given the laborious nature of annotating 3D occupancy,
we further introduce a Depth-aware Teacher Student (DTS) framework to enhance
prediction accuracy using unlabeled data. Our solution achieves 51.27\% mIoU on
the official leaderboard with single model, placing 3rd in this challenge.",None,-1
57c82be7-1a2f-4817-a7ce-05097f58dfe7,Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning,0.210933,"We propose Pgx, a suite of board game reinforcement learning (RL)
environments written in JAX and optimized for GPU/TPU accelerators. By
leveraging JAX's auto-vectorization and parallelization over accelerators, Pgx
can efficiently scale to thousands of simultaneous simulations over
accelerators. In our experiments on a DGX-A100 workstation, we discovered that
Pgx can simulate RL environments 10-100x faster than existing implementations
available in Python. Pgx includes RL environments commonly used as benchmarks
in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx
offers miniature game sets and baseline models to facilitate rapid research
cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm
with Pgx environments. Overall, Pgx provides high-performance environment
simulators for researchers to accelerate their RL experiments. Pgx is available
at http://github.com/sotetsuk/pgx.",None,-1
43642c09-dddc-492d-8e31-1a7aa7a8faf4,Open-TI: Open Traffic Intelligence with Augmented Language Model,0.791498,"Transportation has greatly benefited the cities' development in the modern
civilization process. Intelligent transportation, leveraging advanced computer
algorithms, could further increase people's daily commuting efficiency.
However, intelligent transportation, as a cross-discipline, often requires
practitioners to comprehend complicated algorithms and obscure neural networks,
bringing a challenge for the advanced techniques to be trusted and deployed in
practical industries. Recognizing the expressiveness of the pre-trained large
language models, especially the potential of being augmented with abilities to
understand and execute intricate commands, we introduce Open-TI. Serving as a
bridge to mitigate the industry-academic gap, Open-TI is an innovative model
targeting the goal of Turing Indistinguishable Traffic Intelligence, it is
augmented with the capability to harness external traffic analysis packages
based on existing conversations. Marking its distinction, Open-TI is the first
method capable of conducting exhaustive traffic analysis from scratch -
spanning from map data acquisition to the eventual execution in complex
simulations. Besides, Open-TI is able to conduct task-specific embodiment like
training and adapting the traffic signal control policies (TSC), explore demand
optimizations, etc. Furthermore, we explored the viability of LLMs directly
serving as control agents, by understanding the expected intentions from
Open-TI, we designed an agent-to-agent communication mode to support Open-TI
conveying messages to ChatZero (control agent), and then the control agent
would choose from the action space to proceed the execution. We eventually
provide the formal implementation structure, and the open-ended design invites
further community-driven enhancements.",None,-1
29206abf-00d5-40f2-9017-e1524d52cdd6,"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",0.919375,"Large language models (LLMs) demonstrate impressive reasoning abilities, but
translating reasoning into actions in the real world remains challenging. In
particular, it remains unclear how to complete a given task provably within a
minimum number of interactions with the external environment, e.g., through an
internal mechanism of reasoning. To this end, we propose a principled framework
with provable regret guarantees to orchestrate reasoning and acting, which we
call ""reason for future, act for now"" (\texttt{RAFA}). Specifically, we design
a prompt template for reasoning that learns from the memory buffer and plans a
future trajectory over a long horizon (""reason for future""). At each step, the
LLM agent takes the initial action of the planned trajectory (""act for now""),
stores the collected feedback in the memory buffer, and reinvokes the reasoning
routine to replan the future trajectory from the new state.
  The key idea is to cast reasoning in LLMs as learning and planning in
Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt
LLMs to form an updated posterior of the unknown environment from the memory
buffer (learning) and generate an optimal trajectory for multiple future steps
that maximizes a value function (planning). The learning and planning
subroutines are performed in an ""in-context"" manner to emulate the actor-critic
update for MDPs. Our theoretical analysis proves that the novel combination of
long-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. Here,
$T$ denotes the number of online interactions. In particular, the regret bound
highlights an intriguing interplay between the prior knowledge obtained through
pretraining and the uncertainty reduction achieved by reasoning and acting. Our
empirical validation shows that it outperforms various existing frameworks and
achieves nearly perfect scores on a few benchmarks.",None,-1
14a70377-24b8-45da-8ed5-627b9bb02fc2,Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words,0.0815854,"The performance of sentence encoders can be significantly improved through
the simple practice of fine-tuning using contrastive loss. A natural question
arises: what characteristics do models acquire during contrastive learning?
This paper theoretically and experimentally shows that contrastive-based
sentence encoders implicitly weight words based on information-theoretic
quantities; that is, more informative words receive greater weight, while
others receive less. The theory states that, in the lower bound of the optimal
value of the contrastive learning objective, the norm of word embedding
reflects the information gain associated with the distribution of surrounding
words. We also conduct comprehensive experiments using various models, multiple
datasets, two methods to measure the implicit weighting of models (Integrated
Gradients and SHAP), and two information-theoretic quantities (information gain
and self-information). The results provide empirical evidence that contrastive
fine-tuning emphasizes informative words.",None,-1
47e47774-1b0c-435d-a9cd-2571d1fae989,Founder-GPT: Self-play to evaluate the Founder-Idea fit,0.0330701,"This research introduces an innovative evaluation method for the
""founder-idea"" fit in early-stage startups, utilizing advanced large language
model techniques to assess founders' profiles against their startup ideas to
enhance decision-making. Embeddings, self-play, tree-of-thought, and
critique-based refinement techniques show early promising results that each
idea's success patterns are unique and they should be evaluated based on the
context of the founder's background.",None,-1
848d00bf-150a-4588-8ba1-45d71d5471d2,Real-Time Simultaneous Localization and Mapping with LiDAR intensity,0.582636,"We propose a novel real-time LiDAR intensity image-based simultaneous
localization and mapping method , which addresses the geometry degeneracy
problem in unstructured environments. Traditional LiDAR-based front-end
odometry mostly relies on geometric features such as points, lines and planes.
A lack of these features in the environment can lead to the failure of the
entire odometry system. To avoid this problem, we extract feature points from
the LiDAR-generated point cloud that match features identified in LiDAR
intensity images. We then use the extracted feature points to perform scan
registration and estimate the robot ego-movement. For the back-end, we jointly
optimize the distance between the corresponding feature points, and the point
to plane distance for planes identified in the map. In addition, we use the
features extracted from intensity images to detect loop closure candidates from
previous scans and perform pose graph optimization. Our experiments show that
our method can run in real time with high accuracy and works well with
illumination changes, low-texture, and unstructured environments.",None,-1
78029f87-0e3c-4260-a95e-86a5c9d6d29b,Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation,0.526661,"Interpretability and efficiency are two important considerations for the
adoption of neural automatic metrics. In this work, we develop
strong-performing automatic metrics for reference-based summarization
evaluation, based on a two-stage evaluation pipeline that first extracts basic
information units from one text sequence and then checks the extracted units in
another sequence. The metrics we developed include two-stage metrics that can
provide high interpretability at both the fine-grained unit level and summary
level, and one-stage metrics that achieve a balance between efficiency and
interpretability. We make the developed tools publicly available at
https://github.com/Yale-LILY/AutoACU.",None,-1
54f7f99e-7e4e-4008-a7ab-8949e38c806c,Neural Machine Translation Models Can Learn to be Few-shot Learners,0.301717,"The emergent ability of Large Language Models to use a small number of
examples to learn to perform in novel domains and tasks, also called in-context
learning (ICL). In this work, we show that a much smaller model can be trained
to perform ICL by fine-tuning towards a specialized training objective,
exemplified on the task of domain adaptation for neural machine translation.
With this capacity for ICL, the model can take advantage of relevant few-shot
examples to adapt its output towards the domain. We compare the quality of this
domain adaptation to traditional supervised techniques and ICL with a
40B-parameter Large Language Model. Our approach allows efficient batch
inference on a mix of domains and outperforms state-of-the-art baselines in
terms of both translation quality and immediate adaptation rate, i.e. the
ability to reproduce a specific term after being shown a single example.",None,-1
455cc4d7-e010-4e85-8e35-6054bb6180e3,A Fully First-Order Method for Stochastic Bilevel Optimization,0.978149,"We consider stochastic unconstrained bilevel optimization problems when only
the first-order gradient oracles are available. While numerous optimization
methods have been proposed for tackling bilevel problems, existing methods
either tend to require possibly expensive calculations regarding Hessians of
lower-level objectives, or lack rigorous finite-time performance guarantees. In
this work, we propose a Fully First-order Stochastic Approximation (F2SA)
method, and study its non-asymptotic convergence properties. Specifically, we
show that F2SA converges to an $\epsilon$-stationary solution of the bilevel
problem after $\epsilon^{-7/2}, \epsilon^{-5/2}$, and $\epsilon^{-3/2}$
iterations (each iteration using $O(1)$ samples) when stochastic noises are in
both level objectives, only in the upper-level objective, and not present
(deterministic settings), respectively. We further show that if we employ
momentum-assisted gradient estimators, the iteration complexities can be
improved to $\epsilon^{-5/2}, \epsilon^{-4/2}$, and $\epsilon^{-3/2}$,
respectively. We demonstrate even superior practical performance of the
proposed method over existing second-order based approaches on MNIST
data-hypercleaning experiments.",None,-1
78e15f7c-9ad1-40f0-ab1c-d4a89b743f7c,LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation,0.61298,"Existing automatic evaluation on text-to-image synthesis can only provide an
image-text matching score, without considering the object-level
compositionality, which results in poor correlation with human judgments. In
this work, we propose LLMScore, a new framework that offers evaluation scores
with multi-granularity compositionality. LLMScore leverages the large language
models (LLMs) to evaluate text-to-image models. Initially, it transforms the
image into image-level and object-level visual descriptions. Then an evaluation
instruction is fed into the LLMs to measure the alignment between the
synthesized image and the text, ultimately generating a score accompanied by a
rationale. Our substantial analysis reveals the highest correlation of LLMScore
with human judgments on a wide range of datasets (Attribute Binding Contrast,
Concept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore
achieves Kendall's tau correlation with human evaluations that is 58.8% and
31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP,
respectively.",None,-1
8709bb69-783b-446c-a99e-fb78fae4da30,DynamicDet: A Unified Dynamic Architecture for Object Detection,0.406882,"Dynamic neural network is an emerging research topic in deep learning. With
adaptive inference, dynamic models can achieve remarkable accuracy and
computational efficiency. However, it is challenging to design a powerful
dynamic detector, because of no suitable dynamic architecture and exiting
criterion for object detection. To tackle these difficulties, we propose a
dynamic framework for object detection, named DynamicDet. Firstly, we carefully
design a dynamic architecture based on the nature of the object detection task.
Then, we propose an adaptive router to analyze the multi-scale information and
to decide the inference route automatically. We also present a novel
optimization strategy with an exiting criterion based on the detection losses
for our dynamic detectors. Last, we present a variable-speed inference
strategy, which helps to realize a wide range of accuracy-speed trade-offs with
only one dynamic detector. Extensive experiments conducted on the COCO
benchmark demonstrate that the proposed DynamicDet achieves new
state-of-the-art accuracy-speed trade-offs. For instance, with comparable
accuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpasses
YOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code is
available at https://github.com/VDIGPKU/DynamicDet.",None,-1
b7a1dc81-0265-4b0d-b5cd-05f7cfd2dc74,An Overview on Language Models: Recent Developments and Outlook,0.452605,"Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner, while pre-trained
language models (PLMs) cover broader concepts and can be used in both causal
sequential modeling and fine-tuning for downstream applications. PLMs have
their own training paradigms (usually self-supervised) and serve as foundation
models in modern NLP systems. This overview paper provides an introduction to
both CLMs and PLMs from five aspects, i.e., linguistic units, architectures,
training methods, evaluation methods, and applications. Furthermore, we discuss
the relationship between CLMs and PLMs and shed light on the future directions
of language modeling in the pre-trained era.",None,-1
37a5fc65-21d0-47a5-8f90-d6a5ac798f9d,KG-BERTScore: Incorporating Knowledge Graph into BERTScore for Reference-Free Machine Translation Evaluation,0.401574,"BERTScore is an effective and robust automatic metric for referencebased
machine translation evaluation. In this paper, we incorporate multilingual
knowledge graph into BERTScore and propose a metric named KG-BERTScore, which
linearly combines the results of BERTScore and bilingual named entity matching
for reference-free machine translation evaluation. From the experimental
results on WMT19 QE as a metric without references shared tasks, our metric
KG-BERTScore gets higher overall correlation with human judgements than the
current state-of-the-art metrics for reference-free machine translation
evaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore
and the parameter for linear combination are also studied in this paper.",None,-1
45ae6077-1ebd-4dad-8c03-2ef34eb10a3c,GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,0.999874,"Multi-query attention (MQA), which only uses a single key-value head,
drastically speeds up decoder inference. However, MQA can lead to quality
degradation, and moreover it may not be desirable to train a separate model
just for faster inference. We (1) propose a recipe for uptraining existing
multi-head language model checkpoints into models with MQA using 5% of original
pre-training compute, and (2) introduce grouped-query attention (GQA), a
generalization of multi-query attention which uses an intermediate (more than
one, less than number of query heads) number of key-value heads. We show that
uptrained GQA achieves quality close to multi-head attention with comparable
speed to MQA.",None,-1
dd65916c-7877-402f-8abe-f133cc039737,Using Auxiliary Tasks In Multimodal Fusion Of Wav2vec 2.0 And BERT For Multimodal Emotion Recognition,0.683391,"The lack of data and the difficulty of multimodal fusion have always been
challenges for multimodal emotion recognition (MER). In this paper, we propose
to use pretrained models as upstream network, wav2vec 2.0 for audio modality
and BERT for text modality, and finetune them in downstream task of MER to cope
with the lack of data. For the difficulty of multimodal fusion, we use a
K-layer multi-head attention mechanism as a downstream fusion module. Starting
from the MER task itself, we design two auxiliary tasks to alleviate the
insufficient fusion between modalities and guide the network to capture and
align emotion-related features. Compared to the previous state-of-the-art
models, we achieve a better performance by 78.42% Weighted Accuracy (WA) and
79.71% Unweighted Accuracy (UA) on the IEMOCAP dataset.",None,-1
0dd2d5dd-12f4-4790-a7ac-26aa98289137,Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs,0.620706,"Dynamic scene graphs generated from video clips could help enhance the
semantic visual understanding in a wide range of challenging tasks such as
environmental perception, autonomous navigation, and task planning of
self-driving vehicles and mobile robots. In the process of temporal and spatial
modeling during dynamic scene graph generation, it is particularly intractable
to learn time-variant relations in dynamic scene graphs among frames. In this
paper, we propose a Time-variant Relation-aware TRansformer (TR$^2$), which
aims to model the temporal change of relations in dynamic scene graphs.
Explicitly, we leverage the difference of text embeddings of prompted sentences
about relation labels as the supervision signal for relations. In this way,
cross-modality feature guidance is realized for the learning of time-variant
relations. Implicitly, we design a relation feature fusion module with a
transformer and an additional message token that describes the difference
between adjacent frames. Extensive experiments on the Action Genome dataset
prove that our TR$^2$ can effectively model the time-variant relations. TR$^2$
significantly outperforms previous state-of-the-art methods under two different
settings by 2.1% and 2.6% respectively.",None,-1
84d0437b-9cb2-4210-87c3-7801687f7071,ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories,0.0874346,"Recently, Large Language Models (LLMs) have been serving as general-purpose
interfaces, posing a significant demand for comprehensive visual knowledge.
However, it remains unclear how well current LLMs and their visually augmented
counterparts (VaLMs) can master visual commonsense knowledge. To investigate
this, we propose ImageNetVC, a human-annotated dataset specifically designed
for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet
categories. Utilizing ImageNetVC, we benchmark the fundamental visual
commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze
the factors affecting the visual commonsense knowledge of large-scale models,
providing insights into the development of language models enriched with visual
commonsense knowledge. Our code and dataset are available at
https://github.com/hemingkx/ImageNetVC.",None,-1
386f1eea-9c28-4ca2-b093-65413fbcddcd,DocMAE: Document Image Rectification via Self-supervised Representation Learning,0.252279,"Tremendous efforts have been made on document image rectification, but how to
learn effective representation of such distorted images is still
under-explored. In this paper, we present DocMAE, a novel self-supervised
framework for document image rectification. Our motivation is to encode the
structural cues in document images by leveraging masked autoencoder to benefit
the rectification, i.e., the document boundaries, and text lines. Specifically,
we first mask random patches of the background-excluded document images and
then reconstruct the missing pixels. With such a self-supervised learning
approach, the network is encouraged to learn the intrinsic structure of
deformed documents by restoring document boundaries and missing text lines.
Transfer performance in the downstream rectification task validates the
effectiveness of our method. Extensive experiments are conducted to demonstrate
the effectiveness of our method.",None,-1
1d3eeb7e-0818-44de-94bf-988c5026498f,Eight Things to Know about Large Language Models,0.410348,"The widespread public deployment of large language models (LLMs) in recent
months has prompted a wave of new attention and engagement from advocates,
policymakers, and scholars from many fields. This attention is a timely
response to the many urgent questions that this technology raises, but it can
sometimes miss important considerations. This paper surveys the evidence for
eight potentially surprising such points:
  1. LLMs predictably get more capable with increasing investment, even without
targeted innovation.
  2. Many important LLM behaviors emerge unpredictably as a byproduct of
increasing investment.
  3. LLMs often appear to learn and use representations of the outside world.
  4. There are no reliable techniques for steering the behavior of LLMs.
  5. Experts are not yet able to interpret the inner workings of LLMs.
  6. Human performance on a task isn't an upper bound on LLM performance.
  7. LLMs need not express the values of their creators nor the values encoded
in web text.
  8. Brief interactions with LLMs are often misleading.",None,-1
5f22efdf-b173-488c-a5cc-758b1615a66b,Test-Time Self-Adaptive Small Language Models for Question Answering,0.0673886,"Recent instruction-finetuned large language models (LMs) have achieved
notable performances in various tasks, such as question-answering (QA).
However, despite their ability to memorize a vast amount of general knowledge
across diverse tasks, they might be suboptimal on specific tasks due to their
limited capacity to transfer and adapt knowledge to target tasks. Moreover,
further finetuning LMs with labeled datasets is often infeasible due to their
absence, but it is also questionable if we can transfer smaller LMs having
limited knowledge only with unlabeled test data. In this work, we show and
investigate the capabilities of smaller self-adaptive LMs, only with unlabeled
test data. In particular, we first stochastically generate multiple answers,
and then ensemble them while filtering out low-quality samples to mitigate
noise from inaccurate labels. Our proposed self-adaption strategy demonstrates
significant performance improvements on benchmark QA datasets with higher
robustness across diverse prompts, enabling LMs to stay stable. Code is
available at: https://github.com/starsuzi/T-SAS.",None,-1
06377f8b-a474-4e06-b9b7-b411f6d8d71a,Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution,0.656719,"Flow-based methods have demonstrated promising results in addressing the
ill-posed nature of super-resolution (SR) by learning the distribution of
high-resolution (HR) images with the normalizing flow. However, these methods
can only perform a predefined fixed-scale SR, limiting their potential in
real-world applications. Meanwhile, arbitrary-scale SR has gained more
attention and achieved great progress. Nonetheless, previous arbitrary-scale SR
methods ignore the ill-posed problem and train the model with per-pixel L1
loss, leading to blurry SR outputs. In this work, we propose ""Local Implicit
Normalizing Flow"" (LINF) as a unified solution to the above problems. LINF
models the distribution of texture details under different scaling factors with
normalizing flow. Thus, LINF can generate photo-realistic HR images with rich
texture details in arbitrary scale factors. We evaluate LINF with extensive
experiments and show that LINF achieves the state-of-the-art perceptual quality
compared with prior arbitrary-scale SR methods.",None,-1
ad05b4fc-fe02-4cb7-8587-d75a7a3a8875,Egocentric Audio-Visual Object Localization,0.878794,"Humans naturally perceive surrounding scenes by unifying sound and sight in a
first-person view. Likewise, machines are advanced to approach human
intelligence by learning with multisensory inputs from an egocentric
perspective. In this paper, we explore the challenging egocentric audio-visual
object localization task and observe that 1) egomotion commonly exists in
first-person recordings, even within a short duration; 2) The out-of-view sound
components can be created while wearers shift their attention. To address the
first problem, we propose a geometry-aware temporal aggregation module to
handle the egomotion explicitly. The effect of egomotion is mitigated by
estimating the temporal geometry transformation and exploiting it to update
visual representations. Moreover, we propose a cascaded feature enhancement
module to tackle the second issue. It improves cross-modal localization
robustness by disentangling visually-indicated audio representation. During
training, we take advantage of the naturally available audio-visual temporal
synchronization as the ``free'' self-supervision to avoid costly labeling. We
also annotate and create the Epic Sounding Object dataset for evaluation
purposes. Extensive experiments show that our method achieves state-of-the-art
localization performance in egocentric videos and can be generalized to diverse
audio-visual scenes.",None,-1
0a4ed4a4-f3c7-4ae6-b412-3a85a4f3e5d2,Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps,0.744144,"The biodiversity of our planet is under threat, with approximately one
million species expected to become extinct within decades. The reason; negative
human actions, which include hunting, overfishing, pollution, and the
conversion of land for urbanisation and agricultural purposes. Despite
significant investment from charities and governments for activities that
benefit nature, global wildlife populations continue to decline. Local wildlife
guardians have historically played a critical role in global conservation
efforts and have shown their ability to achieve sustainability at various
levels. In 2021, COP26 recognised their contributions and pledged US$1.7
billion per year; however, this is a fraction of the global biodiversity budget
available (between US$124 billion and US$143 billion annually) given they
protect 80% of the planets biodiversity. This paper proposes a radical new
solution based on ""Interspecies Money,"" where animals own their own money.
Creating a digital twin for each species allows animals to dispense funds to
their guardians for the services they provide. For example, a rhinoceros may
release a payment to its guardian each time it is detected in a camera trap as
long as it remains alive and well. To test the efficacy of this approach 27
camera traps were deployed over a 400km2 area in Welgevonden Game Reserve in
Limpopo Province in South Africa. The motion-triggered camera traps were
operational for ten months and, using deep learning, we managed to capture
images of 12 distinct animal species. For each species, a makeshift bank
account was set up and credited with {\pounds}100. Each time an animal was
captured in a camera and successfully classified, 1 penny (an arbitrary amount
- mechanisms still need to be developed to determine the real value of species)
was transferred from the animal account to its associated guardian.",None,-1
f9a3140e-fcfd-4dd7-b14a-133dde9bc1ce,A Study on Social Robot Behavior in Group Conversation,0.261288,"Recently, research in human-robot interaction began to consider a robot's
influence at the group level. Despite the recent growth in research
investigating the effects of robots within groups of people, our overall
understanding of what happens when robots are placed within groups or teams of
people is still limited. This paper investigates several key problems for
social robots that manage conversations in a group setting, where the number of
participants is more than two. In a group setting, the conversation dynamics
are a lot more complicated than the conventional one-to-one conversation, thus,
there are more challenges need to be solved.",None,-1
5a3d0329-a266-48ce-a0f0-6022d4d92dd5,Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering,0.0662681,"Answering time-sensitive questions from long documents requires temporal
reasoning over the times in questions and documents. An important open question
is whether large language models can perform such reasoning solely using a
provided text document, or whether they can benefit from additional temporal
information extracted using other systems. We address this research question by
applying existing temporal information extraction systems to construct temporal
graphs of events, times, and temporal relations in questions and documents. We
then investigate different approaches for fusing these graphs into Transformer
models. Experimental results show that our proposed approach for fusing
temporal graphs into input text substantially enhances the temporal reasoning
capabilities of Transformer models with or without fine-tuning. Additionally,
our proposed method outperforms various graph convolution-based approaches and
establishes a new state-of-the-art performance on SituatedQA and three splits
of TimeQA.",None,-1
c8fc1179-f24c-446b-84e5-ea5c6df0da3d,Biomedical Entity Linking with Triple-aware Pre-Training,0.432897,"Linking biomedical entities is an essential aspect in biomedical natural
language processing tasks, such as text mining and question answering. However,
a difficulty of linking the biomedical entities using current large language
models (LLM) trained on a general corpus is that biomedical entities are
scarcely distributed in texts and therefore have been rarely seen during
training by the LLM. At the same time, those LLMs are not aware of high level
semantic connection between different biomedical entities, which are useful in
identifying similar concepts in different textual contexts. To cope with
aforementioned problems, some recent works focused on injecting knowledge graph
information into LLMs. However, former methods either ignore the relational
knowledge of the entities or lead to catastrophic forgetting. Therefore, we
propose a novel framework to pre-train the powerful generative LLM by a corpus
synthesized from a KG. In the evaluations we are unable to confirm the benefit
of including synonym, description or relational information.",None,-1
2cbfae63-cbde-4929-935d-fe91780998ea,FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual Question Answering,0.106454,"The widely used Fact-based Visual Question Answering (FVQA) dataset contains
visually-grounded questions that require information retrieval using common
sense knowledge graphs to answer. It has been observed that the original
dataset is highly imbalanced and concentrated on a small portion of its
associated knowledge graph. We introduce FVQA 2.0 which contains adversarial
variants of test questions to address this imbalance. We show that systems
trained with the original FVQA train sets can be vulnerable to adversarial
samples and we demonstrate an augmentation scheme to reduce this vulnerability
without human annotations.",None,-1
8693bda0-5b18-4c59-b3d6-2eee07da4e6c,ActiveAED: A Human in the Loop Improves Annotation Error Detection,0.167126,"Manually annotated datasets are crucial for training and evaluating Natural
Language Processing models. However, recent work has discovered that even
widely-used benchmark datasets contain a substantial number of erroneous
annotations. This problem has been addressed with Annotation Error Detection
(AED) models, which can flag such errors for human re-annotation. However, even
though many of these AED methods assume a final curation step in which a human
annotator decides whether the annotation is erroneous, they have been developed
as static models without any human-in-the-loop component. In this work, we
propose ActiveAED, an AED method that can detect errors more accurately by
repeatedly querying a human for error corrections in its prediction loop. We
evaluate ActiveAED on eight datasets spanning five different tasks and find
that it leads to improvements over the state of the art on seven of them, with
gains of up to six percentage points in average precision.",None,-1
02316f20-69fe-49cf-8958-6935edef092e,APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses,0.434316,"The efficacy of availability poisoning, a method of poisoning data by
injecting imperceptible perturbations to prevent its use in model training, has
been a hot subject of investigation. Previous research suggested that it was
difficult to effectively counteract such poisoning attacks. However, the
introduction of various defense methods has challenged this notion. Due to the
rapid progress in this field, the performance of different novel methods cannot
be accurately validated due to variations in experimental setups. To further
evaluate the attack and defense capabilities of these poisoning methods, we
have developed a benchmark -- APBench for assessing the efficacy of adversarial
poisoning. APBench consists of 9 state-of-the-art availability poisoning
attacks, 8 defense algorithms, and 4 conventional data augmentation techniques.
We also have set up experiments with varying different poisoning ratios, and
evaluated the attacks on multiple datasets and their transferability across
model architectures. We further conducted a comprehensive evaluation of 2
additional attacks specifically targeting unsupervised models. Our results
reveal the glaring inadequacy of existing attacks in safeguarding individual
privacy. APBench is open source and available to the deep learning community:
https://github.com/lafeat/apbench.",None,-1
5c24e8dc-09da-4a0b-8421-9007b58cd5be,Can Programming Languages Boost Each Other via Instruction Tuning?,0.337616,"When human programmers have mastered a programming language, it would be
easier when they learn a new programming language. In this report, we focus on
exploring whether programming languages can boost each other during the
instruction fine-tuning phase of code large language models. We conduct
extensive experiments of 8 popular programming languages (Python, JavaScript,
TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that
programming languages can significantly improve each other. For example,
CodeM-Python 15B trained on Python is able to increase Java by an absolute
17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B
trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our
training data is released at https://github.com/NL2Code/CodeM.",None,-1
27c78ba9-b6db-4665-897b-a7a156b0f9bd,Selectively Hard Negative Mining for Alleviating Gradient Vanishing in Image-Text Matching,0.057904,"Recently, a series of Image-Text Matching (ITM) methods achieve impressive
performance. However, we observe that most existing ITM models suffer from
gradients vanishing at the beginning of training, which makes these models
prone to falling into local minima. Most ITM models adopt triplet loss with
Hard Negative mining (HN) as the optimization objective. We find that
optimizing an ITM model using only the hard negative samples can easily lead to
gradient vanishing. In this paper, we derive the condition under which the
gradient vanishes during training. When the difference between the positive
pair similarity and the negative pair similarity is close to 0, the gradients
on both the image and text encoders will approach 0. To alleviate the gradient
vanishing problem, we propose a Selectively Hard Negative Mining (SelHN)
strategy, which chooses whether to mine hard negative samples according to the
gradient vanishing condition. SelHN can be plug-and-play applied to existing
ITM models to give them better training behavior. To further ensure the
back-propagation of gradients, we construct a Residual Visual Semantic
Embedding model with SelHN, denoted as RVSE++. Extensive experiments on two ITM
benchmarks demonstrate the strength of RVSE++, achieving state-of-the-art
performance.",None,-1
1b003adc-dbf1-48d7-94fe-b3190bee8951,Whether you can locate or not? Interactive Referring Expression Generation,0.214971,"Referring Expression Generation (REG) aims to generate unambiguous Referring
Expressions (REs) for objects in a visual scene, with a dual task of Referring
Expression Comprehension (REC) to locate the referred object. Existing methods
construct REG models independently by using only the REs as ground truth for
model training, without considering the potential interaction between REG and
REC models. In this paper, we propose an Interactive REG (IREG) model that can
interact with a real REC model, utilizing signals indicating whether the object
is located and the visual region located by the REC model to gradually modify
REs. Our experimental results on three RE benchmark datasets, RefCOCO,
RefCOCO+, and RefCOCOg show that IREG outperforms previous state-of-the-art
methods on popular evaluation metrics. Furthermore, a human evaluation shows
that IREG generates better REs with the capability of interaction.",None,-1
ff1391bd-b12b-4f04-b296-9fe7a481b2ab,Stable Yaw Estimation of Boats from the Viewpoint of UAVs and USVs,0.0658819,"Yaw estimation of boats from the viewpoint of unmanned aerial vehicles (UAVs)
and unmanned surface vehicles (USVs) or boats is a crucial task in various
applications such as 3D scene rendering, trajectory prediction, and navigation.
However, the lack of literature on yaw estimation of objects from the viewpoint
of UAVs has motivated us to address this domain. In this paper, we propose a
method based on HyperPosePDF for predicting the orientation of boats in the 6D
space. For that, we use existing datasets, such as PASCAL3D+ and our own
datasets, SeaDronesSee-3D and BOArienT, which we annotated manually. We extend
HyperPosePDF to work in video-based scenarios, such that it yields robust
orientation predictions across time. Naively applying HyperPosePDF on video
data yields single-point predictions, resulting in far-off predictions and
often incorrect symmetric orientations due to unseen or visually different
data. To alleviate this issue, we propose aggregating the probability
distributions of pose predictions, resulting in significantly improved
performance, as shown in our experimental evaluation. Our proposed method could
significantly benefit downstream tasks in marine robotics.",None,-1
bf3efa89-d99e-4c67-b75c-123eec820c74,Multi-Architecture Multi-Expert Diffusion Models,0.210236,"In this paper, we address the performance degradation of efficient diffusion
models by introducing Multi-architecturE Multi-Expert diffusion models (MEME).
We identify the need for tailored operations at different time-steps in
diffusion processes and leverage this insight to create compact yet
high-performing models. MEME assigns distinct architectures to different
time-step intervals, balancing convolution and self-attention operations based
on observed frequency characteristics. We also introduce a soft interval
assignment strategy for comprehensive training. Empirically, MEME operates 3.3
times faster than baselines while improving image generation quality (FID
scores) by 0.62 (FFHQ) and 0.37 (CelebA). Though we validate the effectiveness
of assigning more optimal architecture per time-step, where efficient models
outperform the larger models, we argue that MEME opens a new design choice for
diffusion models that can be easily applied in other scenarios, such as large
multi-expert models.",None,-1
a1977b7c-6b9a-4731-aab1-382fff8236de,Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering,0.0873898,"Few-shot learning for open domain multi-hop question answering typically
relies on the incontext learning capability of large language models (LLMs).
While powerful, these LLMs usually contain tens or hundreds of billions of
parameters, making them rather inefficient at inference time. To improve
performance of smaller language models, we propose a data synthesis framework
for multi-hop question answering that requires less than 10 human annotated
question answer pairs. Our framework depends only on rich, naturally-occurring
relationships among documents and is built upon the data generation functions
parameterized by LLMs and prompts. We synthesize millions of multi-hop
questions and claims to finetune language models, evaluated on popular
benchmarks for multi-hop question answering and fact verification. Empirically,
our approach improves model performance significantly, allowing the finetuned
models to be competitive with GPT-3.5 based approaches while being almost
one-third the size in parameter count.",None,-1
eed814f5-ab4d-44a7-9bb4-189702546f33,An empirical study of using radiology reports and images to improve ICU mortality prediction,0.472384,"Background: The predictive Intensive Care Unit (ICU) scoring system plays an
important role in ICU management because it predicts important outcomes,
especially mortality. Many scoring systems have been developed and used in the
ICU. These scoring systems are primarily based on the structured clinical data
in the electronic health record (EHR), which may suffer the loss of important
clinical information in the narratives and images. Methods: In this work, we
build a deep learning based survival prediction model with multi-modality data
to predict ICU mortality. Four sets of features are investigated: (1)
physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2)
common thorax diseases pre-defined by radiologists, (3) BERT-based text
representations, and (4) chest X-ray image features. We use the Medical
Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the
proposed model. Results: Our model achieves the average C-index of 0.7829 (95%
confidence interval, 0.7620-0.8038), which substantially exceeds that of the
baseline with SAPS-II features (0.7470 (0.7263-0.7676)). Ablation studies
further demonstrate the contributions of pre-defined labels (2.00%), text
features (2.44%), and image features (2.82%).",None,-1
1d3c56d2-a721-49d3-bece-509ebf901cf4,Causal reasoning in typical computer vision tasks,0.0436106,"Deep learning has revolutionized the field of artificial intelligence. Based
on the statistical correlations uncovered by deep learning-based methods,
computer vision has contributed to tremendous growth in areas like autonomous
driving and robotics. Despite being the basis of deep learning, such
correlation is not stable and is susceptible to uncontrolled factors. In the
absence of the guidance of prior knowledge, statistical correlations can easily
turn into spurious correlations and cause confounders. As a result, researchers
are now trying to enhance deep learning methods with causal theory. Causal
theory models the intrinsic causal structure unaffected by data bias and is
effective in avoiding spurious correlations. This paper aims to comprehensively
review the existing causal methods in typical vision and vision-language tasks
such as semantic segmentation, object detection, and image captioning. The
advantages of causality and the approaches for building causal paradigms will
be summarized. Future roadmaps are also proposed, including facilitating the
development of causal theory and its application in other complex scenes and
systems.",None,-1
48f70f30-faa2-4151-881e-05eb1dbda6f2,In-Context Exemplars as Clues to Retrieving from Large Associative Memory,0.165903,"Recently, large language models (LLMs) have made remarkable progress in
natural language processing. The most representative ability of LLMs is
in-context learning (ICL), which enables LLMs to learn patterns from in-context
exemplars without training. The performance of ICL greatly depends on the
exemplars used. However, how to choose exemplars remains unclear due to the
lack of understanding of how in-context learning works. In this paper, we
present a novel perspective on ICL by conceptualizing it as contextual
retrieval from a model of associative memory. We establish a theoretical
framework of ICL based on Hopfield Networks. Based on our framework, we look
into how in-context exemplars influence the performance of ICL and propose more
efficient active exemplar selection. Our study sheds new light on the mechanism
of ICL by connecting it to memory retrieval, with potential implications for
advancing the understanding of LLMs.",None,-1
4f29243a-71a8-4f3e-995e-658031feaf2d,"Point-Query Quadtree for Crowd Counting, Localization, and More",0.823747,"We show that crowd counting can be viewed as a decomposable point querying
process. This formulation enables arbitrary points as input and jointly reasons
whether the points are crowd and where they locate. The querying processing,
however, raises an underlying problem on the number of necessary querying
points. Too few imply underestimation; too many increase computational
overhead. To address this dilemma, we introduce a decomposable structure, i.e.,
the point-query quadtree, and propose a new counting model, termed Point quEry
Transformer (PET). PET implements decomposable point querying via
data-dependent quadtree splitting, where each querying point could split into
four new points when necessary, thus enabling dynamic processing of sparse and
dense regions. Such a querying process yields an intuitive, universal modeling
of crowd as both the input and output are interpretable and steerable. We
demonstrate the applications of PET on a number of crowd-related tasks,
including fully-supervised crowd counting and localization, partial annotation
learning, and point annotation refinement, and also report state-of-the-art
performance. For the first time, we show that a single counting model can
address multiple crowd-related tasks across different learning paradigms. Code
is available at https://github.com/cxliu0/PET.",None,-1
5153bf89-243b-4351-809d-380aeebe52dc,BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation,0.279296,"This paper presents a novel approach for estimating human body shape and pose
from monocular images that effectively addresses the challenges of occlusions
and depth ambiguity. Our proposed method BoPR, the Body-aware Part Regressor,
first extracts features of both the body and part regions using an
attention-guided mechanism. We then utilize these features to encode extra
part-body dependency for per-part regression, with part features as queries and
body feature as a reference. This allows our network to infer the spatial
relationship of occluded parts with the body by leveraging visible parts and
body reference information. Our method outperforms existing state-of-the-art
methods on two benchmark datasets, and our experiments show that it
significantly surpasses existing methods in terms of depth ambiguity and
occlusion handling. These results provide strong evidence of the effectiveness
of our approach.The code and data are available for research purposes at
https://github.com/cyk990422/BoPR.",None,-1
5755136d-505c-4f69-a900-33281bf26c4a,Neural Machine Translation for Code Generation,0.483239,"Neural machine translation (NMT) methods developed for natural language
processing have been shown to be highly successful in automating translation
from one natural language to another. Recently, these NMT methods have been
adapted to the generation of program code. In NMT for code generation, the task
is to generate output source code that satisfies constraints expressed in the
input. In the literature, a variety of different input scenarios have been
explored, including generating code based on natural language description,
lower-level representations such as binary or assembly (neural decompilation),
partial representations of source code (code completion and repair), and source
code in another language (code translation). In this paper we survey the NMT
for code generation literature, cataloging the variety of methods that have
been explored according to input and output representations, model
architectures, optimization techniques used, data sets, and evaluation methods.
We discuss the limitations of existing methods and future research directions",None,-1
3075a65f-045e-41ee-a1d2-d91b8cb0137a,FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue,0.0771802,"Pre-trained language models based on general text enable huge success in the
NLP scenario. But the intrinsical difference of linguistic patterns between
general text and task-oriented dialogues makes existing pre-trained language
models less useful in practice. Current dialogue pre-training methods rely on a
contrastive framework and face the challenges of both selecting true positives
and hard negatives. In this paper, we propose a novel dialogue pre-training
model, FutureTOD, which distills future knowledge to the representation of the
previous dialogue context using a self-training framework. Our intuition is
that a good dialogue representation both learns local context information and
predicts future information. Extensive experiments on diverse downstream
dialogue tasks demonstrate the effectiveness of our model, especially the
generalization, robustness, and learning discriminative dialogue
representations capabilities.",None,-1
2d35debc-2b6c-4de0-b2ab-69c0c0ab718c,DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices,0.75417,"Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
https://github.com/ismailnejjar/DARE-GRAM.",None,-1
9d73ccac-2f48-46f0-b9d9-e12bae95eb3d,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,0.768886,"Recently, deep learning models have shown the potential to predict breast
cancer risk and enable targeted screening strategies, but current models do not
consider the change in the breast over time. In this paper, we present a new
method, PRIME+, for breast cancer risk prediction that leverages prior
mammograms using a transformer decoder, outperforming a state-of-the-art risk
prediction method that only uses mammograms from a single time point. We
validate our approach on a dataset with 16,113 exams and further demonstrate
that it effectively captures patterns of changes from prior mammograms, such as
changes in breast density, resulting in improved short-term and long-term
breast cancer risk prediction. Experimental results show that our model
achieves a statistically significant improvement in performance over the
state-of-the-art based model, with a C-index increase from 0.68 to 0.73 (p <
0.05) on held-out test sets.",None,-1
3e63011f-857a-4a56-8dd7-fd8fa55b5ebb,Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction,0.830708,"Multi-Modal Relation Extraction (MMRE) aims at identifying the relation
between two entities in texts that contain visual clues. Rich visual content is
valuable for the MMRE task, but existing works cannot well model finer
associations among different modalities, failing to capture the truly helpful
visual information and thus limiting relation extraction performance. In this
paper, we propose a novel MMRE framework to better capture the deeper
correlations of text, entity pair, and image/objects, so as to mine more
helpful information for the task, termed as DGF-PT. We first propose a
prompt-based autoregressive encoder, which builds the associations of
intra-modal and inter-modal features related to the task, respectively by
entity-oriented and object-oriented prefixes. To better integrate helpful
visual information, we design a dual-gated fusion module to distinguish the
importance of image/objects and further enrich text representations. In
addition, a generative decoder is introduced with entity type restriction on
relations, better filtering out candidates. Extensive experiments conducted on
the benchmark dataset show that our approach achieves excellent performance
compared to strong competitors, even in the few-shot situation.",None,-1
e38450e2-fbfb-4993-8ad8-094432c9c4f8,Discovering Knowledge-Critical Subnetworks in Pretrained Language Models,0.231007,"Pretrained language models (LMs) encode implicit representations of knowledge
in their parameters. However, localizing these representations and
disentangling them from each other remains an open problem. In this work, we
investigate whether pretrained language models contain various
knowledge-critical subnetworks: particular sparse computational subgraphs
responsible for encoding specific knowledge the model has memorized. We propose
a multi-objective differentiable weight masking scheme to discover these
subnetworks and show that we can use them to precisely remove specific
knowledge from models while minimizing adverse effects on the behavior of the
original language model. We demonstrate our method on multiple GPT2 variants,
uncovering highly sparse subnetworks (98%+) that are solely responsible for
specific collections of relational knowledge. When these subnetworks are
removed, the remaining network maintains most of its initial capacity (modeling
language and other memorized relational knowledge) but struggles to express the
removed knowledge, and suffers performance drops on examples needing this
removed knowledge on downstream tasks after finetuning.",None,-1
7b195a41-8fe1-40f3-86b5-e5fedcb09bfc,Segment Anything Meets Semantic Communication,0.738594,"In light of the diminishing returns of traditional methods for enhancing
transmission rates, the domain of semantic communication presents promising new
frontiers. Focusing on image transmission, this paper explores the application
of foundation models, particularly the Segment Anything Model (SAM) developed
by Meta AI Research, to improve semantic communication. SAM is a promptable
image segmentation model that has gained attention for its ability to perform
zero-shot segmentation tasks without explicit training or domain-specific
knowledge. By employing SAM's segmentation capability and lightweight neural
network architecture for semantic coding, we propose a practical approach to
semantic communication. We demonstrate that this approach retains critical
semantic features, achieving higher image reconstruction quality and reducing
communication overhead. This practical solution eliminates the
resource-intensive stage of training a segmentation model and can be applied to
any semantic coding architecture, paving the way for real-world applications.",None,-1
5d687106-e916-4e94-a0ac-62163029ada6,Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring,0.775477,"Automated essay scoring (AES) aims to score essays written for a given
prompt, which defines the writing topic. Most existing AES systems assume to
grade essays of the same prompt as used in training and assign only a holistic
score. However, such settings conflict with real-education situations;
pre-graded essays for a particular prompt are lacking, and detailed trait
scores of sub-rubrics are required. Thus, predicting various trait scores of
unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining
challenge of AES. In this paper, we propose a robust model: prompt- and trait
relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay
representation by essay-prompt attention and utilizing the topic-coherence
feature extracted by the topic-modeling mechanism without access to labeled
data; therefore, our model considers the prompt adherence of an essay, even in
a cross-prompt setting. To facilitate multi-trait scoring, we design
trait-similarity loss that encapsulates the correlations of traits. Experiments
prove the efficacy of our model, showing state-of-the-art results for all
prompts and traits. Significant improvements in low-resource-prompt and
inferior traits further indicate our model's strength.",None,-1
d322a4da-7df4-4133-99b0-8d2215eb2158,Classification of Primitive Manufacturing Tasks from Filtered Event Data,0.573349,"Collaborative robots are increasingly present in industry to support human
activities. However, to make the human-robot collaborative process more
effective, there are several challenges to be addressed. Collaborative robotic
systems need to be aware of the human activities to (1) anticipate
collaborative/assistive actions, (2) learn by demonstration, and (3) activate
safety procedures in shared workspace. This study proposes an action
classification system to recognize primitive assembly tasks from human motion
events data captured by a Dynamic and Active-pixel Vision Sensor (DAVIS).
Several filters are compared and combined to remove event data noise. Task
patterns are classified from a continuous stream of event data using advanced
deep learning and recurrent networks to classify spatial and temporal features.
Experiments were conducted on a novel dataset, the dataset of manufacturing
tasks (DMT22), featuring 5 classes of representative manufacturing primitives
(PickUp, Place, Screw, Hold, Idle) from 5 participants. Results show that the
proposed filters remove about 65\% of all events (noise) per recording,
conducting to a classification accuracy up to 99,37\% for subjects that trained
the system and 97.08\% for new subjects. Data from a left-handed subject were
successfully classified using only right-handed training data. These results
are object independent.",None,-1
779e2940-8083-4723-9dd4-58ea2f1fad65,Understanding Client Reactions in Online Mental Health Counseling,0.904181,"Communication success relies heavily on reading participants' reactions. Such
feedback is especially important for mental health counselors, who must
carefully consider the client's progress and adjust their approach accordingly.
However, previous NLP research on counseling has mainly focused on studying
counselors' intervention strategies rather than their clients' reactions to the
intervention. This work aims to fill this gap by developing a theoretically
grounded annotation framework that encompasses counselors' strategies and
client reaction behaviors. The framework has been tested against a large-scale,
high-quality text-based counseling dataset we collected over the past two years
from an online welfare counseling platform. Our study shows how clients react
to counselors' strategies, how such reactions affect the final counseling
outcomes, and how counselors can adjust their strategies in response to these
reactions. We also demonstrate that this study can help counselors
automatically predict their clients' states.",None,-1
45cb1afc-831b-4f33-a75e-5929299afdcc,Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints,0.0612848,"Neural QCFG is a grammar-based sequence-tosequence (seq2seq) model with
strong inductive biases on hierarchical structures. It excels in
interpretability and generalization but suffers from expensive inference. In
this paper, we study two low-rank variants of Neural QCFG for faster inference
with different trade-offs between efficiency and expressiveness. Furthermore,
utilizing the symbolic interface provided by the grammar, we introduce two soft
constraints over tree hierarchy and source coverage. We experiment with various
datasets and find that our models outperform vanilla Neural QCFG in most
settings.",None,-1
6a616a14-001f-4e37-adb0-fc697d6c0631,Probabilistic Adaptation of Text-to-Video Models,0.626257,"Large text-to-video models trained on internet-scale data have demonstrated
exceptional capabilities in generating high-fidelity videos from arbitrary
textual descriptions. However, adapting these models to tasks with limited
domain-specific data, such as animation or robotics videos, poses a significant
computational challenge, since finetuning a pretrained large model can be
prohibitively expensive. Inspired by how a small modifiable component (e.g.,
prompts, prefix-tuning) can adapt a large language model to perform new tasks
without requiring access to the model weights, we investigate how to adapt a
large pretrained text-to-video model to a variety of downstream domains and
tasks without finetuning. In answering this question, we propose Video Adapter,
which leverages the score function of a large pretrained video diffusion model
as a probabilistic prior to guide the generation of a task-specific small video
model. Our experiments show that Video Adapter is capable of incorporating the
broad knowledge and preserving the high fidelity of a large pretrained video
model in a task-specific small video model that is able to generate
high-quality yet specialized videos on a variety of tasks such as animation,
egocentric modeling, and modeling of simulated and real-world robotics data.
More videos can be found on the website https://video-adapter.github.io/.",None,-1
8d80204c-26a8-42f5-bae0-534d842ee4be,DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization,0.476516,"Deep Neural Networks have exhibited considerable success in various visual
tasks. However, when applied to unseen test datasets, state-of-the-art models
often suffer performance degradation due to domain shifts. In this paper, we
introduce a novel approach for domain generalization from a novel perspective
of enhancing the robustness of channels in feature maps to domain shifts. We
observe that models trained on source domains contain a substantial number of
channels that exhibit unstable activations across different domains, which are
inclined to capture domain-specific features and behave abnormally when exposed
to unseen target domains. To address the issue, we propose a DomainDrop
framework to continuously enhance the channel robustness to domain shifts,
where a domain discriminator is used to identify and drop unstable channels in
feature maps of each network layer during forward propagation. We theoretically
prove that our framework could effectively lower the generalization bound.
Extensive experiments on several benchmarks indicate that our framework
achieves state-of-the-art performance compared to other competing methods. Our
code is available at https://github.com/lingeringlight/DomainDrop.",None,-1
082241f9-4b40-410b-88bf-1aeb936f2b57,Self-supervised dense representation learning for live-cell microscopy with time arrow prediction,0.794942,"State-of-the-art object detection and segmentation methods for microscopy
images rely on supervised machine learning, which requires laborious manual
annotation of training data. Here we present a self-supervised method based on
time arrow prediction pre-training that learns dense image representations from
raw, unlabeled live-cell microscopy videos. Our method builds upon the task of
predicting the correct order of time-flipped image regions via a single-image
feature extractor followed by a time arrow prediction head that operates on the
fused features. We show that the resulting dense representations capture
inherently time-asymmetric biological processes such as cell divisions on a
pixel-level. We furthermore demonstrate the utility of these representations on
several live-cell microscopy datasets for detection and segmentation of
dividing cells, as well as for cell state classification. Our method
outperforms supervised methods, particularly when only limited ground truth
annotations are available as is commonly the case in practice. We provide code
at https://github.com/weigertlab/tarrow.",None,-1
1a076b3b-6af7-4371-832e-b5eb40cb4851,Multi-Scale Prototypical Transformer for Whole Slide Image Classification,0.837679,"Whole slide image (WSI) classification is an essential task in computational
pathology. Despite the recent advances in multiple instance learning (MIL) for
WSI classification, accurate classification of WSIs remains challenging due to
the extreme imbalance between the positive and negative instances in bags, and
the complicated pre-processing to fuse multi-scale information of WSI. To this
end, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI
classification, which includes a prototypical Transformer (PT) module and a
multi-scale feature fusion module (MFFM). The PT is developed to reduce
redundant instances in bags by integrating prototypical learning into the
Transformer architecture. It substitutes all instances with cluster prototypes,
which are then re-calibrated through the self-attention mechanism of the
Trans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypes
of different scales, which employs MLP-Mixer to enhance the information
communication between prototypes. The experimental results on two public WSI
datasets demonstrate that the proposed MSPT outperforms all the compared
algorithms, suggesting its potential applications.",None,-1
be6a6bae-3830-45bc-9fbd-bffcbb0461bb,Cost-Sensitive Best Subset Selection for Logistic Regression: A Mixed-Integer Conic Optimization Perspective,0.19611,"A key challenge in machine learning is to design interpretable models that
can reduce their inputs to the best subset for making transparent predictions,
especially in the clinical domain. In this work, we propose a certifiably
optimal feature selection procedure for logistic regression from a
mixed-integer conic optimization perspective that can take an auxiliary cost to
obtain features into account. Based on an extensive review of the literature,
we carefully create a synthetic dataset generator for clinical prognostic model
research. This allows us to systematically evaluate different heuristic and
optimal cardinality- and budget-constrained feature selection procedures. The
analysis shows key limitations of the methods for the low-data regime and when
confronted with label noise. Our paper not only provides empirical
recommendations for suitable methods and dataset designs, but also paves the
way for future research in the area of meta-learning.",None,-1
bee6581a-fd0a-4bc1-ad65-fc55f0a0e38d,USA-Net: Unified Semantic and Affordance Representations for Robot Memory,0.597776,"In order for robots to follow open-ended instructions like ""go open the brown
cabinet over the sink"", they require an understanding of both the scene
geometry and the semantics of their environment. Robotic systems often handle
these through separate pipelines, sometimes using very different representation
spaces, which can be suboptimal when the two objectives conflict. In this work,
we present USA-Net, a simple method for constructing a world representation
that encodes both the semantics and spatial affordances of a scene in a
differentiable map. This allows us to build a gradient-based planner which can
navigate to locations in the scene specified using open-ended vocabulary. We
use this planner to consistently generate trajectories which are both shorter
5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than
paths from comparable grid-based planners which don't leverage gradient
information. To our knowledge, this is the first end-to-end differentiable
planner optimizes for both semantics and affordance in a single implicit map.
Code and visuals are available at our website: https://usa.bolte.cc/",None,-1
d2790db7-75a5-49ec-bc2f-432930b1081e,FinnWoodlands Dataset,0.552329,"While the availability of large and diverse datasets has contributed to
significant breakthroughs in autonomous driving and indoor applications,
forestry applications are still lagging behind and new forest datasets would
most certainly contribute to achieving significant progress in the development
of data-driven methods for forest-like scenarios. This paper introduces a
forest dataset called \textit{FinnWoodlands}, which consists of RGB stereo
images, point clouds, and sparse depth maps, as well as ground truth manual
annotations for semantic, instance, and panoptic segmentation.
\textit{FinnWoodlands} comprises a total of 4226 objects manually annotated,
out of which 2562 objects (60.6\%) correspond to tree trunks classified into
three different instance categories, namely ""Spruce Tree"", ""Birch Tree"", and
""Pine Tree"". Besides tree trunks, we also annotated ""Obstacles"" objects as
instances as well as the semantic stuff classes ""Lake"", ""Ground"", and ""Track"".
Our dataset can be used in forestry applications where a holistic
representation of the environment is relevant. We provide an initial benchmark
using three models for instance segmentation, panoptic segmentation, and depth
completion, and illustrate the challenges that such unstructured scenarios
introduce.",None,-1
257d116b-56a6-435a-b98c-553df4e1ea3a,Reverse Knowledge Distillation: Training a Large Model using a Small One for Retinal Image Matching on Limited Data,0.457935,"Retinal image matching plays a crucial role in monitoring disease progression
and treatment response. However, datasets with matched keypoints between
temporally separated pairs of images are not available in abundance to train
transformer-based model. We propose a novel approach based on reverse knowledge
distillation to train large models with limited data while preventing
overfitting. Firstly, we propose architectural modifications to a CNN-based
semi-supervised method called SuperRetina that help us improve its results on a
publicly available dataset. Then, we train a computationally heavier model
based on a vision transformer encoder using the lighter CNN-based model, which
is counter-intuitive in the field knowledge-distillation research where
training lighter models based on heavier ones is the norm. Surprisingly, such
reverse knowledge distillation improves generalization even further. Our
experiments suggest that high-dimensional fitting in representation space may
prevent overfitting unlike training directly to match the final output. We also
provide a public dataset with annotations for retinal image keypoint detection
and matching to help the research community develop algorithms for retinal
image applications.",None,-1
64d843a9-7454-4eaf-b60e-a027792a8ebf,NeMF: Inverse Volume Rendering with Neural Microflake Field,0.750486,"Recovering the physical attributes of an object's appearance from its images
captured under an unknown illumination is challenging yet essential for
photo-realistic rendering. Recent approaches adopt the emerging implicit scene
representations and have shown impressive results.However, they unanimously
adopt a surface-based representation,and hence can not well handle scenes with
very complex geometry, translucent object and etc. In this paper, we propose to
conduct inverse volume rendering, in contrast to surface-based, by representing
a scene using microflake volume, which assumes the space is filled with
infinite small flakes and light reflects or scatters at each spatial location
according to microflake distributions. We further adopt the coordinate networks
to implicitly encode the microflake volume, and develop a differentiable
microflake volume renderer to train the network in an end-to-end way in
principle.Our NeMF enables effective recovery of appearance attributes for
highly complex geometry and scattering object, enables high-quality relighting,
material editing, and especially simulates volume rendering effects, such as
scattering, which is infeasible for surface-based approaches.",None,-1
723ee770-0d85-4168-b419-9fad1960e430,Attentive Graph Enhanced Region Representation Learning,0.523219,"Representing urban regions accurately and comprehensively is essential for
various urban planning and analysis tasks. Recently, with the expansion of the
city, modeling long-range spatial dependencies with multiple data sources plays
an important role in urban region representation. In this paper, we propose the
Attentive Graph Enhanced Region Representation Learning (ATGRL) model, which
aims to capture comprehensive dependencies from multiple graphs and learn rich
semantic representations of urban regions. Specifically, we propose a
graph-enhanced learning module to construct regional graphs by incorporating
mobility flow patterns, point of interests (POIs) functions, and check-in
semantics with noise filtering. Then, we present a multi-graph aggregation
module to capture both local and global spatial dependencies between regions by
integrating information from multiple graphs. In addition, we design a
dual-stage fusion module to facilitate information sharing between different
views and efficiently fuse multi-view representations for urban region
embedding using an improved linear attention mechanism. Finally, extensive
experiments on real-world datasets for three downstream tasks demonstrate the
superior performance of our model compared to state-of-the-art methods.",None,-1
96275ac6-3164-417e-b587-f4af4d2fae5f,Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation,0.410868,"Few-shot question answering (QA) aims at precisely discovering answers to a
set of questions from context passages while only a few training samples are
available. Although existing studies have made some progress and can usually
achieve proper results, they suffer from understanding deep semantics for
reasoning out the questions. In this paper, we develop Gotta, a Generative
prOmpT-based daTa Augmentation framework to mitigate the challenge above.
Inspired by the human reasoning process, we propose to integrate the cloze task
to enhance few-shot QA learning. Following the recent success of prompt-tuning,
we present the cloze task in the same format as the main QA task, allowing the
model to learn both tasks seamlessly together to fully take advantage of the
power of prompt-tuning. Extensive experiments on widely used benchmarks
demonstrate that Gotta consistently outperforms competitive baselines,
validating the effectiveness of our proposed prompt-tuning-based cloze task,
which not only fine-tunes language models but also learns to guide reasoning in
QA tasks. Further analysis shows that the prompt-based loss incorporates the
auxiliary task better than the multi-task loss, highlighting the strength of
prompt-tuning on the few-shot QA task.",None,-1
f050857c-233d-4d44-9c64-45c1281f31d4,"MOSO: Decomposing MOtion, Scene and Object for Video Prediction",0.65593,"Motion, scene and object are three primary visual components of a video. In
particular, objects represent the foreground, scenes represent the background,
and motion traces their dynamics. Based on this insight, we propose a two-stage
MOtion, Scene and Object decomposition framework (MOSO) for video prediction,
consisting of MOSO-VQVAE and MOSO-Transformer. In the first stage, MOSO-VQVAE
decomposes a previous video clip into the motion, scene and object components,
and represents them as distinct groups of discrete tokens. Then, in the second
stage, MOSO-Transformer predicts the object and scene tokens of the subsequent
video clip based on the previous tokens and adds dynamic motion at the token
level to the generated object and scene tokens. Our framework can be easily
extended to unconditional video generation and video frame interpolation tasks.
Experimental results demonstrate that our method achieves new state-of-the-art
performance on five challenging benchmarks for video prediction and
unconditional video generation: BAIR, RoboNet, KTH, KITTI and UCF101. In
addition, MOSO can produce realistic videos by combining objects and scenes
from different videos.",None,-1
19009ad8-8553-46d4-a168-c1017dd9b610,Reorganizing Educational Institutional Domain using Faceted Ontological Principles,0.188771,"The purpose of this work is to find out how different library classification
systems and linguistic ontologies arrange a particular domain of interest and
what are the limitations for information retrieval. We use knowledge
representation techniques and languages for construction of a domain specific
ontology. This ontology would help not only in problem solving, but it would
demonstrate the ease with which complex queries can be handled using principles
of domain ontology, thereby facilitating better information retrieval.",None,-1
c5ecfd70-397d-45bc-8d0a-ca3215c15242,Current and Future Challenges in Knowledge Representation and Reasoning,0.367045,"Knowledge Representation and Reasoning is a central, longstanding, and active
area of Artificial Intelligence. Over the years it has evolved significantly;
more recently it has been challenged and complemented by research in areas such
as machine learning and reasoning under uncertainty. In July 2022 a Dagstuhl
Perspectives workshop was held on Knowledge Representation and Reasoning. The
goal of the workshop was to describe the state of the art in the field,
including its relation with other areas, its shortcomings and strengths,
together with recommendations for future progress. We developed this manifesto
based on the presentations, panels, working groups, and discussions that took
place at the Dagstuhl Workshop. It is a declaration of our views on Knowledge
Representation: its origins, goals, milestones, and current foci; its relation
to other disciplines, especially to Artificial Intelligence; and on its
challenges, along with key priorities for the next decade.",None,-1
400e4611-6a39-4393-a935-7957853691cf,Multilingual Speech-to-Speech Translation into Multiple Target Languages,0.367332,"Speech-to-speech translation (S2ST) enables spoken communication between
people talking in different languages. Despite a few studies on multilingual
S2ST, their focus is the multilinguality on the source side, i.e., the
translation from multiple source languages to one target language. We present
the first work on multilingual S2ST supporting multiple target languages.
Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we
equip these key components with multilingual capability. Speech-to-masked-unit
(S2MU) is the multilingual extension of S2U, which applies masking to units
which don't belong to the given target language to reduce the language
interference. We also propose multilingual vocoder which is trained with
language embedding and the auxiliary loss of language identification. On
benchmark translation testsets, our proposed multilingual model shows superior
performance than bilingual models in the translation from English into $16$
target languages.",None,-1
21e54c28-af36-4b2c-a175-0e6fbf7034ed,rynSpeech: A multi-purpose Yorb Speech Corpus,0.831193,"We introduce \`{I}r\`{o}y\`{i}nSpeech, a new corpus influenced by the desire
to increase the amount of high quality, contemporary Yor\`{u}b\'{a} speech
data, which can be used for both Text-to-Speech (TTS) and Automatic Speech
Recognition (ASR) tasks. We curated about 23000 text sentences from news and
creative writing domains with the open license CC-BY-4.0. To encourage a
participatory approach to data creation, we provide 5000 curated sentences to
the Mozilla Common Voice platform to crowd-source the recording and validation
of Yor\`{u}b\'{a} speech data. In total, we created about 42 hours of speech
data recorded by 80 volunteers in-house, and 6 hours of validated recordings on
Mozilla Common Voice platform. Our TTS evaluation suggests that a
high-fidelity, general domain, single-speaker Yor\`{u}b\'{a} voice is possible
with as little as 5 hours of speech. Similarly, for ASR we obtained a baseline
word error rate (WER) of 23.8.",None,-1
3e854395-0d89-4215-a86b-c4ba0e033269,An Empirical Study of Translation Hypothesis Ensembling with Large Language Models,0.145431,"Large language models (LLMs) are becoming a one-fits-many solution, but they
sometimes hallucinate or produce unreliable output. In this paper, we
investigate how hypothesis ensembling can improve the quality of the generated
text for the specific problem of LLM-based machine translation. We experiment
with several techniques for ensembling hypotheses produced by LLMs such as
ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple
dimensions, including the method to generate hypotheses (multiple prompts,
temperature-based sampling, and beam search) and the strategy to produce the
final translation (instruction-based, quality-based reranking, and minimum
Bayes risk (MBR) decoding). Our results show that MBR decoding is a very
effective method, that translation quality can be improved using a small number
of samples, and that instruction tuning has a strong impact on the relation
between the diversity of the hypotheses and the sampling temperature.",None,-1
622dd007-2662-42b4-ba6f-35e59b8680fb,MarkQA: A large scale KBQA dataset with numerical reasoning,0.449994,"While question answering over knowledge bases (KBQA) has shown progress in
addressing factoid questions, KBQA with numerical reasoning remains relatively
unexplored. In this paper, we focus on the complex numerical reasoning in KBQA
and propose a new task, NR-KBQA, which necessitates the ability to perform both
multi-hop reasoning and numerical reasoning. We design a logic form in Python
format called PyQL to represent the reasoning process of numerical reasoning
questions. To facilitate the development of NR-KBQA, we present a large dataset
called MarkQA, which is automatically constructed from a small set of seeds.
Each question in MarkQA is equipped with its corresponding SPARQL query,
alongside the step-by-step reasoning process in the QDMR format and PyQL
program. Experimental results of some state-of-the-art QA methods on the MarkQA
show that complex numerical reasoning in KBQA faces great challenges.",None,-1
e5c4af0a-8ff5-47a0-b4e1-75e1dd81206f,Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models,0.448003,"Considerable effort has been dedicated to mitigating toxicity, but existing
methods often require drastic modifications to model parameters or the use of
computationally intensive auxiliary models. Furthermore, previous approaches
have often neglected the crucial factor of language's evolving nature over
time. In this work, we present a comprehensive perspective on toxicity
mitigation that takes into account its changing nature. We introduce
Goodtriever, a flexible methodology that matches the current state-of-the-art
toxicity mitigation while achieving 43% relative latency reduction during
inference and being more computationally efficient. By incorporating a
retrieval-based approach at decoding time, Goodtriever enables
toxicity-controlled text generation. Our research advocates for an increased
focus on adaptable mitigation techniques, which better reflect the data drift
models face when deployed in the wild. Code and data are available at
https://github.com/for-ai/goodtriever.",None,-1
76e48609-1fb5-4b29-969a-98f4bcf338af,ConsistentNeRF: Enhancing Neural Radiance Fields with 3D Consistency for Sparse View Synthesis,0.465343,"Neural Radiance Fields (NeRF) has demonstrated remarkable 3D reconstruction
capabilities with dense view images. However, its performance significantly
deteriorates under sparse view settings. We observe that learning the 3D
consistency of pixels among different views is crucial for improving
reconstruction quality in such cases. In this paper, we propose ConsistentNeRF,
a method that leverages depth information to regularize both multi-view and
single-view 3D consistency among pixels. Specifically, ConsistentNeRF employs
depth-derived geometry information and a depth-invariant loss to concentrate on
pixels that exhibit 3D correspondence and maintain consistent depth
relationships. Extensive experiments on recent representative works reveal that
our approach can considerably enhance model performance in sparse view
conditions, achieving improvements of up to 94% in PSNR, 76% in SSIM, and 31%
in LPIPS compared to the vanilla baselines across various benchmarks, including
DTU, NeRF Synthetic, and LLFF.",None,-1
b4839bb7-b64f-490f-add9-8ada86da4fa5,Learning in POMDPs is Sample-Efficient with Hindsight Observability,0.986986,"POMDPs capture a broad class of decision making problems, but hardness
results suggest that learning is intractable even in simple settings due to the
inherent partial observability. However, in many realistic problems, more
information is either revealed or can be computed during some point of the
learning process. Motivated by diverse applications ranging from robotics to
data center scheduling, we formulate a Hindsight Observable Markov Decision
Process (HOMDP) as a POMDP where the latent states are revealed to the learner
in hindsight and only during training. We introduce new algorithms for the
tabular and function approximation settings that are provably sample-efficient
with hindsight observability, even in POMDPs that would otherwise be
statistically intractable. We give a lower bound showing that the tabular
algorithm is optimal in its dependence on latent state and observation
cardinalities.",None,-1
24463e6e-9a01-45d1-92f0-6eb341056912,Reflexion: Language Agents with Verbal Reinforcement Learning,1.0,"Large language models (LLMs) have been increasingly used to interact with
external environments (e.g., games, compilers, APIs) as goal-driven agents.
However, it remains challenging for these language agents to quickly and
efficiently learn from trial-and-error as traditional reinforcement learning
methods require extensive training samples and expensive model fine-tuning. We
propose Reflexion, a novel framework to reinforce language agents not by
updating weights, but instead through linguistic feedback. Concretely,
Reflexion agents verbally reflect on task feedback signals, then maintain their
own reflective text in an episodic memory buffer to induce better
decision-making in subsequent trials. Reflexion is flexible enough to
incorporate various types (scalar values or free-form language) and sources
(external or internally simulated) of feedback signals, and obtains significant
improvements over a baseline agent across diverse tasks (sequential
decision-making, coding, language reasoning). For example, Reflexion achieves a
91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous
state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis
studies using different feedback signals, feedback incorporation methods, and
agent types, and provide insights into how they affect performance.",None,-1
d0ac0260-310b-4e68-83a0-7a0d837cbed9,Text2shape Deep Retrieval Model: Generating Initial Cases for Mechanical Part Redesign under the Context of Case-Based Reasoning,0.0888435,"Retrieving the similar solutions from the historical case base for new design
requirements is the first step in mechanical part redesign under the context of
case-based reasoning. However, the manual retrieving method has the problem of
low efficiency when the case base is large. Additionally, it is difficult for
simple reasoning algorithms (e.g., rule-based reasoning, decision tree) to
cover all the features in complicated design solutions. In this regard, a
text2shape deep retrieval model is established in order to support text
description-based mechanical part shapes retrieval, where the texts are for
describing the structural features of the target mechanical parts. More
specifically, feature engineering is applied to identify the key structural
features of the target mechanical parts. Based on the identified key structural
features, a training set of 1000 samples was constructed, where each sample
consisted of a paragraph of text description of a group of structural features
and the corresponding 3D shape of the structural features. RNN and 3D CNN
algorithms were customized to build the text2shape deep retrieval model.
Orthogonal experiments were used for modeling turning. Eventually, the highest
accuracy of the model was 0.98; therefore, the model can be effective for
retrieving initial cases for mechanical part redesign.",None,-1
f41118f2-0bd5-41ac-8252-f2cdb8af6a53,Patched Line Segment Learning for Vector Road Mapping,0.454504,"This paper presents a novel approach to computing vector road maps from
satellite remotely sensed images, building upon a well-defined Patched Line
Segment (PaLiS) representation for road graphs that holds geometric
significance. Unlike prevailing methods that derive road vector representations
from satellite images using binary masks or keypoints, our method employs line
segments. These segments not only convey road locations but also capture their
orientations, making them a robust choice for representation. More precisely,
given an input image, we divide it into non-overlapping patches and predict a
suitable line segment within each patch. This strategy enables us to capture
spatial and structural cues from these patch-based line segments, simplifying
the process of constructing the road network graph without the necessity of
additional neural networks for connectivity. In our experiments, we demonstrate
how an effective representation of a road graph significantly enhances the
performance of vector road mapping on established benchmarks, without requiring
extensive modifications to the neural network architecture. Furthermore, our
method achieves state-of-the-art performance with just 6 GPU hours of training,
leading to a substantial 32-fold reduction in training costs in terms of GPU
hours.",None,-1
dbedd9c6-40dc-413b-8200-693cdfae159c,An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems,0.771619,"The Segment Anything Model (SAM) has demonstrated exceptional performance and
versatility, making it a promising tool for various related tasks. In this
report, we explore the application of SAM in Weakly-Supervised Semantic
Segmentation (WSSS). Particularly, we adapt SAM as the pseudo-label generation
pipeline given only the image-level class labels. While we observed impressive
results in most cases, we also identify certain limitations. Our study includes
performance evaluations on PASCAL VOC and MS-COCO, where we achieved remarkable
improvements over the latest state-of-the-art methods on both datasets. We
anticipate that this report encourages further explorations of adopting SAM in
WSSS, as well as wider real-world applications.",None,-1
8e99571e-5809-40f4-b136-6c28352cb403,deep learning of segment-level feature representation for speech emotion recognition in conversations,0.341275,"Accurately detecting emotions in conversation is a necessary yet challenging
task due to the complexity of emotions and dynamics in dialogues. The emotional
state of a speaker can be influenced by many different factors, such as
interlocutor stimulus, dialogue scene, and topic. In this work, we propose a
conversational speech emotion recognition method to deal with capturing
attentive contextual dependency and speaker-sensitive interactions. First, we
use a pretrained VGGish model to extract segment-based audio representation in
individual utterances. Second, an attentive bi-directional gated recurrent unit
(GRU) models contextual-sensitive information and explores intra- and
inter-speaker dependencies jointly in a dynamic manner. The experiments
conducted on the standard conversational dataset MELD demonstrate the
effectiveness of the proposed method when compared against state-of the-art
methods.",None,-1
9f9c2639-6ab1-488e-af7a-1895ea819e9a,Multimodal Industrial Anomaly Detection via Hybrid Fusion,0.953207,"2D-based Industrial Anomaly Detection has been widely discussed, however,
multimodal industrial anomaly detection based on 3D point clouds and RGB images
still has many untouched fields. Existing multimodal industrial anomaly
detection methods directly concatenate the multimodal features, which leads to
a strong disturbance between features and harms the detection performance. In
this paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly
detection method with hybrid fusion scheme: firstly, we design an unsupervised
feature fusion with patch-wise contrastive learning to encourage the
interaction of different modal features; secondly, we use a decision layer
fusion with multiple memory banks to avoid loss of information and additional
novelty classifiers to make the final decision. We further propose a point
feature alignment operation to better align the point cloud and RGB features.
Extensive experiments show that our multimodal industrial anomaly detection
model outperforms the state-of-the-art (SOTA) methods on both detection and
segmentation precision on MVTec-3D AD dataset. Code is available at
https://github.com/nomewang/M3DM.",None,-1
96974dab-c9e1-48db-aec1-a717b75bd24a,Smooth Non-Stationary Bandits,0.611182,"In many applications of online decision making, the environment is
non-stationary and it is therefore crucial to use bandit algorithms that handle
changes. Most existing approaches are designed to protect against non-smooth
changes, constrained only by total variation or Lipschitzness over time, where
they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice
environments are often changing {\bf smoothly}, so such algorithms may incur
higher-than-necessary regret in these settings and do not leverage information
on the rate of change. We study a non-stationary two-armed bandits problem
where we assume that an arm's mean reward is a $\beta$-H\""older function over
(normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously
differentiable. We show the first separation between the smooth and non-smooth
regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$.
We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound
for any integer $\beta\ge 1$, which matches our upper bound for $\beta=2$.",None,-1
8565fd9b-3e96-420e-9a62-57b9d5fbb433,Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models,0.388475,"The escalating debate on AI's capabilities warrants developing reliable
metrics to assess machine ""intelligence"". Recently, many anecdotal examples
were used to suggest that newer large language models (LLMs) like ChatGPT and
GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached
conflicting conclusions regarding those abilities. We investigate the extent of
LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs
exhibit certain N-ToM abilities, this behavior is far from being robust. We
further examine the factors impacting performance on N-ToM tasks and discover
that LLMs struggle with adversarial examples, indicating reliance on shallow
heuristics rather than robust ToM abilities. We caution against drawing
conclusions from anecdotal examples, limited benchmark testing, and using
human-designed psychological tests to evaluate models.",None,-1
a660ec22-69d0-4444-9e68-99f5328efb54,Text Classification of Cancer Clinical Trial Eligibility Criteria,0.80928,"Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.",None,-1
441307d1-1592-431e-b32c-c982bb38fc2a,A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition,0.464029,"The goal of building a benchmark (suite of datasets) is to provide a unified
protocol for fair evaluation and thus facilitate the evolution of a specific
area. Nonetheless, we point out that existing protocols of action recognition
could yield partial evaluations due to several limitations. To comprehensively
probe the effectiveness of spatiotemporal representation learning, we introduce
BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18
video datasets grouped into 5 categories (anomaly, gesture, daily, sports, and
instructional), which covers a diverse set of real-world applications. With
BEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both
supervised and self-supervised learning. We also report transfer performance
via standard finetuning, few-shot finetuning, and unsupervised domain
adaptation. Our observation suggests that current state-of-the-art cannot
solidly guarantee high performance on datasets close to real-world
applications, and we hope BEAR can serve as a fair and challenging evaluation
benchmark to gain insights on building next-generation spatiotemporal learners.
Our dataset, code, and models are released at:
https://github.com/AndongDeng/BEAR",None,-1
d44a1ca6-8922-4e09-afa6-1e58cfc27b25,Logic programming for deliberative robotic task planning,0.367839,"Over the last decade, the use of robots in production and daily life has
increased. With increasingly complex tasks and interaction in different
environments including humans, robots are required a higher level of autonomy
for efficient deliberation. Task planning is a key element of deliberation. It
combines elementary operations into a structured plan to satisfy a prescribed
goal, given specifications on the robot and the environment. In this
manuscript, we present a survey on recent advances in the application of logic
programming to the problem of task planning. Logic programming offers several
advantages compared to other approaches, including greater expressivity and
interpretability which may aid in the development of safe and reliable robots.
We analyze different planners and their suitability for specific robotic
applications, based on expressivity in domain representation, computational
efficiency and software implementation. In this way, we support the robotic
designer in choosing the best tool for his application.",None,-1
fc70bd0e-3ec8-4515-bdd9-2cc343bf9d39,Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models,0.527267,"We analyze sentiment analysis and toxicity detection models to detect the
presence of explicit bias against people with disability (PWD). We employ the
bias identification framework of Perturbation Sensitivity Analysis to examine
conversations related to PWD on social media platforms, specifically Twitter
and Reddit, in order to gain insight into how disability bias is disseminated
in real-world social settings. We then create the \textit{Bias Identification
Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any
sentiment analysis and toxicity detection models. Our study utilizes BITS to
uncover significant biases in four open AIaaS (AI as a Service) sentiment
analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API,
DistilBERT and two toxicity detection models, namely two versions of
Toxic-BERT. Our findings indicate that all of these models exhibit
statistically significant explicit bias against PWD.",None,-1
ec619d8b-cc3c-496c-9f57-efb42bcb95cf,Stochastic Multi-Person 3D Motion Forecasting,0.889615,"This paper aims to deal with the ignored real-world complexities in prior
work on human motion forecasting, emphasizing the social properties of
multi-person motion, the diversity of motion and social interactions, and the
complexity of articulated motion. To this end, we introduce a novel task of
stochastic multi-person 3D motion forecasting. We propose a dual-level
generative modeling framework that separately models independent individual
motion at the local level and social interactions at the global level. Notably,
this dual-level modeling mechanism can be achieved within a shared generative
model, through introducing learnable latent codes that represent intents of
future motion and switching the codes' modes of operation at different levels.
Our framework is general; we instantiate it with different generative models,
including generative adversarial networks and diffusion models, and various
multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,
and SoMoF benchmarks show that our approach produces diverse and accurate
multi-person predictions, significantly outperforming the state of the art.",None,-1
549a7d24-ede5-4cdf-944d-3c2b60d5c1ad,Dynamic Coarse-to-Fine Learning for Oriented Tiny Object Detection,0.624324,"Detecting arbitrarily oriented tiny objects poses intense challenges to
existing detectors, especially for label assignment. Despite the exploration of
adaptive label assignment in recent oriented object detectors, the extreme
geometry shape and limited feature of oriented tiny objects still induce severe
mismatch and imbalance issues. Specifically, the position prior, positive
sample feature, and instance are mismatched, and the learning of extreme-shaped
objects is biased and unbalanced due to little proper feature supervision. To
tackle these issues, we propose a dynamic prior along with the coarse-to-fine
assigner, dubbed DCFL. For one thing, we model the prior, label assignment, and
object representation all in a dynamic manner to alleviate the mismatch issue.
For another, we leverage the coarse prior matching and finer posterior
constraint to dynamically assign labels, providing appropriate and relatively
balanced supervision for diverse instances. Extensive experiments on six
datasets show substantial improvements to the baseline. Notably, we obtain the
state-of-the-art performance for one-stage detectors on the DOTA-v1.5,
DOTA-v2.0, and DIOR-R datasets under single-scale training and testing. Codes
are available at https://github.com/Chasel-Tsui/mmrotate-dcfl.",None,-1
51248d71-bac8-4d0a-8767-906a1251afec,SUIT: Learning Significance-guided Information for 3D Temporal Detection,0.16507,"3D object detection from LiDAR point cloud is of critical importance for
autonomous driving and robotics. While sequential point cloud has the potential
to enhance 3D perception through temporal information, utilizing these temporal
features effectively and efficiently remains a challenging problem. Based on
the observation that the foreground information is sparsely distributed in
LiDAR scenes, we believe sufficient knowledge can be provided by sparse format
rather than dense maps. To this end, we propose to learn Significance-gUided
Information for 3D Temporal detection (SUIT), which simplifies temporal
information as sparse features for information fusion across frames.
Specifically, we first introduce a significant sampling mechanism that extracts
information-rich yet sparse features based on predicted object centroids. On
top of that, we present an explicit geometric transformation learning
technique, which learns the object-centric transformations among sparse
features across frames. We evaluate our method on large-scale nuScenes and
Waymo dataset, where our SUIT not only significantly reduces the memory and
computation cost of temporal fusion, but also performs well over the
state-of-the-art baselines.",None,-1
40e660db-50ba-4201-b78f-707648ec000f,Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles,0.788563,"This paper shows that the heterogeneity in neuronal and synaptic dynamics
reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while
improving prediction performance, enabling spike-efficient (unsupervised)
learning. We analytically show that the diversity in neurons'
integration/relaxation dynamics improves an RSNN's ability to learn more
distinct input patterns (higher memory capacity), leading to improved
classification and prediction performance. We further prove that heterogeneous
Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking
activity but preserve memory capacity. The analytical results motivate
Heterogeneous RSNN design using Bayesian optimization to determine
heterogeneity in neurons and synapses to improve $\mathcal{E}$, defined as the
ratio of spiking activity and memory capacity. The empirical results on time
series classification and prediction tasks show that optimized HRSNN increases
performance and reduces spiking activity compared to a homogeneous RSNN.",None,-1
2085b21d-f02e-4e28-a895-3c2f39c59b9b,NeAT: Neural Artistic Tracing for Beautiful Style Transfer,0.398484,"Style transfer is the task of reproducing the semantic contents of a source
image in the artistic style of a second target image. In this paper, we present
NeAT, a new state-of-the art feed-forward style transfer method. We
re-formulate feed-forward style transfer as image editing, rather than image
generation, resulting in a model which improves over the state-of-the-art in
both preserving the source content and matching the target style. An important
component of our model's success is identifying and fixing ""style halos"", a
commonly occurring artefact across many style transfer techniques. In addition
to training and testing on standard datasets, we introduce the BBST-4M dataset,
a new, large scale, high resolution dataset of 4M images. As a component of
curating this data, we present a novel model able to classify if an image is
stylistic. We use BBST-4M to improve and measure the generalization of NeAT
across a huge variety of styles. Not only does NeAT offer state-of-the-art
quality and generalization, it is designed and trained for fast inference at
high resolution.",None,-1
cbad1145-c0fe-4636-b05f-64b4c922f79b,Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement,0.624692,"Mentions of new concepts appear regularly in texts and require automated
approaches to harvest and place them into Knowledge Bases (KB), e.g.,
ontologies and taxonomies. Existing datasets suffer from three issues, (i)
mostly assuming that a new concept is pre-discovered and cannot support
out-of-KB mention discovery; (ii) only using the concept label as the input
along with the KB and thus lacking the contexts of a concept label; and (iii)
mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,
instead of complex concepts, i.e., with logical operators. To address these
issues, we propose a new benchmark, adapting MedMentions dataset (PubMed
abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases
sub-category and the broader categories of Clinical finding, Procedure, and
Pharmaceutical / biologic product. We provide usage on the evaluation with the
dataset for out-of-KB mention discovery and concept placement, adapting recent
Large Language Model based methods.",None,-1
852056ea-361b-4df3-b84b-7c7719cf5a8e,ThoughtSource: A central hub for large language model reasoning data,0.452033,"Large language models (LLMs) such as GPT-4 have recently demonstrated
impressive results across a wide range of tasks. LLMs are still limited,
however, in that they frequently fail at complex reasoning, their reasoning
processes are opaque, they are prone to 'hallucinate' facts, and there are
concerns about their underlying biases. Letting models verbalize reasoning
steps as natural language, a technique known as chain-of-thought prompting, has
recently been proposed as a way to address some of these issues. Here we
present ThoughtSource, a meta-dataset and software library for chain-of-thought
(CoT) reasoning. The goal of ThoughtSource is to improve future artificial
intelligence systems by facilitating qualitative understanding of CoTs,
enabling empirical evaluations, and providing training data. This first release
of ThoughtSource integrates seven scientific/medical, three general-domain and
five math word question answering datasets.",None,-1
edc79c98-b0bf-48e4-b432-224ecbd631ef,Agent-based Collaborative Random Search for Hyper-parameter Tuning and Global Function Optimization,0.580747,"Hyper-parameter optimization is one of the most tedious yet crucial steps in
training machine learning models. There are numerous methods for this vital
model-building stage, ranging from domain-specific manual tuning guidelines
suggested by the oracles to the utilization of general-purpose black-box
optimization techniques. This paper proposes an agent-based collaborative
technique for finding near-optimal values for any arbitrary set of
hyper-parameters (or decision variables) in a machine learning model (or
general function optimization problem). The developed method forms a
hierarchical agent-based architecture for the distribution of the searching
operations at different dimensions and employs a cooperative searching
procedure based on an adaptive width-based random sampling technique to locate
the optima. The behavior of the presented model, specifically against the
changes in its design parameters, is investigated in both machine learning and
global function optimization applications, and its performance is compared with
that of two randomized tuning strategies that are commonly used in practice.
According to the empirical results, the proposed model outperformed the
compared methods in the experimented classification, regression, and
multi-dimensional function optimization tasks, notably in a higher number of
dimensions and in the presence of limited on-device computational resources.",None,-1
300d717d-2f32-448b-a811-491886e706f8,PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback,0.714947,"Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.",None,-1
a4cb9074-cb19-4ded-8888-b7a549421ca7,Learning in Factored Domains with Information-Constrained Visual Representations,0.0679539,"Humans learn quickly even in tasks that contain complex visual information.
This is due in part to the efficient formation of compressed representations of
visual information, allowing for better generalization and robustness. However,
compressed representations alone are insufficient for explaining the high speed
of human learning. Reinforcement learning (RL) models that seek to replicate
this impressive efficiency may do so through the use of factored
representations of tasks. These informationally simplistic representations of
tasks are similarly motivated as the use of compressed representations of
visual information. Recent studies have connected biological visual perception
to disentangled and compressed representations. This raises the question of how
humans learn to efficiently represent visual information in a manner useful for
learning tasks. In this paper we present a model of human factored
representation learning based on an altered form of a $\beta$-Variational
Auto-encoder used in a visual learning task. Modelling results demonstrate a
trade-off in the informational complexity of model latent dimension spaces,
between the speed of learning and the accuracy of reconstructions.",None,-1
86b927f6-3316-4638-92c3-a7f8c6eafe0f,An Adaptive Method for Camera Attribution under Complex Radial Distortion Corrections,0.100828,"Radial correction distortion, applied by in-camera or out-camera
software/firmware alters the supporting grid of the image so as to hamper
PRNU-based camera attribution. Existing solutions to deal with this problem try
to invert/estimate the correction using radial transformations parameterized
with few variables in order to restrain the computational load; however, with
ever more prevalent complex distortion corrections their performance is
unsatisfactory. In this paper we propose an adaptive algorithm that by dividing
the image into concentric annuli is able to deal with sophisticated corrections
like those applied out-camera by third party software like Adobe Lightroom,
Photoshop, Gimp and PT-Lens. We also introduce a statistic called cumulative
peak of correlation energy (CPCE) that allows for an efficient early stopping
strategy. Experiments on a large dataset of in-camera and out-camera radially
corrected images show that our solution improves the state of the art in terms
of both accuracy and computational cost.",None,-1
e9ae49f1-a66e-4dbc-b381-0ead67422fbb,Guiding Computational Stance Detection with Expanded Stance Triangle Framework,0.0578504,"Stance detection determines whether the author of a piece of text is in favor
of, against, or neutral towards a specified target, and can be used to gain
valuable insights into social media. The ubiquitous indirect referral of
targets makes this task challenging, as it requires computational solutions to
model semantic features and infer the corresponding implications from a literal
statement. Moreover, the limited amount of available training data leads to
subpar performance in out-of-domain and cross-target scenarios, as data-driven
approaches are prone to rely on superficial and domain-specific features. In
this work, we decompose the stance detection task from a linguistic
perspective, and investigate key components and inference paths in this task.
The stance triangle is a generic linguistic framework previously proposed to
describe the fundamental ways people express their stance. We further expand it
by characterizing the relationship between explicit and implicit objects. We
then use the framework to extend one single training corpus with additional
annotation. Experimental results show that strategically-enriched data can
significantly improve the performance on out-of-domain and cross-target
evaluation.",None,-1
f9a9a1c3-a06f-4b57-b512-ec38e06e7de1,NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images,0.896954,"We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.",None,-1
10d9ac55-3284-49c3-8b41-6108f40bb0c0,Deep Learning in the Field of Biometric Template Protection: An Overview,0.922522,"Today, deep learning represents the most popular and successful form of
machine learning. Deep learning has revolutionised the field of pattern
recognition, including biometric recognition. Biometric systems utilising deep
learning have been shown to achieve auspicious recognition accuracy, surpassing
human performance. Apart from said breakthrough advances in terms of biometric
performance, the use of deep learning was reported to impact different
covariates of biometrics such as algorithmic fairness, vulnerability to
attacks, or template protection. Technologies of biometric template protection
are designed to enable a secure and privacy-preserving deployment of
biometrics. In the recent past, deep learning techniques have been frequently
applied in biometric template protection systems for various purposes. This
work provides an overview of how advances in deep learning take influence on
the field of biometric template protection. The interrelation between improved
biometric performance rates and security in biometric template protection is
elaborated. Further, the use of deep learning for obtaining feature
representations that are suitable for biometric template protection is
discussed. Novel methods that apply deep learning to achieve various goals of
biometric template protection are surveyed along with deep learning-based
attacks.",None,-1
03083ea2-81c8-4219-991d-c2f7d271a425,PACO: Parts and Attributes of Common Objects,0.90905,"Object models are gradually progressing from predicting just category labels
to providing detailed descriptions of object instances. This motivates the need
for large datasets which go beyond traditional object masks and provide richer
annotations such as part masks and attributes. Hence, we introduce PACO: Parts
and Attributes of Common Objects. It spans 75 object categories, 456
object-part categories and 55 attributes across image (LVIS) and video (Ego4D)
datasets. We provide 641K part masks annotated across 260K object boxes, with
roughly half of them exhaustively annotated with attributes as well. We design
evaluation metrics and provide benchmark results for three tasks on the
dataset: part mask segmentation, object and part attribute prediction and
zero-shot instance detection. Dataset, models, and code are open-sourced at
https://github.com/facebookresearch/paco.",None,-1
2048b443-a5cc-489c-9dc5-2ed1aa035b14,One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data,0.527608,"There is a recent growing interest in applying Deep Learning techniques to
tabular data, in order to replicate the success of other Artificial
Intelligence areas in this structured domain. Specifically interesting is the
case in which tabular data have a time dependence, such as, for instance
financial transactions. However, the heterogeneity of the tabular values, in
which categorical elements are mixed with numerical items, makes this
adaptation difficult. In this paper we propose a Transformer architecture to
represent heterogeneous time-dependent tabular data, in which numerical
features are represented using a set of frequency functions and the whole
network is uniformly trained with a unique loss function.",None,-1
f8f415dd-1dcf-4a8d-889c-45c2e6c139a3,Age-Friendly Route Planner: Calculating Comfortable Routes for Senior Citizens,0.535536,"The application of routing algorithms to real-world situations is a widely
studied research topic. Despite this, routing algorithms and applications are
usually developed for a general purpose, meaning that certain groups, such as
ageing people, are often marginalized due to the broad approach of the designed
algorithms. This situation may pose a problem in cities which are suffering a
slow but progressive ageing of their populations. With this motivation in mind,
this paper focuses on describing our implemented Age-Friendly Route Planner,
whose goal is to improve the experience in the city for senior citizens. In
order to measure the age-friendliness of a route, several variables have been
deemed, such as the number of amenities along the route, the amount of
comfortable elements found, or the avoidance of sloppy sections. In this paper,
we describe one of the main features of the Age-Friendly Route Planner: the
preference-based routes, and we also demonstrate how it can contribute to the
creation of adapted friendly routes.",None,-1
7c9af2f4-6a91-4656-aedd-605e59139ae1,Using simulation to quantify the performance of automotive perception systems,0.127789,"The design and evaluation of complex systems can benefit from a software
simulation - sometimes called a digital twin. The simulation can be used to
characterize system performance or to test its performance under conditions
that are difficult to measure (e.g., nighttime for automotive perception
systems). We describe the image system simulation software tools that we use to
evaluate the performance of image systems for object (automobile) detection. We
describe experiments with 13 different cameras with a variety of optics and
pixel sizes. To measure the impact of camera spatial resolution, we designed a
collection of driving scenes that had cars at many different distances. We
quantified system performance by measuring average precision and we report a
trend relating system resolution and object detection performance. We also
quantified the large performance degradation under nighttime conditions,
compared to daytime, for all cameras and a COCO pre-trained network.",None,-1
d1092b3c-bd69-4383-bcbd-87493effe708,BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text,0.930517,"Real-world NLP applications often deal with nonstandard text (e.g.,
dialectal, informal, or misspelled text). However, language models like BERT
deteriorate in the face of dialect variation or noise. How do we push BERT's
modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it
is designed for specializing a model to a task and does not seem to bring about
the deeper, more pervasive changes needed to adapt a model to nonstandard
language. In this paper, we introduce the novel idea of sandwiching BERT's
encoder stack between additional encoder layers trained to perform masked
language modeling on noisy text. We find that our approach, paired with recent
work on including character-level noise in fine-tuning data, can promote
zero-shot transfer to dialectal text, as well as reduce the distance in the
embedding space between words and their noisy counterparts.",None,-1
b7f823ef-56bd-4fae-ae3c-4f382b7f8d63,HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge,0.977209,"Large Language Models (LLMs), such as the LLaMA model, have demonstrated
their effectiveness in various general-domain natural language processing (NLP)
tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain
tasks due to the need for medical expertise in the responses. In response to
this challenge, we propose HuaTuo, a LLaMA-based model that has been
supervised-fine-tuned with generated QA (Question-Answer) instances. The
experimental results demonstrate that HuaTuo generates responses that possess
more reliable medical knowledge. Our proposed HuaTuo model is accessible at
https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.",None,-1
45700f94-b42c-4a91-b960-c1c5f7dc65f5,DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents,0.457834,"Information Extraction from visually rich documents is a challenging task
that has gained a lot of attention in recent years due to its importance in
several document-control based applications and its widespread commercial
value. The majority of the research work conducted on this topic to date follow
a two-step pipeline. First, they read the text using an off-the-shelf Optical
Character Recognition (OCR) engine, then, they extract the fields of interest
from the obtained text. The main drawback of these approaches is their
dependence on an external OCR system, which can negatively impact both
performance and computational speed. Recent OCR-free methods were proposed to
address the previous issues. Inspired by their promising results, we propose in
this paper an OCR-free end-to-end information extraction model named DocParser.
It differs from prior end-to-end approaches by its ability to better extract
discriminative character features. DocParser achieves state-of-the-art results
on various datasets, while still being faster than previous works.",None,-1
3bf467a8-41a8-45bf-a48e-e78095fa25d0,Using Artificial Populations to Study Psychological Phenomena in Neural Models,0.0579342,"The recent proliferation of research into transformer based natural language
processing has led to a number of studies which attempt to detect the presence
of human-like cognitive behavior in the models. We contend that, as is true of
human psychology, the investigation of cognitive behavior in language models
must be conducted in an appropriate population of an appropriate size for the
results to be meaningful. We leverage work in uncertainty estimation in a novel
approach to efficiently construct experimental populations. The resultant tool,
PopulationLM, has been made open source. We provide theoretical grounding in
the uncertainty estimation literature and motivation from current cognitive
work regarding language models. We discuss the methodological lessons from
other scientific communities and attempt to demonstrate their application to
two artificial population studies. Through population based experimentation we
find that language models exhibit behavior consistent with typicality effects
among categories highly represented in training. However, we find that language
models don't tend to exhibit structural priming effects. Generally, our results
show that single models tend to over estimate the presence of cognitive
behaviors in neural models.",None,-1
500e3e3a-0be5-4b43-b767-49c953c618e7,Online Gesture Recognition using Transformer and Natural Language Processing,0.261288,"The Transformer architecture is shown to provide a powerful machine
transduction framework for online handwritten gestures corresponding to glyph
strokes of natural language sentences. The attention mechanism is successfully
used to create latent representations of an end-to-end encoder-decoder model,
solving multi-level segmentation while also learning some language features and
syntax rules. The additional use of a large decoding space with some learned
Byte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and
syntax rules. The encoder stack was directly fed with spatio-temporal data
tokens potentially forming an infinitely large input vocabulary, an approach
that finds applications beyond that of this work. Encoder transfer learning
capabilities is also demonstrated on several languages resulting in faster
optimisation and shared parameters. A new supervised dataset of online
handwriting gestures suitable for generic handwriting recognition tasks was
used to successfully train a small transformer model to an average normalised
Levenshtein accuracy of 96% on English or German sentences and 94% in French.",None,-1
dd2d95b1-b245-46c2-8d48-bf7fc03ee8b0,Honesty Is the Best Policy: Defining and Mitigating AI Deception,0.659979,"Deceptive agents are a challenge for the safety, trustworthiness, and
cooperation of AI systems. We focus on the problem that agents might deceive in
order to achieve their goals (for instance, in our experiments with language
models, the goal of being evaluated as truthful). There are a number of
existing definitions of deception in the literature on game theory and symbolic
AI, but there is no overarching theory of deception for learning agents in
games. We introduce a formal definition of deception in structural causal
games, grounded in the philosophy literature, and applicable to real-world
machine learning systems. Several examples and results illustrate that our
formal definition aligns with the philosophical and commonsense meaning of
deception. Our main technical result is to provide graphical criteria for
deception. We show, experimentally, that these results can be used to mitigate
deception in reinforcement learning agents and language models.",None,-1
62802ceb-e63b-4bf5-9ead-02dffb356d73,Controllable Light Diffusion for Portraits,0.262855,"We introduce light diffusion, a novel method to improve lighting in
portraits, softening harsh shadows and specular highlights while preserving
overall scene illumination. Inspired by professional photographers' diffusers
and scrims, our method softens lighting given only a single portrait photo.
Previous portrait relighting approaches focus on changing the entire lighting
environment, removing shadows (ignoring strong specular highlights), or
removing shading entirely. In contrast, we propose a learning based method that
allows us to control the amount of light diffusion and apply it on in-the-wild
portraits. Additionally, we design a method to synthetically generate plausible
external shadows with sub-surface scattering effects while conforming to the
shape of the subject's face. Finally, we show how our approach can increase the
robustness of higher level vision applications, such as albedo estimation,
geometry estimation and semantic segmentation.",None,-1
3e1930e1-bfe5-4f30-ac2e-6fb0cd95bb93,Towards Bridging the Digital Language Divide,0.0422965,"It is a well-known fact that current AI-based language technology -- language
models, machine translation systems, multilingual dictionaries and corpora --
focuses on the world's 2-3% most widely spoken languages. Recent research
efforts have attempted to expand the coverage of AI technology to
`under-resourced languages.' The goal of our paper is to bring attention to a
phenomenon that we call linguistic bias: multilingual language processing
systems often exhibit a hardwired, yet usually involuntary and hidden
representational preference towards certain languages. Linguistic bias is
manifested in uneven per-language performance even in the case of similar test
conditions. We show that biased technology is often the result of research and
development methodologies that do not do justice to the complexity of the
languages being represented, and that can even become ethically problematic as
they disregard valuable aspects of diversity as well as the needs of the
language communities themselves. As our attempt at building diversity-aware
language resources, we present a new initiative that aims at reducing
linguistic bias through both technological design and methodology, based on an
eye-level collaboration with local communities.",None,-1
2a097229-15d7-4a8b-ab63-261e4d3f44d5,WiCE: Real-World Entailment for Claims in Wikipedia,0.868313,"Textual entailment models are increasingly applied in settings like
fact-checking, presupposition verification in question answering, or summary
evaluation. However, these represent a significant domain shift from existing
entailment datasets, and models underperform as a result. We propose WiCE, a
new fine-grained textual entailment dataset built on natural claim and evidence
pairs extracted from Wikipedia. In addition to standard claim-level entailment,
WiCE provides entailment judgments over sub-sentence units of the claim, and a
minimal subset of evidence sentences that support each subclaim. To support
this, we propose an automatic claim decomposition strategy using GPT-3.5 which
we show is also effective at improving entailment models' performance on
multiple datasets at test time. Finally, we show that real claims in our
dataset involve challenging verification and retrieval problems that existing
models fail to address.",None,-1
a6b0917e-5653-4bae-b9aa-60b2b350aa3d,VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna,0.757909,"Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and
few-shot capabilities in Named Entity Recognition (NER). However, these models
can only be accessed via online APIs, which may cause data leak and
non-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot
NER framework based on the newly released open-source LLM -- Vicuna. VicunaNER
is a two-phase framework, where each phase leverages multi-turn dialogues with
Vicuna to recognize entities from texts. We name the second phase as
Re-Recognition, which recognizes those entities not recognized in the first
phase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues
in each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot
capacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.
Experimental results demonstrate that VicunaNER achieves superior performance
in both shot settings. Additionally, we conduct comprehensive investigations on
Vicuna from multiple perspectives.",None,-1
e9041a98-9cb1-478f-b4f4-563365b19690,Dual-level Interaction for Domain Adaptive Semantic Segmentation,0.146989,"Self-training approach recently secures its position in domain adaptive
semantic segmentation, where a model is trained with target domain
pseudo-labels. Current advances have mitigated noisy pseudo-labels resulting
from the domain gap. However, they still struggle with erroneous pseudo-labels
near the boundaries of the semantic classifier. In this paper, we tackle this
issue by proposing a dual-level interaction for domain adaptation (DIDA) in
semantic segmentation. Explicitly, we encourage the different augmented views
of the same pixel to have not only similar class prediction (semantic-level)
but also akin similarity relationship with respect to other pixels
(instance-level). As it's impossible to keep features of all pixel instances
for a dataset, we, therefore, maintain a labeled instance bank with dynamic
updating strategies to selectively store the informative features of instances.
Further, DIDA performs cross-level interaction with scattering and gathering
techniques to regenerate more reliable pseudo-labels. Our method outperforms
the state-of-the-art by a notable margin, especially on confusing and
long-tailed classes. Code is available at
\href{https://github.com/RainJamesY/DIDA}",None,-1
ad00faf3-6ab6-4907-a81b-b8e5dbd61c64,Open-Source LLMs for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning,0.949258,"This paper studies the performance of open-source Large Language Models
(LLMs) in text classification tasks typical for political science research. By
examining tasks like stance, topic, and relevance classification, we aim to
guide scholars in making informed decisions about their use of LLMs for text
analysis. Specifically, we conduct an assessment of both zero-shot and
fine-tuned LLMs across a range of text annotation tasks using news articles and
tweets datasets. Our analysis shows that fine-tuning improves the performance
of open-source LLMs, allowing them to match or even surpass zero-shot GPT-3.5
and GPT-4, though still lagging behind fine-tuned GPT-3.5. We further establish
that fine-tuning is preferable to few-shot training with a relatively modest
quantity of annotated text. Our findings show that fine-tuned open-source LLMs
can be effectively deployed in a broad spectrum of text annotation
applications. We provide a Python notebook facilitating the application of LLMs
in text annotation for other researchers.",None,-1
bc92fa02-5ecd-4f5f-b465-2a7d9962a2b7,NeRF-Supervised Deep Stereo,0.572812,"We introduce a novel framework for training deep stereo networks effortlessly
and without any ground-truth. By leveraging state-of-the-art neural rendering
solutions, we generate stereo training data from image sequences collected with
a single handheld camera. On top of them, a NeRF-supervised training procedure
is carried out, from which we exploit rendered stereo triplets to compensate
for occlusions and depth maps as proxy labels. This results in stereo networks
capable of predicting sharp and detailed disparity maps. Experimental results
show that models trained under this regime yield a 30-40% improvement over
existing self-supervised methods on the challenging Middlebury dataset, filling
the gap to supervised models and, most times, outperforming them at zero-shot
generalization.",None,-1
041bf4c9-22ea-45d3-847c-ac51e9526cb3,Watch out Venomous Snake Species: A Solution to SnakeCLEF2023,0.779217,"The SnakeCLEF2023 competition aims to the development of advanced algorithms
for snake species identification through the analysis of images and
accompanying metadata. This paper presents a method leveraging utilization of
both images and metadata. Modern CNN models and strong data augmentation are
utilized to learn better representation of images. To relieve the challenge of
long-tailed distribution, seesaw loss is utilized in our method. We also design
a light model to calculate prior probabilities using metadata features
extracted from CLIP in post processing stage. Besides, we attach more
importance to venomous species by assigning venomous species labels to some
examples that model is uncertain about. Our method achieves 91.31% score of the
final metric combined of F1 and other metrics on private leaderboard, which is
the 1st place among the participators. The code is available at
https://github.com/xiaoxsparraw/CLEF2023.",None,-1
43f6f814-9568-47ba-8be3-40e6aa541b3e,Facial Expression Recognition at the Edge: CPU vs GPU vs VPU vs TPU,0.688763,"Facial Expression Recognition (FER) plays an important role in human-computer
interactions and is used in a wide range of applications. Convolutional Neural
Networks (CNN) have shown promise in their ability to classify human facial
expressions, however, large CNNs are not well-suited to be implemented on
resource- and energy-constrained IoT devices. In this work, we present a
hierarchical framework for developing and optimizing hardware-aware CNNs tuned
for deployment at the edge. We perform a comprehensive analysis across various
edge AI accelerators including NVIDIA Jetson Nano, Intel Neural Compute Stick,
and Coral TPU. Using the proposed strategy, we achieved a peak accuracy of
99.49% when testing on the CK+ facial expression recognition dataset.
Additionally, we achieved a minimum inference latency of 0.39 milliseconds and
a minimum power consumption of 0.52 Watts.",None,-1
85668f1d-02c1-4aa1-82f5-9da73f666e0d,KwaiAgents: Generalized Information-seeking Agent System with Large Language Models,0.458342,"Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user's query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system's performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.",None,-1
31aae24d-049f-4ed7-9953-dd1515775b5b,SpotEM: Efficient Video Search for Episodic Memory,0.796142,"The goal in episodic memory (EM) is to search a long egocentric video to
answer a natural language query (e.g., ""where did I leave my purse?""). Existing
EM methods exhaustively extract expensive fixed-length clip features to look
everywhere in the video for the answer, which is infeasible for long
wearable-camera videos that span hours or even days. We propose SpotEM, an
approach to achieve efficiency for a given EM method while maintaining good
accuracy. SpotEM consists of three key ideas: 1) a novel clip selector that
learns to identify promising video regions to search conditioned on the
language query; 2) a set of low-cost semantic indexing features that capture
the context of rooms, objects, and interactions that suggest where to look; and
3) distillation losses that address the optimization issues arising from
end-to-end joint training of the clip selector and EM model. Our experiments on
200+ hours of video from the Ego4D EM Natural Language Queries benchmark and
three different EM models demonstrate the effectiveness of our approach:
computing only 10% - 25% of the clip features, we preserve 84% - 97% of the
original EM model's accuracy. Project page:
https://vision.cs.utexas.edu/projects/spotem",None,-1
a5a934c7-95b9-4cdf-bf97-aac8fa7d2d2e,Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation,0.166288,"Recent advancements in self-supervised learning have demonstrated that
effective visual representations can be learned from unlabeled images. This has
led to increased interest in applying self-supervised learning to the medical
domain, where unlabeled images are abundant and labeled images are difficult to
obtain. However, most self-supervised learning approaches are modeled as image
level discriminative or generative proxy tasks, which may not capture the finer
level representations necessary for dense prediction tasks like multi-organ
segmentation. In this paper, we propose a novel contrastive learning framework
that integrates Localized Region Contrast (LRC) to enhance existing
self-supervised pre-training methods for medical image segmentation. Our
approach involves identifying Super-pixels by Felzenszwalb's algorithm and
performing local contrastive learning using a novel contrastive sampling loss.
Through extensive experiments on three multi-organ segmentation datasets, we
demonstrate that integrating LRC to an existing self-supervised method in a
limited annotation setting significantly improves segmentation performance.
Moreover, we show that LRC can also be applied to fully-supervised pre-training
methods to further boost performance.",None,-1
bda6f0c6-55d1-4f9b-b3e0-fbfce7750201,Conceptual Views on Tree Ensemble Classifiers,0.334503,"Random Forests and related tree-based methods are popular for supervised
learning from table based data. Apart from their ease of parallelization, their
classification performance is also superior. However, this performance,
especially parallelizability, is offset by the loss of explainability.
Statistical methods are often used to compensate for this disadvantage. Yet,
their ability for local explanations, and in particular for global
explanations, is limited. In the present work we propose an algebraic method,
rooted in lattice theory, for the (global) explanation of tree ensembles. In
detail, we introduce two novel conceptual views on tree ensemble classifiers
and demonstrate their explanatory capabilities on Random Forests that were
trained with standard parameters.",None,-1
dcfb1be7-64cc-4b35-8433-0fc36f7a4e54,SEM-POS: Grammatically and Semantically Correct Video Captioning,0.0978036,"Generating grammatically and semantically correct captions in video
captioning is a challenging task. The captions generated from the existing
methods are either word-by-word that do not align with grammatical structure or
miss key information from the input videos. To address these issues, we
introduce a novel global-local fusion network, with a Global-Local Fusion Block
(GLFB) that encodes and fuses features from different parts of speech (POS)
components with visual-spatial features. We use novel combinations of different
POS components - 'determinant + subject', 'auxiliary verb', 'verb', and
'determinant + object' for supervision of the POS blocks - Det + Subject, Aux
Verb, Verb, and Det + Object respectively. The novel global-local fusion
network together with POS blocks helps align the visual features with language
description to generate grammatically and semantically correct captions.
Extensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT
datasets demonstrate that the proposed approach generates more grammatically
and semantically correct captions compared to the existing methods, achieving
the new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate
the impact of the contributions on the proposed method.",None,-1
d48de97e-17f3-4d18-af08-ec4ba6be9b9c,SRFormer: Permuted Self-Attention for Single Image Super-Resolution,0.932902,"Previous works have shown that increasing the window size for
Transformer-based image super-resolution models (e.g., SwinIR) can
significantly improve the model performance but the computation overhead is
also considerable. In this paper, we present SRFormer, a simple but novel
method that can enjoy the benefit of large window self-attention but introduces
even less computational burden. The core of our SRFormer is the permuted
self-attention (PSA), which strikes an appropriate balance between the channel
and spatial information for self-attention. Our PSA is simple and can be easily
applied to existing super-resolution networks based on window self-attention.
Without any bells and whistles, we show that our SRFormer achieves a 33.86dB
PSNR score on the Urban100 dataset, which is 0.46dB higher than that of SwinIR
but uses fewer parameters and computations. We hope our simple and effective
approach can serve as a useful tool for future research in super-resolution
model design.",None,-1
93aaddcd-286d-4d6d-9e65-b261e23d3b42,Forward-Forward Contrastive Learning,0.208508,"Medical image classification is one of the most important tasks for
computer-aided diagnosis. Deep learning models, particularly convolutional
neural networks, have been successfully used for disease classification from
medical images, facilitated by automated feature learning. However, the diverse
imaging modalities and clinical pathology make it challenging to construct
generalized and robust classifications. Towards improving the model
performance, we propose a novel pretraining approach, namely Forward Forward
Contrastive Learning (FFCL), which leverages the Forward-Forward Algorithm in a
contrastive learning framework--both locally and globally. Our experimental
results on the chest X-ray dataset indicate that the proposed FFCL achieves
superior performance (3.69% accuracy over ImageNet pretrained ResNet-18) over
existing pretraining models in the pneumonia classification task. Moreover,
extensive ablation experiments support the particular local and global
contrastive pretraining design in FFCL.",None,-1
2f8be666-2de4-4b81-a120-57d4b22890cd,Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory,0.999999,"The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular ""ObtainDiamond"" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the ""ObtainDiamond"" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the ""ObtainDiamond"" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.",None,-1
dc3e91ba-d303-4af1-bba2-f373cb0e2ace,Design of Chain-of-Thought in Math Problem Solving,0.238756,"Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem
solving. We conduct a comprehensive examination of methods for designing CoT,
comparing conventional natural language CoT with various program CoTs,
including the self-describing program, the comment-describing program, and the
non-describing program. Furthermore, we investigate the impact of programming
language on program CoTs, comparing Python and Wolfram Language. Through
extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs
often have superior effectiveness in math problem solving. Notably, the best
performing combination with 30B parameters beats GPT-3.5-turbo by a significant
margin. The results show that self-describing program offers greater diversity
and thus can generally achieve higher performance. We also find that Python is
a better choice of language than Wolfram for program CoTs. The experimental
results provide a valuable guideline for future CoT designs that take into
account both programming language and coding style for further advancements.
Our datasets and code are publicly available.",None,-1
1ce9c45c-1536-49d2-a3fc-64f927c7393e,Demystifying Misconceptions in Social Bots Research,0.922643,"Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.",None,-1
2b0174bd-3d01-4ff5-9b9a-69248a28ad31,Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero,0.707133,"Artificial Intelligence (AI) systems have made remarkable progress, attaining
super-human performance across various domains. This presents us with an
opportunity to further human knowledge and improve human expert performance by
leveraging the hidden knowledge encoded within these highly performant AI
systems. Yet, this knowledge is often hard to extract, and may be hard to
understand or learn from. Here, we show that this is possible by proposing a
new method that allows us to extract new chess concepts in AlphaZero, an AI
system that mastered the game of chess via self-play without human supervision.
Our analysis indicates that AlphaZero may encode knowledge that extends beyond
the existing human knowledge, but knowledge that is ultimately not beyond human
grasp, and can be successfully learned from. In a human study, we show that
these concepts are learnable by top human experts, as four top chess
grandmasters show improvements in solving the presented concept prototype
positions. This marks an important first milestone in advancing the frontier of
human knowledge by leveraging AI; a development that could bear profound
implications and help us shape how we interact with AI systems across many AI
applications.",None,-1
26d0a1e2-becb-4a0e-8918-d9aa22bef836,PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model,0.586783,"Despite the remarkable progress in natural language understanding with
pretrained Transformers, neural language models often do not handle commonsense
knowledge well. Toward commonsense-aware models, there have been attempts to
obtain knowledge, ranging from automatic acquisition to crowdsourcing. However,
it is difficult to obtain a high-quality knowledge base at a low cost,
especially from scratch. In this paper, we propose PHALM, a method of building
a knowledge graph from scratch, by prompting both crowdworkers and a large
language model (LLM). We used this method to build a Japanese event knowledge
graph and trained Japanese commonsense generation models. Experimental results
revealed the acceptability of the built graph and inferences generated by the
trained models. We also report the difference in prompting humans and an LLM.
Our code, data, and models are available at
github.com/nlp-waseda/comet-atomic-ja.",None,-1
8f96097d-3ebe-495e-943e-de849b779b74,In-context Learning and Gradient Descent Revisited,0.216296,"In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. A recent
line of work suggests that ICL performs gradient descent (GD)-based
optimization implicitly. While appealing, much of the research focuses on
simplified settings, where the parameters of a shallow model are optimized. In
this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks
and models. We find gaps in evaluation, both in terms of problematic metrics
and insufficient baselines. We show that surprisingly, even untrained models
achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,
we explore a major discrepancy in the flow of information throughout the model
between ICL and GD, which we term Layer Causality. We propose a simple GD-based
optimization procedure that respects layer causality, and show it improves
similarity scores significantly.",None,-1
43d2c366-347b-4992-aae6-1ce46d8eedbb,C3: Zero-shot Text-to-SQL with ChatGPT,0.935833,"This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,
which achieves 82.3\% in terms of execution accuracy on the holdout test set of
Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the
Spider Challenge. C3 consists of three key components: Clear Prompting (CP),
Calibration with Hints (CH), and Consistent Output (CO), which are
corresponding to the model input, model bias and model output respectively. It
provides a systematic treatment for zero-shot Text-to-SQL. Extensive
experiments have been conducted to verify the effectiveness and efficiency of
our proposed method.",None,-1
14dbe66c-4c32-4504-83ea-42904882f32e,Spatial-temporal Transformer for Affective Behavior Analysis,0.625558,"The in-the-wild affective behavior analysis has been an important study. In
this paper, we submit our solutions for the 5th Workshop and Competition on
Affective Behavior Analysis in-the-wild (ABAW), which includes V-A Estimation,
Facial Expression Classification and AU Detection Sub-challenges. We propose a
Transformer Encoder with Multi-Head Attention framework to learn the
distribution of both the spatial and temporal features. Besides, there are
virious effective data augmentation strategies employed to alleviate the
problems of sample imbalance during model training. The results fully
demonstrate the effectiveness of our proposed model based on the Aff-Wild2
dataset.",None,-1
29ec6f64-b098-46d8-8b1c-3d8b2f20089f,TopicGPT: A Prompt-based Topic Modeling Framework,0.817646,"Topic modeling is a well-established technique for exploring text corpora.
Conventional topic models (e.g., LDA) represent topics as bags of words that
often require ""reading the tea leaves"" to interpret; additionally, they offer
users minimal control over the formatting and specificity of resulting topics.
To tackle these issues, we introduce TopicGPT, a prompt-based framework that
uses large language models (LLMs) to uncover latent topics in a text
collection. TopicGPT produces topics that align better with human
categorizations compared to competing methods: it achieves a harmonic mean
purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for
the strongest baseline. Its topics are also interpretable, dispensing with
ambiguous bags of words in favor of topics with natural language labels and
associated free-form descriptions. Moreover, the framework is highly adaptable,
allowing users to specify constraints and modify topics without the need for
model retraining. By streamlining access to high-quality and interpretable
topics, TopicGPT represents a compelling, human-centered approach to topic
modeling.",None,-1
5546f0ce-541b-4476-8e06-48d600f715c9,SCB-dataset: A Dataset for Detecting Student Classroom Behavior,0.622878,"The use of deep learning methods for automatic detection of students'
classroom behavior is a promising approach to analyze their class performance
and enhance teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose a Student Classroom Behavior dataset
(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248
labels and 4,003 images, with a focus on hand-raising behavior. We evaluated
the dataset using the YOLOv7 algorithm, achieving a mean average precision
(map) of up to 85.3%. We believe that our dataset can serve as a robust
foundation for future research in the field of student behavior detection and
promote further advancements in this area.Our SCB-dataset can be downloaded
from: https://github.com/Whiffe/SCB-dataset",None,-1
5fd66f23-c386-4511-8496-31d572e8d063,LARP: Language-Agent Role Play for Open-World Games,0.300256,"Language agents have shown impressive problem-solving skills within defined
settings and brief timelines. Yet, with the ever-evolving complexities of
open-world simulations, there's a pressing need for agents that can flexibly
adapt to complex environments and consistently maintain a long-term memory to
ensure coherent actions. To bridge the gap between language agents and
open-world games, we introduce Language Agent for Role-Playing (LARP), which
includes a cognitive architecture that encompasses memory processing and a
decision-making assistant, an environment interaction module with a
feedback-driven learnable action space, and a postprocessing method that
promotes the alignment of various personalities. The LARP framework refines
interactions between users and agents, predefined with unique backgrounds and
personalities, ultimately enhancing the gaming experience in open-world
contexts. Furthermore, it highlights the diverse uses of language models in a
range of areas such as entertainment, education, and various simulation
scenarios. The project page is released at https://miao-ai-lab.github.io/LARP/.",None,-1
566bd8c3-092a-4605-b8ff-f84d8b976a87,An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning,0.369978,"The Rashomon Effect describes the following phenomenon: for a given dataset
there may exist many models with equally good performance but with different
solution strategies. The Rashomon Effect has implications for Explainable
Machine Learning, especially for the comparability of explanations. We provide
a unified view on three different comparison scenarios and conduct a
quantitative evaluation across different datasets, models, attribution methods,
and metrics. We find that hyperparameter-tuning plays a role and that metric
selection matters. Our results provide empirical support for previously
anecdotal evidence and exhibit challenges for both scientists and
practitioners.",None,-1
ebd18bad-b375-450a-b412-3f6bfe5a3caf,"Learning with Constraint Learning: New Perspective, Solution Strategy and Various Applications",0.357507,"The complexity of learning problems, such as Generative Adversarial Network
(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,
and a variety of real-world vision applications, demands a deeper understanding
of their underlying coupling mechanisms. Existing approaches often address
these problems in isolation, lacking a unified perspective that can reveal
commonalities and enable effective solutions. Therefore, in this work, we
proposed a new framework, named Learning with Constraint Learning (LwCL), that
can holistically examine challenges and provide a unified methodology to tackle
all the above-mentioned complex learning and vision problems. Specifically,
LwCL is designed as a general hierarchical optimization model that captures the
essence of these diverse learning and vision problems. Furthermore, we develop
a gradient-response based fast solution strategy to overcome optimization
challenges of the LwCL framework. Our proposed framework efficiently addresses
a wide range of applications in learning and vision, encompassing three
categories and nine different problem types. Extensive experiments on synthetic
tasks and real-world applications verify the effectiveness of our approach. The
LwCL framework offers a comprehensive solution for tackling complex machine
learning and computer vision problems, bridging the gap between theory and
practice.",None,-1
2659b12e-3e97-4d21-8e3e-e7d8251ae3fe,Model-based learning for location-to-channel mapping,0.394077,"Modern communication systems rely on accurate channel estimation to achieve
efficient and reliable transmission of information. As the communication
channel response is highly related to the user's location, one can use a neural
network to map the user's spatial coordinates to the channel coefficients.
However, these latter are rapidly varying as a function of the location, on the
order of the wavelength. Classical neural architectures being biased towards
learning low frequency functions (spectral bias), such mapping is therefore
notably difficult to learn. In order to overcome this limitation, this paper
presents a frugal, model-based network that separates the low frequency from
the high frequency components of the target mapping function. This yields an
hypernetwork architecture where the neural network only learns low frequency
sparse coefficients in a dictionary of high frequency components. Simulation
results show that the proposed neural network outperforms standard approaches
on realistic synthetic data.",None,-1
36b816d0-5cf8-439a-b482-ee1e291892c9,Indonesian Automatic Speech Recognition with XLSR-53,0.317734,"This study focuses on the development of Indonesian Automatic Speech
Recognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for
cross-lingual speech representations. The use of this XLSR-53 pre-trained model
is to significantly reduce the amount of training data in non-English languages
required to achieve a competitive Word Error Rate (WER). The total amount of
data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14
hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common
Voice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in
this study can compete with similar models using the Common Voice dataset split
test. WER can be decreased by around 8% using a language model, resulted in WER
from 20% to 12%. Thus, the results of this study have succeeded in perfecting
previous research in contributing to the creation of a better Indonesian ASR
with a smaller amount of data.",None,-1
c4419a69-b798-47f5-97bc-8dfc9e8ed7ee,Suspicious Vehicle Detection Using Licence Plate Detection And Facial Feature Recognition,0.0929357,"With the increasing need to strengthen vehicle safety and detection, the
availability of pre-existing methods of catching criminals and identifying
vehicles manually through the various traffic surveillance cameras is not only
time-consuming but also inefficient. With the advancement of technology in
every field the use of real-time traffic surveillance models will help
facilitate an easy approach. Keeping this in mind, the main focus of our paper
is to develop a combined face recognition and number plate recognition model to
ensure vehicle safety and real-time tracking of running-away criminals and
stolen vehicles.",None,-1
973e71d9-3f05-4e07-bb03-f3e937a4e8dd,Learning Occupancy for Monocular 3D Object Detection,0.708842,"Monocular 3D detection is a challenging task due to the lack of accurate 3D
information. Existing approaches typically rely on geometry constraints and
dense depth estimates to facilitate the learning, but often fail to fully
exploit the benefits of three-dimensional feature extraction in frustum and 3D
space. In this paper, we propose \textbf{OccupancyM3D}, a method of learning
occupancy for monocular 3D detection. It directly learns occupancy in frustum
and 3D space, leading to more discriminative and informative 3D features and
representations. Specifically, by using synchronized raw sparse LiDAR point
clouds, we define the space status and generate voxel-based occupancy labels.
We formulate occupancy prediction as a simple classification problem and design
associated occupancy losses. Resulting occupancy estimates are employed to
enhance original frustum/3D features. As a result, experiments on KITTI and
Waymo open datasets demonstrate that the proposed method achieves a new state
of the art and surpasses other methods by a significant margin. Codes and
pre-trained models will be available at
\url{https://github.com/SPengLiang/OccupancyM3D}.",None,-1
dcec6d6e-94f0-4709-9bec-129572ea7d4b,Counterfactual Formulation of Patient-Specific Root Causes of Disease,0.333952,"Root causes of disease intuitively correspond to root vertices that increase
the likelihood of a diagnosis. This description of a root cause nevertheless
lacks the rigorous mathematical formulation needed for the development of
computer algorithms designed to automatically detect root causes from data.
Prior work defined patient-specific root causes of disease using an
interventionalist account that only climbs to the second rung of Pearl's Ladder
of Causation. In this theoretical piece, we climb to the third rung by
proposing a counterfactual definition matching clinical intuition based on
fixed factual data alone. We then show how to assign a root causal contribution
score to each variable using Shapley values from explainable artificial
intelligence. The proposed counterfactual formulation of patient-specific root
causes of disease accounts for noisy labels, adapts to disease prevalence and
admits fast computation without the need for counterfactual simulation.",None,-1
f4e83151-f839-47bd-8590-1955b5769d74,Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations,0.406351,"Learned Image Compression (LIC) has recently become the trending technique
for image transmission due to its notable performance. Despite its popularity,
the robustness of LIC with respect to the quality of image reconstruction
remains under-explored. In this paper, we introduce an imperceptible attack
approach designed to effectively degrade the reconstruction quality of LIC,
resulting in the reconstructed image being severely disrupted by noise where
any object in the reconstructed images is virtually impossible. More
specifically, we generate adversarial examples by introducing a Frobenius
norm-based loss function to maximize the discrepancy between original images
and reconstructed adversarial examples. Further, leveraging the insensitivity
of high-frequency components to human vision, we introduce Imperceptibility
Constraint (IC) to ensure that the perturbations remain inconspicuous.
Experiments conducted on the Kodak dataset using various LIC models demonstrate
effectiveness. In addition, we provide several findings and suggestions for
designing future defenses.",None,-1
36965a06-1b98-4ac9-9c50-9d5ac5875529,Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation,0.801062,"We propose a meta-ability decoupling (MAD) paradigm, which brings together
various object navigation methods in an architecture system, allowing them to
mutually enhance each other and evolve together. Based on the MAD paradigm, we
design a multiple thinking (MT) model that leverages distinct thinking to
abstract various meta-abilities. Our method decouples meta-abilities from three
aspects: input, encoding, and reward while employing the multiple thinking
collaboration (MTC) module to promote mutual cooperation between thinking. MAD
introduces a novel qualitative and quantitative interpretability system for
object navigation. Through extensive experiments on AI2-Thor and RoboTHOR, we
demonstrate that our method outperforms state-of-the-art (SOTA) methods on both
typical and zero-shot object navigation tasks.",None,-1
5d30ccf2-4f18-4abc-bd38-98a0490ff0ef,BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion,0.327973,"Spoken languages often utilise intonation, rhythm, intensity, and structure,
to communicate intention, which can be interpreted differently depending on the
rhythm of speech of their utterance. These speech acts provide the foundation
of communication and are unique in expression to the language. Recent
advancements in attention-based models, demonstrating their ability to learn
powerful representations from multilingual datasets, have performed well in
speech tasks and are ideal to model specific tasks in low resource languages.
Here, we develop a novel multimodal approach combining two models, wav2vec2.0
for audio and MarianMT for text translation, by using multimodal attention
fusion to predict speech acts in our prepared Bengali speech corpus. We also
show that our model BeAts ($\underline{\textbf{Be}}$ngali speech acts
recognition using Multimodal $\underline{\textbf{At}}$tention
Fu$\underline{\textbf{s}}$ion) significantly outperforms both the unimodal
baseline using only speech data and a simpler bimodal fusion using both speech
and text data. Project page: https://soumitri2001.github.io/BeAts",None,-1
f782d452-8f26-4fe0-95c0-a37b093f7925,"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion",0.525978,"Producing quality segmentation masks for images is a fundamental problem in
computer vision. Recent research has explored large-scale supervised training
to enable zero-shot segmentation on virtually any image style and unsupervised
training to enable segmentation without dense annotations. However,
constructing a model capable of segmenting anything in a zero-shot manner
without any annotations is still challenging. In this paper, we propose to
utilize the self-attention layers in stable diffusion models to achieve this
goal because the pre-trained stable diffusion model has learned inherent
concepts of objects within its attention layers. Specifically, we introduce a
simple yet effective iterative merging process based on measuring KL divergence
among attention maps to merge them into valid segmentation masks. The proposed
method does not require any training or language dependency to extract quality
segmentation for any images. On COCO-Stuff-27, our method surpasses the prior
unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%
in mean IoU. The project page is at
\url{https://sites.google.com/view/diffseg/home}.",None,-1
cb37aa8c-293e-42dd-bc3d-843e1965ed3f,Emotionally Enhanced Talking Face Generation,0.609572,"Several works have developed end-to-end pipelines for generating lip-synced
talking faces with various real-world applications, such as teaching and
language translation in videos. However, these prior works fail to create
realistic-looking videos since they focus little on people's expressions and
emotions. Moreover, these methods' effectiveness largely depends on the faces
in the training dataset, which means they may not perform well on unseen faces.
To mitigate this, we build a talking face generation framework conditioned on a
categorical emotion to generate videos with appropriate expressions, making
them more realistic and convincing. With a broad range of six emotions, i.e.,
\emph{happiness}, \emph{sadness}, \emph{fear}, \emph{anger}, \emph{disgust},
and \emph{neutral}, we show that our model can adapt to arbitrary identities,
emotions, and languages. Our proposed framework is equipped with a
user-friendly web interface with a real-time experience for talking face
generation with emotions. We also conduct a user study for subjective
evaluation of our interface's usability, design, and functionality. Project
page: https://midas.iiitd.edu.in/emo/",None,-1
14da25a2-5eb4-4ace-8f8d-2967148e71d4,Enhancing Large Language Models with Climate Resources,0.0955383,"Large language models (LLMs) have significantly transformed the landscape of
artificial intelligence by demonstrating their ability in generating human-like
text across diverse topics. However, despite their impressive capabilities,
LLMs lack recent information and often employ imprecise language, which can be
detrimental in domains where accuracy is crucial, such as climate change. In
this study, we make use of recent ideas to harness the potential of LLMs by
viewing them as agents that access multiple sources, including databases
containing recent and precise information about organizations, institutions,
and companies. We demonstrate the effectiveness of our method through a
prototype agent that retrieves emission data from ClimateWatch
(https://www.climatewatchdata.org/) and leverages general Google search. By
integrating these resources with LLMs, our approach overcomes the limitations
associated with imprecise language and delivers more reliable and accurate
information in the critical domain of climate change. This work paves the way
for future advancements in LLMs and their application in domains where
precision is of paramount importance.",None,-1
2792bfb4-fab3-407b-adc3-78a1487e7b03,Enrichment of the NLST and NSCLC-Radiomics computed tomography collections with AI-derived annotations,0.508335,"Public imaging datasets are critical for the development and evaluation of
automated tools in cancer imaging. Unfortunately, many do not include
annotations or image-derived features, complicating their downstream analysis.
Artificial intelligence-based annotation tools have been shown to achieve
acceptable performance and thus can be used to automatically annotate large
datasets. As part of the effort to enrich public data available within NCI
Imaging Data Commons (IDC), here we introduce AI-generated annotations for two
collections of computed tomography images of the chest, NSCLC-Radiomics, and
the National Lung Screening Trial. Using publicly available AI algorithms we
derived volumetric annotations of thoracic organs at risk, their corresponding
radiomics features, and slice-level annotations of anatomical landmarks and
regions. The resulting annotations are publicly available within IDC, where the
DICOM format is used to harmonize the data and achieve FAIR principles. The
annotations are accompanied by cloud-enabled notebooks demonstrating their use.
This study reinforces the need for large, publicly accessible curated datasets
and demonstrates how AI can be used to aid in cancer imaging.",None,-1
d3f87d0c-6249-44bd-bbc7-1c5e681b8034,Designing Behavior Trees from Goal-Oriented LTLf Formulas,0.737184,"Temporal logic can be used to formally specify autonomous agent goals, but
synthesizing planners that guarantee goal satisfaction can be computationally
prohibitive. This paper shows how to turn goals specified using a subset of
finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that
guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for
achievement goals can be derived using achievement-oriented task mission
grammars, leading to missions made up of tasks combined using LTL operators.
Constructing BTs from LTL formulas leads to a relaxed behavior synthesis
problem in which a wide range of planners can implement the action nodes in the
BT. Importantly, any successful trace induced by the planners satisfies the
corresponding LTL formula. The usefulness of the approach is demonstrated in
two ways: a) exploring the alignment between two planners and LTL goals, and b)
solving a sequential key-door problem for a Fetch robot.",None,-1
35430489-967a-4891-8e64-05318d5c4b59,Improving Expert Specialization in Mixture of Experts,0.333294,"Mixture of experts (MoE), introduced over 20 years ago, is the simplest gated
modular neural network architecture. There is renewed interest in MoE because
the conditional computation allows only parts of the network to be used during
each inference, as was recently demonstrated in large scale natural language
processing models. MoE is also of potential interest for continual learning, as
experts may be reused for new tasks, and new experts introduced. The gate in
the MoE architecture learns task decompositions and individual experts learn
simpler functions appropriate to the gate's decomposition. In this paper: (1)
we show that the original MoE architecture and its training method do not
guarantee intuitive task decompositions and good expert utilization, indeed
they can fail spectacularly even for simple data such as MNIST and
FashionMNIST; (2) we introduce a novel gating architecture, similar to
attention, that improves performance and results in a lower entropy task
decomposition; and (3) we introduce a novel data-driven regularization that
improves expert specialization. We empirically validate our methods on MNIST,
FashionMNIST and CIFAR-100 datasets.",None,-1
2cf3f254-4440-45ea-916a-d9bb32b2c66b,CNN-BiLSTM model for English Handwriting Recognition: Comprehensive Evaluation on the IAM Dataset,0.59811,"We present a CNN-BiLSTM system for the problem of offline English handwriting
recognition, with extensive evaluations on the public IAM dataset, including
the effects of model size, data augmentation and the lexicon. Our best model
achieves 3.59\% CER and 9.44\% WER using CNN-BiLSTM network with CTC layer.
Test time augmentation with rotation and shear transformations applied to the
input image, is proposed to increase recognition of difficult cases and found
to reduce the word error rate by 2.5\% points. We also conduct an error
analysis of our proposed method on IAM dataset, show hard cases of handwriting
images and explore samples with erroneous labels. We provide our source code as
public-domain, to foster further research to encourage scientific
reproducibility.",None,-1
d787504f-0e6f-403a-9880-94b61d7ed8f6,Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion,0.425021,"Stochastic Human Motion Prediction (HMP) aims to predict multiple possible
upcoming pose sequences based on past human motion trajectories. Although
previous approaches have shown impressive performance, they face several
issues, including complex training processes and a tendency to generate
predictions that are often inconsistent with the provided history, and
sometimes even becoming entirely unreasonable. To overcome these issues, we
propose DiffMotion, an end-to-end diffusion-based stochastic HMP framework.
DiffMotion's motion predictor is composed of two modules, including (1) a
Transformer-based network for initial motion reconstruction from corrupted
motion, and (2) a Graph Convolutional Network (GCN) to refine the generated
motion considering past observations. Our method, facilitated by this novel
Transformer-GCN module design and a proposed variance scheduler, excels in
predicting accurate, realistic, and consistent motions, while maintaining an
appropriate level of diversity. Our results on benchmark datasets show that
DiffMotion significantly outperforms previous methods in terms of both accuracy
and fidelity, while demonstrating superior robustness.",None,-1
e0217a79-4614-4cdb-9e94-18921dcb82cd,Advancing Referring Expression Segmentation Beyond Single Image,0.794147,"Referring Expression Segmentation (RES) is a widely explored multi-modal
task, which endeavors to segment the pre-existing object within a single image
with a given linguistic expression. However, in broader real-world scenarios,
it is not always possible to determine if the described object exists in a
specific image. Typically, we have a collection of images, some of which may
contain the described objects. The current RES setting curbs its practicality
in such situations. To overcome this limitation, we propose a more realistic
and general setting, named Group-wise Referring Expression Segmentation (GRES),
which expands RES to a collection of related images, allowing the described
objects to be present in a subset of input images. To support this new setting,
we introduce an elaborately compiled dataset named Grouped Referring Dataset
(GRD), containing complete group-wise annotations of target objects described
by given expressions. We also present a baseline method named Grouped Referring
Segmenter (GRSer), which explicitly captures the language-vision and
intra-group vision-vision interactions to achieve state-of-the-art results on
the proposed GRES and related tasks, such as Co-Salient Object Detection and
RES. Our dataset and codes will be publicly released in
https://github.com/yixuan730/group-res.",None,-1
27c49260-c9c4-46c5-81e2-ed883228347a,ABC: Attention with Bilinear Correlation for Infrared Small Target Detection,0.684789,"Infrared small target detection (ISTD) has a wide range of applications in
early warning, rescue, and guidance. However, CNN based deep learning methods
are not effective at segmenting infrared small target (IRST) that it lack of
clear contour and texture features, and transformer based methods also struggle
to achieve significant results due to the absence of convolution induction
bias. To address these issues, we propose a new model called attention with
bilinear correlation (ABC), which is based on the transformer architecture and
includes a convolution linear fusion transformer (CLFT) module with a novel
attention mechanism for feature extraction and fusion, which effectively
enhances target features and suppresses noise. Additionally, our model includes
a u-shaped convolution-dilated convolution (UCDC) module located deeper layers
of the network, which takes advantage of the smaller resolution of deeper
features to obtain finer semantic information. Experimental results on public
datasets demonstrate that our approach achieves state-of-the-art performance.
Code is available at https://github.com/PANPEIWEN/ABC",None,-1
76111383-bf47-463d-ad54-e044e5bcd776,ALDi: Quantifying the Arabic Level of Dialectness of Text,0.81652,"Transcribed speech and user-generated text in Arabic typically contain a
mixture of Modern Standard Arabic (MSA), the standardized language taught in
schools, and Dialectal Arabic (DA), used in daily communications. To handle
this variation, previous work in Arabic NLP has focused on Dialect
Identification (DI) on the sentence or the token level. However, DI treats the
task as binary, whereas we argue that Arabic speakers perceive a spectrum of
dialectness, which we operationalize at the sentence level as the Arabic Level
of Dialectness (ALDi), a continuous linguistic variable. We introduce the
AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences
(17% from news articles and 83% from user comments on those articles) which are
manually labeled with their level of dialectness. We provide a detailed
analysis of AOC-ALDi and show that a model trained on it can effectively
identify levels of dialectness on a range of other corpora (including dialects
and genres not included in AOC-ALDi), providing a more nuanced picture than
traditional DI systems. Through case studies, we illustrate how ALDi can reveal
Arabic speakers' stylistic choices in different situations, a useful property
for sociolinguistic analyses.",None,-1
06b59f20-056a-4950-ab0d-6b31b86d3914,Songs Across Borders: Singable and Controllable Neural Lyric Translation,0.484255,"The development of general-domain neural machine translation (NMT) methods
has advanced significantly in recent years, but the lack of naturalness and
musical constraints in the outputs makes them unable to produce singable lyric
translations. This paper bridges the singability quality gap by formalizing
lyric translation into a constrained translation problem, converting
theoretical guidance and practical techniques from translatology literature to
prompt-driven NMT approaches, exploring better adaptation methods, and
instantiating them to an English-Chinese lyric translation system. Our model
achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and
word boundary recall. In our subjective evaluation, our model shows a 75%
relative enhancement on overall quality, compared against naive fine-tuning
(Code available at https://github.com/Sonata165/ControllableLyricTranslation).",None,-1
03f3007e-3230-4acb-9a68-14cde1493cee,MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR,0.602812,"Based on powerful text-to-image diffusion models, text-to-3D generation has
made significant progress in generating compelling geometry and appearance.
However, existing methods still struggle to recover high-fidelity object
materials, either only considering Lambertian reflectance, or failing to
disentangle BRDF materials from the environment lights. In this work, we
propose Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR
(\textbf{MATLABER}) that leverages a novel latent BRDF auto-encoder for
material generation. We train this auto-encoder with large-scale real-world
BRDF collections and ensure the smoothness of its latent space, which
implicitly acts as a natural distribution of materials. During appearance
modeling in text-to-3D generation, the latent BRDF embeddings, rather than BRDF
parameters, are predicted via a material network. Through exhaustive
experiments, our approach demonstrates the superiority over existing ones in
generating realistic and coherent object materials. Moreover, high-quality
materials naturally enable multiple downstream tasks such as relighting and
material editing. Code and model will be publicly available at
\url{https://sheldontsui.github.io/projects/Matlaber}.",None,-1
47bd00dd-496a-4678-9356-2b4be47639f4,Exploring ordered patterns in the adjacency matrix for improving machine learning on complex networks,0.0766598,"The use of complex networks as a modern approach to understanding the world
and its dynamics is well-established in literature. The adjacency matrix, which
provides a one-to-one representation of a complex network, can also yield
several metrics of the graph. However, it is not always clear that this
representation is unique, as the permutation of lines and rows in the matrix
can represent the same graph. To address this issue, the proposed methodology
employs a sorting algorithm to rearrange the elements of the adjacency matrix
of a complex graph in a specific order. The resulting sorted adjacency matrix
is then used as input for feature extraction and machine learning algorithms to
classify the networks. The results indicate that the proposed methodology
outperforms previous literature results on synthetic and real-world data.",None,-1
e5666935-f7c4-4acb-8181-2652d52e6ef0,A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network,0.583634,"Aiming at the prediction problem of transport capacity risk caused by the
mismatch between the carrying capacity of rail transit network and passenger
flow demand, this paper proposes an explainable prediction method of rail
transit network transport capacity risk based on linear Gaussian Bayesian
network. This method obtains the training data of the prediction model based on
the simulation model of the rail transit system with a three-layer structure
including rail transit network, train flow and passenger flow. A Bayesian
network structure construction method based on the topology of the rail transit
network is proposed, and the MLE (Maximum Likelihood Estimation) method is used
to realize the parameter learning of the Bayesian network. Finally, the
effectiveness of the proposed method is verified by simulation examples.",None,-1
e30921b6-a296-4115-952f-a02804e7059b,OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for Real-time 3D Hand Gesture Classification and Segmentation,0.561454,"Continuous mid-air hand gesture recognition based on captured hand pose
streams is fundamental for human-computer interaction, particularly in AR / VR.
However, many of the methods proposed to recognize heterogeneous hand gestures
are tested only on the classification task, and the real-time low-latency
gesture segmentation in a continuous stream is not well addressed in the
literature. For this task, we propose the On-Off deep Multi-View Multi-Task
paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related
to hand pose and movement to generate rich gesture descriptions, along with
using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the
classical MVMT paradigm, where all of the multiple tasks have to be active at
each time, by allowing specific tasks to switch on/off depending on whether
they can apply to the input. We show that OO-dMVMT defines the new SotA on
continuous/online 3D skeleton-based gesture recognition in terms of gesture
classification accuracy, segmentation accuracy, false positives, and decision
latency while maintaining real-time operation.",None,-1
8fd072bd-29e7-4df6-bc0a-f72fca49f66b,Benchmarking the Generation of Fact Checking Explanations,0.538315,"Fighting misinformation is a challenging, yet crucial, task. Despite the
growing number of experts being involved in manual fact-checking, this activity
is time-consuming and cannot keep up with the ever-increasing amount of Fake
News produced daily. Hence, automating this process is necessary to help curb
misinformation. Thus far, researchers have mainly focused on claim veracity
classification. In this paper, instead, we address the generation of
justifications (textual explanation of why a claim is classified as either true
or false) and benchmark it with novel datasets and advanced baselines. In
particular, we focus on summarization approaches over unstructured knowledge
(i.e. news articles) and we experiment with several extractive and abstractive
strategies. We employed two datasets with different styles and structures, in
order to assess the generalizability of our findings. Results show that in
justification production summarization benefits from the claim information,
and, in particular, that a claim-driven extractive step improves abstractive
summarization performances. Finally, we show that although cross-dataset
experiments suffer from performance degradation, a unique model trained on a
combination of the two datasets is able to retain style information in an
efficient manner.",None,-1
5dbba6cd-76a0-4e36-bb6f-aaeef793a62c,Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits,0.586906,"We present Second Thought, a new learning paradigm that enables language
models (LMs) to re-align with human values. By modeling the chain-of-edits
between value-unaligned and value-aligned text, with LM fine-tuning and
additional refinement through reinforcement learning, Second Thought not only
achieves superior performance in three value alignment benchmark datasets but
also shows strong human-value transfer learning ability in few-shot scenarios.
The generated editing steps also offer better interpretability and ease for
interactive error correction. Extensive human evaluations further confirm its
effectiveness.",None,-1
b48ddbfb-d05f-46d1-aad0-ff65374b24fa,From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models,0.0286899,"Instruction tuning is essential for large language models (LLMs) to become
interactive. While many instruction tuning datasets exist in English, there is
a noticeable lack in other languages. Also, their effectiveness has not been
well verified in non-English languages. We construct a Japanese instruction
dataset by expanding and filtering existing datasets and apply the dataset to a
Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning
on both Japanese and English existing models using our instruction dataset. We
evaluated these models from both quantitative and qualitative perspectives. As
a result, the effectiveness of Japanese instruction datasets is confirmed. The
results also indicate that even with relatively small LLMs, performances in
downstream tasks would be improved through instruction tuning. Our instruction
dataset, tuned models, and implementation are publicly available online.",None,-1
f3a5c47f-34ad-4413-b79b-a4e0efedc439,Multi-Temporal Lip-Audio Memory for Visual Speech Recognition,0.536859,"Visual Speech Recognition (VSR) is a task to predict a sentence or word from
lip movements. Some works have been recently presented which use audio signals
to supplement visual information. However, existing methods utilize only
limited information such as phoneme-level features and soft labels of Automatic
Speech Recognition (ASR) networks. In this paper, we present a Multi-Temporal
Lip-Audio Memory (MTLAM) that makes the best use of audio signals to complement
insufficient information of lip movements. The proposed method is mainly
composed of two parts: 1) MTLAM saves multi-temporal audio features produced
from short- and long-term audio signals, and the MTLAM memorizes a
visual-to-audio mapping to load stored multi-temporal audio features from
visual features at the inference phase. 2) We design an audio temporal model to
produce multi-temporal audio features capturing the context of neighboring
words. In addition, to construct effective visual-to-audio mapping, the audio
temporal models can generate audio features time-aligned with visual features.
Through extensive experiments, we validate the effectiveness of the MTLAM
achieving state-of-the-art performances on two public VSR datasets.",None,-1
8afda243-2faa-4494-a466-2f3340fc1e71,"Metaverse: Requirements, Architecture, Standards, Status, Challenges, and Perspectives",0.671357,"The Metaverse is driving the next wave of innovation for new opportunities by
replacing the digital world (Internet) with the virtual world through a single,
shared, immersive, persistent 3D virtual space. In this paper, we present
requirements, architecture, standards, challenges, and solutions for Metaverse.
Specifically, we provide Metaverse architecture and requirements, and different
standards for Metaverse which serve as the basis for the development and
deployment. Moreover, we present recent status, challenges such as integration
of AI and Metaverse, security and privacy in Metaverse, etc., and perspectives
and solutions.",None,-1
6bf408eb-6d69-4465-aeb2-f96384f4ce3a,"Can I say, now machines can think?",0.0552321,"Generative AI techniques have opened the path for new generations of machines
in diverse domains. These machines have various capabilities for example, they
can produce images, generate answers or stories, and write codes based on the
""prompts"" only provided by users. These machines are considered 'thinking
minds' because they have the ability to generate human-like responses. In this
study, we have analyzed and explored the capabilities of artificial
intelligence-enabled machines. We have revisited on Turing's concept of
thinking machines and compared it with recent technological advancements. The
objections and consequences of the thinking machines are also discussed in this
study, along with available techniques to evaluate machines' cognitive
capabilities. We have concluded that Turing Test is a critical aspect of
evaluating machines' ability. However, there are other aspects of intelligence
too, and AI machines exhibit most of these aspects.",None,-1
5cb3b6fe-052f-42d6-adb8-8627aecf80ff,USTC-NELSLIP at SemEval-2023 Task 2: Statistical Construction and Dual Adaptation of Gazetteer for Multilingual Complex NER,0.376754,"This paper describes the system developed by the USTC-NELSLIP team for
SemEval-2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER
II). A method named Statistical Construction and Dual Adaptation of Gazetteer
(SCDAG) is proposed for Multilingual Complex NER. The method first utilizes a
statistics-based approach to construct a gazetteer. Secondly, the
representations of gazetteer networks and language models are adapted by
minimizing the KL divergence between them at both the sentence-level and
entity-level. Finally, these two networks are then integrated for supervised
named entity recognition (NER) training. The proposed method is applied to
XLM-R with a gazetteer built from Wikidata, and shows great generalization
ability across different tracks. Experimental results and detailed analysis
verify the effectiveness of the proposed method. The official results show that
our system ranked 1st on one track (Hindi) in this task.",None,-1
f731033d-81a2-4ad1-a6e2-fa97e9502e89,Grounded Text-to-Image Synthesis with Attention Refocusing,0.905369,"Driven by the scalable diffusion models trained on large-scale datasets,
text-to-image synthesis methods have shown compelling results. However, these
models still fail to precisely follow the text prompt involving multiple
objects, attributes, or spatial compositions. In this paper, we reveal the
potential causes in the diffusion model's cross-attention and self-attention
layers. We propose two novel losses to refocus attention maps according to a
given spatial layout during sampling. Creating the layouts manually requires
additional effort and can be tedious. Therefore, we explore using large
language models (LLM) to produce these layouts for our method. We conduct
extensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate
our proposed method. We show that our proposed attention refocusing effectively
improves the controllability of existing approaches.",None,-1
b0c623db-e04e-4946-8a20-842fea707bf2,A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit,0.301043,"Data augmentations are known to improve robustness in speech-processing
tasks. In this study, we summarize and compare different data augmentation
strategies using S3PRL toolkit. We explore how HuBERT and wav2vec perform using
different augmentation techniques (SpecAugment, Gaussian Noise, Speed
Perturbation) for Phoneme Recognition (PR) and Automatic Speech Recognition
(ASR) tasks. We evaluate model performance in terms of phoneme error rate (PER)
and word error rate (WER). From the experiments, we observed that SpecAugment
slightly improves the performance of HuBERT and wav2vec on the original
dataset. Also, we show that models trained using the Gaussian Noise and Speed
Perturbation dataset are more robust when tested with augmented test sets.",None,-1
3c57d930-f327-4326-adcc-9bbd593e45d4,Reference-guided Controllable Inpainting of Neural Radiance Fields,0.730219,"The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led
to a desire for NeRF editing tools. Here, we focus on inpainting regions in a
view-consistent and controllable manner. In addition to the typical NeRF inputs
and masks delineating the unwanted region in each view, we require only a
single inpainted view of the scene, i.e., a reference view. We use monocular
depth estimators to back-project the inpainted view to the correct 3D
positions. Then, via a novel rendering technique, a bilateral solver can
construct view-dependent effects in non-reference views, making the inpainted
region appear consistent from any view. For non-reference disoccluded regions,
which cannot be supervised by the single reference view, we devise a method
based on image inpainters to guide both the geometry and appearance. Our
approach shows superior performance to NeRF inpainting baselines, with the
additional advantage that a user can control the generated scene via a single
inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d",None,-1
0233d506-ca12-431a-a805-560e9ad5d898,OpenAi's GPT4 as coding assistant,0.0307734,"Lately, Large Language Models have been widely used in code generation. GPT4
is considered the most potent Large Language Model from Openai. In this paper,
we examine GPT3.5 and GPT4 as coding assistants. More specifically, we have
constructed appropriate tests to check whether the two systems can a) answer
typical questions that can arise during the code development, b) produce
reliable code, and c) contribute to code debugging. The test results are
impressive. The performance of GPT4 is outstanding and signals an increase in
the productivity of programmers and the reorganization of software development
procedures based on these new tools.",None,-1
981ea7f7-9d45-4225-957e-1f2a0424bd02,Referring Camouflaged Object Detection,0.286825,"We consider the problem of referring camouflaged object detection (Ref-COD),
a new task that aims to segment specified camouflaged objects based on a small
set of referring images with salient target objects. We first assemble a
large-scale dataset, called R2C7K, which consists of 7K images covering 64
object categories in real-world scenarios. Then, we develop a simple but strong
dual-branch framework, dubbed R2CNet, with a reference branch embedding the
common representations of target objects from referring images and a
segmentation branch identifying and segmenting camouflaged objects under the
guidance of the common representations. In particular, we design a Referring
Mask Generation module to generate pixel-level prior mask and a Referring
Feature Enrichment module to enhance the capability of identifying specified
camouflaged objects. Extensive experiments show the superiority of our Ref-COD
methods over their COD counterparts in segmenting specified camouflaged objects
and identifying the main body of target objects. Our code and dataset are
publicly available at https://github.com/zhangxuying1004/RefCOD.",None,-1
dd30c008-179b-41d8-8651-fee19cd1b472,Neural Spectro-polarimetric Fields,0.85507,"Modeling the spatial radiance distribution of light rays in a scene has been
extensively explored for applications, including view synthesis. Spectrum and
polarization, the wave properties of light, are often neglected due to their
integration into three RGB spectral bands and their non-perceptibility to human
vision. However, these properties are known to encompass substantial material
and geometric information about a scene. Here, we propose to model
spectro-polarimetric fields, the spatial Stokes-vector distribution of any
light ray at an arbitrary wavelength. We present Neural Spectro-polarimetric
Fields (NeSpoF), a neural representation that models the physically-valid
Stokes vector at given continuous variables of position, direction, and
wavelength. NeSpoF manages inherently noisy raw measurements, showcases memory
efficiency, and preserves physically vital signals - factors that are crucial
for representing the high-dimensional signal of a spectro-polarimetric field.
To validate NeSpoF, we introduce the first multi-view
hyperspectral-polarimetric image dataset, comprised of both synthetic and
real-world scenes. These were captured using our compact
hyperspectral-polarimetric imaging system, which has been calibrated for
robustness against system imperfections. We demonstrate the capabilities of
NeSpoF on diverse scenes.",None,-1
5efda5e6-d6b0-4753-96c2-b224f7e19ea7,ChatGPT is on the Horizon: Could a Large Language Model be Suitable for Intelligent Traffic Safety Research and Applications?,0.575939,"ChatGPT embarks on a new era of artificial intelligence and will
revolutionize the way we approach intelligent traffic safety systems. This
paper begins with a brief introduction about the development of large language
models (LLMs). Next, we exemplify using ChatGPT to address key traffic safety
issues. Furthermore, we discuss the controversies surrounding LLMs, raise
critical questions for their deployment, and provide our solutions. Moreover,
we propose an idea of multi-modality representation learning for smarter
traffic safety decision-making and open more questions for application
improvement. We believe that LLM will both shape and potentially facilitate
components of traffic safety research.",None,-1
04b22659-93b3-4cec-9003-7e7704e162e1,Cross-Lingual Transfer of Cognitive Processing Complexity,0.283124,"When humans read a text, their eye movements are influenced by the structural
complexity of the input sentences. This cognitive phenomenon holds across
languages and recent studies indicate that multilingual language models utilize
structural similarities between languages to facilitate cross-lingual transfer.
We use sentence-level eye-tracking patterns as a cognitive indicator for
structural complexity and show that the multilingual model XLM-RoBERTa can
successfully predict varied patterns for 13 typologically diverse languages,
despite being fine-tuned only on English data. We quantify the sensitivity of
the model to structural complexity and distinguish a range of complexity
characteristics. Our results indicate that the model develops a meaningful bias
towards sentence length but also integrates cross-lingual differences. We
conduct a control experiment with randomized word order and find that the model
seems to additionally capture more complex structural information.",None,-1
04075d7e-e0b2-4290-83c3-72d97f45735f,Large language models in medicine: the potentials and pitfalls,0.505043,"Large language models (LLMs) have been applied to tasks in healthcare,
ranging from medical exam questions to responding to patient questions. With
increasing institutional partnerships between companies producing LLMs and
healthcare systems, real world clinical application is coming closer to
reality. As these models gain traction, it is essential for healthcare
practitioners to understand what LLMs are, their development, their current and
potential applications, and the associated pitfalls when utilized in medicine.
This review and accompanying tutorial aim to give an overview of these topics
to aid healthcare practitioners in understanding the rapidly changing landscape
of LLMs as applied to medicine.",None,-1
4bc19c99-fb0d-465d-b383-2de204113d31,From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,0.994528,"Understanding emotions during conversation is a fundamental aspect of human
communication, driving NLP research for Emotion Recognition in Conversation
(ERC). While considerable research has focused on discerning emotions of
individual speakers in monolingual dialogues, understanding the emotional
dynamics in code-mixed conversations has received relatively less attention.
This motivates our undertaking of ERC for code-mixed conversations in this
study. Recognizing that emotional intelligence encompasses a comprehension of
worldly knowledge, we propose an innovative approach that integrates
commonsense information with dialogue context to facilitate a deeper
understanding of emotions. To achieve this, we devise an efficient pipeline
that extracts relevant commonsense from existing knowledge graphs based on the
code-mixed input. Subsequently, we develop an advanced fusion technique that
seamlessly combines the acquired commonsense information with the dialogue
representation obtained from a dedicated dialogue understanding module. Our
comprehensive experimentation showcases the substantial performance improvement
obtained through the systematic incorporation of commonsense in ERC. Both
quantitative assessments and qualitative analyses further corroborate the
validity of our hypothesis, reaffirming the pivotal role of commonsense
integration in enhancing ERC.",None,-1
42c1ce47-abf4-4146-a6ba-f8d5089ba1fd,Does Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?,0.0798407,"The capabilities of pretrained language models have opened opportunities to
explore new application areas, but applications involving human-human
interaction are limited by the fact that most data is protected from public
release for privacy reasons. Problem-solving human dialogues in real
applications can be much more complex than existing Wizard-of-Oz collections,
preventing successful domain transfer. To support information extraction (IE)
for a private call center dataset, we introduce a human-in-the-loop dialogue
generation framework capable of synthesizing realistic dialogues. In IE
experiments with auto insurance call center dialogues, we observe 25\% relative
improvement in $F_1$ after augmenting a small set of real human conversations
with synthetic data. We release code and our synthetic dataset to illustrate
the complexity of real-world call center conversations and encourage
development of complex dialogue datasets that are more representative of
natural data.",None,-1
5165a9b3-d715-4d4a-b223-13de32516551,HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering,0.208715,"Answering numerical questions over hybrid contents from the given tables and
text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)
have gained significant attention in the NLP community. With the emergence of
large language models, In-Context Learning and Chain-of-Thought prompting have
become two particularly popular research topics in this field. In this paper,
we introduce a new prompting strategy called Hybrid prompt strategy and
Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt
the model to develop the ability of retrieval thinking when dealing with hybrid
data. Our method achieves superior performance compared to the fully-supervised
SOTA on the MultiHiertt dataset in the few-shot setting.",None,-1
4c5ef70f-b205-41e4-a0d6-6bb09585a651,Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction,0.26668,"We tackle the problem of estimating a Manhattan frame, i.e. three orthogonal
vanishing points, and the unknown focal length of the camera, leveraging a
prior vertical direction. The direction can come from an Inertial Measurement
Unit that is a standard component of recent consumer devices, e.g.,
smartphones. We provide an exhaustive analysis of minimal line configurations
and derive two new 2-line solvers, one of which does not suffer from
singularities affecting existing solvers. Additionally, we design a new
non-minimal method, running on an arbitrary number of lines, to boost the
performance in local optimization. Combining all solvers in a hybrid robust
estimator, our method achieves increased accuracy even with a rough prior.
Experiments on synthetic and real-world datasets demonstrate the superior
accuracy of our method compared to the state of the art, while having
comparable runtimes. We further demonstrate the applicability of our solvers
for relative rotation estimation. The code is available at
https://github.com/cvg/VP-Estimation-with-Prior-Gravity.",None,-1
7e3aa28d-c590-4d8f-b0fa-9adcd2a60da2,Personality Understanding of Fictional Characters during Book Reading,0.225562,"Comprehending characters' personalities is a crucial aspect of story reading.
As readers engage with a story, their understanding of a character evolves
based on new events and information; and multiple fine-grained aspects of
personalities can be perceived. This leads to a natural problem of situated and
fine-grained personality understanding. The problem has not been studied in the
NLP field, primarily due to the lack of appropriate datasets mimicking the
process of book reading. We present the first labeled dataset PersoNet for this
problem. Our novel annotation strategy involves annotating user notes from
online reading apps as a proxy for the original books. Experiments and human
studies indicate that our dataset construction is both efficient and accurate;
and our task heavily relies on long-term context to achieve accurate
predictions for both machines and humans. The dataset is available at
https://github.com/Gorov/personet_acl23.",None,-1
532ea431-a48b-4e2d-a370-5fea53aaf935,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,0.589758,"Environmental damage has been of much concern, particularly in coastal areas
and the oceans, given climate change and the drastic effects of pollution and
extreme climate events. Our present-day analytical capabilities, along with
advancements in information acquisition techniques such as remote sensing, can
be utilised for the management and study of coral reef ecosystems. In this
paper, we present Reef-Insight, an unsupervised machine learning framework that
features advanced clustering methods and remote sensing for reef habitat
mapping. Our framework compares different clustering methods for reef habitat
mapping using remote sensing data. We evaluate four major clustering approaches
based on qualitative and visual assessments which include k-means, hierarchical
clustering, Gaussian mixture model, and density-based clustering. We utilise
remote sensing data featuring the One Tree Island reef in Australia's Southern
Great Barrier Reef. Our results indicate that clustering methods using remote
sensing data can well identify benthic and geomorphic clusters in reefs when
compared with other studies. Our results indicate that Reef-Insight can
generate detailed reef habitat maps outlining distinct reef habitats and has
the potential to enable further insights for reef restoration projects.",None,-1
a5cd9dda-415b-464e-951f-440fb29e6a9a,Jigsaw: Learning to Assemble Multiple Fractured Objects,0.869065,"Automated assembly of 3D fractures is essential in orthopedics, archaeology,
and our daily life. This paper presents Jigsaw, a novel framework for
assembling physically broken 3D objects from multiple pieces. Our approach
leverages hierarchical features of global and local geometry to match and align
the fracture surfaces. Our framework consists of four components: (1) front-end
point feature extractor with attention layers, (2) surface segmentation to
separate fracture and original parts, (3) multi-parts matching to find
correspondences among fracture surface points, and (4) robust global alignment
to recover the global poses of the pieces. We show how to jointly learn
segmentation and matching and seamlessly integrate feature matching and
rigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and
achieve superior performance compared to state-of-the-art methods. Our method
also generalizes well to diverse fracture modes, objects, and unseen instances.
To the best of our knowledge, this is the first learning-based method designed
specifically for 3D fracture assembly over multiple pieces. Our code is
available at https://jiaxin-lu.github.io/Jigsaw/.",None,-1
029d38f6-f2b7-4cdc-a52e-fc04bdf08e83,Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation,0.804045,"Target-oriented dialogue systems, designed to proactively steer conversations
toward predefined targets or accomplish specific system-side goals, are an
exciting area in conversational AI. In this work, by formulating a <dialogue
act, topic> pair as the conversation target, we explore a novel problem of
personalized target-oriented dialogue by considering personalization during the
target accomplishment process. However, there remains an emergent need for
high-quality datasets, and building one from scratch requires tremendous human
effort. To address this, we propose an automatic dataset curation framework
using a role-playing approach. Based on this framework, we construct a
large-scale personalized target-oriented dialogue dataset, TopDial, which
comprises about 18K multi-turn dialogues. The experimental results show that
this dataset is of high quality and could contribute to exploring personalized
target-oriented dialogue.",None,-1
518cc5df-1126-46fa-9951-8acd4f93b127,MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments,0.146903,"Self-supervised learning can be used for mitigating the greedy needs of
Vision Transformer networks for very large fully-annotated datasets. Different
classes of self-supervised learning offer representations with either good
contextual reasoning properties, e.g., using masked image modeling strategies,
or invariance to image perturbations, e.g., with contrastive methods. In this
work, we propose a single-stage and standalone method, MOCA, which unifies both
desired properties using novel mask-and-predict objectives defined with
high-level features (instead of pixel-level details). Moreover, we show how to
effectively employ both learning paradigms in a synergistic and
computation-efficient way. Doing so, we achieve new state-of-the-art results on
low-shot settings and strong experimental results in various evaluation
protocols with a training that is at least 3 times faster than prior methods.",None,-1
31f3e3e1-81b4-4be3-a2e2-33e7862bc9af,Integrated Conflict Management for UAM with Strategic Demand Capacity Balancing and Learning-based Tactical Deconfliction,0.84909,"Urban air mobility (UAM) has the potential to revolutionize our daily
transportation, offering rapid and efficient deliveries of passengers and cargo
between dedicated locations within and around the urban environment. Before the
commercialization and adoption of this emerging transportation mode, however,
aviation safety must be guaranteed, i.e., all the aircraft have to be safely
separated by strategic and tactical deconfliction. Reinforcement learning has
demonstrated effectiveness in the tactical deconfliction of en route commercial
air traffic in simulation. However, its performance is found to be dependent on
the traffic density. In this project, we propose a novel framework that
combines demand capacity balancing (DCB) for strategic conflict management and
reinforcement learning for tactical separation. By using DCB to precondition
traffic to proper density levels, we show that reinforcement learning can
achieve much better performance for tactical safety separation. Our results
also indicate that this DCB preconditioning can allow target levels of safety
to be met that are otherwise impossible. In addition, combining strategic DCB
with reinforcement learning for tactical separation can meet these safety
levels while achieving greater operational efficiency than alternative
solutions.",None,-1
e9db9963-d474-4187-920b-8e18ae397bda,Multi-modal Multi-view Clustering based on Non-negative Matrix Factorization,0.243723,"By combining related objects, unsupervised machine learning techniques aim to
reveal the underlying patterns in a data set. Non-negative Matrix Factorization
(NMF) is a data mining technique that splits data matrices by imposing
restrictions on the elements' non-negativity into two matrices: one
representing the data partitions and the other to represent the cluster
prototypes of the data set. This method has attracted a lot of attention and is
used in a wide range of applications, including text mining, clustering,
language modeling, music transcription, and neuroscience (gene separation). The
interpretation of the generated matrices is made simpler by the absence of
negative values. In this article, we propose a study on multi-modal clustering
algorithms and present a novel method called multi-modal multi-view
non-negative matrix factorization, in which we analyze the collaboration of
several local NMF models. The experimental results show the value of the
proposed approach, which was evaluated using a variety of data sets, and the
obtained results are very promising compared to state of art methods.",None,-1
62810077-f469-4c86-b5b6-fd0a1cbe357e,"The logic behind desirable sets of things, and its filter representation",0.116113,"We identify the (filter representation of the) logic behind the recent theory
of coherent sets of desirable (sets of) things, which generalise coherent sets
of desirable (sets of) gambles as well as coherent choice functions, and show
that this identification allows us to establish various representation results
for such coherent models in terms of simpler ones.",None,-1
e7f1cacc-ae55-402e-9beb-639b8a939d59,PINNs-Based Uncertainty Quantification for Transient Stability Analysis,0.28507,"This paper addresses the challenge of transient stability in power systems
with missing parameters and uncertainty propagation in swing equations. We
introduce a novel application of Physics-Informed Neural Networks (PINNs),
specifically an Ensemble of PINNs (E-PINNs), to estimate critical parameters
like rotor angle and inertia coefficient with enhanced accuracy and reduced
computational load. E-PINNs capitalize on the underlying physical principles of
swing equations to provide a robust solution. Our approach not only facilitates
efficient parameter estimation but also quantifies uncertainties, delivering
probabilistic insights into the system behavior. The efficacy of E-PINNs is
demonstrated through the analysis of $1$-bus and $2$-bus systems, highlighting
the model's ability to handle parameter variability and data scarcity. The
study advances the application of machine learning in power system stability,
paving the way for reliable and computationally efficient transient stability
analysis.",None,-1
27c34d7b-a7b2-4013-860d-74b6890607fd,What does CLIP know about a red circle? Visual prompt engineering for VLMs,0.723542,"Large-scale Vision-Language Models, such as CLIP, learn powerful image-text
representations that have found numerous applications, from zero-shot
classification to text-to-image generation. Despite that, their capabilities
for solving novel discriminative tasks via prompting fall behind those of large
language models, such as GPT-3. Here we explore the idea of visual prompt
engineering for solving computer vision tasks beyond classification by editing
in image space instead of text. In particular, we discover an emergent ability
of CLIP, where, by simply drawing a red circle around an object, we can direct
the model's attention to that region, while also maintaining global
information. We show the power of this simple approach by achieving
state-of-the-art in zero-shot referring expressions comprehension and strong
performance in keypoint localization tasks. Finally, we draw attention to some
potential ethical concerns of large language-vision models.",None,-1
b412a3b9-1e4d-4a14-9f63-3d56c81bdb6b,Disentangled Representation for Diversified Recommendations,0.245867,"Accuracy and diversity have long been considered to be two conflicting goals
for recommendations. We point out, however, that as the diversity is typically
measured by certain pre-selected item attributes, e.g., category as the most
popularly employed one, improved diversity can be achieved without sacrificing
recommendation accuracy, as long as the diversification respects the user's
preference about the pre-selected attributes. This calls for a fine-grained
understanding of a user's preferences over items, where one needs to recognize
the user's choice is driven by the quality of the item itself, or the
pre-selected attributes of the item. In this work, we focus on diversity
defined on item categories. We propose a general diversification framework
agnostic to the choice of recommendation algorithms. Our solution disentangles
the learnt user representation in the recommendation module into
category-independent and category-dependent components to differentiate a
user's preference over items from two orthogonal perspectives. Experimental
results on three benchmark datasets and online A/B test demonstrate the
effectiveness of our solution in improving both recommendation accuracy and
diversity. In-depth analysis suggests that the improvement is due to our
improved modeling of users' categorical preferences and refined ranking within
item categories.",None,-1
5b6ccee2-b881-423f-80b1-b7e109e63c57,Learning To Teach Large Language Models Logical Reasoning,0.247565,"Large language models (LLMs) have gained enormous attention from both
academia and industry, due to their exceptional ability in language generation
and extremely powerful generalization. However, current LLMs still output
unreliable content in practical reasoning tasks due to their inherent issues
(e.g., hallucination). To better disentangle this problem, in this paper, we
conduct an in-depth investigation to systematically explore the capability of
LLMs in logical reasoning. More in detail, we first investigate the deficiency
of LLMs in logical reasoning on different tasks, including event relation
extraction and deductive reasoning. Our study demonstrates that LLMs are not
good reasoners in solving tasks with rigorous reasoning and will produce
counterfactual answers, which require us to iteratively refine. Therefore, we
comprehensively explore different strategies to endow LLMs with logical
reasoning ability, and thus enable them to generate more logically consistent
answers across different scenarios. Based on our approach, we also contribute a
synthesized dataset (LLM-LR) involving multi-hop reasoning for evaluation and
pre-training. Extensive quantitative and qualitative analyses on different
tasks also validate the effectiveness and necessity of teaching LLMs with logic
and provide insights for solving practical tasks with LLMs in future work.",None,-1
b7ace227-9bbd-4a69-ba35-293c409328a6,PIVOINE: Instruction Tuning for Open-world Information Extraction,0.660573,"We consider the problem of Open-world Information Extraction (Open-world IE),
which extracts comprehensive entity profiles from unstructured texts. Different
from the conventional closed-world setting of Information Extraction (IE),
Open-world IE considers a more general situation where entities and relations
could be beyond a predefined ontology. More importantly, we seek to develop a
large language model (LLM) that is able to perform Open-world IE to extract
desirable entity profiles characterized by (possibly fine-grained) natural
language instructions. We achieve this by finetuning LLMs using instruction
tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction
tuning dataset for Open-world IE enriched with a comprehensive corpus,
extensive annotations, and diverse instructions. We finetune the pretrained
BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE
with strong instruction-following capabilities. Our experiments demonstrate
that PIVOINE significantly outperforms traditional closed-world methods and
other LLM baselines, displaying impressive generalization capabilities on both
unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as
a promising solution to tackle the open-world challenge in IE effectively.",None,-1
80039060-cf16-425a-b610-564eb977ea9d,Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge,0.629679,"Pre-trained language models (LMs) are used for knowledge intensive tasks like
question answering, but their knowledge gets continuously outdated as the world
changes. Prior work has studied targeted updates to LMs, injecting individual
facts and evaluating whether the model learns these facts while not changing
predictions on other contexts. We take a step forward and study LMs' abilities
to make inferences based on injected facts (or propagate those facts): for
example, after learning that something is a TV show, does an LM predict that
you can watch it? We study this with two cloze-style tasks: an existing dataset
of real-world sentences about novel entities (ECBD) as well as a new controlled
benchmark with manually designed templates requiring varying levels of
inference about injected knowledge. Surprisingly, we find that existing methods
for updating knowledge (gradient-based fine-tuning and modifications of this
approach) show little propagation of injected knowledge. These methods improve
performance on cloze instances only when there is lexical overlap between
injected facts and target inferences. Yet, prepending entity definitions in an
LM's context improves performance across all settings, suggesting that there is
substantial headroom for parameter-updating approaches for knowledge injection.",None,-1
3a64e7b4-422f-4d74-ac2a-1a483caadb18,Tile Networks: Learning Optimal Geometric Layout for Whole-page Recommendation,0.644287,"Finding optimal configurations in a geometric space is a key challenge in
many technological disciplines. Current approaches either rely heavily on human
domain expertise and are difficult to scale. In this paper we show it is
possible to solve configuration optimization problems for whole-page
recommendation using reinforcement learning. The proposed \textit{Tile
Networks} is a neural architecture that optimizes 2D geometric configurations
by arranging items on proper positions. Empirical results on real dataset
demonstrate its superior performance compared to traditional learning to rank
approaches and recent deep models.",None,-1
2b478bd8-9693-4f70-97a2-3de501e2b552,IvyGPT: InteractiVe Chinese pathwaY language model in medical domain,0.213911,"General large language models (LLMs) such as ChatGPT have shown remarkable
success. However, such LLMs have not been widely adopted for medical purposes,
due to poor accuracy and inability to provide medical advice. We propose
IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality
medical question-answer (QA) instances and Reinforcement Learning from Human
Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn
conversation capabilities, but it cannot perform like a doctor in other
aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output
richer diagnosis and treatment answers that are closer to human. In the
training, we used QLoRA to train 33 billion parameters on a small number of
NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed
other medical GPT models.",None,-1
ae28d68a-1e44-479f-8860-76decf8620fe,Multi-View Azimuth Stereo via Tangent Space Consistency,0.640484,"We present a method for 3D reconstruction only using calibrated multi-view
surface azimuth maps. Our method, multi-view azimuth stereo, is effective for
textureless or specular surfaces, which are difficult for conventional
multi-view stereo methods. We introduce the concept of tangent space
consistency: Multi-view azimuth observations of a surface point should be
lifted to the same tangent space. Leveraging this consistency, we recover the
shape by optimizing a neural implicit surface representation. Our method
harnesses the robust azimuth estimation capabilities of photometric stereo
methods or polarization imaging while bypassing potentially complex zenith
angle estimation. Experiments using azimuth maps from various sources validate
the accurate shape recovery with our method, even without zenith angles.",None,-1
7f6c4708-7dc1-4116-b1ed-e1adeb286938,Class-Incremental Learning based on Label Generation,0.776989,"Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.",None,-1
53197608-4cd6-477b-95b8-d6694d36031e,Exploring How Generative Adversarial Networks Learn Phonological Representations,0.0426937,"This paper explores how Generative Adversarial Networks (GANs) learn
representations of phonological phenomena. We analyze how GANs encode
contrastive and non-contrastive nasality in French and English vowels by
applying the ciwGAN architecture (Begus 2021a). Begus claims that ciwGAN
encodes linguistically meaningful representations with categorical variables in
its latent space and manipulating the latent variables shows an almost one to
one corresponding control of the phonological features in ciwGAN's generated
outputs. However, our results show an interactive effect of latent variables on
the features in the generated outputs, which suggests the learned
representations in neural networks are different from the phonological
representations proposed by linguists. On the other hand, ciwGAN is able to
distinguish contrastive and noncontrastive features in English and French by
encoding them differently. Comparing the performance of GANs learning from
different languages results in a better understanding of what language specific
features contribute to developing language specific phonological
representations. We also discuss the role of training data frequencies in
phonological feature learning.",None,-1
1b4c2583-f57b-4377-812b-f7db44970527,"Multi3WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",0.912754,"Creating high-quality annotated data for task-oriented dialog (ToD) is known
to be notoriously difficult, and the challenges are amplified when the goal is
to create equitable, culturally adapted, and large-scale ToD datasets for
multiple languages. Therefore, the current datasets are still very scarce and
suffer from limitations such as translation-based non-native dialogs with
translation artefacts, small scale, or lack of cultural adaptation, among
others. In this work, we first take stock of the current landscape of
multilingual ToD datasets, offering a systematic overview of their properties
and limitations. Aiming to reduce all the detected limitations, we then
introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD
dataset. It is large-scale and offers culturally adapted dialogs in 4 languages
to enable training and evaluation of multilingual and cross-lingual ToD
systems. We describe a complex bottom-up data collection process that yielded
the final dataset, and offer the first sets of baseline scores across different
ToD-related tasks for future reference, also highlighting its challenging
nature.",None,-1
7f3c92f2-9cd3-4e36-afa5-323a2dcf25e9,Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions,0.040278,"While there is much recent interest in studying why Transformer-based large
language models make predictions the way they do, the complex computations
performed within each layer have made their behavior somewhat opaque. To
mitigate this opacity, this work presents a linear decomposition of final
hidden states from autoregressive language models based on each initial input
token, which is exact for virtually all contemporary Transformer architectures.
This decomposition allows the definition of probability distributions that
ablate the contribution of specific input tokens, which can be used to analyze
their influence on model probabilities over a sequence of upcoming words with
only one forward pass from the model. Using the change in next-word probability
as a measure of importance, this work first examines which context words make
the biggest contribution to language model predictions. Regression experiments
suggest that Transformer-based language models rely primarily on collocational
associations, followed by linguistic factors such as syntactic dependencies and
coreference relationships in making next-word predictions. Additionally,
analyses using these measures to predict syntactic dependencies and coreferent
mention spans show that collocational association and repetitions of the same
token largely explain the language models' predictions on these tasks.",None,-1
c27d1ec3-1bcd-480a-9cd3-93c701b40f86,TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training,0.321341,"Sparsely gated Mixture-of-Expert (MoE) has demonstrated its effectiveness in
scaling up deep neural networks to an extreme scale. Despite that numerous
efforts have been made to improve the performance of MoE from the model design
or system optimization perspective, existing MoE dispatch patterns are still
not able to fully exploit the underlying heterogeneous network environments. In
this paper, we propose TA-MoE, a topology-aware routing strategy for
large-scale MoE trainging, from a model-system co-design perspective, which can
dynamically adjust the MoE dispatch pattern according to the network topology.
Based on communication modeling, we abstract the dispatch problem into an
optimization objective and obtain the approximate dispatch pattern under
different topologies. On top of that, we design a topology-aware auxiliary
loss, which can adaptively route the data to fit in the underlying topology
without sacrificing the model accuracy. Experiments show that TA-MoE can
substantially outperform its counterparts on various hardware and model
configurations, with roughly 1.01x-1.61x, 1.01x-4.77x, 1.25x-1.54x improvements
over the popular DeepSpeed-MoE, FastMoE and FasterMoE.",None,-1
55ae226e-6607-4f42-a4e7-b5c824ea525d,RoboCLIP: One Demonstration is Enough to Learn Robot Policies,0.906173,"Reward specification is a notoriously difficult problem in reinforcement
learning, requiring extensive expert supervision to design robust reward
functions. Imitation learning (IL) methods attempt to circumvent these problems
by utilizing expert demonstrations but typically require a large number of
in-domain expert demonstrations. Inspired by advances in the field of
Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation
learning method that uses a single demonstration (overcoming the large data
requirement) in the form of a video demonstration or a textual description of
the task to generate rewards without manual reward function design.
Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like
videos of humans solving the task for reward generation, circumventing the need
to have the same demonstration and deployment domains. RoboCLIP utilizes
pretrained VLMs without any finetuning for reward generation. Reinforcement
learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher
zero-shot performance than competing imitation learning methods on downstream
robot manipulation tasks, doing so using only one video/text demonstration.",None,-1
9ec17ed4-2e29-45f7-a446-494d1db706b2,Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation,0.545382,"Despite significant research effort in the development of automatic dialogue
evaluation metrics, little thought is given to evaluating dialogues other than
in English. At the same time, ensuring metrics are invariant to semantically
similar responses is also an overlooked topic. In order to achieve the desired
properties of robustness and multilinguality for dialogue evaluation metrics,
we propose a novel framework that takes advantage of the strengths of current
evaluation models with the newly-established paradigm of prompting Large
Language Models (LLMs). Empirical results show our framework achieves state of
the art results in terms of mean Spearman correlation scores across several
benchmarks and ranks first place on both the Robust and Multilingual tasks of
the DSTC11 Track 4 ""Automatic Evaluation Metrics for Open-Domain Dialogue
Systems"", proving the evaluation capabilities of prompted LLMs.",None,-1
d0d0a108-29b7-4224-bac5-b90bb9a92866,Why Does ChatGPT Fall Short in Providing Truthful Answers?,0.598812,"Recent advancements in large language models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in providing reliable and accurate
answers to user questions. To better understand the model's particular
weaknesses in providing truthful answers, we embark an in-depth exploration of
open-domain question answering. Specifically, we undertake a detailed
examination of ChatGPT's failures, categorized into: comprehension, factuality,
specificity, and inference. We further pinpoint factuality as the most
contributing failure and identify two critical abilities associated with
factuality: knowledge memorization and knowledge recall. Through experiments
focusing on factuality, we propose several potential enhancement strategies.
Our findings suggest that augmenting the model with granular external knowledge
and cues for knowledge recall can enhance the model's factuality in answering
questions.",None,-1
9db5db8e-c1d6-4c8a-898c-0bbaecf1b721,Exploring the Law of Numbers: Evidence from China's Real Estate,0.119866,"The renowned proverb, Numbers do not lie, underscores the reliability and
insight that lie beneath numbers, a concept of undisputed importance,
especially in economics and finance etc. Despite the prosperity of Benford's
Law in the first digit analysis, its scope fails to remain comprehensiveness
when it comes to deciphering the laws of number. This paper delves into number
laws by taking the financial statements of China real estate as a
representative, quantitatively study not only the first digit, but also depict
the other two dimensions of numbers: frequency and length. The research
outcomes transcend mere reservations about data manipulation and open the door
to discussions surrounding number diversity and the delineation of the usage
insights. This study wields both economic significance and the capacity to
foster a deeper comprehension of numerical phenomena.",None,-1
960cff1a-ffc8-46e3-b03d-d9543226c13b,Adaptation Speed Analysis for Fairness-aware Causal Models,0.550849,"For example, in machine translation tasks, to achieve bidirectional
translation between two languages, the source corpus is often used as the
target corpus, which involves the training of two models with opposite
directions. The question of which one can adapt most quickly to a domain shift
is of significant importance in many fields. Specifically, consider an original
distribution p that changes due to an unknown intervention, resulting in a
modified distribution p*. In aligning p with p*, several factors can affect the
adaptation rate, including the causal dependencies between variables in p. In
real-life scenarios, however, we have to consider the fairness of the training
process, and it is particularly crucial to involve a sensitive variable (bias)
present between a cause and an effect variable. To explore this scenario, we
examine a simple structural causal model (SCM) with a cause-bias-effect
structure, where variable A acts as a sensitive variable between cause (X) and
effect (Y). The two models, respectively, exhibit consistent and contrary
cause-effect directions in the cause-bias-effect SCM. After conducting unknown
interventions on variables within the SCM, we can simulate some kinds of domain
shifts for analysis. We then compare the adaptation speeds of two models across
four shift scenarios. Additionally, we prove the connection between the
adaptation speeds of the two models across all interventions.",None,-1
93a36a13-8c16-42b9-b30c-16c3c42c24b0,FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation,0.833426,"Recent attention in instance segmentation has focused on query-based models.
Despite being non-maximum suppression (NMS)-free and end-to-end, the
superiority of these models on high-accuracy real-time benchmarks has not been
well demonstrated. In this paper, we show the strong potential of query-based
models on efficient instance segmentation algorithm designs. We present
FastInst, a simple, effective query-based framework for real-time instance
segmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) while
yielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bells
and whistles. Specifically, FastInst follows the meta-architecture of recently
introduced Mask2Former. Its key designs include instance activation-guided
queries, dual-path update strategy, and ground truth mask-guided learning,
which enable us to use lighter pixel decoders, fewer Transformer decoder
layers, while achieving better performance. The experiments show that FastInst
outperforms most state-of-the-art real-time counterparts, including strong
fully convolutional baselines, in both speed and accuracy. Code can be found at
https://github.com/junjiehe96/FastInst .",None,-1
85d876a0-e358-42de-ac4b-cf0aa3cdf73c,MeshDiffusion: Score-based Generative 3D Mesh Modeling,0.974273,"We consider the task of generating realistic 3D shapes, which is useful for a
variety of applications such as automatic scene generation and physical
simulation. Compared to other 3D representations like voxels and point clouds,
meshes are more desirable in practice, because (1) they enable easy and
arbitrary manipulation of shapes for relighting and simulation, and (2) they
can fully leverage the power of modern graphics pipelines which are mostly
optimized for meshes. Previous scalable methods for generating meshes typically
rely on sub-optimal post-processing, and they tend to produce overly-smooth or
noisy surfaces without fine-grained geometric details. To overcome these
shortcomings, we take advantage of the graph structure of meshes and use a
simple yet very effective generative modeling method to generate 3D meshes.
Specifically, we represent meshes with deformable tetrahedral grids, and then
train a diffusion model on this direct parametrization. We demonstrate the
effectiveness of our model on multiple generative tasks.",None,-1
e39a8d06-3426-48b1-9ce3-3083ea2413a4,Designing Participatory AI: Creative Professionals' Worries and Expectations about Generative AI,0.756395,"Generative AI, i.e., the group of technologies that automatically generate
visual or written content based on text prompts, has undergone a leap in
complexity and become widely available within just a few years. Such
technologies potentially introduce a massive disruption to creative fields.
This paper presents the results of a qualitative survey ($N$ = 23)
investigating how creative professionals think about generative AI. The results
show that the advancement of these AI models prompts important reflections on
what defines creativity and how creatives imagine using AI to support their
workflows. Based on these reflections, we discuss how we might design
\textit{participatory AI} in the domain of creative expertise with the goal of
empowering creative professionals in their present and future coexistence with
AI.",None,-1
80c17c1e-7cbf-41a9-bfb0-9de4f9e148d6,NeuralKG-ind: A Python Library for Inductive Knowledge Graph Representation Learning,0.224397,"Since the dynamic characteristics of knowledge graphs, many inductive
knowledge graph representation learning (KGRL) works have been proposed in
recent years, focusing on enabling prediction over new entities. NeuralKG-ind
is the first library of inductive KGRL as an important update of NeuralKG
library. It includes standardized processes, rich existing methods, decoupled
modules, and comprehensive evaluation metrics. With NeuralKG-ind, it is easy
for researchers and engineers to reproduce, redevelop, and compare inductive
KGRL methods. The library, experimental methodologies, and model
re-implementing results of NeuralKG-ind are all publicly released at
https://github.com/zjukg/NeuralKG/tree/ind .",None,-1
2f9cbd67-f75e-4d70-9084-838bd652e88b,Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways,0.018955,"We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrained
our masked language models with three ingredients: an initial pretraining with
music data, training on shorter sequences before training on longer ones, and
masking specific tokens to target some of the BLiMP subtasks. Overall, our
baseline models performed above chance, but far below the performance levels of
larger LLMs trained on more data. We found that training on short sequences
performed better than training on longer sequences.Pretraining on music may
help performance marginally, but, if so, the effect seems small. Our targeted
Masked Language Modeling augmentation did not seem to improve model performance
in general, but did seem to help on some of the specific BLiMP tasks that we
were targeting (e.g., Negative Polarity Items). Training performant LLMs on
small amounts of data is a difficult but potentially informative task. While
some of our techniques showed some promise, more work is needed to explore
whether they can improve performance more than the modest gains here. Our code
is available at https://github.com/venkatasg/Lil-Bevo and out models at
https://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a",None,-1
d7c1f0f8-569d-4b6a-b5e6-a754cdb50fe4,TabLib: A Dataset of 627M Tables with Context,0.607764,"It is well-established that large, diverse datasets play a pivotal role in
the performance of modern AI systems for text and image modalities. However,
there are no datasets for tabular data of comparable size and diversity to
those available for text and images. Thus we present ""TabLib'', a compilation
of 627 million tables totaling 69 TiB, along with 867B tokens of context.
TabLib was extracted from numerous file formats, including CSV, HTML, SQLite,
PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and
diversity of TabLib offer considerable promise in the table modality,
reminiscent of the original promise of foundational datasets for text and
images, such as The Pile and LAION.",None,-1
821203b7-786b-4843-a3ce-8ca139d55734,Knowledge Graph Embedding with 3D Compound Geometric Transformations,0.428263,"The cascade of 2D geometric transformations were exploited to model relations
between entities in a knowledge graph (KG), leading to an effective KG
embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was
proposed as a new KGE model, Rotate3D, by leveraging its non-commutative
property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric
transformations, including translation, rotation, scaling, reflection, and
shear and propose a family of KGE models, named CompoundE3D, in this work.
CompoundE3D allows multiple design variants to match rich underlying
characteristics of a KG. Since each variant has its own advantages on a subset
of relations, an ensemble of multiple variants can yield superior performance.
The effectiveness and flexibility of CompoundE3D are experimentally verified on
four popular link prediction datasets.",None,-1
2f8e45d3-ea21-4f78-af1f-abe371422082,Emotion-Cause Pair Extraction as Question Answering,0.716379,"The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all
potential emotion-cause pairs of a document without any annotation of emotion
or cause clauses. Previous approaches on ECPE have tried to improve
conventional two-step processing schemes by using complex architectures for
modeling emotion-cause interaction. In this paper, we cast the ECPE task to the
question answering (QA) problem and propose simple yet effective BERT-based
solutions to tackle it. Given a document, our Guided-QA model first predicts
the best emotion clause using a fixed question. Then the predicted emotion is
used as a question to predict the most potential cause for the emotion. We
evaluate our model on a standard ECPE corpus. The experimental results show
that despite its simplicity, our Guided-QA achieves promising results and is
easy to reproduce. The code of Guided-QA is also provided.",None,-1
903a38bb-d05e-4ffd-a753-73bf821a4b88,Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks,0.1176,"Coherence is an important aspect of text quality, and various approaches have
been applied to coherence modeling. However, existing methods solely focus on a
single document's coherence patterns, ignoring the underlying correlation
between documents. We investigate a GCN-based coherence model that is capable
of capturing structural similarities between documents. Our model first creates
a graph structure for each document, from where we mine different subgraph
patterns. We then construct a heterogeneous graph for the training corpus,
connecting documents based on their shared subgraphs. Finally, a GCN is applied
to the heterogeneous graph to model the connectivity relationships. We evaluate
our method on two tasks, assessing discourse coherence and automated essay
scoring. Results show that our GCN-based model outperforms all baselines,
achieving a new state-of-the-art on both tasks.",None,-1
bcb2589b-b7df-4b36-b1d4-ef81b7299c22,Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving,0.790069,"In recent years there have been remarkable advancements in autonomous
driving. While autonomous vehicles demonstrate high performance in closed-set
conditions, they encounter difficulties when confronted with unexpected
situations. At the same time, world models emerged in the field of model-based
reinforcement learning as a way to enable agents to predict the future
depending on potential actions. This led to outstanding results in sparse
reward and complex control tasks. This work provides an overview of how world
models can be leveraged to perform anomaly detection in the domain of
autonomous driving. We provide a characterization of world models and relate
individual components to previous works in anomaly detection to facilitate
further research in the field.",None,-1
72ddd4dd-1171-495d-b4d8-3040ee0319ea,Multi Modal Facial Expression Recognition with Transformer-Based Fusion Networks and Dynamic Sampling,0.770541,"Facial expression recognition is an essential task for various applications,
including emotion detection, mental health analysis, and human-machine
interactions. In this paper, we propose a multi-modal facial expression
recognition method that exploits audio information along with facial images to
provide a crucial clue to differentiate some ambiguous facial expressions.
Specifically, we introduce a Modal Fusion Module (MFM) to fuse audio-visual
information, where image and audio features are extracted from Swin
Transformer. Additionally, we tackle the imbalance problem in the dataset by
employing dynamic data resampling. Our model has been evaluated in the
Affective Behavior in-the-wild (ABAW) challenge of CVPR 2023.",None,-1
ecb1eae1-5610-474b-b80b-c607b23fb249,Disentangling Neuron Representations with Concept Vectors,0.417703,"Mechanistic interpretability aims to understand how models store
representations by breaking down neural networks into interpretable units.
However, the occurrence of polysemantic neurons, or neurons that respond to
multiple unrelated features, makes interpreting individual neurons challenging.
This has led to the search for meaningful vectors, known as concept vectors, in
activation space instead of individual neurons. The main contribution of this
paper is a method to disentangle polysemantic neurons into concept vectors
encapsulating distinct features. Our method can search for fine-grained
concepts according to the user's desired level of concept separation. The
analysis shows that polysemantic neurons can be disentangled into directions
consisting of linear combinations of neurons. Our evaluations show that the
concept vectors found encode coherent, human-understandable features.",None,-1
effb8d40-80f5-4ff7-9a86-d3e67aaadfd4,Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages,0.534582,"Connectionist Temporal Classification (CTC) models are popular for their
balance between speed and performance for Automatic Speech Recognition (ASR).
However, these CTC models still struggle in other areas, such as
personalization towards custom words. A recent approach explores Contextual
Adapters, wherein an attention-based biasing model for CTC is used to improve
the recognition of custom entities. While this approach works well with enough
data, we showcase that it isn't an effective strategy for low-resource
languages. In this work, we propose a supervision loss for smoother training of
the Contextual Adapters. Further, we explore a multilingual strategy to improve
performance with limited training data. Our method achieves 48% F1 improvement
in retrieving unseen custom entities for a low-resource language.
Interestingly, as a by-product of training the Contextual Adapters, we see a
5-11% Word Error Rate (WER) reduction in the performance of the base CTC model
as well.",None,-1
181f28d8-01af-4a83-a653-f6de16390e6a,Frequency Perception Network for Camouflaged Object Detection,0.439063,"Camouflaged object detection (COD) aims to accurately detect objects hidden
in the surrounding environment. However, the existing COD methods mainly locate
camouflaged objects in the RGB domain, their performance has not been fully
exploited in many challenging scenarios. Considering that the features of the
camouflaged object and the background are more discriminative in the frequency
domain, we propose a novel learnable and separable frequency perception
mechanism driven by the semantic hierarchy in the frequency domain. Our entire
network adopts a two-stage model, including a frequency-guided coarse
localization stage and a detail-preserving fine localization stage. With the
multi-level features extracted by the backbone, we design a flexible frequency
perception module based on octave convolution for coarse positioning. Then, we
design the correction fusion module to step-by-step integrate the high-level
features through the prior-guided correction and cross-layer feature channel
association, and finally combine them with the shallow features to achieve the
detailed correction of the camouflaged objects. Compared with the currently
existing models, our proposed method achieves competitive performance in three
popular benchmark datasets both qualitatively and quantitatively.",None,-1
216de838-a1fb-4226-9f5b-5dad72304b22,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,0.237352,"Self-supervised methods in vision have been mostly focused on large
architectures as they seem to suffer from a significant performance drop for
smaller architectures. In this paper, we propose a simple self-supervised
distillation technique that can train high performance low-compute neural
networks. Our main insight is that existing joint-embedding based SSL methods
can be repurposed for knowledge distillation from a large self-supervised
teacher to a small student model. Thus, we call our method Replace one Branch
(RoB) as it simply replaces one branch of the joint-embedding training with a
large teacher model. RoB is widely applicable to a number of architectures such
as small ResNets, MobileNets and ViT, and pretrained models such as DINO, SwAV
or iBOT. When pretraining on the ImageNet dataset, RoB yields models that
compete with supervised knowledge distillation. When applied to MSN, RoB
produces students with strong semi-supervised capabilities. Finally, our best
ViT-Tiny models improve over prior SSL state-of-the-art on ImageNet by $2.3\%$
and are on par or better than a supervised distilled DeiT on five downstream
transfer tasks (iNaturalist, CIFAR, Clevr/Count, Clevr/Dist and Places). We
hope RoB enables practical self-supervision at smaller scale.",None,-1
120a26bb-0813-43be-9eac-bc6e2b6d920e,Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2023): Workshop and Shared Task Report,0.482958,"We provide a summary of the sixth edition of the CASE workshop that is held
in the scope of RANLP 2023. The workshop consists of regular papers, three
keynotes, working papers of shared task participants, and shared task overview
papers. This workshop series has been bringing together all aspects of event
information collection across technical and social science fields. In addition
to contributing to the progress in text based event extraction, the workshop
provides a space for the organization of a multimodal event information
collection task.",None,-1
022bbb79-510c-45e2-84cb-8c929a4db60b,CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts,0.520377,"The powerful ability of ChatGPT has caused widespread concern in the academic
community. Malicious users could synthesize dummy academic content through
ChatGPT, which is extremely harmful to academic rigor and originality. The need
to develop ChatGPT-written content detection algorithms call for large-scale
datasets. In this paper, we initially investigate the possible negative impact
of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract
dataset (CHEAT) to support the development of detection algorithms. In
particular, the ChatGPT-written abstract dataset contains 35,304 synthetic
abstracts, with Generation, Polish, and Mix as prominent representatives. Based
on these data, we perform a thorough analysis of the existing text synthesis
detection algorithms. We show that ChatGPT-written abstracts are detectable,
while the detection difficulty increases with human involvement.Our dataset is
available in https://github.com/botianzhe/CHEAT.",None,-1
26c9c08b-0172-4383-a7d4-6456503252f9,From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models,0.181373,"Reasoning is a distinctive human capacity, enabling us to address complex
problems by breaking them down into a series of manageable cognitive steps.
Yet, complex logical reasoning is still cumbersome for language models. Based
on the dual process theory in cognitive science, we are the first to unravel
the cognitive reasoning abilities of language models. Our framework employs an
iterative methodology to construct a Cognitive Tree (CogTree). The root node of
this tree represents the initial query, while the leaf nodes consist of
straightforward questions that can be answered directly. This construction
involves two main components: the implicit extraction module (referred to as
the intuitive system) and the explicit reasoning module (referred to as the
reflective system). The intuitive system rapidly generates multiple responses
by utilizing in-context examples, while the reflective system scores these
responses using comparative learning. The scores guide the intuitive system in
its subsequent generation step. Our experimental results on two popular and
challenging reasoning tasks indicate that it is possible to achieve a
performance level comparable to that of GPT-3.5 (with 175B parameters), using a
significantly smaller language model that contains fewer parameters (<=7B) than
5% of GPT-3.5.",None,-1
5ee6e792-b5c1-4bc4-b66e-653dc66c4728,A Template Is All You Meme,0.654088,"Memes are a modern form of communication and meme templates possess a base
semantics that is customizable by whomever posts it on social media. Machine
learning systems struggle with memes, which is likely due to such systems
having insufficient context to understand memes, as there is more to memes than
the obvious image and text. Here, to aid understanding of memes, we release a
knowledge base of memes and information found on www.knowyourmeme.com, which we
call the Know Your Meme Knowledge Base (KYMKB), composed of more than 54,000
images. The KYMKB includes popular meme templates, examples of each template,
and detailed information about the template. We hypothesize that meme templates
can be used to inject models with the context missing from previous approaches.
To test our hypothesis, we create a non-parametric majority-based classifier,
which we call Template-Label Counter (TLC). We find TLC more effective than or
competitive with fine-tuned baselines. To demonstrate the power of meme
templates and the value of both our knowledge base and method, we conduct
thorough classification experiments and exploratory data analysis in the
context of five meme analysis tasks.",None,-1
b8e5cbeb-0369-4028-b98f-31a6dfd80484,An Image Processing Pipeline for Autonomous Deep-Space Optical Navigation,0.440582,"A new era of space exploration and exploitation is fast approaching. A
multitude of spacecraft will flow in the future decades under the propulsive
momentum of the new space economy. Yet, the flourishing proliferation of
deep-space assets will make it unsustainable to pilot them from ground with
standard radiometric tracking. The adoption of autonomous navigation
alternatives is crucial to overcoming these limitations. Among these, optical
navigation is an affordable and fully ground-independent approach. Probes can
triangulate their position by observing visible beacons, e.g., planets or
asteroids, by acquiring their line-of-sight in deep space. To do so, developing
efficient and robust image processing algorithms providing information to
navigation filters is a necessary action. This paper proposes an innovative
pipeline for unresolved beacon recognition and line-of-sight extraction from
images for autonomous interplanetary navigation. The developed algorithm
exploits the k-vector method for the non-stellar object identification and
statistical likelihood to detect whether any beacon projection is visible in
the image. Statistical results show that the accuracy in detecting the planet
position projection is independent of the spacecraft position uncertainty.
Whereas, the planet detection success rate is higher than 95% when the
spacecraft position is known with a 3sigma accuracy up to 10^5 km.",None,-1
997e5087-5394-4c73-9cf3-8ce1163fa8c3,Learning Concise and Descriptive Attributes for Visual Recognition,0.932853,"Recent advances in foundation models present new opportunities for
interpretable visual recognition -- one can first query Large Language Models
(LLMs) to obtain a set of attributes that describe each class, then apply
vision-language models to classify images via these attributes. Pioneering work
shows that querying thousands of attributes can achieve performance competitive
with image features. However, our further investigation on 8 datasets reveals
that LLM-generated attributes in a large quantity perform almost the same as
random words. This surprising finding suggests that significant noise may be
present in these attributes. We hypothesize that there exist subsets of
attributes that can maintain the classification performance with much smaller
sizes, and propose a novel learning-to-search method to discover those concise
sets of attributes. As a result, on the CUB dataset, our method achieves
performance close to that of massive LLM-generated attributes (e.g., 10k
attributes for CUB), yet using only 32 attributes in total to distinguish 200
bird species. Furthermore, our new paradigm demonstrates several additional
benefits: higher interpretability and interactivity for humans, and the ability
to summarize knowledge for a recognition task.",None,-1
2ac3a803-6253-41ae-9631-3c1405e9102b,Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting,0.926004,"Knowledge Graph Completion (KGC) often requires both KG structural and
textual information to be effective. Pre-trained Language Models (PLMs) have
been used to learn the textual information, usually under the fine-tune
paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly
focus on the textual information and overlook structural knowledge. To tackle
this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC)
which maintains a balance between structural information and textual knowledge.
CSProm-KG only tunes the parameters of Conditional Soft Prompts that are
generated by the entities and relations representations. We verify the
effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR,
FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and
ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new
state-of-the-art on these benchmarks. We conduct further analysis to show (i)
the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG,
and (iii) the flexibility of CSProm-KG.",None,-1
2cfebcd9-9206-4407-9def-2aa6f999c769,Mitigating Data Scarcity for Large Language Models,0.0470046,"In recent years, pretrained neural language models (PNLMs) have taken the
field of natural language processing by storm, achieving new benchmarks and
state-of-the-art performances. These models often rely heavily on annotated
data, which may not always be available. Data scarcity are commonly found in
specialized domains, such as medical, or in low-resource languages that are
underexplored by AI research. In this dissertation, we focus on mitigating data
scarcity using data augmentation and neural ensemble learning techniques for
neural language models. In both research directions, we implement neural
network algorithms and evaluate their impact on assisting neural language
models in downstream NLP tasks. Specifically, for data augmentation, we explore
two techniques: 1) creating positive training data by moving an answer span
around its original context and 2) using text simplification techniques to
introduce a variety of writing styles to the original training data. Our
results indicate that these simple and effective solutions improve the
performance of neural language models considerably in low-resource NLP domains
and tasks. For neural ensemble learning, we use a multilabel neural classifier
to select the best prediction outcome from a variety of individual pretrained
neural language models trained for a low-resource medical text simplification
task.",None,-1
522a71d7-bcbd-4569-a561-8c483a8188fa,Enhancing Few-shot NER with Prompt Ordering based Data Augmentation,0.159037,"Recently, data augmentation (DA) methods have been proven to be effective for
pre-trained language models (PLMs) in low-resource settings, including few-shot
named entity recognition (NER). However, conventional NER DA methods are mostly
aimed at sequence labeling models, i.e., token-level classification, and few
are compatible with unified autoregressive generation frameworks, which can
handle a wider range of NER tasks, such as nested NER. Furthermore, these
generation frameworks have a strong assumption that the entities will appear in
the target sequence with the same left-to-right order as the source sequence.
In this paper, we claim that there is no need to keep this strict order, and
more diversified but reasonable target entity sequences can be provided during
the training stage as a novel DA method. Nevertheless, a naive mixture of
augmented data can confuse the model since one source sequence will then be
paired with different target sequences. Therefore, we propose a simple but
effective Prompt Ordering based Data Augmentation (PODA) method to improve the
training of unified autoregressive generation frameworks under few-shot NER
scenarios. Experimental results on three public NER datasets and further
analyses demonstrate the effectiveness of our approach.",None,-1
32b51878-f147-493c-bd35-ad156bf0c491,Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation,0.614387,"Human evaluation is critical for validating the performance of text-to-image
generative models, as this highly cognitive process requires deep comprehension
of text and images. However, our survey of 37 recent papers reveals that many
works rely solely on automatic measures (e.g., FID) or perform poorly described
human evaluations that are not reliable or repeatable. This paper proposes a
standardized and well-defined human evaluation protocol to facilitate
verifiable and reproducible human evaluation in future works. In our pilot data
collection, we experimentally show that the current automatic measures are
incompatible with human perception in evaluating the performance of the
text-to-image generation results. Furthermore, we provide insights for
designing human evaluation experiments reliably and conclusively. Finally, we
make several resources publicly available to the community to facilitate easy
and fast implementations.",None,-1
07b9579c-8d9b-4f1f-b5fc-cc99157a999e,Exploration via Epistemic Value Estimation,0.0292539,"How to efficiently explore in reinforcement learning is an open problem. Many
exploration algorithms employ the epistemic uncertainty of their own value
predictions -- for instance to compute an exploration bonus or upper confidence
bound. Unfortunately the required uncertainty is difficult to estimate in
general with function approximation.
  We propose epistemic value estimation (EVE): a recipe that is compatible with
sequential decision making and with neural network function approximators. It
equips agents with a tractable posterior over all their parameters from which
epistemic value uncertainty can be computed efficiently.
  We use the recipe to derive an epistemic Q-Learning agent and observe
competitive performance on a series of benchmarks. Experiments confirm that the
EVE recipe facilitates efficient exploration in hard exploration tasks.",None,-1
bff791be-1ec1-4140-8b86-ee29fa83fdc1,FIMO: A Challenge Formal Dataset for Automated Theorem Proving,0.999407,"We present FIMO, an innovative dataset comprising formal mathematical problem
statements sourced from the International Mathematical Olympiad (IMO)
Shortlisted Problems. Designed to facilitate advanced automated theorem proving
at the IMO level, FIMO is currently tailored for the Lean formal language. It
comprises 149 formal problem statements, accompanied by both informal problem
descriptions and their corresponding LaTeX-based informal proofs. Through
initial experiments involving GPT-4, our findings underscore the existing
limitations in current methodologies, indicating a substantial journey ahead
before achieving satisfactory IMO-level automated theorem proving outcomes.",None,-1
6c68581e-c214-49a3-87be-be5558923317,Clustering-Aware Negative Sampling for Unsupervised Sentence Representation,0.551766,"Contrastive learning has been widely studied in sentence representation
learning. However, earlier works mainly focus on the construction of positive
examples, while in-batch samples are often simply treated as negative examples.
This approach overlooks the importance of selecting appropriate negative
examples, potentially leading to a scarcity of hard negatives and the inclusion
of false negatives. To address these issues, we propose ClusterNS
(Clustering-aware Negative Sampling), a novel method that incorporates cluster
information into contrastive learning for unsupervised sentence representation
learning. We apply a modified K-means clustering algorithm to supply hard
negatives and recognize in-batch false negatives during training, aiming to
solve the two issues in one unified framework. Experiments on semantic textual
similarity (STS) tasks demonstrate that our proposed ClusterNS compares
favorably with baselines in unsupervised sentence representation learning. Our
code has been made publicly available.",None,-1
408fd06f-c67c-4b64-ac35-001bcf96285a,Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation,0.566415,"Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel ""Fighting Fire with Fire"" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.",None,-1
72376ee1-eb44-4d1f-a258-b4637df5c7a3,Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations,0.933434,"Unlike empathetic dialogues, the system in emotional support conversations
(ESC) is expected to not only convey empathy for comforting the help-seeker,
but also proactively assist in exploring and addressing their problems during
the conversation. In this work, we study the problem of mixed-initiative ESC
where the user and system can both take the initiative in leading the
conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC
systems with a tailor-designed schema that divides utterances into different
types with speaker roles and initiative types. Four emotional support metrics
are proposed to evaluate the mixed-initiative interactions. The analysis
reveals the necessity and challenges of building mixed-initiative ESC systems.
In the light of this, we propose a knowledge-enhanced mixed-initiative
framework (KEMI) for ESC, which retrieves actual case knowledge from a
large-scale mental health knowledge graph for generating mixed-initiative
responses. Experimental results on two ESC datasets show the superiority of
KEMI in both content-preserving evaluation and mixed initiative related
analyses.",None,-1
150008c5-ae3d-4fb0-aa75-f29b453b07b7,Attention! Dynamic Epistemic Logic Models of (In)attentive Agents,0.522978,"Attention is the crucial cognitive ability that limits and selects what
information we observe. Previous work by Bolander et al. (2016) proposes a
model of attention based on dynamic epistemic logic (DEL) where agents are
either fully attentive or not attentive at all. While introducing the realistic
feature that inattentive agents believe nothing happens, the model does not
represent the most essential aspect of attention: its selectivity. Here, we
propose a generalization that allows for paying attention to subsets of atomic
formulas. We introduce the corresponding logic for propositional attention, and
show its axiomatization to be sound and complete. We then extend the framework
to account for inattentive agents that, instead of assuming nothing happens,
may default to a specific truth-value of what they failed to attend to (a sort
of prior concerning the unattended atoms). This feature allows for a more
cognitively plausible representation of the inattentional blindness phenomenon,
where agents end up with false beliefs due to their failure to attend to
conspicuous but unexpected events. Both versions of the model define
attention-based learning through appropriate DEL event models based on a few
and clear edge principles. While the size of such event models grow
exponentially both with the number of agents and the number of atoms, we
introduce a new logical language for describing event models syntactically and
show that using this language our event models can be represented linearly in
the number of agents and atoms. Furthermore, representing our event models
using this language is achieved by a straightforward formalisation of the
aforementioned edge principles.",None,-1
f608a136-9e3c-4df5-9d7f-08e72497a520,Analyzing Multiple-Choice Reading and Listening Comprehension Tests,0.234017,"Multiple-choice reading and listening comprehension tests are an important
part of language assessment. Content creators for standard educational tests
need to carefully curate questions that assess the comprehension abilities of
candidates taking the tests. However, recent work has shown that a large number
of questions in general multiple-choice reading comprehension datasets can be
answered without comprehension, by leveraging world knowledge instead. This
work investigates how much of a contextual passage needs to be read in
multiple-choice reading based on conversation transcriptions and listening
comprehension tests to be able to work out the correct answer. We find that
automated reading comprehension systems can perform significantly better than
random with partial or even no access to the context passage. These findings
offer an approach for content creators to automatically capture the trade-off
between comprehension and world knowledge required for their proposed
questions.",None,-1
bd975269-08a0-4645-a02f-8893bf19e87f,Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model,0.0797644,"Pretrained language models have achieved remarkable success in various
natural language processing tasks. However, pretraining has recently shifted
toward larger models and larger data, and this has resulted in significant
computational and energy costs. In this paper, we propose Influence Subset
Selection (ISS) for language model, which explicitly utilizes end-task
knowledge to select a tiny subset of the pretraining corpus. Specifically, the
ISS selects the samples that will provide the most positive influence on the
performance of the end-task. Furthermore, we design a gradient matching based
influence estimation method, which can drastically reduce the computation time
of influence. With only 0.45% of the data and a three-orders-of-magnitude lower
computational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight
datasets covering four domains.",None,-1
8828e581-0764-4cb8-8634-b065bb43d7a9,Dynamic Video Frame Interpolation with integrated Difficulty Pre-Assessment,0.0948829,"Video frame interpolation(VFI) has witnessed great progress in recent years.
While existing VFI models still struggle to achieve a good trade-off between
accuracy and efficiency: fast models often have inferior accuracy; accurate
models typically run slowly. However, easy samples with small motion or clear
texture can achieve competitive results with simple models and do not require
heavy computation. In this paper, we present an integrated pipeline which
combines difficulty assessment with video frame interpolation. Specifically, it
firstly leverages a pre-assessment model to measure the interpolation
difficulty level of input frames, and then dynamically selects an appropriate
VFI model to generate interpolation results. Furthermore, a large-scale VFI
difficulty assessment dataset is collected and annotated to train our
pre-assessment model. Extensive experiments show that easy samples pass through
fast models while difficult samples inference with heavy models, and our
proposed pipeline can improve the accuracy-efficiency trade-off for VFI.",None,-1
f48d87d9-adad-4def-ac54-add49ff90c75,INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification,0.586227,"This paper describes our approach to the multi-modal fact verification
(FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of
social media, fake news can spread rapidly and negatively impact social
security. Automatic claim verification becomes more and more crucial to combat
fake news. In fact verification involving multiple modal data, there should be
a structural coherence between claim and document. Therefore, we proposed a
structure coherence-based multi-modal fact verification scheme to classify fake
news. Our structure coherence includes the following four aspects: sentence
length, vocabulary similarity, semantic similarity, and image similarity.
Specifically, CLIP and Sentence BERT are combined to extract text features, and
ResNet50 is used to extract image features. In addition, we also extract the
length of the text as well as the lexical similarity. Then the features were
concatenated and passed through the random forest classifier. Finally, our
weighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2.",None,-1
7ea74fc3-a00c-464a-b40f-dce37afbf420,Motion Question Answering via Modular Motion Programs,0.189515,"In order to build artificial intelligence systems that can perceive and
reason with human behavior in the real world, we must first design models that
conduct complex spatio-temporal reasoning over motion sequences. Moving towards
this goal, we propose the HumanMotionQA task to evaluate complex, multi-step
reasoning abilities of models on long-form human motion sequences. We generate
a dataset of question-answer pairs that require detecting motor cues in small
portions of motion sequences, reasoning temporally about when events occur, and
querying specific motion attributes. In addition, we propose NSPose, a
neuro-symbolic method for this task that uses symbolic reasoning and a modular
design to ground motion through learning motion concepts, attribute neural
operators, and temporal relations. We demonstrate the suitability of NSPose for
the HumanMotionQA task, outperforming all baseline methods.",None,-1
ff55ae36-5cca-4e0c-b250-cd4319cce72f,Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction,0.7577,"Safety-critical applications such as autonomous vehicles and social robots
require fast computation and accurate probability density estimation on
trajectory prediction. To address both requirements, this paper presents a new
normalizing flow-based trajectory prediction model named FlowChain. FlowChain
is a stack of conditional continuously-indexed flows (CIFs) that are expressive
and allow analytical probability density computation. This analytical
computation is faster than the generative models that need additional
approximations such as kernel density estimation. Moreover, FlowChain is more
accurate than the Gaussian mixture-based models due to fewer assumptions on the
estimated density. FlowChain also allows a rapid update of estimated
probability densities. This update is achieved by adopting the \textit{newest
observed position} and reusing the flow transformations and its
log-det-jacobians that represent the \textit{motion trend}. This update is
completed in less than one millisecond because this reuse greatly omits the
computational cost. Experimental results showed our FlowChain achieved
state-of-the-art trajectory prediction accuracy compared to previous methods.
Furthermore, our FlowChain demonstrated superiority in the accuracy and speed
of density estimation. Our code is available at
\url{https://github.com/meaten/FlowChain-ICCV2023}",None,-1
7a871583-561a-4d8a-a5c6-0f76b9c5cb5a,InfoStyler: Disentanglement Information Bottleneck for Artistic Style Transfer,0.456517,"Artistic style transfer aims to transfer the style of an artwork to a
photograph while maintaining its original overall content. Many prior works
focus on designing various transfer modules to transfer the style statistics to
the content image. Although effective, ignoring the clear disentanglement of
the content features and the style features from the first beginning, they have
difficulty in balancing between content preservation and style transferring. To
tackle this problem, we propose a novel information disentanglement method,
named InfoStyler, to capture the minimal sufficient information for both
content and style representations from the pre-trained encoding network.
InfoStyler formulates the disentanglement representation learning as an
information compression problem by eliminating style statistics from the
content image and removing the content structure from the style image. Besides,
to further facilitate disentanglement learning, a cross-domain Information
Bottleneck (IB) learning strategy is proposed by reconstructing the content and
style domains. Extensive experiments demonstrate that our InfoStyler can
synthesize high-quality stylized images while balancing content structure
preservation and style pattern richness.",None,-1
88de5df8-3d8b-4b39-93d8-8b4e86d92419,"Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals",0.428229,"Counterfactual explanations are an increasingly popular form of post hoc
explanation due to their (i) applicability across problem domains, (ii)
proposed legal compliance (e.g., with GDPR), and (iii) reliance on the
contrastive nature of human explanation. Although counterfactual explanations
are normally used to explain individual predictive-instances, we explore a
novel use case in which groups of similar instances are explained in a
collective fashion using ``group counterfactuals'' (e.g., to highlight a
repeating pattern of illness in a group of patients). These group
counterfactuals meet a human preference for coherent, broad explanations
covering multiple events/instances. A novel, group-counterfactual algorithm is
proposed to generate high-coverage explanations that are faithful to the
to-be-explained model. This explanation strategy is also evaluated in a large,
controlled user study (N=207), using objective (i.e., accuracy) and subjective
(i.e., confidence, explanation satisfaction, and trust) psychological measures.
The results show that group counterfactuals elicit modest but definite
improvements in people's understanding of an AI system. The implications of
these findings for counterfactual methods and for XAI are discussed.",None,-1
ee4443f1-ba37-4817-a5af-8e0e46977c1e,DeltaEdit: Exploring Text-free Training for Text-Driven Image Manipulation,0.525493,"Text-driven image manipulation remains challenging in training or inference
flexibility. Conditional generative models depend heavily on expensive
annotated training data. Meanwhile, recent frameworks, which leverage
pre-trained vision-language models, are limited by either per text-prompt
optimization or inference-time hyper-parameters tuning. In this work, we
propose a novel framework named \textit{DeltaEdit} to address these problems.
Our key idea is to investigate and identify a space, namely delta image and
text space that has well-aligned distribution between CLIP visual feature
differences of two images and CLIP textual embedding differences of source and
target texts. Based on the CLIP delta space, the DeltaEdit network is designed
to map the CLIP visual features differences to the editing directions of
StyleGAN at training phase. Then, in inference phase, DeltaEdit predicts the
StyleGAN's editing directions from the differences of the CLIP textual
features. In this way, DeltaEdit is trained in a text-free manner. Once
trained, it can well generalize to various text prompts for zero-shot inference
without bells and whistles. Code is available at
https://github.com/Yueming6568/DeltaEdit.",None,-1
06fe3575-630b-4572-993d-22a6d39622a9,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,0.361165,"While large language models have made strides in natural language processing,
their proficiency in complex reasoning tasks requiring formal language
comprehension, such as chess, remains less investigated. This paper probes the
performance of ChatGPT, a sophisticated language model by OpenAI in tackling
such complex reasoning tasks, using chess as a case study. Through robust
metrics examining both the legality and quality of moves, we assess ChatGPT's
understanding of the chessboard, adherence to chess rules, and strategic
decision-making abilities. Our evaluation identifies limitations within
ChatGPT's attention mechanism that affect its formal language comprehension and
uncovers the model's underdeveloped self-regulation abilities. Our study also
reveals ChatGPT's propensity for a coherent strategy in its gameplay and a
noticeable uptick in decision-making assertiveness when the model is presented
with a greater volume of natural language or possesses a more lucid
understanding of the state of the chessboard. These findings contribute to the
growing exploration of language models' abilities beyond natural language
processing, providing valuable information for future research towards models
demonstrating human-like cognitive abilities.",None,-1
4b1058a2-286c-4da0-b62b-8c3e2ef70230,Characterising Decision Theories with Mechanised Causal Graphs,0.148999,"How should my own decisions affect my beliefs about the outcomes I expect to
achieve? If taking a certain action makes me view myself as a certain type of
person, it might affect how I think others view me, and how I view others who
are similar to me. This can influence my expected utility calculations and
change which action I perceive to be best. Whether and how it should is subject
to debate, with contenders for how to think about it including evidential
decision theory, causal decision theory, and functional decision theory. In
this paper, we show that mechanised causal models can be used to characterise
and differentiate the most important decision theories, and generate a taxonomy
of different decision theories.",None,-1
4f9c356b-5cf6-4dc2-82c3-816f5270daff,Market Concentration Implications of Foundation Models,0.191685,"We analyze the structure of the market for foundation models, i.e., large AI
models such as those that power ChatGPT and that are adaptable to downstream
uses, and we examine the implications for competition policy and regulation. We
observe that the most capable models will have a tendency towards natural
monopoly and may have potentially vast markets. This calls for a two-pronged
regulatory response: (i) Antitrust authorities need to ensure the
contestability of the market by tackling strategic behavior, in particular by
ensuring that monopolies do not propagate vertically to downstream uses, and
(ii) given the diminished potential for market discipline, there is a role for
regulators to ensure that the most capable models meet sufficient quality
standards (including safety, privacy, non-discrimination, reliability and
interoperability standards) to maximally contribute to social welfare.
Regulators should also ensure a level regulatory playing field between AI and
non-AI applications in all sectors of the economy. For models that are behind
the frontier, we expect competition to be quite intense, implying a more
limited role for competition policy, although a role for regulation remains.",None,-1
3cafdfdc-8d76-492d-8507-dccb1ed59437,Better Sampling of Negatives for Distantly Supervised Named Entity Recognition,0.21054,"Distantly supervised named entity recognition (DS-NER) has been proposed to
exploit the automatically labeled training data instead of human annotations.
The distantly annotated datasets are often noisy and contain a considerable
number of false negatives. The recent approach uses a weighted sampling
approach to select a subset of negative samples for training. However, it
requires a good classifier to assign weights to the negative samples. In this
paper, we propose a simple and straightforward approach for selecting the top
negative samples that have high similarities with all the positive samples for
training. Our method achieves consistent performance improvements on four
distantly supervised NER datasets. Our analysis also shows that it is critical
to differentiate the true negatives from the false negatives.",None,-1
e43ea477-02ed-4329-b4b3-3bb9cd64d994,Colo-SCRL: Self-Supervised Contrastive Representation Learning for Colonoscopic Video Retrieval,0.608015,"Colonoscopic video retrieval, which is a critical part of polyp treatment,
has great clinical significance for the prevention and treatment of colorectal
cancer. However, retrieval models trained on action recognition datasets
usually produce unsatisfactory retrieval results on colonoscopic datasets due
to the large domain gap between them. To seek a solution to this problem, we
construct a large-scale colonoscopic dataset named Colo-Pair for medical
practice. Based on this dataset, a simple yet effective training method called
Colo-SCRL is proposed for more robust representation learning. It aims to
refine general knowledge from colonoscopies through masked autoencoder-based
reconstruction and momentum contrast to improve retrieval performance. To the
best of our knowledge, this is the first attempt to employ the contrastive
learning paradigm for medical video retrieval. Empirical results show that our
method significantly outperforms current state-of-the-art methods in the
colonoscopic video retrieval task.",None,-1
19cd5548-d873-4972-a711-c0274bf4ea4f,"SAM Struggles in Concealed Scenes -- Empirical Study on ""Segment Anything""",0.883141,"Segmenting anything is a ground-breaking step toward artificial general
intelligence, and the Segment Anything Model (SAM) greatly fosters the
foundation models for computer vision. We could not be more excited to probe
the performance traits of SAM. In particular, exploring situations in which SAM
does not perform well is interesting. In this report, we choose three concealed
scenes, i.e., camouflaged animals, industrial defects, and medical lesions, to
evaluate SAM under unprompted settings. Our main observation is that SAM looks
unskilled in concealed scenes.",None,-1
61c179ec-24a4-49a2-bcd4-4249a4f02f82,Investigations on convergence behaviour of Physics Informed Neural Networks across spectral ranges and derivative orders,0.256368,"An important inference from Neural Tangent Kernel (NTK) theory is the
existence of spectral bias (SB), that is, low frequency components of the
target function of a fully connected Artificial Neural Network (ANN) being
learnt significantly faster than the higher frequencies during training. This
is established for Mean Square Error (MSE) loss functions with very low
learning rate parameters. Physics Informed Neural Networks (PINNs) are designed
to learn the solutions of differential equations (DE) of arbitrary orders; in
PINNs the loss functions are obtained as the residues of the conservative form
of the DEs and represent the degree of dissatisfaction of the equations. So
there has been an open question whether (a) PINNs also exhibit SB and (b) if
so, how does this bias vary across the orders of the DEs. In this work, a
series of numerical experiments are conducted on simple sinusoidal functions of
varying frequencies, compositions and equation orders to investigate these
issues. It is firmly established that under normalized conditions, PINNs do
exhibit strong spectral bias, and this increases with the order of the
differential equation.",None,-1
3d655189-51ef-45ca-b036-3b6588b9113b,Multi-Agent Consensus Seeking via Large Language Models,0.831454,"Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: westlakeintelligentrobotics.github.io/ConsensusLLM/.",None,-1
67a9780e-faf4-4297-9ee3-cf8048161d89,How is Fatherhood Framed Online in Singapore?,0.164394,"The proliferation of discussion about fatherhood in Singapore attests to its
significance, indicating the need for an exploration of how fatherhood is
framed, aiding policy-making around fatherhood in Singapore. Sound and holistic
policy around fatherhood in Singapore may reduce stigma and apprehension around
being a parent, critical to improving the nations flagging birth rate. We
analyzed 15,705 articles and 56,221 posts to study how fatherhood is framed in
Singapore across a range of online platforms (news outlets, parenting forums,
Twitter). We used NLP techniques to understand these differences. While
fatherhood was framed in a range of ways on the Singaporean online environment,
it did not seem that fathers were framed as central to the Singaporean family
unit. A strength of our work is how the different techniques we have applied
validate each other.",None,-1
413f868c-d1e0-4032-bfba-bd26aedce3a9,Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection,0.375434,"Recent progress in weakly supervised object detection is featured by a
combination of multiple instance detection networks (MIDN) and ordinal online
refinement. However, with only image-level annotation, MIDN inevitably assigns
high scores to some unexpected region proposals when generating pseudo labels.
These inaccurate high-scoring region proposals will mislead the training of
subsequent refinement modules and thus hamper the detection performance. In
this work, we explore how to ameliorate the quality of pseudo-labeling in MIDN.
Formally, we devise Cyclic-Bootstrap Labeling (CBL), a novel weakly supervised
object detection pipeline, which optimizes MIDN with rank information from a
reliable teacher network. Specifically, we obtain this teacher network by
introducing a weighted exponential moving average strategy to take advantage of
various refinement modules. A novel class-specific ranking distillation
algorithm is proposed to leverage the output of weighted ensembled teacher
network for distilling MIDN with rank information. As a result, MIDN is guided
to assign higher scores to accurate proposals among their neighboring ones,
thus benefiting the subsequent pseudo labeling. Extensive experiments on the
prevalent PASCAL VOC 2007 \& 2012 and COCO datasets demonstrate the superior
performance of our CBL framework. Code will be available at
https://github.com/Yinyf0804/WSOD-CBL/.",None,-1
c42c45fd-9d26-494b-8a76-3b94906b21c8,Controlled Randomness Improves the Performance of Transformer Models,0.0354975,"During the pre-training step of natural language models, the main objective
is to learn a general representation of the pre-training dataset, usually
requiring large amounts of textual data to capture the complexity and diversity
of natural language. Contrasting this, in most cases, the size of the data
available to solve the specific downstream task is often dwarfed by the
aforementioned pre-training dataset, especially in domains where data is
scarce. We introduce controlled randomness, i.e. noise, into the training
process to improve fine-tuning language models and explore the performance of
targeted noise in addition to the parameters of these models. We find that
adding such noise can improve the performance in our two downstream tasks of
joint named entity recognition and relation extraction and text summarization.",None,-1
3de989ba-53d9-4a14-9d49-ca440f961108,Guideline Learning for In-context Information Extraction,0.665379,"Large language models (LLMs) can perform a new task by merely conditioning on
task instructions and a few input-output examples, without optimizing any
parameters. This is called In-Context Learning (ICL). In-context Information
Extraction (IE) has recently garnered attention in the research community.
However, the performance of In-context IE generally lags behind the
state-of-the-art supervised expert models. We highlight a key reason for this
shortfall: underspecified task description. The limited-length context
struggles to thoroughly express the intricate IE task instructions and various
edge cases, leading to misalignment in task comprehension with humans. In this
paper, we propose a Guideline Learning (GL) framework for In-context IE which
reflectively learns and follows guidelines. During the learning phrase, GL
automatically synthesizes a set of guidelines based on a few error cases, and
during inference, GL retrieves helpful guidelines for better ICL. Moreover, we
propose a self-consistency-based active learning method to enhance the
efficiency of GL. Experiments on event extraction and relation extraction show
that GL can significantly improve the performance of in-context IE.",None,-1
58b0db8c-c505-4bec-affc-bb8b8c5e540d,Personal Protective Equipment Detection in Extreme Construction Conditions,0.398769,"Object detection has been widely applied for construction safety management,
especially personal protective equipment (PPE) detection. Though the existing
PPE detection models trained on conventional datasets have achieved excellent
results, their performance dramatically declines in extreme construction
conditions. A robust detection model NST-YOLOv5 is developed by combining the
neural style transfer (NST) and YOLOv5 technologies. Five extreme conditions
are considered and simulated via the NST module to endow the detection model
with excellent robustness, including low light, intense light, sand dust, fog,
and rain. Experiments show that the NST has great potential as a tool for
extreme data synthesis since it is better at simulating extreme conditions than
other traditional image processing algorithms and helps the NST-YOLOv5 achieve
0.141 and 0.083 mAP_(05:95) improvements in synthesized and real-world extreme
data. This study provides a new feasible way to obtain a more robust detection
model for extreme construction conditions.",None,-1
7fbf29a2-82a5-4c6a-afa2-e6475db99168,From Chaos to Clarity: Claim Normalization to Empower Fact-Checking,0.673253,"With the rise of social media, users are exposed to many misleading claims.
However, the pervasive noise inherent in these posts presents a challenge in
identifying precise and prominent claims that require verification. Extracting
the important claims from such posts is arduous and time-consuming, yet it is
an underexplored problem. Here, we aim to bridge this gap. We introduce a novel
task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and
noisy social media posts into more straightforward and understandable forms,
termed normalized claims. We propose CACN, a pioneering approach that leverages
chain-of-thought and claim check-worthiness estimation, mimicking human
reasoning processes, to comprehend intricate claims. Moreover, we capitalize on
the in-context learning capabilities of large language models to provide
guidance and to improve claim normalization. To evaluate the effectiveness of
our proposed model, we meticulously compile a comprehensive real-world dataset,
CLAN, comprising more than 6k instances of social media posts alongside their
respective normalized claims. Our experiments demonstrate that CACN outperforms
several baselines across various evaluation measures. Finally, our rigorous
error analysis validates CACN's capabilities and pitfalls.",None,-1
4840005a-0992-4f11-a24b-87f96a6c264a,Rethinking the Physical Symbol Systems Hypothesis,0.176673,"It is now more than a half-century since the Physical Symbol Systems
Hypothesis (PSSH) was first articulated as an empirical hypothesis. More recent
evidence from work with neural networks and cognitive architectures has
weakened it, but it has not yet been replaced in any satisfactory manner. Based
on a rethinking of the nature of computational symbols -- as atoms or
placeholders -- and thus also of the systems in which they participate, a
hybrid approach is introduced that responds to these challenges while also
helping to bridge the gap between symbolic and neural approaches, resulting in
two new hypotheses, one that is to replace the PSSH and other focused more
directly on cognitive architectures.",None,-1
63c5920c-54f3-4044-94b7-1ee3843f7028,Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model,0.796142,"In this study, we developed an automated short answer grading (ASAG) model
that provided both analytic scores and final holistic scores. Short answer
items typically consist of multiple sub-questions, and providing an analytic
score and the text span relevant to each sub-question can increase the
interpretability of the automated scores. Furthermore, they can be used to
generate actionable feedback for students. Despite these advantages, most
studies have focused on predicting only holistic scores due to the difficulty
in constructing dataset with manual annotations. To address this difficulty, we
used large language model (LLM)-based one-shot prompting and a text similarity
scoring model with domain adaptation using small manually annotated dataset.
The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a
subset of the publicly available ASAG dataset. The model achieved a substantial
improvement over the majority baseline.",None,-1
ea43f2bf-d5db-4d16-af02-ba2b5dd052f5,"Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility",0.474602,"Deploying machine learning (ML) models often requires both fairness and
privacy guarantees. Both of these objectives present unique trade-offs with the
utility (e.g., accuracy) of the model. However, the mutual interactions between
fairness, privacy, and utility are less well-understood. As a result, often
only one objective is optimized, while the others are tuned as
hyper-parameters. Because they implicitly prioritize certain objectives, such
designs bias the model in pernicious, undetectable ways. To address this, we
adopt impartiality as a principle: design of ML pipelines should not favor one
objective over another. We propose impartially-specified models, which provide
us with accurate Pareto frontiers that show the inherent trade-offs between the
objectives. Extending two canonical ML frameworks for privacy-preserving
learning, we provide two methods (FairDP-SGD and FairPATE) to train
impartially-specified models and recover the Pareto frontier. Through
theoretical privacy analysis and a comprehensive empirical study, we provide an
answer to the question of where fairness mitigation should be integrated within
a privacy-aware ML pipeline.",None,-1
d7a45caf-2e88-4e01-8e28-46de0410ccf7,MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities,0.186856,"Text classification is essential for organizing unstructured text.
Traditional methods rely on human annotations or, more recently, a set of class
seed words for supervision, which can be costly, particularly for specialized
or emerging domains. To address this, using class surface names alone as
extremely weak supervision has been proposed. However, existing approaches
treat different levels of text granularity (documents, sentences, or words)
independently, disregarding inter-granularity class disagreements and the
context identifiable exclusively through joint extraction. In order to tackle
these issues, we introduce MEGClass, an extremely weakly-supervised text
classification method that leverages Mutually-Enhancing Text Granularities.
MEGClass utilizes coarse- and fine-grained context signals obtained by jointly
considering a document's most class-indicative words and sentences. This
approach enables the learning of a contextualized document representation that
captures the most discriminative class indicators. By preserving the
heterogeneity of potential classes, MEGClass can select the most informative
class-indicative documents as iterative feedback to enhance the initial
word-based class representations and ultimately fine-tune a pre-trained text
classifier. Extensive experiments on seven benchmark datasets demonstrate that
MEGClass outperforms other weakly and extremely weakly supervised methods.",None,-1
74ce9382-8dcb-4979-a476-e9d4b6467d9a,Minimum Description Length Clustering to Measure Meaningful Image Complexity,0.176377,"Existing image complexity metrics cannot distinguish meaningful content from
noise. This means that white noise images, which contain no meaningful
information, are judged as highly complex. We present a new image complexity
metric through hierarchical clustering of patches. We use the minimum
description length principle to determine the number of clusters and designate
certain points as outliers and, hence, correctly assign white noise a low
score. The presented method has similarities to theoretical ideas for measuring
meaningful complexity. We conduct experiments on seven different sets of
images, which show that our method assigns the most accurate scores to all
images considered. Additionally, comparing the different levels of the
hierarchy of clusters can reveal how complexity manifests at different scales,
from local detail to global structure. We then present ablation studies showing
the contribution of the components of our method, and that it continues to
assign reasonable scores when the inputs are modified in certain ways,
including the addition of Gaussian noise and the lowering of the resolution.",None,-1
bf02a055-53d0-42a2-9f20-876fb9a39395,Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation,0.762063,"Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.",None,-1
5eed57a0-b8c0-496c-980c-3503355e47b6,"Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques",0.516021,"This paper describes the participation of team QUST in the SemEval2023 task
3. The monolingual models are first evaluated with the under-sampling of the
majority classes in the early stage of the task. Then, the pre-trained
multilingual model is fine-tuned with a combination of the class weights and
the sample weights. Two different fine-tuning strategies, the task-agnostic and
the task-dependent, are further investigated. All experiments are conducted
under the 10-fold cross-validation, the multilingual approaches are superior to
the monolingual ones. The submitted system achieves the second best in Italian
and Spanish (zero-shot) in subtask-1.",None,-1
6e9c1424-4e69-4758-9888-59d620a1f16f,GREC: Generalized Referring Expression Comprehension,0.481225,"The objective of Classic Referring Expression Comprehension (REC) is to
produce a bounding box corresponding to the object mentioned in a given textual
description. Commonly, existing datasets and techniques in classic REC are
tailored for expressions that pertain to a single target, meaning a sole
expression is linked to one specific object. Expressions that refer to multiple
targets or involve no specific target have not been taken into account. This
constraint hinders the practical applicability of REC. This study introduces a
new benchmark termed as Generalized Referring Expression Comprehension (GREC).
This benchmark extends the classic REC by permitting expressions to describe
any number of target objects. To achieve this goal, we have built the first
large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of
expressions: those referring to multiple targets, expressions with no specific
target, and the single-target expressions. The design of GREC and gRefCOCO
ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a
GREC method implementation code, and GREC evaluation code are available at
https://github.com/henghuiding/gRefCOCO.",None,-1
c6fa0a3c-e401-45a1-a0ac-9c43b02fae13,DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction,0.951032,"Most current Event Extraction (EE) methods focus on the high-resource
scenario, which requires a large amount of annotated data and can hardly be
applied to low-resource domains. To address EE more effectively with limited
resources, we propose the Demonstration-enhanced Schema-guided Generation
(DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we
propose the demonstration-based learning paradigm for EE to fully use the
annotated data, which transforms them into demonstrations to illustrate the
extraction process and help the model learn effectively. Secondly, we formulate
EE as a natural language generation task guided by schema-based prompts,
thereby leveraging label semantics and promoting knowledge transfer in
low-resource scenarios. We conduct extensive experiments under in-domain and
domain adaptation low-resource settings on three datasets, and study the
robustness of DemoSG. The results show that DemoSG significantly outperforms
current methods in low-resource scenarios.",None,-1
087c6a6d-eb45-4001-8fef-90cf76ad96b8,Diffusion Model for Generative Image Denoising,0.444449,"In supervised learning for image denoising, usually the paired clean images
and noisy images are collected or synthesised to train a denoising model. L2
norm loss or other distance functions are used as the objective function for
training. It often leads to an over-smooth result with less image details. In
this paper, we regard the denoising task as a problem of estimating the
posterior distribution of clean images conditioned on noisy images. We apply
the idea of diffusion model to realize generative image denoising. According to
the noise model in denoising tasks, we redefine the diffusion process such that
it is different from the original one. Hence, the sampling of the posterior
distribution is a reverse process of dozens of steps from the noisy image. We
consider three types of noise model, Gaussian, Gamma and Poisson noise. With
the guarantee of theory, we derive a unified strategy for model training. Our
method is verified through experiments on three types of noise models and
achieves excellent performance.",None,-1
0c3d1fef-30c2-4143-a1de-ad43b717fc49,Unsupervised Semantic Correspondence Using Stable Diffusion,0.845484,"Text-to-image diffusion models are now capable of generating images that are
often indistinguishable from real images. To generate such images, these models
must understand the semantics of the objects they are asked to generate. In
this work we show that, without any training, one can leverage this semantic
knowledge within diffusion models to find semantic correspondences - locations
in multiple images that have the same semantic meaning. Specifically, given an
image, we optimize the prompt embeddings of these models for maximum attention
on the regions of interest. These optimized embeddings capture semantic
information about the location, which can then be transferred to another image.
By doing so we obtain results on par with the strongly supervised state of the
art on the PF-Willow dataset and significantly outperform (20.9% relative for
the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow,
CUB-200 and SPair-71k datasets.",None,-1
d5b038f7-19a3-44e3-accb-6b6b1deb99fc,Named Entity Resolution in Personal Knowledge Graphs,0.202436,"Entity Resolution (ER) is the problem of determining when two entities refer
to the same underlying entity. The problem has been studied for over 50 years,
and most recently, has taken on new importance in an era of large,
heterogeneous 'knowledge graphs' published on the Web and used widely in
domains as wide ranging as social media, e-commerce and search. This chapter
will discuss the specific problem of named ER in the context of personal
knowledge graphs (PKGs). We begin with a formal definition of the problem, and
the components necessary for doing high-quality and efficient ER. We also
discuss some challenges that are expected to arise for Web-scale data. Next, we
provide a brief literature review, with a special focus on how existing
techniques can potentially apply to PKGs. We conclude the chapter by covering
some applications, as well as promising directions for future research.",None,-1
5ebaa540-8dc7-496f-83c9-372dd1a2ae95,FFT-based Dynamic Token Mixer for Vision,0.0393262,"Multi-head-self-attention (MHSA)-equipped models have achieved notable
performance in computer vision. Their computational complexity is proportional
to quadratic numbers of pixels in input feature maps, resulting in slow
processing, especially when dealing with high-resolution images. New types of
token-mixer are proposed as an alternative to MHSA to circumvent this problem:
an FFT-based token-mixer involves global operations similar to MHSA but with
lower computational complexity. However, despite its attractive properties, the
FFT-based token-mixer has not been carefully examined in terms of its
compatibility with the rapidly evolving MetaFormer architecture. Here, we
propose a novel token-mixer called Dynamic Filter and novel image recognition
models, DFFormer and CDFFormer, to close the gaps above. The results of image
classification and downstream tasks, analysis, and visualization show that our
models are helpful. Notably, their throughput and memory efficiency when
dealing with high-resolution image recognition is remarkable. Our results
indicate that Dynamic Filter is one of the token-mixer options that should be
seriously considered. The code is available at
https://github.com/okojoalg/dfformer",None,-1
364455d5-5f96-4454-b3aa-ea646bd8c2c8,Turning a CLIP Model into a Scene Text Detector,0.512344,"The recent large-scale Contrastive Language-Image Pretraining (CLIP) model
has shown great potential in various downstream tasks via leveraging the
pretrained vision and language knowledge. Scene text, which contains rich
textual and visual information, has an inherent connection with a model like
CLIP. Recently, pretraining approaches based on vision language models have
made effective progresses in the field of text detection. In contrast to these
works, this paper proposes a new method, termed TCM, focusing on Turning the
CLIP Model directly for text detection without pretraining process. We
demonstrate the advantages of the proposed TCM as follows: (1) The underlying
principle of our framework can be applied to improve existing scene text
detector. (2) It facilitates the few-shot training capability of existing
methods, e.g., by using 10% of labeled data, we significantly improve the
performance of the baseline method with an average of 22% in terms of the
F-measure on 4 benchmarks. (3) By turning the CLIP model into existing scene
text detection methods, we further achieve promising domain adaptation ability.
The code will be publicly released at https://github.com/wenwenyu/TCM.",None,-1
85c3ae6f-e70e-4de0-8521-5e4c7e4bc413,Likelihood-Based Generative Radiance Field with Latent Space Energy-Based Model for 3D-Aware Disentangled Image Representation,0.216564,"We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.",None,-1
b40df4fc-d6a7-4e3b-9e37-607eeb9236cc,FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding,0.726271,"Although Domain Adaptation in Semantic Scene Segmentation has shown
impressive improvement in recent years, the fairness concerns in the domain
adaptation have yet to be well defined and addressed. In addition, fairness is
one of the most critical aspects when deploying the segmentation models into
human-related real-world applications, e.g., autonomous driving, as any unfair
predictions could influence human safety. In this paper, we propose a novel
Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In
particular, from the proposed formulated fairness objective, a new adaptation
framework will be introduced based on the fair treatment of class
distributions. Moreover, to generally model the context of structural
dependency, a new conditional structural constraint is introduced to impose the
consistency of predicted segmentation. Thanks to the proposed Conditional
Structure Network, the self-attention mechanism has sufficiently modeled the
structural information of segmentation. Through the ablation studies, the
proposed method has shown the performance improvement of the segmentation
models and promoted fairness in the model predictions. The experimental results
on the two standard benchmarks, i.e., SYNTHIA $\to$ Cityscapes and GTA5 $\to$
Cityscapes, have shown that our method achieved State-of-the-Art (SOTA)
performance.",None,-1
26e52a8d-6e6e-4d5f-b6c7-da909ec8fb8d,Preference Transformer: Modeling Human Preferences using Transformers for RL,0.790777,"Preference-based reinforcement learning (RL) provides a framework to train
agents using human preferences between two behaviors. However, preference-based
RL has been challenging to scale since it requires a large amount of human
feedback to learn a reward function aligned with human intent. In this paper,
we present Preference Transformer, a neural architecture that models human
preferences using transformers. Unlike prior approaches assuming human judgment
is based on the Markovian rewards which contribute to the decision equally, we
introduce a new preference model based on the weighted sum of non-Markovian
rewards. We then design the proposed preference model using a transformer
architecture that stacks causal and bidirectional self-attention layers. We
demonstrate that Preference Transformer can solve a variety of control tasks
using real human preferences, while prior approaches fail to work. We also show
that Preference Transformer can induce a well-specified reward and attend to
critical events in the trajectory by automatically capturing the temporal
dependencies in human decision-making. Code is available on the project
website: https://sites.google.com/view/preference-transformer.",None,-1
0b991003-6e0d-478a-bbed-a0d60c34ed13,Agent Instructs Large Language Models to be General Zero-Shot Reasoners,0.436075,"We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.",None,-1
b0acbeb4-3991-47f7-96ae-553fbf9427e3,SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,0.868656,"We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent's action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.",None,-1
9e03baa7-3ab7-4f2c-ba48-e6a27d96b6d3,Neuron Structure Modeling for Generalizable Remote Physiological Measurement,0.972568,"Remote photoplethysmography (rPPG) technology has drawn increasing attention
in recent years. It can extract Blood Volume Pulse (BVP) from facial videos,
making many applications like health monitoring and emotional analysis more
accessible. However, as the BVP signal is easily affected by environmental
changes, existing methods struggle to generalize well for unseen domains. In
this paper, we systematically address the domain shift problem in the rPPG
measurement task. We show that most domain generalization methods do not work
well in this problem, as domain labels are ambiguous in complicated
environmental changes. In light of this, we propose a domain-label-free
approach called NEuron STructure modeling (NEST). NEST improves the
generalization capacity by maximizing the coverage of feature space during
training, which reduces the chance for under-optimized feature activation
during inference. Besides, NEST can also enrich and enhance domain invariant
features across multi-domain. We create and benchmark a large-scale domain
generalization protocol for the rPPG measurement task. Extensive experiments
show that our approach outperforms the state-of-the-art methods on both
cross-dataset and intra-dataset settings.",None,-1
41d14c9a-deda-4b85-a2f0-5419cd033063,BiLMa: Bidirectional Local-Matching for Text-based Person Re-identification,0.826398,"Text-based person re-identification (TBPReID) aims to retrieve person images
represented by a given textual query. In this task, how to effectively align
images and texts globally and locally is a crucial challenge. Recent works have
obtained high performances by solving Masked Language Modeling (MLM) to align
image/text parts. However, they only performed uni-directional (i.e., from
image to text) local-matching, leaving room for improvement by introducing
opposite-directional (i.e., from text to image) local-matching. In this work,
we introduce Bidirectional Local-Matching (BiLMa) framework that jointly
optimize MLM and Masked Image Modeling (MIM) in TBPReID model training. With
this framework, our model is trained so as the labels of randomly masked both
image and text tokens are predicted by unmasked tokens. In addition, to narrow
the semantic gap between image and text in MIM, we propose Semantic MIM
(SemMIM), in which the labels of masked image tokens are automatically given by
a state-of-the-art human parser. Experimental results demonstrate that our
BiLMa framework with SemMIM achieves state-of-the-art Rank@1 and mAP scores on
three benchmarks.",None,-1
b3687265-4e96-45af-92d3-49fd6f0f04ac,Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation,0.443066,"Uncertainty estimation is an important research area to make deep neural
networks (DNNs) more trustworthy. While extensive research on uncertainty
estimation has been conducted with unimodal data, uncertainty estimation for
multimodal data remains a challenge. Neural processes (NPs) have been
demonstrated to be an effective uncertainty estimation method for unimodal data
by providing the reliability of Gaussian processes with efficient and powerful
DNNs. While NPs hold significant potential for multimodal uncertainty
estimation, the adaptation of NPs for multimodal data has not been carefully
studied. To bridge this gap, we propose Multimodal Neural Processes (MNPs) by
generalising NPs for multimodal uncertainty estimation. Based on the framework
of NPs, MNPs consist of several novel and principled mechanisms tailored to the
characteristics of multimodal data. In extensive empirical evaluation, our
method achieves state-of-the-art multimodal uncertainty estimation performance,
showing its appealing robustness against noisy samples and reliability in
out-of-distribution detection with faster computation time compared to the
current state-of-the-art multimodal uncertainty estimation method.",None,-1
043e2415-33f6-4036-83dd-d4b55ec6a2f0,Instance-based Explanations for Gradient Boosting Machine Predictions with AXIL Weights,0.096829,"We show that regression predictions from linear and tree-based models can be
represented as linear combinations of target instances in the training data.
This also holds for models constructed as ensembles of trees, including Random
Forests and Gradient Boosting Machines. The weights used in these linear
combinations are measures of instance importance, complementing existing
measures of feature importance, such as SHAP and LIME. We refer to these
measures as AXIL weights (Additive eXplanations with Instance Loadings). Since
AXIL weights are additive across instances, they offer both local and global
explanations. Our work contributes to the broader effort to make machine
learning predictions more interpretable and explainable.",None,-1
3fb18932-2670-4f41-90f3-8069f6d44557,Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations,0.825474,"Large language models (LLMs) can generate fluent natural language texts when
given relevant documents as background context. This ability has attracted
considerable interest in developing industry applications of LLMs. However,
LLMs are prone to generate hallucinations that are not supported by the
provided sources. In this paper, we propose a hierarchical framework to detect
and mitigate such ungrounded hallucination. Our framework uses Chain of Natural
Language Inference (CoNLI) for hallucination detection and hallucination
reduction via post-editing. Our approach achieves state-of-the-art performance
on hallucination detection and enhances text quality through rewrite, using
LLMs without any fine-tuning or domain-specific prompt engineering. We show
that this simple plug-and-play framework can serve as an effective choice for
hallucination detection and reduction, achieving competitive performance across
various contexts.",None,-1
20867284-79d8-4a82-8a5b-4eb275247433,Coverage-based Example Selection for In-Context Learning,0.446988,"In-context learning (ICL), the ability of large language models to perform
novel tasks by conditioning on a prompt with a few task examples, requires
these examples to be informative about the test instance. The standard approach
of independently ranking and selecting the most similar examples selects
redundant examples while omitting important information. In this work, we show
that BERTScore-Recall (BSR) selects better examples that demonstrate more of
the salient aspects, e.g. reasoning patterns, of the test input. We further
extend BSR and many standard metrics to easily optimizable set-level metrics,
giving still better coverage of those salient aspects. On 15 datasets spanning
6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric
for in-context example selection across the board, and (2) for compositional
tasks, set selection using Set-BSR outperforms independent ranking by up to 17
points on average and, despite being training-free, surpasses methods that
leverage task or LLM-specific training.",None,-1
496acd16-bf33-402d-ba3c-477802a8963d,Denoising Diffusion Autoencoders are Unified Self-supervised Learners,0.643802,"Inspired by recent advances in diffusion models, which are reminiscent of
denoising autoencoders, we investigate whether they can acquire discriminative
representations for classification via generative pre-training. This paper
shows that the networks in diffusion models, namely denoising diffusion
autoencoders (DDAE), are unified self-supervised learners: by pre-training on
unconditional image generation, DDAE has already learned strongly
linear-separable representations within its intermediate layers without
auxiliary encoders, thus making diffusion pre-training emerge as a general
approach for generative-and-discriminative dual learning. To validate this, we
conduct linear probe and fine-tuning evaluations. Our diffusion-based approach
achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and
Tiny-ImageNet, respectively, and is comparable to contrastive learning and
masked autoencoders for the first time. Transfer learning from ImageNet also
confirms the suitability of DDAE for Vision Transformers, suggesting the
potential to scale DDAEs as unified foundation models. Code is available at
github.com/FutureXiang/ddae.",None,-1
f6acfe0b-4596-40a1-aa0b-038c4a7856ef,Multimodal Chain-of-Thought Reasoning in Language Models,0.964243,"Large language models (LLMs) have shown impressive performance on complex
reasoning by leveraging chain-of-thought (CoT) prompting to generate
intermediate reasoning chains as the rationale to infer the answer. However,
existing CoT studies have primarily focused on the language modality. We
propose Multimodal-CoT that incorporates language (text) and vision (images)
modalities into a two-stage framework that separates rationale generation and
answer inference. In this way, answer inference can leverage better generated
rationales that are based on multimodal information. Experimental results on
ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed
approach. With Multimodal-CoT, our model under 1 billion parameters achieves
state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates
that Multimodal-CoT offers the advantages of mitigating hallucination and
enhancing convergence speed. Code is publicly available at
https://github.com/amazon-science/mm-cot.",None,-1
fa50b24d-eda2-4ec2-b202-102f360f256c,A Unified Conditional Framework for Diffusion-based Image Restoration,0.726608,"Diffusion Probabilistic Models (DPMs) have recently shown remarkable
performance in image generation tasks, which are capable of generating highly
realistic images. When adopting DPMs for image restoration tasks, the crucial
aspect lies in how to integrate the conditional information to guide the DPMs
to generate accurate and natural output, which has been largely overlooked in
existing works. In this paper, we present a unified conditional framework based
on diffusion models for image restoration. We leverage a lightweight UNet to
predict initial guidance and the diffusion model to learn the residual of the
guidance. By carefully designing the basic module and integration module for
the diffusion model block, we integrate the guidance and other auxiliary
conditional information into every block of the diffusion model to achieve
spatially-adaptive generation conditioning. To handle high-resolution images,
we propose a simple yet effective inter-step patch-splitting strategy to
produce arbitrary-resolution images without grid artifacts. We evaluate our
conditional framework on three challenging tasks: extreme low-light denoising,
deblurring, and JPEG restoration, demonstrating its significant improvements in
perceptual quality and the generalization to restoration tasks.",None,-1
13273539-dc72-4acd-8d22-80b00ec8019b,Synthetically generated text for supervised text analysis,0.101369,"Supervised text models are a valuable tool for political scientists but
present several obstacles to their use, including the expense of hand-labeling
documents, the difficulty of retrieving rare relevant documents for annotation,
and copyright and privacy concerns involved in sharing annotated documents.
This article proposes a partial solution to these three issues, in the form of
controlled generation of synthetic text with large language models. I provide a
conceptual overview of text generation, guidance on when researchers should
prefer different techniques for generating synthetic text, a discussion of
ethics, and a simple technique for improving the quality of synthetic text. I
demonstrate the usefulness of synthetic text with three applications:
generating synthetic tweets describing the fighting in Ukraine, synthetic news
articles describing specified political events for training an event detection
system, and a multilingual corpus of populist manifesto statements for training
a sentence-level populism classifier.",None,-1
46b93eda-3992-4de0-820d-6576a89966b8,Nemo: First Glimpse of a New Rule Engine,0.667434,"This system demonstration presents Nemo, a new logic programming engine with
a focus on reliability and performance. Nemo is built for data-centric analytic
computations, modelled in a fully declarative Datalog dialect. Its scalability
for these tasks matches or exceeds that of leading Datalog systems. We
demonstrate uses in reasoning with knowledge graphs and ontologies with 10^5 to
10^8 input facts, all on a laptop. Nemo is written in Rust and available as a
free and open source tool.",None,-1
dfc216fa-6459-457d-910d-7831cdad68cd,Everyone Deserves A Reward: Learning Customized Human Preferences,0.750768,"Reward models (RMs) are essential for aligning large language models (LLMs)
with human preferences to improve interaction quality. However, the real world
is pluralistic, which leads to diversified human preferences with respect to
different religions, politics, cultures, etc. Moreover, each individual can
have their unique preferences on various topics. Neglecting the diversity of
human preferences, current human feedback aligning methods only consider a
general reward model, which is below satisfaction for customized or
personalized application scenarios. To explore customized preference learning,
we collect a domain-specific preference (DSP) dataset, which includes preferred
responses for each given query from four practical domains. Besides, from the
perspective of data efficiency, we propose a three-stage customized RM learning
scheme, then empirically verify its effectiveness on both general preference
datasets and our DSP set. Furthermore, we test multiple training and data
strategies on the three learning stages. We find several ways to better
preserve the general preferring ability while training the customized RMs,
especially general preference enrichment, and customized preference imitation
learning. The DSP dataset and code are available at
https://github.com/Linear95/DSP.",None,-1
077f7b87-8297-4809-af28-db582a11f763,STTracker: Spatio-Temporal Tracker for 3D Single Object Tracking,0.480941,"3D single object tracking with point clouds is a critical task in 3D computer
vision. Previous methods usually input the last two frames and use the
predicted box to get the template point cloud in previous frame and the search
area point cloud in the current frame respectively, then use similarity-based
or motion-based methods to predict the current box. Although these methods
achieved good tracking performance, they ignore the historical information of
the target, which is important for tracking. In this paper, compared to
inputting two frames of point clouds, we input multi-frame of point clouds to
encode the spatio-temporal information of the target and learn the motion
information of the target implicitly, which could build the correlations among
different frames to track the target in the current frame efficiently.
Meanwhile, rather than directly using the point feature for feature fusion, we
first crop the point cloud features into many patches and then use sparse
attention mechanism to encode the patch-level similarity and finally fuse the
multi-frame features. Extensive experiments show that our method achieves
competitive results on challenging large-scale benchmarks (62.6% in KITTI and
49.66% in NuScenes).",None,-1
8b90975d-d146-4602-8aeb-c4210ba2f12f,OPHAvatars: One-shot Photo-realistic Head Avatars,0.084743,"We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.",None,-1
9b1d1d88-a596-4ea6-89f8-b788423339ab,Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?,0.492632,"ASR systems are generally built for the spoken 'standard', and their
performance declines for non-standard dialects/varieties. This is a problem for
a language like Irish, where there is no single spoken standard, but rather
three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a
diagnostic to quantify the effect of the speaker's dialect on recognition
performance, 12 ASR systems were trained, firstly using baseline
dialect-balanced training corpora, and then using modified versions of the
baseline corpora, where dialect-specific materials were either subtracted or
added. Results indicate that dialect-balanced corpora do not yield a similar
performance across the dialects: the Ul dialect consistently underperforms,
whereas Mu yields lowest WERs. There is a close relationship between Co and Mu
dialects, but one that is not symmetrical. These results will guide future
corpus collection and system building strategies to optimise for cross-dialect
performance equity.",None,-1
fb86d646-00c1-4142-8a5c-a2b0bc89ab4d,SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models,0.83607,"Interpreting remote sensing imagery enables numerous downstream applications
ranging from land-use planning to deforestation monitoring. Robustly
classifying this data is challenging due to the Earth's geographic diversity.
While many distinct satellite and aerial image classification datasets exist,
there is yet to be a benchmark curated that suitably covers this diversity. In
this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from
27 existing remotely sensed datasets, and comprehensively evaluate the
zero-shot transfer classification capabilities of a broad range of
vision-language (VL) models on SATIN. We find SATIN to be a challenging
benchmark-the strongest method we evaluate achieves a classification accuracy
of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public
leaderboard}}$ to guide and track the progress of VL models in this important
domain.",None,-1
95c749a6-b8a3-4ebc-ad86-427657273bb5,Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning,0.46406,"In multi-agent reinforcement learning (MARL), self-interested agents attempt
to establish equilibrium and achieve coordination depending on game structure.
However, existing MARL approaches are mostly bound by the simultaneous actions
of all agents in the Markov game (MG) framework, and few works consider the
formation of equilibrium strategies via asynchronous action coordination. In
view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium,
we construct a spatio-temporal sequential decision-making structure derived
from the MG and propose an N-level policy model based on a conditional
hypernetwork shared by all agents. This approach allows for asymmetric training
with symmetric execution, with each agent responding optimally conditioned on
the decisions made by superior agents. Agents can learn heterogeneous SE
policies while still maintaining parameter sharing, which leads to reduced cost
for learning and storage and enhanced scalability as the number of agents
increases. Experiments demonstrate that our method effectively converges to the
SE policies in repeated matrix game scenarios, and performs admirably in
immensely complex settings including cooperative tasks and mixed tasks.",None,-1
2c8360c1-8065-4905-8146-3fc589ff8d97,Video Generation Beyond a Single Clip,0.0584624,"We tackle the long video generation problem, i.e.~generating videos beyond
the output length of video generation models. Due to the computation resource
constraints, video generation models can only generate video clips that are
relatively short compared with the length of real videos. Existing works apply
a sliding window approach to generate long videos at inference time, which is
often limited to generating recurrent events or homogeneous content. To
generate long videos covering diverse content and multiple events, we propose
to use additional guidance to control the video generation process. We further
present a two-stage approach to the problem, which allows us to utilize
existing video generation models to generate high-quality videos within a small
time window while modeling the video holistically based on the input guidance.
The proposed approach is complementary to existing efforts on video generation,
which focus on generating realistic video within a fixed time window. Extensive
experiments on challenging real-world videos validate the benefit of the
proposed method, which improves over state-of-the-art by up to 9.5% in
objective metrics and is preferred by users more than 80% of time.",None,-1
1e59fd34-0097-4ed5-baf0-9481709fc2c6,Using Z3 for Formal Modeling and Verification of FNN Global Robustness,0.853753,"While Feedforward Neural Networks (FNNs) have achieved remarkable success in
various tasks, they are vulnerable to adversarial examples. Several techniques
have been developed to verify the adversarial robustness of FNNs, but most of
them focus on robustness verification against the local perturbation
neighborhood of a single data point. There is still a large research gap in
global robustness analysis. The global-robustness verifiable framework
DeepGlobal has been proposed to identify \textit{all} possible Adversarial
Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In
this paper, we propose a complete specification and implementation of
DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and
propose several improvements to DeepGlobal for more efficient verification. To
evaluate the effectiveness of our implementation and improvements, we conduct
extensive experiments on a set of benchmark datasets. Visualization of our
experiment results shows the validity and effectiveness of the approach.",None,-1
5b95362f-0b1c-46ed-822b-8ac96b9b6260,Identifiability of Direct Effects from Summary Causal Graphs,0.144259,"Dynamic structural causal models (SCMs) are a powerful framework for
reasoning in dynamic systems about direct effects which measure how a change in
one variable affects another variable while holding all other variables
constant. The causal relations in a dynamic structural causal model can be
qualitatively represented with an acyclic full-time causal graph. Assuming
linearity and no hidden confounding and given the full-time causal graph, the
direct causal effect is always identifiable. However, in many application such
a graph is not available for various reasons but nevertheless experts have
access to the summary causal graph of the full-time causal graph which
represents causal relations between time series while omitting temporal
information and allowing cycles. This paper presents a complete identifiability
result which characterizes all cases for which the direct effect is graphically
identifiable from a summary causal graph and gives two sound finite adjustment
sets that can be used to estimate the direct effect whenever it is
identifiable.",None,-1
1e15785b-df4c-4240-bf0d-fcbb49bb4490,Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training,0.130355,"Parallel training of neural networks at scale is challenging due to
significant overheads arising from communication. Recently, deep learning
researchers have developed a variety of pruning algorithms that are capable of
pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to
yield sparse subnetworks that equal the accuracy of the unpruned parent
network. In this work, we propose a novel approach that exploits these sparse
subnetworks to optimize the memory utilization and communication in two popular
algorithms for parallel deep learning namely -- data and inter-layer
parallelism. We integrate our approach into AxoNN, a highly scalable framework
for parallel deep learning that relies on data and inter-layer parallelism, and
demonstrate the reduction in communication time and memory utilization. On 512
NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7
billion parameter model by 74%, and the total communication time by 40%, thus
providing an overall speedup of 34% over AxoNN, 32% over DeepSpeed-3D and 46%
over Sputnik, a sparse matrix computation baseline.",None,-1
7827d154-fc9a-469f-9983-08c7ef99bf7a,Erasing Concepts from Diffusion Models,0.849365,"Motivated by recent advancements in text-to-image diffusion, we study erasure
of specific concepts from the model's weights. While Stable Diffusion has shown
promise in producing explicit or realistic artwork, it has raised concerns
regarding its potential for misuse. We propose a fine-tuning method that can
erase a visual concept from a pre-trained diffusion model, given only the name
of the style and using negative guidance as a teacher. We benchmark our method
against previous approaches that remove sexually explicit content and
demonstrate its effectiveness, performing on par with Safe Latent Diffusion and
censored training. To evaluate artistic style removal, we conduct experiments
erasing five modern artists from the network and conduct a user study to assess
the human perception of the removed styles. Unlike previous methods, our
approach can remove concepts from a diffusion model permanently rather than
modifying the output at the inference time, so it cannot be circumvented even
if a user has access to model weights. Our code, data, and results are
available at https://erasing.baulab.info/",None,-1
2e7f1fd7-393e-4774-8365-0f17a36f62c2,An Overview Of Temporal Commonsense Reasoning and Acquisition,0.364733,"Temporal commonsense reasoning refers to the ability to understand the
typical temporal context of phrases, actions, and events, and use it to reason
over problems requiring such knowledge. This trait is essential in temporal
natural language processing tasks, with possible applications such as timeline
summarization, temporal question answering, and temporal natural language
inference. Recent research on the performance of large language models suggests
that, although they are adept at generating syntactically correct sentences and
solving classification tasks, they often take shortcuts in their reasoning and
fall prey to simple linguistic traps. This article provides an overview of
research in the domain of temporal commonsense reasoning, particularly focusing
on enhancing language model performance through a variety of augmentations and
their evaluation across a growing number of datasets. However, these augmented
models still struggle to approach human performance on reasoning tasks over
temporal common sense properties, such as the typical occurrence times,
orderings, or durations of events. We further emphasize the need for careful
interpretation of research to guard against overpromising evaluation results in
light of the shallow reasoning present in transformers. This can be achieved by
appropriately preparing datasets and suitable evaluation metrics.",None,-1
fc062c03-e15b-4465-b54b-ac0543d3ee91,ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis,0.616197,"Generative AI has received substantial attention in recent years due to its
ability to synthesize data that closely resembles the original data source.
While Generative Adversarial Networks (GANs) have provided innovative
approaches for histopathological image analysis, they suffer from limitations
such as mode collapse and overfitting in discriminator. Recently, Denoising
Diffusion models have demonstrated promising results in computer vision. These
models exhibit superior stability during training, better distribution
coverage, and produce high-quality diverse images. Additionally, they display a
high degree of resilience to noise and perturbations, making them well-suited
for use in digital pathology, where images commonly contain artifacts and
exhibit significant variations in staining. In this paper, we present a novel
approach, namely ViT-DAE, which integrates vision transformers (ViT) and
diffusion autoencoders for high-quality histopathology image synthesis. This
marks the first time that ViT has been introduced to diffusion autoencoders in
computational pathology, allowing the model to better capture the complex and
intricate details of histopathology images. We demonstrate the effectiveness of
ViT-DAE on three publicly available datasets. Our approach outperforms recent
GAN-based and vanilla DAE methods in generating realistic images.",None,-1
dc50b9a9-15cc-4551-bb5a-e817d71d8b6d,AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder,0.954397,"The recently introduced Segment Anything Model (SAM) combines a clever
architecture and large quantities of training data to obtain remarkable image
segmentation capabilities. However, it fails to reproduce such results for
Out-Of-Distribution (OOD) domains such as medical images. Moreover, while SAM
is conditioned on either a mask or a set of points, it may be desirable to have
a fully automatic solution. In this work, we replace SAM's conditioning with an
encoder that operates on the same input image. By adding this encoder and
without further fine-tuning SAM, we obtain state-of-the-art results on multiple
medical images and video benchmarks. This new encoder is trained via gradients
provided by a frozen SAM. For inspecting the knowledge within it, and providing
a lightweight segmentation solution, we also learn to decode it into a mask by
a shallow deconvolution network.",None,-1
a084458f-03ec-4854-a7aa-9f7c6c74eb2e,How Can Large Language Models Help Humans in Design and Manufacturing?,0.591619,"The advancement of Large Language Models (LLMs), including GPT-4, provides
exciting new opportunities for generative design. We investigate the
application of this tool across the entire design and manufacturing workflow.
Specifically, we scrutinize the utility of LLMs in tasks such as: converting a
text-based prompt into a design specification, transforming a design into
manufacturing instructions, producing a design space and design variations,
computing the performance of a design, and searching for designs predicated on
performance. Through a series of examples, we highlight both the benefits and
the limitations of the current LLMs. By exposing these limitations, we aspire
to catalyze the continued improvement and progression of these models.",None,-1
4e2090ab-2e70-4d84-9a40-9b8bc6e88787,Bridging the Transparency Gap: What Can Explainable AI Learn From the AI Act?,0.341543,"The European Union has proposed the Artificial Intelligence Act which
introduces detailed requirements of transparency for AI systems. Many of these
requirements can be addressed by the field of explainable AI (XAI), however,
there is a fundamental difference between XAI and the Act regarding what
transparency is. The Act views transparency as a means that supports wider
values, such as accountability, human rights, and sustainable innovation. In
contrast, XAI views transparency narrowly as an end in itself, focusing on
explaining complex algorithmic properties without considering the
socio-technical context. We call this difference the ``transparency gap''.
Failing to address the transparency gap, XAI risks leaving a range of
transparency issues unaddressed. To begin to bridge this gap, we overview and
clarify the terminology of how XAI and European regulation -- the Act and the
related General Data Protection Regulation (GDPR) -- view basic definitions of
transparency. By comparing the disparate views of XAI and regulation, we arrive
at four axes where practical work could bridge the transparency gap: defining
the scope of transparency, clarifying the legal status of XAI, addressing
issues with conformity assessment, and building explainability for datasets.",None,-1
a39b380c-fee3-499a-8453-d643c0dfadf7,Trusting Your Evidence: Hallucinate Less with Context-aware Decoding,0.912161,"Language models (LMs) often struggle to pay enough attention to the input
context, and generate texts that are unfaithful or contain hallucinations. To
mitigate this issue, we present context-aware decoding (CAD), which follows a
contrastive output distribution that amplifies the difference between the
output probabilities when a model is used with and without context. Our
experiments show that CAD, without additional training, significantly improves
the faithfulness of different LM families, including OPT, GPT, LLaMA and
FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality
metrics). Furthermore, CAD is particularly effective in overriding a model's
prior knowledge when it contradicts the provided context, leading to
substantial improvements in tasks where resolving the knowledge conflict is
essential.",None,-1
5ffe325c-7c6a-4a07-83b1-f1a888af310c,Score-balanced Loss for Multi-aspect Pronunciation Assessment,0.283453,"With rapid technological growth, automatic pronunciation assessment has
transitioned toward systems that evaluate pronunciation in various aspects,
such as fluency and stress. However, despite the highly imbalanced score labels
within each aspect, existing studies have rarely tackled the data imbalance
problem. In this paper, we suggest a novel loss function, score-balanced loss,
to address the problem caused by uneven data, such as bias toward the majority
scores. As a re-weighting approach, we assign higher costs when the predicted
score is of the minority class, thus, guiding the model to gain positive
feedback for sparse score prediction. Specifically, we design two weighting
factors by leveraging the concept of an effective number of samples and using
the ranks of scores. We evaluate our method on the speechocean762 dataset,
which has noticeably imbalanced scores for several aspects. Improved results
particularly on such uneven aspects prove the effectiveness of our method.",None,-1
0efddf23-eb89-4283-8a07-914744f5c9d2,Applying Large Language Models for Causal Structure Learning in Non Small Cell Lung Cancer,0.656434,"Causal discovery is becoming a key part in medical AI research. These methods
can enhance healthcare by identifying causal links between biomarkers,
demographics, treatments and outcomes. They can aid medical professionals in
choosing more impactful treatments and strategies. In parallel, Large Language
Models (LLMs) have shown great potential in identifying patterns and generating
insights from text data. In this paper we investigate applying LLMs to the
problem of determining the directionality of edges in causal discovery.
Specifically, we test our approach on a deidentified set of Non Small Cell Lung
Cancer(NSCLC) patients that have both electronic health record and genomic
panel data. Graphs are validated using Bayesian Dirichlet estimators using
tabular data. Our result shows that LLMs can accurately predict the
directionality of edges in causal graphs, outperforming existing
state-of-the-art methods. These findings suggests that LLMs can play a
significant role in advancing causal discovery and help us better understand
complex systems.",None,-1
e8e9e04a-b730-4bc6-a92a-b99f976f187e,"What Should Be Balanced in a ""Balanced"" Face Recognition Dataset?",0.663357,"The issue of demographic disparities in face recognition accuracy has
attracted increasing attention in recent years. Various face image datasets
have been proposed as 'fair' or 'balanced' to assess the accuracy of face
recognition algorithms across demographics. These datasets typically balance
the number of identities and images across demographics. It is important to
note that the number of identities and images in an evaluation dataset are {\em
not} driving factors for 1-to-1 face matching accuracy. Moreover, balancing the
number of identities and images does not ensure balance in other factors known
to impact accuracy, such as head pose, brightness, and image quality. We
demonstrate these issues using several recently proposed datasets. To improve
the ability to perform less biased evaluations, we propose a bias-aware toolkit
that facilitates creation of cross-demographic evaluation datasets balanced on
factors mentioned in this paper.",None,-1
45b2edf1-82c9-4dd8-9017-de661c89cd7d,Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing,0.25631,"Recent works in end-to-end speech-to-text translation (ST) have proposed
multi-tasking methods with soft parameter sharing which leverage machine
translation (MT) data via secondary encoders that map text inputs to an
eventual cross-modal representation. In this work, we instead propose a ST/MT
multi-tasking framework with hard parameter sharing in which all model
parameters are shared cross-modally. Our method reduces the speech-text
modality gap via a pre-processing stage which converts speech and text inputs
into two discrete token sequences of similar length -- this allows models to
indiscriminately process both modalities simply using a joint vocabulary. With
experiments on MuST-C, we demonstrate that our multi-tasking framework improves
attentional encoder-decoder, Connectionist Temporal Classification (CTC),
transducer, and joint CTC/attention models by an average of +0.5 BLEU without
any external MT data. Further, we show that this framework incorporates
external MT data, yielding +0.8 BLEU, and also improves transfer learning from
pre-trained textual models, yielding +1.8 BLEU.",None,-1
1cadf93a-5c63-4d1b-9b91-08c907431244,Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data,0.209793,"Most research on hate speech detection has focused on English where a
sizeable amount of labeled training data is available. However, to expand hate
speech detection into more languages, approaches that require minimal training
data are needed. In this paper, we test whether natural language inference
(NLI) models which perform well in zero- and few-shot settings can benefit hate
speech detection performance in scenarios where only a limited amount of
labeled data is available in the target language. Our evaluation on five
languages demonstrates large performance improvements of NLI fine-tuning over
direct fine-tuning in the target language. However, the effectiveness of
previous work that proposed intermediate fine-tuning on English data is hard to
match. Only in settings where the English training data does not match the test
domain, can our customised NLI-formulation outperform intermediate fine-tuning
on English. Based on our extensive experiments, we propose a set of
recommendations for hate speech detection in languages where minimal labeled
training data is available.",None,-1
26b32978-2f65-4e9a-a3f8-924587a16e39,Towards predicting Pedestrian Evacuation Time and Density from Floorplans using a Vision Transformer,0.673457,"Conventional pedestrian simulators are inevitable tools in the design process
of a building, as they enable project engineers to prevent overcrowding
situations and plan escape routes for evacuation. However, simulation runtime
and the multiple cumbersome steps in generating simulation results are
potential bottlenecks during the building design process. Data-driven
approaches have demonstrated their capability to outperform conventional
methods in speed while delivering similar or even better results across many
disciplines. In this work, we present a deep learning-based approach based on a
Vision Transformer to predict density heatmaps over time and total evacuation
time from a given floorplan. Specifically, due to limited availability of
public datasets, we implement a parametric data generation pipeline including a
conventional simulator. This enables us to build a large synthetic dataset that
we use to train our architecture. Furthermore, we seamlessly integrate our
model into a BIM-authoring tool to generate simulation results instantly and
automatically.",None,-1
90ceea1d-e48c-43cd-a0b0-1cf7d78d36b5,Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization,0.843981,"Backdoor defense, which aims to detect or mitigate the effect of malicious
triggers introduced by attackers, is becoming increasingly critical for machine
learning security and integrity. Fine-tuning based on benign data is a natural
defense to erase the backdoor effect in a backdoored model. However, recent
studies show that, given limited benign data, vanilla fine-tuning has poor
defense performance. In this work, we provide a deep study of fine-tuning the
backdoored model from the neuron perspective and find that backdoorrelated
neurons fail to escape the local minimum in the fine-tuning process. Inspired
by observing that the backdoorrelated neurons often have larger norms, we
propose FTSAM, a novel backdoor defense paradigm that aims to shrink the norms
of backdoor-related neurons by incorporating sharpness-aware minimization with
fine-tuning. We demonstrate the effectiveness of our method on several
benchmark datasets and network architectures, where it achieves
state-of-the-art defense performance. Overall, our work provides a promising
avenue for improving the robustness of machine learning models against backdoor
attacks.",None,-1
a2c94a0e-e01b-48c5-b371-67a3a2e3c3b6,GridMM: Grid Memory Map for Vision-and-Language Navigation,0.784122,"Vision-and-language navigation (VLN) enables the agent to navigate to a
remote location following the natural language instruction in 3D environments.
To represent the previously visited environment, most approaches for VLN
implement memory using recurrent states, topological maps, or top-down semantic
maps. In contrast to these approaches, we build the top-down egocentric and
dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited
environment. From a global perspective, historical observations are projected
into a unified grid map in a top-down view, which can better represent the
spatial relations of the environment. From a local perspective, we further
propose an instruction relevance aggregation method to capture fine-grained
visual clues in each grid region. Extensive experiments are conducted on both
the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE
dataset in the continuous environments, showing the superiority of our proposed
method.",None,-1
28d09d72-210f-437b-8b80-545e9055222a,Hybrid Open-set Segmentation with Synthetic Negative Data,0.106281,"Open-set segmentation can be conceived by complementing closed-set
classification with anomaly detection. Many of the existing dense anomaly
detectors operate through generative modelling of regular data or by
discriminating with respect to negative data. These two approaches optimize
different objectives and therefore exhibit different failure modes.
Consequently, we propose a novel anomaly score that fuses generative and
discriminative cues. Our score can be implemented by upgrading any closed-set
segmentation model with dense estimates of dataset posterior and unnormalized
data likelihood. The resulting dense hybrid open-set models require negative
training images that can be sampled from an auxiliary negative dataset, from a
jointly trained generative model, or from a mixture of both sources. We
evaluate our contributions on benchmarks for dense anomaly detection and
open-set segmentation. The experiments reveal strong open-set performance in
spite of negligible computational overhead.",None,-1
dec976f0-8ea7-41ff-a1f3-126c7749fbbe,Assessing SATNet's Ability to Solve the Symbol Grounding Problem,0.942299,"SATNet is an award-winning MAXSAT solver that can be used to infer logical
rules and integrated as a differentiable layer in a deep neural network. It had
been shown to solve Sudoku puzzles visually from examples of puzzle digit
images, and was heralded as an impressive achievement towards the longstanding
AI goal of combining pattern recognition with logical reasoning. In this paper,
we clarify SATNet's capabilities by showing that in the absence of intermediate
labels that identify individual Sudoku digit images with their logical
representations, SATNet completely fails at visual Sudoku (0% test accuracy).
More generally, the failure can be pinpointed to its inability to learn to
assign symbols to perceptual phenomena, also known as the symbol grounding
problem, which has long been thought to be a prerequisite for intelligent
agents to perform real-world logical reasoning. We propose an MNIST based test
as an easy instance of the symbol grounding problem that can serve as a sanity
check for differentiable symbolic solvers in general. Naive applications of
SATNet on this test lead to performance worse than that of models without
logical reasoning capabilities. We report on the causes of SATNet's failure and
how to prevent them.",None,-1
735ac9d8-abe7-4765-9f69-3f9ee6f020d2,"Language Models Hallucinate, but May Excel at Fact Verification",0.35762,"Recent progress in natural language processing (NLP) owes much to remarkable
advances in large language models (LLMs). Nevertheless, LLMs frequently
""hallucinate,"" resulting in non-factual outputs. Our carefully-designed human
evaluation substantiates the serious hallucination issue, revealing that even
GPT-3.5 produces factual outputs less than 25% of the time. This underscores
the importance of fact verifiers in order to measure and incentivize progress.
Our systematic investigation affirms that LLMs can be repurposed as effective
fact verifiers with strong correlations with human judgments. Surprisingly,
FLAN-T5-11B, the least factual generator in our study, performs the best as a
fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.
Delving deeper, we analyze the reliance of these LLMs on high-quality evidence,
as well as their deficiencies in robustness and generalization ability. Our
study presents insights for developing trustworthy generation models.",None,-1
4a3ad2f0-feb1-40d0-9a1c-da86c32daef1,Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks,0.703159,"In this study, we investigated whether self-supervised pretraining could
produce a neural network feature extractor applicable to multiple
classification tasks in B-mode lung ultrasound analysis. When fine-tuning on
three lung ultrasound tasks, pretrained models resulted in an improvement of
the average across-task area under the receiver operating curve (AUC) by 0.032
and 0.061 on local and external test sets respectively. Compact nonlinear
classifiers trained on features outputted by a single pretrained model did not
improve performance across all tasks; however, they did reduce inference time
by 49% compared to serial execution of separate fine-tuned models. When
training using 1% of the available labels, pretrained models consistently
outperformed fully supervised models, with a maximum observed test AUC increase
of 0.396 for the task of view classification. Overall, the results indicate
that self-supervised pretraining is useful for producing initial weights for
lung ultrasound classifiers.",None,-1
ba930d01-bf5a-4caf-bb8e-5d19fd9f8e8d,Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion,0.798131,"Diffusion models have shown superior performance in image generation and
manipulation, but the inherent stochasticity presents challenges in preserving
and manipulating image content and identity. While previous approaches like
DreamBooth and Textual Inversion have proposed model or latent representation
personalization to maintain the content, their reliance on multiple reference
images and complex training limits their practicality. In this paper, we
present a simple yet highly effective approach to personalization using highly
personalized (HiPer) text embedding by decomposing the CLIP embedding space for
personalization and content manipulation. Our method does not require model
fine-tuning or identifiers, yet still enables manipulation of background,
texture, and motion with just a single image and target text. Through
experiments on diverse target texts, we demonstrate that our approach produces
highly personalized and complex semantic image edits across a wide range of
tasks. We believe that the novel understanding of the text embedding space
presented in this work has the potential to inspire further research across
various tasks.",None,-1
af15cfdb-4aad-4699-b87f-7e6631675b38,MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies,0.133785,"Autoregressive language models are trained by minimizing the cross-entropy of
the model distribution Q relative to the data distribution P -- that is,
minimizing the forward cross-entropy, which is equivalent to maximum likelihood
estimation (MLE). We have observed that models trained in this way may
""over-generalize"", in the sense that they produce non-human-like text.
Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P
relative to Q, is a better reflection of how a human would evaluate text
generated by a model. Hence, we propose learning with MixCE, an objective that
mixes the forward and reverse cross-entropies. We evaluate models trained with
this objective on synthetic data settings (where P is known) and real data, and
show that the resulting models yield better generated text without complex
decoding strategies. Our code and models are publicly available at
https://github.com/bloomberg/mixce-acl2023",None,-1
5c68fe2a-b315-4234-9e8f-d6019555a45f,An Error-Guided Correction Model for Chinese Spelling Error Correction,0.837056,"Although existing neural network approaches have achieved great success on
Chinese spelling correction, there is still room to improve. The model is
required to avoid over-correction and to distinguish a correct token from its
phonological and visually similar ones. In this paper, we propose an
error-guided correction model (EGCM) to improve Chinese spelling correction. By
borrowing the powerful ability of BERT, we propose a novel zero-shot error
detection method to do a preliminary detection, which guides our model to
attend more on the probably wrong tokens in encoding and to avoid modifying the
correct tokens in generating. Furthermore, we introduce a new loss function to
integrate the error confusion set, which enables our model to distinguish
easily misused tokens. Moreover, our model supports highly parallel decoding to
meet real application requirements. Experiments are conducted on widely used
benchmarks. Our model achieves superior performance against state-of-the-art
approaches by a remarkable margin, on both the correction quality and
computation speed.",None,-1
c0fda8e9-4c2b-4b56-9506-1b08e17089d8,Randomized Adversarial Style Perturbations for Domain Generalization,0.0796691,"We propose a novel domain generalization technique, referred to as Randomized
Adversarial Style Perturbation (RASP), which is motivated by the observation
that the characteristics of each domain are captured by the feature statistics
corresponding to style. The proposed algorithm perturbs the style of a feature
in an adversarial direction towards a randomly selected class, and makes the
model learn against being misled by the unexpected styles observed in unseen
target domains. While RASP is effective to handle domain shifts, its naive
integration into the training procedure might degrade the capability of
learning knowledge from source domains because it has no restriction on the
perturbations of representations. This challenge is alleviated by Normalized
Feature Mixup (NFM), which facilitates the learning of the original features
while achieving robustness to perturbed representations via their mixup during
training. We evaluate the proposed algorithm via extensive experiments on
various benchmarks and show that our approach improves domain generalization
performance, especially in large-scale benchmarks.",None,-1
3306f17b-daf9-404b-a176-6d1ad67458ca,InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling,0.898925,"Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.",None,-1
550f95ed-ae9e-487f-9f6f-1eb031838c72,Egocentric Auditory Attention Localization in Conversations,0.801655,"In a noisy conversation environment such as a dinner party, people often
exhibit selective auditory attention, or the ability to focus on a particular
speaker while tuning out others. Recognizing who somebody is listening to in a
conversation is essential for developing technologies that can understand
social behavior and devices that can augment human hearing by amplifying
particular sound sources. The computer vision and audio research communities
have made great strides towards recognizing sound sources and speakers in
scenes. In this work, we take a step further by focusing on the problem of
localizing auditory attention targets in egocentric video, or detecting who in
a camera wearer's field of view they are listening to. To tackle the new and
challenging Selective Auditory Attention Localization problem, we propose an
end-to-end deep learning approach that uses egocentric video and multichannel
audio to predict the heatmap of the camera wearer's auditory attention. Our
approach leverages spatiotemporal audiovisual features and holistic reasoning
about the scene to make predictions, and outperforms a set of baselines on a
challenging multi-speaker conversation dataset. Project page:
https://fkryan.github.io/saal",None,-1
17ff3fad-216d-413a-9278-446ac2374d58,Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation,0.127334,"We study the effect of tokenization on gender bias in machine translation, an
aspect that has been largely overlooked in previous works. Specifically, we
focus on the interactions between the frequency of gendered profession names in
training data, their representation in the subword tokenizer's vocabulary, and
gender bias. We observe that female and non-stereotypical gender inflections of
profession names (e.g., Spanish ""doctora"" for ""female doctor"") tend to be split
into multiple subword tokens. Our results indicate that the imbalance of gender
forms in the model's training corpus is a major factor contributing to gender
bias and has a greater impact than subword splitting. We show that analyzing
subword splits provides good estimates of gender-form imbalance in the training
data and can be used even when the corpus is not publicly available. We also
demonstrate that fine-tuning just the token embedding layer can decrease the
gap in gender prediction accuracy between female and male forms without
impairing the translation quality.",None,-1
0670c22e-ea71-4fd8-973a-0234e9431a26,Self-Supervised Adversarial Imitation Learning,0.0396426,"Behavioural cloning is an imitation learning technique that teaches an agent
how to behave via expert demonstrations. Recent approaches use self-supervision
of fully-observable unlabelled snapshots of the states to decode state pairs
into actions. However, the iterative learning scheme employed by these
techniques is prone to get trapped into bad local minima. Previous work uses
goal-aware strategies to solve this issue. However, this requires manual
intervention to verify whether an agent has reached its goal. We address this
limitation by incorporating a discriminator into the original framework,
offering two key advantages and directly solving a learning problem previous
work had. First, it disposes of the manual intervention requirement. Second, it
helps in learning by guiding function approximation based on the state
transition of the expert's trajectories. Third, the discriminator solves a
learning issue commonly present in the policy model, which is to sometimes
perform a `no action' within the environment until the agent finally halts.",None,-1
fdd500b4-f799-425e-ab0e-4decf9f5edad,Hate Speech Targets Detection in Parler using BERT,0.337135,"Online social networks have become a fundamental component of our everyday
life. Unfortunately, these platforms are also a stage for hate speech. Popular
social networks have regularized rules against hate speech. Consequently,
social networks like Parler and Gab advocating and claiming to be free speech
platforms have evolved. These platforms have become a district for hate speech
against diverse targets. We present in our paper a pipeline for detecting hate
speech and its targets and use it for creating Parler hate targets'
distribution. The pipeline consists of two models; one for hate speech
detection and the second for target classification, both based on BERT with
Back-Translation and data pre-processing for improved results. The source code
used in this work, as well as other relevant sources, are available at:
https://github.com/NadavSc/HateRecognition.git",None,-1
1f57e986-d86f-4f6e-a249-a5297a8c45ec,PharmacyGPT: The AI Pharmacist,0.51322,"In this study, we introduce PharmacyGPT, a novel framework to assess the
capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in
emulating the role of clinical pharmacists. Our methodology encompasses the
utilization of LLMs to generate comprehensible patient clusters, formulate
medication plans, and forecast patient outcomes. We conduct our investigation
using real data acquired from the intensive care unit (ICU) at the University
of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable
insights into the potential applications and limitations of LLMs in the field
of clinical pharmacy, with implications for both patient care and the
development of future AI-driven healthcare solutions. By evaluating the
performance of PharmacyGPT, we aim to contribute to the ongoing discourse
surrounding the integration of artificial intelligence in healthcare settings,
ultimately promoting the responsible and efficacious use of such technologies.",None,-1
3ee93835-ecdf-4105-8e08-c69373b03e69,ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation,0.766259,"Recent advancements in dense out-of-distribution (OOD) detection have
primarily focused on scenarios where the training and testing datasets share a
similar domain, with the assumption that no domain shift exists between them.
However, in real-world situations, domain shift often exits and significantly
affects the accuracy of existing out-of-distribution (OOD) detection models. In
this work, we propose a dual-level OOD detection framework to handle domain
shift and semantic shift jointly. The first level distinguishes whether domain
shift exists in the image by leveraging global low-level features, while the
second level identifies pixels with semantic shift by utilizing dense
high-level feature maps. In this way, we can selectively adapt the model to
unseen domains as well as enhance model's capacity in detecting novel classes.
We validate the efficacy of our proposed method on several OOD segmentation
benchmarks, including those with significant domain shifts and those without,
observing consistent performance improvements across various baseline models.
Code is available at
${\href{https://github.com/gaozhitong/ATTA}{https://github.com/gaozhitong/ATTA}}$.",None,-1
dd49e829-4c07-4cfe-8d9e-f7e66f26d063,Dual-Alignment Pre-training for Cross-lingual Sentence Embedding,0.808211,"Recent studies have shown that dual encoder models trained with the
sentence-level translation ranking task are effective methods for cross-lingual
sentence embedding. However, our research indicates that token-level alignment
is also crucial in multilingual scenarios, which has not been fully explored
previously. Based on our findings, we propose a dual-alignment pre-training
(DAP) framework for cross-lingual sentence embedding that incorporates both
sentence-level and token-level alignment. To achieve this, we introduce a novel
representation translation learning (RTL) task, where the model learns to use
one-side contextualized token representation to reconstruct its translation
counterpart. This reconstruction objective encourages the model to embed
translation information into the token representation. Compared to other
token-level alignment methods such as translation language modeling, RTL is
more suitable for dual encoder architectures and is computationally efficient.
Extensive experiments on three sentence-level cross-lingual benchmarks
demonstrate that our approach can significantly improve sentence embedding. Our
code is available at https://github.com/ChillingDream/DAP.",None,-1
8d746740-353a-4a16-ba24-31fe3aec815d,Enhancing Video Super-Resolution via Implicit Resampling-based Alignment,0.0789836,"In video super-resolution, it is common to use a frame-wise alignment to
support the propagation of information over time. The role of alignment is
well-studied for low-level enhancement in video, but existing works overlook a
critical step -- resampling. We show through extensive experiments that for
alignment to be effective, the resampling should preserve the reference
frequency spectrum while minimizing spatial distortions. However, most existing
works simply use a default choice of bilinear interpolation for resampling even
though bilinear interpolation has a smoothing effect and hinders
super-resolution. From these observations, we propose an implicit
resampling-based alignment. The sampling positions are encoded by a sinusoidal
positional encoding, while the value is estimated with a coordinate network and
a window-based cross-attention. We show that bilinear interpolation inherently
attenuates high-frequency information while an MLP-based coordinate network can
approximate more frequencies. Experiments on synthetic and real-world datasets
show that alignment with our proposed implicit resampling enhances the
performance of state-of-the-art frameworks with minimal impact on both compute
and parameters.",None,-1
5b9b754c-4ec1-4e97-bb9a-a62f12603a34,LineFormer: Rethinking Line Chart Data Extraction as Instance Segmentation,0.121466,"Data extraction from line-chart images is an essential component of the
automated document understanding process, as line charts are a ubiquitous data
visualization format. However, the amount of visual and structural variations
in multi-line graphs makes them particularly challenging for automated parsing.
Existing works, however, are not robust to all these variations, either taking
an all-chart unified approach or relying on auxiliary information such as
legends for line data extraction. In this work, we propose LineFormer, a robust
approach to line data extraction using instance segmentation. We achieve
state-of-the-art performance on several benchmark synthetic and real chart
datasets. Our implementation is available at
https://github.com/TheJaeLal/LineFormer .",None,-1
ba85aa52-d930-4f71-ac07-493d1bcb36d4,BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models,0.123298,"Retrieval augmentation addresses many critical problems in large language
models such as hallucination, staleness, and privacy leaks. However, running
retrieval-augmented language models (LMs) is slow and difficult to scale due to
processing large amounts of retrieved text. We introduce binary token
representations (BTR), which use 1-bit vectors to precompute every token in
passages, significantly reducing computation during inference. Despite the
potential loss of accuracy, our new calibration techniques and training
objectives restore performance. Combined with offline and runtime compression,
this only requires 127GB of disk space for encoding 3 billion tokens in
Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR
accelerates state-of-the-art inference by up to 4x and reduces storage by over
100x while maintaining over 95% task performance.",None,-1
0f5e19d2-3536-4ab0-9614-71f480f17ca1,Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction,0.99957,"Legal syllogism is a form of deductive reasoning commonly used by legal
professionals to analyze cases. In this paper, we propose legal syllogism
prompting (LoT), a simple prompting method to teach large language models
(LLMs) for legal judgment prediction. LoT teaches only that in the legal
syllogism the major premise is law, the minor premise is the fact, and the
conclusion is judgment. Then the models can produce a syllogism reasoning of
the case and give the judgment without any learning, fine-tuning, or examples.
On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment
prediction experiments with GPT-3 models. Our results show that LLMs with LoT
achieve better performance than the baseline and chain of thought prompting,
the state-of-art prompting method on diverse reasoning tasks. LoT enables the
model to concentrate on the key information relevant to the judgment and to
correctly understand the legal meaning of acts, as compared to other methods.
Our method enables LLMs to predict judgment along with law articles and
justification, which significantly enhances the explainability of models.",None,-1
c5063379-9489-4201-a395-e626940b2d34,Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning,0.341735,"Existing Knowledge Base Question Answering (KBQA) architectures are hungry
for annotated data, which make them costly and time-consuming to deploy. We
introduce the problem of few-shot transfer learning for KBQA, where the target
domain offers only a few labeled examples, but a large labeled training dataset
is available in a source domain. We propose a novel KBQA architecture called
FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,
re-ranks using an LLM and uses this as input for LLM few-shot in-context
learning to generate logical forms. These are further refined using
execution-guided feedback. Experiments over multiple source-target KBQA pairs
of varying complexity show that FuSIC-KBQA significantly outperforms
adaptations of SoTA KBQA models for this setting. Additional experiments show
that FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when
training data is limited.",None,-1
187eaf48-8361-4dc1-ad5f-65215ab24814,DAVA: Disentangling Adversarial Variational Autoencoder,0.428561,"The use of well-disentangled representations offers many advantages for
downstream tasks, e.g. an increased sample efficiency, or better
interpretability. However, the quality of disentangled interpretations is often
highly dependent on the choice of dataset-specific hyperparameters, in
particular the regularization strength. To address this issue, we introduce
DAVA, a novel training procedure for variational auto-encoders. DAVA completely
alleviates the problem of hyperparameter selection. We compare DAVA to models
with optimal hyperparameters. Without any hyperparameter tuning, DAVA is
competitive on a diverse range of commonly used datasets. Underlying DAVA, we
discover a necessary condition for unsupervised disentanglement, which we call
PIPE. We demonstrate the ability of PIPE to positively predict the performance
of downstream models in abstract reasoning. We also thoroughly investigate
correlations with existing supervised and unsupervised metrics. The code is
available at https://github.com/besterma/dava.",None,-1
5221601a-294c-405e-b423-c8c3ba94cfb5,Automating question generation from educational text,0.0528862,"The use of question-based activities (QBAs) is wide-spread in education,
traditionally forming an integral part of the learning and assessment process.
In this paper, we design and evaluate an automated question generation tool for
formative and summative assessment in schools. We present an expert survey of
one hundred and four teachers, demonstrating the need for automated generation
of QBAs, as a tool that can significantly reduce the workload of teachers and
facilitate personalized learning experiences. Leveraging the recent
advancements in generative AI, we then present a modular framework employing
transformer based language models for automatic generation of multiple-choice
questions (MCQs) from textual content. The presented solution, with distinct
modules for question generation, correct answer prediction, and distractor
formulation, enables us to evaluate different language models and generation
techniques. Finally, we perform an extensive quantitative and qualitative
evaluation, demonstrating trade-offs in the use of different techniques and
models.",None,-1
61ac6b42-eabe-41ca-8c0e-1d85319e4098,StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding,0.999778,"Analogy-making between narratives is crucial for human reasoning. In this
paper, we evaluate the ability to identify and generate analogies by
constructing a first-of-its-kind large-scale story-level analogy corpus,
\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with
human annotations on two similarities from the extended Structure-Mapping
Theory. We design a set of tests on \textsc{StoryAnalogy}, presenting the first
evaluation of story-level analogy identification and generation. Interestingly,
we find that the analogy identification tasks are incredibly difficult not only
for sentence embedding models but also for the recent large language models
(LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around
30% accuracy in multiple-choice questions (compared to over 85% accuracy for
humans). Furthermore, we observe that the data in \textsc{StoryAnalogy} can
improve the quality of analogy generation in LLMs, where a fine-tuned
FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",None,-1
228997c4-cc0c-4451-bc85-29858d8473b2,Hyper-Decision Transformer for Efficient Online Policy Adaptation,0.965957,"Decision Transformers (DT) have demonstrated strong performances in offline
reinforcement learning settings, but quickly adapting to unseen novel tasks
remains challenging. To address this challenge, we propose a new framework,
called Hyper-Decision Transformer (HDT), that can generalize to novel tasks
from a handful of demonstrations in a data- and parameter-efficient manner. To
achieve such a goal, we propose to augment the base DT with an adaptation
module, whose parameters are initialized by a hyper-network. When encountering
unseen tasks, the hyper-network takes a handful of demonstrations as inputs and
initializes the adaptation module accordingly. This initialization enables HDT
to efficiently adapt to novel tasks by only fine-tuning the adaptation module.
We validate HDT's generalization capability on object manipulation tasks. We
find that with a single expert demonstration and fine-tuning only 0.5% of DT
parameters, HDT adapts faster to unseen tasks than fine-tuning the whole DT
model. Finally, we explore a more challenging setting where expert actions are
not available, and we show that HDT outperforms state-of-the-art baselines in
terms of task success rates by a large margin.",None,-1
96948dc8-e3bf-4bc7-aa73-4e8340370d2c,OMNI: Open-endedness via Models of human Notions of Interestingness,0.426649,"Open-ended algorithms aim to learn new, interesting behaviors forever. That
requires a vast environment search space, but there are thus infinitely many
possible tasks. Even after filtering for tasks the current agent can learn
(i.e., learning progress), countless learnable yet uninteresting tasks remain
(e.g., minor variations of previously learned tasks). An Achilles Heel of
open-endedness research is the inability to quantify (and thus prioritize)
tasks that are not just learnable, but also $\textit{interesting}$ (e.g.,
worthwhile and novel). We propose solving this problem by
$\textit{Open-endedness via Models of human Notions of Interestingness}$
(OMNI). The insight is that we can utilize foundation models (FMs) as a model
of interestingness (MoI), because they $\textit{already}$ internalize human
concepts of interestingness from training on vast amounts of human-generated
data, where humans naturally write about what they find interesting or boring.
We show that FM-based MoIs improve open-ended learning by focusing on tasks
that are both learnable $\textit{and interesting}$, outperforming baselines
based on uniform task sampling or learning progress alone. This approach has
the potential to dramatically advance the ability to intelligently select which
tasks to focus on next (i.e., auto-curricula), and could be seen as AI
selecting its own next task to learn, facilitating self-improving AI and
AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/",None,-1
59f16204-9db2-40ea-84c3-0e9d6fc36b40,Depth Super-Resolution from Explicit and Implicit High-Frequency Features,0.291027,"We propose a novel multi-stage depth super-resolution network, which
progressively reconstructs high-resolution depth maps from explicit and
implicit high-frequency features. The former are extracted by an efficient
transformer processing both local and global contexts, while the latter are
obtained by projecting color images into the frequency domain. Both are
combined together with depth features by means of a fusion strategy within a
multi-stage and multi-scale framework. Experiments on the main benchmarks, such
as NYUv2, Middlebury, DIML and RGBDD, show that our approach outperforms
existing methods by a large margin (~20% on NYUv2 and DIML against the
contemporary work DADA, with 16x upsampling), establishing a new
state-of-the-art in the guided depth super-resolution task.",None,-1
d0cb9a01-b0ea-440b-94ae-91127fb489bc,Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?,0.526926,"The rapid advancement of Large Language Models (LLMs) has spurred discussions
about their potential to enhance quantitative trading strategies. LLMs excel in
analyzing sentiments about listed companies from financial news, providing
critical insights for trading decisions. However, the performance of LLMs in
this task varies substantially due to their inherent characteristics. This
paper introduces a standardized experimental procedure for comprehensive
evaluations. We detail the methodology using three distinct LLMs, each
embodying a unique approach to performance enhancement, applied specifically to
the task of sentiment factor extraction from large volumes of Chinese news
summaries. Subsequently, we develop quantitative trading strategies using these
sentiment factors and conduct back-tests in realistic scenarios. Our results
will offer perspectives about the performances of Large Language Models applied
to extracting sentiments from Chinese news texts.",None,-1
092edb3b-3bd5-4b0c-a7ea-4b2d35484d54,Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction,0.74759,"Recently, aspect sentiment quad prediction has received widespread attention
in the field of aspect-based sentiment analysis. Existing studies extract
quadruplets via pre-trained generative language models to paraphrase the
original sentence into a templated target sequence. However, previous works
only focus on what to generate but ignore what not to generate. We argue that
considering the negative samples also leads to potential benefits. In this
work, we propose a template-agnostic method to control the token-level
generation, which boosts original learning and reduces mistakes simultaneously.
Specifically, we introduce Monte Carlo dropout to understand the built-in
uncertainty of pre-trained language models, acquiring the noises and errors. We
further propose marginalized unlikelihood learning to suppress the
uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to
balance the effects of marginalized unlikelihood learning. Extensive
experiments on four public datasets demonstrate the effectiveness of our
approach on various generation templates.",None,-1
02faa2ac-1cce-4de5-9787-0f1dcfa6a672,A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack,0.188492,"Rutting of asphalt pavements is a crucial design criterion in various
pavement design guides. A good road transportation base can provide security
for the transportation of oil and gas in road transportation. This study
attempts to develop a robust artificial intelligence model to estimate
different asphalt pavements' rutting depth clips, temperature, and load axes as
primary characteristics. The experiment data were obtained from 19 asphalt
pavements with different crude oil sources on a 2.038 km long full-scale field
accelerated pavement test track (RIOHTrack, Road Track Institute) in Tongzhou,
Beijing. In addition, this paper also proposes to build complex networks with
different pavement rutting depths through complex network methods and the
Louvain algorithm for community detection. The most critical structural
elements can be selected from different asphalt pavement rutting data, and
similar structural elements can be found. An extreme learning machine algorithm
with residual correction (RELM) is designed and optimized using an independent
adaptive particle swarm algorithm. The experimental results of the proposed
method are compared with several classical machine learning algorithms, with
predictions of Average Root Mean Squared Error, Average Mean Absolute Error,
and Average Mean Absolute Percentage Error for 19 asphalt pavements reaching
1.742, 1.363, and 1.94\% respectively. The experiments demonstrate that the
RELM algorithm has an advantage over classical machine learning methods in
dealing with non-linear problems in road engineering. Notably, the method
ensures the adaptation of the simulated environment to different levels of
abstraction through the cognitive analysis of the production environment
parameters.",None,-1
194ec922-94c0-4820-9a8a-4ee41c068208,Search for universal minimum drag resistance underwater vehicle hull using CFD,0.711708,"In Autonomous Underwater Vehicles (AUVs) design, hull resistance is an
important factor in determining the power requirements and range of vehicle and
consequently affect battery size, weight, and volume requirement of the design.
In this paper, we leverage on AI-based optimization algorithm along with
Computational Fluid Dynamics (CFD) simulation to study the optimal hull design
that minimizing the resistance. By running the CFD-based optimization at
different operating velocities and turbulence intensity, we want to
study/search the possibility of a universal design that will provide least
resistance/near-optimal design across all operating conditions (operating
velocity) and environmental conditions (turbulence intensity). Early result
demonstrated that the optimal design found at low velocity and low turbulence
condition performs very poor at high velocity and high turbulence conditions.
However, a design that is optimal at high velocity and high turbulence
conditions performs near-optimal across many considered velocity and turbulence
conditions.",None,-1
46c6d33c-16df-4cfc-981f-8ac82728dc90,Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering,0.0341415,"Event-Level Video Question Answering (EVQA) requires complex reasoning across
video events to obtain the visual information needed to provide optimal
answers. However, despite significant progress in model performance, few
studies have focused on using the explicit semantic connections between the
question and visual information especially at the event level. There is need
for using such semantic connections to facilitate complex reasoning across
video frames. Therefore, we propose a semantic-aware dynamic
retrospective-prospective reasoning approach for video-based question
answering. Specifically, we explicitly use the Semantic Role Labeling (SRL)
structure of the question in the dynamic reasoning process where we decide to
move to the next frame based on which part of the SRL structure (agent, verb,
patient, etc.) of the question is being focused on. We conduct experiments on a
benchmark EVQA dataset - TrafficQA. Results show that our proposed approach
achieves superior performance compared to previous state-of-the-art models. Our
code will be made publicly available for research use.",None,-1
79cad5f5-e677-4fa0-8672-d01d44ed977a,Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation,0.388172,"Gender bias is a significant issue in machine translation, leading to ongoing
research efforts in developing bias mitigation techniques. However, most works
focus on debiasing bilingual models without much consideration for multilingual
systems. In this paper, we specifically target the gender bias issue of
multilingual machine translation models for unambiguous cases where there is a
single correct translation, and propose a bias mitigation method based on a
novel approach. Specifically, we propose Gender-Aware Contrastive Learning,
GACL, which encodes contextual gender information into the representations of
non-explicit gender words. Our method is target language-agnostic and is
applicable to pre-trained multilingual machine translation models via
fine-tuning. Through multilingual evaluation, we show that our approach
improves gender accuracy by a wide margin without hampering translation
performance. We also observe that incorporated gender information transfers and
benefits other target languages regarding gender accuracy. Finally, we
demonstrate that our method is applicable and beneficial to models of various
sizes.",None,-1
764dfebd-5444-4de3-b78d-ef8c613170c2,Deep learning model for Mongolian Citizens Feedback Analysis using Word Vector Embeddings,0.103444,"A large amount of feedback was collected over the years. Many feedback
analysis models have been developed focusing on the English language.
Recognizing the concept of feedback is challenging and crucial in languages
which do not have applicable corpus and tools employed in Natural Language
Processing (i.e., vocabulary corpus, sentence structure rules, etc). However,
in this paper, we study a feedback classification in Mongolian language using
two different word embeddings for deep learning. We compare the results of
proposed approaches. We use feedback data in Cyrillic collected from 2012-2018.
The result indicates that word embeddings using their own dataset improve the
deep learning based proposed model with the best accuracy of 80.1% and 82.7%
for two classification tasks.",None,-1
2c3d2d5b-ab5f-4e9d-a0d5-66bfeb87b9c0,Aligned: A Platform-based Process for Alignment,0.325968,"We are introducing Aligned, a platform for global governance and alignment of
frontier models, and eventually superintelligence. While previous efforts at
the major AI labs have attempted to gather inputs for alignment, these are
often conducted behind closed doors. We aim to set the foundation for a more
trustworthy, public-facing approach to safety: a constitutional committee
framework. Initial tests with 680 participants result in a 30-guideline
constitution with 93% overall support. We show the platform naturally scales,
instilling confidence and enjoyment from the community. We invite other AI labs
and teams to plug and play into the Aligned ecosystem.",None,-1
2af95135-4570-4e87-a4da-63195519f53a,CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion,0.825314,"This paper proposes a novel diffusion-based model, CompoDiff, for solving
zero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper
also introduces a new synthetic dataset, named SynthTriplets18M, with 18.8
million reference images, conditions, and corresponding target image triplets
to train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the
previous CIR approaches, such as poor generalizability due to the small dataset
scale and the limited types of conditions. CompoDiff not only achieves a new
state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,
and GeneCIS, but also enables a more versatile and controllable CIR by
accepting various conditions, such as negative text, and image mask conditions.
CompoDiff also shows the controllability of the condition strength between text
and image queries and the trade-off between inference speed and performance,
which are unavailable with existing CIR methods. The code and dataset are
available at https://github.com/navervision/CompoDiff",None,-1
a65ab528-f6a6-4fa4-9dc8-82e8a727d597,NMS Threshold matters for Ego4D Moment Queries -- 2nd place solution to the Ego4D Moment Queries Challenge 2023,0.18013,"This report describes our submission to the Ego4D Moment Queries Challenge
2023. Our submission extends ActionFormer, a latest method for temporal action
localization. Our extension combines an improved ground-truth assignment
strategy during training and a refined version of SoftNMS at inference time.
Our solution is ranked 2nd on the public leaderboard with 26.62% average mAP
and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming
the strong baseline from 2023 challenge. Our code is available at
https://github.com/happyharrycn/actionformer_release.",None,-1
1981aa55-6454-4810-aff0-075c0b442bb2,Decidable Fragments of LTLf Modulo Theories (Extended Version),0.819908,"We study Linear Temporal Logic Modulo Theories over Finite Traces (LTLfMT), a
recently introduced extension of LTL over finite traces (LTLf) where
propositions are replaced by first-order formulas and where first-order
variables referring to different time points can be compared. In general,
LTLfMT was shown to be semi-decidable for any decidable first-order theory
(e.g., linear arithmetics), with a tableau-based semi-decision procedure.
  In this paper we present a sound and complete pruning rule for the LTLfMT
tableau. We show that for any LTLfMT formula that satisfies an abstract,
semantic condition, that we call finite memory, the tableau augmented with the
new rule is also guaranteed to terminate. Last but not least, this technique
allows us to establish novel decidability results for the satisfiability of
several fragments of LTLfMT, as well as to give new decidability proofs for
classes that are already known.",None,-1
976fdf72-6e70-4e0d-9912-9ae980934c75,IMF: Interactive Multimodal Fusion Model for Link Prediction,0.8842,"Link prediction aims to identify potential missing triples in knowledge
graphs. To get better results, some recent studies have introduced multimodal
information to link prediction. However, these methods utilize multimodal
information separately and neglect the complicated interaction between
different modalities. In this paper, we aim at better modeling the
inter-modality information and thus introduce a novel Interactive Multimodal
Fusion (IMF) model to integrate knowledge from different modalities. To this
end, we propose a two-stage multimodal fusion framework to preserve
modality-specific knowledge as well as take advantage of the complementarity
between different modalities. Instead of directly projecting different
modalities into a unified space, our multimodal fusion module limits the
representations of different modalities independent while leverages bilinear
pooling for fusion and incorporates contrastive learning as additional
constraints. Furthermore, the decision fusion module delivers the learned
weighted average over the predictions of all modalities to better incorporate
the complementarity of different modalities. Our approach has been demonstrated
to be effective through empirical evaluations on several real-world datasets.
The implementation code is available online at
https://github.com/HestiaSky/IMF-Pytorch.",None,-1
6d7217ff-0730-47ef-bca5-63c108d631c2,FrameFinder: Explorative Multi-Perspective Framing Extraction from News Headlines,0.148889,"Revealing the framing of news articles is an important yet neglected task in
information seeking and retrieval. In the present work, we present FrameFinder,
an open tool for extracting and analyzing frames in textual data. FrameFinder
visually represents the frames of text from three perspectives, i.e., (i) frame
labels, (ii) frame dimensions, and (iii) frame structure. By analyzing the
well-established gun violence frame corpus, we demonstrate the merits of our
proposed solution to support social science research and call for subsequent
integration into information interactions.",None,-1
79a669b4-dafb-42b1-84f0-6651793a2c04,Red Teaming Language Model Detectors with Language Models,0.700509,"The prevalence and strong capability of large language models (LLMs) present
significant safety and ethical risks if exploited by malicious users. To
prevent the potentially deceptive usage of LLMs, recent works have proposed
algorithms to detect LLM-generated text and protect LLMs. In this paper, we
investigate the robustness and reliability of these LLM detectors under
adversarial attacks. We study two types of attack strategies: 1) replacing
certain words in an LLM's output with their synonyms given the context; 2)
automatically searching for an instructional prompt to alter the writing style
of the generation. In both strategies, we leverage an auxiliary LLM to generate
the word replacements or the instructional prompt. Different from previous
works, we consider a challenging setting where the auxiliary LLM can also be
protected by a detector. Experiments reveal that our attacks effectively
compromise the performance of all detectors in the study with plausible
generations, underscoring the urgent need to improve the robustness of
LLM-generated text detection systems.",None,-1
887cc5e1-b1e4-497c-a485-a1f5a68a3163,Customising General Large Language Models for Specialised Emotion Recognition Tasks,0.526573,"The advent of large language models (LLMs) has gained tremendous attention
over the past year. Previous studies have shown the astonishing performance of
LLMs not only in other tasks but also in emotion recognition in terms of
accuracy, universality, explanation, robustness, few/zero-shot learning, and
others. Leveraging the capability of LLMs inevitably becomes an essential
solution for emotion recognition. To this end, we further comprehensively
investigate how LLMs perform in linguistic emotion recognition if we
concentrate on this specific task. Specifically, we exemplify a publicly
available and widely used LLM -- Chat General Language Model, and customise it
for our target by using two different modal adaptation techniques, i.e., deep
prompt tuning and low-rank adaptation. The experimental results obtained on six
widely used datasets present that the adapted LLM can easily outperform other
state-of-the-art but specialised deep models. This indicates the strong
transferability and feasibility of LLMs in the field of emotion recognition.",None,-1
f2f9aa8b-9bf2-42fa-be5b-0facff42f9e9,A New Class of Explanations for Classifiers with Non-Binary Features,0.423838,"Two types of explanations have been receiving increased attention in the
literature when analyzing the decisions made by classifiers. The first type
explains why a decision was made and is known as a sufficient reason for the
decision, also an abductive explanation or a PI-explanation. The second type
explains why some other decision was not made and is known as a necessary
reason for the decision, also a contrastive or counterfactual explanation.
These explanations were defined for classifiers with binary, discrete and, in
some cases, continuous features. We show that these explanations can be
significantly improved in the presence of non-binary features, leading to a new
class of explanations that relay more information about decisions and the
underlying classifiers. Necessary and sufficient reasons were also shown to be
the prime implicates and implicants of the complete reason for a decision,
which can be obtained using a quantification operator. We show that our
improved notions of necessary and sufficient reasons are also prime implicates
and implicants but for an improved notion of complete reason obtained by a new
quantification operator that we also define and study.",None,-1
7b6bfb2f-be0a-4590-b58d-990712929c73,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,0.926625,"The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection",None,-1
308e64f8-1231-41c4-afa8-d5094edae33c,T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,0.628843,"Cross-lingual text classification leverages text classifiers trained in a
high-resource language to perform text classification in other languages with
no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,
cross-lingual text classifiers are typically built on large-scale, multilingual
language models (LMs) pretrained on a variety of languages of interest.
However, the performance of these models vary significantly across languages
and classification tasks, suggesting that the superposition of the language
modelling and classification tasks is not always effective. For this reason, in
this paper we propose revisiting the classic ""translate-and-test"" pipeline to
neatly separate the translation and classification stages. The proposed
approach couples 1) a neural machine translator translating from the targeted
language to a high-resource language, with 2) a text classifier trained in the
high-resource language, but the neural machine translator generates ""soft""
translations to permit end-to-end backpropagation during fine-tuning of the
pipeline. Extensive experiments have been carried out over three cross-lingual
text classification datasets (XNLI, MLDoc and MultiEURLEX), with the results
showing that the proposed approach has significantly improved performance over
a competitive baseline.",None,-1
279b5d67-3c60-485f-9a9e-db1bf4ef984d,Does Federated Learning Really Need Backpropagation?,0.510565,"Federated learning (FL) is a general principle for decentralized clients to
train a server model collectively without sharing local data. FL is a promising
framework with practical applications, but its standard training paradigm
requires the clients to backpropagate through the model to compute gradients.
Since these clients are typically edge devices and not fully trusted, executing
backpropagation on them incurs computational and storage overhead as well as
white-box vulnerability. In light of this, we develop backpropagation-free
federated learning, dubbed BAFFLE, in which backpropagation is replaced by
multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient
and easily fits uploading bandwidth; 2) compatible with inference-only hardware
optimization and model quantization or pruning; and 3) well-suited to trusted
execution environments, because the clients in BAFFLE only execute forward
propagation and return a set of scalars to the server. Empirically we use
BAFFLE to train deep models from scratch or to finetune pretrained models,
achieving acceptable results. Code is available in
https://github.com/FengHZ/BAFFLE.",None,-1
628284ee-2c7c-4d24-9b28-db800f2aa648,Learning Logic Specifications for Soft Policy Guidance in POMCP,0.851188,"Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for
Partially Observable Markov Decision Processes (POMDPs). It allows scaling to
large state spaces by computing an approximation of the optimal policy locally
and online, using a Monte Carlo Tree Search based strategy. However, POMCP
suffers from sparse reward function, namely, rewards achieved only when the
final goal is reached, particularly in environments with large state spaces and
long horizons. Recently, logic specifications have been integrated into POMCP
to guide exploration and to satisfy safety requirements. However, such
policy-related rules require manual definition by domain experts, especially in
real-world scenarios. In this paper, we use inductive logic programming to
learn logic specifications from traces of POMCP executions, i.e., sets of
belief-action pairs generated by the planner. Specifically, we learn rules
expressed in the paradigm of answer set programming. We then integrate them
inside POMCP to provide soft policy bias toward promising actions. In the
context of two benchmark scenarios, rocksample and battery, we show that the
integration of learned rules from small task instances can improve performance
with fewer Monte Carlo simulations and in larger task instances. We make our
modified version of POMCP publicly available at
https://github.com/GiuMaz/pomcp_clingo.git.",None,-1
42e62ceb-4699-4f55-b061-e482c40a2523,SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings,0.221102,"Adding interpretability to word embeddings represents an area of active
research in text representation. Recent work has explored thepotential of
embedding words via so-called polar dimensions (e.g. good vs. bad, correct vs.
wrong). Examples of such recent approaches include SemAxis, POLAR, FrameAxis,
and BiImp. Although these approaches provide interpretable dimensions for
words, they have not been designed to deal with polysemy, i.e. they can not
easily distinguish between different senses of words. To address this
limitation, we present SensePOLAR, an extension of the original POLAR framework
that enables word-sense aware interpretability for pre-trained contextual word
embeddings. The resulting interpretable word embeddings achieve a level of
performance that is comparable to original contextual word embeddings across a
variety of natural language processing tasks including the GLUE and SQuAD
benchmarks. Our work removes a fundamental limitation of existing approaches by
offering users sense aware interpretations for contextual word embeddings.",None,-1
10159f49-cc3b-4898-9b8d-bcdbba0d8064,Is it indeed bigger better? The comprehensive study of claim detection LMs applied for disinformation tackling,0.107666,"This study compares the performance of (1) fine-tuned models and (2)
extremely large language models on the task of check-worthy claim detection.
For the purpose of the comparison we composed a multilingual and multi-topical
dataset comprising texts of various sources and styles. Building on this, we
performed a benchmark analysis to determine the most general multilingual and
multi-topical claim detector.
  We chose three state-of-the-art models in the check-worthy claim detection
task and fine-tuned them. Furthermore, we selected three state-of-the-art
extremely large language models without any fine-tuning. We made modifications
to the models to adapt them for multilingual settings and through extensive
experimentation and evaluation. We assessed the performance of all the models
in terms of accuracy, recall, and F1-score in in-domain and cross-domain
scenarios. Our results demonstrate that despite the technological progress in
the area of natural language processing, the models fine-tuned for the task of
check-worthy claim detection still outperform the zero-shot approaches in a
cross-domain settings.",None,-1
c7eda735-6f24-42fd-a443-b42a96e6f535,Learning Geometry-aware Representations by Sketching,0.1916,"Understanding geometric concepts, such as distance and shape, is essential
for understanding the real world and also for many vision tasks. To incorporate
such information into a visual representation of a scene, we propose learning
to represent the scene by sketching, inspired by human behavior. Our method,
coined Learning by Sketching (LBS), learns to convert an image into a set of
colored strokes that explicitly incorporate the geometric information of the
scene in a single inference step without requiring a sketch dataset. A sketch
is then generated from the strokes where CLIP-based perceptual loss maintains a
semantic similarity between the sketch and the image. We show theoretically
that sketching is equivariant with respect to arbitrary affine transformations
and thus provably preserves geometric information. Experimental results show
that LBS substantially improves the performance of object attribute
classification on the unlabeled CLEVR dataset, domain transfer between CLEVR
and STL-10 datasets, and for diverse downstream tasks, confirming that LBS
provides rich geometric information.",None,-1
89f6679a-1bb2-4198-9e37-ee7c85de8ad5,MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension,0.347289,"The large language models have achieved superior performance on various
natural language tasks. One major drawback of such approaches is they are
resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a
resource-efficient solution to fine-tune the pre-trained language models (PLMs)
while keeping their weight frozen. Existing soft prompt methods mainly focus on
designing the input-independent prompts that steer the model to fit the domain
of the new dataset. Those methods often ignore the fine-grained information
about the task and context of the text. In this paper, we propose a multi-level
prompt tuning (MPrompt) method for machine reading comprehension. It utilizes
prompts at task-specific, domain-specific, and context-specific levels to
enhance the comprehension of input semantics at different granularities. We
also propose an independence constraint to steer each domain-specific prompt to
focus on information within its domain to avoid redundancy. Moreover, we
present a prompt generator that incorporates context-related knowledge in the
prompt generation to enhance contextual relevancy. We conducted extensive
experiments on 12 benchmarks of various QA formats and achieved an average
improvement of 1.94\% over the state-of-the-art methods.",None,-1
62d82681-b713-4977-8e37-be1060ff149a,WeditGAN: Few-Shot Image Generation via Latent Space Relocation,0.197654,"In few-shot image generation, directly training GAN models on just a handful
of images faces the risk of overfitting. A popular solution is to transfer the
models pretrained on large source domains to small target ones. In this work,
we introduce WeditGAN, which realizes model transfer by editing the
intermediate latent codes $w$ in StyleGANs with learned constant offsets
($\Delta w$), discovering and constructing target latent spaces via simply
relocating the distribution of source latent spaces. The established one-to-one
mapping between latent spaces can naturally prevents mode collapse and
overfitting. Besides, we also propose variants of WeditGAN to further enhance
the relocation process by regularizing the direction or finetuning the
intensity of $\Delta w$. Experiments on a collection of widely used
source/target datasets manifest the capability of WeditGAN in generating
realistic and diverse images, which is simple yet highly effective in the
research area of few-shot image generation. Codes are available at
https://github.com/Ldhlwh/WeditGAN.",None,-1
4ca71203-0ad7-496d-aef1-902a1c768946,GNN-based Passenger Request Prediction,0.777338,"Passenger request prediction is essential for operations planning, control,
and management in ride-sharing platforms. While the demand prediction problem
has been studied extensively, the Origin-Destination (OD) flow prediction of
passengers has received less attention from the research community. This paper
develops a Graph Neural Network framework along with the Attention Mechanism to
predict the OD flow of passengers. The proposed framework exploits various
linear and non-linear dependencies that arise among requests originating from
different locations and captures the repetition pattern and the contextual data
of that place. Moreover, the optimal size of the grid cell that covers the road
network and preserves the complexity and accuracy of the model is determined.
Extensive simulations are conducted to examine the characteristics of our
proposed approach and its various components. The results show the superior
performance of our proposed model compared to the existing baselines.",None,-1
2e281e2d-c737-422d-b48b-a5dc4d1d78ad,ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification,0.781883,"Few-shot text classification has recently been promoted by the meta-learning
paradigm which aims to identify target classes with knowledge transferred from
source classes with sets of small tasks named episodes. Despite their success,
existing works building their meta-learner based on Prototypical Networks are
unsatisfactory in learning discriminative text representations between similar
classes, which may lead to contradictions during label prediction. In addition,
the tasklevel and instance-level overfitting problems in few-shot text
classification caused by a few training examples are not sufficiently tackled.
In this work, we propose a contrastive learning framework named ContrastNet to
tackle both discriminative representation and overfitting problems in few-shot
text classification. ContrastNet learns to pull closer text representations
belonging to the same class and push away text representations belonging to
different classes, while simultaneously introducing unsupervised contrastive
regularization at both task-level and instance-level to prevent overfitting.
Experiments on 8 few-shot text classification datasets show that ContrastNet
outperforms the current state-of-the-art models.",None,-1
0a044a85-34a6-4de9-9094-76fa4f302d3e,Reason to explain: Interactive contrastive explanations (REASONX),0.0995051,"Many high-performing machine learning models are not interpretable. As they
are increasingly used in decision scenarios that can critically affect
individuals, it is necessary to develop tools to better understand their
outputs. Popular explanation methods include contrastive explanations. However,
they suffer several shortcomings, among others an insufficient incorporation of
background knowledge, and a lack of interactivity. While (dialogue-like)
interactivity is important to better communicate an explanation, background
knowledge has the potential to significantly improve their quality, e.g., by
adapting the explanation to the needs of the end-user. To close this gap, we
present REASONX, an explanation tool based on Constraint Logic Programming
(CLP). REASONX provides interactive contrastive explanations that can be
augmented by background knowledge, and allows to operate under a setting of
under-specified information, leading to increased flexibility in the provided
explanations. REASONX computes factual and constrative decision rules, as well
as closest constrative examples. It provides explanations for decision trees,
which can be the ML models under analysis, or global/local surrogate models of
any ML model. While the core part of REASONX is built on CLP, we also provide a
program layer that allows to compute the explanations via Python, making the
tool accessible to a wider audience. We illustrate the capability of REASONX on
a synthetic data set, and on a a well-developed example in the credit domain.
In both cases, we can show how REASONX can be flexibly used and tailored to the
needs of the user.",None,-1
bc1fd4da-f822-4e74-a286-fa158b2e3c47,Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making,0.958815,"In AI-assisted decision-making, it is critical for human decision-makers to
know when to trust AI and when to trust themselves. However, prior studies
calibrated human trust only based on AI confidence indicating AI's correctness
likelihood (CL) but ignored humans' CL, hindering optimal team decision-making.
To mitigate this gap, we proposed to promote humans' appropriate trust based on
the CL of both sides at a task-instance level. We first modeled humans' CL by
approximating their decision-making models and computing their potential
performance in similar instances. We demonstrated the feasibility and
effectiveness of our model via two preliminary studies. Then, we proposed three
CL exploitation strategies to calibrate users' trust explicitly/implicitly in
the AI-assisted decision-making process. Results from a between-subjects
experiment (N=293) showed that our CL exploitation strategies promoted more
appropriate human trust in AI, compared with only using AI confidence. We
further provided practical implications for more human-compatible AI-assisted
decision-making.",None,-1
2430fde4-1a82-4f74-980f-b81db7f82fea,A theory for the sparsity emerged in the Forward Forward algorithm,0.64992,"This report explores the theory that explains the high sparsity phenomenon
\citep{tosato2023emergent} observed in the forward-forward algorithm
\citep{hinton2022forward}. The two theorems proposed predict the sparsity
changes of a single data point's activation in two cases: Theorem
\ref{theorem:1}: Decrease the goodness of the whole batch. Theorem
\ref{theorem:2}: Apply the complete forward forward algorithm to decrease the
goodness for negative data and increase the goodness for positive data. The
theory aligns well with the experiments tested on the MNIST dataset.",None,-1
24628ce6-73a3-49a5-ab20-faa08a4eeddc,Backpropagation of Unrolled Solvers with Folded Optimization,0.61748,"The integration of constrained optimization models as components in deep
networks has led to promising advances on many specialized learning tasks. A
central challenge in this setting is backpropagation through the solution of an
optimization problem, which typically lacks a closed form. One typical strategy
is algorithm unrolling, which relies on automatic differentiation through the
operations of an iterative solver. While flexible and general, unrolling can
encounter accuracy and efficiency issues in practice. These issues can be
avoided by analytical differentiation of the optimization, but current
frameworks impose rigid requirements on the optimization problem's form. This
paper provides theoretical insights into the backward pass of unrolled
optimization, leading to a system for generating efficiently solvable
analytical models of backpropagation. Additionally, it proposes a unifying view
of unrolling and analytical differentiation through optimization mappings.
Experiments over various model-based learning tasks demonstrate the advantages
of the approach both computationally and in terms of enhanced expressiveness.",None,-1
2dad8b81-ad33-454a-8792-09dfb3f0b3ce,Zero-Shot Multi-Label Topic Inference with Sentence Encoders,0.0542019,"Sentence encoders have indeed been shown to achieve superior performances for
many downstream text-mining tasks and, thus, claimed to be fairly general.
Inspired by this, we performed a detailed study on how to leverage these
sentence encoders for the ""zero-shot topic inference"" task, where the topics
are defined/provided by the users in real-time. Extensive experiments on seven
different datasets demonstrate that Sentence-BERT demonstrates superior
generality compared to other encoders, while Universal Sentence Encoder can be
preferred when efficiency is a top priority.",None,-1
bf61769c-d7d2-4080-8872-d6e5022115c2,GECCO: Geometrically-Conditioned Point Diffusion Models,0.554375,"Diffusion models generating images conditionally on text, such as Dall-E 2
and Stable Diffusion, have recently made a splash far beyond the computer
vision community. Here, we tackle the related problem of generating point
clouds, both unconditionally, and conditionally with images. For the latter, we
introduce a novel geometrically-motivated conditioning scheme based on
projecting sparse image features into the point cloud and attaching them to
each individual point, at every step in the denoising process. This approach
improves geometric consistency and yields greater fidelity than current methods
relying on unstructured, global latent codes. Additionally, we show how to
apply recent continuous-time diffusion schemes. Our method performs on par or
above the state of art on conditional and unconditional experiments on
synthetic data, while being faster, lighter, and delivering tractable
likelihoods. We show it can also scale to diverse indoors scenes.",None,-1
dba023d0-b0f5-43ef-a8f8-296e80a6bca7,AutoHall: Automated Hallucination Dataset Generation for Large Language Models,0.0617634,"While Large language models (LLMs) have garnered widespread applications
across various domains due to their powerful language understanding and
generation capabilities, the detection of non-factual or hallucinatory content
generated by LLMs remains scarce. Currently, one significant challenge in
hallucination detection is the laborious task of time-consuming and expensive
manual annotation of the hallucinatory generation. To address this issue, this
paper first introduces a method for automatically constructing model-specific
hallucination datasets based on existing fact-checking datasets called
AutoHall. Furthermore, we propose a zero-resource and black-box hallucination
detection method based on self-contradiction. We conduct experiments towards
prevalent open-/closed-source LLMs, achieving superior hallucination detection
performance compared to extant baselines. Moreover, our experiments reveal
variations in hallucination proportions and types among different models.",None,-1
ccdc0af6-b5fd-472b-8f16-841968f897cc,Artificial Intelligence Impact On The Labour Force -- Searching For The Analytical Skills Of The Future Software Engineers,0.236324,"This systematic literature review aims to investigate the impact of
artificial intelligence (AI) on the labour force in software engineering, with
a particular focus on the skills needed for future software engineers, the
impact of AI on the demand for software engineering skills, and the future of
work for software engineers. The review identified 42 relevant publications
through a comprehensive search strategy and analysed their findings. The
results indicate that future software engineers will need to be competent in
programming and have soft skills such as problem-solving and interpersonal
communication. AI will have a significant impact on the software engineering
workforce, with the potential to automate many jobs currently done by software
engineers. The role of a software engineer is changing and will continue to
change in the future, with AI-assisted software development posing challenges
for the software engineering profession. The review suggests that the software
engineering profession must adapt to the changing landscape to remain relevant
and effective in the future.",None,-1
c58509fc-5508-47c5-b89e-dfc08f3a81a9,STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning,0.445063,"Scale variation is a deep-rooted problem in object counting, which has not
been effectively addressed by existing scale-aware algorithms. An important
factor is that they typically involve cooperative learning across
multi-resolutions, which could be suboptimal for learning the most
discriminative features from each scale. In this paper, we propose a novel
method termed STEERER (\textbf{S}elec\textbf{T}iv\textbf{E}
inh\textbf{ER}itance l\textbf{E}a\textbf{R}ning) that addresses the issue of
scale variations in object counting. STEERER selects the most suitable scale
for patch objects to boost feature extraction and only inherits discriminative
features from lower to higher resolution progressively. The main insights of
STEERER are a dedicated Feature Selection and Inheritance Adaptor (FSIA), which
selectively forwards scale-customized features at each scale, and a Masked
Selection and Inheritance Loss (MSIL) that helps to achieve high-quality
density maps across all scales. Our experimental results on nine datasets with
counting and localization tasks demonstrate the unprecedented scale
generalization ability of STEERER. Code is available at
\url{https://github.com/taohan10200/STEERER}.",None,-1
13fe50b7-cf1e-4102-b65e-0b03f5be82f8,Automated Query Generation for Evidence Collection from Web Search Engines,0.111231,"It is widely accepted that so-called facts can be checked by searching for
information on the Internet. This process requires a fact-checker to formulate
a search query based on the fact and to present it to a search engine. Then,
relevant and believable passages need to be identified in the search results
before a decision is made. This process is carried out by sub-editors at many
news and media organisations on a daily basis. Here, we ask the question as to
whether it is possible to automate the first step, that of query generation.
Can we automatically formulate search queries based on factual statements which
are similar to those formulated by human experts? Here, we consider similarity
both in terms of textual similarity and with respect to relevant documents
being returned by a search engine. First, we introduce a moderate-sized
evidence collection dataset which includes 390 factual statements together with
associated human-generated search queries and search results. Then, we
investigate generating queries using a number of rule-based and automatic text
generation methods based on pre-trained large language models (LLMs). We show
that these methods have different merits and propose a hybrid approach which
has superior performance in practice.",None,-1
b8109c22-05de-4d11-8b77-a4e4b377c624,DiffusionAD: Norm-guided One-step Denoising Diffusion for Anomaly Detection,0.0776775,"Anomaly detection has garnered extensive applications in real industrial
manufacturing due to its remarkable effectiveness and efficiency. However,
previous generative-based models have been limited by suboptimal reconstruction
quality, hampering their overall performance. A fundamental enhancement lies in
our reformulation of the reconstruction process using a diffusion model into a
noise-to-norm paradigm. Here, anomalous regions are perturbed with Gaussian
noise and reconstructed as normal, overcoming the limitations of previous
models by facilitating anomaly-free restoration. Additionally, we propose a
rapid one-step denoising paradigm, significantly faster than the traditional
iterative denoising in diffusion models. Furthermore, the introduction of the
norm-guided paradigm elevates the accuracy and fidelity of reconstructions. The
segmentation sub-network predicts pixel-level anomaly scores using the input
image and its anomaly-free restoration. Comprehensive evaluations on four
standard and challenging benchmarks reveal that DiffusionAD outperforms current
state-of-the-art approaches, demonstrating the effectiveness and broad
applicability of the proposed pipeline.",None,-1
3cf5329d-7dcb-4e24-8bfd-3bc02d48c5d0,"Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection",0.866978,"Fake news detection has been a critical task for maintaining the health of
the online news ecosystem. However, very few existing works consider the
temporal shift issue caused by the rapidly-evolving nature of news data in
practice, resulting in significant performance degradation when training on
past data and testing on future data. In this paper, we observe that the
appearances of news events on the same topic may display discernible patterns
over time, and posit that such patterns can assist in selecting training
instances that could make the model adapt better to future data. Specifically,
we design an effective framework FTT (Forecasting Temporal Trends), which could
forecast the temporal distribution patterns of news data and then guide the
detector to fast adapt to future distribution. Experiments on the real-world
temporally split dataset demonstrate the superiority of our proposed framework.
The code is available at https://github.com/ICTMCG/FTT-ACL23.",None,-1
052f3848-176e-4c6a-8485-abf395884668,LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa,0.880884,"Named Entity Recognition(NER) is a task of recognizing entities at a token
level in a sentence. This paper focuses on solving NER tasks in a multilingual
setting for complex named entities. Our team, LLM-RM participated in the
recently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual
Complex Named Entity Recognition. We approach the problem by leveraging
cross-lingual representation provided by fine-tuning XLM-Roberta base model on
datasets of all of the 12 languages provided -- Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and
Ukrainian",None,-1
02e84961-3f15-4aee-af29-f1c539cf2534,Explaining CLIP through Co-Creative Drawings and Interaction,0.436432,"This paper analyses a visual archive of drawings produced by an interactive
robotic art installation where audience members narrated their dreams into a
system powered by CLIPdraw deep learning (DL) model that interpreted and
transformed their dreams into images. The resulting archive of prompt-image
pairs were examined and clustered based on concept representation accuracy. As
a result of the analysis, the paper proposes four groupings for describing and
explaining CLIP-generated results: clear concept, text-to-text as image,
indeterminacy and confusion, and lost in translation. This article offers a
glimpse into a collection of dreams interpreted, mediated and given form by
Artificial Intelligence (AI), showcasing oftentimes unexpected, visually
compelling or, indeed, the dream-like output of the system, with the emphasis
on processes and results of translations between languages, sign-systems and
various modules of the installation. In the end, the paper argues that proposed
clusters support better understanding of the neural model.",None,-1
923f709b-1a30-40db-8ba7-7a175367ffc7,No Free Lunch in Self Supervised Representation Learning,0.591619,"Self-supervised representation learning in computer vision relies heavily on
hand-crafted image transformations to learn meaningful and invariant features.
However few extensive explorations of the impact of transformation design have
been conducted in the literature. In particular, the dependence of downstream
performances to transformation design has been established, but not studied in
depth. In this work, we explore this relationship, its impact on a domain other
than natural images, and show that designing the transformations can be viewed
as a form of supervision. First, we demonstrate that not only do
transformations have an effect on downstream performance and relevance of
clustering, but also that each category in a supervised dataset can be impacted
in a different way. Following this, we explore the impact of transformation
design on microscopy images, a domain where the difference between classes is
more subtle and fuzzy than in natural images. In this case, we observe a
greater impact on downstream tasks performances. Finally, we demonstrate that
transformation design can be leveraged as a form of supervision, as careful
selection of these by a domain expert can lead to a drastic increase in
performance on a given downstream task.",None,-1
363e10e0-d71f-4bc4-a504-49fa42a354bf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,0.749905,"Monocular Depth Estimation (MDE) is a critical component in applications such
as autonomous driving. There are various attacks against MDE networks. These
attacks, especially the physical ones, pose a great threat to the security of
such systems. Traditional adversarial training method requires ground-truth
labels hence cannot be directly applied to self-supervised MDE that does not
have ground-truth depth. Some self-supervised model hardening techniques (e.g.,
contrastive learning) ignore the domain knowledge of MDE and can hardly achieve
optimal performance. In this work, we propose a novel adversarial training
method for self-supervised MDE models based on view synthesis without using
ground-truth depth. We improve adversarial robustness against physical-world
attacks using L0-norm-bounded perturbation in training. We compare our method
with supervised learning based and contrastive learning based methods that are
tailored for MDE. Results on two representative MDE networks show that we
achieve better robustness against various adversarial attacks with nearly no
benign performance degradation.",None,-1
a25416be-fece-42cf-bd86-0e07630a9731,Loop Closure Detection Based on Object-level Spatial Layout and Semantic Consistency,0.334523,"Visual simultaneous localization and mapping (SLAM) systems face challenges
in detecting loop closure under the circumstance of large viewpoint changes. In
this paper, we present an object-based loop closure detection method based on
the spatial layout and semanic consistency of the 3D scene graph. Firstly, we
propose an object-level data association approach based on the semantic
information from semantic labels, intersection over union (IoU), object color,
and object embedding. Subsequently, multi-view bundle adjustment with the
associated objects is utilized to jointly optimize the poses of objects and
cameras. We represent the refined objects as a 3D spatial graph with semantics
and topology. Then, we propose a graph matching approach to select
correspondence objects based on the structure layout and semantic property
similarity of vertices' neighbors. Finally, we jointly optimize camera
trajectories and object poses in an object-level pose graph optimization, which
results in a globally consistent map. Experimental results demonstrate that our
proposed data association approach can construct more accurate 3D semantic
maps, and our loop closure method is more robust than point-based and
object-based methods in circumstances with large viewpoint changes.",None,-1
2ea77c20-3f4e-4b22-98bc-0249ca1f5ace,Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis,0.702924,"ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.",None,-1
7ecb96d1-96cd-45d3-8e06-ddd4192b3cad,DoubleH: Twitter User Stance Detection via Bipartite Graph Neural Networks,0.276207,"Given the development and abundance of social media, studying the stance of
social media users is a challenging and pressing issue. Social media users
express their stance by posting tweets and retweeting. Therefore, the
homogeneous relationship between users and the heterogeneous relationship
between users and tweets are relevant for the stance detection task. Recently,
graph neural networks (GNNs) have developed rapidly and have been applied to
social media research. In this paper, we crawl a large-scale dataset of the
2020 US presidential election and automatically label all users by manually
tagged hashtags. Subsequently, we propose a bipartite graph neural network
model, DoubleH, which aims to better utilize homogeneous and heterogeneous
information in user stance detection tasks. Specifically, we first construct a
bipartite graph based on posting and retweeting relations for two kinds of
nodes, including users and tweets. We then iteratively update the node's
representation by extracting and separately processing heterogeneous and
homogeneous information in the node's neighbors. Finally, the representations
of user nodes are used for user stance classification. Experimental results
show that DoubleH outperforms the state-of-the-art methods on popular
benchmarks. Further analysis illustrates the model's utilization of information
and demonstrates stability and efficiency at different numbers of layers.",None,-1
543cd890-e0e5-432f-8d12-32f9435ee429,"YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus",0.693543,"Machine learning for sign languages is bottlenecked by data. In this paper,
we present YouTube-ASL, a large-scale, open-domain corpus of American Sign
Language (ASL) videos and accompanying English captions drawn from YouTube.
With ~1000 hours of videos and >2500 unique signers, YouTube-ASL is ~3x as
large and has ~10x as many unique signers as the largest prior ASL dataset. We
train baseline models for ASL to English translation on YouTube-ASL and
evaluate them on How2Sign, where we achieve a new finetuned state of the art of
12.39 BLEU and, for the first time, report zero-shot results.",None,-1
6427d9db-6a1b-4699-b150-1f49a375cdcf,Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,0.80379,"Intracerebral hemorrhage (ICH) is the second most common and deadliest form
of stroke. Despite medical advances, predicting treat ment outcomes for ICH
remains a challenge. This paper proposes a novel prognostic model that utilizes
both imaging and tabular data to predict treatment outcome for ICH. Our model
is trained on observational data collected from non-randomized controlled
trials, providing reliable predictions of treatment success. Specifically, we
propose to employ a variational autoencoder model to generate a low-dimensional
prognostic score, which can effectively address the selection bias resulting
from the non-randomized controlled trials. Importantly, we develop a
variational distributions combination module that combines the information from
imaging data, non-imaging clinical data, and treatment assignment to accurately
generate the prognostic score. We conducted extensive experiments on a
real-world clinical dataset of intracerebral hemorrhage. Our proposed method
demonstrates a substantial improvement in treatment outcome prediction compared
to existing state-of-the-art approaches. Code is available at
https://github.com/med-air/TOP-GPM",None,-1
a3fe1eb9-140d-47be-97c0-ef1745c576a1,Procedural Text Mining with Large Language Models,0.64656,"Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.",None,-1
1dbfd773-e470-4314-a184-b739e8423035,The Gradient of Generative AI Release: Methods and Considerations,0.871335,"As increasingly powerful generative AI systems are developed, the release
method greatly varies. We propose a framework to assess six levels of access to
generative AI systems: fully closed; gradual or staged access; hosted access;
cloud-based or API access; downloadable access; and fully open. Each level,
from fully closed to fully open, can be viewed as an option along a gradient.
We outline key considerations across this gradient: release methods come with
tradeoffs, especially around the tension between concentrating power and
mitigating risks. Diverse and multidisciplinary perspectives are needed to
examine and mitigate risk in generative AI systems from conception to
deployment. We show trends in generative system release over time, noting
closedness among large companies for powerful systems and openness among
organizations founded on principles of openness. We also enumerate safety
controls and guardrails for generative systems and necessary investments to
improve future releases.",None,-1
a7fa4484-ea07-43c6-b02b-107cf530fa7d,"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",0.530422,"Recent investigations show that large language models (LLMs), specifically
GPT-4, not only have remarkable capabilities in common Natural Language
Processing (NLP) tasks but also exhibit human-level performance on various
professional and academic benchmarks. However, whether GPT-4 can be directly
used in practical applications and replace traditional artificial intelligence
(AI) tools in specialized domains requires further experimental validation. In
this paper, we explore the potential of LLMs such as GPT-4 to outperform
traditional AI tools in dementia diagnosis. Comprehensive comparisons between
GPT-4 and traditional AI tools are conducted to examine their diagnostic
accuracy in a clinical setting. Experimental results on two real clinical
datasets show that, although LLMs like GPT-4 demonstrate potential for future
advancements in dementia diagnosis, they currently do not surpass the
performance of traditional AI tools. The interpretability and faithfulness of
GPT-4 are also evaluated by comparison with real doctors. We discuss the
limitations of GPT-4 in its current state and propose future research
directions to enhance GPT-4 in dementia diagnosis.",None,-1
cabcab32-5395-44c4-9b9b-91af57cb1786,Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception,0.827748,"Multi-agent collaborative perception as a potential application for
vehicle-to-everything communication could significantly improve the perception
performance of autonomous vehicles over single-agent perception. However,
several challenges remain in achieving pragmatic information sharing in this
emerging research. In this paper, we propose SCOPE, a novel collaborative
perception framework that aggregates the spatio-temporal awareness
characteristics across on-road agents in an end-to-end manner. Specifically,
SCOPE has three distinct strengths: i) it considers effective semantic cues of
the temporal context to enhance current representations of the target agent;
ii) it aggregates perceptually critical spatial information from heterogeneous
agents and overcomes localization errors via multi-scale feature interactions;
iii) it integrates multi-source representations of the target agent based on
their complementary contributions by an adaptive fusion paradigm. To thoroughly
evaluate SCOPE, we consider both real-world and simulated scenarios of
collaborative 3D object detection tasks on three datasets. Extensive
experiments demonstrate the superiority of our approach and the necessity of
the proposed components.",None,-1
cc2dedd2-6ab7-4079-a06e-a54fa0880f2e,Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition,0.425078,"In recent years, Large Language Models such as GPT-3 showed remarkable
capabilities in performing NLP tasks in the zero and few shot settings. On the
other hand, the experiments highlighted the difficulty of GPT-3 in carrying out
tasks that require a certain degree of reasoning, such as arithmetic
operations. In this paper we evaluate the ability of Transformer Language
Models to perform arithmetic operations following a pipeline that, before
performing computations, decomposes numbers in units, tens, and so on. We
denote the models fine-tuned with this pipeline with the name Calculon and we
test them in the task of performing additions, subtractions and multiplications
on the same test sets of GPT-3. Results show an increase of accuracy of 63% in
the five-digit addition task. Moreover, we demonstrate the importance of the
decomposition pipeline introduced, since fine-tuning the same Language Model
without decomposing numbers results in 0% accuracy in the five-digit addition
task.",None,-1
9b446596-afaa-4a72-8bd5-6f8a97d1d65e,Lexi: Self-Supervised Learning of the UI Language,0.25152,"Humans can learn to operate the user interface (UI) of an application by
reading an instruction manual or how-to guide. Along with text, these resources
include visual content such as UI screenshots and images of application icons
referenced in the text. We explore how to leverage this data to learn generic
visio-linguistic representations of UI screens and their components. These
representations are useful in many real applications, such as accessibility,
voice navigation, and task automation. Prior UI representation models rely on
UI metadata (UI trees and accessibility labels), which is often missing,
incompletely defined, or not accessible. We avoid such a dependency, and
propose Lexi, a pre-trained vision and language model designed to handle the
unique features of UI screens, including their text richness and context
sensitivity. To train Lexi we curate the UICaption dataset consisting of 114k
UI images paired with descriptions of their functionality. We evaluate Lexi on
four tasks: UI action entailment, instruction-based UI image retrieval,
grounding referring expressions, and UI entity recognition.",None,-1
dd9db295-d16c-4593-8713-414864e57d63,On the Risk of Misinformation Pollution with Large Language Models,0.343358,"In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.",None,-1
1504d478-eb69-40bd-9399-f478917e91d1,Ensemble of Counterfactual Explainers,0.614329,"In eXplainable Artificial Intelligence (XAI), several counterfactual
explainers have been proposed, each focusing on some desirable properties of
counterfactual instances: minimality, actionability, stability, diversity,
plausibility, discriminative power. We propose an ensemble of counterfactual
explainers that boosts weak explainers, which provide only a subset of such
properties, to a powerful method covering all of them. The ensemble runs weak
explainers on a sample of instances and of features, and it combines their
results by exploiting a diversity-driven selection function. The method is
model-agnostic and, through a wrapping approach based on autoencoders, it is
also data-agnostic.",None,-1
3cced239-ee57-4aa7-88d6-41ef5ddd9501,Adversarial Word Dilution as Text Data Augmentation in Low-Resource Regime,0.285796,"Data augmentation is widely used in text classification, especially in the
low-resource regime where a few examples for each class are available during
training. Despite the success, generating data augmentations as hard positive
examples that may increase their effectiveness is under-explored. This paper
proposes an Adversarial Word Dilution (AWD) method that can generate hard
positive examples as text data augmentations to train the low-resource text
classification model efficiently. Our idea of augmenting the text data is to
dilute the embedding of strong positive words by weighted mixing with
unknown-word embedding, making the augmented inputs hard to be recognized as
positive by the classification model. We adversarially learn the dilution
weights through a constrained min-max optimization process with the guidance of
the labels. Empirical studies on three benchmark datasets show that AWD can
generate more effective data augmentations and outperform the state-of-the-art
text data augmentation methods. The additional analysis demonstrates that the
data augmentations generated by AWD are interpretable and can flexibly extend
to new examples without further training.",None,-1
d13c2278-08eb-4c91-9100-dfd13caadf0d,Householder Projector for Unsupervised Latent Semantics Discovery,0.733369,"Generative Adversarial Networks (GANs), especially the recent style-based
generators (StyleGANs), have versatile semantics in the structured latent
space. Latent semantics discovery methods emerge to move around the latent code
such that only one factor varies during the traversal. Recently, an
unsupervised method proposed a promising direction to directly use the
eigenvectors of the projection matrix that maps latent codes to features as the
interpretable directions. However, one overlooked fact is that the projection
matrix is non-orthogonal and the number of eigenvectors is too large. The
non-orthogonality would entangle semantic attributes in the top few
eigenvectors, and the large dimensionality might result in meaningless
variations among the directions even if the matrix is orthogonal. To avoid
these issues, we propose Householder Projector, a flexible and general low-rank
orthogonal matrix representation based on Householder transformations, to
parameterize the projection matrix. The orthogonality guarantees that the
eigenvectors correspond to disentangled interpretable semantics, while the
low-rank property encourages that each identified direction has meaningful
variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and
evaluate the models on several benchmarks. Within only $1\%$ of the original
training steps for fine-tuning, our projector helps StyleGANs to discover more
disentangled and precise semantic attributes without sacrificing image
fidelity.",None,-1
567cbff5-d614-41cb-bbf3-eaa1a9a53b6c,Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation,0.0760653,"Understanding Arabic text and generating human-like responses is a
challenging endeavor. While many researchers have proposed models and solutions
for individual problems, there is an acute shortage of a comprehensive Arabic
natural language generation toolkit that is capable of handling a wide range of
tasks. In this work, we present a novel Arabic text-to-text Transformer model,
namely AraT5v2. Our new model is methodically trained on extensive and diverse
data, utilizing an extended sequence length of 2,048 tokens. We explore various
pretraining strategies including unsupervised, supervised, and joint
pertaining, under both single and multitask settings. Our models outperform
competitive baselines with large margins. We take our work one step further by
developing and publicly releasing Octopus, a Python-based package and
command-line toolkit tailored for eight Arabic generation tasks all exploiting
a single model. We release the models and the toolkit on our public repository.",None,-1
c12ed45c-a66a-490e-8d3b-919aa1fe5384,Late Breaking Results: Scalable and Efficient Hyperdimensional Computing for Network Intrusion Detection,0.37337,"Cybersecurity has emerged as a critical challenge for the industry. With the
large complexity of the security landscape, sophisticated and costly deep
learning models often fail to provide timely detection of cyber threats on edge
devices. Brain-inspired hyperdimensional computing (HDC) has been introduced as
a promising solution to address this issue. However, existing HDC approaches
use static encoders and require very high dimensionality and hundreds of
training iterations to achieve reasonable accuracy. This results in a serious
loss of learning efficiency and causes huge latency for detecting attacks. In
this paper, we propose CyberHD, an innovative HDC learning framework that
identifies and regenerates insignificant dimensions to capture complicated
patterns of cyber threats with remarkably lower dimensionality. Additionally,
the holographic distribution of patterns in high dimensional space provides
CyberHD with notably high robustness against hardware errors.",None,-1
cd9d6677-d1f5-4f4d-9ece-4c3ef004e529,Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models,0.893902,"We present Text2Room, a method for generating room-scale textured 3D meshes
from a given text prompt as input. To this end, we leverage pre-trained 2D
text-to-image models to synthesize a sequence of images from different poses.
In order to lift these outputs into a consistent 3D scene representation, we
combine monocular depth estimation with a text-conditioned inpainting model.
The core idea of our approach is a tailored viewpoint selection such that the
content of each image can be fused into a seamless, textured 3D mesh. More
specifically, we propose a continuous alignment strategy that iteratively fuses
scene frames with the existing geometry to create a seamless mesh. Unlike
existing works that focus on generating single objects or zoom-out trajectories
from text, our method generates complete 3D scenes with multiple objects and
explicit 3D geometry. We evaluate our approach using qualitative and
quantitative metrics, demonstrating it as the first method to generate
room-scale 3D geometry with compelling textures from only text as input.",None,-1
179d2b74-5284-466d-82b4-348a92106504,ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,0.770769,"ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods.",None,-1
75908882-b9c2-4f27-b1de-d368652f4f35,BOLLWM: A real-world dataset for bollworm pest monitoring from cotton fields in India,0.213312,"This paper presents a dataset of agricultural pest images captured over five
years by thousands of small holder farmers and farming extension workers across
India. The dataset has been used to support a mobile application that relies on
artificial intelligence to assist farmers with pest management decisions.
Creation came from a mix of organized data collection, and from mobile
application usage that was less controlled. This makes the dataset unique
within the pest detection community, exhibiting a number of characteristics
that place it closer to other non-agricultural objected detection datasets.
This not only makes the dataset applicable to future pest management
applications, it opens the door for a wide variety of other research agendas.",None,-1
83e0b3db-a7eb-49d5-ac0e-deaccebf0d7c,The perpetual motion machine of AI-generated data and the distraction of ChatGPT-as-scientist,0.0341217,"Since ChatGPT works so well, are we on the cusp of solving science with AI?
Is not AlphaFold2 suggestive that the potential of LLMs in biology and the
sciences more broadly is limitless? Can we use AI itself to bridge the lack of
data in the sciences in order to then train an AI? Herein we present a
discussion of these topics.",None,-1
22047407-f8ff-4332-8076-cbc78b0216e8,Towards Open-Domain Topic Classification,0.669927,"We introduce an open-domain topic classification system that accepts
user-defined taxonomy in real time. Users will be able to classify a text
snippet with respect to any candidate labels they want, and get instant
response from our web interface. To obtain such flexibility, we build the
backend model in a zero-shot way. By training on a new dataset constructed from
Wikipedia, our label-aware text classifier can effectively utilize implicit
knowledge in the pretrained language model to handle labels it has never seen
before. We evaluate our model across four datasets from various domains with
different label sets. Experiments show that the model significantly improves
over existing zero-shot baselines in open-domain scenarios, and performs
competitively with weakly-supervised models trained on in-domain data.",None,-1
41649626-6ff0-4cf0-b771-0bd75e5dd2cf,MatKB: Semantic Search for Polycrystalline Materials Synthesis Procedures,0.140398,"In this paper, we present a novel approach to knowledge extraction and
retrieval using Natural Language Processing (NLP) techniques for material
science. Our goal is to automatically mine structured knowledge from millions
of research articles in the field of polycrystalline materials and make it
easily accessible to the broader community. The proposed method leverages NLP
techniques such as entity recognition and document classification to extract
relevant information and build an extensive knowledge base, from a collection
of 9.5 Million publications. The resulting knowledge base is integrated into a
search engine, which enables users to search for information about specific
materials, properties, and experiments with greater precision than traditional
search engines like Google. We hope our results can enable material scientists
quickly locate desired experimental procedures, compare their differences, and
even inspire them to design new experiments. Our website will be available at
Github \footnote{https://github.com/Xianjun-Yang/PcMSP.git} soon.",None,-1
c9a98381-b61c-4d93-900c-fdf61621d997,Measuring a Priori Voting Power -- Taking Delegations Seriously,0.440599,"We introduce new power indices to measure the a priori voting power of voters
in liquid democracy elections where an underlying network restricts
delegations. We argue that our power indices are natural extensions of the
standard Penrose-Banzhaf index in simple voting games. We show that computing
the criticality of a voter is #P-hard even when voting weights are
polynomially-bounded in the size of the instance. However, for specific
settings, such as when the underlying network is a bipartite or complete graph,
recursive formulas can compute these indices for weighted voting games in
pseudo-polynomial time. We highlight their theoretical properties and provide
numerical results to illustrate how restricting the possible delegations can
alter voters' voting power.",None,-1
1a743ecd-f40d-4458-b53e-9551dc1a16f5,Pretraining on the Test Set Is All You Need,0.479073,"Inspired by recent work demonstrating the promise of smaller
Transformer-based language models pretrained on carefully curated data, we
supercharge such approaches by investing heavily in curating a novel, high
quality, non-synthetic data mixture based solely on evaluation benchmarks.
Using our novel dataset mixture consisting of less than 100 thousand tokens, we
pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL}
(pronounced ``fictional"") that achieves perfect results across diverse academic
benchmarks, strictly outperforming all known foundation models.
\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen
grokking-like ability to accurately predict downstream evaluation benchmarks'
canaries.",None,-1
f8381ecf-af05-4b0b-94bc-0551ab958970,MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset,0.72586,"Sentence Boundary Detection (SBD) is one of the foundational building blocks
of Natural Language Processing (NLP), with incorrectly split sentences heavily
influencing the output quality of downstream tasks. It is a challenging task
for algorithms, especially in the legal domain, considering the complex and
different sentence structures used. In this work, we curated a diverse
multilingual legal dataset consisting of over 130'000 annotated sentences in 6
languages. Our experimental results indicate that the performance of existing
SBD models is subpar on multilingual legal data. We trained and tested
monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers,
demonstrating state-of-the-art performance. We also show that our multilingual
models outperform all baselines in the zero-shot setting on a Portuguese test
set. To encourage further research and development by the community, we have
made our dataset, models, and code publicly available.",None,-1
b8408ea9-776a-4d77-976c-3c603db7c06a,3-Objective Pareto Optimization for Problems with Chance Constraints,0.200416,"Evolutionary multi-objective algorithms have successfully been used in the
context of Pareto optimization where a given constraint is relaxed into an
additional objective. In this paper, we explore the use of 3-objective
formulations for problems with chance constraints. Our formulation trades off
the expected cost and variance of the stochastic component as well as the given
deterministic constraint. We point out benefits that this 3-objective
formulation has compared to a bi-objective one recently investigated for chance
constraints with Normally distributed stochastic components. Our analysis shows
that the 3-objective formulation allows to compute all required trade-offs
using 1-bit flips only, when dealing with a deterministic cardinality
constraint. Furthermore, we carry out experimental investigations for the
chance constrained dominating set problem and show the benefit for this
classical NP-hard problem.",None,-1
2d0e1cc8-e733-478b-9d37-0cba8d457ed2,"Cybernetic Environment: A Historical Reflection on System, Design, and Machine Intelligence",0.12234,"Taking on a historical lens, this paper traces the development of cybernetics
and systems thinking back to the 1950s, when a group of interdisciplinary
scholars converged to create a new theoretical model based on machines and
systems for understanding matters of meaning, information, consciousness, and
life. By presenting a genealogy of research in the landscape architecture
discipline, the paper argues that landscape architects have been an important
part of the development of cybernetics by materializing systems based on
cybernetic principles in the environment through ecologically based landscape
design. The landscape discipline has developed a design framework that provides
transformative insights into understanding machine intelligence. The paper
calls for a new paradigm of environmental engagement to understand matters of
design and machine intelligence.",None,-1
22b7c4b5-ec3d-4055-98d2-92e669764103,Unbalanced Optimal Transport for Unbalanced Word Alignment,0.0661258,"Monolingual word alignment is crucial to model semantic interactions between
sentences. In particular, null alignment, a phenomenon in which words have no
corresponding counterparts, is pervasive and critical in handling semantically
divergent sentences. Identification of null alignment is useful on its own to
reason about the semantic similarity of sentences by indicating there exists
information inequality. To achieve unbalanced word alignment that values both
alignment and null alignment, this study shows that the family of optimal
transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and
powerful approaches even without tailor-made techniques. Our extensive
experiments covering unsupervised and supervised settings indicate that our
generic OT-based alignment methods are competitive against the
state-of-the-arts specially designed for word alignment, remarkably on
challenging datasets with high null alignment frequencies.",None,-1
59f8121d-5d4d-4ff5-8c76-0ed31bf41ee6,Adversarial Contrastive Distillation with Adaptive Denoising,0.843592,"Adversarial Robustness Distillation (ARD) is a novel method to boost the
robustness of small models. Unlike general adversarial training, its robust
knowledge transfer can be less easily restricted by the model capacity.
However, the teacher model that provides the robustness of knowledge does not
always make correct predictions, interfering with the student's robust
performances. Besides, in the previous ARD methods, the robustness comes
entirely from one-to-one imitation, ignoring the relationship between examples.
To this end, we propose a novel structured ARD method called Contrastive
Relationship DeNoise Distillation (CRDND). We design an adaptive compensation
module to model the instability of the teacher. Moreover, we utilize the
contrastive relationship to explore implicit robustness knowledge among
multiple examples. Experimental results on multiple attack benchmarks show
CRDND can transfer robust knowledge efficiently and achieves state-of-the-art
performances.",None,-1
4510ed9d-8ce4-455e-b3b9-beefc6d1fedd,"Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution",0.991115,"Geocoding is the task of converting location mentions in text into structured
data that encodes the geospatial semantics. We propose a new architecture for
geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to
generate a list of candidate entries from the geospatial ontology. Then it
reranks the candidate entries using a transformer-based neural network that
incorporates information from the ontology such as the entry's population. This
generate-and-rerank process is applied twice: first to resolve the less
ambiguous countries, states, and counties, and second to resolve the remaining
location mentions, using the identified countries, states, and counties as
context. Our proposed toponym resolution framework achieves state-of-the-art
performance on multiple datasets. Code and models are available at
\url{https://github.com/clulab/geonorm}.",None,-1
276857af-1b29-4894-a81e-84cb306f2aaf,"Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science",0.726076,"This chapter presents some of the fundamental assumptions and principles that
could form the philosophical foundation of GeoAI and spatial data science.
Instead of reviewing the well-established characteristics of spatial data
(analysis), including interaction, neighborhoods, and autocorrelation, the
chapter highlights themes such as sustainability, bias in training data,
diversity in schema knowledge, and the (potential lack of) neutrality of GeoAI
systems from a unifying ethical perspective. Reflecting on our profession's
ethical implications will assist us in conducting potentially disruptive
research more responsibly, identifying pitfalls in designing, training, and
deploying GeoAI-based systems, and developing a shared understanding of the
benefits but also potential dangers of artificial intelligence and machine
learning research across academic fields, all while sharing our unique
(geo)spatial perspective with others.",None,-1
0e186ba1-9632-4009-9d85-fa1a0e3ff02f,Towards Explainable TOPSIS: Visual Insights into the Effects of Weights and Aggregations on Rankings,0.545257,"Multi-Criteria Decision Analysis (MCDA) is extensively used across diverse
industries to assess and rank alternatives. Among numerous MCDA methods
developed to solve real-world ranking problems, TOPSIS remains one of the most
popular choices in many application areas. TOPSIS calculates distances between
the considered alternatives and two predefined ones, namely the ideal and the
anti-ideal, and creates a ranking of the alternatives according to a chosen
aggregation of these distances. However, the interpretation of the inner
workings of TOPSIS is difficult, especially when the number of criteria is
large. To this end, recent research has shown that TOPSIS aggregations can be
expressed using the means (M) and standard deviations (SD) of alternatives,
creating MSD-space, a tool for visualizing and explaining aggregations. Even
though MSD-space is highly useful, it assumes equally important criteria,
making it less applicable to real-world ranking problems. In this paper, we
generalize the concept of MSD-space to weighted criteria by introducing the
concept of WMSD-space defined by what is referred to as weight-scaled means and
standard deviations. We demonstrate that TOPSIS and similar distance-based
aggregation methods can be successfully illustrated in a plane and interpreted
even when the criteria are weighted, regardless of their number. The proposed
WMSD-space offers a practical method for explaining TOPSIS rankings in
real-world decision problems.",None,-1
7b6b396d-4059-489a-bb84-bafe2fae036a,Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation,0.6673,"Medical image segmentation is a fundamental task in the community of medical
image analysis. In this paper, a novel network architecture, referred to as
Convolution, Transformer, and Operator (CTO), is proposed. CTO employs a
combination of Convolutional Neural Networks (CNNs), Vision Transformer (ViT),
and an explicit boundary detection operator to achieve high recognition
accuracy while maintaining an optimal balance between accuracy and efficiency.
The proposed CTO follows the standard encoder-decoder segmentation paradigm,
where the encoder network incorporates a popular CNN backbone for capturing
local semantic information, and a lightweight ViT assistant for integrating
long-range dependencies. To enhance the learning capacity on boundary, a
boundary-guided decoder network is proposed that uses a boundary mask obtained
from a dedicated boundary detection operator as explicit supervision to guide
the decoding learning process. The performance of the proposed method is
evaluated on six challenging medical image segmentation datasets, demonstrating
that CTO achieves state-of-the-art accuracy with a competitive model
complexity.",None,-1
6194704e-87a0-4b1e-9730-719a5139df9e,EM Pre-training for Multi-party Dialogue Response Generation,0.851785,"Dialogue response generation requires an agent to generate a response
according to the current dialogue history, in terms of which two-party
dialogues have been well studied, but leaving a great gap for multi-party
dialogues at the same time. Different from two-party dialogues where each
response is a direct reply to its previous utterance, the addressee of a
response utterance should be specified before it is generated in the
multi-party scenario. Thanks to the huge amount of two-party conversational
data, various pre-trained language models for two-party dialogue response
generation have been proposed. However, due to the lack of annotated addressee
labels in multi-party dialogue datasets, it is hard to use them to pre-train a
response generation model for multi-party dialogues. To tackle this obstacle,
we propose an Expectation-Maximization (EM) approach that iteratively performs
the expectation steps to generate addressee labels, and the maximization steps
to optimize a response generation model. Theoretical analyses and extensive
experiments have justified the feasibility and effectiveness of our proposed
method.",None,-1
0ff7e8e7-7023-48a4-9f10-3e60d89231ff,Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,0.926908,"Most unsupervised image anomaly localization methods suffer from
overgeneralization because of the high generalization abilities of
convolutional neural networks, leading to unreliable predictions. To mitigate
the overgeneralization, this study proposes to collaboratively optimize normal
and abnormal feature distributions with the assistance of synthetic anomalies,
namely collaborative discrepancy optimization (CDO). CDO introduces a margin
optimization module and an overlap optimization module to optimize the two key
factors determining the localization performance, i.e., the margin and the
overlap between the discrepancy distributions (DDs) of normal and abnormal
samples. With CDO, a large margin and a small overlap between normal and
abnormal DDs are obtained, and the prediction reliability is boosted.
Experiments on MVTec2D and MVTec3D show that CDO effectively mitigates the
overgeneralization and achieves great anomaly localization performance with
real-time computation efficiency. A real-world automotive plastic parts
inspection application further demonstrates the capability of the proposed CDO.
Code is available on https://github.com/caoyunkang/CDO.",None,-1
5b7abfab-2913-4f7c-8606-3dc8c1175334,A Family of Pretrained Transformer Language Models for Russian,0.221118,"Transformer language models (LMs) are fundamental to NLP research
methodologies and applications in various languages. However, developing such
models specifically for the Russian language has received little attention.
This paper introduces a collection of 13 Russian Transformer LMs, which spans
encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder
(ruT5, FRED-T5) architectures. We provide a report on the model architecture
design and pretraining, and the results of evaluating their generalization
abilities on Russian language understanding and generation datasets and
benchmarks. By pretraining and releasing these specialized Transformer LMs, we
aim to broaden the scope of the NLP research directions and enable the
development of industrial solutions for the Russian language.",None,-1
20d37897-b8fb-42a3-96f2-447e705339eb,Towards Higher Pareto Frontier in Multilingual Machine Translation,0.0935875,"Multilingual neural machine translation has witnessed remarkable progress in
recent years. However, the long-tailed distribution of multilingual corpora
poses a challenge of Pareto optimization, i.e., optimizing for some languages
may come at the cost of degrading the performance of others. Existing balancing
training strategies are equivalent to a series of Pareto optimal solutions,
which trade off on a Pareto frontier. In this work, we propose a new training
framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto
frontier outwards rather than making trade-offs. Specifically, Pareto-MD
collaboratively trains two Pareto optimal solutions that favor different
languages and allows them to learn from the strengths of each other via
knowledge distillation. Furthermore, we introduce a novel strategy to enable
stronger communication between Pareto optimal solutions and broaden the
applicability of our approach. Experimental results on the widely-used WMT and
TED datasets show that our method significantly pushes the Pareto frontier and
outperforms baselines by up to +2.46 BLEU.",None,-1
25217840-8f82-4d7a-941c-48b24409838f,CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification,0.657088,"Medical image classification poses unique challenges due to the long-tailed
distribution of diseases, the co-occurrence of diagnostic findings, and the
multiple views available for each study or patient. This paper introduces our
solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed
Classification on Chest X-Rays. Our approach introduces CheXFusion, a
transformer-based fusion module incorporating multi-view images. The fusion
module, guided by self-attention and cross-attention mechanisms, efficiently
aggregates multi-view features while considering label co-occurrence.
Furthermore, we explore data balancing and self-training methods to optimize
the model's performance. Our solution achieves state-of-the-art results with
0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our
success in the task underscores the significance of considering multi-view
settings, class imbalance, and label co-occurrence in medical image
classification. Public code is available at
https://github.com/dongkyuk/CXR-LT-public-solution",None,-1
5e139439-ac51-4360-bafc-b49cb1ee55e7,Japanese SimCSE Technical Report,0.109091,"We report the development of Japanese SimCSE, Japanese sentence embedding
models fine-tuned with SimCSE. Since there is a lack of sentence embedding
models for Japanese that can be used as a baseline in sentence embedding
research, we conducted extensive experiments on Japanese sentence embeddings
involving 24 pre-trained Japanese or multilingual language models, five
supervised datasets, and four unsupervised datasets. In this report, we provide
the detailed training setup for Japanese SimCSE and their evaluation results.",None,-1
8249ca98-55f1-40ab-b4aa-dfafe6ebe2d8,Meet in the Middle: A New Pre-training Paradigm,0.193522,"Most language models (LMs) are trained and applied in an autoregressive
left-to-right fashion, assuming that the next token only depends on the
preceding ones. However, this assumption ignores the potential benefits of
using the full sequence information during training, and the possibility of
having context from both sides during inference. In this paper, we propose a
new pre-training paradigm with techniques that jointly improve the training
data efficiency and the capabilities of the LMs in the infilling task. The
first is a training objective that aligns the predictions of a left-to-right LM
with those of a right-to-left LM, trained on the same data but in reverse
order. The second is a bidirectional inference procedure that enables both LMs
to meet in the middle. We show the effectiveness of our pre-training paradigm
with extensive experiments on both programming and natural language models,
outperforming strong baselines.",None,-1
083fff5e-f811-4740-890a-136e6b1dfc83,Implant Global and Local Hierarchy Information to Sequence based Code Representation Models,0.173255,"Source code representation with deep learning techniques is an important
research field. There have been many studies that learn sequential or
structural information for code representation. But sequence-based models and
non-sequence-models both have their limitations. Researchers attempt to
incorporate structural information to sequence-based models, but they only mine
part of token-level hierarchical structure information. In this paper, we
analyze how the complete hierarchical structure influences the tokens in code
sequences and abstract this influence as a property of code tokens called
hierarchical embedding. The hierarchical embedding is further divided into
statement-level global hierarchy and token-level local hierarchy. Furthermore,
we propose the Hierarchy Transformer (HiT), a simple but effective sequence
model to incorporate the complete hierarchical embeddings of source code into a
Transformer model. We demonstrate the effectiveness of hierarchical embedding
on learning code structure with an experiment on variable scope detection task.
Further evaluation shows that HiT outperforms SOTA baseline models and show
stable training efficiency on three source code-related tasks involving
classification and generation tasks across 8 different datasets.",None,-1
26d6eb1e-c2b4-4adc-9193-ca68f61eac15,Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models,0.274324,"Recent advancements in large scale text-to-image models have opened new
possibilities for guiding the creation of images through human-devised natural
language. However, while prior literature has primarily focused on the
generation of individual images, it is essential to consider the capability of
these models to ensure coherency within a sequence of images to fulfill the
demands of real-world applications such as storytelling. To address this, here
we present a novel neural pipeline for generating a coherent storybook from the
plain text of a story. Specifically, we leverage a combination of a pre-trained
Large Language Model and a text-guided Latent Diffusion Model to generate
coherent images. While previous story synthesis frameworks typically require a
large-scale text-to-image model trained on expensive image-caption pairs to
maintain the coherency, we employ simple textual inversion techniques along
with detector-based semantic image editing which allows zero-shot generation of
the coherent storybook. Experimental results show that our proposed method
outperforms state-of-the-art image editing baselines.",None,-1
3c2cd55f-3c44-40ff-867a-3b06545509c5,Rigidity-Aware Detection for 6D Object Pose Estimation,0.955794,"Most recent 6D object pose estimation methods first use object detection to
obtain 2D bounding boxes before actually regressing the pose. However, the
general object detection methods they use are ill-suited to handle cluttered
scenes, thus producing poor initialization to the subsequent pose network. To
address this, we propose a rigidity-aware detection method exploiting the fact
that, in 6D pose estimation, the target objects are rigid. This lets us
introduce an approach to sampling positive object regions from the entire
visible object area during training, instead of naively drawing samples from
the bounding box center where the object might be occluded. As such, every
visible object part can contribute to the final bounding box prediction,
yielding better detection robustness. Key to the success of our approach is a
visibility map, which we propose to build using a minimum barrier distance
between every pixel in the bounding box and the box boundary. Our results on
seven challenging 6D pose estimation datasets evidence that our method
outperforms general detection frameworks by a large margin. Furthermore,
combined with a pose regression network, we obtain state-of-the-art pose
estimation results on the challenging BOP benchmark.",None,-1
84b0be41-a512-4041-a2de-a47e2f68ffa5,NormBank: A Knowledge Bank of Situational Social Norms,0.997417,"We present NormBank, a knowledge bank of 155k situational norms. This
resource is designed to ground flexible normative reasoning for interactive,
assistive, and collaborative AI systems. Unlike prior commonsense resources,
NormBank grounds each inference within a multivalent sociocultural frame, which
includes the setting (e.g., restaurant), the agents' contingent roles (waiter,
customer), their attributes (age, gender), and other physical, social, and
cultural constraints (e.g., the temperature or the country of operation). In
total, NormBank contains 63k unique constraints from a taxonomy that we
introduce and iteratively refine here. Constraints then apply in different
combinations to frame social norms. Under these manipulations, norms are
non-monotonic - one can cancel an inference by updating its frame even
slightly. Still, we find evidence that neural models can help reliably extend
the scope and coverage of NormBank. We further demonstrate the utility of this
resource with a series of transfer experiments.",None,-1
387a6210-d4f4-494d-b9e9-215ca9917b0b,How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?,0.743904,"Annotating cross-document event coreference links is a time-consuming and
cognitively demanding task that can compromise annotation quality and
efficiency. To address this, we propose a model-in-the-loop annotation approach
for event coreference resolution, where a machine learning model suggests
likely corefering event pairs only. We evaluate the effectiveness of this
approach by first simulating the annotation process and then, using a novel
annotator-centric Recall-Annotation effort trade-off metric, we compare the
results of various underlying models and datasets. We finally present a method
for obtaining 97\% recall while substantially reducing the workload required by
a fully manual annotation process. Code and data can be found at
https://github.com/ahmeshaf/model_in_coref",None,-1
9264f459-d206-4ff6-a04d-7133f6f1ab4f,Pre-Trained Large Language Models for Industrial Control,0.381217,"For industrial control, developing high-performance controllers with few
samples and low technical debt is appealing. Foundation models, possessing rich
prior knowledge obtained from pre-training with Internet-scale corpus, have the
potential to be a good controller with proper prompts. In this paper, we take
HVAC (Heating, Ventilation, and Air Conditioning) building control as an
example to examine the ability of GPT-4 (one of the first-tier foundation
models) as the controller. To control HVAC, we wrap the task as a language game
by providing text including a short description for the task, several selected
demonstrations, and the current observation to GPT-4 on each step and execute
the actions responded by GPT-4. We conduct series of experiments to answer the
following questions: 1)~How well can GPT-4 control HVAC? 2)~How well can GPT-4
generalize to different scenarios for HVAC control? 3) How different parts of
the text context affect the performance? In general, we found GPT-4 achieves
the performance comparable to RL methods with few samples and low technical
debt, indicating the potential of directly applying foundation models to
industrial control tasks.",None,-1
b868548d-aca9-482a-b30c-68cb5b3f288b,Real-Word Error Correction with Trigrams: Correcting Multiple Errors in a Sentence,0.339864,"Spelling correction is a fundamental task in Text Mining. In this study, we
assess the real-word error correction model proposed by Mays, Damerau and
Mercer and describe several drawbacks of the model. We propose a new variation
which focuses on detecting and correcting multiple real-word errors in a
sentence, by manipulating a Probabilistic Context-Free Grammar (PCFG) to
discriminate between items in the search space. We test our approach on the
Wall Street Journal corpus and show that it outperforms Hirst and Budanitsky's
WordNet-based method and Wilcox-O'Hearn, Hirst, and Budanitsky's fixed windows
size method.-O'Hearn, Hirst, and Budanitsky's fixed windows size method.",None,-1
034525ab-8638-43fc-a9f9-378f777d801c,Whale Detection Enhancement through Synthetic Satellite Images,0.0371427,"With a number of marine populations in rapid decline, collecting and
analyzing data about marine populations has become increasingly important to
develop effective conservation policies for a wide range of marine animals,
including whales. Modern computer vision algorithms allow us to detect whales
in images in a wide range of domains, further speeding up and enhancing the
monitoring process. However, these algorithms heavily rely on large training
datasets, which are challenging and time-consuming to collect particularly in
marine or aquatic environments. Recent advances in AI however have made it
possible to synthetically create datasets for training machine learning
algorithms, thus enabling new solutions that were not possible before. In this
work, we present a solution - SeaDroneSim2 benchmark suite, which addresses
this challenge by generating aerial, and satellite synthetic image datasets to
improve the detection of whales and reduce the effort required for training
data collection. We show that we can achieve a 15% performance boost on whale
detection compared to using the real data alone for training, by augmenting a
10% real data. We open source both the code of the simulation platform
SeaDroneSim2 and the dataset generated through it.",None,-1
1b43fda2-e235-43fe-bf1b-a8fb3680ac9d,SpeechAlign: a Framework for Speech Translation Alignment Evaluation,0.343819,"Speech-to-Speech and Speech-to-Text translation are currently dynamic areas
of research. In our commitment to advance these fields, we present SpeechAlign,
a framework designed to evaluate the underexplored field of source-target
alignment in speech models. The SpeechAlign framework has two core components.
First, to tackle the absence of suitable evaluation datasets, we introduce the
Speech Gold Alignment dataset, built upon a English-German text translation
gold alignment dataset. Secondly, we introduce two novel metrics, Speech
Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate
(TW-SAER), which enable the evaluation of alignment quality within speech
models. While the former gives equal importance to each word, the latter
assigns weights based on the length of the words in the speech signal. By
publishing SpeechAlign we provide an accessible evaluation framework for model
assessment, and we employ it to benchmark open-source Speech Translation
models. In doing so, we contribute to the ongoing research progress within the
fields of Speech-to-Speech and Speech-to-Text translation.",None,-1
c5b7e139-9d2c-4379-b514-5bca98fd9c22,Explainable Depression Detection via Head Motion Patterns,0.106086,"While depression has been studied via multimodal non-verbal behavioural cues,
head motion behaviour has not received much attention as a biomarker. This
study demonstrates the utility of fundamental head-motion units, termed
\emph{kinemes}, for depression detection by adopting two distinct approaches,
and employing distinctive features: (a) discovering kinemes from head motion
data corresponding to both depressed patients and healthy controls, and (b)
learning kineme patterns only from healthy controls, and computing statistics
derived from reconstruction errors for both the patient and control classes.
Employing machine learning methods, we evaluate depression classification
performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings
indicate that: (1) head motion patterns are effective biomarkers for detecting
depressive symptoms, and (2) explanatory kineme patterns consistent with prior
findings can be observed for the two classes. Overall, we achieve peak F1
scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary
classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over
videos for AVEC2013.",None,-1
b9587e1a-d108-4773-95ba-ac8faa9bf9df,Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network,0.785543,"Building extraction aims to segment building pixels from remote sensing
images and plays an essential role in many applications, such as city planning
and urban dynamic monitoring. Over the past few years, deep learning methods
with encoder-decoder architectures have achieved remarkable performance due to
their powerful feature representation capability. Nevertheless, due to the
varying scales and styles of buildings, conventional deep learning models
always suffer from uncertain predictions and cannot accurately distinguish the
complete footprints of the building from the complex distribution of ground
objects, leading to a large degree of omission and commission. In this paper,
we realize the importance of uncertain prediction and propose a novel and
straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To
verify the performance of our proposed UANet, we conduct extensive experiments
on three public building datasets, including the WHU building dataset, the
Massachusetts building dataset, and the Inria aerial image dataset. Results
demonstrate that the proposed UANet outperforms other state-of-the-art
algorithms by a large margin.",None,-1
914c7718-7be4-45a0-898d-959693b266ae,Distilling Internet-Scale Vision-Language Models into Embodied Agents,0.280423,"Instruction-following agents must ground language into their observation and
action spaces. Learning to ground language is challenging, typically requiring
domain-specific engineering or large quantities of human interaction data. To
address this challenge, we propose using pretrained vision-language models
(VLMs) to supervise embodied agents. We combine ideas from model distillation
and hindsight experience replay (HER), using a VLM to retroactively generate
language describing the agent's behavior. Simple prompting allows us to control
the supervision signal, teaching an agent to interact with novel objects based
on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered
environment. Fewshot prompting lets us teach abstract category membership,
including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary
preferences over objects). Our work outlines a new and effective way to use
internet-scale VLMs, repurposing the generic language grounding acquired by
such models to teach task-relevant groundings to embodied agents.",None,-1
4720e8ea-74f9-455f-a0fb-29383ac6bc1b,Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees,0.0358757,"Machine learning algorithms, especially Neural Networks (NNs), are a valuable
tool used to approximate non-linear relationships, like the AC-Optimal Power
Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several
orders of magnitude when deployed for use. Often in power systems literature,
the NNs are trained with a fixed dataset generated prior to the training
process. In this paper, we show that adapting the NN training dataset during
training can improve the NN performance and substantially reduce its worst-case
violations. This paper proposes an algorithm that identifies and enriches the
training dataset with critical datapoints that reduce the worst-case violations
and deliver a neural network with improved worst-case performance guarantees.
We demonstrate the performance of our algorithm in four test power systems,
ranging from 39-buses to 162-buses.",None,-1
08060c98-c9b6-4586-a374-eacccaebaaa0,DACOS-A Manually Annotated Dataset of Code Smells,0.182738,"Researchers apply machine-learning techniques for code smell detection to
counter the subjectivity of many code smells. Such approaches need a large,
manually annotated dataset for training and benchmarking. Existing literature
offers a few datasets; however, they are small in size and, more importantly,
do not focus on the subjective code snippets. In this paper, we present DACOS,
a manually annotated dataset containing 10,267 annotations for 5,192 code
snippets. The dataset targets three kinds of code smells at different
granularity: multifaceted abstraction, complex method, and long parameter list.
The dataset is created in two phases. The first phase helps us identify the
code snippets that are potentially subjective by determining the thresholds of
metrics used to detect a smell. The second phase collects annotations for
potentially subjective snippets. We also offer an extended dataset DACOSX that
includes definitely benign and definitely smelly snippets by using the
thresholds identified in the first phase. We have developed TagMan, a web
application to help annotators view and mark the snippets one-by-one and record
the provided annotations. We make the datasets and the web application
accessible publicly. This dataset will help researchers working on smell
detection techniques to build relevant and context-aware machine-learning
models.",None,-1
2b57eb27-4a05-47ff-b891-ed95800030f7,Motion-DVAE: Unsupervised learning for fast human motion denoising,0.179293,"Pose and motion priors are crucial for recovering realistic and accurate
human motion from noisy observations. Substantial progress has been made on
pose and shape estimation from images, and recent works showed impressive
results using priors to refine frame-wise predictions. However, a lot of motion
priors only model transitions between consecutive poses and are used in
time-consuming optimization procedures, which is problematic for many
applications requiring real-time motion capture. We introduce Motion-DVAE, a
motion prior to capture the short-term dependencies of human motion. As part of
the dynamical variational autoencoder (DVAE) models family, Motion-DVAE
combines the generative capability of VAE models and the temporal modeling of
recurrent architectures. Together with Motion-DVAE, we introduce an
unsupervised learned denoising method unifying regression- and
optimization-based approaches in a single framework for real-time 3D human pose
estimation. Experiments show that the proposed approach reaches competitive
performance with state-of-the-art methods while being much faster.",None,-1
01a95438-2499-42f2-9706-a3b025746a40,Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning,0.513725,"This is the first work to look at the application of large language models
(LLMs) for the purpose of model space edits in automated planning tasks. To set
the stage for this union, we explore two different flavors of model space
problems that have been studied in the AI planning literature and explore the
effect of an LLM on those tasks. We empirically demonstrate how the performance
of an LLM contrasts with combinatorial search (CS) -- an approach that has been
traditionally used to solve model space tasks in planning, both with the LLM in
the role of a standalone model space reasoner as well as in the role of a
statistical signal in concert with the CS approach as part of a two-stage
process. Our experiments show promising results suggesting further forays of
LLMs into the exciting world of model space reasoning for planning tasks in the
future.",None,-1
1217c730-d3ae-4a82-9e90-8f9cf5c73b48,A GOA-Based Fault-Tolerant Trajectory Tracking Control for an Underwater Vehicle of Multi-Thruster System without Actuator Saturation,0.698725,"This paper proposes an intelligent fault-tolerant control (FTC) strategy to
tackle the trajectory tracking problem of an underwater vehicle (UV) under
thruster damage (power loss) cases and meanwhile resolve the actuator
saturation brought by the vehicle's physical constraints. In the proposed
control strategy, the trajectory tracking component is formed by a refined
backstepping algorithm that controls the velocity variation and a sliding mode
control deducts the torque/force outputs; the fault-tolerant component is
established based on a Grasshopper Optimization Algorithm (GOA), which provides
fast convergence speed as well as satisfactory accuracy of deducting optimized
reallocation of the thruster forces to compensate for the power loss in
different fault cases. Simulations with or without environmental perturbations
under different fault cases and comparisons to other traditional FTCs are
presented, thus verifying the effectiveness and robustness of the proposed
GOA-based fault-tolerant trajectory tracking design.",None,-1
bffe7689-7e64-406d-8a4e-1e062137d9a9,ChatGPT-Powered Hierarchical Comparisons for Image Classification,0.433263,"The zero-shot open-vocabulary challenge in image classification is tackled by
pretrained vision-language models like CLIP, which benefit from incorporating
class-specific knowledge from large language models (LLMs) like ChatGPT.
However, biases in CLIP lead to similar descriptions for distinct but related
classes, prompting our novel image classification framework via hierarchical
comparisons: using LLMs to recursively group classes into hierarchies and
classifying images by comparing image-text embeddings at each hierarchy level,
resulting in an intuitive, effective, and explainable approach.",None,-1
c47618f6-5ce4-4fbc-8493-6eb9f0b068cf,Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes,0.282947,"Current scene graph datasets suffer from strong long-tail distributions of
their predicate classes. Due to a very low number of some predicate classes in
the test sets, no reliable metrics can be retrieved for the rarest classes. We
construct a new panoptic scene graph dataset and a set of metrics that are
designed as a benchmark for the predictive performance especially on rare
predicate classes. To construct the new dataset, we propose a model-assisted
annotation pipeline that efficiently finds rare predicate classes that are
hidden in a large set of images like needles in a haystack.
  Contrary to prior scene graph datasets, Haystack contains explicit negative
annotations, i.e. annotations that a given relation does not have a certain
predicate class. Negative annotations are helpful especially in the field of
scene graph generation and open up a whole new set of possibilities to improve
current scene graph generation models.
  Haystack is 100% compatible with existing panoptic scene graph datasets and
can easily be integrated with existing evaluation pipelines. Our dataset and
code can be found here: https://lorjul.github.io/haystack/. It includes
annotation files and simple to use scripts and utilities, to help with
integrating our dataset in existing work.",None,-1
e3c4e4be-0280-4bbc-a727-bde8176de5dd,DomainAdaptor: A Novel Approach to Test-time Adaptation,0.661406,"To deal with the domain shift between training and test samples, current
methods have primarily focused on learning generalizable features during
training and ignore the specificity of unseen samples that are also critical
during the test. In this paper, we investigate a more challenging task that
aims to adapt a trained CNN model to unseen domains during the test. To
maximumly mine the information in the test data, we propose a unified method
called DomainAdaptor for the test-time adaptation, which consists of an
AdaMixBN module and a Generalized Entropy Minimization (GEM) loss.
Specifically, AdaMixBN addresses the domain shift by adaptively fusing training
and test statistics in the normalization layer via a dynamic mixture
coefficient and a statistic transformation operation. To further enhance the
adaptation ability of AdaMixBN, we design a GEM loss that extends the Entropy
Minimization loss to better exploit the information in the test data. Extensive
experiments show that DomainAdaptor consistently outperforms the
state-of-the-art methods on four benchmarks. Furthermore, our method brings
more remarkable improvement against existing methods on the few-data unseen
domain. The code is available at https://github.com/koncle/DomainAdaptor.",None,-1
dd4e188e-7be5-4bde-b454-e725ed912d67,Measuring Massive Multitask Chinese Understanding,0.966169,"The development of large-scale Chinese language models is flourishing, yet
there is a lack of corresponding capability assessments. Therefore, we propose
a test to measure the multitask accuracy of large Chinese language models. This
test encompasses four major domains, including medicine, law, psychology, and
education, with 15 subtasks in medicine and 8 subtasks in education. We found
that the best-performing models in the zero-shot setting outperformed the
worst-performing models by nearly 18.6 percentage points on average. Across the
four major domains, the highest average zero-shot accuracy of all models is
0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot
accuracy of 0.693 in clinical medicine, which was the highest accuracy among
all models across all subtasks. All models performed poorly in the legal
domain, with the highest zero-shot accuracy reaching only 0.239. By
comprehensively evaluating the breadth and depth of knowledge across multiple
disciplines, this test can more accurately identify the shortcomings of the
models.",None,-1
8bd1cb25-08d9-4c7e-af57-e39107aac9b3,Head Rotation in Denoising Diffusion Models,0.0761638,"Denoising Diffusion Models (DDM) are emerging as the cutting-edge technology
in the realm of deep generative modeling, challenging the dominance of
Generative Adversarial Networks. However, effectively exploring the latent
space's semantics and identifying compelling trajectories for manipulating and
editing important attributes of the generated samples remains challenging,
primarily due to the high-dimensional nature of the latent space. In this
study, we specifically concentrate on face rotation, which is known to be one
of the most intricate editing operations. By leveraging a recent embedding
technique for Denoising Diffusion Implicit Models (DDIM), we achieve, in many
cases, noteworthy manipulations encompassing a wide rotation angle of $\pm
30^o$, preserving the distinct characteristics of the individual. Our
methodology exploits the computation of trajectories approximating clouds of
latent representations of dataset samples with different yaw rotations through
linear regression. Specific trajectories are obtained by restricting the
analysis to subsets of data sharing significant attributes with the source
image. One of these attributes is the light provenance: a byproduct of our
research is a labeling of CelebA, categorizing images into three major groups
based on the illumination direction: left, center, and right.",None,-1
42ebdd39-2154-498a-b19b-9dd8bd9d897f,HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation,0.814038,"Monocular 3D human pose and shape estimation is an ill-posed problem since
multiple 3D solutions can explain a 2D image of a subject. Recent approaches
predict a probability distribution over plausible 3D pose and shape parameters
conditioned on the image. We show that these approaches exhibit a trade-off
between three key properties: (i) accuracy - the likelihood of the ground-truth
3D solution under the predicted distribution, (ii) sample-input consistency -
the extent to which 3D samples from the predicted distribution match the
visible 2D image evidence, and (iii) sample diversity - the range of plausible
3D solutions modelled by the predicted distribution. Our method, HuManiFlow,
predicts simultaneously accurate, consistent and diverse distributions. We use
the human kinematic tree to factorise full body pose into ancestor-conditioned
per-body-part pose distributions in an autoregressive manner. Per-body-part
distributions are implemented using normalising flows that respect the manifold
structure of SO(3), the Lie group of per-body-part poses. We show that
ill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,
and employ only probabilistic training losses. Code is available at:
https://github.com/akashsengupta1997/HuManiFlow.",None,-1
23b088b7-b289-4811-be61-a2a0455e7f9d,Realistic Noise Synthesis with Diffusion Models,0.0798535,"Deep image denoising models often rely on large amount of training data for
the high quality performance. However, it is challenging to obtain sufficient
amount of data under real-world scenarios for the supervised training. As such,
synthesizing realistic noise becomes an important solution. However, existing
techniques have limitations in modeling complex noise distributions, resulting
in residual noise and edge artifacts in denoising methods relying on synthetic
data. To overcome these challenges, we propose a novel method that synthesizes
realistic noise using diffusion models, namely Realistic Noise Synthesize
Diffusor (RNSD). In particular, the proposed time-aware controlling module can
simulate various environmental conditions under given camera settings. RNSD can
incorporate guided multiscale content, such that more realistic noise with
spatial correlations can be generated at multiple frequencies. In addition, we
construct an inversion mechanism to predict the unknown camera setting, which
enables the extension of RNSD to datasets without setting information.
Extensive experiments demonstrate that our RNSD method significantly
outperforms the existing methods not only in the synthesized noise under
multiple realism metrics, but also in the single image denoising performances.",None,-1
7b27cb6d-2815-4f75-b814-443849b749b3,Unbiased Scene Graph Generation in Videos,0.580101,"The task of dynamic scene graph generation (SGG) from videos is complicated
and challenging due to the inherent dynamics of a scene, temporal fluctuation
of model predictions, and the long-tailed distribution of the visual
relationships in addition to the already existing challenges in image-based
SGG. Existing methods for dynamic SGG have primarily focused on capturing
spatio-temporal context using complex architectures without addressing the
challenges mentioned above, especially the long-tailed distribution of
relationships. This often leads to the generation of biased scene graphs. To
address these challenges, we introduce a new framework called TEMPURA: TEmporal
consistency and Memory Prototype guided UnceRtainty Attenuation for unbiased
dynamic SGG. TEMPURA employs object-level temporal consistencies via
transformer-based sequence modeling, learns to synthesize unbiased relationship
representations using memory-guided training, and attenuates the predictive
uncertainty of visual relations using a Gaussian Mixture Model (GMM). Extensive
experiments demonstrate that our method achieves significant (up to 10% in some
cases) performance gain over existing methods highlighting its superiority in
generating more unbiased scene graphs.",None,-1
732e07cf-da0c-4b17-a5a8-bca56221e739,JacobiNeRF: NeRF Shaping with Mutual Information Gradients,0.299054,"We propose a method that trains a neural radiance field (NeRF) to encode not
only the appearance of the scene but also semantic correlations between scene
points, regions, or entities -- aiming to capture their mutual co-variation
patterns. In contrast to the traditional first-order photometric reconstruction
objective, our method explicitly regularizes the learning dynamics to align the
Jacobians of highly-correlated entities, which proves to maximize the mutual
information between them under random scene perturbations. By paying attention
to this second-order information, we can shape a NeRF to express semantically
meaningful synergies when the network weights are changed by a delta along the
gradient of a single entity, region, or even a point. To demonstrate the merit
of this mutual information modeling, we leverage the coordinated behavior of
scene entities that emerges from our shaping to perform label propagation for
semantic and instance segmentation. Our experiments show that a JacobiNeRF is
more efficient in propagating annotations among 2D pixels and 3D points
compared to NeRFs without mutual information shaping, especially in extremely
sparse label regimes -- thus reducing annotation burden. The same machinery can
further be used for entity selection or scene modifications.",None,-1
58246cc4-a523-4a6d-8bd6-5b84a386cc89,Chain of Thought Prompting Elicits Knowledge Augmentation,0.754477,"The knowledge-augmented deep learning paradigm refers to a paradigm in which
domain knowledge is identified and integrated into deep models. Conventional
methods typically employ task-specific approaches to gather external knowledge
from various sources. In contrast, large language models are extensively
pre-trained and can serve as a comprehensive source of external knowledge. In
this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments
knowledge for deep learning. CoT-KA avoids the need for additional knowledge
retrieval or knowledge reasoning models, as required in conventional
augmentation methods. Our results demonstrate that CoT-KA outperforms both pure
CoT-based methods and the non-augmented method across the majority of eleven
publicly available benchmarks for various reasoning tasks.",None,-1
71a65d6b-22c5-42bf-a180-d62e919b62b1,Toward Unsupervised 3D Point Cloud Anomaly Detection using Variational Autoencoder,0.515316,"In this paper, we present an end-to-end unsupervised anomaly detection
framework for 3D point clouds. To the best of our knowledge, this is the first
work to tackle the anomaly detection task on a general object represented by a
3D point cloud. We propose a deep variational autoencoder-based unsupervised
anomaly detection network adapted to the 3D point cloud and an anomaly score
specifically for 3D point clouds. To verify the effectiveness of the model, we
conducted extensive experiments on the ShapeNet dataset. Through quantitative
and qualitative evaluation, we demonstrate that the proposed method outperforms
the baseline method. Our code is available at
https://github.com/llien30/point_cloud_anomaly_detection.",None,-1
f9fb8a94-1b1d-4801-8fc9-f8f0e5fd3577,RotoGBML: Towards Out-of-Distribution Generalization for Gradient-Based Meta-Learning,0.355581,"Gradient-based meta-learning (GBML) algorithms are able to fast adapt to new
tasks by transferring the learned meta-knowledge, while assuming that all tasks
come from the same distribution (in-distribution, ID). However, in the real
world, they often suffer from an out-of-distribution (OOD) generalization
problem, where tasks come from different distributions. OOD exacerbates
inconsistencies in magnitudes and directions of task gradients, which brings
challenges for GBML to optimize the meta-knowledge by minimizing the sum of
task gradients in each minibatch. To address this problem, we propose RotoGBML,
a novel approach to homogenize OOD task gradients. RotoGBML uses reweighted
vectors to dynamically balance diverse magnitudes to a common scale and uses
rotation matrixes to rotate conflicting directions close to each other. To
reduce overhead, we homogenize gradients with the features rather than the
network parameters. On this basis, to avoid the intervention of non-causal
features (e.g., backgrounds), we also propose an invariant self-information
(ISI) module to extract invariant causal features (e.g., the outlines of
objects). Finally, task gradients are homogenized based on these invariant
causal features. Experiments show that RotoGBML outperforms other
state-of-the-art methods on various few-shot image classification benchmarks.",None,-1
c5eed6bf-87d6-4c42-aaf6-c100532869c0,SAOR: Single-View Articulated Object Reconstruction,0.457083,"We introduce SAOR, a novel approach for estimating the 3D shape, texture, and
viewpoint of an articulated object from a single image captured in the wild.
Unlike prior approaches that rely on pre-defined category-specific 3D templates
or tailored 3D skeletons, SAOR learns to articulate shapes from single-view
image collections with a skeleton-free part-based model without requiring any
3D object shape priors. To prevent ill-posed solutions, we propose a
cross-instance consistency loss that exploits disentangled object shape
deformation and articulation. This is helped by a new silhouette-based sampling
mechanism to enhance viewpoint diversity during training. Our method only
requires estimated object silhouettes and relative depth maps from
off-the-shelf pre-trained networks during training. At inference time, given a
single-view image, it efficiently outputs an explicit mesh representation. We
obtain improved qualitative and quantitative results on challenging quadruped
animals compared to relevant existing work.",None,-1
f49ddcf5-b519-407a-922e-d6fb7fcd56ff,Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals,0.998751,"In the contemporary landscape of social media, an alarming number of users
express negative emotions, some of which manifest as strong suicidal
intentions. This situation underscores a profound need for trained
psychological counselors who can enact effective mental interventions. However,
the development of these professionals is often an imperative but
time-consuming task. Consequently, the mobilization of non-professionals or
volunteers in this capacity emerges as a pressing concern. Leveraging the
capabilities of artificial intelligence, and in particular, the recent advances
in large language models, offers a viable solution to this challenge. This
paper introduces a novel model constructed on the foundation of large language
models to fully assist non-professionals in providing psychological
interventions on online user discourses. This framework makes it plausible to
harness the power of non-professional counselors in a meaningful way. A
comprehensive study was conducted involving ten professional psychological
counselors of varying expertise, evaluating the system across five critical
dimensions. The findings affirm that our system is capable of analyzing
patients' issues with relative accuracy and proffering professional-level
strategies recommendations, thereby enhancing support for non-professionals.
This research serves as a compelling validation of the application of large
language models in the field of psychology and lays the groundwork for a new
paradigm of community-based mental health support.",None,-1
a01673a1-01d0-4231-b385-2e7100bfcb05,On the Zero-Shot Generalization of Machine-Generated Text Detectors,0.676686,"The rampant proliferation of large language models, fluent enough to generate
text indistinguishable from human-written language, gives unprecedented
importance to the detection of machine-generated text. This work is motivated
by an important research question: How will the detectors of machine-generated
text perform on outputs of a new generator, that the detectors were not trained
on? We begin by collecting generation data from a wide range of LLMs, and train
neural detectors on data from each generator and test its performance on
held-out generators. While none of the detectors can generalize to all
generators, we observe a consistent and interesting pattern that the detectors
trained on data from a medium-size LLM can zero-shot generalize to the larger
version. As a concrete application, we demonstrate that robust detectors can be
built on an ensemble of training data from medium-sized models.",None,-1
11a3490d-2e09-4fa6-8a2a-fc52ef8051b7,Improving Open Language Models by Learning from Organic Interactions,0.492052,"We present BlenderBot 3x, an update on the conversational model BlenderBot 3,
which is now trained using organic conversation and feedback data from
participating users of the system in order to improve both its skills and
safety. We are publicly releasing the participating de-identified interaction
data for use by the research community, in order to spur further progress.
Training models with organic data is challenging because interactions with
people ""in the wild"" include both high quality conversations and feedback, as
well as adversarial and toxic behavior. We study techniques that enable
learning from helpful teachers while avoiding learning from people who are
trying to trick the model into unhelpful or toxic responses. BlenderBot 3x is
both preferred in conversation to BlenderBot 3, and is shown to produce safer
responses in challenging situations. While our current models are still far
from perfect, we believe further improvement can be achieved by continued use
of the techniques explored in this work.",None,-1
411a5c73-1888-4cd5-86a2-624b09e09196,Polynomial Implicit Neural Representations For Large Diverse Datasets,0.450356,"Implicit neural representations (INR) have gained significant popularity for
signal and image representation for many end-tasks, such as superresolution, 3D
modeling, and more. Most INR architectures rely on sinusoidal positional
encoding, which accounts for high-frequency information in data. However, the
finite encoding size restricts the model's representational power. Higher
representational power is needed to go from representing a single given image
to representing large and diverse datasets. Our approach addresses this gap by
representing an image with a polynomial function and eliminates the need for
positional encodings. Therefore, to achieve a progressively higher degree of
polynomial representation, we use element-wise multiplications between features
and affine-transformed coordinate locations after every ReLU layer. The
proposed method is evaluated qualitatively and quantitatively on large datasets
like ImageNet. The proposed Poly-INR model performs comparably to
state-of-the-art generative models without any convolution, normalization, or
self-attention layers, and with far fewer trainable parameters. With much fewer
training parameters and higher representative power, our approach paves the way
for broader adoption of INR models for generative modeling tasks in complex
domains. The code is available at \url{https://github.com/Rajhans0/Poly_INR}",None,-1
dcd5393b-60fd-4c08-82a1-0af63335f449,"Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs",0.0473578,"In our opinion the exuberance surrounding the relative success of data-driven
large language models (LLMs) is slightly misguided and for several reasons (i)
LLMs cannot be relied upon for factual information since for LLMs all ingested
text (factual or non-factual) was created equal; (ii) due to their subsymbolic
na-ture, whatever 'knowledge' these models acquire about language will always
be buried in billions of microfeatures (weights), none of which is meaningful
on its own; and (iii) LLMs will often fail to make the correct inferences in
several linguistic contexts (e.g., nominal compounds, copredication, quantifier
scope ambi-guities, intensional contexts. Since we believe the relative success
of data-driven large language models (LLMs) is not a reflection on the symbolic
vs. subsymbol-ic debate but a reflection on applying the successful strategy of
a bottom-up reverse engineering of language at scale, we suggest in this paper
applying the effective bottom-up strategy in a symbolic setting resulting in
symbolic, explainable, and ontologically grounded language models.",None,-1
326f015a-2157-44b6-9558-6d6baa60d6ad,Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics,0.0293992,"This paper presents a novel approach named Persona-Grouping-Intelligence
(PGI), which has been crafted to tackle the challenges posed by GPT models when
applied to real-world business issues. PGI leverages the inherent capabilities
of the GPT model to comprehend intricate language structures and generate
responses that are contextually relevant. The experiment occurred in a business
scenario where human intelligence was being underutilized due to less optimized
business processes. The primary objective of this approach is to leverage GPT
models to reduce the workload on humans in tasks that are extensive,
monotonous, and repetitive. Instead, the focus is redirected toward
decision-making activities. Remarkably, the experiment yielded an accuracy rate
of 93.81% in validating 4,000 responses generated by the model, underscoring
the effectiveness of the PGI strategies. Effectively addressing the issue of
underutilized human intelligence, this paradigm shift aligns business
environments with dynamic machine intelligence, enabling them to navigate the
intricacies of real-world challenges. This approach facilitates the practical
utilization of these models to tackle actual problems. The methodology offers
an opportunity to reshape the fundamental structure of business processes by
seamlessly integrating human decision-making with adaptable machine
intelligence. Consequently, this optimization enhances operational efficiency
and elevates strategic decision-making across diverse business contexts.",None,-1
2a113652-6bef-4ac6-bb50-03ed1aec7307,DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction using Neural Radiance Fields,0.709705,"Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance
for 3D computer vision tasks, including novel view synthesis and 3D shape
reconstruction. However, these methods fail in adverse weather conditions. To
address this challenge, we introduce DehazeNeRF as a framework that robustly
operates in hazy conditions. DehazeNeRF extends the volume rendering equation
by adding physically realistic terms that model atmospheric scattering. By
parameterizing these terms using suitable networks that match the physical
properties, we introduce effective inductive biases, which, together with the
proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view
haze removal, novel view synthesis, and 3D shape reconstruction where existing
approaches fail.",None,-1
2b089c66-e310-40be-9084-055cd6569285,Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning,0.219748,"Deep Reinforcement Learning (DRL) has achieved remarkable success in
sequential decision-making problems. However, existing DRL agents make
decisions in an opaque fashion, hindering the user from establishing trust and
scrutinizing weaknesses of the agents. While recent research has developed
Interpretable Policy Extraction (IPE) methods for explaining how an agent takes
actions, their explanations are often inconsistent with the agent's behavior
and thus, frequently fail to explain. To tackle this issue, we propose a novel
method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by
analyzing the optimization mechanism of existing IPE methods, elaborating on
the issue of ignoring consistency while increasing cumulative rewards. We then
design a fidelity-induced mechanism by integrate a fidelity measurement into
the reinforcement learning feedback. We conduct experiments in the complex
control environment of StarCraft II, an arena typically avoided by current IPE
methods. The experiment results demonstrate that FIPE outperforms the baselines
in terms of interaction performance and consistency, meanwhile easy to
understand.",None,-1
f5baeb94-4ba8-4b4a-96d9-1fce7e84f563,Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording,0.0628001,"Large language models (LLMs) have become mainstream technology with their
versatile use cases and impressive performance. Despite the countless
out-of-the-box applications, LLMs are still not reliable. A lot of work is
being done to improve the factual accuracy, consistency, and ethical standards
of these models through fine-tuning, prompting, and Reinforcement Learning with
Human Feedback (RLHF), but no systematic analysis of the responses of these
models to different categories of statements, or on their potential
vulnerabilities to simple prompting changes is available. In this work, we
analyze what confuses GPT-3: how the model responds to certain sensitive topics
and what effects the prompt wording has on the model response. We find that
GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes
mistakes with common Misconceptions and Controversies. The model responses are
inconsistent across prompts and settings, highlighting GPT-3's unreliability.
Dataset and code of our analysis is available in
https://github.com/tanny411/GPT3-Reliability-Check.",None,-1
ff49e5f6-cdf7-410d-8757-c25a303064cf,AGI: Artificial General Intelligence for Education,0.562375,"Artificial general intelligence (AGI) has gained global recognition as a
future technology due to the emergence of breakthrough large language models
and chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional
AI models, typically designed for a limited range of tasks, demand significant
amounts of domain-specific data for training and may not always consider
intricate interpersonal dynamics in education. AGI, driven by the recent large
pre-trained models, represents a significant leap in the capability of machines
to perform tasks that require human-level intelligence, such as reasoning,
problem-solving, decision-making, and even understanding human emotions and
social interactions. This position paper reviews AGI's key concepts,
capabilities, scope, and potential within future education, including achieving
future educational goals, designing pedagogy and curriculum, and performing
assessments. It highlights that AGI can significantly improve intelligent
tutoring systems, educational assessment, and evaluation procedures. AGI
systems can adapt to individual student needs, offering tailored learning
experiences. They can also provide comprehensive feedback on student
performance and dynamically adjust teaching methods based on student progress.
The paper emphasizes that AGI's capabilities extend to understanding human
emotions and social interactions, which are critical in educational settings.
The paper discusses that ethical issues in education with AGI include data
bias, fairness, and privacy and emphasizes the need for codes of conduct to
ensure responsible AGI use in academic settings like homework, teaching, and
recruitment. We also conclude that the development of AGI necessitates
interdisciplinary collaborations between educators and AI engineers to advance
research and application efforts.",None,-1
8cd961d2-753d-4d56-8918-5b27270ffbf5,Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition,0.787181,"Existing pedestrian attribute recognition (PAR) algorithms are mainly
developed based on a static image. However, the performance is not reliable for
images with challenging factors, such as heavy occlusion, motion blur, etc. In
this work, we propose to understand human attributes using video frames that
can make full use of temporal information. Specifically, we formulate the
video-based PAR as a vision-language fusion problem and adopt pre-trained big
models CLIP to extract the feature embeddings of given video frames. To better
utilize the semantic information, we take the attribute list as another input
and transform the attribute words/phrase into the corresponding sentence via
split, expand, and prompt. Then, the text encoder of CLIP is utilized for
language embedding. The averaged visual tokens and text tokens are concatenated
and fed into a fusion Transformer for multi-modal interactive learning. The
enhanced tokens will be fed into a classification head for pedestrian attribute
prediction. Extensive experiments on a large-scale video-based PAR dataset
fully validated the effectiveness of our proposed framework.",None,-1
89b12338-d677-42f9-b1de-ac5e07a65239,How Powerful are Decoder-Only Transformer Neural Models?,0.125728,"In this article we prove that the general transformer neural model
undergirding modern large language models (LLMs) is Turing complete under
reasonable assumptions. This is the first work to directly address the Turing
completeness of the underlying technology employed in GPT-x as past work has
focused on the more expressive, full auto-encoder transformer architecture.
From this theoretical analysis, we show that the sparsity/compressibility of
the word embedding is an important consideration for Turing completeness to
hold. We also show that Transformers are are a variant of B machines studied by
Hao Wang.",None,-1
f34c142c-a718-4e46-adcf-a5ec0332ae81,CalibNet: Dual-branch Cross-modal Calibration for RGB-D Salient Instance Segmentation,0.454504,"We propose a novel approach for RGB-D salient instance segmentation using a
dual-branch cross-modal feature calibration architecture called CalibNet. Our
method simultaneously calibrates depth and RGB features in the kernel and mask
branches to generate instance-aware kernels and mask features. CalibNet
consists of three simple modules, a dynamic interactive kernel (DIK) and a
weight-sharing fusion (WSF), which work together to generate effective
instance-aware kernels and integrate cross-modal features. To improve the
quality of depth features, we incorporate a depth similarity assessment (DSA)
module prior to DIK and WSF. In addition, we further contribute a new DSIS
dataset, which contains 1,940 images with elaborate instance-level annotations.
Extensive experiments on three challenging benchmarks show that CalibNet yields
a promising result, i.e., 58.0% AP with 320*480 input size on the COME15K-N
test set, which significantly surpasses the alternative frameworks. Our code
and dataset are available at: https://github.com/PJLallen/CalibNet.",None,-1
2a8c6176-c63f-4c64-93d2-c70126f37c08,Scaling Laws of RoPE-based Extrapolation,0.999813,"The extrapolation capability of Large Language Models (LLMs) based on Rotary
Position Embedding is currently a topic of considerable interest. The
mainstream approach to addressing extrapolation with LLMs involves modifying
RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the
original RoPE, with a larger value and providing longer fine-tuning text. In
this work, we first observe that fine-tuning a RoPE-based LLM with either a
smaller or larger base in pre-training context length could significantly
enhance its extrapolation performance. After that, we propose
\textbf{\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework
from the periodic perspective, to describe the relationship between the
extrapolation performance and base value as well as tuning context length. In
this process, we also explain the origin of the RoPE-based extrapolation issue
by \textbf{\textit{critical dimension for extrapolation}}. Besides these
observations and analyses, we achieve extrapolation up to 1 million context
length within only 16K training length on LLaMA2 7B and 13B.",None,-1
a63bbd39-05e9-433b-a495-7d70710272d6,Learning to Optimize for Reinforcement Learning,0.0574946,"In recent years, by leveraging more data, computation, and diverse tasks,
learned optimizers have achieved remarkable success in supervised learning,
outperforming classical hand-designed optimizers. Reinforcement learning (RL)
is essentially different from supervised learning, and in practice, these
learned optimizers do not work well even in simple RL tasks. We investigate
this phenomenon and identify two issues. First, the agent-gradient distribution
is non-independent and identically distributed, leading to inefficient
meta-training. Moreover, due to highly stochastic agent-environment
interactions, the agent-gradients have high bias and variance, which increases
the difficulty of learning an optimizer for RL. We propose pipeline training
and a novel optimizer structure with a good inductive bias to address these
issues, making it possible to learn an optimizer for reinforcement learning
from scratch. We show that, although only trained in toy tasks, our learned
optimizer can generalize to unseen complex tasks in Brax.",None,-1
43acfabf-3225-427f-9197-94dd5d53f0e9,Local Region Perception and Relationship Learning Combined with Feature Fusion for Facial Action Unit Detection,0.93233,"Human affective behavior analysis plays a vital role in human-computer
interaction (HCI) systems. In this paper, we introduce our submission to the
CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW). We
propose a single-stage trained AU detection framework. Specifically, in order
to effectively extract facial local region features related to AU detection, we
use a local region perception module to effectively extract features of
different AUs. Meanwhile, we use a graph neural network-based relational
learning module to capture the relationship between AUs. In addition,
considering the role of the overall feature of the target face on AU detection,
we also use the feature fusion module to fuse the feature information extracted
by the backbone network and the AU feature information extracted by the
relationship learning module. We also adopted some sampling methods, data
augmentation techniques and post-processing strategies to further improve the
performance of the model.",None,-1
4b8fd880-4f04-4017-8b7d-cbf9c38b660c,MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic,0.975648,"Theory of Mind (ToM) is a critical component of intelligence but its
assessment remains the subject of heated debates. Prior research applied human
ToM assessments to natural language processing models using either
human-created standardized tests or rule-based templates. However, these
methods primarily focus on simplistic reasoning and require further validation.
Here, we leverage dynamic epistemic logic to isolate a particular component of
ToM and to generate controlled problems. We also introduce new verbalization
techniques to express these problems in English natural language. Our findings
indicate that some language model scaling (from 70M to 6B and 350M to 174B)
does not consistently yield results better than random chance. While GPT-4
demonstrates superior epistemic reasoning capabilities, there is still room for
improvement. Our code and datasets are publicly available
(https://huggingface.co/datasets/sileod/mindgames ,
https://github.com/sileod/llm-theory-of-mind )",None,-1
5d1710cb-0d15-4343-a023-5b0aa57d9cf4,Hallucination Improves the Performance of Unsupervised Visual Representation Learning,0.877504,"Contrastive learning models based on Siamese structure have demonstrated
remarkable performance in self-supervised learning. Such a success of
contrastive learning relies on two conditions, a sufficient number of positive
pairs and adequate variations between them. If the conditions are not met,
these frameworks will lack semantic contrast and be fragile on overfitting. To
address these two issues, we propose Hallucinator that could efficiently
generate additional positive samples for further contrast. The Hallucinator is
differentiable and creates new data in the feature space. Thus, it is optimized
directly with the pre-training task and introduces nearly negligible
computation. Moreover, we reduce the mutual information of hallucinated pairs
and smooth them through non-linear operations. This process helps avoid
over-confident contrastive learning models during the training and achieves
more transformation-invariant feature embeddings. Remarkably, we empirically
prove that the proposed Hallucinator generalizes well to various contrastive
learning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linear
classification protocol, a stable accuracy gain is achieved, ranging from 0.3%
to 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement is
also observed in transferring pre-train encoders to the downstream tasks,
including object detection and segmentation.",None,-1
c7592ff1-b49b-404f-a677-14910c30e588,Towards accurate instance segmentation in large-scale LiDAR point clouds,0.664284,"Panoptic segmentation is the combination of semantic and instance
segmentation: assign the points in a 3D point cloud to semantic categories and
partition them into distinct object instances. It has many obvious applications
for outdoor scene understanding, from city mapping to forest management.
Existing methods struggle to segment nearby instances of the same semantic
category, like adjacent pieces of street furniture or neighbouring trees, which
limits their usability for inventory- or management-type applications that rely
on object instances. This study explores the steps of the panoptic segmentation
pipeline concerned with clustering points into object instances, with the goal
to alleviate that bottleneck. We find that a carefully designed clustering
strategy, which leverages multiple types of learned point embeddings,
significantly improves instance segmentation. Experiments on the NPM3D urban
mobile mapping dataset and the FOR-instance forest dataset demonstrate the
effectiveness and versatility of the proposed strategy.",None,-1
37229caf-9caf-4ae3-975b-c9b4aee4014f,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,0.709166,"Current Deep Network (DN) visualization and interpretability methods rely
heavily on data space visualizations such as scoring which dimensions of the
data are responsible for their associated prediction or generating new data
features or samples that best match a given DN unit or representation. In this
paper, we go one step further by developing the first provably exact method for
computing the geometry of a DN's mapping - including its decision boundary -
over a specified region of the data space. By leveraging the theory of
Continuous Piece-Wise Linear (CPWL) spline DNs, SplineCam exactly computes a
DNs geometry without resorting to approximations such as sampling or
architecture simplification. SplineCam applies to any DN architecture based on
CPWL nonlinearities, including (leaky-)ReLU, absolute value, maxout, and
max-pooling and can also be applied to regression DNs such as implicit neural
representations. Beyond decision boundary visualization and characterization,
SplineCam enables one to compare architectures, measure generalizability and
sample from the decision boundary on or off the manifold. Project Website:
bit.ly/splinecam.",None,-1
488b060a-ae1c-491b-96bd-e98bb95d28db,Comparison of SAT-based and ASP-based Algorithms for Inconsistency Measurement,0.302334,"We present algorithms based on satisfiability problem (SAT) solving, as well
as answer set programming (ASP), for solving the problem of determining
inconsistency degrees in propositional knowledge bases. We consider six
different inconsistency measures whose respective decision problems lie on the
first level of the polynomial hierarchy. Namely, these are the contension
inconsistency measure, the forgetting-based inconsistency measure, the hitting
set inconsistency measure, the max-distance inconsistency measure, the
sum-distance inconsistency measure, and the hit-distance inconsistency measure.
In an extensive experimental analysis, we compare the SAT-based and ASP-based
approaches with each other, as well as with a set of naive baseline algorithms.
Our results demonstrate that overall, both the SAT-based and the ASP-based
approaches clearly outperform the naive baseline methods in terms of runtime.
The results further show that the proposed ASP-based approaches perform
superior to the SAT-based ones with regard to all six inconsistency measures
considered in this work. Moreover, we conduct additional experiments to explain
the aforementioned results in greater detail.",None,-1
d3c40a43-db00-40aa-9b1e-5fc4fe75b494,A request for clarity over the End of Sequence token in the Self-Critical Sequence Training,0.0643003,"The Image Captioning research field is currently compromised by the lack of
transparency and awareness over the End-of-Sequence token (<Eos>) in the
Self-Critical Sequence Training. If the <Eos> token is omitted, a model can
boost its performance up to +4.1 CIDEr-D using trivial sentence fragments.
While this phenomenon poses an obstacle to a fair evaluation and comparison of
established works, people involved in new projects are given the arduous choice
between lower scores and unsatisfactory descriptions due to the competitive
nature of the research. This work proposes to solve the problem by spreading
awareness of the issue itself. In particular, we invite future works to share a
simple and informative signature with the help of a library called SacreEOS.
Code available at
\emph{\href{https://github.com/jchenghu/sacreeos}{https://github.com/jchenghu/sacreeos}}",None,-1
8940aa1c-0984-4d76-8304-8bf6c2a9d2e4,Large Language Models Can Be Good Privacy Protection Learners,0.810937,"The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.",None,-1
4299e831-a6ac-4f27-ba58-5dcdfd21408e,Scalable 3D Captioning with Pretrained Models,0.902648,"We introduce Cap3D, an automatic approach for generating descriptive text for
3D objects. This approach utilizes pretrained models from image captioning,
image-text alignment, and LLM to consolidate captions from multiple views of a
3D asset, completely side-stepping the time-consuming and costly process of
manual annotation. We apply Cap3D to the recently introduced large-scale 3D
dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted
using 41k human annotations from the same dataset, demonstrates that Cap3D
surpasses human-authored descriptions in terms of quality, cost, and speed.
Through effective prompt engineering, Cap3D rivals human performance in
generating geometric descriptions on 17k collected annotations from the ABO
dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,
and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,
and DreamFusion.",None,-1
9b4351a1-14ff-46ec-b5e9-d65900b9bd0d,Modular Visual Question Answering via Code Generation,0.743643,"We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.",None,-1
2a006c42-1f66-4e12-b452-23a3cedfc0ad,Real-time elastic partial shape matching using a neural network-based adjoint method,0.0593398,"Surface matching usually provides significant deformations that can lead to
structural failure due to the lack of physical policy. In this context, partial
surface matching of non-linear deformable bodies is crucial in engineering to
govern structure deformations. In this article, we propose to formulate the
registration problem as an optimal control problem using an artificial neural
network where the unknown is the surface force distribution that applies to the
object and the resulting deformation computed using a hyper-elastic model. The
optimization problem is solved using an adjoint method where the hyper-elastic
problem is solved using the feed-forward neural network and the adjoint problem
is obtained through the backpropagation of the network. Our process improves
the computation speed by multiple orders of magnitude while providing
acceptable registration errors.",None,-1
b017153a-3e96-42d7-a382-2508901274ca,A Psycholinguistic Analysis of BERT's Representations of Compounds,0.497774,"This work studies the semantic representations learned by BERT for compounds,
that is, expressions such as sunlight or bodyguard. We build on recent studies
that explore semantic information in Transformers at the word level and test
whether BERT aligns with human semantic intuitions when dealing with
expressions (e.g., sunlight) whose overall meaning depends -- to a various
extent -- on the semantics of the constituent words (sun, light). We leverage a
dataset that includes human judgments on two psycholinguistic measures of
compound semantic analysis: lexeme meaning dominance (LMD; quantifying the
weight of each constituent toward the compound meaning) and semantic
transparency (ST; evaluating the extent to which the compound meaning is
recoverable from the constituents' semantics). We show that BERT-based measures
moderately align with human intuitions, especially when using contextualized
representations, and that LMD is overall more predictable than ST. Contrary to
the results reported for 'standard' words, higher, more contextualized layers
are the best at representing compound meaning. These findings shed new light on
the abilities of BERT in dealing with fine-grained semantic phenomena.
Moreover, they can provide insights into how speakers represent compounds.",None,-1
8d79b543-6346-4343-921d-cd488d3b1a0c,Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,0.346874,"Fairness has become increasingly pivotal in medical image recognition.
However, without mitigating bias, deploying unfair medical AI systems could
harm the interests of underprivileged populations. In this paper, we observe
that while features extracted from the deeper layers of neural networks
generally offer higher accuracy, fairness conditions deteriorate as we extract
features from deeper layers. This phenomenon motivates us to extend the concept
of multi-exit frameworks. Unlike existing works mainly focusing on accuracy,
our multi-exit framework is fairness-oriented; the internal classifiers are
trained to be more accurate and fairer, with high extensibility to apply to
most existing fairness-aware frameworks. During inference, any instance with
high confidence from an internal classifier is allowed to exit early.
Experimental results show that the proposed framework can improve the fairness
condition over the state-of-the-art in two dermatological disease datasets.",None,-1
f049a8b3-0904-43d6-b82a-f6772505375b,Region-Aware Portrait Retouching with Sparse Interactive Guidance,0.277925,"Portrait retouching aims to improve the aesthetic quality of input portrait
photos and especially requires human-region priority. The deep learning-based
methods largely elevate the retouching efficiency and provide promising
retouched results. However, existing portrait retouching methods focus on
automatic retouching, which treats all human-regions equally and ignores users'
preferences for specific individuals, thus suffering from limited flexibility
in interactive scenarios. In this work, we emphasize the importance of users'
intents and explore the interactive portrait retouching task. Specifically, we
propose a region-aware retouching framework with two branches: an automatic
branch and an interactive branch. The automatic branch involves an
encoding-decoding process, which searches region candidates and performs
automatic region-aware retouching without user guidance. The interactive branch
encodes sparse user guidance into a priority condition vector and modulates
latent features with a region selection module to further emphasize the
user-specified regions. Experimental results show that our interactive branch
effectively captures users' intents and generalizes well to unseen scenes with
sparse user guidance, while our automatic branch also outperforms the
state-of-the-art retouching methods due to improved region-awareness.",None,-1
43d117b2-b75c-4749-973f-fcc179ceb721,A Comprehensive Study of Object Tracking in Low-Light Environments,0.453206,"Accurate object tracking in low-light environments is crucial, particularly
in surveillance and ethology applications. However, achieving this is
significantly challenging due to the poor quality of captured sequences.
Factors such as noise, color imbalance, and low contrast contribute to these
challenges. This paper presents a comprehensive study examining the impact of
these distortions on automatic object trackers. Additionally, we propose a
solution to enhance tracking performance by integrating denoising and low-light
enhancement methods into the transformer-based object tracking system.
Experimental results show that the proposed tracker, trained with low-light
synthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.",None,-1
583d08c6-89e2-47ec-ae26-7691835672b1,Challenge Results Are Not Reproducible,0.105293,"While clinical trials are the state-of-the-art methods to assess the effect
of new medication in a comparative manner, benchmarking in the field of medical
image analysis is performed by so-called challenges. Recently, comprehensive
analysis of multiple biomedical image analysis challenges revealed large
discrepancies between the impact of challenges and quality control of the
design and reporting standard. This work aims to follow up on these results and
attempts to address the specific question of the reproducibility of the
participants methods. In an effort to determine whether alternative
interpretations of the method description may change the challenge ranking, we
reproduced the algorithms submitted to the 2019 Robust Medical Image
Segmentation Challenge (ROBUST-MIS). The leaderboard differed substantially
between the original challenge and reimplementation, indicating that challenge
rankings may not be sufficiently reproducible.",None,-1
02869338-f8ef-415a-9fac-85398ce6865e,"Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy",0.135951,"How does one measure ""ability to understand language""? If it is a person's
ability that is being measured, this is a question that almost never poses
itself in an unqualified manner: Whatever formal test is applied, it takes
place on the background of the person's language use in daily social practice,
and what is measured is a specialised variety of language understanding (e.g.,
of a second language; or of written, technical language). Computer programs do
not have this background. What does that mean for the applicability of formal
tests of language understanding? I argue that such tests need to be
complemented with tests of language use embedded in a practice, to arrive at a
more comprehensive evaluation of ""artificial language understanding"". To do
such tests systematically, I propose to use ""Dialogue Games"" -- constructed
activities that provide a situational embedding for language use. I describe a
taxonomy of Dialogue Game types, linked to a model of underlying capabilites
that are tested, and thereby giving an argument for the \emph{construct
validity} of the test. I close with showing how the internal structure of the
taxonomy suggests an ordering from more specialised to more general situational
language understanding, which potentially can provide some strategic guidance
for development in this field.",None,-1
260d7446-76d7-40ab-ae04-fa1b0bfc36be,Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation,0.857866,"Language models have achieved impressive performances on dialogue generation
tasks. However, when generating responses for a conversation that requires
factual knowledge, they are far from perfect, due to an absence of mechanisms
to retrieve, encode, and reflect the knowledge in the generated responses. Some
knowledge-grounded dialogue generation methods tackle this problem by
leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee
that the model utilizes a relevant piece of knowledge from the KG. To overcome
this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a
framework for generating context-relevant and knowledge-grounded dialogues with
the KG. Specifically, our SURGE framework first retrieves the relevant subgraph
from the KG, and then enforces consistency across facts by perturbing their
word embeddings conditioned by the retrieved subgraph. Then, we utilize
contrastive learning to ensure that the generated texts have high similarity to
the retrieved subgraphs. We validate our SURGE framework on OpendialKG and
KOMODIS datasets, showing that it generates high-quality dialogues that
faithfully reflect the knowledge from KG.",None,-1
2dc8554b-2c4e-47be-9785-50b959398a89,Document-level Relation Extraction with Cross-sentence Reasoning Graph,0.844489,"Relation extraction (RE) has recently moved from the sentence-level to
document-level, which requires aggregating document information and using
entities and mentions for reasoning. Existing works put entity nodes and
mention nodes with similar representations in a document-level graph, whose
complex edges may incur redundant information. Furthermore, existing studies
only focus on entity-level reasoning paths without considering global
interactions among entities cross-sentence. To these ends, we propose a novel
document-level RE model with a GRaph information Aggregation and Cross-sentence
Reasoning network (GRACR). Specifically, a simplified document-level graph is
constructed to model the semantic information of all mentions and sentences in
a document, and an entity-level graph is designed to explore relations of
long-distance cross-sentence entity pairs. Experimental results show that GRACR
achieves excellent performance on two public datasets of document-level RE. It
is especially effective in extracting potential relations of cross-sentence
entity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.",None,-1
a8b1ea0e-3407-4dc4-848c-1272e3811fc0,LLaMA: Open and Efficient Foundation Language Models,1.0,"We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.",None,-1
2056718d-5f58-46a6-ba60-4bc0490edd04,Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?,0.917292,"Making moral judgments is an essential step toward developing ethical AI
systems. Prevalent approaches are mostly implemented in a bottom-up manner,
which uses a large set of annotated data to train models based on crowd-sourced
opinions about morality. These approaches have been criticized for
overgeneralizing the moral stances of a limited group of annotators and lacking
explainability. This work proposes a flexible top-down framework to steer
(Large) Language Models (LMs) to perform moral reasoning with well-established
moral theories from interdisciplinary research. The theory-guided top-down
framework can incorporate various moral theories. Our experiments demonstrate
the effectiveness of the proposed framework on datasets derived from moral
theories. Furthermore, we show the alignment between different moral theories
and existing morality datasets. Our analysis exhibits the potential and flaws
in existing resources (models and datasets) in developing explainable moral
judgment-making systems.",None,-1
20c8d239-4b11-4a1c-877f-00c0e4a32a84,Benchmarking LLM-based Machine Translation on Cultural Awareness,0.770707,"Translating cultural-specific content is crucial for effective cross-cultural
communication. However, many MT systems still struggle to translate sentences
containing cultural-specific entities accurately and understandably. Recent
advancements in in-context learning utilize lightweight prompts to guide large
language models (LLMs) in machine translation tasks. Nevertheless, the
effectiveness of this approach in enhancing machine translation with cultural
awareness remains uncertain. To address this gap, we introduce a new data
curation pipeline to construct a culturally relevant parallel corpus, enriched
with annotations of cultural-specific items. Furthermore, we devise a novel
evaluation metric to assess the understandability of translations in a
reference-free manner by GPT-4. We evaluate a variety of neural machine
translation (NMT) and LLM-based MT systems using our dataset. Additionally, we
propose several prompting strategies for LLMs to incorporate external and
internal cultural knowledge into the translation process. Our results
demonstrate that eliciting explanations can significantly enhance the
understandability of cultural-specific entities, especially those without
well-known translations.",None,-1
da0beb60-3312-4618-913c-d82049ce5372,AI Security Threats against Pervasive Robotic Systems: A Course for Next Generation Cybersecurity Workforce,0.0930048,"Robotics, automation, and related Artificial Intelligence (AI) systems have
become pervasive bringing in concerns related to security, safety, accuracy,
and trust. With growing dependency on physical robots that work in close
proximity to humans, the security of these systems is becoming increasingly
important to prevent cyber-attacks that could lead to privacy invasion,
critical operations sabotage, and bodily harm. The current shortfall of
professionals who can defend such systems demands development and integration
of such a curriculum. This course description includes details about seven
self-contained and adaptive modules on ""AI security threats against pervasive
robotic systems"". Topics include: 1) Introduction, examples of attacks, and
motivation; 2) - Robotic AI attack surfaces and penetration testing; 3) -
Attack patterns and security strategies for input sensors; 4) - Training
attacks and associated security strategies; 5) - Inference attacks and
associated security strategies; 6) - Actuator attacks and associated security
strategies; and 7) - Ethics of AI, robotics, and cybersecurity.",None,-1
7c69bb95-2383-4d25-9d86-f789401e7d04,ChatHaruhi: Reviving Anime Character in Reality via Large Language Model,0.658915,"Role-playing chatbots built on large language models have drawn interest, but
better techniques are needed to enable mimicking specific fictional characters.
We propose an algorithm that controls language models via an improved prompt
and memories of the character extracted from scripts. We construct ChatHaruhi,
a dataset covering 32 Chinese / English TV / anime characters with over 54k
simulated dialogues. Both automatic and human evaluations show our approach
improves role-playing ability over baselines. Code and data are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya .",None,-1
3683f89b-9243-4747-a229-efe30a79592b,Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the LIAR Dataset,0.844836,"The detection of political fake statements is crucial for maintaining
information integrity and preventing the spread of misinformation in society.
Historically, state-of-the-art machine learning models employed various methods
for detecting deceptive statements. These methods include the use of metadata
(W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic
(Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent
advancements in large language models, such as GPT-3 (Brown et al., 2020) have
achieved state-of-the-art performance on a wide range of tasks. In this study,
we conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018)
and achieved higher accuracy than state-of-the-art models without using any
additional meta or linguistic features. Additionally, we experimented with
zero-shot learning using a carefully designed prompt and achieved near
state-of-the-art performance. An advantage of this approach is that the model
provided evidence for its decision, which adds transparency to the model's
decision-making and offers a chance for users to verify the validity of the
evidence provided.",None,-1
461ed1fd-54d0-41b1-b59e-acf92ab69976,Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation,0.596777,"We present a new loss function for joint disparity and uncertainty estimation
in deep stereo matching. Our work is motivated by the need for precise
uncertainty estimates and the observation that multi-task learning often leads
to improved performance in all tasks. We show that this can be achieved by
requiring the distribution of uncertainty to match the distribution of
disparity errors via a KL divergence term in the network's loss function. A
differentiable soft-histogramming technique is used to approximate the
distributions so that they can be used in the loss. We experimentally assess
the effectiveness of our approach and observe significant improvements in both
disparity and uncertainty prediction on large datasets.",None,-1
c2ae0a79-fde6-4dec-a805-14f43a7a25b6,Integrating Large Pre-trained Models into Multimodal Named Entity Recognition with Evidential Fusion,0.411278,"Multimodal Named Entity Recognition (MNER) is a crucial task for information
extraction from social media platforms such as Twitter. Most current methods
rely on attention weights to extract information from both text and images but
are often unreliable and lack interpretability. To address this problem, we
propose incorporating uncertainty estimation into the MNER task, producing
trustworthy predictions. Our proposed algorithm models the distribution of each
modality as a Normal-inverse Gamma distribution, and fuses them into a unified
distribution with an evidential fusion mechanism, enabling hierarchical
characterization of uncertainties and promotion of prediction accuracy and
trustworthiness. Additionally, we explore the potential of pre-trained large
foundation models in MNER and propose an efficient fusion approach that
leverages their robust feature representations. Experiments on two datasets
demonstrate that our proposed method outperforms the baselines and achieves new
state-of-the-art performance.",None,-1
c83e8d8f-309c-4bd8-96b7-fba99dabaca4,Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models,0.625143,"We address the challenge of ensuring differential privacy (DP) guarantees in
training deep retrieval systems. Training these systems often involves the use
of contrastive-style losses, which are typically non-per-example decomposable,
making them difficult to directly DP-train with since common techniques require
per-example gradients. To address this issue, we propose an approach that
prioritizes ensuring query privacy prior to training a deep retrieval system.
Our method employs DP language models (LMs) to generate private synthetic
queries representative of the original data. These synthetic queries can be
used in downstream retrieval system training without compromising privacy. Our
approach demonstrates a significant enhancement in retrieval quality compared
to direct DP-training, all while maintaining query-level privacy guarantees.
This work highlights the potential of harnessing LMs to overcome limitations in
standard DP-training methods.",None,-1
75ec088b-a936-4299-a64c-af207d1e28a1,CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations,0.910751,"Geo-tagged images are publicly available in large quantities, whereas labels
such as object classes are rather scarce and expensive to collect. Meanwhile,
contrastive learning has achieved tremendous success in various natural image
and language tasks with limited labeled data. However, existing methods fail to
fully leverage geospatial information, which can be paramount to distinguishing
objects that are visually similar. To directly leverage the abundant geospatial
information associated with images in pre-training, fine-tuning, and inference
stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised
learning framework for geo-tagged images. We use a dual-encoder to separately
encode the images and their corresponding geo-locations, and use contrastive
objectives to learn effective location representations from images, which can
be transferred to downstream supervised tasks such as image classification.
Experiments show that CSP can improve model performance on both iNat2018 and
fMoW datasets. Especially, on iNat2018, CSP significantly boosts the model
performance with 10-34% relative improvement with various labeled training data
sampling ratios.",None,-1
5d75ba91-88c5-41d5-9666-fd09783b81a5,Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills,0.057652,"Autonomously learning diverse behaviors without an extrinsic reward signal
has been a problem of interest in reinforcement learning. However, the nature
of learning in such mechanisms is unconstrained, often resulting in the
accumulation of several unusable, unsafe or misaligned skills. In order to
avoid such issues and ensure the discovery of safe and human-aligned skills, it
is necessary to incorporate humans into the unsupervised training process,
which remains a largely unexplored research area. In this work, we propose
Controlled Diversity with Preference (CDP), a novel, collaborative human-guided
mechanism for an agent to learn a set of skills that is diverse as well as
desirable. The key principle is to restrict the discovery of skills to those
regions that are deemed to be desirable as per a preference model trained using
human preference labels on trajectory pairs. We evaluate our approach on 2D
navigation and Mujoco environments and demonstrate the ability to discover
diverse, yet desirable skills.",None,-1
51dc8d88-d51c-401c-b62b-e1079c07ea1e,To Risk or Not to Risk: Learning with Risk Quantification for IoT Task Offloading in UAVs,0.0915583,"A deep reinforcement learning technique is presented for task offloading
decision-making algorithms for a multi-access edge computing (MEC) assisted
unmanned aerial vehicle (UAV) network in a smart farm Internet of Things (IoT)
environment. The task offloading technique uses financial concepts such as cost
functions and conditional variable at risk (CVaR) in order to quantify the
damage that may be caused by each risky action. The approach was able to
quantify potential risks to train the reinforcement learning agent to avoid
risky behaviors that will lead to irreversible consequences for the farm. Such
consequences include an undetected fire, pest infestation, or a UAV being
unusable. The proposed CVaR-based technique was compared to other deep
reinforcement learning techniques and two fixed rule-based techniques. The
simulation results show that the CVaR-based risk quantifying method eliminated
the most dangerous risk, which was exceeding the deadline for a fire detection
task. As a result, it reduced the total number of deadline violations with a
negligible increase in energy consumption.",None,-1
eb3626cc-df8f-4857-9f12-52aad1ebca59,Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation,0.255173,"Current reinforcement learning (RL) often suffers when solving a challenging
exploration problem where the desired outcomes or high rewards are rarely
observed. Even though curriculum RL, a framework that solves complex tasks by
proposing a sequence of surrogate tasks, shows reasonable results, most of the
previous works still have difficulty in proposing curriculum due to the absence
of a mechanism for obtaining calibrated guidance to the desired outcome state
without any prior domain knowledge. To alleviate it, we propose an uncertainty
& temporal distance-aware curriculum goal generation method for the
outcome-directed RL via solving a bipartite matching problem. It could not only
provide precisely calibrated guidance of the curriculum to the desired outcome
states but also bring much better sample efficiency and geometry-agnostic
curriculum goal proposal capability compared to previous curriculum RL methods.
We demonstrate that our algorithm significantly outperforms these prior methods
in a variety of challenging navigation tasks and robotic manipulation tasks in
a quantitative and qualitative way.",None,-1
90767c75-6736-45d4-ac00-4ec1ce684a52,Teach LLMs to Personalize -- An Approach inspired by Writing Education,0.2724,"Personalized text generation is an emerging research area that has attracted
much attention in recent years. Most studies in this direction focus on a
particular domain by designing bespoke features or models. In this work, we
propose a general approach for personalized text generation using large
language models (LLMs). Inspired by the practice of writing education, we
develop a multistage and multitask framework to teach LLMs for personalized
generation. In writing instruction, the task of writing from sources is often
decomposed into multiple steps that involve finding, evaluating, summarizing,
synthesizing, and integrating information. Analogously, our approach to
personalized text generation consists of multiple stages: retrieval, ranking,
summarization, synthesis, and generation. In addition, we introduce a multitask
setting that helps the model improve its generation ability further, which is
inspired by the observation in education that a student's reading proficiency
and writing ability are often correlated. We evaluate our approach on three
public datasets, each of which covers a different and representative domain.
Our results show significant improvements over a variety of baselines.",None,-1
4809ff85-14e1-4cc6-8f57-7ec5ac6f0ee9,SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration,0.280043,"The potential social harms that large language models pose, such as
generating offensive content and reinforcing biases, are steeply rising.
Existing works focus on coping with this concern while interacting with
ill-intentioned users, such as those who explicitly make hate speech or elicit
harmful responses. However, discussions on sensitive issues can become toxic
even if the users are well-intentioned. For safer models in such scenarios, we
present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a
large-scale Korean dataset of 49k sensitive questions with 42k acceptable and
46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA
in a human-in-the-loop manner based on real news headlines. Experiments show
that acceptable response generation significantly improves for HyperCLOVA and
GPT-3, demonstrating the efficacy of this dataset.",None,-1
8f5bc5f8-6eec-4eaf-a137-c4fd7dbab17f,Deep Clustering Survival Machines with Interpretable Expert Distributions,0.121201,"Conventional survival analysis methods are typically ineffective to
characterize heterogeneity in the population while such information can be used
to assist predictive modeling. In this study, we propose a hybrid survival
analysis method, referred to as deep clustering survival machines, that
combines the discriminative and generative mechanisms. Similar to the mixture
models, we assume that the timing information of survival data is generatively
described by a mixture of certain numbers of parametric distributions, i.e.,
expert distributions. We learn weights of the expert distributions for
individual instances according to their features discriminatively such that
each instance's survival information can be characterized by a weighted
combination of the learned constant expert distributions. This method also
facilitates interpretable subgrouping/clustering of all instances according to
their associated expert distributions. Extensive experiments on both real and
synthetic datasets have demonstrated that the method is capable of obtaining
promising clustering results and competitive time-to-event predicting
performance.",None,-1
9d94a2c8-390b-4298-8a15-64d83dd87225,RLTF: Reinforcement Learning from Unit Test Feedback,0.96901,"The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.",None,-1
fbab80d3-cc62-4149-b359-49fa04f864ff,Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks,0.882324,"Arguments often do not make explicit how a conclusion follows from its
premises. To compensate for this lack, we enrich arguments with structured
background knowledge to support knowledge-intense argumentation tasks. We
present a new unsupervised method for constructing Contextualized Commonsense
Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from
large knowledge graphs (KGs) efficiently and at high quality. Our work goes
beyond context-insensitive knowledge extraction heuristics by computing
semantic similarity between KG triplets and textual arguments. Using these
triplet similarities as weights, we extract contextualized knowledge paths that
connect a conclusion to its premise, while maximizing similarity to the
argument. We combine multiple paths into a CCKG that we optionally prune to
reduce noise and raise precision. Intrinsic evaluation of the quality of our
graphs shows that our method is effective for (re)constructing human
explanation graphs. Manual evaluations in a large-scale knowledge selection
setup confirm high recall and precision of implicit CSK in the CCKGs. Finally,
we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument
quality rating task, outperforming strong baselines and rivaling a GPT-3 based
system.",None,-1
80ce33b3-ac87-4344-b8a4-aaf009e53442,Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT,0.883215,"In this paper, we aimed to provide a review and tutorial for researchers in
the field of medical imaging using language models to improve their tasks at
hand. We began by providing an overview of the history and concepts of language
models, with a special focus on large language models. We then reviewed the
current literature on how language models are being used to improve medical
imaging, emphasizing different applications such as image captioning, report
generation, report classification, finding extraction, visual question
answering, interpretable diagnosis, and more for various modalities and organs.
The ChatGPT was specially highlighted for researchers to explore more potential
applications. We covered the potential benefits of accurate and efficient
language models for medical imaging analysis, including improving clinical
workflow efficiency, reducing diagnostic errors, and assisting healthcare
professionals in providing timely and accurate diagnoses. Overall, our goal was
to bridge the gap between language models and medical imaging and inspire new
ideas and innovations in this exciting area of research. We hope that this
review paper will serve as a useful resource for researchers in this field and
encourage further exploration of the possibilities of language models in
medical imaging.",None,-1
d81c8979-fa7c-4e82-b2a9-f66da55baca3,Robustness of Segment Anything Model (SAM) for Autonomous Driving in Adverse Weather Conditions,0.289185,"Segment Anything Model (SAM) has gained considerable interest in recent times
for its remarkable performance and has emerged as a foundational model in
computer vision. It has been integrated in diverse downstream tasks, showcasing
its strong zero-shot transfer capabilities. Given its impressive performance,
there is a strong desire to apply SAM in autonomous driving to improve the
performance of vision tasks, particularly in challenging scenarios such as
driving under adverse weather conditions. However, its robustness under adverse
weather conditions remains uncertain. In this work, we investigate the
application of SAM in autonomous driving and specifically explore its
robustness under adverse weather conditions. Overall, this work aims to enhance
understanding of SAM's robustness in challenging scenarios before integrating
it into autonomous driving vision tasks, providing valuable insights for future
applications.",None,-1
75fa7142-2f8f-4654-8c5a-70cf05bdb758,Experimental results from applying GPT-4 to an unpublished formal language,0.210909,"Can large language models be used to complete mathematical tasks that are
traditionally performed either manually or with the aid of theorem provers? To
answer this question, a state-of-the-art system, GPT-4, was provided with a
concise natural language specification for a previously unpublished formal
system and asked to complete a number of tasks, from stating function and type
definitions to proving simple theorems and verifying user-supplied proofs. The
system completed all tasks successfully, showed extensive domain knowledge,
invented helpful new syntax and semantics, and exhibited generalization and
inference abilities. So the answer seems to be: yes.",None,-1
7cecc8d2-ebce-4827-a889-7aa7f6f117a4,Learning to Upsample by Learning to Sample,0.326454,"We present DySample, an ultra-lightweight and effective dynamic upsampler.
While impressive performance gains have been witnessed from recent kernel-based
dynamic upsamplers such as CARAFE, FADE, and SAPA, they introduce much
workload, mostly due to the time-consuming dynamic convolution and the
additional sub-network used to generate dynamic kernels. Further, the need for
high-res feature guidance of FADE and SAPA somehow limits their application
scenarios. To address these concerns, we bypass dynamic convolution and
formulate upsampling from the perspective of point sampling, which is more
resource-efficient and can be easily implemented with the standard built-in
function in PyTorch. We first showcase a naive design, and then demonstrate how
to strengthen its upsampling behavior step by step towards our new upsampler,
DySample. Compared with former kernel-based dynamic upsamplers, DySample
requires no customized CUDA package and has much fewer parameters, FLOPs, GPU
memory, and latency. Besides the light-weight characteristics, DySample
outperforms other upsamplers across five dense prediction tasks, including
semantic segmentation, object detection, instance segmentation, panoptic
segmentation, and monocular depth estimation. Code is available at
https://github.com/tiny-smart/dysample.",None,-1
6b259d43-9a1e-4ee6-8123-e7d4be6be9a7,High-Fidelity Zero-Shot Texture Anomaly Localization Using Feature Correspondence Analysis,0.432781,"We propose a novel method for Zero-Shot Anomaly Localization on textures. The
task refers to identifying abnormal regions in an otherwise homogeneous image.
To obtain a high-fidelity localization, we leverage a bijective mapping derived
from the 1-dimensional Wasserstein Distance. As opposed to using holistic
distances between distributions, the proposed approach allows pinpointing the
non-conformity of a pixel in a local context with increased precision. By
aggregating the contribution of the pixel to the errors of all nearby patches
we obtain a reliable anomaly score estimate. We validate our solution on
several datasets and obtain more than a 40% reduction in error over the
previous state of the art on the MVTec AD dataset in a zero-shot setting. Also
see https://reality.tf.fau.de/pub/ardelean2024highfidelity.html.",None,-1
e0842ac5-ecec-404d-a23d-2b214b2e98e9,Target-Side Augmentation for Document-Level Machine Translation,0.696954,"Document-level machine translation faces the challenge of data sparsity due
to its long input length and a small amount of training data, increasing the
risk of learning spurious patterns. To address this challenge, we propose a
target-side augmentation method, introducing a data augmentation (DA) model to
generate many potential translations for each source document. Learning on
these wider range translations, an MT model can learn a smoothed distribution,
thereby reducing the risk of data sparsity. We demonstrate that the DA model,
which estimates the posterior distribution, largely improves the MT
performance, outperforming the previous best system by 2.30 s-BLEU on News and
achieving new state-of-the-art on News and Europarl benchmarks. Our code is
available at https://github.com/baoguangsheng/target-side-augmentation.",None,-1
8bd97c80-432e-475d-aa0e-756eab15e845,Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation,0.202354,"Unsupervised domain adaptation uses source data from different distributions
to solve the problem of classifying data from unlabeled target domains.
However, conventional methods require access to source data, which often raise
concerns about data privacy. In this paper, we consider a more practical but
challenging setting where the source domain data is unavailable and the target
domain data is unlabeled. Specifically, we address the domain discrepancy
problem from the perspective of contrastive learning. The key idea of our work
is to learn a domain-invariant feature by 1) performing clustering directly in
the original feature space with nearest neighbors; 2) constructing truly hard
negative pairs by extended neighbors without introducing additional
computational complexity; and 3) combining noise-contrastive estimation theory
to gain computational advantage. We conduct careful ablation studies and
extensive experiments on three common benchmarks: VisDA, Office-Home, and
Office-31. The results demonstrate the superiority of our methods compared with
other state-of-the-art works.",None,-1
a6456068-ab0b-4a3e-adb1-01b807981eba,CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders,0.407268,"Large-scale self-supervised pre-trained speech encoders outperform
conventional approaches in speech recognition and translation tasks. Due to the
high cost of developing these large models, building new encoders for new tasks
and deploying them to on-device applications are infeasible. Prior studies
propose model compression methods to address this issue, but those works focus
on smaller models and less realistic tasks. Thus, we propose Contrastive
Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to
compress pre-trained speech encoders by leveraging masked prediction and
contrastive learning to train student models to copy the behavior of a large
teacher model. CoLLD outperforms prior methods and closes the gap between small
and large models on multilingual speech-to-text translation and recognition
benchmarks.",None,-1
bfb3a714-fb8a-4de4-a9cf-4370a3e51e64,Causal Abstraction for Faithful Model Interpretation,0.937303,"A faithful and interpretable explanation of an AI model's behavior and
internal structure is a high-level explanation that is human-intelligible but
also consistent with the known, but often opaque low-level causal details of
the model. We argue that the theory of causal abstraction provides the
mathematical foundations for the desired kinds of model explanations. In causal
abstraction analysis, we use interventions on model-internal states to
rigorously assess whether an interpretable high-level causal model is a
faithful description of an AI model. Our contributions in this area are: (1) We
generalize causal abstraction to cyclic causal structures and typed high-level
variables. (2) We show how multi-source interchange interventions can be used
to conduct causal abstraction analyses. (3) We define a notion of approximate
causal abstraction that allows us to assess the degree to which a high-level
causal model is a causal abstraction of a lower-level one. (4) We prove
constructive causal abstraction can be decomposed into three operations we
refer to as marginalization, variable-merge, and value-merge. (5) We formalize
the XAI methods of LIME, causal effect estimation, causal mediation analysis,
iterated nullspace projection, and circuit-based explanations as special cases
of causal abstraction analysis.",None,-1
7c389077-f227-495d-808a-a23806806060,NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis,0.605777,"This paper describes our system developed for the SemEval-2023 Task 12
""Sentiment Analysis for Low-resource African Languages using Twitter Dataset"".
Sentiment analysis is one of the most widely studied applications in natural
language processing. However, most prior work still focuses on a small number
of high-resource languages. Building reliable sentiment analysis systems for
low-resource languages remains challenging, due to the limited training data in
this task. In this work, we propose to leverage language-adaptive and
task-adaptive pretraining on African texts and study transfer learning with
source language selection on top of an African language-centric pretrained
language model. Our key findings are: (1) Adapting the pretrained model to the
target language and task using a small yet relevant corpus improves performance
remarkably by more than 10 F1 score points. (2) Selecting source languages with
positive transfer gains during training can avoid harmful interference from
dissimilar languages, leading to better results in multilingual and
cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
and, in particular, performs best in the multilingual evaluation.",None,-1
412790f0-d139-4c02-81d7-ca17b396a540,CoReFace: Sample-Guided Contrastive Regularization for Deep Face Recognition,0.588239,"The discriminability of feature representation is the key to open-set face
recognition. Previous methods rely on the learnable weights of the
classification layer that represent the identities. However, the evaluation
process learns no identity representation and drops the classifier from
training. This inconsistency could confuse the feature encoder in understanding
the evaluation goal and hinder the effect of identity-based methods. To
alleviate the above problem, we propose a novel approach namely Contrastive
Regularization for Face recognition (CoReFace) to apply image-level
regularization in feature representation learning. Specifically, we employ
sample-guided contrastive learning to regularize the training with the
image-image relationship directly, which is consistent with the evaluation
process. To integrate contrastive learning into face recognition, we augment
embeddings instead of images to avoid the image quality degradation. Then, we
propose a novel contrastive loss for the representation distribution by
incorporating an adaptive margin and a supervised contrastive mask to generate
steady loss values and avoid the collision with the classification supervision
signal. Finally, we discover and solve the semantically repetitive signal
problem in contrastive learning by exploring new pair coupling protocols.
Extensive experiments demonstrate the efficacy and efficiency of our CoReFace
which is highly competitive with the state-of-the-art approaches.",None,-1
d155021b-163b-4ea4-86a1-b7e5538ce3be,Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification,0.471349,"Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuning
techniques designed to make the training of language models more efficient.
Previous results demonstrated that these methods can even improve performance
on some classification tasks. This paper complements the existing research by
investigating how these techniques influence the classification performance and
computation costs compared to full fine-tuning when applied to multilingual
text classification tasks (genre, framing, and persuasion techniques detection;
with different input lengths, number of predicted classes and classification
difficulty), some of which have limited training data. In addition, we conduct
in-depth analyses of their efficacy across different training scenarios
(training on the original multilingual data; on the translations into English;
and on a subset of English-only data) and different languages. Our findings
provide valuable insights into the applicability of the parameter-efficient
fine-tuning techniques, particularly to complex multilingual and multilabel
classification tasks.",None,-1
006bf871-9402-4367-a92a-1833520815a9,PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow,0.807914,"During industrial processing, unforeseen defects may arise in products due to
uncontrollable factors. Although unsupervised methods have been successful in
defect localization, the usual use of pre-trained models results in
low-resolution outputs, which damages visual performance. To address this
issue, we propose PyramidFlow, the first fully normalizing flow method without
pre-trained models that enables high-resolution defect localization.
Specifically, we propose a latent template-based defect contrastive
localization paradigm to reduce intra-class variance, as the pre-trained models
do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for
multi-scale fusing and volume normalization to help generalization. Our
comprehensive studies on MVTecAD demonstrate the proposed method outperforms
the comparable algorithms that do not use external priors, even achieving
state-of-the-art performance in more challenging BTAD scenarios.",None,-1
2f3ed641-9132-4c0c-840f-00a5ec0887fc,Is Knowledge All Large Language Models Needed for Causal Reasoning?,0.182083,"This paper explores the causal reasoning of large language models (LLMs) to
enhance their interpretability and reliability in advancing artificial
intelligence. Despite the proficiency of LLMs in a range of tasks, their
potential for understanding causality requires further exploration. We propose
a novel causal attribution model that utilizes ``do-operators"" for constructing
counterfactual scenarios, allowing us to systematically quantify the influence
of input numerical data and LLMs' pre-existing knowledge on their causal
reasoning processes. Our newly developed experimental setup assesses LLMs'
reliance on contextual information and inherent knowledge across various
domains. Our evaluation reveals that LLMs' causal reasoning ability mainly
depends on the context and domain-specific knowledge provided. In the absence
of such knowledge, LLMs can still maintain a degree of causal reasoning using
the available numerical data, albeit with limitations in the calculations. This
motivates the proposed fine-tuned LLM for pairwise causal discovery,
effectively leveraging both knowledge and numerical information.",None,-1
80c875c6-c53a-4360-b50d-37f61e48f0ac,HAT-GAE: Self-Supervised Graph Auto-encoders with Hierarchical Adaptive Masking and Trainable Corruption,0.0863891,"Self-supervised auto-encoders have emerged as a successful framework for
representation learning in computer vision and natural language processing in
recent years, However, their application to graph data has been met with
limited performance due to the non-Euclidean and complex structure of graphs in
comparison to images or text, as well as the limitations of conventional
auto-encoder architectures. In this paper, we investigate factors impacting the
performance of auto-encoders on graph data and propose a novel auto-encoder
model for graph representation learning. Our model incorporates a hierarchical
adaptive masking mechanism to incrementally increase the difficulty of training
in order to mimic the process of human cognitive learning, and a trainable
corruption scheme to enhance the robustness of learned representations. Through
extensive experimentation on ten benchmark datasets, we demonstrate the
superiority of our proposed method over state-of-the-art graph representation
learning models.",None,-1
1941b5f2-eaf4-4221-9a05-30be3c033c7d,Efficient Guided Generation for Large Language Models,0.231362,"In this article we show how the problem of neural text generation can be
constructively reformulated in terms of transitions between the states of a
finite-state machine. This framework leads to an efficient approach to guiding
text generation with regular expressions and context-free grammars by allowing
the construction of an index over a language model's vocabulary. The approach
is model agnostic, allows one to enforce domain-specific knowledge and
constraints, and enables the construction of reliable interfaces by
guaranteeing the structure of the generated text. It adds little overhead to
the token sequence generation process and significantly outperforms existing
solutions. An implementation is provided in the open source Python library
Outlines",None,-1
1add42ab-5ec1-4085-a0f7-a38fd559bf2b,Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification,0.451894,"Due to the expensive costs of collecting labels in multi-label classification
datasets, partially annotated multi-label classification has become an emerging
field in computer vision. One baseline approach to this task is to assume
unobserved labels as negative labels, but this assumption induces label noise
as a form of false negative. To understand the negative impact caused by false
negative labels, we study how these labels affect the model's explanation. We
observe that the explanation of two models, trained with full and partial
labels each, highlights similar regions but with different scaling, where the
latter tends to have lower attribution scores. Based on these findings, we
propose to boost the attribution scores of the model trained with partial
labels to make its explanation resemble that of the model trained with full
labels. Even with the conceptually simple approach, the multi-label
classification performance improves by a large margin in three different
datasets on a single positive label setting and one on a large-scale partial
label setting. Code is available at
https://github.com/youngwk/BridgeGapExplanationPAMC.",None,-1
4f45a587-6819-4363-8eeb-9d193b769ffd,HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining,0.734559,"Human-centric perceptions include a variety of vision tasks, which have
widespread industrial applications, including surveillance, autonomous driving,
and the metaverse. It is desirable to have a general pretrain model for
versatile human-centric downstream tasks. This paper forges ahead along this
path from the aspects of both benchmark and pretraining methods. Specifically,
we propose a \textbf{HumanBench} based on existing datasets to comprehensively
evaluate on the common ground the generalization abilities of different
pretraining methods on 19 datasets from 6 diverse downstream tasks, including
person ReID, pose estimation, human parsing, pedestrian attribute recognition,
pedestrian detection, and crowd counting. To learn both coarse-grained and
fine-grained knowledge in human bodies, we further propose a \textbf{P}rojector
\textbf{A}ssis\textbf{T}ed \textbf{H}ierarchical pretraining method
(\textbf{PATH}) to learn diverse knowledge at different granularity levels.
Comprehensive evaluations on HumanBench show that our PATH achieves new
state-of-the-art results on 17 downstream datasets and on-par results on the
other 2 datasets. The code will be publicly at
\href{https://github.com/OpenGVLab/HumanBench}{https://github.com/OpenGVLab/HumanBench}.",None,-1
e92c95fd-fbe7-43c4-8ab2-982b53b918f2,WISE: full-Waveform variational Inference via Subsurface Extensions,0.997912,"We introduce a probabilistic technique for full-waveform inversion, employing
variational inference and conditional normalizing flows to quantify uncertainty
in migration-velocity models and its impact on imaging. Our approach integrates
generative artificial intelligence with physics-informed common-image gathers,
reducing reliance on accurate initial velocity models. Considered case studies
demonstrate its efficacy producing realizations of migration-velocity models
conditioned by the data. These models are used to quantify amplitude and
positioning effects during subsequent imaging.",None,-1
abe9bd59-352e-4cd0-927a-9210acf53ddb,Neural Microfacet Fields for Inverse Rendering,0.344812,"We present Neural Microfacet Fields, a method for recovering materials,
geometry, and environment illumination from images of a scene. Our method uses
a microfacet reflectance model within a volumetric setting by treating each
sample along the ray as a (potentially non-opaque) surface. Using surface-based
Monte Carlo rendering in a volumetric setting enables our method to perform
inverse rendering efficiently by combining decades of research in surface-based
light transport with recent advances in volume rendering for view synthesis.
Our approach outperforms prior work in inverse rendering, capturing high
fidelity geometry and high frequency illumination details; its novel view
synthesis results are on par with state-of-the-art methods that do not recover
illumination or materials.",None,-1
2cced6ad-d68d-43cb-8848-771d1f3e11d9,FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model,0.677035,"Recently, conditional diffusion models have gained popularity in numerous
applications due to their exceptional generation ability. However, many
existing methods are training-required. They need to train a time-dependent
classifier or a condition-dependent score estimator, which increases the cost
of constructing conditional diffusion models and is inconvenient to transfer
across different conditions. Some current works aim to overcome this limitation
by proposing training-free solutions, but most can only be applied to a
specific category of tasks and not to more general conditions. In this work, we
propose a training-Free conditional Diffusion Model (FreeDoM) used for various
conditions. Specifically, we leverage off-the-shelf pre-trained networks, such
as a face detection model, to construct time-independent energy functions,
which guide the generation process without requiring training. Furthermore,
because the construction of the energy function is very flexible and adaptable
to various conditions, our proposed FreeDoM has a broader range of applications
than existing training-free methods. FreeDoM is advantageous in its simplicity,
effectiveness, and low cost. Experiments demonstrate that FreeDoM is effective
for various conditions and suitable for diffusion models of diverse data
domains, including image and latent code domains.",None,-1
2e596461-0b09-44b0-bfb0-0af32cb61d74,An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8,0.513535,"Qualitative Spatial Reasoning (QSR) is well explored area of Commonsense
Reasoning and has multiple applications ranging from Geographical Information
Systems to Robotics and Computer Vision. Recently many claims have been made
for the capabilities of Large Language Models (LLMs). In this paper we
investigate the extent to which one particular LLM can perform classical
qualitative spatial reasoning tasks on the mereotopological calculus, RCC-8.",None,-1
3f66c5bc-4d45-4114-a18a-e462da9de5bc,My Actions Speak Louder Than Your Words: When User Behavior Predicts Their Beliefs about Agents' Attributes,0.142962,"An implicit expectation of asking users to rate agents, such as an AI
decision-aid, is that they will use only relevant information -- ask them about
an agent's benevolence, and they should consider whether or not it was kind.
Behavioral science, however, suggests that people sometimes use irrelevant
information. We identify an instance of this phenomenon, where users who
experience better outcomes in a human-agent interaction systematically rated
the agent as having better abilities, being more benevolent, and exhibiting
greater integrity in a post hoc assessment than users who experienced worse
outcome -- which were the result of their own behavior -- with the same agent.
Our analyses suggest the need for augmentation of models so that they account
for such biased perceptions as well as mechanisms so that agents can detect and
even actively work to correct this and similar biases of users.",None,-1
b96ed7e3-2cb9-40cf-905d-3c214ff4fe6f,"HealthEdge: A Machine Learning-Based Smart Healthcare Framework for Prediction of Type 2 Diabetes in an Integrated IoT, Edge, and Cloud Computing System",0.725276,"Diabetes Mellitus has no permanent cure to date and is one of the leading
causes of death globally. The alarming increase in diabetes calls for the need
to take precautionary measures to avoid/predict the occurrence of diabetes.
This paper proposes HealthEdge, a machine learning-based smart healthcare
framework for type 2 diabetes prediction in an integrated IoT-edge-cloud
computing system. Numerical experiments and comparative analysis were carried
out between the two most used machine learning algorithms in the literature,
Random Forest (RF) and Logistic Regression (LR), using two real-life diabetes
datasets. The results show that RF predicts diabetes with 6% more accuracy on
average compared to LR.",None,-1
8891452f-2252-4b88-9f7f-e64a3e383414,FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios,0.900563,"The emergence of generative pre-trained models has facilitated the synthesis
of high-quality text, but it has also posed challenges in identifying factual
errors in the generated text. In particular: (1) A wider range of tasks now
face an increasing risk of containing factual errors when handled by generative
models. (2) Generated texts tend to be lengthy and lack a clearly defined
granularity for individual facts. (3) There is a scarcity of explicit evidence
available during the process of fact checking. With the above challenges in
mind, in this paper, we propose FacTool, a task and domain agnostic framework
for detecting factual errors of texts generated by large language models (e.g.,
ChatGPT). Experiments on four different tasks (knowledge-based QA, code
generation, mathematical reasoning, and scientific literature review) show the
efficacy of the proposed method. We release the code of FacTool associated with
ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",None,-1
b4de9704-ae8f-4483-8bae-81f6dcd813a8,MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition,0.521802,"We present a novel approach for action recognition in UAV videos. Our
formulation is designed to handle occlusion and viewpoint changes caused by the
movement of a UAV. We use the concept of mutual information to compute and
align the regions corresponding to human action or motion in the temporal
domain. This enables our recognition model to learn from the key features
associated with the motion. We also propose a novel frame sampling method that
uses joint mutual information to acquire the most informative frame sequence in
UAV videos. We have integrated our approach with X3D and evaluated the
performance on multiple datasets. In practice, we achieve 18.9% improvement in
Top-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al.,
2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16%
improvement on NEC Drones(Choi et al., 2020).",None,-1
670ba3a7-6a14-4bfd-a9bf-06e589f59545,M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer,0.696786,"Ensuring traffic safety and preventing accidents is a critical goal in daily
driving, where the advancement of computer vision technologies can be leveraged
to achieve this goal. In this paper, we present a multi-view, multi-scale
framework for naturalistic driving action recognition and localization in
untrimmed videos, namely M$^2$DAR, with a particular focus on detecting
distracted driving behaviors. Our system features a weight-sharing, multi-scale
Transformer-based action recognition network that learns robust hierarchical
representations. Furthermore, we propose a new election algorithm consisting of
aggregation, filtering, merging, and selection processes to refine the
preliminary results from the action recognition module across multiple views.
Extensive experiments conducted on the 7th AI City Challenge Track 3 dataset
demonstrate the effectiveness of our approach, where we achieved an overlap
score of 0.5921 on the A2 test set. Our source code is available at
\url{https://github.com/PurdueDigitalTwin/M2DAR}.",None,-1
bff8e1ce-da5d-46e4-9a24-110062cf2862,Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations,0.877148,"In this paper, we advocate for using large pre-trained monolingual language
models in cross lingual zero-shot word sense disambiguation (WSD) coupled with
a contextualized mapping mechanism. We also report rigorous experiments that
illustrate the effectiveness of employing sparse contextualized word
representations obtained via a dictionary learning procedure. Our experimental
results demonstrate that the above modifications yield a significant
improvement of nearly 6.5 points of increase in the average F-score (from 62.0
to 68.5) over a collection of 17 typologically diverse set of target languages.
We release our source code for replicating our experiments at
https://github.com/begab/sparsity_makes_sense.",None,-1
a8519837-7ef7-4047-9c14-64883b97cfc3,Large Language Models for Propaganda Detection,0.166213,"The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models' performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.",None,-1
626db1a1-1e4e-44f1-98af-5b4521b57fe8,In-Contextual Gender Bias Suppression for Large Language Models,0.388836,"Despite their impressive performance in a wide range of NLP tasks, Large
Language Models (LLMs) have been reported to encode worrying-levels of gender
biases. Prior work has proposed debiasing methods that require human labelled
examples, data augmentation and fine-tuning of LLMs, which are computationally
costly. Moreover, one might not even have access to the model parameters for
performing debiasing such as in the case of closed LLMs such as GPT-4. To
address this challenge, we propose bias suppression that prevents biased
generations of LLMs by simply providing textual preambles constructed from
manually designed templates and real-world statistics, without accessing to
model parameters. We show that, using CrowsPairs dataset, our textual preambles
covering counterfactual statements can suppress gender biases in English LLMs
such as LLaMA2. Moreover, we find that gender-neutral descriptions of
gender-biased objects can also suppress their gender biases. Moreover, we show
that bias suppression has acceptable adverse effect on downstream task
performance with HellaSwag and COPA.",None,-1
e4900e60-7639-4952-a938-2a3eec4c4079,MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation,0.969988,"Recent advances in text-to-image generation with diffusion models present
transformative capabilities in image quality. However, user controllability of
the generated image, and fast adaptation to new tasks still remains an open
challenge, currently mostly addressed by costly and long re-training and
fine-tuning or ad-hoc adaptations to specific image generation tasks. In this
work, we present MultiDiffusion, a unified framework that enables versatile and
controllable image generation, using a pre-trained text-to-image diffusion
model, without any further training or finetuning. At the center of our
approach is a new generation process, based on an optimization task that binds
together multiple diffusion generation processes with a shared set of
parameters or constraints. We show that MultiDiffusion can be readily applied
to generate high quality and diverse images that adhere to user-provided
controls, such as desired aspect ratio (e.g., panorama), and spatial guiding
signals, ranging from tight segmentation masks to bounding boxes. Project
webpage: https://multidiffusion.github.io",None,-1
74db2d0b-73c7-4ded-890a-05c08438caf4,COMBO: A Complete Benchmark for Open KG Canonicalization,0.188653,"Open knowledge graph (KG) consists of (subject, relation, object) triples
extracted from millions of raw text. The subject and object noun phrases and
the relation in open KG have severe redundancy and ambiguity and need to be
canonicalized. Existing datasets for open KG canonicalization only provide gold
entity-level canonicalization for noun phrases. In this paper, we present
COMBO, a Complete Benchmark for Open KG canonicalization. Compared with
existing datasets, we additionally provide gold canonicalization for relation
phrases, gold ontology-level canonicalization for noun phrases, as well as
source sentences from which triples are extracted. We also propose metrics for
evaluating each type of canonicalization. On the COMBO dataset, we empirically
compare previously proposed canonicalization methods as well as a few simple
baseline methods based on pretrained language models. We find that properly
encoding the phrases in a triple using pretrained language models results in
better relation canonicalization and ontology-level canonicalization of the
noun phrase. We release our dataset, baselines, and evaluation scripts at
https://github.com/jeffchy/COMBO/tree/main.",None,-1
5bbc203e-59d8-45eb-9495-bf508d9bfd34,MADI: Inter-domain Matching and Intra-domain Discrimination for Cross-domain Speech Recognition,0.444822,"End-to-end automatic speech recognition (ASR) usually suffers from
performance degradation when applied to a new domain due to domain shift.
Unsupervised domain adaptation (UDA) aims to improve the performance on the
unlabeled target domain by transferring knowledge from the source to the target
domain. To improve transferability, existing UDA approaches mainly focus on
matching the distributions of the source and target domains globally and/or
locally, while ignoring the model discriminability. In this paper, we propose a
novel UDA approach for ASR via inter-domain MAtching and intra-domain
DIscrimination (MADI), which improves the model transferability by fine-grained
inter-domain matching and discriminability by intra-domain contrastive
discrimination simultaneously. Evaluations on the Libri-Adapt dataset
demonstrate the effectiveness of our approach. MADI reduces the relative word
error rate (WER) on cross-device and cross-environment ASR by 17.7% and 22.8%,
respectively.",None,-1
2868206e-01d3-4cee-88c8-44446ab31850,Exploring Large-scale Unlabeled Faces to Enhance Facial Expression Recognition,0.770541,"Facial Expression Recognition (FER) is an important task in computer vision
and has wide applications in human-computer interaction, intelligent security,
emotion analysis, and other fields. However, the limited size of FER datasets
limits the generalization ability of expression recognition models, resulting
in ineffective model performance. To address this problem, we propose a
semi-supervised learning framework that utilizes unlabeled face data to train
expression recognition models effectively. Our method uses a dynamic threshold
module (\textbf{DTM}) that can adaptively adjust the confidence threshold to
fully utilize the face recognition (FR) data to generate pseudo-labels, thus
improving the model's ability to model facial expressions. In the ABAW5 EXPR
task, our method achieved excellent results on the official validation set.",None,-1
ebea1eae-4901-4744-9e60-32f410e03e55,1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation,0.397605,"Video panoptic segmentation is a challenging task that serves as the
cornerstone of numerous downstream applications, including video editing and
autonomous driving. We believe that the decoupling strategy proposed by DVIS
enables more effective utilization of temporal information for both ""thing"" and
""stuff"" objects. In this report, we successfully validated the effectiveness of
the decoupling strategy in video panoptic segmentation. Finally, our method
achieved a VPQ score of 51.4 and 53.7 in the development and test phases,
respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUW
Challenge. The code is available at https://github.com/zhang-tao-whu/DVIS",None,-1
d93c7aa8-f097-4a32-9e6b-aa244e833e93,SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation,0.552713,"Detecting objects and estimating their 6D poses is essential for automated
systems to interact safely with the environment. Most 6D pose estimators,
however, rely on a single camera frame and suffer from occlusions and
ambiguities due to object symmetries. We overcome this issue by presenting a
novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach
efficiently fuses the RGB-D frames from multiple perspectives in a deep
multi-directional fusion network and predicts predefined keypoints for all
objects in the scene simultaneously. Based on the keypoints and an instance
semantic segmentation, we efficiently compute the 6D poses by least-squares
fitting. To address the ambiguity issues for symmetric objects, we propose a
novel training procedure for symmetry-aware keypoint detection including a new
objective function. Our SyMFM6D network significantly outperforms the
state-of-the-art in both single-view and multi-view 6D pose estimation. We
furthermore show the effectiveness of our symmetry-aware training procedure and
demonstrate that our approach is robust towards inaccurate camera calibration
and dynamic camera setups.",None,-1
83748743-fcaa-4a58-b73c-feed0083c73a,Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora,0.695636,"Grammatical error correction (GEC) is the task of correcting typos, spelling,
punctuation and grammatical issues in text. Approaching the problem as a
sequence-to-sequence task, we compare the use of a common subword unit
vocabulary and byte-level encoding. Initial synthetic training data is created
using an error-generating pipeline, and used for finetuning two subword-level
models and one byte-level model. Models are then finetuned further on
hand-corrected error corpora, including texts written by children, university
students, dyslexic and second-language writers, and evaluated over different
error types and origins. We show that a byte-level model enables higher
correction quality than a subword approach, not only for simple spelling
errors, but also for more complex semantic, stylistic and grammatical issues.
In particular, initial training on synthetic corpora followed by finetuning on
a relatively small parallel corpus of real-world errors helps the byte-level
model correct a wide range of commonly occurring errors. Our experiments are
run for the Icelandic language but should hold for other similar languages,
particularly morphologically rich ones.",None,-1
793d1495-72b8-4a22-94eb-13b25f1baf49,Memory-Augmented Theory of Mind Network,0.587321,"Social reasoning necessitates the capacity of theory of mind (ToM), the
ability to contextualise and attribute mental states to others without having
access to their internal cognitive structure. Recent machine learning
approaches to ToM have demonstrated that we can train the observer to read the
past and present behaviours of other agents and infer their beliefs (including
false beliefs about things that no longer exist), goals, intentions and future
actions. The challenges arise when the behavioural space is complex, demanding
skilful space navigation for rapidly changing contexts for an extended period.
We tackle the challenges by equipping the observer with novel neural memory
mechanisms to encode, and hierarchical attention to selectively retrieve
information about others. The memories allow rapid, selective querying of
distal related past behaviours of others to deliberatively reason about their
current mental state, beliefs and future behaviours. This results in ToMMY, a
theory of mind model that learns to reason while making little assumptions
about the underlying mental processes. We also construct a new suite of
experiments to demonstrate that memories facilitate the learning process and
achieve better theory of mind performance, especially for high-demand
false-belief tasks that require inferring through multiple steps of changes.",None,-1
bea630be-5a86-4b85-ace0-107f9cf8ebb2,Outlier detection using flexible categorisation and interrogative agendas,0.59811,"Categorization is one of the basic tasks in machine learning and data
analysis. Building on formal concept analysis (FCA), the starting point of the
present work is that different ways to categorize a given set of objects exist,
which depend on the choice of the sets of features used to classify them, and
different such sets of features may yield better or worse categorizations,
relative to the task at hand. In their turn, the (a priori) choice of a
particular set of features over another might be subjective and express a
certain epistemic stance (e.g. interests, relevance, preferences) of an agent
or a group of agents, namely, their interrogative agenda. In the present paper,
we represent interrogative agendas as sets of features, and explore and compare
different ways to categorize objects w.r.t. different sets of features
(agendas). We first develop a simple unsupervised FCA-based algorithm for
outlier detection which uses categorizations arising from different agendas. We
then present a supervised meta-learning algorithm to learn suitable (fuzzy)
agendas for categorization as sets of features with different weights or
masses. We combine this meta-learning algorithm with the unsupervised outlier
detection algorithm to obtain a supervised outlier detection algorithm. We show
that these algorithms perform at par with commonly used algorithms for outlier
detection on commonly used datasets in outlier detection. These algorithms
provide both local and global explanations of their results.",None,-1
76e29e6c-5349-4317-905d-e9c892588786,Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications,0.315703,"In this work, I discuss how Large Language Models can be applied in the legal
domain, circumventing their current drawbacks. Despite their large success and
acceptance, their lack of explainability hinders legal experts to trust in
their output, and this happens rightfully so. However, in this paper, I argue
in favor of a new view, Justifiable Artificial Intelligence, instead of
focusing on Explainable Artificial Intelligence. I discuss in this paper how
gaining evidence for and against a Large Language Model's output may make their
generated texts more trustworthy - or hold them accountable for misinformation.",None,-1
7a4c549d-d2e1-4a58-9410-aa2b83d32dbb,BlendFields: Few-Shot Example-Driven Facial Modeling,0.249688,"Generating faithful visualizations of human faces requires capturing both
coarse and fine-level details of the face geometry and appearance. Existing
methods are either data-driven, requiring an extensive corpus of data not
publicly accessible to the research community, or fail to capture fine details
because they rely on geometric face models that cannot represent fine-grained
details in texture with a mesh discretization and linear deformation designed
to model only a coarse face geometry. We introduce a method that bridges this
gap by drawing inspiration from traditional computer graphics techniques.
Unseen expressions are modeled by blending appearance from a sparse set of
extreme poses. This blending is performed by measuring local volumetric changes
in those expressions and locally reproducing their appearance whenever a
similar expression is performed at test time. We show that our method
generalizes to unseen expressions, adding fine-grained effects on top of smooth
volumetric deformations of a face, and demonstrate how it generalizes beyond
faces.",None,-1
dbd84ac9-cd52-4b0b-8fde-32483b90863d,Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation,0.993873,"One critical challenge in 6D object pose estimation from a single RGBD image
is efficient integration of two different modalities, i.e., color and depth. In
this work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)
block that can aggregate cross-modality features for improving pose estimation.
Unlike existing fusion methods, the proposed DFTr can better model
cross-modality semantic correlation by leveraging their semantic similarity,
such that globally enhanced features from different modalities can be better
integrated for improved information extraction. Moreover, to further improve
robustness and efficiency, we introduce a novel weighted vector-wise voting
algorithm that employs a non-iterative global optimization strategy for precise
3D keypoint localization while achieving near real-time inference. Extensive
experiments show the effectiveness and strong generalization capability of our
proposed 3D keypoint voting algorithm. Results on four widely used benchmarks
also demonstrate that our method outperforms the state-of-the-art methods by
large margins.",None,-1
de322303-4dc8-4bdb-9b13-9b2f6d61939b,Sample-Efficient Learning of Novel Visual Concepts,0.214324,"Despite the advances made in visual object recognition, state-of-the-art deep
learning models struggle to effectively recognize novel objects in a few-shot
setting where only a limited number of examples are provided. Unlike humans who
excel at such tasks, these models often fail to leverage known relationships
between entities in order to draw conclusions about such objects. In this work,
we show that incorporating a symbolic knowledge graph into a state-of-the-art
recognition model enables a new approach for effective few-shot classification.
In our proposed neuro-symbolic architecture and training methodology, the
knowledge graph is augmented with additional relationships extracted from a
small set of examples, improving its ability to recognize novel objects by
considering the presence of interconnected entities. Unlike existing few-shot
classifiers, we show that this enables our model to incorporate not only
objects but also abstract concepts and affordances. The existence of the
knowledge graph also makes this approach amenable to interpretability through
analysis of the relationships contained within it. We empirically show that our
approach outperforms current state-of-the-art few-shot multi-label
classification methods on the COCO dataset and evaluate the addition of
abstract concepts and affordances on the Visual Genome dataset.",None,-1
040142c5-11ed-4c8f-9aaf-ba6219d5a9af,Chinese Spelling Correction as Rephrasing Language Model,0.75813,"This paper studies Chinese Spelling Correction (CSC), which aims to detect
and correct the potential spelling errors in a given sentence. Current
state-of-the-art methods regard CSC as a sequence tagging task and fine-tune
BERT-based models on sentence pairs. However, we note a critical flaw in the
process of tagging one character to another, that the correction is excessively
conditioned on the error. This is opposite from human mindset, where
individuals rephrase the complete sentence based on its semantics, rather than
solely on the error patterns memorized before. Such a counter-intuitive
learning process results in the bottleneck of generalizability and
transferability of machine spelling correction. To address this, we propose
Rephrasing Language Model (ReLM), where the model is trained to rephrase the
entire sentence by infilling additional slots, instead of
character-to-character tagging. This novel training paradigm achieves the new
state-of-the-art results across fine-tuned and zero-shot CSC benchmarks,
outperforming previous counterparts by a large margin. Our method also learns
transferable language representation when CSC is jointly trained with other
tasks.",None,-1
24aed727-f4ef-4cbc-afcf-acc404413a0c,A semantically enhanced dual encoder for aspect sentiment triplet extraction,0.870332,"Aspect sentiment triplet extraction (ASTE) is a crucial subtask of
aspect-based sentiment analysis (ABSA) that aims to comprehensively identify
sentiment triplets. Previous research has focused on enhancing ASTE through
innovative table-filling strategies. However, these approaches often overlook
the multi-perspective nature of language expressions, resulting in a loss of
valuable interaction information between aspects and opinions. To address this
limitation, we propose a framework that leverages both a basic encoder,
primarily based on BERT, and a particular encoder comprising a Bi-LSTM network
and graph convolutional network (GCN ). The basic encoder captures the
surface-level semantics of linguistic expressions, while the particular encoder
extracts deeper semantics, including syntactic and lexical information. By
modeling the dependency tree of comments and considering the part-of-speech and
positional information of words, we aim to capture semantics that are more
relevant to the underlying intentions of the sentences. An interaction strategy
combines the semantics learned by the two encoders, enabling the fusion of
multiple perspectives and facilitating a more comprehensive understanding of
aspect--opinion relationships. Experiments conducted on benchmark datasets
demonstrate the state-of-the-art performance of our proposed framework.",None,-1
0bff5e12-7538-417d-9d9a-03abc30ccc02,X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation,0.482172,"An important component of human analysis of medical images and their context
is the ability to relate newly seen things to related instances in our memory.
In this paper we mimic this ability by using multi-modal retrieval augmentation
and apply it to several tasks in chest X-ray analysis. By retrieving similar
images and/or radiology reports we expand and regularize the case at hand with
additional knowledge, while maintaining factual knowledge consistency. The
method consists of two components. First, vision and language modalities are
aligned using a pre-trained CLIP model. To enforce that the retrieval focus
will be on detailed disease-related content instead of global visual appearance
it is fine-tuned using disease class information. Subsequently, we construct a
non-parametric retrieval index, which reaches state-of-the-art retrieval
levels. We use this index in our downstream tasks to augment image
representations through multi-head attention for disease classification and
report retrieval. We show that retrieval augmentation gives considerable
improvements on these tasks. Our downstream report retrieval even shows to be
competitive with dedicated report generation methods, paving the path for this
method in medical imaging.",None,-1
2fdd73d2-c377-42a7-b2a7-0d581626c37f,XPert: Peripheral Circuit & Neural Architecture Co-search for Area and Energy-efficient Xbar-based Computing,0.17714,"The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)
implemented on In-memory Computing (IMC) architectures primarily depend on the
DNN architecture and the peripheral circuit parameters. It is therefore
essential to holistically co-search the network and peripheral parameters to
achieve optimal performance. To this end, we propose XPert, which co-searches
network architecture in tandem with peripheral parameters such as the type and
precision of analog-to-digital converters, crossbar column sharing and the
layer-specific input precision using an optimization-based design space
exploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower
EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)
accuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper is
available at https://github.com/Intelligent-Computing-Lab-Yale/XPert.",None,-1
c08bb8fc-a846-43c3-b901-96dc3ea71921,Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models,0.302666,"As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model's
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.",None,-1
558c8bd1-0c2d-49f8-b11a-4586fa7c7d69,Randomly Initialized Subnetworks with Iterative Weight Recycling,0.202314,"The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized
neural networks contain several subnetworks that achieve comparable accuracy to
fully trained models of the same architecture. However, current methods require
that the network is sufficiently overparameterized. In this work, we propose a
modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that
finds high-accuracy subnetworks with no additional storage cost or scaling. The
algorithm, Iterative Weight Recycling, identifies subsets of important weights
within a randomly initialized network for intra-layer reuse. Empirically we
show improvements on smaller network architectures and higher prune rates,
finding that model sparsity can be increased through the ""recycling"" of
existing weights. In addition to Iterative Weight Recycling, we complement the
Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy,
randomly initialized subnetwork's produce diverse masks, despite being
generated with the same hyperparameter's and pruning strategy. We explore the
landscapes of these masks, which show high variability.",None,-1
0020c4af-68d9-4145-984a-5e22a624475a,GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency,0.693178,"We present a novel framework to regularize Neural Radiance Field (NeRF) in a
few-shot setting with a geometry-aware consistency regularization. The proposed
approach leverages a rendered depth map at unobserved viewpoint to warp sparse
input images to the unobserved viewpoint and impose them as pseudo ground
truths to facilitate learning of NeRF. By encouraging such geometry-aware
consistency at a feature-level instead of using pixel-level reconstruction
loss, we regularize the NeRF at semantic and structural levels while allowing
for modeling view dependent radiance to account for color variations across
viewpoints. We also propose an effective method to filter out erroneous warped
solutions, along with training strategies to stabilize training during
optimization. We show that our model achieves competitive results compared to
state-of-the-art few-shot NeRF models. Project page is available at
https://ku-cvlab.github.io/GeCoNeRF/.",None,-1
98de1593-df6e-4425-b1ae-a32dd77415c7,Unsupervised Deep Graph Matching Based on Cycle Consistency,0.494695,"We contribute to the sparsely populated area of unsupervised deep graph
matching with application to keypoint matching in images. Contrary to the
standard \emph{supervised} approach, our method does not require ground truth
correspondences between keypoint pairs. Instead, it is self-supervised by
enforcing consistency of matchings between images of the same object category.
As the matching and the consistency loss are discrete, their derivatives cannot
be straightforwardly used for learning. We address this issue in a principled
way by building our method upon the recent results on black-box differentiation
of combinatorial solvers. This makes our method exceptionally flexible, as it
is compatible with arbitrary network architectures and combinatorial solvers.
Our experimental evaluation suggests that our technique sets a new
state-of-the-art for unsupervised graph matching.",None,-1
52e36a62-1b9f-4c92-b766-2b92111ce5e3,How to Design Translation Prompts for ChatGPT: An Empirical Study,0.886096,"The recently released ChatGPT has demonstrated surprising abilities in
natural language understanding and natural language generation. Machine
translation relies heavily on the abilities of language understanding and
generation. Thus, in this paper, we explore how to assist machine translation
with ChatGPT. We adopt several translation prompts on a wide range of
translations. Our experimental results show that ChatGPT with designed
translation prompts can achieve comparable or better performance over
commercial translation systems for high-resource language translations. We
further evaluate the translation quality using multiple references, and ChatGPT
achieves superior performance compared to commercial systems. We also conduct
experiments on domain-specific translations, the final results show that
ChatGPT is able to comprehend the provided domain keyword and adjust
accordingly to output proper translations. At last, we perform few-shot prompts
that show consistent improvement across different base prompts. Our work
provides empirical evidence that ChatGPT still has great potential in
translations.",None,-1
4bb6b91b-5a8a-4608-ba3d-af435f69459b,Transformer and Snowball Graph Convolution Learning for Brain functional network Classification,0.111274,"Advanced deep learning methods, especially graph neural networks (GNNs), are
increasingly expected to learn from brain functional network data and predict
brain disorders. In this paper, we proposed a novel Transformer and snowball
encoding networks (TSEN) for brain functional network classification, which
introduced Transformer architecture with graph snowball connection into GNNs
for learning whole-graph representation. TSEN combined graph snowball
connection with graph Transformer by snowball encoding layers, which enhanced
the power to capture multi-scale information and global patterns of brain
functional networks. TSEN also introduced snowball graph convolution as
position embedding in Transformer structure, which was a simple yet effective
method for capturing local patterns naturally. We evaluated the proposed model
by two large-scale brain functional network datasets from autism spectrum
disorder and major depressive disorder respectively, and the results
demonstrated that TSEN outperformed the state-of-the-art GNN models and the
graph-transformer based GNN models.",None,-1
f3d540c7-6070-49ff-a88e-c9639519710c,How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model,0.721834,"Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as ""The war lasted from the year 1732 to the year 17"", and predict valid
two-digit end years (years > 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.",None,-1
1af141ec-7db8-47e3-8db0-9e2d89dfa6d5,Recent Advances in Direct Speech-to-text Translation,0.908992,"Recently, speech-to-text translation has attracted more and more attention
and many studies have emerged rapidly. In this paper, we present a
comprehensive survey on direct speech translation aiming to summarize the
current state-of-the-art techniques. First, we categorize the existing research
work into three directions based on the main challenges -- modeling burden,
data scarcity, and application issues. To tackle the problem of modeling
burden, two main structures have been proposed, encoder-decoder framework
(Transformer and the variants) and multitask frameworks. For the challenge of
data scarcity, recent work resorts to many sophisticated techniques, such as
data augmentation, pre-training, knowledge distillation, and multilingual
modeling. We analyze and summarize the application issues, which include
real-time, segmentation, named entity, gender bias, and code-switching.
Finally, we discuss some promising directions for future work.",None,-1
c5c65b7d-b2da-42fd-9b2d-cf3cd9909ca7,Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs,0.313875,"In comparative linguistics, colexification refers to the phenomenon of a
lexical form conveying two or more distinct meanings. Existing work on
colexification patterns relies on annotated word lists, limiting scalability
and usefulness in NLP. In contrast, we identify colexification patterns of more
than 2,000 concepts across 1,335 languages directly from an unannotated
parallel corpus. We then propose simple and effective methods to build
multilingual graphs from the colexification patterns: ColexNet and ColexNet+.
ColexNet's nodes are concepts and its edges are colexifications. In ColexNet+,
concept nodes are additionally linked through intermediate nodes, each
representing an ngram in one of 1,334 languages. We use ColexNet+ to train
$\overrightarrow{\mbox{ColexNet+}}$, high-quality multilingual embeddings that
are well-suited for transfer learning. In our experiments, we first show that
ColexNet achieves high recall on CLICS, a dataset of crosslingual
colexifications. We then evaluate $\overrightarrow{\mbox{ColexNet+}}$ on
roundtrip translation, sentence retrieval and sentence classification and show
that our embeddings surpass several transfer learning baselines. This
demonstrates the benefits of using colexification as a source of information in
multilingual NLP.",None,-1
6a5ff8f3-b15c-4c29-8deb-194f2a8c61fc,Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration,0.318885,"We propose a novel end-to-end document understanding model called SeRum
(SElective Region Understanding Model) for extracting meaningful information
from document images, including document analysis, retrieval, and office
automation.
  Unlike state-of-the-art approaches that rely on multi-stage technical schemes
and are computationally expensive,
  SeRum converts document image understanding and recognition tasks into a
local decoding process of the visual tokens of interest, using a content-aware
token merge module.
  This mechanism enables the model to pay more attention to regions of interest
generated by the query decoder, improving the model's effectiveness and
speeding up the decoding speed of the generative scheme.
  We also designed several pre-training tasks to enhance the understanding and
local awareness of the model.
  Experimental results demonstrate that SeRum achieves state-of-the-art
performance on document understanding tasks and competitive results on text
spotting tasks.
  SeRum represents a substantial advancement towards enabling efficient and
effective end-to-end document understanding.",None,-1
ff684689-42f3-4ac8-8977-40d9967a2b6b,LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?,0.230168,"Large language models (LLMs) have exhibited impressive capabilities in
comprehending complex instructions. However, their blind adherence to provided
instructions has led to concerns regarding risks of malicious use. Existing
defence mechanisms, such as model fine-tuning or output censorship using LLMs,
have proven to be fallible, as LLMs can still generate problematic responses.
Commonly employed censorship approaches treat the issue as a machine learning
problem and rely on another LM to detect undesirable content in LLM outputs. In
this paper, we present the theoretical limitations of such semantic censorship
approaches. Specifically, we demonstrate that semantic censorship can be
perceived as an undecidable problem, highlighting the inherent challenges in
censorship that arise due to LLMs' programmatic and instruction-following
capabilities. Furthermore, we argue that the challenges extend beyond semantic
censorship, as knowledgeable attackers can reconstruct impermissible outputs
from a collection of permissible ones. As a result, we propose that the problem
of censorship needs to be reevaluated; it should be treated as a security
problem which warrants the adaptation of security-based approaches to mitigate
potential risks.",None,-1
27f6309c-0c3f-4a67-9314-e93211b657b5,Learning Structured Output Representations from Attributes using Deep Conditional Generative Models,0.212488,"Structured output representation is a generative task explored in computer
vision that often times requires the mapping of low dimensional features to
high dimensional structured outputs. Losses in complex spatial information in
deterministic approaches such as Convolutional Neural Networks (CNN) lead to
uncertainties and ambiguous structures within a single output representation. A
probabilistic approach through deep Conditional Generative Models (CGM) is
presented by Sohn et al. in which a particular model known as the Conditional
Variational Auto-encoder (CVAE) is introduced and explored. While the original
paper focuses on the task of image segmentation, this paper adopts the CVAE
framework for the task of controlled output representation through attributes.
This approach allows us to learn a disentangled multimodal prior distribution,
resulting in more controlled and robust approach to sample generation. In this
work we recreate the CVAE architecture and train it on images conditioned on
various attributes obtained from two image datasets; the Large-scale CelebFaces
Attributes (CelebA) dataset and the Caltech-UCSD Birds (CUB-200-2011) dataset.
We attempt to generate new faces with distinct attributes such as hair color
and glasses, as well as different bird species samples with various attributes.
We further introduce strategies for improving generalized sample generation by
applying a weighted term to the variational lower bound.",None,-1
fe38b8f1-e24b-4731-9185-c4fc72736657,CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare,0.652744,"In the era of modern healthcare, swiftly generating medical question
summaries is crucial for informed and timely patient care. Despite the
increasing complexity and volume of medical data, existing studies have focused
solely on text-based summarization, neglecting the integration of visual
information. Recognizing the untapped potential of combining textual queries
with visual representations of medical conditions, we introduce the Multimodal
Medical Question Summarization (MMQS) Dataset. This dataset, a major
contribution to our work, pairs medical queries with visual aids, facilitating
a richer and more nuanced understanding of patient needs. We also propose a
framework, utilizing the power of Contrastive Language Image Pretraining(CLIP)
and Large Language Models(LLMs), consisting of four modules that identify
medical disorders, generate relevant context, filter medical concepts, and
craft visually aware summaries. Our comprehensive framework harnesses the power
of CLIP, a multimodal foundation model, and various general-purpose LLMs,
comprising four main modules: the medical disorder identification module, the
relevant context generation module, the context filtration module for
distilling relevant medical concepts and knowledge, and finally, a
general-purpose LLM to generate visually aware medical question summaries.
Leveraging our MMQS dataset, we showcase how visual cues from images enhance
the generation of medically nuanced summaries. This multimodal approach not
only enhances the decision-making process in healthcare but also fosters a more
nuanced understanding of patient queries, laying the groundwork for future
research in personalized and responsive medical care",None,-1
c963b73c-f2af-43c5-a1aa-2bdc4d81dc12,FV-MgNet: Fully Connected V-cycle MgNet for Interpretable Time Series Forecasting,0.19287,"By investigating iterative methods for a constrained linear model, we propose
a new class of fully connected V-cycle MgNet for long-term time series
forecasting, which is one of the most difficult tasks in forecasting. MgNet is
a CNN model that was proposed for image classification based on the multigrid
(MG) methods for solving discretized partial differential equations (PDEs). We
replace the convolutional operations with fully connected operations in the
existing MgNet and then apply them to forecasting problems. Motivated by the
V-cycle structure in MG, we further propose the FV-MgNet, a V-cycle version of
the fully connected MgNet, to extract features hierarchically. By evaluating
the performance of FV-MgNet on popular data sets and comparing it with
state-of-the-art models, we show that the FV-MgNet achieves better results with
less memory usage and faster inference speed. In addition, we develop ablation
experiments to demonstrate that the structure of FV-MgNet is the best choice
among the many variants.",None,-1
efe0e27a-2dd4-481e-b54a-b57b2a42c288,A Unified One-Step Solution for Aspect Sentiment Quad Prediction,0.29926,"Aspect sentiment quad prediction (ASQP) is a challenging yet significant
subtask in aspect-based sentiment analysis as it provides a complete
aspect-level sentiment structure. However, existing ASQP datasets are usually
small and low-density, hindering technical advancement. To expand the capacity,
in this paper, we release two new datasets for ASQP, which contain the
following characteristics: larger size, more words per sample, and higher
density. With such datasets, we unveil the shortcomings of existing strong ASQP
baselines and therefore propose a unified one-step solution for ASQP, namely
One-ASQP, to detect the aspect categories and to identify the
aspect-opinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds
several unique advantages: (1) by separating ASQP into two subtasks and solving
them independently and simultaneously, we can avoid error propagation in
pipeline-based methods and overcome slow training and inference in
generation-based methods; (2) by introducing sentiment-specific horns tagging
schema in a token-pair-based two-dimensional matrix, we can exploit deeper
interactions between sentiment elements and efficiently decode the AOS
triplets; (3) we design ``[NULL]'' token can help us effectively identify the
implicit aspects or opinions. Experiments on two benchmark datasets and our
released two datasets demonstrate the advantages of our One-ASQP. The two new
datasets are publicly released at
\url{https://www.github.com/Datastory-CN/ASQP-Datasets}.",None,-1
456bfb4a-c22e-4a22-b6fa-bb8f58a9dbab,Collage Diffusion,0.224005,"We seek to give users precise control over diffusion-based image generation
by modeling complex scenes as sequences of layers, which define the desired
spatial arrangement and visual attributes of objects in the scene. Collage
Diffusion harmonizes the input layers to make objects fit together -- the key
challenge involves minimizing changes in the positions and key visual
attributes of the input layers while allowing other attributes to change in the
harmonization process. We ensure that objects are generated in the correct
locations by modifying text-image cross-attention with the layers' alpha masks.
We preserve key visual attributes of input layers by learning specialized text
representations per layer and by extending ControlNet to operate on layers.
Layer input allows users to control the extent of image harmonization on a
per-object basis, and users can even iteratively edit individual objects in
generated images while keeping other objects fixed. By leveraging the rich
information present in layer input, Collage Diffusion generates globally
harmonized images that maintain desired object characteristics better than
prior approaches.",None,-1
2aecbe1b-2734-4f04-b485-8710a6e19aef,MRecGen: Multimodal Appropriate Reaction Generator,0.815906,"Verbal and non-verbal human reaction generation is a challenging task, as
different reactions could be appropriate for responding to the same behaviour.
This paper proposes the first multiple and multimodal (verbal and nonverbal)
appropriate human reaction generation framework that can generate appropriate
and realistic human-style reactions (displayed in the form of synchronised
text, audio and video streams) in response to an input user behaviour. This
novel technique can be applied to various human-computer interaction scenarios
by generating appropriate virtual agent/robot behaviours. Our demo is available
at \url{https://github.com/SSYSteve/MRecGen}.",None,-1
f95f0d9d-c3c8-40df-9479-cff5b27ba040,TuPy-E: detecting hate speech in Brazilian Portuguese social media with a novel dataset and comprehensive analysis of models,0.433628,"Social media has become integral to human interaction, providing a platform
for communication and expression. However, the rise of hate speech on these
platforms poses significant risks to individuals and communities. Detecting and
addressing hate speech is particularly challenging in languages like Portuguese
due to its rich vocabulary, complex grammar, and regional variations. To
address this, we introduce TuPy-E, the largest annotated Portuguese corpus for
hate speech detection. TuPy-E leverages an open-source approach, fostering
collaboration within the research community. We conduct a detailed analysis
using advanced techniques like BERT models, contributing to both academic
understanding and practical applications",None,-1
8d1a4a80-e1f9-41c5-9aa4-d38f296926b2,Improving User Controlled Table-To-Text Generation Robustness,0.417825,"In this work we study user controlled table-to-text generation where users
explore the content in a table by selecting cells and reading a natural
language description thereof automatically produce by a natural language
generator. Such generation models usually learn from carefully selected cell
combinations (clean cell selections); however, in practice users may select
unexpected, redundant, or incoherent cell combinations (noisy cell selections).
In experiments, we find that models perform well on test sets coming from the
same distribution as the train data but their performance drops when evaluated
on realistic noisy user inputs. We propose a fine-tuning regime with additional
user-simulated noisy cell selections. Models fine-tuned with the proposed
regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test
cases; and achieve comparable state-of-the-art performance on the ToTTo
dataset.",None,-1
57e6e97e-f33c-4096-8af2-1602a0132383,From Shortcuts to Triggers: Backdoor Defense with Denoised PoE,0.704826,"Language models are often at risk of diverse backdoor attacks, especially
data poisoning. Thus, it is important to investigate defense solutions for
addressing them. Existing backdoor defense methods mainly focus on backdoor
attacks with explicit triggers, leaving a universal defense against various
backdoor attacks with diverse triggers largely unexplored. In this paper, we
propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised
Product-of-Experts), which is inspired by the shortcut nature of backdoor
attacks, to defend various backdoor attacks. DPoE consists of two models: a
shallow model that captures the backdoor shortcuts and a main model that is
prevented from learning the backdoor shortcuts. To address the label flip
caused by backdoor attackers, DPoE incorporates a denoising design. Experiments
on SST-2 dataset show that DPoE significantly improves the defense performance
against various types of backdoor triggers including word-level,
sentence-level, and syntactic triggers. Furthermore, DPoE is also effective
under a more challenging but practical setting that mixes multiple types of
trigger.",None,-1
d5ae9127-f96d-4e47-9cda-fd8dfee2b0ba,Exploring the Potential of Large Language Models to Generate Formative Programming Feedback,0.998081,"Ever since the emergence of large language models (LLMs) and related
applications, such as ChatGPT, its performance and error analysis for
programming tasks have been subject to research. In this work-in-progress
paper, we explore the potential of such LLMs for computing educators and
learners, as we analyze the feedback it generates to a given input containing
program code. In particular, we aim at (1) exploring how an LLM like ChatGPT
responds to students seeking help with their introductory programming tasks,
and (2) identifying feedback types in its responses. To achieve these goals, we
used students' programming sequences from a dataset gathered within a CS1
course as input for ChatGPT along with questions required to elicit feedback
and correct solutions. The results show that ChatGPT performs reasonably well
for some of the introductory programming tasks and student errors, which means
that students can potentially benefit. However, educators should provide
guidance on how to use the provided feedback, as it can contain misleading
information for novices.",None,-1
aa7f85cd-41ec-4311-8e6d-befb94dbc338,Improving RNN-Transducers with Acoustic LookAhead,0.647641,"RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end
model for speech to text conversion because of their high accuracy and
streaming capabilities. A typical RNN-T independently encodes the input audio
and the text context, and combines the two encodings by a thin joint network.
While this architecture provides SOTA streaming accuracy, it also makes the
model vulnerable to strong LM biasing which manifests as multi-step
hallucination of text without acoustic evidence. In this paper we propose
LookAhead that makes text representations more acoustically grounded by looking
ahead into the future within the audio input. This technique yields a
significant 5%-20% relative reduction in word error rate on both in-domain and
out-of-domain evaluation sets.",None,-1
a004fc66-0ef8-4f58-940c-b16c56363f0f,MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models,0.417439,"Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts
within knowledge graphs and automatically infer missing links. Existing methods
can mainly be categorized into structure-based or description-based. On the one
hand, structure-based methods effectively represent relational facts in
knowledge graphs using entity embeddings. However, they struggle with
semantically rich real-world entities due to limited structural information and
fail to generalize to unseen entities. On the other hand, description-based
methods leverage pre-trained language models (PLMs) to understand textual
information. They exhibit strong robustness towards unseen entities. However,
they have difficulty with larger negative sampling and often lag behind
structure-based methods. To address these issues, in this paper, we propose
Momentum Contrast for knowledge graph completion with Structure-Augmented
pre-trained language models (MoCoSA), which allows the PLM to perceive the
structural information by the adaptable structure encoder. To improve learning
efficiency, we proposed momentum hard negative and intra-relation negative
sampling. Experimental results demonstrate that our approach achieves
state-of-the-art performance in terms of mean reciprocal rank (MRR), with
improvements of 2.5% on WN18RR and 21% on OpenBG500.",None,-1
9d461b13-e887-4361-ae05-67eeabb476d3,Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents,0.78911,"The optimized certainty equivalent (OCE) is a family of risk measures that
cover important examples such as entropic risk, conditional value-at-risk and
mean-variance models. In this paper, we propose a new episodic risk-sensitive
reinforcement learning formulation based on tabular Markov decision processes
with recursive OCEs. We design an efficient learning algorithm for this problem
based on value iteration and upper confidence bound. We derive an upper bound
on the regret of the proposed algorithm, and also establish a minimax lower
bound. Our bounds show that the regret rate achieved by our proposed algorithm
has optimal dependence on the number of episodes and the number of actions.",None,-1
6851d828-4151-492a-a41f-c8d7e2515623,A store-and-forward cloud-based telemonitoring system for automatic assessing dysarthria evolution in neurological diseases from video-recording analysis,0.900509,"Background and objectives: Patients suffering from neurological diseases may
develop dysarthria, a motor speech disorder affecting the execution of speech.
Close and quantitative monitoring of dysarthria evolution is crucial for
enabling clinicians to promptly implement patient management strategies and
maximizing effectiveness and efficiency of communication functions in term of
restoring, compensating or adjusting. In the clinical assessment of orofacial
structures and functions, at rest condition or during speech and non-speech
movements, a qualitative evaluation is usually performed, throughout visual
observation. Methods: To overcome limitations posed by qualitative assessments,
this work presents a store-and-forward self-service telemonitoring system that
integrates, within its cloud architecture, a convolutional neural network (CNN)
for analyzing video recordings acquired by individuals with dysarthria. This
architecture, called facial landmark Mask RCNN, aims at locating facial
landmarks as a prior for assessing the orofacial functions related to speech
and examining dysarthria evolution in neurological diseases. Results: When
tested on the Toronto NeuroFace dataset, a publicly available annotated dataset
of video recordings from patients with amyotrophic lateral sclerosis (ALS) and
stroke, the proposed CNN achieved a normalized mean error equal to 1.79 on
localizing the facial landmarks. We also tested our system in a real-life
scenario on 11 bulbar-onset ALS subjects, obtaining promising outcomes in terms
of facial landmark position estimation. Discussion and conclusions: This
preliminary study represents a relevant step towards the use of remote tools to
support clinicians in monitoring the evolution of dysarthria.",None,-1
d33565f1-f371-4a28-936b-77a3a9e0ad9d,A Suite of Fairness Datasets for Tabular Classification,0.315653,"There have been many papers with algorithms for improving fairness of
machine-learning classifiers for tabular data. Unfortunately, most use only
very few datasets for their experimental evaluation. We introduce a suite of
functions for fetching 20 fairness datasets and providing associated fairness
metadata. Hopefully, these will lead to more rigorous experimental evaluations
in future fairness-aware machine learning research.",None,-1
75545d22-28ef-4734-9964-a061bed93585,Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification,0.974722,"Autism spectrum disorder (ASD) is a prevalent psychiatric condition
characterized by atypical cognitive, emotional, and social patterns. Timely and
accurate diagnosis is crucial for effective interventions and improved outcomes
in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced
Transformer framework, METAFormer, ASD classification. Our framework utilizes
resting-state functional magnetic resonance imaging data from the ABIDE I
dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer
employs a multi-atlas approach, where flattened connectivity matrices from the
AAL, CC200, and DOS160 atlases serve as input to the transformer encoder.
Notably, we demonstrate that self-supervised pretraining, involving the
reconstruction of masked values from the input, significantly enhances
classification performance without the need for additional or separate training
data. Through stratified cross-validation, we evaluate the proposed framework
and show that it surpasses state-of-the-art performance on the ABIDE I dataset,
with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our
framework is available at https://github.com/Lugges991/METAFormer",None,-1
60c72d9d-58c2-4302-8e4e-92049603437d,Physics-Aware Semi-Supervised Underwater Image Enhancement,0.512716,"Underwater images normally suffer from degradation due to the transmission
medium of water bodies. Both traditional prior-based approaches and deep
learning-based methods have been used to address this problem. However, the
inflexible assumption of the former often impairs their effectiveness in
handling diverse underwater scenes, while the generalization of the latter to
unseen images is usually weakened by insufficient data. In this study, we
leverage both the physics-based underwater Image Formation Model (IFM) and deep
learning techniques for Underwater Image Enhancement (UIE). To this end, we
propose a novel Physics-Aware Dual-Stream Underwater Image Enhancement Network,
i.e., PA-UIENet, which comprises a Transmission Estimation Steam (T-Stream) and
an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE
task by explicitly estimating the degradation parameters of the IFM. We also
adopt an IFM-inspired semi-supervised learning framework, which exploits both
the labeled and unlabeled images, to address the issue of insufficient data.
Our method performs better than, or at least comparably to, eight baselines
across five testing sets in the degradation estimation and UIE tasks. This
should be due to the fact that it not only can model the degradation but also
can learn the characteristics of diverse underwater scenes.",None,-1
0f77135a-fcc6-44df-a348-fd3db1f18a79,Atmospheric Turbulence Correction via Variational Deep Diffusion,0.612773,"Atmospheric Turbulence (AT) correction is a challenging restoration task as
it consists of two distortions: geometric distortion and spatially variant
blur. Diffusion models have shown impressive accomplishments in photo-realistic
image synthesis and beyond. In this paper, we propose a novel deep conditional
diffusion model under a variational inference framework to solve the AT
correction problem. We use this framework to improve performance by learning
latent prior information from the input and degradation processes. We use the
learned information to further condition the diffusion model. Experiments are
conducted in a comprehensive synthetic AT dataset. We show that the proposed
framework achieves good quantitative and qualitative results.",None,-1
03915660-c7ee-4993-a48f-d10f3c262408,"Privacy in Large Language Models: Attacks, Defenses and Future Directions",0.850771,"The advancement of large language models (LLMs) has significantly enhanced
the ability to effectively tackle various downstream NLP tasks and unify these
tasks into generative pipelines. On the one hand, powerful language models,
trained on massive textual data, have brought unparalleled accessibility and
usability for both models and users. On the other hand, unrestricted access to
these models can also introduce potential malicious and unintentional privacy
risks. Despite ongoing efforts to address the safety and privacy concerns
associated with LLMs, the problem remains unresolved. In this paper, we provide
a comprehensive analysis of the current privacy attacks targeting LLMs and
categorize them according to the adversary's assumed capabilities to shed light
on the potential vulnerabilities present in LLMs. Then, we present a detailed
overview of prominent defense strategies that have been developed to counter
these privacy attacks. Beyond existing works, we identify upcoming privacy
concerns as LLMs evolve. Lastly, we point out several potential avenues for
future exploration.",None,-1
69f860f7-5e69-4c6a-925b-35a79bcbc085,Beyond the Prior Forgery Knowledge: Mining Critical Clues for General Face Forgery Detection,0.757616,"Face forgery detection is essential in combating malicious digital face
attacks. Previous methods mainly rely on prior expert knowledge to capture
specific forgery clues, such as noise patterns, blending boundaries, and
frequency artifacts. However, these methods tend to get trapped in local
optima, resulting in limited robustness and generalization capability. To
address these issues, we propose a novel Critical Forgery Mining (CFM)
framework, which can be flexibly assembled with various backbones to boost
their generalization and robustness performance. Specifically, we first build a
fine-grained triplet and suppress specific forgery traces through prior
knowledge-agnostic data augmentation. Subsequently, we propose a fine-grained
relation learning prototype to mine critical information in forgeries through
instance and local similarity-aware losses. Moreover, we design a novel
progressive learning controller to guide the model to focus on principal
feature components, enabling it to learn critical forgery features in a
coarse-to-fine manner. The proposed method achieves state-of-the-art forgery
detection performance under various challenging evaluation settings.",None,-1
e2128fc5-d39f-47c0-a711-a847986eecdb,Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations,0.91337,"Causal abstraction is a promising theoretical framework for explainable
artificial intelligence that defines when an interpretable high-level causal
model is a faithful simplification of a low-level deep learning system.
However, existing causal abstraction methods have two major limitations: they
require a brute-force search over alignments between the high-level model and
the low-level one, and they presuppose that variables in the high-level model
will align with disjoint sets of neurons in the low-level one. In this paper,
we present distributed alignment search (DAS), which overcomes these
limitations. In DAS, we find the alignment between high-level and low-level
models using gradient descent rather than conducting a brute-force search, and
we allow individual neurons to play multiple distinct roles by analyzing
representations in non-standard bases-distributed representations. Our
experiments show that DAS can discover internal structure that prior approaches
miss. Overall, DAS removes previous obstacles to conducting causal abstraction
analyses and allows us to find conceptual structure in trained neural nets.",None,-1
26dff59b-2970-4b1d-a668-948550b52777,RECLIP: Resource-efficient CLIP by Training with Small Images,0.267802,"We present RECLIP (Resource-efficient CLIP), a simple method that minimizes
computational resource footprint for CLIP (Contrastive Language Image
Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we
leverage small images to learn from large-scale language supervision
efficiently, and finetune the model with high-resolution data in the end. Since
the complexity of the vision transformer heavily depends on input image size,
our approach significantly reduces the training resource requirements both in
theory and in practice. Using the same batch size and training epoch, RECLIP
achieves highly competitive zero-shot classification and image-text retrieval
accuracy with 6 to 8x less computational resources and 7 to 9x fewer FLOPs than
the baseline. Compared to the state-of-the-art contrastive learning methods,
RECLIP demonstrates 5 to 59x training resource savings while maintaining highly
competitive zero-shot classification and retrieval performance. Finally, RECLIP
matches the state of the art in transfer learning to open-vocabulary detection
tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the
broader research community to explore language supervised pretraining in
resource-friendly settings.",None,-1
d82953c7-048e-417d-80db-7fe9d7f22744,Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation,0.460028,"In end-to-end speech translation, speech and text pre-trained models improve
translation quality. Recently proposed models simply connect the pre-trained
models of speech and text as encoder and decoder. Therefore, only the
information from the final layer of encoders is input to the decoder. Since it
is clear that the speech pre-trained model outputs different information from
each layer, the simple connection method cannot fully utilize the information
that the speech pre-trained model has. In this study, we propose an
inter-connection mechanism that aggregates the information from each layer of
the speech pre-trained model by weighted sums and inputs into the decoder. This
mechanism increased BLEU by approximately 2 points in en-de, en-ja, and en-zh
by increasing parameters by 2K when the speech pre-trained model was frozen.
Furthermore, we investigated the contribution of each layer for each language
by visualizing layer weights and found that the contributions were different.",None,-1
4643260f-afb6-4ce5-8676-8c20e3626424,Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance,0.0713055,"The NLP community has long advocated for the construction of multi-annotator
datasets to better capture the nuances of language interpretation,
subjectivity, and ambiguity. This paper conducts a retrospective study to show
how performance scores can vary when a dataset expands from a single annotation
per instance to multiple annotations. We propose a novel multi-annotator
simulation process to generate datasets with varying annotation budgets. We
show that similar datasets with the same annotation budget can lead to varying
performance gains. Our findings challenge the popular belief that models
trained on multi-annotation examples always lead to better performance than
models trained on single or few-annotation examples.",None,-1
020a297e-a572-40b7-a7ba-984fd5e43658,International Governance of Civilian AI: A Jurisdictional Certification Approach,0.939734,"This report describes trade-offs in the design of international governance
arrangements for civilian artificial intelligence (AI) and presents one
approach in detail. This approach represents the extension of a standards,
licensing, and liability regime to the global level. We propose that states
establish an International AI Organization (IAIO) to certify state
jurisdictions (not firms or AI projects) for compliance with international
oversight standards. States can give force to these international standards by
adopting regulations prohibiting the import of goods whose supply chains embody
AI from non-IAIO-certified jurisdictions. This borrows attributes from models
of existing international organizations, such as the International Civilian
Aviation Organization (ICAO), the International Maritime Organization (IMO),
and the Financial Action Task Force (FATF). States can also adopt multilateral
controls on the export of AI product inputs, such as specialized hardware, to
non-certified jurisdictions. Indeed, both the import and export standards could
be required for certification. As international actors reach consensus on risks
of and minimum standards for advanced AI, a jurisdictional certification regime
could mitigate a broad range of potential harms, including threats to public
safety.",None,-1
e8e82fd9-847d-4432-a769-7f4348125d82,Unsupervised Meta-Learning via Few-shot Pseudo-supervised Contrastive Learning,0.527867,"Unsupervised meta-learning aims to learn generalizable knowledge across a
distribution of tasks constructed from unlabeled data. Here, the main challenge
is how to construct diverse tasks for meta-learning without label information;
recent works have proposed to create, e.g., pseudo-labeling via pretrained
representations or creating synthetic samples via generative models. However,
such a task construction strategy is fundamentally limited due to heavy
reliance on the immutable pseudo-labels during meta-learning and the quality of
the representations or the generated samples. To overcome the limitations, we
propose a simple yet effective unsupervised meta-learning framework, coined
Pseudo-supervised Contrast (PsCo), for few-shot classification. We are inspired
by the recent self-supervised learning literature; PsCo utilizes a momentum
network and a queue of previous batches to improve pseudo-labeling and
construct diverse tasks in a progressive manner. Our extensive experiments
demonstrate that PsCo outperforms existing unsupervised meta-learning methods
under various in-domain and cross-domain few-shot classification benchmarks. We
also validate that PsCo is easily scalable to a large-scale benchmark, while
recent prior-art meta-schemes are not.",None,-1
3d08c173-7c43-49fa-9f7d-3e3e53ad6ac9,SHAP-IQ: Unified Approximation of any-order Shapley Interactions,0.379623,"Predominately in explainable artificial intelligence (XAI) research, the
Shapley value (SV) is applied to determine feature attributions for any black
box model. Shapley interaction indices extend the SV to define any-order
feature interactions. Defining a unique Shapley interaction index is an open
research question and, so far, three definitions have been proposed, which
differ by their choice of axioms. Moreover, each definition requires a specific
approximation technique. Here, we propose SHAPley Interaction Quantification
(SHAP-IQ), an efficient sampling-based approximator to compute Shapley
interactions for arbitrary cardinal interaction indices (CII), i.e. interaction
indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based
on a novel representation and, in contrast to existing methods, we provide
theoretical guarantees for its approximation quality, as well as estimates for
the variance of the point estimates. For the special case of SV, our approach
reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP
with a greatly simplified calculation. We illustrate the computational
efficiency and effectiveness by explaining language, image classification and
high-dimensional synthetic models.",None,-1
e212b3b9-d746-4c8f-8187-ab4fa40548ca,Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection,0.678621,"The spread of disinformation and propagandistic content poses a threat to
societal harmony, undermining informed decision-making and trust in reliable
sources. Online platforms often serve as breeding grounds for such content, and
malicious actors exploit the vulnerabilities of audiences to shape public
opinion. Although there have been research efforts aimed at the automatic
identification of disinformation and propaganda in social media content, there
remain challenges in terms of performance. The ArAIEval shared task aims to
further research on these particular issues within the context of the Arabic
language. In this paper, we discuss our participation in these shared tasks. We
competed in subtasks 1A and 2A, where our submitted system secured positions
9th and 10th, respectively. Our experiments consist of fine-tuning transformer
models and using zero- and few-shot learning with GPT-4.",None,-1
09bba4d3-eb32-4e47-b468-afc5e1c41f7c,Granular-ball Optimization Algorithm,0.111733,"The existing intelligent optimization algorithms are designed based on the
finest granularity, i.e., a point. This leads to weak global search ability and
inefficiency. To address this problem, we proposed a novel multi-granularity
optimization algorithm, namely granular-ball optimization algorithm (GBO), by
introducing granular-ball computing. GBO uses many granular-balls to cover the
solution space. Quite a lot of small and fine-grained granular-balls are used
to depict the important parts, and a little number of large and coarse-grained
granular-balls are used to depict the inessential parts. Fine multi-granularity
data description ability results in a higher global search capability and
faster convergence speed. In comparison with the most popular and
state-of-the-art algorithms, the experiments on twenty benchmark functions
demonstrate its better performance. The faster speed, higher approximation
ability of optimal solution, no hyper-parameters, and simpler design of GBO
make it an all-around replacement of most of the existing popular intelligent
optimization algorithms.",None,-1
52ed57cb-3213-40c5-920b-22e0366d3314,Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning,0.42661,"Few-shot class-incremental learning (FSCIL) presents the primary challenge of
balancing underfitting to a new session's task and forgetting the tasks from
previous sessions. To address this challenge, we develop a simple yet powerful
learning scheme that integrates effective methods for each core component of
the FSCIL network, including the feature extractor, base session classifiers,
and incremental session classifiers. In feature extractor training, our goal is
to obtain balanced generic representations that benefit both current viewable
and unseen or past classes. To achieve this, we propose a balanced supervised
contrastive loss that effectively balances these two objectives. In terms of
classifiers, we analyze and emphasize the importance of unifying initialization
methods for both the base and incremental session classifiers. Our method
demonstrates outstanding ability for new task learning and preventing
forgetting on CUB200, CIFAR100, and miniImagenet datasets, with significant
improvements over previous state-of-the-art methods across diverse metrics. We
conduct experiments to analyze the significance and rationale behind our
approach and visualize the effectiveness of our representations on new tasks.
Furthermore, we conduct diverse ablation studies to analyze the effects of each
module.",None,-1
940e5e15-e3db-4e29-bd00-346dba9fccf2,Reimagining Retrieval Augmented Language Models for Answering Queries,0.112884,"We present a reality check on large language models and inspect the promise
of retrieval augmented language models in comparison. Such language models are
semi-parametric, where models integrate model parameters and knowledge from
external data sources to make their predictions, as opposed to the parametric
nature of vanilla large language models. We give initial experimental findings
that semi-parametric architectures can be enhanced with views, a query
analyzer/planner, and provenance to make a significantly more powerful system
for question answering in terms of accuracy and efficiency, and potentially for
other NLP tasks",None,-1
257a4de4-2de3-45b0-8453-6dfcbf58e086,MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models,0.98973,"Large language models (LLMs) have achieved remarkable performance in natural
language understanding and generation tasks. However, they often suffer from
limitations such as difficulty in incorporating new knowledge, generating
hallucinations, and explaining their reasoning process. To address these
challenges, we propose a novel prompting pipeline, named \method, that
leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.
Our method enables LLMs to comprehend KG inputs and infer with a combination of
implicit and external knowledge. Moreover, our method elicits the mind map of
LLMs, which reveals their reasoning pathways based on the ontology of
knowledge. We evaluate our method on diverse question \& answering tasks,
especially in medical domains, and show significant improvements over
baselines. We also introduce a new hallucination evaluation benchmark and
analyze the effects of different components of our method. Our results
demonstrate the effectiveness and robustness of our method in merging knowledge
from LLMs and KGs for combined inference. To reproduce our results and extend
the framework further, we make our codebase available at
https://github.com/wyl-willing/MindMap.",None,-1
1174cca2-60e3-4ea8-805f-debe7b6bbf69,ChatGPT Hallucinates when Attributing Answers,0.255414,"Can ChatGPT provide evidence to support its answers? Does the evidence it
suggests actually exist and does it really support its answer? We investigate
these questions using a collection of domain-specific knowledge-based
questions, specifically prompting ChatGPT to provide both an answer and
supporting evidence in the form of references to external sources. We also
investigate how different prompts impact answers and evidence. We find that
ChatGPT provides correct or partially correct answers in about half of the
cases (50.6% of the times), but its suggested references only exist 14% of the
times. We further provide insights on the generated references that reveal
common traits among the references that ChatGPT generates, and show how even if
a reference provided by the model does exist, this reference often does not
support the claims ChatGPT attributes to it. Our findings are important because
(1) they are the first systematic analysis of the references created by ChatGPT
in its answers; (2) they suggest that the model may leverage good quality
information in producing correct answers, but is unable to attribute real
evidence to support its answers. Prompts, raw result files and manual analysis
are made publicly available.",None,-1
d82b62e8-13cf-4685-a4b2-6f1c10349ef4,Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems,0.412486,"Transferring the knowledge of large language models (LLMs) is a promising
technique to incorporate linguistic knowledge into end-to-end automatic speech
recognition (ASR) systems. However, existing works only transfer a single
representation of LLM (e.g. the last layer of pretrained BERT), while the
representation of a text is inherently non-unique and can be obtained variously
from different layers, contexts and models. In this work, we explore a wide
range of techniques to obtain and transfer multiple representations of LLMs
into a transducer-based ASR system. While being conceptually simple, we show
that transferring multiple representations of LLMs can be an effective
alternative to transferring only a single representation.",None,-1
d59d75b6-994d-4b1d-a1c2-9eb1d795fc4d,A Simple Explanation for the Phase Transition in Large Language Models with List Decoding,0.0212877,"Various recent experimental results show that large language models (LLM)
exhibit emergent abilities that are not present in small models. System
performance is greatly improved after passing a certain critical threshold of
scale. In this letter, we provide a simple explanation for such a phase
transition phenomenon. For this, we model an LLM as a sequence-to-sequence
random function. Instead of using instant generation at each step, we use a
list decoder that keeps a list of candidate sequences at each step and defers
the generation of the output sequence at the end. We show that there is a
critical threshold such that the expected number of erroneous candidate
sequences remains bounded when an LLM is below the threshold, and it grows
exponentially when an LLM is above the threshold. Such a threshold is related
to the basic reproduction number in a contagious disease.",None,-1
ed525ae0-63d8-40c7-8266-1835657ce18e,SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised Point Cloud Shape Correspondence,0.662247,"Unsupervised point cloud shape correspondence aims to obtain dense
point-to-point correspondences between point clouds without manually annotated
pairs. However, humans and some animals have bilateral symmetry and various
orientations, which lead to severe mispredictions of symmetrical parts.
Besides, point cloud noise disrupts consistent representations for point cloud
and thus degrades the shape correspondence accuracy. To address the above
issues, we propose a Self-Ensembling ORientation-aware Network termed SE-ORNet.
The key of our approach is to exploit an orientation estimation module with a
domain adaptive discriminator to align the orientations of point cloud pairs,
which significantly alleviates the mispredictions of symmetrical parts.
Additionally, we design a selfensembling framework for unsupervised point cloud
shape correspondence. In this framework, the disturbances of point cloud noise
are overcome by perturbing the inputs of the student and teacher networks with
different data augmentations and constraining the consistency of predictions.
Extensive experiments on both human and animal datasets show that our SE-ORNet
can surpass state-of-the-art unsupervised point cloud shape correspondence
methods.",None,-1
6c814b50-6a11-4034-90ba-65f231e92ec6,"TRESTLE: Toolkit for Reproducible Execution of Speech, Text and Language Experiments",0.249168,"The evidence is growing that machine and deep learning methods can learn the
subtle differences between the language produced by people with various forms
of cognitive impairment such as dementia and cognitively healthy individuals.
Valuable public data repositories such as TalkBank have made it possible for
researchers in the computational community to join forces and learn from each
other to make significant advances in this area. However, due to variability in
approaches and data selection strategies used by various researchers, results
obtained by different groups have been difficult to compare directly. In this
paper, we present TRESTLE (\textbf{T}oolkit for \textbf{R}eproducible
\textbf{E}xecution of \textbf{S}peech \textbf{T}ext and \textbf{L}anguage
\textbf{E}xperiments), an open source platform that focuses on two datasets
from the TalkBank repository with dementia detection as an illustrative domain.
Successfully deployed in the hackallenge (Hackathon/Challenge) of the
International Workshop on Health Intelligence at AAAI 2022, TRESTLE provides a
precise digital blueprint of the data pre-processing and selection strategies
that can be reused via TRESTLE by other researchers seeking comparable results
with their peers and current state-of-the-art (SOTA) approaches.",None,-1
3da63107-1e22-4606-a0c8-cd9513922c69,Transformer Based Implementation for Automatic Book Summarization,0.254361,"Document Summarization is the procedure of generating a meaningful and
concise summary of a given document with the inclusion of relevant and
topic-important points. There are two approaches: one is picking up the most
relevant statements from the document itself and adding it to the Summary known
as Extractive and the other is generating sentences for the Summary known as
Abstractive Summarization. Training a machine learning model to perform tasks
that are time-consuming or very difficult for humans to evaluate is a major
challenge. Book Abstract generation is one of such complex tasks. Traditional
machine learning models are getting modified with pre-trained transformers.
Transformer based language models trained in a self-supervised fashion are
gaining a lot of attention; when fine-tuned for Natural Language
Processing(NLP) downstream task like text summarization. This work is an
attempt to use Transformer based techniques for Abstract generation.",None,-1
5385c84e-7e07-448d-ab18-9816c25e58e4,Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing,0.694765,"We present Impossible Distillation, a novel framework for paraphrasing and
sentence summarization, that distills a high-quality dataset and model from a
low-quality teacher that itself cannot perform these tasks. Unlike prior works
that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific
architecture, we hypothesize and verify the paraphrastic proximity intrinsic to
pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in
the LM distribution. By identifying and distilling generations from these
subspaces, Impossible Distillation produces a high-quality dataset and model
even from GPT2-scale LMs. We evaluate our method on multiple benchmarks
spanning unconstrained / syntax-controlled paraphrase generation and sentence
summarization. Our model with 770M parameters consistently outperforms strong
baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT
itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher
diversity and fidelity than up to 13 times larger datasets.",None,-1
0889e5b3-68aa-4c1c-8d16-5e540f60ee99,Passive Radio Frequency-based 3D Indoor Positioning System via Ensemble Learning,0.832189,"Passive radio frequency (PRF)-based indoor positioning systems (IPS) have
attracted researchers' attention due to their low price, easy and customizable
configuration, and non-invasive design. This paper proposes a PRF-based
three-dimensional (3D) indoor positioning system (PIPS), which is able to use
signals of opportunity (SoOP) for positioning and also capture a scenario
signature. PIPS passively monitors SoOPs containing scenario signatures through
a single receiver. Moreover, PIPS leverages the Dynamic Data Driven
Applications System (DDDAS) framework to devise and customize the sampling
frequency, enabling the system to use the most impacted frequency band as the
rated frequency band. Various regression methods within three ensemble learning
strategies are used to train and predict the receiver position. The PRF
spectrum of 60 positions is collected in the experimental scenario, and three
criteria are applied to evaluate the performance of PIPS. Experimental results
show that the proposed PIPS possesses the advantages of high accuracy,
configurability, and robustness.",None,-1
ff6b941a-5440-4706-bd5a-89bec0588c33,PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs,0.219989,"Temporal facts, the facts for characterizing events that hold in specific
time periods, are attracting rising attention in the knowledge graph (KG)
research communities. In terms of quality management, the introduction of time
restrictions brings new challenges to maintaining the temporal consistency of
KGs and detecting potential temporal conflicts. Previous studies rely on
manually enumerated temporal constraints to detect conflicts, which are
labor-intensive and may have granularity issues. We start from the common
pattern of temporal facts and constraints and propose a pattern-based temporal
constraint mining method, PaTeCon. PaTeCon uses automatically determined graph
patterns and their relevant statistical information over the given KG instead
of human experts to generate time constraints. Specifically, PaTeCon
dynamically attaches class restriction to candidate constraints according to
their measuring scores.We evaluate PaTeCon on two large-scale datasets based on
Wikidata and Freebase respectively. The experimental results show that
pattern-based automatic constraint mining is powerful in generating valuable
temporal constraints.",None,-1
dac56b9e-478f-41b4-b813-76abd581e593,The Hardness of Reasoning about Probabilities and Causality,0.77687,"We study formal languages which are capable of fully expressing quantitative
probabilistic reasoning and do-calculus reasoning for causal effects, from a
computational complexity perspective. We focus on satisfiability problems whose
instance formulas allow expressing many tasks in probabilistic and causal
inference. The main contribution of this work is establishing the exact
computational complexity of these satisfiability problems. We introduce a new
natural complexity class, named succ$\exists$R, which can be viewed as a
succinct variant of the well-studied class $\exists$R, and show that the
problems we consider are complete for succ$\exists$R. Our results imply even
stronger algorithmic limitations than were proven by Fagin, Halpern, and
Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of
the standard languages used commonly in probabilistic and causal inference.",None,-1
e577ab7f-3342-4ccc-89c0-884254b829bc,USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense Active Learning for Super-resolution,0.397103,"Dense regression is a widely used approach in computer vision for tasks such
as image super-resolution, enhancement, depth estimation, etc. However, the
high cost of annotation and labeling makes it challenging to achieve accurate
results. We propose incorporating active learning into dense regression models
to address this problem. Active learning allows models to select the most
informative samples for labeling, reducing the overall annotation cost while
improving performance. Despite its potential, active learning has not been
widely explored in high-dimensional computer vision regression tasks like
super-resolution. We address this research gap and propose a new framework
called USIM-DAL that leverages the statistical properties of colour images to
learn informative priors using probabilistic deep neural networks that model
the heteroscedastic predictive distribution allowing uncertainty
quantification. Moreover, the aleatoric uncertainty from the network serves as
a proxy for error that is used for active learning. Our experiments on a wide
variety of datasets spanning applications in natural images (visual genome,
BSD100), medical imaging (histopathology slides), and remote sensing (satellite
images) demonstrate the efficacy of the newly proposed USIM-DAL and superiority
over several dense regression active learning methods.",None,-1
f0be3e87-e02b-4796-bdc7-a513a8f1df6c,DCFace: Synthetic Face Generation with Dual Condition Diffusion Model,0.993007,"Generating synthetic datasets for training face recognition models is
challenging because dataset generation entails more than creating high fidelity
images. It involves generating multiple images of same subjects under different
factors (\textit{e.g.}, variations in pose, illumination, expression, aging and
occlusion) which follows the real image conditional distribution. Previous
works have studied the generation of synthetic datasets using GAN or 3D models.
In this work, we approach the problem from the aspect of combining subject
appearance (ID) and external factor (style) conditions. These two conditions
provide a direct way to control the inter-class and intra-class variations. To
this end, we propose a Dual Condition Face Generator (DCFace) based on a
diffusion model. Our novel Patch-wise style extractor and Time-step dependent
ID loss enables DCFace to consistently produce face images of the same subject
under different styles with precise control. Face recognition models trained on
synthetic images from the proposed DCFace provide higher verification
accuracies compared to previous works by $6.11\%$ on average in $4$ out of $5$
test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code is available at
https://github.com/mk-minchul/dcface",None,-1
c84292f3-7290-4c79-9533-803612e47c0c,Learn to Not Link: Exploring NIL Prediction in Entity Linking,0.467514,"Entity linking models have achieved significant success via utilizing
pretrained language models to capture semantic features. However, the NIL
prediction problem, which aims to identify mentions without a corresponding
entity in the knowledge base, has received insufficient attention. We
categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase,
and propose an entity linking dataset NEL that focuses on the NIL prediction
problem. NEL takes ambiguous entities as seeds, collects relevant mention
context in the Wikipedia corpus, and ensures the presence of mentions linking
to NIL by human annotation and entity masking. We conduct a series of
experiments with the widely used bi-encoder and cross-encoder entity linking
models, results show that both types of NIL mentions in training data have a
significant influence on the accuracy of NIL prediction. Our code and dataset
can be accessed at https://github.com/solitaryzero/NIL_EL",None,-1
57aa06f7-db25-439b-afcf-9d48c43c13d0,Zero-Shot Co-salient Object Detection Framework,0.777138,"Co-salient Object Detection (CoSOD) endeavors to replicate the human visual
system's capacity to recognize common and salient objects within a collection
of images. Despite recent advancements in deep learning models, these models
still rely on training with well-annotated CoSOD datasets. The exploration of
training-free zero-shot CoSOD frameworks has been limited. In this paper,
taking inspiration from the zero-shot transfer capabilities of foundational
computer vision models, we introduce the first zero-shot CoSOD framework that
harnesses these models without any training process. To achieve this, we
introduce two novel components in our proposed framework: the group prompt
generation (GPG) module and the co-saliency map generation (CMP) module. We
evaluate the framework's performance on widely-used datasets and observe
impressive results. Our approach surpasses existing unsupervised methods and
even outperforms fully supervised methods developed before 2020, while
remaining competitive with some fully supervised methods developed before 2022.",None,-1
9db21e01-3b4f-472f-bdb9-c867ee405051,Learning Harmonic Molecular Representations on Riemannian Manifold,0.426129,"Molecular representation learning plays a crucial role in AI-assisted drug
discovery research. Encoding 3D molecular structures through Euclidean neural
networks has become the prevailing method in the geometric deep learning
community. However, the equivariance constraints and message passing in
Euclidean space may limit the network expressive power. In this work, we
propose a Harmonic Molecular Representation learning (HMR) framework, which
represents a molecule using the Laplace-Beltrami eigenfunctions of its
molecular surface. HMR offers a multi-resolution representation of molecular
geometric and chemical features on 2D Riemannian manifold. We also introduce a
harmonic message passing method to realize efficient spectral message passing
over the surface manifold for better molecular encoding. Our proposed method
shows comparable predictive power to current models in small molecule property
prediction, and outperforms the state-of-the-art deep learning models for
ligand-binding protein pocket classification and the rigid protein docking
challenge, demonstrating its versatility in molecular representation learning.",None,-1
3c576720-5005-436c-85c4-826ae7bb1e0d,Generative Semantic Segmentation,0.602743,"We present Generative Semantic Segmentation (GSS), a generative learning
approach for semantic segmentation. Uniquely, we cast semantic segmentation as
an image-conditioned mask generation problem. This is achieved by replacing the
conventional per-pixel discriminative learning with a latent prior learning
process. Specifically, we model the variational posterior distribution of
latent variables given the segmentation mask. To that end, the segmentation
mask is expressed with a special type of image (dubbed as maskige). This
posterior distribution allows to generate segmentation masks unconditionally.
To achieve semantic segmentation on a given image, we further introduce a
conditioning network. It is optimized by minimizing the divergence between the
posterior distribution of maskige (i.e., segmentation masks) and the latent
prior distribution of input training images. Extensive experiments on standard
benchmarks show that our GSS can perform competitively to prior art
alternatives in the standard semantic segmentation setting, whilst achieving a
new state of the art in the more challenging cross-domain setting.",None,-1
24cdbee8-464b-482a-b506-86c431056849,Semi-supervised learning made simple with self-supervised clustering,0.812127,"Self-supervised learning models have been shown to learn rich visual
representations without requiring human annotations. However, in many
real-world scenarios, labels are partially available, motivating a recent line
of work on semi-supervised methods inspired by self-supervised principles. In
this paper, we propose a conceptually simple yet empirically powerful approach
to turn clustering-based self-supervised methods such as SwAV or DINO into
semi-supervised learners. More precisely, we introduce a multi-task framework
merging a supervised objective using ground-truth labels and a self-supervised
objective relying on clustering assignments with a single cross-entropy loss.
This approach may be interpreted as imposing the cluster centroids to be class
prototypes. Despite its simplicity, we provide empirical evidence that our
approach is highly effective and achieves state-of-the-art performance on
CIFAR100 and ImageNet.",None,-1
30409f0e-c891-4d78-8d3a-a32712e44fb7,TherapyView: Visualizing Therapy Sessions with Temporal Topic Modeling and AI-Generated Arts,0.76477,"We present the TherapyView, a demonstration system to help therapists
visualize the dynamic contents of past treatment sessions, enabled by the
state-of-the-art neural topic modeling techniques to analyze the topical
tendencies of various psychiatric conditions and deep learning-based image
generation engine to provide a visual summary. The system incorporates temporal
modeling to provide a time-series representation of topic similarities at a
turn-level resolution and AI-generated artworks given the dialogue segments to
provide a concise representations of the contents covered in the session,
offering interpretable insights for therapists to optimize their strategies and
enhance the effectiveness of psychotherapy. This system provides a proof of
concept of AI-augmented therapy tools with e in-depth understanding of the
patient's mental state and enabling more effective treatment.",None,-1
17380dd1-77b1-478b-af7a-d8c40f3b0ff9,An Information-Theoretic Perspective on Variance-Invariance-Covariance Regularization,0.432141,"Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised
learning (SSL) method that has shown promising results on a variety of tasks.
However, the fundamental mechanisms underlying VICReg remain unexplored. In
this paper, we present an information-theoretic perspective on the VICReg
objective. We begin by deriving information-theoretic quantities for
deterministic networks as an alternative to unrealistic stochastic network
assumptions. We then relate the optimization of the VICReg objective to mutual
information optimization, highlighting underlying assumptions and facilitating
a constructive comparison with other SSL algorithms and derive a generalization
bound for VICReg, revealing its inherent advantages for downstream tasks.
Building on these results, we introduce a family of SSL methods derived from
information-theoretic principles that outperform existing SSL techniques.",None,-1
67490e2d-21c6-4bf7-81b1-b60eefc6a3be,Large-Scale Traffic Signal Control Using Constrained Network Partition and Adaptive Deep Reinforcement Learning,0.376614,"Multi-agent Deep Reinforcement Learning (MADRL) based traffic signal control
becomes a popular research topic in recent years. To alleviate the scalability
issue of completely centralized RL techniques and the non-stationarity issue of
completely decentralized RL techniques on large-scale traffic networks, some
literature utilizes a regional control approach where the whole network is
firstly partitioned into multiple disjoint regions, followed by applying the
centralized RL approach to each region. However, the existing partitioning
rules either have no constraints on the topology of regions or require the same
topology for all regions. Meanwhile, no existing regional control approach
explores the performance of optimal joint action in an exponentially growing
regional action space when intersections are controlled by 4-phase traffic
signals (EW, EWL, NS, NSL). In this paper, we propose a novel RL training
framework named RegionLight to tackle the above limitations. Specifically, the
topology of regions is firstly constrained to a star network which comprises
one center and an arbitrary number of leaves. Next, the network partitioning
problem is modeled as an optimization problem to minimize the number of
regions. Then, an Adaptive Branching Dueling Q-Network (ABDQ) model is proposed
to decompose the regional control task into several joint signal control
sub-tasks corresponding to particular intersections. Subsequently, these
sub-tasks maximize the regional benefits cooperatively. Finally, the global
control strategy for the whole network is obtained by concatenating the optimal
joint actions of all regions. Experimental results demonstrate the superiority
of our proposed framework over all baselines under both real and synthetic
datasets in all evaluation metrics.",None,-1
ea85e93f-7079-4906-96a1-ff03e077d096,Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods,0.34942,"We study how to efficiently combine formal methods, Monte Carlo Tree Search
(MCTS), and deep learning in order to produce high-quality receding horizon
policies in large Markov Decision processes (MDPs). In particular, we use
model-checking techniques to guide the MCTS algorithm in order to generate
offline samples of high-quality decisions on a representative set of states of
the MDP. Those samples can then be used to train a neural network that imitates
the policy used to generate them. This neural network can either be used as a
guide on a lower-latency MCTS online search, or alternatively be used as a
full-fledged policy when minimal latency is required. We use statistical model
checking to detect when additional samples are needed and to focus those
additional samples on configurations where the learnt neural network policy
differs from the (computationally-expensive) offline policy. We illustrate the
use of our method on MDPs that model the Frozen Lake and Pac-Man environments
-- two popular benchmarks to evaluate reinforcement-learning algorithms.",None,-1
14399b38-a7aa-4600-b5ff-06a6f8c20e70,At Your Fingertips: Extracting Piano Fingering Instructions from Videos,0.632121,"Piano fingering -- knowing which finger to use to play each note in a musical
piece, is a hard and important skill to master when learning to play the piano.
While some sheet music is available with expert-annotated fingering
information, most pieces lack this information, and people often resort to
learning the fingering from demonstrations in online videos. We consider the AI
task of automating the extraction of fingering information from videos. This is
a non-trivial task as fingers are often occluded by other fingers, and it is
often not clear from the video which of the keys were pressed, requiring the
synchronization of hand position information and knowledge about the notes that
were played. We show how to perform this task with high-accuracy using a
combination of deep-learning modules, including a GAN-based approach for
fine-tuning on out-of-domain data. We extract the fingering information with an
f1 score of 97\%. We run the resulting system on 90 videos, resulting in
high-quality piano fingering information of 150K notes, the largest available
dataset of piano-fingering to date.",None,-1
01f592ff-6746-4f6c-b9e4-5d5b8b0306fe,Normalization-Equivariant Neural Networks with Application to Image Denoising,0.237778,"In many information processing systems, it may be desirable to ensure that
any change of the input, whether by shifting or scaling, results in a
corresponding change in the system response. While deep neural networks are
gradually replacing all traditional automatic processing methods, they
surprisingly do not guarantee such normalization-equivariance (scale + shift)
property, which can be detrimental in many applications. To address this issue,
we propose a methodology for adapting existing neural networks so that
normalization-equivariance holds by design. Our main claim is that not only
ordinary convolutional layers, but also all activation functions, including the
ReLU (rectified linear unit), which are applied element-wise to the
pre-activated neurons, should be completely removed from neural networks and
replaced by better conditioned alternatives. To this end, we introduce
affine-constrained convolutions and channel-wise sort pooling layers as
surrogates and show that these two architectural modifications do preserve
normalization-equivariance without loss of performance. Experimental results in
image denoising show that normalization-equivariant neural networks, in
addition to their better conditioning, also provide much better generalization
across noise levels.",None,-1
3850aa9e-9080-4efd-92ca-54dadc673df9,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,0.212929,"We address the challenging problem of jointly inferring the 3D flow and
volumetric densities moving in a fluid from a monocular input video with a deep
neural network. Despite the complexity of this task, we show that it is
possible to train the corresponding networks without requiring any 3D ground
truth for training. In the absence of ground truth data we can train our model
with observations from real-world capture setups instead of relying on
synthetic reconstructions. We make this unsupervised training approach possible
by first generating an initial prototype volume which is then moved and
transported over time without the need for volumetric supervision. Our approach
relies purely on image-based losses, an adversarial discriminator network, and
regularization. Our method can estimate long-term sequences in a stable manner,
while achieving closely matching targets for inputs such as rising smoke
plumes.",None,-1
c235454b-9bbe-4ae2-ba9e-cde2e1f07824,InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems,0.446333,"Large language models (LLMs) have been used for diverse tasks in natural
language processing (NLP), yet remain under-explored for task-oriented dialogue
systems (TODS), especially for end-to-end TODS. We present InstructTODS, a
novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue
systems that can adapt to diverse domains without fine-tuning. By leveraging
LLMs, InstructTODS generates a proxy belief state that seamlessly translates
user intentions into dynamic queries for efficient interaction with any KB. Our
extensive experiments demonstrate that InstructTODS achieves comparable
performance to fully fine-tuned TODS in guiding dialogues to successful
completion without prior knowledge or task-specific data. Furthermore, a
rigorous human evaluation of end-to-end TODS shows that InstructTODS produces
dialogue responses that notably outperform both the gold responses and the
state-of-the-art TODS in terms of helpfulness, informativeness, and humanness.
Moreover, the effectiveness of LLMs in TODS is further supported by our
comprehensive evaluations on TODS subtasks: dialogue state tracking, intent
classification, and response generation. Code and implementations could be
found here https://github.com/WillyHC22/InstructTODS/",None,-1
5f8aca1b-c073-4415-aae7-560527aca2ae,NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support,0.590679,"We present a method for generating high-quality watertight manifold meshes
from multi-view input images. Existing volumetric rendering methods are robust
in optimization but tend to generate noisy meshes with poor topology.
Differentiable rasterization-based methods can generate high-quality meshes but
are sensitive to initialization. Our method combines the benefits of both
worlds; we take the geometry initialization obtained from neural volumetric
fields, and further optimize the geometry as well as a compact neural texture
representation with differentiable rasterizers. Through extensive experiments,
we demonstrate that our method can generate accurate mesh reconstructions with
faithful appearance that are comparable to previous volume rendering methods
while being an order of magnitude faster in rendering. We also show that our
generated mesh and neural texture reconstruction is compatible with existing
graphics pipelines and enables downstream 3D applications such as simulation.
Project page: https://sarahweiii.github.io/neumanifold/",None,-1
2590fc82-2dd3-4372-9031-5611f6ab187b,DAG Learning on the Permutahedron,0.890169,"We propose a continuous optimization framework for discovering a latent
directed acyclic graph (DAG) from observational data. Our approach optimizes
over the polytope of permutation vectors, the so-called Permutahedron, to learn
a topological ordering. Edges can be optimized jointly, or learned conditional
on the ordering via a non-differentiable subroutine. Compared to existing
continuous optimization approaches our formulation has a number of advantages
including: 1. validity: optimizes over exact DAGs as opposed to other
relaxations optimizing approximate DAGs; 2. modularity: accommodates any
edge-optimization procedure, edge structural parameterization, and optimization
loss; 3. end-to-end: either alternately iterates between node-ordering and
edge-optimization, or optimizes them jointly. We demonstrate, on real-world
data problems in protein-signaling and transcriptional network discovery, that
our approach lies on the Pareto frontier of two key metrics, the SID and SHD.",None,-1
030edee2-468d-4f1a-b0a5-4b65bd4e9c63,DeDrift: Robust Similarity Search under Content Drift,0.811387,"The statistical distribution of content uploaded and searched on media
sharing sites changes over time due to seasonal, sociological and technical
factors. We investigate the impact of this ""content drift"" for large-scale
similarity search tools, based on nearest neighbor search in embedding space.
Unless a costly index reconstruction is performed frequently, content drift
degrades the search accuracy and efficiency. The degradation is especially
severe since, in general, both the query and database distributions change.
  We introduce and analyze real-world image and video datasets for which
temporal information is available over a long time period. Based on the
learnings, we devise DeDrift, a method that updates embedding quantizers to
continuously adapt large-scale indexing structures on-the-fly. DeDrift almost
eliminates the accuracy degradation due to the query and database content drift
while being up to 100x faster than a full index reconstruction.",None,-1
980e62ab-f62f-479a-90c4-81531c8dca98,"Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators",0.865335,"Large language models that exhibit instruction-following behaviour represent
one of the biggest recent upheavals in conversational interfaces, a trend in
large part fuelled by the release of OpenAI's ChatGPT, a proprietary large
language model for text generation fine-tuned through reinforcement learning
from human feedback (LLM+RLHF). We review the risks of relying on proprietary
software and survey the first crop of open-source projects of comparable
architecture and functionality. The main contribution of this paper is to show
that openness is differentiated, and to offer scientific documentation of
degrees of openness in this fast-moving field. We evaluate projects in terms of
openness of code, training data, model weights, RLHF data, licensing,
scientific documentation, and access methods. We find that while there is a
fast-growing list of projects billing themselves as 'open source', many inherit
undocumented data of dubious legality, few share the all-important
instruction-tuning (a key site where human annotation labour is involved), and
careful scientific documentation is exceedingly rare. Degrees of openness are
relevant to fairness and accountability at all points, from data collection and
curation to model architecture, and from training and fine-tuning to release
and deployment.",None,-1
e3938ee7-0c97-428f-9d3b-22c7283aeb25,Truth Machines: Synthesizing Veracity in AI Language Models,0.261611,"As AI technologies are rolled out into healthcare, academia, human resources,
law, and a multitude of other domains, they become de-facto arbiters of truth.
But truth is highly contested, with many different definitions and approaches.
This article discusses the struggle for truth in AI systems and the general
responses to date. It then investigates the production of truth in InstructGPT,
a large language model, highlighting how data harvesting, model architectures,
and social feedback mechanisms weave together disparate understandings of
veracity. It conceptualizes this performance as an operationalization of truth,
where distinct, often conflicting claims are smoothly synthesized and
confidently presented into truth-statements. We argue that these same logics
and inconsistencies play out in Instruct's successor, ChatGPT, reiterating
truth as a non-trivial problem. We suggest that enriching sociality and
thickening ""reality"" are two promising vectors for enhancing the
truth-evaluating capacities of future language models. We conclude, however, by
stepping back to consider AI truth-telling as a social practice: what kind of
""truth"" do we as listeners desire?",None,-1
6840e3f3-2b24-45dc-afd2-e933ad3d9a9c,Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and Air Quality Index,0.550671,"This is a study on the potential widespread usage of alternative fuel
vehicles, linking them with the socio-economic status of the respective
consumers as well as the impact on the resulting air quality index. Research in
this area aims to leverage machine learning techniques in order to promote
appropriate policies for the proliferation of alternative fuel vehicles such as
electric vehicles with due justice to different population groups. Pearson
correlation coefficient is deployed in the modeling the relationships between
socio-economic data, air quality index and data on alternative fuel vehicles.
Linear regression is used to conduct predictive modeling on air quality index
as per the adoption of alternative fuel vehicles, based on socio-economic
factors. This work exemplifies artificial intelligence for social good.",None,-1
7de9b1b1-d144-4ceb-bd2f-17501f3d7296,PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records,0.278155,"This paper describes PULSAR, our system submission at the ImageClef 2023
MediQA-Sum task on summarising patient-doctor dialogues into clinical records.
The proposed framework relies on domain-specific pre-training, to produce a
specialised language model which is trained on task-specific natural data
augmented by synthetic data generated by a black-box LLM. We find limited
evidence towards the efficacy of domain-specific pre-training and data
augmentation, while scaling up the language model yields the best performance
gains. Our approach was ranked second and third among 13 submissions on task B
of the challenge. Our code is available at https://github.com/yuping-wu/PULSAR.",None,-1
3e736ceb-aee6-4506-8805-c2b1c219f9e7,Finite-sample Guarantees for Nash Q-learning with Linear Function Approximation,0.133282,"Nash Q-learning may be considered one of the first and most known algorithms
in multi-agent reinforcement learning (MARL) for learning policies that
constitute a Nash equilibrium of an underlying general-sum Markov game. Its
original proof provided asymptotic guarantees and was for the tabular case.
Recently, finite-sample guarantees have been provided using more modern RL
techniques for the tabular case. Our work analyzes Nash Q-learning using linear
function approximation -- a representation regime introduced when the state
space is large or continuous -- and provides finite-sample guarantees that
indicate its sample efficiency. We find that the obtained performance nearly
matches an existing efficient result for single-agent RL under the same
representation and has a polynomial gap when compared to the best-known result
for the tabular case.",None,-1
3752d862-6c5b-46eb-a40a-509c5eb5af04,dugMatting: Decomposed-Uncertainty-Guided Matting,0.396371,"Cutting out an object and estimating its opacity mask, known as image
matting, is a key task in image and video editing. Due to the highly ill-posed
issue, additional inputs, typically user-defined trimaps or scribbles, are
usually needed to reduce the uncertainty. Although effective, it is either time
consuming or only suitable for experienced users who know where to place the
strokes. In this work, we propose a decomposed-uncertainty-guided matting
(dugMatting) algorithm, which explores the explicitly decomposed uncertainties
to efficiently and effectively improve the results. Basing on the
characteristic of these uncertainties, the epistemic uncertainty is reduced in
the process of guiding interaction (which introduces prior knowledge), while
the aleatoric uncertainty is reduced in modeling data distribution (which
introduces statistics for both data and possible noise). The proposed matting
framework relieves the requirement for users to determine the interaction areas
by using simple and efficient labeling. Extensively quantitative and
qualitative results validate that the proposed method significantly improves
the original matting algorithms in terms of both efficiency and efficacy.",None,-1
d075da2d-23d7-44c3-a9ff-4019221d3e21,Learnable Ophthalmology SAM,0.607272,"Segmentation is vital for ophthalmology image analysis. But its various modal
images hinder most of the existing segmentation algorithms applications, as
they rely on training based on a large number of labels or hold weak
generalization ability. Based on Segment Anything (SAM), we propose a simple
but effective learnable prompt layer suitable for multiple target segmentation
in ophthalmology multi-modal images, named Learnable Ophthalmology Segment
Anything (SAM). The learnable prompt layer learns medical prior knowledge from
each transformer layer. During training, we only train the prompt layer and
task head based on a one-shot mechanism. We demonstrate the effectiveness of
our thought based on four medical segmentation tasks based on nine publicly
available datasets. Moreover, we only provide a new improvement thought for
applying the existing fundamental CV models in the medical field. Our codes are
available at \href{https://github.com/Qsingle/LearnablePromptSAM}{website}.",None,-1
f600ec70-150f-4644-b2fd-70fd1de33787,SwinIA: Self-Supervised Blind-Spot Image Denoising with Zero Convolutions,0.143228,"The essence of self-supervised image denoising is to restore the signal from
the noisy image alone. State-of-the-art solutions for this task rely on the
idea of masking pixels and training a fully-convolutional neural network to
impute them. This most often requires multiple forward passes, information
about the noise model, and intricate regularization functions. In this paper,
we propose a Swin Transformer-based Image Autoencoder (SwinIA), the first
convolution-free architecture for self-supervised denoising. It can be trained
end-to-end with a simple mean squared error loss without masking and does not
require any prior knowledge about clean data or noise distribution. Despite its
simplicity, SwinIA establishes state-of-the-art on several common benchmarks.",None,-1
fb241db8-3cc2-43c1-bb7e-d5767779ac05,GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer,0.731505,"Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.",None,-1
6014002f-4dbf-47c1-883a-13f9b7c7ccaf,Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks,0.813266,"The vision of the upcoming 6G technologies, characterized by ultra-dense
network, low latency, and fast data rate is to support Pervasive AI (PAI) using
zero-touch solutions enabling self-X (e.g., self-configuration,
self-monitoring, and self-healing) services. However, the research on 6G is
still in its infancy, and only the first steps have been taken to conceptualize
its design, investigate its implementation, and plan for use cases. Toward this
end, academia and industry communities have gradually shifted from theoretical
studies of AI distribution to real-world deployment and standardization. Still,
designing an end-to-end framework that systematizes the AI distribution by
allowing easier access to the service using a third-party application assisted
by a zero-touch service provisioning has not been well explored. In this
context, we introduce a novel platform architecture to deploy a zero-touch
PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart
system. This platform aims to standardize the pervasive AI at all levels of the
architecture and unify the interfaces in order to facilitate the service
deployment across application and infrastructure domains, relieve the users
worries about cost, security, and resource allocation, and at the same time,
respect the 6G stringent performance requirements. As a proof of concept, we
present a Federated Learning-as-a-service use case where we evaluate the
ability of our proposed system to self-optimize and self-adapt to the dynamics
of 6G networks in addition to minimizing the users' perceived costs.",None,-1
2f3a9c0a-88ec-4af1-a492-d4f04ac62fb9,LostNet: A smart way for lost and find,0.917915,"Due to the enormous population growth of cities in recent years, objects are
frequently lost and unclaimed on public transportation, in restaurants, or any
other public areas. While services like Find My iPhone can easily identify lost
electronic devices, more valuable objects cannot be tracked in an intelligent
manner, making it impossible for administrators to reclaim a large number of
lost and found items in a timely manner. We present a method that significantly
reduces the complexity of searching by comparing previous images of lost and
recovered things provided by the owner with photos taken when registered lost
and found items are received. In this research, we will primarily design a
photo matching network by combining the fine-tuning method of MobileNetv2 with
CBAM Attention and using the Internet framework to develop an online lost and
found image identification system. Our implementation gets a testing accuracy
of 96.8% using only 665.12M GLFOPs and 3.5M training parameters. It can
recognize practice images and can be run on a regular laptop.",None,-1
6e4437f6-b08a-4f92-a326-d9f3ee46bddc,A novel efficient Multi-view traffic-related object detection framework,0.684739,"With the rapid development of intelligent transportation system applications,
a tremendous amount of multi-view video data has emerged to enhance vehicle
perception. However, performing video analytics efficiently by exploiting the
spatial-temporal redundancy from video data remains challenging. Accordingly,
we propose a novel traffic-related framework named CEVAS to achieve efficient
object detection using multi-view video data. Briefly, a fine-grained input
filtering policy is introduced to produce a reasonable region of interest from
the captured images. Also, we design a sharing object manager to manage the
information of objects with spatial redundancy and share their results with
other vehicles. We further derive a content-aware model selection policy to
select detection methods adaptively. Experimental results show that our
framework significantly reduces response latency while achieving the same
detection accuracy as the state-of-the-art methods.",None,-1
1331d77f-f1fc-4fd7-b9f5-be58ce65c247,Mixing Backward- with Forward-Chaining for Metacognitive Skill Acquisition and Transfer,0.978073,"Metacognitive skills have been commonly associated with preparation for
future learning in deductive domains. Many researchers have regarded strategy-
and time-awareness as two metacognitive skills that address how and when to use
a problem-solving strategy, respectively. It was shown that students who are
both strategy-and time-aware (StrTime) outperformed their nonStrTime peers
across deductive domains. In this work, students were trained on a logic tutor
that supports a default forward-chaining (FC) and a backward-chaining (BC)
strategy. We investigated the impact of mixing BC with FC on teaching strategy-
and time-awareness for nonStrTime students. During the logic instruction, the
experimental students (Exp) were provided with two BC worked examples and some
problems in BC to practice how and when to use BC. Meanwhile, their control
(Ctrl) and StrTime peers received no such intervention. Six weeks later, all
students went through a probability tutor that only supports BC to evaluate
whether the acquired metacognitive skills are transferred from logic. Our
results show that on both tutors, Exp outperformed Ctrl and caught up with
StrTime.",None,-1
4e463616-5270-4e66-9360-546b00123a70,PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs,0.0907776,"In this paper, we develop a new method to automatically convert 2D line
drawings from three orthographic views into 3D CAD models. Existing methods for
this problem reconstruct 3D models by back-projecting the 2D observations into
3D space while maintaining explicit correspondence between the input and
output. Such methods are sensitive to errors and noises in the input, thus
often fail in practice where the input drawings created by human designers are
imperfect. To overcome this difficulty, we leverage the attention mechanism in
a Transformer-based sequence generation model to learn flexible mappings
between the input and output. Further, we design shape programs which are
suitable for generating the objects of interest to boost the reconstruction
accuracy and facilitate CAD modeling applications. Experiments on a new
benchmark dataset show that our method significantly outperforms existing ones
when the inputs are noisy or incomplete.",None,-1
9d0626df-821b-4328-96e5-e3c9dcc04db0,Procedural Content Generation via Knowledge Transformation (PCG-KT),0.0405491,"We introduce the concept of Procedural Content Generation via Knowledge
Transformation (PCG-KT), a new lens and framework for characterizing PCG
methods and approaches in which content generation is enabled by the process of
knowledge transformation -- transforming knowledge derived from one domain in
order to apply it in another. Our work is motivated by a substantial number of
recent PCG works that focus on generating novel content via repurposing derived
knowledge. Such works have involved, for example, performing transfer learning
on models trained on one game's content to adapt to another game's content, as
well as recombining different generative distributions to blend the content of
two or more games. Such approaches arose in part due to limitations in PCG via
Machine Learning (PCGML) such as producing generative models for games lacking
training data and generating content for entirely new games. In this paper, we
categorize such approaches under this new lens of PCG-KT by offering a
definition and framework for describing such methods and surveying existing
works using this framework. Finally, we conclude by highlighting open problems
and directions for future research in this area.",None,-1
8c272c41-eb8f-4703-aec4-8bc6df52fe0a,Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs,0.380221,"Despite the recent progress in text summarization made by large language
models (LLMs), they often generate summaries that are factually inconsistent
with original articles, known as ""hallucinations"" in text generation. Unlike
previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes
but more sophisticated ones, such as imposing cause and effect, adding false
details, overgeneralizing, etc. These hallucinations are challenging to detect
through traditional methods, which poses great challenges for improving the
factual consistency of text summarization. In this paper, we propose an
adversarially DEcoupling method to disentangle the Comprehension and
EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based
efficient training to cover the shortage of sensitivity for true and false in
the training process of LLMs. In this way, LLMs are less confused about
embellishing and understanding; thus, they can execute the instructions more
accurately and have enhanced abilities to distinguish hallucinations.
Experimental results show that DECENT significantly improves the reliability of
text summarization based on LLMs.",None,-1
1d3c724b-80ae-4e5f-8430-4362df6b0155,Both eyes open: Vigilant Incentives help Regulatory Markets improve AI Safety,0.184282,"In the context of rapid discoveries by leaders in AI, governments must
consider how to design regulation that matches the increasing pace of new AI
capabilities. Regulatory Markets for AI is a proposal designed with
adaptability in mind. It involves governments setting outcome-based targets for
AI companies to achieve, which they can show by purchasing services from a
market of private regulators. We use an evolutionary game theory model to
explore the role governments can play in building a Regulatory Market for AI
systems that deters reckless behaviour. We warn that it is alarmingly easy to
stumble on incentives which would prevent Regulatory Markets from achieving
this goal. These 'Bounty Incentives' only reward private regulators for
catching unsafe behaviour. We argue that AI companies will likely learn to
tailor their behaviour to how much effort regulators invest, discouraging
regulators from innovating. Instead, we recommend that governments always
reward regulators, except when they find that those regulators failed to detect
unsafe behaviour that they should have. These 'Vigilant Incentives' could
encourage private regulators to find innovative ways to evaluate cutting-edge
AI systems.",None,-1
d80b9044-6d03-40be-b3d2-37975767e620,Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification,0.468754,"Clothes-invariant feature extraction is critical to the clothes-changing
person re-identification (CC-ReID). It can provide discriminative identity
features and eliminate the negative effects caused by the confounder--clothing
changes. But we argue that there exists a strong spurious correlation between
clothes and human identity, that restricts the common likelihood-based ReID
method P(Y|X) to extract clothes-irrelevant features. In this paper, we propose
a new Causal Clothes-Invariant Learning (CCIL) method to achieve
clothes-invariant feature learning by modeling causal intervention P(Y|do(X)).
This new causality-based model is inherently invariant to the confounder in the
causal view, which can achieve the clothes-invariant features and avoid the
barrier faced by the likelihood-based methods. Extensive experiments on three
CC-ReID benchmarks, including PRCC, LTCC, and VC-Clothes, demonstrate the
effectiveness of our approach, which achieves a new state of the art.",None,-1
4e996435-debb-4178-8a3b-17666efb17bf,On Evaluating and Mitigating Gender Biases in Multilingual Settings,0.707932,"While understanding and removing gender biases in language models has been a
long-standing problem in Natural Language Processing, prior research work has
primarily been limited to English. In this work, we investigate some of the
challenges with evaluating and mitigating biases in multilingual settings which
stem from a lack of existing benchmarks and resources for bias evaluation
beyond English especially for non-western context. In this paper, we first
create a benchmark for evaluating gender biases in pre-trained masked language
models by extending DisCo to different Indian languages using human
annotations. We extend various debiasing methods to work beyond English and
evaluate their effectiveness for SOTA massively multilingual models on our
proposed metric. Overall, our work highlights the challenges that arise while
studying social biases in multilingual settings and provides resources as well
as mitigation techniques to take a step toward scaling to more languages.",None,-1
4c309588-2ca5-48c2-9cc6-3890b5114886,Randomized Adversarial Training via Taylor Expansion,0.552244,"In recent years, there has been an explosion of research into developing more
robust deep neural networks against adversarial examples. Adversarial training
appears as one of the most successful methods. To deal with both the robustness
against adversarial examples and the accuracy over clean examples, many works
develop enhanced adversarial training methods to achieve various trade-offs
between them. Leveraging over the studies that smoothed update on weights
during training may help find flat minima and improve generalization, we
suggest reconciling the robustness-accuracy trade-off from another perspective,
i.e., by adding random noise into deterministic weights. The randomized weights
enable our design of a novel adversarial training method via Taylor expansion
of a small Gaussian noise, and we show that the new adversarial training method
can flatten loss landscape and find flat minima. With PGD, CW, and Auto
Attacks, an extensive set of experiments demonstrate that our method enhances
the state-of-the-art adversarial training methods, boosting both robustness and
clean accuracy. The code is available at
https://github.com/Alexkael/Randomized-Adversarial-Training.",None,-1
6b06ee0b-fe7d-42ba-9c4b-cf6b233bcc2d,PASTA: Pretrained Action-State Transformer Agents,0.216436,"Self-supervised learning has brought about a revolutionary paradigm shift in
various computing domains, including NLP, vision, and biology. Recent
approaches involve pre-training transformer models on vast amounts of unlabeled
data, serving as a starting point for efficiently solving downstream tasks. In
reinforcement learning, researchers have recently adapted these approaches,
developing models pre-trained on expert trajectories. This advancement enables
the models to tackle a broad spectrum of tasks, ranging from robotics to
recommendation systems. However, existing methods mostly rely on intricate
pre-training objectives tailored to specific downstream applications. This
paper conducts a comprehensive investigation of models, referred to as
pre-trained action-state transformer agents (PASTA). Our study covers a unified
methodology and covers an extensive set of general downstream tasks including
behavioral cloning, offline RL, sensor failure robustness, and dynamics change
adaptation. Our objective is to systematically compare various design choices
and offer valuable insights that will aid practitioners in developing robust
models. Key highlights of our study include tokenization at the component level
for actions and states, the use of fundamental pre-training objectives such as
next token prediction or masked language modeling, simultaneous training of
models across multiple domains, and the application of various fine-tuning
strategies. In this study, the developed models contain fewer than 7 million
parameters allowing a broad community to use these models and reproduce our
experiments. We hope that this study will encourage further research into the
use of transformers with first principle design choices to represent RL
trajectories and contribute to robust policy learning.",None,-1
785e8aad-c989-48f8-8593-d05a8732ad74,Stochastic Directly-Follows Process Discovery Using Grammatical Inference,0.211783,"Starting with a collection of traces generated by process executions, process
discovery is the task of constructing a simple model that describes the
process, where simplicity is often measured in terms of model size. The
challenge of process discovery is that the process of interest is unknown, and
that while the input traces constitute positive examples of process executions,
no negative examples are available. Many commercial tools discover
Directly-Follows Graphs, in which nodes represent the observable actions of the
process, and directed arcs indicate execution order possibilities over the
actions. We propose a new approach for discovering sound Directly-Follows
Graphs that is grounded in grammatical inference over the input traces. To
promote the discovery of small graphs that also describe the process accurately
we design and evaluate a genetic algorithm that supports the convergence of the
inference parameters to the areas that lead to the discovery of interesting
models. Experiments over real-world datasets confirm that our new approach can
construct smaller models that represent the input traces and their frequencies
more accurately than the state-of-the-art technique. Reasoning over the
frequencies of encoded traces also becomes possible, due to the stochastic
semantics of the action graphs we propose, which, for the first time, are
interpreted as models that describe the stochastic languages of action traces.",None,-1
97bcf76f-26d6-4aeb-8ffd-c6662d6ef64b,An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning,1.0,"Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning
when a model forgets previously learned information while acquiring new
knowledge. As large language models (LLMs) have demonstrated remarkable
performance, it is intriguing to investigate whether CF exists during the
continual instruction tuning of LLMs. This study empirically evaluates the
forgetting phenomenon in LLMs' knowledge during continual instruction tuning
from the perspectives of domain knowledge, reasoning, and reading
comprehension. The experiments reveal that catastrophic forgetting is generally
observed in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale
increases, the severity of forgetting intensifies. Comparing the decoder-only
model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less
forgetting and retains more knowledge. Interestingly, we also observe that LLMs
can mitigate language biases, such as gender bias, during continual
fine-tuning. Furthermore, our findings indicate that ALPACA maintains more
knowledge and capacity compared to LLAMA during continual fine-tuning,
suggesting that general instruction tuning can help alleviate the forgetting
phenomenon in LLMs during subsequent fine-tuning processes.",None,-1
1b11d985-206d-4410-808d-11f13022e593,Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata,0.497441,"In this work, we explore the use of Large Language Models (LLMs) for
knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.
For this task, given subject and relation pairs sourced from Wikidata, we
utilize pre-trained LLMs to produce the relevant objects in string format and
link them to their respective Wikidata QIDs. We developed a pipeline using LLMs
for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata
entity mapping. The method achieved a macro-averaged F1-score of 0.701 across
the properties, with the scores varying from 1.00 to 0.328. These results
demonstrate that the knowledge of LLMs varies significantly depending on the
domain and that further experimentation is required to determine the
circumstances under which LLMs can be used for automatic Knowledge Base (e.g.,
Wikidata) completion and correction. The investigation of the results also
suggests the promising contribution of LLMs in collaborative knowledge
engineering. LLMKE won Track 2 of the challenge. The implementation is
available at https://github.com/bohuizhang/LLMKE.",None,-1
8e7dd217-ec49-4242-bc91-104efbd3ceb3,Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples,0.719559,"The objective of this work is to explore the learning of visually grounded
speech models (VGS) from multilingual perspective. Bilingual VGS models are
generally trained with an equal number of spoken captions from both languages.
However, in reality, there can be an imbalance among the languages for the
available spoken captions. Our key contribution in this work is to leverage the
power of a high-resource language in a bilingual visually grounded speech model
to improve the performance of a low-resource language. We introduce two methods
to distill the knowledge of high-resource language into low-resource languages:
(1) incorporating a strong pre-trained high-resource language encoder and (2)
using semantically similar spoken captions. Our experiments show that combining
these two approaches effectively enables the low-resource language to surpass
the performances of monolingual and bilingual counterparts for cross-modal
retrieval tasks.",None,-1
5f95f1f1-cb5d-4614-84f5-070990da8783,PandaGPT: One Model To Instruction-Follow Them All,0.999999,"We present PandaGPT, an approach to emPower large lANguage moDels with visual
and Auditory instruction-following capabilities. Our pilot experiments show
that PandaGPT can perform complex tasks such as detailed image description
generation, writing stories inspired by videos, and answering questions about
audios. More interestingly, PandaGPT can take multimodal inputs simultaneously
and compose their semantics naturally. For example, PandaGPT can connect how
objects look in an image/video and how they sound in an audio. To do so,
PandaGPT combines the multimodal encoders from ImageBind and the large language
models from Vicuna. Notably, only aligned image-text pairs are required for the
training of PandaGPT. Thanks to the strong capability of ImageBind in embedding
data from different modalities into the same space, PandaGPT displays emergent,
i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g.,
video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an
initial step toward building AGI that can perceive and understand inputs in
different modalities holistically, as we humans do. Our project page is at
https://panda-gpt.github.io/.",None,-1
4a6136fe-1ccd-49df-a444-9d294a37e7cc,Cos R-CNN for Online Few-shot Object Detection,0.111746,"We propose Cos R-CNN, a simple exemplar-based R-CNN formulation that is
designed for online few-shot object detection. That is, it is able to localise
and classify novel object categories in images with few examples without
fine-tuning. Cos R-CNN frames detection as a learning-to-compare task: unseen
classes are represented as exemplar images, and objects are detected based on
their similarity to these exemplars. The cosine-based classification head
allows for dynamic adaptation of classification parameters to the exemplar
embedding, and encourages the clustering of similar classes in embedding space
without the need for manual tuning of distance-metric hyperparameters. This
simple formulation achieves best results on the recently proposed 5-way
ImageNet few-shot detection benchmark, beating the online 1/5/10-shot scenarios
by more than 8/3/1%, as well as performing up to 20% better in online 20-way
few-shot VOC across all shots on novel classes.",None,-1
47097e39-2a6b-49ed-a113-13fa0fafd6db,Whose Opinions Do Language Models Reflect?,0.988972,"Language models (LMs) are increasingly being used in open-ended contexts,
where the opinions reflected by LMs in response to subjective queries can have
a profound impact, both on user satisfaction, as well as shaping the views of
society at large. In this work, we put forth a quantitative framework to
investigate the opinions reflected by LMs -- by leveraging high-quality public
opinion polls and their associated human responses. Using this framework, we
create OpinionsQA, a new dataset for evaluating the alignment of LM opinions
with those of 60 US demographic groups over topics ranging from abortion to
automation. Across topics, we find substantial misalignment between the views
reflected by current LMs and those of US demographic groups: on par with the
Democrat-Republican divide on climate change. Notably, this misalignment
persists even after explicitly steering the LMs towards particular demographic
groups. Our analysis not only confirms prior observations about the
left-leaning tendencies of some human feedback-tuned LMs, but also surfaces
groups whose opinions are poorly reflected by current LMs (e.g., 65+ and
widowed individuals). Our code and data are available at
https://github.com/tatsu-lab/opinions_qa.",None,-1
5b6e9662-2a40-4da6-ab30-6da274a82e2d,On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence,0.999999,"Large pre-trained models, also known as foundation models (FMs), are trained
in a task-agnostic manner on large-scale data and can be adapted to a wide
range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.
Despite their successes in language and vision tasks, we have yet seen an
attempt to develop foundation models for geospatial artificial intelligence
(GeoAI). In this work, we explore the promises and challenges of developing
multimodal foundation models for GeoAI. We first investigate the potential of
many existing FMs by testing their performances on seven tasks across multiple
geospatial subdomains including Geospatial Semantics, Health Geography, Urban
Geography, and Remote Sensing. Our results indicate that on several geospatial
tasks that only involve text modality such as toponym recognition, location
description recognition, and US state-level/county-level dementia time series
forecasting, these task-agnostic LLMs can outperform task-specific
fully-supervised models in a zero-shot or few-shot learning setting. However,
on other geospatial tasks, especially tasks that involve multiple data
modalities (e.g., POI-based urban function classification, street view
image-based urban noise intensity classification, and remote sensing image
scene classification), existing foundation models still underperform
task-specific models. Based on these observations, we propose that one of the
major challenges of developing a FM for GeoAI is to address the multimodality
nature of geospatial tasks. After discussing the distinct challenges of each
geospatial data modality, we suggest the possibility of a multimodal foundation
model which can reason over various types of geospatial data through geospatial
alignments. We conclude this paper by discussing the unique risks and
challenges to develop such a model for GeoAI.",None,-1
e91fd438-7dff-4f4c-9c5a-b59df6749ba1,Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors,0.375919,"A major challenge in the practical use of Machine Translation (MT) is that
users lack guidance to make informed decisions about when to rely on outputs.
Progress in quality estimation research provides techniques to automatically
assess MT quality, but these techniques have primarily been evaluated in vitro
by comparison against human judgments outside of a specific context of use.
This paper evaluates quality estimation feedback in vivo with a human study
simulating decision-making in high-stakes medical settings. Using Emergency
Department discharge instructions, we study how interventions based on quality
estimation versus backtranslation assist physicians in deciding whether to show
MT outputs to a patient. We find that quality estimation improves appropriate
reliance on MT, but backtranslation helps physicians detect more clinically
harmful errors that QE alone often misses.",None,-1
04e664ce-07c7-40ff-ba3b-3632296bd709,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),0.809814,"Recently, there has been a remarkable increase in the interest towards
skeleton-based action recognition within the research community, owing to its
various advantageous features, including computational efficiency,
representative features, and illumination invariance. Despite this, researchers
continue to explore and investigate the most optimal way to represent human
actions through skeleton representation and the extracted features. As a
result, the growth and availability of human action recognition datasets have
risen substantially. In addition, deep learning-based algorithms have gained
widespread popularity due to the remarkable advancements in various computer
vision tasks. Most state-of-the-art contributions in skeleton-based action
recognition incorporate a Graph Neural Network (GCN) architecture for
representing the human body and extracting features. Our research demonstrates
that Convolutional Neural Networks (CNNs) can attain comparable results to GCN,
provided that the proper training techniques, augmentations, and optimizers are
applied. Our approach has been rigorously validated, and we have achieved a
score of 95% on the NTU-60 dataset",None,-1
7fd760f2-ce49-4ebe-ad03-0be7874e00bb,SUnAA: Sparse Unmixing using Archetypal Analysis,0.283329,"This paper introduces a new sparse unmixing technique using archetypal
analysis (SUnAA). First, we design a new model based on archetypal analysis. We
assume that the endmembers of interest are a convex combination of endmembers
provided by a spectral library and that the number of endmembers of interest is
known. Then, we propose a minimization problem. Unlike most conventional sparse
unmixing methods, here the minimization problem is non-convex. We minimize the
optimization objective iteratively using an active set algorithm. Our method is
robust to the initialization and only requires the number of endmembers of
interest. SUnAA is evaluated using two simulated datasets for which results
confirm its better performance over other conventional and advanced techniques
in terms of signal-to-reconstruction error. SUnAA is also applied to Cuprite
dataset and the results are compared visually with the available geological map
provided for this dataset. The qualitative assessment demonstrates the
successful estimation of the minerals abundances and significantly improves the
detection of dominant minerals compared to the conventional regression-based
sparse unmixing methods. The Python implementation of SUnAA can be found at:
https://github.com/BehnoodRasti/SUnAA.",None,-1
bff30f11-9b84-46ca-99fe-c1c5c14abf2a,Large Language Model Enhanced Multi-Agent Systems for 6G Communications,0.897881,"The rapid development of the Large Language Model (LLM) presents huge
opportunities for 6G communications, e.g., network optimization and management
by allowing users to input task requirements to LLMs by nature language.
However, directly applying native LLMs in 6G encounters various challenges,
such as a lack of private communication data and knowledge, limited logical
reasoning, evaluation, and refinement abilities. Integrating LLMs with the
capabilities of retrieval, planning, memory, evaluation and reflection in
agents can greatly enhance the potential of LLMs for 6G communications. To this
end, we propose a multi-agent system with customized communication knowledge
and tools for solving communication related tasks using natural language,
comprising three components: (1) Multi-agent Data Retrieval (MDR), which
employs the condensate and inference agents to refine and summarize
communication knowledge from the knowledge base, expanding the knowledge
boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning
(MCP), which utilizes multiple planning agents to generate feasible solutions
for the communication related task from different perspectives based on the
retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which
utilizes the evaluation agent to assess the solutions, and applies the
reflexion agent and refinement agent to provide improvement suggestions for
current solutions. Finally, we validate the effectiveness of the proposed
multi-agent system by designing a semantic communication system, as a case
study of 6G communications.",None,-1
06efc933-39a2-47be-ab64-eaa22cc7b28a,Conditional Generation of Audio from Video via Foley Analogies,0.437592,"The sound effects that designers add to videos are designed to convey a
particular artistic effect and, thus, may be quite different from a scene's
true sound. Inspired by the challenges of creating a soundtrack for a video
that differs from its true sound, but that nonetheless matches the actions
occurring on screen, we propose the problem of conditional Foley. We present
the following contributions to address this problem. First, we propose a
pretext task for training our model to predict sound for an input video clip
using a conditional audio-visual clip sampled from another time within the same
source video. Second, we propose a model for generating a soundtrack for a
silent input video, given a user-supplied example that specifies what the video
should ""sound like"". We show through human studies and automated evaluation
metrics that our model successfully generates sound from video, while varying
its output according to the content of a supplied example. Project site:
https://xypb.github.io/CondFoleyGen/",None,-1
17ceb794-dcfc-4727-a0ee-6611cdc29420,Exploring Partial Knowledge Base Inference in Biomedical Entity Linking,0.437878,"Biomedical entity linking (EL) consists of named entity recognition (NER) and
named entity disambiguation (NED). EL models are trained on corpora labeled by
a predefined KB. However, it is a common scenario that only entities within a
subset of the KB are precious to stakeholders. We name this scenario partial
knowledge base inference: training an EL model with one KB and inferring on the
part of it without further training. In this work, we give a detailed
definition and evaluation procedures for this practically valuable but
significantly understudied scenario and evaluate methods from three
representative EL paradigms. We construct partial KB inference benchmarks and
witness a catastrophic degradation in EL performance due to dramatically
precision drop. Our findings reveal these EL paradigms can not correctly handle
unlinkable mentions (NIL), so they are not robust to partial KB inference. We
also propose two simple-and-effective redemption methods to combat the NIL
issue with little computational overhead. Codes are released at
https://github.com/Yuanhy1997/PartialKB-EL.",None,-1
11901ac9-03cb-4b75-a369-d7694bb50992,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,0.897062,"As large language models (LLMs) have become the norm in NLP, demonstrating
good performance in generation and reasoning tasks, one of its most fatal
disadvantages is the lack of factual correctness. Generating unfactual texts
not only leads to lower performances but also degrades the trust and validity
of their applications. Chain-of-Thought (CoT) prompting improves trust and
model performance on complex reasoning tasks by generating interpretable
reasoning chains, but still suffers from factuality concerns in
knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit
framework for CoT prompting, which seeks to increase prediction factuality by
post-editing reasoning chains according to external knowledge. Building on top
of GPT-3, our framework lead to accuracy improvements in multiple open-domain
question-answering tasks.",None,-1
3a0631f7-1b37-43bd-8cd7-07902c258cc0,Decentralized Adversarial Training over Graphs,0.0886389,"The vulnerability of machine learning models to adversarial attacks has been
attracting considerable attention in recent years. Most existing studies focus
on the behavior of stand-alone single-agent learners. In comparison, this work
studies adversarial training over graphs, where individual agents are subjected
to perturbations of varied strength levels across space. It is expected that
interactions by linked agents, and the heterogeneity of the attack models that
are possible over the graph, can help enhance robustness in view of the
coordination power of the group. Using a min-max formulation of diffusion
learning, we develop a decentralized adversarial training framework for
multi-agent systems. We analyze the convergence properties of the proposed
scheme for both convex and non-convex environments, and illustrate the enhanced
robustness to adversarial attacks.",None,-1
28edcd43-2c50-4ee3-845e-4cdd3755f857,HanoiT: Enhancing Context-aware Translation via Selective Context,0.25694,"Context-aware neural machine translation aims to use the document-level
context to improve translation quality. However, not all words in the context
are helpful. The irrelevant or trivial words may bring some noise and distract
the model from learning the relationship between the current sentence and the
auxiliary context. To mitigate this problem, we propose a novel end-to-end
encoder-decoder model with a layer-wise selection mechanism to sift and refine
the long document context. To verify the effectiveness of our method, extensive
experiments and extra quantitative analysis are conducted on four
document-level machine translation benchmarks. The experimental results
demonstrate that our model significantly outperforms previous models on all
datasets via the soft selection mechanism.",None,-1
3663ef41-b4a6-4fed-8059-813c512433d0,Traj-MAE: Masked Autoencoders for Trajectory Prediction,0.864259,"Trajectory prediction has been a crucial task in building a reliable
autonomous driving system by anticipating possible dangers. One key issue is to
generate consistent trajectory predictions without colliding. To overcome the
challenge, we propose an efficient masked autoencoder for trajectory prediction
(Traj-MAE) that better represents the complicated behaviors of agents in the
driving environment. Specifically, our Traj-MAE employs diverse masking
strategies to pre-train the trajectory encoder and map encoder, allowing for
the capture of social and temporal information among agents while leveraging
the effect of environment from multiple granularities. To address the
catastrophic forgetting problem that arises when pre-training the network with
multiple masking strategies, we introduce a continual pre-training framework,
which can help Traj-MAE learn valuable and diverse information from various
strategies efficiently. Our experimental results in both multi-agent and
single-agent settings demonstrate that Traj-MAE achieves competitive results
with state-of-the-art methods and significantly outperforms our baseline model.",None,-1
0ad1e116-2dab-4cdf-b0e7-9ee46544ff30,Deep Neural Networks for Encrypted Inference with TFHE,0.465609,"Fully homomorphic encryption (FHE) is an encryption method that allows to
perform computation on encrypted data, without decryption. FHE preserves the
privacy of the users of online services that handle sensitive data, such as
health data, biometrics, credit scores and other personal information. A common
way to provide a valuable service on such data is through machine learning and,
at this time, Neural Networks are the dominant machine learning model for
unstructured data. In this work we show how to construct Deep Neural Networks
(DNN) that are compatible with the constraints of TFHE, an FHE scheme that
allows arbitrary depth computation circuits. We discuss the constraints and
show the architecture of DNNs for two computer vision tasks. We benchmark the
architectures using the Concrete stack, an open-source implementation of TFHE.",None,-1
0a968261-697a-49d2-98f1-51851aa939eb,PMatch: Paired Masked Image Modeling for Dense Geometric Matching,0.772557,"Dense geometric matching determines the dense pixel-wise correspondence
between a source and support image corresponding to the same 3D structure.
Prior works employ an encoder of transformer blocks to correlate the two-frame
features. However, existing monocular pretraining tasks, e.g., image
classification, and masked image modeling (MIM), can not pretrain the
cross-frame module, yielding less optimal performance. To resolve this, we
reformulate the MIM from reconstructing a single masked image to reconstructing
a pair of masked images, enabling the pretraining of transformer module.
Additionally, we incorporate a decoder into pretraining for improved upsampling
results. Further, to be robust to the textureless area, we propose a novel
cross-frame global matching module (CFGM). Since the most textureless area is
planar surfaces, we propose a homography loss to further regularize its
learning. Combined together, we achieve the State-of-The-Art (SoTA) performance
on geometric matching. Codes and models are available at
https://github.com/ShngJZ/PMatch.",None,-1
d44ee76d-00d2-47d0-86e0-9165fa498537,Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization,0.548189,"Domain generalization (DG) is proposed to deal with the issue of domain
shift, which occurs when statistical differences exist between source and
target domains. However, most current methods do not account for a common
realistic scenario where the source and target domains have different classes.
To overcome this deficiency, open set domain generalization (OSDG) then emerges
as a more practical setting to recognize unseen classes in unseen domains. An
intuitive approach is to use multiple one-vs-all classifiers to define decision
boundaries for each class and reject the outliers as unknown. However, the
significant class imbalance between positive and negative samples often causes
the boundaries biased towards positive ones, resulting in misclassification for
known samples in the unseen target domain. In this paper, we propose a novel
meta-learning-based framework called dualistic MEta-learning with joint
DomaIn-Class matching (MEDIC), which considers gradient matching towards
inter-domain and inter-class splits simultaneously to find a generalizable
boundary balanced for all tasks. Experimental results demonstrate that MEDIC
not only outperforms previous methods in open set scenarios, but also maintains
competitive close set generalization ability at the same time. Our code is
available at https://github.com/zzwdx/MEDIC.",None,-1
730c477e-bd88-48cb-a18b-c331a82ecb13,Detecting Stance of Authorities towards Rumors in Arabic Tweets: A Preliminary Study,0.30504,"A myriad of studies addressed the problem of rumor verification in Twitter by
either utilizing evidence from the propagation networks or external evidence
from the Web. However, none of these studies exploited evidence from trusted
authorities. In this paper, we define the task of detecting the stance of
authorities towards rumors in tweets, i.e., whether a tweet from an authority
agrees, disagrees, or is unrelated to the rumor. We believe the task is useful
to augment the sources of evidence utilized by existing rumor verification
systems. We construct and release the first Authority STance towards Rumors
(AuSTR) dataset, where evidence is retrieved from authority timelines in Arabic
Twitter. Due to the relatively limited size of our dataset, we study the
usefulness of existing datasets for stance detection in our task. We show that
existing datasets are somewhat useful for the task; however, they are clearly
insufficient, which motivates the need to augment them with annotated data
constituting stance of authorities from Twitter.",None,-1
41807d98-e3fa-443d-a883-5e636934e6b2,DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion,0.770817,"Denosing diffusion model, as a generative model, has received a lot of
attention in the field of image generation recently, thanks to its powerful
generation capability. However, diffusion models have not yet received
sufficient research in the field of image fusion. In this article, we introduce
diffusion model to the image fusion field, treating the image fusion task as
image-to-image translation and designing two different conditional injection
modulation modules (i.e., style transfer modulation and wavelet modulation) to
inject coarse-grained style information and fine-grained high-frequency and
low-frequency information into the diffusion UNet, thereby generating fused
images. In addition, we also discussed the residual learning and the selection
of training objectives of the diffusion model in the image fusion task.
Extensive experimental results based on quantitative and qualitative
assessments compared with benchmarks demonstrates state-of-the-art results and
good generalization performance in image fusion tasks. Finally, it is hoped
that our method can inspire other works and gain insight into this field to
better apply the diffusion model to image fusion tasks. Code shall be released
for better reproducibility.",None,-1
6e40e3e7-3e00-415c-8c13-42ece462c361,Object Goal Navigation with Recursive Implicit Maps,0.790261,"Object goal navigation aims to navigate an agent to locations of a given
object category in unseen environments. Classical methods explicitly build maps
of environments and require extensive engineering while lacking semantic
information for object-oriented exploration. On the other hand, end-to-end
learning methods alleviate manual map design and predict actions using implicit
representations. Such methods, however, lack an explicit notion of geometry and
may have limited ability to encode navigation history. In this work, we propose
an implicit spatial map for object goal navigation. Our implicit map is
recursively updated with new observations at each step using a transformer. To
encourage spatial reasoning, we introduce auxiliary tasks and train our model
to reconstruct explicit maps as well as to predict visual features, semantic
labels and actions. Our method significantly outperforms the state of the art
on the challenging MP3D dataset and generalizes well to the HM3D dataset. We
successfully deploy our model on a real robot and achieve encouraging object
goal navigation results in real scenes using only a few real-world
demonstrations. Code, trained models and videos are available at
\url{https://www.di.ens.fr/willow/research/onav_rim/}.",None,-1
35516420-c7fb-4879-ac4f-b4fdde42611e,Mastering Strategy Card Game (Hearthstone) with Improved Techniques,0.596312,"Strategy card game is a well-known genre that is demanding on the intelligent
game-play and can be an ideal test-bench for AI. Previous work combines an
end-to-end policy function and an optimistic smooth fictitious play, which
shows promising performances on the strategy card game Legend of Code and
Magic. In this work, we apply such algorithms to Hearthstone, a famous
commercial game that is more complicated in game rules and mechanisms. We
further propose several improved techniques and consequently achieve
significant progress. For a machine-vs-human test we invite a Hearthstone
streamer whose best rank was top 10 of the official league in China region that
is estimated to be of millions of players. Our models defeat the human player
in all Best-of-5 tournaments of full games (including both deck building and
battle), showing a strong capability of decision making.",None,-1
faf05366-adfe-4186-a5ab-834573966836,Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-driven Approach,0.614179,"Just noticeable difference (JND) refers to the maximum visual change that
human eyes cannot perceive, and it has a wide range of applications in
multimedia systems. However, most existing JND approaches only focus on a
single modality, and rarely consider the complementary effects of multimodal
information. In this article, we investigate the JND modeling from an
end-to-end homologous multimodal perspective, namely hmJND-Net. Specifically,
we explore three important visually sensitive modalities, including saliency,
depth, and segmentation. To better utilize homologous multimodal information,
we establish an effective fusion method via summation enhancement and
subtractive offset, and align homologous multimodal features based on a
self-attention driven encoder-decoder paradigm. Extensive experimental results
on eight different benchmark datasets validate the superiority of our hmJND-Net
over eight representative methods.",None,-1
06a01169-b85b-43c8-a6c6-8666870f22eb,Safe Navigation: Training Autonomous Vehicles using Deep Reinforcement Learning in CARLA,0.118546,"Autonomous vehicles have the potential to revolutionize transportation, but
they must be able to navigate safely in traffic before they can be deployed on
public roads. The goal of this project is to train autonomous vehicles to make
decisions to navigate in uncertain environments using deep reinforcement
learning techniques using the CARLA simulator. The simulator provides a
realistic and urban environment for training and testing self-driving models.
Deep Q-Networks (DQN) are used to predict driving actions. The study involves
the integration of collision sensors, segmentation, and depth camera for better
object detection and distance estimation. The model is tested on 4 different
trajectories in presence of different types of 4-wheeled vehicles and
pedestrians. The segmentation and depth cameras were utilized to ensure
accurate localization of objects and distance measurement. Our proposed method
successfully navigated the self-driving vehicle to its final destination with a
high success rate without colliding with other vehicles, pedestrians, or going
on the sidewalk. To ensure the optimal performance of our reinforcement
learning (RL) models in navigating complex traffic scenarios, we implemented a
pre-processing step to reduce the state space. This involved processing the
images and sensor output before feeding them into the model. Despite
significantly decreasing the state space, our approach yielded robust models
that successfully navigated through traffic with high levels of safety and
accuracy.",None,-1
de549c2c-cb2d-484a-9b82-49a03eaeca62,MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,0.667286,"Monocular 3D object detection reveals an economical but challenging task in
autonomous driving. Recently center-based monocular methods have developed
rapidly with a great trade-off between speed and accuracy, where they usually
depend on the object center's depth estimation via 2D features. However, the
visual semantic features without sufficient pixel geometry information, may
affect the performance of clues for spatial 3D detection tasks. To alleviate
this, we propose MonoPGC, a novel end-to-end Monocular 3D object detection
framework with rich Pixel Geometry Contexts. We introduce the pixel depth
estimation as our auxiliary task and design depth cross-attention pyramid
module (DCPM) to inject local and global depth geometry knowledge into visual
features. In addition, we present the depth-space-aware transformer (DSAT) to
integrate 3D space position and depth-aware features efficiently. Besides, we
design a novel depth-gradient positional encoding (DGPE) to bring more distinct
pixel geometry contexts into the transformer for better object detection.
Extensive experiments demonstrate that our method achieves the state-of-the-art
performance on the KITTI dataset.",None,-1
a94a152d-fcfe-437c-afbe-cef0762a0ce8,Large Language Models Are State-of-the-Art Evaluators of Translation Quality,1.0,"We describe GEMBA, a GPT-based metric for assessment of translation quality,
which works both with a reference translation and without. In our evaluation,
we focus on zero-shot prompting, comparing four prompt variants in two modes,
based on the availability of the reference. We investigate nine versions of GPT
models, including ChatGPT and GPT-4. We show that our method for translation
quality assessment only works with GPT~3.5 and larger models. Comparing to
results from WMT22's Metrics shared task, our method achieves state-of-the-art
accuracy in both modes when compared to MQM-based human labels. Our results are
valid on the system level for all three WMT22 Metrics shared task language
pairs, namely English into German, English into Russian, and Chinese into
English. This provides a first glimpse into the usefulness of pre-trained,
generative large language models for quality assessment of translations. We
publicly release all our code and prompt templates used for the experiments
described in this work, as well as all corresponding scoring results, to allow
for external validation and reproducibility.",None,-1
dadbf3b2-2ce0-4893-85bb-d600c23c76c3,Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation,0.122263,"In this paper, we address the hallucination problem commonly found in natural
language generation tasks. Language models often generate fluent and convincing
content but can lack consistency with the provided source, resulting in
potential inaccuracies. We propose a new decoding method called
Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive
search framework with context-aware regularization terms. FECS promotes tokens
that are semantically similar to the provided source while penalizing
repetitiveness in the generated text. We demonstrate its effectiveness across
two tasks prone to hallucination: abstractive summarization and dialogue
generation. Results show that FECS consistently enhances faithfulness across
various language model sizes while maintaining output diversity comparable to
well-performing decoding algorithms.",None,-1
e3718841-4d72-47ad-b183-39430aea0956,AI-assisted coding: Experiments with GPT-4,0.715421,"Artificial intelligence (AI) tools based on large language models have
acheived human-level performance on some computer programming tasks. We report
several experiments using GPT-4 to generate computer code. These experiments
demonstrate that AI code generation using the current generation of tools,
while powerful, requires substantial human validation to ensure accurate
performance. We also demonstrate that GPT-4 refactoring of existing code can
significantly improve that code along several established metrics for code
quality, and we show that GPT-4 can generate tests with substantial coverage,
but that many of the tests fail when applied to the associated code. These
findings suggest that while AI coding tools are very powerful, they still
require humans in the loop to ensure validity and accuracy of the results.",None,-1
09f09e77-98d3-4b38-82f4-cae9e0b45d73,Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems,0.809944,"We present Soda (Symbolic Objective Descriptive Analysis), a language that
helps to treat qualities and quantities in a natural way and greatly simplifies
the task of checking their correctness. We present key properties for the
language motivated by the design of a descriptive language to encode complex
requirements on computer systems, and we explain how these key properties must
be addressed to model these requirements with simple definitions. We give an
overview of a tool that helps to describe problems in an easy way that we
consider more transparent and less error-prone.",None,-1
3e8a7ab4-f1f6-4ea3-8ef8-b2755ca1fb7e,Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model,0.324766,"This paper presents a series of approaches aimed at enhancing the performance
of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic
information from a Semantic Role Labeling (SRL) model. We propose a novel
end-to-end Semantic Role Labeling model that effectively captures most of the
structured semantic information within the Transformer hidden state. We believe
that this end-to-end model is well-suited for our newly proposed models that
incorporate semantic information. We evaluate the proposed models in two
languages, English and Czech, employing ELECTRA-small models. Our combined
models improve ABSA performance in both languages. Moreover, we achieved new
state-of-the-art results on the Czech ABSA.",None,-1
f6af92c4-b13d-445a-bdf5-f7957e95e026,A Simple and Effective Method of Cross-Lingual Plagiarism Detection,0.461543,"We present a simple cross-lingual plagiarism detection method applicable to a
large number of languages. The presented approach leverages open multilingual
thesauri for candidate retrieval task and pre-trained multilingual BERT-based
language models for detailed analysis. The method does not rely on machine
translation and word sense disambiguation when in use, and therefore is
suitable for a large number of languages, including under-resourced languages.
The effectiveness of the proposed approach is demonstrated for several existing
and new benchmarks, achieving state-of-the-art results for French, Russian, and
Armenian languages.",None,-1
ffa194c8-7c3b-4962-b820-e8ef4ba2f662,Synthcity: facilitating innovative use cases of synthetic data in different data modalities,0.796039,"Synthcity is an open-source software package for innovative use cases of
synthetic data in ML fairness, privacy and augmentation across diverse tabular
data modalities, including static data, regular and irregular time series, data
with censoring, multi-source data, composite data, and more. Synthcity provides
the practitioners with a single access point to cutting edge research and tools
in synthetic data. It also offers the community a playground for rapid
experimentation and prototyping, a one-stop-shop for SOTA benchmarks, and an
opportunity for extending research impact. The library can be accessed on
GitHub (https://github.com/vanderschaarlab/synthcity) and pip
(https://pypi.org/project/synthcity/). We warmly invite the community to join
the development effort by providing feedback, reporting bugs, and contributing
code.",None,-1
7947b3a2-fe91-408d-b891-3940bfa43aab,GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions,0.532998,"Image restoration in adverse weather conditions is a difficult task in
computer vision. In this paper, we propose a novel transformer-based framework
called GridFormer which serves as a backbone for image restoration under
adverse weather conditions. GridFormer is designed in a grid structure using a
residual dense transformer block, and it introduces two core designs. First, it
uses an enhanced attention mechanism in the transformer layer. The mechanism
includes stages of the sampler and compact self-attention to improve
efficiency, and a local enhancement stage to strengthen local information.
Second, we introduce a residual dense transformer block (RDTB) as the final
GridFormer layer. This design further improves the network's ability to learn
effective features from both preceding and current local features. The
GridFormer framework achieves state-of-the-art results on five diverse image
restoration tasks in adverse weather conditions, including image deraining,
dehazing, deraining \& dehazing, desnowing, and multi-weather restoration. The
source code and pre-trained models are available at
https://github.com/TaoWangzj/GridFormer.",None,-1
b1656814-172b-4e94-8514-578f63a3877d,Generative-Contrastive Learning for Self-Supervised Latent Representations of 3D Shapes from Multi-Modal Euclidean Input,0.283802,"We propose a combined generative and contrastive neural architecture for
learning latent representations of 3D volumetric shapes. The architecture uses
two encoder branches for voxel grids and multi-view images from the same
underlying shape. The main idea is to combine a contrastive loss between the
resulting latent representations with an additional reconstruction loss. That
helps to avoid collapsing the latent representations as a trivial solution for
minimizing the contrastive loss. A novel switching scheme is used to
cross-train two encoders with a shared decoder. The switching scheme also
enables the stop gradient operation on a random branch. Further classification
experiments show that the latent representations learned with our
self-supervised method integrate more useful information from the additional
input data implicitly, thus leading to better reconstruction and classification
performance.",None,-1
d0b17c34-3ce1-455c-83b8-4b18966b6eed,SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep Models for Kidney Stone Classification,0.572601,"Recently, deep learning has produced encouraging results for kidney stone
classification using endoscope images. However, the shortage of annotated
training data poses a severe problem in improving the performance and
generalization ability of the trained model. It is thus crucial to fully
exploit the limited data at hand. In this paper, we propose SegPrompt to
alleviate the data shortage problems by exploiting segmentation maps from two
aspects. First, SegPrompt integrates segmentation maps to facilitate
classification training so that the classification model is aware of the
regions of interest. The proposed method allows the image and segmentation
tokens to interact with each other to fully utilize the segmentation map
information. Second, we use the segmentation maps as prompts to tune the
pretrained deep model, resulting in much fewer trainable parameters than
vanilla finetuning. We perform extensive experiments on the collected kidney
stone dataset. The results show that SegPrompt can achieve an advantageous
balance between the model fitting ability and the generalization ability,
eventually leading to an effective model with limited training data.",None,-1
0c743f25-5a02-4072-9c3d-6fc9db402d5c,Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search,0.56061,"Anytime multi-agent path finding (MAPF) is a promising approach to scalable
path optimization in large-scale multi-agent systems. State-of-the-art anytime
MAPF is based on Large Neighborhood Search (LNS), where a fast initial solution
is iteratively optimized by destroying and repairing a fixed number of parts,
i.e., the neighborhood, of the solution, using randomized destroy heuristics
and prioritized planning. Despite their recent success in various MAPF
instances, current LNS-based approaches lack exploration and flexibility due to
greedy optimization with a fixed neighborhood size which can lead to low
quality solutions in general. So far, these limitations have been addressed
with extensive prior effort in tuning or offline machine learning beyond actual
planning. In this paper, we focus on online learning in LNS and propose
Bandit-based Adaptive LArge Neighborhood search Combined with Exploration
(BALANCE). BALANCE uses a bi-level multi-armed bandit scheme to adapt the
selection of destroy heuristics and neighborhood sizes on the fly during
search. We evaluate BALANCE on multiple maps from the MAPF benchmark set and
empirically demonstrate cost improvements of at least 50% compared to
state-of-the-art anytime MAPF in large-scale scenarios. We find that Thompson
Sampling performs particularly well compared to alternative multi-armed bandit
algorithms.",None,-1
3e496c78-8271-455b-8d55-7e63f85423e2,Source-Free Domain Adaptation for RGB-D Semantic Segmentation with Vision Transformers,0.558383,"With the increasing availability of depth sensors, multimodal frameworks that
combine color information with depth data are gaining interest. However, ground
truth data for semantic segmentation is burdensome to provide, thus making
domain adaptation a significant research area. Yet most domain adaptation
methods are not able to effectively handle multimodal data. Specifically, we
address the challenging source-free domain adaptation setting where the
adaptation is performed without reusing source data. We propose MISFIT:
MultImodal Source-Free Information fusion Transformer, a depth-aware framework
which injects depth data into a segmentation module based on vision
transformers at multiple stages, namely at the input, feature and output
levels. Color and depth style transfer helps early-stage domain alignment while
re-wiring self-attention between modalities creates mixed features, allowing
the extraction of better semantic content. Furthermore, a depth-based entropy
minimization strategy is also proposed to adaptively weight regions at
different distances. Our framework, which is also the first approach using
RGB-D vision transformers for source-free semantic segmentation, shows
noticeable performance improvements with respect to standard strategies.",None,-1
87bc61b7-82ca-4b6e-9740-fe8135d67fab,Deep Anomaly Detection under Labeling Budget Constraints,0.498994,"Selecting informative data points for expert feedback can significantly
improve the performance of anomaly detection (AD) in various contexts, such as
medical diagnostics or fraud detection. In this paper, we determine a set of
theoretical conditions under which anomaly scores generalize from labeled
queries to unlabeled data. Motivated by these results, we propose a data
labeling strategy with optimal data coverage under labeling budget constraints.
In addition, we propose a new learning framework for semi-supervised AD.
Extensive experiments on image, tabular, and video data sets show that our
approach results in state-of-the-art semi-supervised AD performance under
labeling budget constraints.",None,-1
8117fdfe-537f-40e7-b31e-a4ecb38404ea,See Through the Fog: Curriculum Learning with Progressive Occlusion in Medical Imaging,0.0550397,"In recent years, deep learning models have revolutionized medical image
interpretation, offering substantial improvements in diagnostic accuracy.
However, these models often struggle with challenging images where critical
features are partially or fully occluded, which is a common scenario in
clinical practice. In this paper, we propose a novel curriculum learning-based
approach to train deep learning models to handle occluded medical images
effectively. Our method progressively introduces occlusion, starting from
clear, unobstructed images and gradually moving to images with increasing
occlusion levels. This ordered learning process, akin to human learning, allows
the model to first grasp simple, discernable patterns and subsequently build
upon this knowledge to understand more complicated, occluded scenarios.
Furthermore, we present three novel occlusion synthesis methods, namely
Wasserstein Curriculum Learning (WCL), Information Adaptive Learning (IAL), and
Geodesic Curriculum Learning (GCL). Our extensive experiments on diverse
medical image datasets demonstrate substantial improvements in model robustness
and diagnostic accuracy over conventional training methodologies.",None,-1
da9a8f48-7fef-4427-b645-a81363e6c648,Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?,0.493521,"Centralized Training with Decentralized Execution (CTDE) has recently emerged
as a popular framework for cooperative Multi-Agent Reinforcement Learning
(MARL), where agents can use additional global state information to guide
training in a centralized way and make their own decisions only based on
decentralized local policies. Despite the encouraging results achieved, CTDE
makes an independence assumption on agent policies, which limits agents to
adopt global cooperative information from each other during centralized
training. Therefore, we argue that existing CTDE methods cannot fully utilize
global information for training, leading to an inefficient joint-policy
exploration and even suboptimal results. In this paper, we introduce a novel
Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent
reinforcement learning, that not only enables an efficacious message exchange
among agents during training but also guarantees the independent policies for
execution. Firstly, CADP endows agents the explicit communication channel to
seek and take advices from different agents for more centralized training. To
further ensure the decentralized execution, we propose a smooth model pruning
mechanism to progressively constraint the agent communication into a closed one
without degradation in agent cooperation capability. Empirical evaluations on
StarCraft II micromanagement and Google Research Football benchmarks
demonstrate that the proposed framework achieves superior performance compared
with the state-of-the-art counterparts. Our code will be made publicly
available.",None,-1
660c446c-8c18-4a17-8bd8-ccacfc146c2f,Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis,0.715418,"Multimodal relation extraction (MRE) is the task of identifying the semantic
relationships between two entities based on the context of the sentence image
pair. Existing retrieval-augmented approaches mainly focused on modeling the
retrieved textual knowledge, but this may not be able to accurately identify
complex relations. To improve the prediction, this research proposes to
retrieve textual and visual evidence based on the object, sentence, and whole
image. We further develop a novel approach to synthesize the object-level,
image-level, and sentence-level information for better reasoning between the
same and different modalities. Extensive experiments and analyses show that the
proposed method is able to effectively select and compare evidence across
modalities and significantly outperforms state-of-the-art models.",None,-1
4f91df43-cd66-47e8-9847-05c5b63279df,Mastering Strategy Card Game (Legends of Code and Magic) via End-to-End Policy and Optimistic Smooth Fictitious Play,0.66329,"Deep Reinforcement Learning combined with Fictitious Play shows impressive
results on many benchmark games, most of which are, however, single-stage. In
contrast, real-world decision making problems may consist of multiple stages,
where the observation spaces and the action spaces can be completely different
across stages. We study a two-stage strategy card game Legends of Code and
Magic and propose an end-to-end policy to address the difficulties that arise
in multi-stage game. We also propose an optimistic smooth fictitious play
algorithm to find the Nash Equilibrium for the two-player game. Our approach
wins double championships of COG2022 competition. Extensive studies verify and
show the advancement of our approach.",None,-1
7a15d322-2232-4761-ba6c-05584552b77d,Unsupervised Learning of Robust Spectral Shape Matching,0.801059,"We propose a novel learning-based approach for robust 3D shape matching. Our
method builds upon deep functional maps and can be trained in a fully
unsupervised manner. Previous deep functional map methods mainly focus on
predicting optimised functional maps alone, and then rely on off-the-shelf
post-processing to obtain accurate point-wise maps during inference. However,
this two-stage procedure for obtaining point-wise maps often yields sub-optimal
performance. In contrast, building upon recent insights about the relation
between functional maps and point-wise maps, we propose a novel unsupervised
loss to couple the functional maps and point-wise maps, and thereby directly
obtain point-wise maps without any post-processing. Our approach obtains
accurate correspondences not only for near-isometric shapes, but also for more
challenging non-isometric shapes and partial shapes, as well as shapes with
different discretisation or topological noise. Using a total of nine diverse
datasets, we extensively evaluate the performance and demonstrate that our
method substantially outperforms previous state-of-the-art methods, even
compared to recent supervised methods. Our code is available at
https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.",None,-1
ed48dbe4-38d4-4b9a-ade8-0943ca33379a,Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples,0.0973564,"Deploying deep visual models can lead to performance drops due to the
discrepancies between source and target distributions. Several approaches
leverage labeled source data to estimate target domain accuracy, but accessing
labeled source data is often prohibitively difficult due to data
confidentiality or resource limitations on serving devices. Our work proposes a
new framework to estimate model accuracy on unlabeled target data without
access to source data. We investigate the feasibility of using pseudo-labels
for accuracy estimation and evolve this idea into adopting recent advances in
source-free domain adaptation algorithms. Our approach measures the
disagreement rate between the source hypothesis and the target pseudo-labeling
function, adapted from the source hypothesis. We mitigate the impact of
erroneous pseudo-labels that may arise due to a high ideal joint hypothesis
risk by employing adaptive adversarial perturbation on the input of the target
model. Our proposed source-free framework effectively addresses the challenging
distribution shift scenarios and outperforms existing methods requiring source
data and labels for training.",None,-1
95c767e8-4e4e-4a63-a02c-1ba9d8e8df65,Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory,0.417567,"Human Object Interaction (HOI) detection aims to localize and infer the
relationships between a human and an object. Arguably, training supervised
models for this task from scratch presents challenges due to the performance
drop over rare classes and the high computational cost and time required to
handle long-tailed distributions of HOIs in complex HOI scenes in realistic
settings. This observation motivates us to design an HOI detector that can be
trained even with long-tailed labeled data and can leverage existing knowledge
from pre-trained models. Inspired by the powerful generalization ability of the
large Vision-Language Models (VLM) on classification and retrieval tasks, we
propose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM).
ADA-CM has two operating modes. The first mode makes it tunable without
learning new parameters in a training-free paradigm. Its second mode
incorporates an instance-aware adapter mechanism that can further efficiently
boost performance if updating a lightweight set of parameters can be afforded.
Our proposed method achieves competitive results with state-of-the-art on the
HICO-DET and V-COCO datasets with much less training time. Code can be found at
https://github.com/ltttpku/ADA-CM.",None,-1
642e1d84-e557-4f8a-93b5-d1031e68d0d5,Strivec: Sparse Tri-Vector Radiance Fields,0.83269,"We propose Strivec, a novel neural representation that models a 3D scene as a
radiance field with sparsely distributed and compactly factorized local tensor
feature grids. Our approach leverages tensor decomposition, following the
recent work TensoRF, to model the tensor grids. In contrast to TensoRF which
uses a global tensor and focuses on their vector-matrix decomposition, we
propose to utilize a cloud of local tensors and apply the classic
CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple
vectors that express local feature distributions along spatial axes and
compactly encode a local neural field. We also apply multi-scale tensor grids
to discover the geometry and appearance commonalities and exploit spatial
coherence with the tri-vector factorization at multiple local scales. The final
radiance field properties are regressed by aggregating neural features from
multiple local tensors across all scales. Our tri-vector tensors are sparsely
distributed around the actual scene surface, discovered by a fast coarse
reconstruction, leveraging the sparsity of a 3D scene. We demonstrate that our
model can achieve better rendering quality while using significantly fewer
parameters than previous methods, including TensoRF and Instant-NGP.",None,-1
bce80fc2-f683-4281-913a-7fbfb5e8b305,A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld,0.146537,"Building open-ended agents that can autonomously discover a diversity of
behaviours is one of the long-standing goals of artificial intelligence. This
challenge can be studied in the framework of autotelic RL agents, i.e. agents
that learn by selecting and pursuing their own goals, self-organizing a
learning curriculum. Recent work identified language as a key dimension of
autotelic learning, in particular because it enables abstract goal sampling and
guidance from social peers for hindsight relabelling. Within this perspective,
we study the following open scientific questions: What is the impact of
hindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can
the agent learn from very rare language goal examples in its experience replay?
How can multiple forms of exploration be combined, and take advantage of easier
goals as stepping stones to reach harder ones? To address these questions, we
use ScienceWorld, a textual environment with rich abstract and combinatorial
physics. We show the importance of selectivity from the social peer's feedback;
that experience replay needs to over-sample examples of rare goals; and that
following self-generated goal sequences where the agent's competence is
intermediate leads to significant improvements in final performance.",None,-1
2b224dfc-b267-4a83-aee5-8762db37cb0c,Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling,0.038327,"Dropout is a widely used regularization trick to resolve the overfitting
issue in large feedforward neural networks trained on a small dataset, which
performs poorly on the held-out test subset. Although the effectiveness of this
regularization trick has been extensively studied for convolutional neural
networks, there is a lack of analysis of it for unsupervised models and in
particular, VAE-based neural topic models. In this paper, we have analyzed the
consequences of dropout in the encoder as well as in the decoder of the VAE
architecture in three widely used neural topic models, namely, contextualized
topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly
available datasets. We characterize the dropout effect on these models in terms
of the quality and predictive performance of the generated topics.",None,-1
98c8198f-234b-4ef7-aa21-396f852642e4,Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data,0.314819,"Relation extraction (RE) aims to extract relations from sentences and
documents. Existing relation extraction models typically rely on supervised
machine learning. However, recent studies showed that many RE datasets are
incompletely annotated. This is known as the false negative problem in which
valid relations are falsely annotated as 'no_relation'. Models trained with
such data inevitably make similar mistakes during the inference stage.
Self-training has been proven effective in alleviating the false negative
problem. However, traditional self-training is vulnerable to confirmation bias
and exhibits poor performance in minority classes. To overcome this limitation,
we proposed a novel class-adaptive re-sampling self-training framework.
Specifically, we re-sampled the pseudo-labels for each class by precision and
recall scores. Our re-sampling strategy favored the pseudo-labels of classes
with high precision and low recall, which improved the overall recall without
significantly compromising precision. We conducted experiments on
document-level and biomedical relation extraction datasets, and the results
showed that our proposed self-training framework consistently outperforms
existing competitive methods on the Re-DocRED and ChemDisgene datasets when the
training data are incompletely annotated. Our code is released at
https://github.com/DAMO-NLP-SG/CAST.",None,-1
c208c0f6-9a34-4fdf-9384-b4d8cf0bc54b,Semiconductor Fab Scheduling with Self-Supervised and Reinforcement Learning,0.506828,"Semiconductor manufacturing is a notoriously complex and costly multi-step
process involving a long sequence of operations on expensive and
quantity-limited equipment. Recent chip shortages and their impacts have
highlighted the importance of semiconductors in the global supply chains and
how reliant on those our daily lives are. Due to the investment cost,
environmental impact, and time scale needed to build new factories, it is
difficult to ramp up production when demand spikes.
  This work introduces a method to successfully learn to schedule a
semiconductor manufacturing facility more efficiently using deep reinforcement
and self-supervised learning. We propose the first adaptive scheduling approach
to handle complex, continuous, stochastic, dynamic, modern semiconductor
manufacturing models. Our method outperforms the traditional hierarchical
dispatching strategies typically used in semiconductor manufacturing plants,
substantially reducing each order's tardiness and time until completion. As a
result, our method yields a better allocation of resources in the semiconductor
manufacturing process.",None,-1
93d46a3f-b051-4bd0-bfd0-e41dff17f114,2-D SSM: A General Spatial Layer for Visual Transformers,0.622787,"A central objective in computer vision is to design models with appropriate
2-D inductive bias. Desiderata for 2D inductive bias include two-dimensional
position awareness, dynamic spatial locality, and translation and permutation
invariance. To address these goals, we leverage an expressive variation of the
multidimensional State Space Model (SSM). Our approach introduces efficient
parameterization, accelerated computation, and a suitable normalization scheme.
Empirically, we observe that incorporating our layer at the beginning of each
transformer block of Vision Transformers (ViT) significantly enhances
performance for multiple ViT backbones and across datasets. The new layer is
effective even with a negligible amount of additional parameters and inference
time. Ablation studies and visualizations demonstrate that the layer has a
strong 2-D inductive bias. For example, vision transformers equipped with our
layer exhibit effective performance even without positional encoding",None,-1
3e77ac94-2072-4ad0-9121-69c57ea3bd74,Bridging the Granularity Gap for Acoustic Modeling,0.325915,"While Transformer has become the de-facto standard for speech, modeling upon
the fine-grained frame-level features remains an open challenge of capturing
long-distance dependencies and distributing the attention weights. We propose
\textit{Progressive Down-Sampling} (PDS) which gradually compresses the
acoustic features into coarser-grained units containing more complete semantic
information, like text-level representation. In addition, we develop a
representation fusion method to alleviate information loss that occurs
inevitably during high compression. In this way, we compress the acoustic
features into 1/32 of the initial length while achieving better or comparable
performances on the speech recognition task. And as a bonus, it yields
inference speedups ranging from 1.20$\times$ to 1.47$\times$. By reducing the
modeling burden, we also achieve competitive results when training on the more
challenging speech translation task.",None,-1
03e89b22-2df3-4c1c-8928-2d4f39ca053a,See and Think: Embodied Agent in Virtual Environment,0.447497,"Large language models (LLMs) have achieved impressive progress on several
open-world tasks. Recently, using LLMs to build embodied agents has been a
hotspot. In this paper, we propose STEVE, a comprehensive and visionary
embodied agent in the Minecraft virtual environment. STEVE consists of three
key components: vision perception, language instruction, and code action.
Vision perception involves the interpretation of visual information in the
environment, which is then integrated into the LLMs component with agent state
and task instruction. Language instruction is responsible for iterative
reasoning and decomposing complex tasks into manageable guidelines. Code action
generates executable skill actions based on retrieval in skill database,
enabling the agent to interact effectively within the Minecraft environment. We
also collect STEVE-21K dataset, which includes 600$+$ vision-environment pairs,
20K knowledge question-answering pairs, and 200$+$ skill-code pairs. We conduct
continuous block search, knowledge question and answering, and tech tree
mastery to evaluate the performance. Extensive experiments show that STEVE
achieves at most $1.5 \times$ faster unlocking key tech trees and $2.5 \times$
quicker in block search tasks compared to previous state-of-the-art methods.",None,-1
c65b6ec9-7cf1-4fed-94f0-152582fb2b8d,3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining,0.871314,"Masked autoencoders (MAE) have recently been introduced to 3D self-supervised
pretraining for point clouds due to their great success in NLP and computer
vision. Unlike MAEs used in the image domain, where the pretext task is to
restore features at the masked pixels, such as colors, the existing 3D MAE
works reconstruct the missing geometry only, i.e, the location of the masked
points. In contrast to previous studies, we advocate that point location
recovery is inessential and restoring intrinsic point features is much
superior. To this end, we propose to ignore point position reconstruction and
recover high-order features at masked points including surface normals and
surface variations, through a novel attention-based decoder which is
independent of the encoder design. We validate the effectiveness of our pretext
task and decoder design using different encoder structures for 3D training and
demonstrate the advantages of our pretrained networks on various point cloud
analysis tasks.",None,-1
3eee4107-bb92-44b2-8804-fc88ae4bc4f6,DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion,0.57696,"We propose a new formulation of temporal action detection (TAD) with
denoising diffusion, DiffTAD in short. Taking as input random temporal
proposals, it can yield action proposals accurately given an untrimmed long
video. This presents a generative modeling perspective, against previous
discriminative learning manners. This capability is achieved by first diffusing
the ground-truth proposals to random ones (i.e., the forward/noising process)
and then learning to reverse the noising process (i.e., the backward/denoising
process). Concretely, we establish the denoising process in the Transformer
decoder (e.g., DETR) by introducing a temporal location query design with
faster convergence in training. We further propose a cross-step selective
conditioning algorithm for inference acceleration. Extensive evaluations on
ActivityNet and THUMOS show that our DiffTAD achieves top performance compared
to previous art alternatives. The code will be made available at
https://github.com/sauradip/DiffusionTAD.",None,-1
440f0881-edf9-429e-a501-6a226f5402c9,Enhancing object detection robustness: A synthetic and natural perturbation approach,0.0773383,"Robustness against real-world distribution shifts is crucial for the
successful deployment of object detection models in practical applications. In
this paper, we address the problem of assessing and enhancing the robustness of
object detection models against natural perturbations, such as varying lighting
conditions, blur, and brightness. We analyze four state-of-the-art deep neural
network models, Detr-ResNet-101, Detr-ResNet-50, YOLOv4, and YOLOv4-tiny, using
the COCO 2017 dataset and ExDark dataset. By simulating synthetic perturbations
with the AugLy package, we systematically explore the optimal level of
synthetic perturbation required to improve the models robustness through data
augmentation techniques. Our comprehensive ablation study meticulously
evaluates the impact of synthetic perturbations on object detection models
performance against real-world distribution shifts, establishing a tangible
connection between synthetic augmentation and real-world robustness. Our
findings not only substantiate the effectiveness of synthetic perturbations in
improving model robustness, but also provide valuable insights for researchers
and practitioners in developing more robust and reliable object detection
models tailored for real-world applications.",None,-1
37adff8a-9577-4ae1-822a-5f14195da44b,Responsible AI Research Needs Impact Statements Too,0.415281,"All types of research, development, and policy work can have unintended,
adverse consequences - work in responsible artificial intelligence (RAI),
ethical AI, or ethics in AI is no exception.",None,-1
66eef24c-e86f-480e-8c0f-48abdac83bea,Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning,0.545031,"In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly
autonomously in solving a task, but can request help from an external expert
when needed. However, knowing when to request such assistance is critical: too
few requests can lead to the robot making mistakes, but too many requests can
overload the expert. In this paper, we present a Reinforcement Learning based
approach to this problem, where a semi-autonomous agent asks for external
assistance when it has low confidence in the eventual success of the task. The
confidence level is computed by estimating the variance of the return from the
current state. We show that this estimate can be iteratively improved during
training using a Bellman-like recursion. On discrete navigation problems with
both fully- and partially-observable state information, we show that our method
makes effective use of a limited budget of expert calls at run-time, despite
having no access to the expert at training time.",None,-1
ad12c3a2-da91-403c-ae50-fdeca0490f7b,HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification,0.197664,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification as the labels form a complex hierarchical structure.
Existing dual-encoder methods in HTC achieve weak performance gains with huge
memory overheads and their structure encoders heavily rely on domain knowledge.
Under such observation, we tend to investigate the feasibility of a
memory-friendly model with strong generalization capability that could boost
the performance of HTC without prior statistics or label semantics. In this
paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance
the text representations with only syntactic information of the label
hierarchy. Specifically, we convert the label hierarchy into an unweighted tree
structure, termed coding tree, with the guidance of structural entropy. Then we
design a structure encoder to incorporate hierarchy-aware information in the
coding tree into text representations. Besides the text encoder, HiTIN only
contains a few multi-layer perceptions and linear transformations, which
greatly saves memory. We conduct experiments on three commonly used datasets
and the results demonstrate that HiTIN could achieve better test performance
and less memory consumption than state-of-the-art (SOTA) methods.",None,-1
8afce7d8-49e6-4048-bc2a-236354c3e097,SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT,0.0918735,"Second language acquisition (SLA) research has extensively studied
cross-linguistic transfer, the influence of linguistic structure of a speaker's
native language [L1] on the successful acquisition of a foreign language [L2].
Effects of such transfer can be positive (facilitating acquisition) or negative
(impeding acquisition). We find that NLP literature has not given enough
attention to the phenomenon of negative transfer. To understand patterns of
both positive and negative transfer between L1 and L2, we model sequential
second language acquisition in LMs. Further, we build a Mutlilingual Age
Ordered CHILDES (MAO-CHILDES) -- a dataset consisting of 5 typologically
diverse languages, i.e., German, French, Polish, Indonesian, and Japanese -- to
understand the degree to which native Child-Directed Speech (CDS) [L1] can help
or conflict with English language acquisition [L2]. To examine the impact of
native CDS, we use the TILT-based cross lingual transfer learning approach
established by Papadimitriou and Jurafsky (2020) and find that, as in human
SLA, language family distance predicts more negative transfer. Additionally, we
find that conversational speech data shows greater facilitation for language
acquisition than scripted speech data. Our findings call for further research
using our novel Transformer-based SLA models and we would like to encourage it
by releasing our code, data, and models.",None,-1
59f75945-e887-4120-8c7b-503150de4cc0,Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information,0.241639,"Automotive radar has increasingly attracted attention due to growing interest
in autonomous driving technologies. Acquiring situational awareness using
multimodal data collected at high sampling rates by various sensing devices
including cameras, LiDAR, and radar requires considerable power, memory and
compute resources which are often limited at an edge device. In this paper, we
present a novel adaptive radar sub-sampling algorithm designed to identify
regions that require more detailed/accurate reconstruction based on prior
environmental conditions' knowledge, enabling near-optimal performance at
considerably lower effective sampling rates. Designed to robustly perform under
variable weather conditions, the algorithm was shown on the Oxford raw radar
and RADIATE dataset to achieve accurate reconstruction utilizing only 10% of
the original samples in good weather and 20% in extreme (snow, fog) weather
conditions. A further modification of the algorithm incorporates object motion
to enable reliable identification of important regions. This includes
monitoring possible future occlusions caused by objects detected in the present
frame. Finally, we train a YOLO network on the RADIATE dataset to perform
object detection directly on RADAR data and obtain a 6.6% AP50 improvement over
the baseline Faster R-CNN network.",None,-1
be656241-4d06-4363-abcb-d071eb03c7cd,Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation,0.684019,"Emotion regulation is a crucial element in dealing with emotional events and
has positive effects on mental health. This paper aims to provide a more
comprehensive understanding of emotional events by introducing a new French
corpus of emotional narratives collected using a questionnaire for emotion
regulation. We follow the theoretical framework of the Component Process Model
which considers emotions as dynamic processes composed of four interrelated
components (behavior, feeling, thinking and territory). Each narrative is
related to a discrete emotion and is structured based on all emotion components
by the writers. We study the interaction of components and their impact on
emotion classification with machine learning methods and pre-trained language
models. Our results show that each component improves prediction performance,
and that the best results are achieved by jointly considering all components.
Our results also show the effectiveness of pre-trained language models in
predicting discrete emotion from certain components, which reveal differences
in how emotion components are expressed.",None,-1
0e6d3c86-561c-4bb2-b591-f827b3e31b60,A Transformer-based Approach for Arabic Offline Handwritten Text Recognition,0.852553,"Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.",None,-1
7551623a-6942-43c5-9ab8-5e06681c91b9,Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection,0.827955,"Large Language Models (LLMs) can adapt to new tasks via in-context learning
(ICL). ICL is efficient as it does not require any parameter updates to the
trained LLM, but only few annotated examples as input for the LLM. In this
work, we investigate an active learning approach for ICL, where there is a
limited budget for annotating examples. We propose a model-adaptive
optimization-free algorithm, termed AdaICL, which identifies examples that the
model is uncertain about, and performs semantic diversity-based example
selection. Diversity-based sampling improves overall effectiveness, while
uncertainty sampling improves budget efficiency and helps the LLM learn new
information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage
problem, that dynamically adapts based on the model's feedback and can be
approximately solved via greedy algorithms. Extensive experiments on nine
datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy
points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient
than performing annotations uniformly at random, while it outperforms SOTA with
2x fewer ICL examples.",None,-1
19ff7a2c-282c-44af-9ae0-cb99d3d1008a,ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes,0.411352,"Understanding the continuous states of objects is essential for task learning
and planning in the real world. However, most existing task learning benchmarks
assume discrete (e.g., binary) object goal states, which poses challenges for
the learning of complex tasks and transferring learned policy from simulated
environments to the real world. Furthermore, state discretization limits a
robot's ability to follow human instructions based on the grounding of actions
and states. To tackle these challenges, we present ARNOLD, a benchmark that
evaluates language-grounded task learning with continuous states in realistic
3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve
understanding object states and learning policies for continuous goals. To
promote language-instructed learning, we provide expert demonstrations with
template-generated language descriptions. We assess task performance by
utilizing the latest language-conditioned policy learning models. Our results
indicate that current models for language-conditioned manipulations continue to
experience significant challenges in novel goal-state generalizations, scene
generalizations, and object generalizations. These findings highlight the need
to develop new algorithms that address this gap and underscore the potential
for further research in this area. Project website:
https://arnold-benchmark.github.io.",None,-1
f3f3fd1d-affb-4860-8811-b67d4a94b358,The Pipeline for the Continuous Development of Artificial Intelligence Models -- Current State of Research and Practice,0.617448,"Companies struggle to continuously develop and deploy AI models to complex
production systems due to AI characteristics while assuring quality. To ease
the development process, continuous pipelines for AI have become an active
research area where consolidated and in-depth analysis regarding the
terminology, triggers, tasks, and challenges is required. This paper includes a
Multivocal Literature Review where we consolidated 151 relevant formal and
informal sources. In addition, nine-semi structured interviews with
participants from academia and industry verified and extended the obtained
information. Based on these sources, this paper provides and compares
terminologies for DevOps and CI/CD for AI, MLOps, (end-to-end) lifecycle
management, and CD4ML. Furthermore, the paper provides an aggregated list of
potential triggers for reiterating the pipeline, such as alert systems or
schedules. In addition, this work uses a taxonomy creation strategy to present
a consolidated pipeline comprising tasks regarding the continuous development
of AI. This pipeline consists of four stages: Data Handling, Model Learning,
Software Development and System Operations. Moreover, we map challenges
regarding pipeline implementation, adaption, and usage for the continuous
development of AI to these four stages.",None,-1
02889e54-f8e7-48be-a7fe-30bd7939fd66,Story Visualization by Online Text Augmentation with Context Memory,0.139489,"Story visualization (SV) is a challenging text-to-image generation task for
the difficulty of not only rendering visual details from the text descriptions
but also encoding a long-term context across multiple sentences. While prior
efforts mostly focus on generating a semantically relevant image for each
sentence, encoding a context spread across the given paragraph to generate
contextually convincing images (e.g., with a correct character or with a proper
background of the scene) remains a challenge. To this end, we propose a novel
memory architecture for the Bi-directional Transformer framework with an online
text augmentation that generates multiple pseudo-descriptions as supplementary
supervision during training for better generalization to the language variation
at inference. In extensive experiments on the two popular SV benchmarks, i.e.,
the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms
the state of the arts in various metrics including FID, character F1, frame
accuracy, BLEU-2/3, and R-precision with similar or less computational
complexity.",None,-1
28bbc089-9bbc-4fee-9979-a0951b6ed3bc,Learning with Exposure Constraints in Recommendation Systems,0.414813,"Recommendation systems are dynamic economic systems that balance the needs of
multiple stakeholders. A recent line of work studies incentives from the
content providers' point of view. Content providers, e.g., vloggers and
bloggers, contribute fresh content and rely on user engagement to create
revenue and finance their operations. In this work, we propose a contextual
multi-armed bandit setting to model the dependency of content providers on
exposure. In our model, the system receives a user context in every round and
has to select one of the arms. Every arm is a content provider who must receive
a minimum number of pulls every fixed time period (e.g., a month) to remain
viable in later rounds; otherwise, the arm departs and is no longer available.
The system aims to maximize the users' (content consumers) welfare. To that
end, it should learn which arms are vital and ensure they remain viable by
subsidizing arm pulls if needed. We develop algorithms with sub-linear regret,
as well as a lower bound that demonstrates that our algorithms are optimal up
to logarithmic factors.",None,-1
5eb85344-c5a1-4054-8cd1-59f9994d3b7c,Context-Gloss Augmentation for Improving Arabic Target Sense Verification,0.770509,"Arabic language lacks semantic datasets and sense inventories. The most
common semantically-labeled dataset for Arabic is the ArabGlossBERT, a
relatively small dataset that consists of 167K context-gloss pairs (about 60K
positive and 107K negative pairs), collected from Arabic dictionaries. This
paper presents an enrichment to the ArabGlossBERT dataset, by augmenting it
using (Arabic-English-Arabic) machine back-translation. Augmentation increased
the dataset size to 352K pairs (149K positive and 203K negative pairs). We
measure the impact of augmentation using different data configurations to
fine-tune BERT on target sense verification (TSV) task. Overall, the accuracy
ranges between 78% to 84% for different data configurations. Although our
approach performed at par with the baseline, we did observe some improvements
for some POS tags in some experiments. Furthermore, our fine-tuned models are
trained on a larger dataset covering larger vocabulary and contexts. We provide
an in-depth analysis of the accuracy for each part-of-speech (POS).",None,-1
a88a7594-f3f1-4f2a-bb93-4cb5339ff4bc,Bandwidth-efficient Inference for Neural Image Compression,0.229427,"With neural networks growing deeper and feature maps growing larger, limited
communication bandwidth with external memory (or DRAM) and power constraints
become a bottleneck in implementing network inference on mobile and edge
devices. In this paper, we propose an end-to-end differentiable bandwidth
efficient neural inference method with the activation compressed by neural data
compression method. Specifically, we propose a transform-quantization-entropy
coding pipeline for activation compression with symmetric exponential Golomb
coding and a data-dependent Gaussian entropy model for arithmetic coding.
Optimized with existing model quantization methods, low-level task of image
compression can achieve up to 19x bandwidth reduction with 6.21x energy saving.",None,-1
560c4e6d-317d-4274-8781-bf98f4ccb23f,Semantic relatedness in DBpedia: A comparative and experimental assessment,0.407235,"Evaluating semantic relatedness of Web resources is still an open challenge.
This paper focuses on knowledge-based methods, which represent an alternative
to corpus-based approaches, and rely in general on the availability of
knowledge graphs. In particular, we have selected 10 methods from the existing
literature, that have been organized according to it adjacent resources, triple
patterns, and triple weights-based methods. They have been implemented and
evaluated by using DBpedia as reference RDF knowledge graph. Since DBpedia is
continuously evolving, the experimental results provided by these methods in
the literature are not comparable. For this reason, in this work, such methods
have been experimented by running them all at once on the same DBpedia release
and against 14 well-known golden datasets. On the basis of the correlation
values with human judgment obtained according to the experimental results,
weighting the RDF triples in combination with evaluating all the directed paths
linking the compared resources is the best strategy in order to compute
semantic relatedness in DBpedia.",None,-1
2c0079a6-131e-47c1-9f6f-008a147f3bd6,Robust Multi-Agent Pickup and Delivery with Delays,0.620802,"Multi-Agent Pickup and Delivery (MAPD) is the problem of computing
collision-free paths for a group of agents such that they can safely reach
delivery locations from pickup ones. These locations are provided at runtime,
making MAPD a combination between classical Multi-Agent Path Finding (MAPF) and
online task assignment. Current algorithms for MAPD do not consider many of the
practical issues encountered in real applications: real agents often do not
follow the planned paths perfectly, and may be subject to delays and failures.
In this paper, we study the problem of MAPD with delays, and we present two
solution approaches that provide robustness guarantees by planning paths that
limit the effects of imperfect execution. In particular, we introduce two
algorithms, k-TP and p-TP, both based on a decentralized algorithm typically
used to solve MAPD, Token Passing (TP), which offer deterministic and
probabilistic guarantees, respectively. Experimentally, we compare our
algorithms against a version of TP enriched with online replanning. k-TP and
p-TP provide robust solutions, significantly reducing the number of replans
caused by delays, with little or no increase in solution cost and running time.",None,-1
ea8f8173-9878-4ec2-a83c-eeb874627d06,A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,0.667166,"Recent instruction fine-tuned models can solve multiple NLP tasks when
prompted to do so, with machine translation (MT) being a prominent use case.
However, current research often focuses on standard performance benchmarks,
leaving compelling fairness and ethical considerations behind. In MT, this
might lead to misgendered translations, resulting, among other harms, in the
perpetuation of stereotypes and prejudices. In this work, we address this gap
by investigating whether and to what extent such models exhibit gender bias in
machine translation and how we can mitigate it. Concretely, we compute
established gender bias metrics on the WinoMT corpus from English to German and
Spanish. We discover that IFT models default to male-inflected translations,
even disregarding female occupational stereotypes. Next, using interpretability
methods, we unveil that models systematically overlook the pronoun indicating
the gender of a target occupation in misgendered translations. Finally, based
on this finding, we propose an easy-to-implement and effective bias mitigation
solution based on few-shot learning that leads to significantly fairer
translations.",None,-1
b684bd6d-51d4-434b-9665-4ce415b6febc,The sample complexity of multi-distribution learning,0.550959,"Multi-distribution learning generalizes the classic PAC learning to handle
data coming from multiple distributions. Given a set of $k$ data distributions
and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis
that minimizes the maximum population loss over $k$ distributions, up to
$\epsilon$ additive error. In this paper, we settle the sample complexity of
multi-distribution learning by giving an algorithm of sample complexity
$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the
lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem
of Awasthi, Haghtalab and Zhao [AHZ23].",None,-1
d828ac3d-c108-44ac-a0fa-153d4034d198,DPPD: Deformable Polar Polygon Object Detection,0.039385,"Regular object detection methods output rectangle bounding boxes, which are
unable to accurately describe the actual object shapes. Instance segmentation
methods output pixel-level labels, which are computationally expensive for
real-time applications. Therefore, a polygon representation is needed to
achieve precise shape alignment, while retaining low computation cost. We
develop a novel Deformable Polar Polygon Object Detection method (DPPD) to
detect objects in polygon shapes. In particular, our network predicts, for each
object, a sparse set of flexible vertices to construct the polygon, where each
vertex is represented by a pair of angle and distance in the Polar coordinate
system. To enable training, both ground truth and predicted polygons are
densely resampled to have the same number of vertices with equal-spaced
raypoints. The resampling operation is fully differentable, allowing gradient
back-propagation. Sparse polygon predicton ensures high-speed runtime inference
while dense resampling allows the network to learn object shapes with high
precision. The polygon detection head is established on top of an anchor-free
and NMS-free network architecture. DPPD has been demonstrated successfully in
various object detection tasks for autonomous driving such as traffic-sign,
crosswalk, vehicle and pedestrian objects.",None,-1
73f0133f-0d65-4af6-a17d-01ac1ab2d6c4,Improving Online Lane Graph Extraction by Object-Lane Clustering,0.506368,"Autonomous driving requires accurate local scene understanding information.
To this end, autonomous agents deploy object detection and online BEV lane
graph extraction methods as a part of their perception stack. In this work, we
propose an architecture and loss formulation to improve the accuracy of local
lane graph estimates by using 3D object detection outputs. The proposed method
learns to assign the objects to centerlines by considering the centerlines as
cluster centers and the objects as data points to be assigned a probability
distribution over the cluster centers. This training scheme ensures direct
supervision on the relationship between lanes and objects, thus leading to
better performance. The proposed method improves lane graph estimation
substantially over state-of-the-art methods. The extensive ablations show that
our method can achieve significant performance improvements by using the
outputs of existing 3D object detection methods. Since our method uses the
detection outputs rather than detection method intermediate representations, a
single model of our method can use any detection method at test time.",None,-1
7aaa21a1-5cfa-47dd-b28f-3f4ccf5bb45a,Improving Long Context Document-Level Machine Translation,0.721162,"Document-level context for neural machine translation (NMT) is crucial to
improve the translation consistency and cohesion, the translation of ambiguous
inputs, as well as several other linguistic phenomena. Many works have been
published on the topic of document-level NMT, but most restrict the system to
only local context, typically including just the one or two preceding sentences
as additional information. This might be enough to resolve some ambiguous
inputs, but it is probably not sufficient to capture some document-level
information like the topic or style of a conversation. When increasing the
context size beyond just the local context, there are two challenges: (i)
the~memory usage increases exponentially (ii) the translation performance
starts to degrade. We argue that the widely-used attention mechanism is
responsible for both issues. Therefore, we propose a constrained attention
variant that focuses the attention on the most relevant parts of the sequence,
while simultaneously reducing the memory consumption. For evaluation, we
utilize targeted test sets in combination with novel evaluation techniques to
analyze the translations in regards to specific discourse-related phenomena. We
find that our approach is a good compromise between sentence-level NMT vs
attending to the full context, especially in low resource scenarios.",None,-1
109f1995-05fa-4319-a72b-83866cb5abe9,Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection,0.843841,"Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.",None,-1
f64a82aa-1068-45ff-957c-c7dd7cf26fca,A Deep Behavior Path Matching Network for Click-Through Rate Prediction,0.557947,"User behaviors on an e-commerce app not only contain different kinds of
feedback on items but also sometimes imply the cognitive clue of the user's
decision-making. For understanding the psychological procedure behind user
decisions, we present the behavior path and propose to match the user's current
behavior path with historical behavior paths to predict user behaviors on the
app. Further, we design a deep neural network for behavior path matching and
solve three difficulties in modeling behavior paths: sparsity, noise
interference, and accurate matching of behavior paths. In particular, we
leverage contrastive learning to augment user behavior paths, provide behavior
path self-activation to alleviate the effect of noise, and adopt a two-level
matching mechanism to identify the most appropriate candidate. Our model shows
excellent performance on two real-world datasets, outperforming the
state-of-the-art CTR model. Moreover, our model has been deployed on the
Meituan food delivery platform and has accumulated 1.6% improvement in CTR and
1.8% improvement in advertising revenue.",None,-1
90f316e5-ffe4-4068-8c31-58b8eca457d9,An Improved Baseline Framework for Pose Estimation Challenge at ECCV 2022 Visual Perception for Navigation in Human Environments Workshop,0.105471,"This technical report describes our first-place solution to the pose
estimation challenge at ECCV 2022 Visual Perception for Navigation in Human
Environments Workshop. In this challenge, we aim to estimate human poses from
in-the-wild stitched panoramic images. Our method is built based on Faster
R-CNN for human detection, and HRNet for human pose estimation. We describe
technical details for the JRDB-Pose dataset, together with some experimental
results. In the competition, we achieved 0.303 $\text{OSPA}_{\text{IOU}}$ and
64.047\% $\text{AP}_{\text{0.5}}$ on the test set of JRDB-Pose.",None,-1
3f5a7c57-bacf-416f-9865-d85de3a532b3,SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion,0.38652,"Semantic scene completion (SSC) jointly predicts the semantics and geometry
of the entire 3D scene, which plays an essential role in 3D scene understanding
for autonomous driving systems. SSC has achieved rapid progress with the help
of semantic context in segmentation. However, how to effectively exploit the
relationships between the semantic context in semantic segmentation and
geometric structure in scene completion remains under exploration. In this
paper, we propose to solve outdoor SSC from the perspective of representation
separation and BEV fusion. Specifically, we present the network, named SSC-RS,
which uses separate branches with deep supervision to explicitly disentangle
the learning procedure of the semantic and geometric representations. And a BEV
fusion network equipped with the proposed Adaptive Representation Fusion (ARF)
module is presented to aggregate the multi-scale features effectively and
efficiently. Due to the low computational burden and powerful representation
ability, our model has good generality while running in real-time. Extensive
experiments on SemanticKITTI demonstrate our SSC-RS achieves state-of-the-art
performance.",None,-1
12181ffa-ac0b-463f-937d-e879fc3a0728,Zero-shot Causal Graph Extrapolation from Text via LLMs,0.808854,"We evaluate the ability of large language models (LLMs) to infer causal
relations from natural language. Compared to traditional natural language
processing and deep learning techniques, LLMs show competitive performance in a
benchmark of pairwise relations without needing (explicit) training samples.
This motivates us to extend our approach to extrapolating causal graphs through
iterated pairwise queries. We perform a preliminary analysis on a benchmark of
biomedical abstracts with ground-truth causal graphs validated by experts. The
results are promising and support the adoption of LLMs for such a crucial step
in causal inference, especially in medical domains, where the amount of
scientific text to analyse might be huge, and the causal statements are often
implicit.",None,-1
e9246ab0-c489-4826-b2f5-94c1e11c6e62,A Multi-Modal Multilingual Benchmark for Document Image Classification,0.258473,"Document image classification is different from plain-text document
classification and consists of classifying a document by understanding the
content and structure of documents such as forms, emails, and other such
documents. We show that the only existing dataset for this task (Lewis et al.,
2006) has several limitations and we introduce two newly curated multilingual
datasets WIKI-DOC and MULTIEURLEX-DOC that overcome these limitations. We
further undertake a comprehensive study of popular visually-rich document
understanding or Document AI models in previously untested setting in document
image classification such as 1) multi-label classification, and 2) zero-shot
cross-lingual transfer setup. Experimental results show limitations of
multilingual Document AI models on cross-lingual transfer across typologically
distant languages. Our datasets and findings open the door for future research
into improving Document AI models.",None,-1
4874b6b4-6e03-4178-9427-8c0cd872cc5d,Parmesan: mathematical concept extraction for education,0.507478,"Mathematics is a highly specialized domain with its own unique set of
challenges that has seen limited study in natural language processing. However,
mathematics is used in a wide variety of fields and multidisciplinary research
in many different domains often relies on an understanding of mathematical
concepts. To aid researchers coming from other fields, we develop a prototype
system for searching for and defining mathematical concepts in context,
focusing on the field of category theory. This system, Parmesan, depends on
natural language processing components including concept extraction, relation
extraction, definition extraction, and entity linking. In developing this
system, we show that existing techniques cannot be applied directly to the
category theory domain, and suggest hybrid techniques that do perform well,
though we expect the system to evolve over time. We also provide two cleaned
mathematical corpora that power the prototype system, which are based on
journal articles and wiki pages, respectively. The corpora have been annotated
with dependency trees, lemmas, and part-of-speech tags.",None,-1
a39a0063-758e-45ac-bf0b-8b5c95fd5a84,IDA: Informed Domain Adaptive Semantic Segmentation,0.449823,"Mixup-based data augmentation has been validated to be a critical stage in
the self-training framework for unsupervised domain adaptive semantic
segmentation (UDA-SS), which aims to transfer knowledge from a well-annotated
(source) domain to an unlabeled (target) domain. Existing self-training methods
usually adopt the popular region-based mixup techniques with a random sampling
strategy, which unfortunately ignores the dynamic evolution of different
semantics across various domains as training proceeds. To improve the UDA-SS
performance, we propose an Informed Domain Adaptation (IDA) model, a
self-training framework that mixes the data based on class-level segmentation
performance, which aims to emphasize small-region semantics during mixup. In
our IDA model, the class-level performance is tracked by an expected confidence
score (ECS). We then use a dynamic schedule to determine the mixing ratio for
data in different domains. Extensive experimental results reveal that our
proposed method is able to outperform the state-of-the-art UDA-SS method by a
margin of 1.1 mIoU in the adaptation of GTA-V to Cityscapes and of 0.9 mIoU in
the adaptation of SYNTHIA to Cityscapes.",None,-1
66ac3c44-3fe0-45d5-86e5-9bb1ce71a566,Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks,0.458681,"We introduce a novel and efficient method for Event Coreference Resolution
(ECR) applied to a lower-resourced language domain. By framing ECR as a graph
reconstruction task, we are able to combine deep semantic embeddings with
structural coreference chain knowledge to create a parameter-efficient family
of Graph Autoencoder models (GAE). Our method significantly outperforms
classical mention-pair methods on a large Dutch event coreference corpus in
terms of overall score, efficiency and training speed. Additionally, we show
that our models are consistently able to classify more difficult coreference
links and are far more robust in low-data settings when compared to
transformer-based mention-pair coreference algorithms.",None,-1
6732d1a5-263c-4204-a14e-5e8e154c27e3,Kattis vs. ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence,0.650209,"AI-powered education technologies can support students and teachers in
computer science education. However, with the recent developments in generative
AI, and especially the increasingly emerging popularity of ChatGPT, the
effectiveness of using large language models for solving programming tasks has
been underexplored. The present study examines ChatGPT's ability to generate
code solutions at different difficulty levels for introductory programming
courses. We conducted an experiment where ChatGPT was tested on 127 randomly
selected programming problems provided by Kattis, an automatic software grading
tool for computer science programs, often used in higher education. The results
showed that ChatGPT independently could solve 19 out of 127 programming tasks
generated and assessed by Kattis. Further, ChatGPT was found to be able to
generate accurate code solutions for simple problems but encountered
difficulties with more complex programming tasks. The results contribute to the
ongoing debate on the utility of AI-powered tools in programming education.",None,-1
053238ca-2472-4770-a9e8-094419d4b261,Scalable Prompt Generation for Semi-supervised Learning with Language Models,0.652661,"Prompt-based learning methods in semi-supervised learning (SSL) settings have
been shown to be effective on multiple natural language understanding (NLU)
datasets and tasks in the literature. However, manually designing multiple
prompts and verbalizers requires domain knowledge and human effort, making it
difficult and expensive to scale across different datasets. In this paper, we
propose two methods to automatically design multiple prompts and integrate
automatic verbalizer in SSL settings without sacrificing performance. The first
method uses various demonstration examples with learnable continuous prompt
tokens to create diverse prompt models. The second method uses a varying number
of soft prompt tokens to encourage language models to learn different prompts.
For the verbalizer, we use the prototypical verbalizer to replace the manual
one. In summary, we obtained the best average accuracy of 73.2% (a relative
improvement of 2.52% over even the previous state-of-the-art SSL method with
manual prompts and verbalizers) in different few-shot learning settings.",None,-1
3fa7b1d2-6470-43b2-a6b0-87501328c01d,Ontology Pre-training for Poison Prediction,0.0199351,"Integrating human knowledge into neural networks has the potential to improve
their robustness and interpretability. We have developed a novel approach to
integrate knowledge from ontologies into the structure of a Transformer network
which we call ontology pre-training: we train the network to predict membership
in ontology classes as a way to embed the structure of the ontology into the
network, and subsequently fine-tune the network for the particular prediction
task. We apply this approach to a case study in predicting the potential
toxicity of a small molecule based on its molecular structure, a challenging
task for machine learning in life sciences chemistry. Our approach improves on
the state of the art, and moreover has several additional benefits. First, we
are able to show that the model learns to focus attention on more meaningful
chemical groups when making predictions with ontology pre-training than
without, paving a path towards greater robustness and interpretability. Second,
the training time is reduced after ontology pre-training, indicating that the
model is better placed to learn what matters for toxicity prediction with the
ontology pre-training than without. This strategy has general applicability as
a neuro-symbolic approach to embed meaningful semantics into neural networks.",None,-1
df0fe517-6ce0-43c1-9b0e-6e74457fefd5,Brain subtle anomaly detection based on auto-encoders latent space analysis : application to de novo parkinson patients,0.289691,"Neural network-based anomaly detection remains challenging in clinical
applications with little or no supervised information and subtle anomalies such
as hardly visible brain lesions. Among unsupervised methods, patch-based
auto-encoders with their efficient representation power provided by their
latent space, have shown good results for visible lesion detection. However,
the commonly used reconstruction error criterion may limit their performance
when facing less obvious lesions. In this work, we design two alternative
detection criteria. They are derived from multivariate analysis and can more
directly capture information from latent space representations. Their
performance compares favorably with two additional supervised learning methods,
on a difficult de novo Parkinson Disease (PD) classification task.",None,-1
0110d55a-4efb-4bf4-a852-a609d5610088,What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation,0.0991503,"The pretrain-finetune paradigm usually improves downstream performance over
training a model from scratch on the same task, becoming commonplace across
many areas of machine learning. While pretraining is empirically observed to be
beneficial for a range of tasks, there is not a clear understanding yet of the
reasons for this effect. In this work, we examine the relationship between
pretrained vision transformers and the corresponding finetuned versions on
several benchmark datasets and tasks. We present new metrics that specifically
investigate the degree to which invariances learned by a pretrained model are
retained or forgotten during finetuning. Using these metrics, we present a
suite of empirical findings, including that pretraining induces transferable
invariances in shallow layers and that invariances from deeper pretrained
layers are compressed towards shallower layers during finetuning. Together,
these findings contribute to understanding some of the reasons for the
successes of pretrained models and the changes that a pretrained model
undergoes when finetuned on a downstream task.",None,-1
988bc631-5b4d-42c4-ae0c-e5f8795e476c,Automated Action Model Acquisition from Narrative Texts,0.596158,"Action models, which take the form of precondition/effect axioms, facilitate
causal and motivational connections between actions for AI agents. Action model
acquisition has been identified as a bottleneck in the application of planning
technology, especially within narrative planning. Acquiring action models from
narrative texts in an automated way is essential, but challenging because of
the inherent complexities of such texts. We present NaRuto, a system that
extracts structured events from narrative text and subsequently generates
planning-language-style action models based on predictions of commonsense event
relations, as well as textual contradictions and similarities, in an
unsupervised manner. Experimental results in classical narrative planning
domains show that NaRuto can generate action models of significantly better
quality than existing fully automated methods, and even on par with those of
semi-automated methods.",None,-1
7d39e70c-ed16-4bc2-a3b9-9039b4d97fcc,Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping,0.119637,"Inverse reinforcement learning (IRL) is computationally challenging, with
common approaches requiring the solution of multiple reinforcement learning
(RL) sub-problems. This work motivates the use of potential-based reward
shaping to reduce the computational burden of each RL sub-problem. This work
serves as a proof-of-concept and we hope will inspire future developments
towards computationally efficient IRL.",None,-1
2937dc75-51d0-4add-a855-78c4b1fa9e75,PersonalTailor: Personalizing 2D Pattern Design from 3D Garment Point Clouds,0.413855,"Garment pattern design aims to convert a 3D garment to the corresponding 2D
panels and their sewing structure. Existing methods rely either on template
fitting with heuristics and prior assumptions, or on model learning with
complicated shape parameterization. Importantly, both approaches do not allow
for personalization of the output garment, which today has increasing demands.
To fill this demand, we introduce PersonalTailor: a personalized 2D pattern
design method, where the user can input specific constraints or demands (in
language or sketch) for personal 2D panel fabrication from 3D point clouds.
PersonalTailor first learns a multi-modal panel embeddings based on
unsupervised cross-modal association and attentive fusion. It then predicts a
binary panel masks individually using a transformer encoder-decoder framework.
Extensive experiments show that our PersonalTailor excels on both personalized
and standard pattern fabrication tasks.",None,-1
34e8dcfc-afd5-43cc-a527-160ba646fa22,Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks,0.450334,"Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.",None,-1
4855f299-8226-4b42-810c-81b4b7a63843,Mitigating Test-Time Bias for Fair Image Retrieval,0.556688,"We address the challenge of generating fair and unbiased image retrieval
results given neutral textual queries (with no explicit gender or race
connotations), while maintaining the utility (performance) of the underlying
vision-language (VL) model. Previous methods aim to disentangle learned
representations of images and text queries from gender and racial
characteristics. However, we show these are inadequate at alleviating bias for
the desired equal representation result, as there usually exists test-time bias
in the target retrieval set. So motivated, we introduce a straightforward
technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from
the pre-trained vision-language model. We evaluate our algorithm on real-world
image search datasets, Occupation 1 and 2, as well as two large-scale
image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest
bias, compared with various existing bias-mitigation methods, in text-based
image retrieval result while maintaining satisfactory retrieval performance.
The source code is publicly available at
\url{https://anonymous.4open.science/r/Fair_Text_based_Image_Retrieval-D8B2}.",None,-1
f717b78c-d351-4d76-9d92-d00369029fc4,Learning Topology-Preserving Data Representations,0.841901,"We propose a method for learning topology-preserving data representations
(dimensionality reduction). The method aims to provide topological similarity
between the data manifold and its latent representation via enforcing the
similarity in topological features (clusters, loops, 2D voids, etc.) and their
localization. The core of the method is the minimization of the Representation
Topology Divergence (RTD) between original high-dimensional data and
low-dimensional representation in latent space. RTD minimization provides
closeness in topological features with strong theoretical guarantees. We
develop a scheme for RTD differentiation and apply it as a loss term for the
autoencoder. The proposed method ""RTD-AE"" better preserves the global structure
and topology of the data manifold than state-of-the-art competitors as measured
by linear correlation, triplet distance ranking accuracy, and Wasserstein
distance between persistence barcodes.",None,-1
931570d3-5a54-4abf-a7c1-23ae4a156364,Factual and Personalized Recommendations using Language Models and Reinforcement Learning,0.124796,"Recommender systems (RSs) play a central role in connecting users to content,
products, and services, matching candidate items to users based on their
preferences. While traditional RSs rely on implicit user feedback signals,
conversational RSs interact with users in natural language. In this work, we
develop a comPelling, Precise, Personalized, Preference-relevant language model
(P4LM) that recommends items to users while putting emphasis on explaining item
characteristics and their relevance. P4LM uses the embedding space
representation of a user's preferences to generate compelling responses that
are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we
develop a joint reward function that measures precision, appeal, and
personalization, which we use as AI-based feedback in a reinforcement
learning-based language model framework. Using the MovieLens 25M dataset, we
demonstrate that P4LM delivers compelling, personalized movie narratives to
users.",None,-1
f5b0bead-8a60-4baa-ab2d-75ef5eeaea2e,Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23,0.291153,"This paper describes the submission of the UPC Machine Translation group to
the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems
utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We
incorporate a Siamese pretraining step of the speech and text encoders with CTC
and Optimal Transport, to adapt the speech representations to the space of the
text model, thus maximizing transfer learning from MT. After this pretraining,
we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge
Distillation. Apart from the available ST corpora, we create synthetic data
with SegAugment to better adapt our models to the custom segmentations of the
IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C
tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released
IWSLT.ACLdev2023.",None,-1
4252fc9c-ec7b-4f33-9690-aae280276213,Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers,0.547752,"Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element's contribution to the prediction
results.",None,-1
200cadf9-1901-4b8a-86d8-ac05399c071c,Reinforcement Learning and Data-Generation for Syntax-Guided Synthesis,0.133553,"Program synthesis is the task of automatically generating code based on a
specification. In Syntax-Guided Synthesis (SyGuS) this specification is a
combination of a syntactic template and a logical formula, and the result is
guaranteed to satisfy both.
  We present a reinforcement-learning guided algorithm for SyGuS which uses
Monte-Carlo Tree Search (MCTS) to search the space of candidate solutions. Our
algorithm learns policy and value functions which, combined with the upper
confidence bound for trees, allow it to balance exploration and exploitation. A
common challenge in applying machine learning approaches to syntax-guided
synthesis is the scarcity of training data. To address this, we present a
method for automatically generating training data for SyGuS based on
anti-unification of existing first-order satisfiability problems, which we use
to train our MCTS policy. We implement and evaluate this setup and demonstrate
that learned policy and value improve the synthesis performance over a baseline
by over 26 percentage points in the training and testing sets. Our tool
outperforms state-of-the-art tool cvc5 on the training set and performs
comparably in terms of the total number of problems solved on the testing set
(solving 23% of the benchmarks on which cvc5 fails). We make our data set
publicly available, to enable further application of machine learning methods
to the SyGuS problem.",None,-1
1ad3a381-a611-4788-8e09-7a7edc089cc1,PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models,0.593796,"Prompts have significantly improved the performance of pretrained Large
Language Models (LLMs) on various downstream tasks recently, making them
increasingly indispensable for a diverse range of LLM application scenarios.
However, the backdoor vulnerability, a serious security threat that can
maliciously alter the victim model's normal predictions, has not been
sufficiently explored for prompt-based LLMs. In this paper, we present
POISONPROMPT, a novel backdoor attack capable of successfully compromising both
hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and
robustness of POISONPROMPT through extensive experiments on three popular
prompt methods, using six datasets and three widely used LLMs. Our findings
highlight the potential security threats posed by backdoor attacks on
prompt-based LLMs and emphasize the need for further research in this area.",None,-1
48a8f26d-8f81-42c9-af90-c77afaf88b18,Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction,0.423737,"Given their flexibility and encouraging performance, deep-learning models are
becoming standard for motion prediction in autonomous driving. However, with
great flexibility comes a lack of interpretability and possible violations of
physical constraints. Accompanying these data-driven methods with
differentially-constrained motion models to provide physically feasible
trajectories is a promising future direction. The foundation for this work is a
previously introduced graph-neural-network-based model, MTP-GO. The neural
network learns to compute the inputs to an underlying motion model to provide
physically feasible trajectories. This research investigates the performance of
various motion models in combination with numerical solvers for the prediction
task. The study shows that simpler models, such as low-order integrator models,
are preferred over more complex, e.g., kinematic models, to achieve accurate
predictions. Further, the numerical solver can have a substantial impact on
performance, advising against commonly used first-order methods like Euler
forward. Instead, a second-order method like Heun's can greatly improve
predictions.",None,-1
6dafd940-0294-497b-b821-1f3f934c1e74,FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views,0.727817,"We present FlexNeRF, a method for photorealistic freeviewpoint rendering of
humans in motion from monocular videos. Our approach works well with sparse
views, which is a challenging scenario when the subject is exhibiting
fast/complex motions. We propose a novel approach which jointly optimizes a
canonical time and pose configuration, with a pose-dependent motion field and
pose-independent temporal deformations complementing each other. Thanks to our
novel temporal and cyclic consistency constraints along with additional losses
on intermediate representation such as segmentation, our approach provides high
quality outputs as the observed views become sparser. We empirically
demonstrate that our method significantly outperforms the state-of-the-art on
public benchmark datasets as well as a self-captured fashion dataset. The
project page is available at: https://flex-nerf.github.io/",None,-1
ef2d9b32-73c8-4c65-af94-4e8041b545ac,SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,0.989537,"Prompt tuning is a technology that tunes a small set of parameters to steer a
pre-trained language model (LM) to directly generate the output for downstream
tasks. Recently, prompt tuning has demonstrated its storage and computation
efficiency in both natural language processing (NLP) and speech processing
fields. These advantages have also revealed prompt tuning as a candidate
approach to serving pre-trained LM for multiple tasks in a unified manner. For
speech processing, SpeechPrompt shows its high parameter efficiency and
competitive performance on a few speech classification tasks. However, whether
SpeechPrompt is capable of serving a large number of tasks is unanswered. In
this work, we propose SpeechPrompt v2, a prompt tuning framework capable of
performing a wide variety of speech classification tasks, covering multiple
languages and prosody-related tasks. The experiment result shows that
SpeechPrompt v2 achieves performance on par with prior works with less than
0.15M trainable parameters in a unified framework.",None,-1
e5b58002-9477-4765-89ce-49fc83093cb1,DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4,0.199629,"Human preference judgments are pivotal in guiding large language models
(LLMs) to produce outputs that align with human values. Human evaluations are
also used in summarization tasks to compare outputs from various systems,
complementing existing automatic metrics. Despite their significance, however,
there has been limited research probing these pairwise or $k$-wise comparisons.
The collective impact and relative importance of factors such as output length,
informativeness, fluency, and factual consistency are still not well
understood. It is also unclear if there are other hidden factors influencing
human judgments. In this paper, we conduct an in-depth examination of a
collection of pairwise human judgments released by OpenAI. Utilizing the
Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in
these human judgments. We find that the most favored factors vary across tasks
and genres, whereas the least favored factors tend to be consistent, e.g.,
outputs are too brief, contain excessive off-focus content or hallucinated
facts. Our findings have implications on the construction of balanced datasets
in human preference evaluations, which is a crucial step in shaping the
behaviors of future LLMs.",None,-1
1fda0296-43ce-410f-b692-3191130480d8,Single-view Neural Radiance Fields with Depth Teacher,0.0697325,"Neural Radiance Fields (NeRF) have been proposed for photorealistic novel
view rendering. However, it requires many different views of one scene for
training. Moreover, it has poor generalizations to new scenes and requires
retraining or fine-tuning on each scene. In this paper, we develop a new NeRF
model for novel view synthesis using only a single image as input. We propose
to combine the (coarse) planar rendering and the (fine) volume rendering to
achieve higher rendering quality and better generalizations. We also design a
depth teacher net that predicts dense pseudo depth maps to supervise the joint
rendering mechanism and boost the learning of consistent 3D geometry. We
evaluate our method on three challenging datasets. It outperforms
state-of-the-art single-view NeRFs by achieving 5$\sim$20\% improvements in
PSNR and reducing 20$\sim$50\% of the errors in the depth rendering. It also
shows excellent generalization abilities to unseen data without the need to
fine-tune on each new scene.",None,-1
2fbde41c-5b4d-4080-bed7-df5873e27150,XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,0.814564,"In recent years, pre-trained language models have undergone rapid development
with the emergence of large-scale models. However, there is a lack of
open-sourced chat models specifically designed for the Chinese language,
especially in the field of Chinese finance, at the scale of hundreds of
billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese
chat model to date, built upon the BLOOM-176B architecture. Additionally, we
propose a novel training method called hybrid-tuning to mitigate catastrophic
forgetting. By combining general-domain with domain-specific knowledge and
integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable
of providing accurate and contextually appropriate responses in the Chinese
financial domain.",None,-1
44b6f04a-e9bb-4741-bed2-9e95ddda2bba,Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers,0.378468,"This paper introduces Content-aware Token Sharing (CTS), a token reduction
approach that improves the computational efficiency of semantic segmentation
networks that use Vision Transformers (ViTs). Existing works have proposed
token reduction approaches to improve the efficiency of ViT-based image
classification networks, but these methods are not directly applicable to
semantic segmentation, which we address in this work. We observe that, for
semantic segmentation, multiple image patches can share a token if they contain
the same semantic class, as they contain redundant information. Our approach
leverages this by employing an efficient, class-agnostic policy network that
predicts if image patches contain the same semantic class, and lets them share
a token if they do. With experiments, we explore the critical design choices of
CTS and show its effectiveness on the ADE20K, Pascal Context and Cityscapes
datasets, various ViT backbones, and different segmentation decoders. With
Content-aware Token Sharing, we are able to reduce the number of processed
tokens by up to 44%, without diminishing the segmentation quality.",None,-1
c8892f42-c192-463b-98c7-a03d04ac3fac,TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision,0.44145,"In this paper, we investigate an open research task of generating
controllable 3D textured shapes from the given textual descriptions. Previous
works either require ground truth caption labeling or extensive optimization
time. To resolve these issues, we present a novel framework, TAPS3D, to train a
text-guided 3D shape generator with pseudo captions. Specifically, based on
rendered 2D images, we retrieve relevant words from the CLIP vocabulary and
construct pseudo captions using templates. Our constructed captions provide
high-level semantic supervision for generated 3D shapes. Further, in order to
produce fine-grained textures and increase geometry diversity, we propose to
adopt low-level image regularization to enable fake-rendered images to align
with the real ones. During the inference phase, our proposed model can generate
3D textured shapes from the given text without any additional optimization. We
conduct extensive experiments to analyze each of our proposed components and
show the efficacy of our framework in generating high-fidelity 3D textured and
text-relevant shapes.",None,-1
9314641c-3813-4e46-b569-dbf444b4185d,Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism,0.323201,"Transformer-based Large Language Models (LLMs) are the state-of-the-art for
natural language tasks. Recent work has attempted to decode, by reverse
engineering the role of linear layers, the internal mechanisms by which LLMs
arrive at their final predictions for text completion tasks. Yet little is
known about the specific role of attention heads in producing the final token
prediction. We propose Attention Lens, a tool that enables researchers to
translate the outputs of attention heads into vocabulary tokens via learned
attention-head-specific transformations called lenses. Preliminary findings
from our trained lenses indicate that attention heads play highly specialized
roles in language models. The code for Attention Lens is available at
github.com/msakarvadia/AttentionLens.",None,-1
2859e232-f2ff-414c-a17e-cc1150eb1a45,Causal Disentangled Variational Auto-Encoder for Preference Understanding in Recommendation,0.569905,"Recommendation models are typically trained on observational user interaction
data, but the interactions between latent factors in users' decision-making
processes lead to complex and entangled data. Disentangling these latent
factors to uncover their underlying representation can improve the robustness,
interpretability, and controllability of recommendation models. This paper
introduces the Causal Disentangled Variational Auto-Encoder (CaD-VAE), a novel
approach for learning causal disentangled representations from interaction data
in recommender systems. The CaD-VAE method considers the causal relationships
between semantically related factors in real-world recommendation scenarios,
rather than enforcing independence as in existing disentanglement methods. The
approach utilizes structural causal models to generate causal representations
that describe the causal relationship between latent factors. The results
demonstrate that CaD-VAE outperforms existing methods, offering a promising
solution for disentangling complex user behavior data in recommendation
systems.",None,-1
fe5398b9-dc14-4915-88ae-0f7dd58ef1cb,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,0.106787,"In this paper, we develop a novel benchmark suite including both a 2D
synthetic image dataset and a 3D synthetic point cloud dataset. Our work is a
sub-task in the framework of a remanufacturing project, in which small electric
motors are used as fundamental objects. Apart from the given detection,
classification, and segmentation annotations, the key objects also have
multiple learnable attributes with ground truth provided. This benchmark can be
used for computer vision tasks including 2D/3D detection, classification,
segmentation, and multi-attribute learning. It is worth mentioning that most
attributes of the motors are quantified as continuously variable rather than
binary, which makes our benchmark well-suited for the less explored regression
tasks. In addition, appropriate evaluation metrics are adopted or developed for
each task and promising baseline results are provided. We hope this benchmark
can stimulate more research efforts on the sub-domain of object attribute
learning and multi-task learning in the future.",None,-1
b7459c98-bef6-4bda-9392-a51421fce15c,Text-guided High-definition Consistency Texture Model,0.0947607,"With the advent of depth-to-image diffusion models, text-guided generation,
editing, and transfer of realistic textures are no longer difficult. However,
due to the limitations of pre-trained diffusion models, they can only create
low-resolution, inconsistent textures. To address this issue, we present the
High-definition Consistency Texture Model (HCTM), a novel method that can
generate high-definition and consistent textures for 3D meshes according to the
text prompts. We achieve this by leveraging a pre-trained depth-to-image
diffusion model to generate single viewpoint results based on the text prompt
and a depth map. We fine-tune the diffusion model with Parameter-Efficient
Fine-Tuning to quickly learn the style of the generated result, and apply the
multi-diffusion strategy to produce high-resolution and consistent results from
different viewpoints. Furthermore, we propose a strategy that prevents the
appearance of noise on the textures caused by backpropagation. Our proposed
approach has demonstrated promising results in generating high-definition and
consistent textures for 3D meshes, as demonstrated through a series of
experiments.",None,-1
0950e918-4774-4cf7-af75-7729a386a141,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,0.230425,"The presence of a large number of bots in Online Social Networks (OSN) leads
to undesirable social effects. Graph neural networks (GNNs) are effective in
detecting bots as they utilize user interactions. However, class-imbalanced
issues can affect bot detection performance. To address this, we propose an
over-sampling strategy for GNNs (OS-GNN) that generates samples for the
minority class without edge synthesis. First, node features are mapped to a
feature space through neighborhood aggregation. Then, we generate samples for
the minority class in the feature space. Finally, the augmented features are
used to train the classifiers. This framework is general and can be easily
extended into different GNN architectures. The proposed framework is evaluated
using three real-world bot detection benchmark datasets, and it consistently
exhibits superiority over the baselines.",None,-1
38bbfc99-f1c4-4722-a93f-82bd31e512ef,XFEVER: Exploring Fact Verification across Languages,0.355132,"This paper introduces the Cross-lingual Fact Extraction and VERification
(XFEVER) dataset designed for benchmarking the fact verification models across
different languages. We constructed it by translating the claim and evidence
texts of the Fact Extraction and VERification (FEVER) dataset into six
languages. The training and development sets were translated using machine
translation, whereas the test set includes texts translated by professional
translators and machine-translated texts. Using the XFEVER dataset, two
cross-lingual fact verification scenarios, zero-shot learning and
translate-train learning, are defined, and baseline models for each scenario
are also proposed in this paper. Experimental results show that the
multilingual language model can be used to build fact verification models in
different languages efficiently. However, the performance varies by language
and is somewhat inferior to the English case. We also found that we can
effectively mitigate model miscalibration by considering the prediction
similarity between the English and target languages. The XFEVER dataset, code,
and model checkpoints are available at
https://github.com/nii-yamagishilab/xfever.",None,-1
07c276ed-2cd2-4541-8407-683881cdf45c,Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,0.828616,"ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that
are slated to promise different applications in diverse areas. In education,
these AI technologies have been tested for applications in assessment and
teaching. In assessment, AI has long been used in automated essay scoring and
automated item generation. One psychometric property that these tools must have
to assist or replace humans in assessment is high reliability in terms of
agreement between AI scores and human raters. In this paper, we measure the
reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and
trained humans in perceiving and rating the complexity of writing prompts.
Intraclass correlation (ICC) as a performance metric showed that the
inter-reliability of both the OpenAI ChatGPT and the Google Bard were low
against the gold standard of human ratings.",None,-1
113d1778-df8d-4d3b-886e-0b163cde65eb,Floaters No More: Radiance Field Gradient Scaling for Improved Near-Camera Training,0.480185,"NeRF acquisition typically requires careful choice of near planes for the
different cameras or suffers from background collapse, creating floating
artifacts on the edges of the captured scene. The key insight of this work is
that background collapse is caused by a higher density of samples in regions
near cameras. As a result of this sampling imbalance, near-camera volumes
receive significantly more gradients, leading to incorrect density buildup. We
propose a gradient scaling approach to counter-balance this sampling imbalance,
removing the need for near planes, while preventing background collapse. Our
method can be implemented in a few lines, does not induce any significant
overhead, and is compatible with most NeRF implementations.",None,-1
161dae27-c7cf-4aa3-a543-c5b428e479e0,Efficient ConvBN Blocks for Transfer Learning and Beyond,0.0346633,"Convolution-BatchNorm (ConvBN) blocks are integral components in various
computer vision tasks and other domains. A ConvBN block can operate in three
modes: Train, Eval, and Deploy. While the Train mode is indispensable for
training models from scratch, the Eval mode is suitable for transfer learning
and beyond, and the Deploy mode is designed for the deployment of models. This
paper focuses on the trade-off between stability and efficiency in ConvBN
blocks: Deploy mode is efficient but suffers from training instability; Eval
mode is widely used in transfer learning but lacks efficiency. To solve the
dilemma, we theoretically reveal the reason behind the diminished training
stability observed in the Deploy mode. Subsequently, we propose a novel Tune
mode to bridge the gap between Eval mode and Deploy mode. The proposed Tune
mode is as stable as Eval mode for transfer learning, and its computational
efficiency closely matches that of the Deploy mode. Through extensive
experiments in object detection, classification, and adversarial example
generation across $5$ datasets and $12$ model architectures, we demonstrate
that the proposed Tune mode retains the performance while significantly
reducing GPU memory footprint and training time, thereby contributing efficient
ConvBN blocks for transfer learning and beyond. Our method has been integrated
into both PyTorch (general machine learning framework) and MMCV/MMEngine
(computer vision framework). Practitioners just need one line of code to enjoy
our efficient ConvBN blocks thanks to PyTorch's builtin machine learning
compilers.",None,-1
41185f5a-ff6a-4ada-bf86-2eb0d8e679e3,Probing the Moral Development of Large Language Models through Defining Issues Test,0.146713,"In this study, we measure the moral reasoning ability of LLMs using the
Defining Issues Test - a psychometric instrument developed for measuring the
moral development stage of a person according to the Kohlberg's Cognitive Moral
Development Model. DIT uses moral dilemmas followed by a set of ethical
considerations that the respondent has to judge for importance in resolving the
dilemma, and then rank-order them by importance. A moral development stage
score of the respondent is then computed based on the relevance rating and
ranking.
  Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning
ability no better than that of a random baseline, while ChatGPT, Llama2-Chat,
PaLM-2 and GPT-4 show significantly better performance on this task, comparable
to adult humans. GPT-4, in fact, has the highest post-conventional moral
reasoning score, equivalent to that of typical graduate school students.
However, we also observe that the models do not perform consistently across all
dilemmas, pointing to important gaps in their understanding and reasoning
abilities.",None,-1
1236c474-a346-41ad-a7ac-61cf40fa96ea,Silver Syntax Pre-training for Cross-Domain Relation Extraction,0.231544,"Relation Extraction (RE) remains a challenging task, especially when
considering realistic out-of-domain evaluations. One of the main reasons for
this is the limited training size of current RE datasets: obtaining
high-quality (manually annotated) data is extremely expensive and cannot
realistically be repeated for each new domain. An intermediate training step on
data from related tasks has shown to be beneficial across many NLP
tasks.However, this setup still requires supplementary annotated data, which is
often not available. In this paper, we investigate intermediate pre-training
specifically for RE. We exploit the affinity between syntactic structure and
semantic RE, and identify the syntactic relations which are closely related to
RE by being on the shortest dependency path between two entities. We then take
advantage of the high accuracy of current syntactic parsers in order to
automatically obtain large amounts of low-cost pre-training data. By
pre-training our RE model on the relevant syntactic relations, we are able to
outperform the baseline in five out of six cross-domain setups, without any
additional annotated data.",None,-1
dc54f390-45a5-4e57-9449-44550c66d364,Fine-tuning of explainable CNNs for skin lesion classification based on dermatologists' feedback towards increasing trust,0.152653,"In this paper, we propose a CNN fine-tuning method which enables users to
give simultaneous feedback on two outputs: the classification itself and the
visual explanation for the classification. We present the effect of this
feedback strategy in a skin lesion classification task and measure how CNNs
react to the two types of user feedback. To implement this approach, we propose
a novel CNN architecture that integrates the Grad-CAM technique for explaining
the model's decision in the training loop. Using simulated user feedback, we
found that fine-tuning our model on both classification and explanation
improves visual explanation while preserving classification accuracy, thus
potentially increasing the trust of users in using CNN-based skin lesion
classifiers.",None,-1
ba77c968-bce9-4424-b914-7d6d43459c70,Multimodality of AI for Education: Towards Artificial General Intelligence,0.888273,"This paper presents a comprehensive examination of how multimodal artificial
intelligence (AI) approaches are paving the way towards the realization of
Artificial General Intelligence (AGI) in educational contexts. It scrutinizes
the evolution and integration of AI in educational systems, emphasizing the
crucial role of multimodality, which encompasses auditory, visual, kinesthetic,
and linguistic modes of learning. This research delves deeply into the key
facets of AGI, including cognitive frameworks, advanced knowledge
representation, adaptive learning mechanisms, strategic planning, sophisticated
language processing, and the integration of diverse multimodal data sources. It
critically assesses AGI's transformative potential in reshaping educational
paradigms, focusing on enhancing teaching and learning effectiveness, filling
gaps in existing methodologies, and addressing ethical considerations and
responsible usage of AGI in educational settings. The paper also discusses the
implications of multimodal AI's role in education, offering insights into
future directions and challenges in AGI development. This exploration aims to
provide a nuanced understanding of the intersection between AI, multimodality,
and education, setting a foundation for future research and development in AGI.",None,-1
48f0e101-f087-4b17-a2be-f17d75689585,Learning Visual Representations via Language-Guided Sampling,0.550505,"Although an object may appear in numerous contexts, we often describe it in a
limited number of ways. Language allows us to abstract away visual variation to
represent and communicate concepts. Building on this intuition, we propose an
alternative approach to visual representation learning: using language
similarity to sample semantically similar image pairs for contrastive learning.
Our approach diverges from image-based contrastive learning by sampling view
pairs using language similarity instead of hand-crafted augmentations or
learned clusters. Our approach also differs from image-text contrastive
learning by relying on pre-trained language models to guide the learning rather
than directly minimizing a cross-modal loss. Through a series of experiments,
we show that language-guided learning yields better features than image-based
and image-text representation learning approaches.",None,-1
ea0788e0-eb06-4914-a0f4-fc285097964a,Retentive Network: A Successor to Transformer for Large Language Models,0.674462,"In this work, we propose Retentive Network (RetNet) as a foundation
architecture for large language models, simultaneously achieving training
parallelism, low-cost inference, and good performance. We theoretically derive
the connection between recurrence and attention. Then we propose the retention
mechanism for sequence modeling, which supports three computation paradigms,
i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel
representation allows for training parallelism. The recurrent representation
enables low-cost $O(1)$ inference, which improves decoding throughput, latency,
and GPU memory without sacrificing performance. The chunkwise recurrent
representation facilitates efficient long-sequence modeling with linear
complexity, where each chunk is encoded parallelly while recurrently
summarizing the chunks. Experimental results on language modeling show that
RetNet achieves favorable scaling results, parallel training, low-cost
deployment, and efficient inference. The intriguing properties make RetNet a
strong successor to Transformer for large language models. Code will be
available at https://aka.ms/retnet.",None,-1
41553ebb-9de1-4442-b496-6c79084734e9,GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images,0.449405,"In this work, we focus on synthesizing high-fidelity novel view images for
arbitrary human performers, given a set of sparse multi-view images. It is a
challenging task due to the large variation among articulated body poses and
heavy self-occlusions. To alleviate this, we introduce an effective
generalizable framework Generalizable Model-based Neural Radiance Fields
(GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a
geometry-guided attention mechanism to register the appearance code from
multi-view 2D images to a geometry proxy which can alleviate the misalignment
between inaccurate geometry prior and pixel space. On top of that, we further
conduct neural rendering and partial gradient backpropagation for efficient
perceptual supervision and improvement of the perceptual quality of synthesis.
To evaluate our method, we conduct experiments on synthesized datasets
THuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The
results demonstrate that our approach outperforms state-of-the-art methods in
terms of novel view synthesis and geometric reconstruction.",None,-1
016e6f91-0e27-49be-8e88-4f225e236bc2,VECO 2.0: Cross-lingual Language Model Pre-training with Multi-granularity Contrastive Learning,0.108783,"Recent studies have demonstrated the potential of cross-lingual
transferability by training a unified Transformer encoder for multiple
languages. In addition to involving the masked language model objective,
existing cross-lingual pre-training works leverage sentence-level contrastive
learning or plugs in extra cross-attention module to complement the
insufficient capabilities of cross-lingual alignment. Nonetheless, synonym
pairs residing in bilingual corpus are not exploited and aligned, which is more
crucial than sentence interdependence establishment for token-level tasks. In
this work, we propose a cross-lingual pre-trained model VECO~2.0 based on
contrastive learning with multi-granularity alignments. Specifically, the
sequence-to-sequence alignment is induced to maximize the similarity of the
parallel pairs and minimize the non-parallel pairs. Then, token-to-token
alignment is integrated to bridge the gap between synonymous tokens excavated
via the thesaurus dictionary from the other unpaired tokens in a bilingual
instance. Experiments show the effectiveness of the proposed strategy for
cross-lingual model pre-training on the XTREME benchmark.",None,-1
80ffc32b-a00e-4c7a-8e8d-b852a978846b,NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models,0.983609,"Automatically generating high-quality real world 3D scenes is of enormous
interest for applications such as virtual reality and robotics simulation.
Towards this goal, we introduce NeuralField-LDM, a generative model capable of
synthesizing complex 3D environments. We leverage Latent Diffusion Models that
have been successfully utilized for efficient high-quality 2D content creation.
We first train a scene auto-encoder to express a set of image and pose pairs as
a neural field, represented as density and feature voxel grids that can be
projected to produce novel views of the scene. To further compress this
representation, we train a latent-autoencoder that maps the voxel grids to a
set of latent representations. A hierarchical diffusion model is then fit to
the latents to complete the scene generation pipeline. We achieve a substantial
improvement over existing state-of-the-art scene generation models.
Additionally, we show how NeuralField-LDM can be used for a variety of 3D
content creation applications, including conditional scene generation, scene
inpainting and scene style manipulation.",None,-1
543054c0-7105-46a8-89c1-9663f024700a,Single-Stage Heavy-Tailed Food Classification,0.870618,"Deep learning based food image classification has enabled more accurate
nutrition content analysis for image-based dietary assessment by predicting the
types of food in eating occasion images. However, there are two major obstacles
to apply food classification in real life applications. First, real life food
images are usually heavy-tailed distributed, resulting in severe
class-imbalance issue. Second, it is challenging to train a single-stage (i.e.
end-to-end) framework under heavy-tailed data distribution, which cause the
over-predictions towards head classes with rich instances and under-predictions
towards tail classes with rare instance. In this work, we address both issues
by introducing a novel single-stage heavy-tailed food classification framework.
Our method is evaluated on two heavy-tailed food benchmark datasets, Food101-LT
and VFN-LT, and achieves the best performance compared to existing work with
over 5% improvements for top-1 accuracy.",None,-1
25470270-5855-4b70-99dc-55b3efe41e33,Exploring Large Language Models for Human Mobility Prediction under Public Events,0.773698,"Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.",None,-1
cdafa630-e9c1-4d2e-adb9-961192aa1099,Giraffe: Adventures in Expanding Context Lengths in LLMs,0.785381,"Modern large language models (LLMs) that rely on attention mechanisms are
typically trained with fixed context lengths which enforce upper limits on the
length of input sequences that they can handle at evaluation time. To use these
models on sequences longer than the train-time context length, one might employ
techniques from the growing family of context length extrapolation methods --
most of which focus on modifying the system of positional encodings used in the
attention mechanism to indicate where tokens or activations are located in the
input sequence. We conduct a wide survey of existing methods of context length
extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own
design as well -- in particular, a new truncation strategy for modifying the
basis for the position encoding.
  We test these methods using three new evaluation tasks (FreeFormQA,
AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to
be less fine-grained as a measure of long context performance of LLMs. We
release the three tasks publicly as datasets on HuggingFace. We discover that
linear scaling is the best method for extending context length, and show that
further gains can be achieved by using longer scales at evaluation time. We
also discover promising extrapolation capabilities in the truncated basis. To
support further research in this area, we release three new 13B parameter
long-context models which we call Giraffe: 4k and 16k context models trained
from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We
also release the code to replicate our results.",None,-1
8c4c04cc-ac74-45f3-88e9-5da8d69dc3d0,On the Neural Tangent Kernel of Equilibrium Models,0.889757,"This work studies the neural tangent kernel (NTK) of the deep equilibrium
(DEQ) model, a practical ``infinite-depth'' architecture which directly
computes the infinite-depth limit of a weight-tied network via root-finding.
Even though the NTK of a fully-connected neural network can be stochastic if
its width and depth both tend to infinity simultaneously, we show that
contrarily a DEQ model still enjoys a deterministic NTK despite its width and
depth going to infinity at the same time under mild conditions. Moreover, this
deterministic NTK can be found efficiently via root-finding.",None,-1
9760d4ea-ba01-42af-b8c5-743c790fae7d,Pose Impact Estimation on Face Recognition using 3D-Aware Synthetic Data with Application to Quality Assessment,0.31313,"Evaluating the quality of facial images is essential for operating face
recognition systems with sufficient accuracy. The recent advances in face
quality standardisation (ISO/IEC CD3 29794-5) recommend the usage of component
quality measures for breaking down face quality into its individual factors,
hence providing valuable feedback for operators to re-capture low-quality
images. In light of recent advances in 3D-aware generative adversarial
networks, we propose a novel dataset, Syn-YawPitch, comprising 1000 identities
with varying yaw-pitch angle combinations. Utilizing this dataset, we
demonstrate that pitch angles beyond 30 degrees have a significant impact on
the biometric performance of current face recognition systems. Furthermore, we
propose a lightweight and explainable pose quality predictor that adheres to
the draft international standard of ISO/IEC CD3 29794-5 and benchmark it
against state-of-the-art face image quality assessment algorithms",None,-1
a500224e-3ae1-4751-93b9-40aea09aeaad,Axiomatic Preference Modeling for Longform Question Answering,0.117877,"The remarkable abilities of large language models (LLMs) like GPT-4 partially
stem from post-training processes like Reinforcement Learning from Human
Feedback (RLHF) involving human preferences encoded in a reward model. However,
these reward models (RMs) often lack direct knowledge of why, or under what
principles, the preferences annotations were made. In this study, we identify
principles that guide RMs to better align with human preferences, and then
develop an axiomatic framework to generate a rich variety of preference signals
to uphold them. We use these axiomatic signals to train a model for scoring
answers to longform questions. Our approach yields a Preference Model with only
about 220M parameters that agrees with gold human-annotated preference labels
more often than GPT-4. The contributions of this work include: training a
standalone preference model that can score human- and LLM-generated answers on
the same scale; developing an axiomatic framework for generating training data
pairs tailored to certain principles; and showing that a small amount of
axiomatic signals can help small models outperform GPT-4 in preference scoring.
We release our model on huggingface:
https://huggingface.co/corbyrosset/axiomatic_preference_model",None,-1
e31114be-92f8-4663-ba7c-cd4318567ede,Accurate and Structured Pruning for Efficient Automatic Speech Recognition,0.531468,"Automatic Speech Recognition (ASR) has seen remarkable advancements with deep
neural networks, such as Transformer and Conformer. However, these models
typically have large model sizes and high inference costs, posing a challenge
to deploy on resource-limited devices. In this paper, we propose a novel
compression strategy that leverages structured pruning and knowledge
distillation to reduce the model size and inference cost of the Conformer model
while preserving high recognition performance. Our approach utilizes a set of
binary masks to indicate whether to retain or prune each Conformer module, and
employs L0 regularization to learn the optimal mask values. To further enhance
pruning performance, we use a layerwise distillation strategy to transfer
knowledge from unpruned to pruned models. Our method outperforms all pruning
baselines on the widely used LibriSpeech benchmark, achieving a 50% reduction
in model size and a 28% reduction in inference cost with minimal performance
loss.",None,-1
2f8ab432-089d-47e9-87b2-bd22f07353ab,Audio-Visual Class-Incremental Learning,0.999428,"In this paper, we introduce audio-visual class-incremental learning, a
class-incremental learning scenario for audio-visual video recognition. We
demonstrate that joint audio-visual modeling can improve class-incremental
learning, but current methods fail to preserve semantic similarity between
audio and visual features as incremental step grows. Furthermore, we observe
that audio-visual correlations learned in previous tasks can be forgotten as
incremental steps progress, leading to poor performance. To overcome these
challenges, we propose AV-CIL, which incorporates Dual-Audio-Visual Similarity
Constraint (D-AVSC) to maintain both instance-aware and class-aware semantic
similarity between audio-visual modalities and Visual Attention Distillation
(VAD) to retain previously learned audio-guided visual attentive ability. We
create three audio-visual class-incremental datasets, AVE-Class-Incremental
(AVE-CI), Kinetics-Sounds-Class-Incremental (K-S-CI), and
VGGSound100-Class-Incremental (VS100-CI) based on the AVE, Kinetics-Sounds, and
VGGSound datasets, respectively. Our experiments on AVE-CI, K-S-CI, and
VS100-CI demonstrate that AV-CIL significantly outperforms existing
class-incremental learning methods in audio-visual class-incremental learning.
Code and data are available at: https://github.com/weiguoPian/AV-CIL_ICCV2023.",None,-1
1fe7c249-5417-41c2-83cb-19c216b66c23,TransCAR: Transformer-based Camera-And-Radar Fusion for 3D Object Detection,0.620433,"Despite radar's popularity in the automotive industry, for fusion-based 3D
object detection, most existing works focus on LiDAR and camera fusion. In this
paper, we propose TransCAR, a Transformer-based Camera-And-Radar fusion
solution for 3D object detection. Our TransCAR consists of two modules. The
first module learns 2D features from surround-view camera images and then uses
a sparse set of 3D object queries to index into these 2D features. The
vision-updated queries then interact with each other via transformer
self-attention layer. The second module learns radar features from multiple
radar scans and then applies transformer decoder to learn the interactions
between radar features and vision-updated queries. The cross-attention layer
within the transformer decoder can adaptively learn the soft-association
between the radar features and vision-updated queries instead of
hard-association based on sensor calibration only. Finally, our model estimates
a bounding box per query using set-to-set Hungarian loss, which enables the
method to avoid non-maximum suppression. TransCAR improves the velocity
estimation using the radar scans without temporal information. The superior
experimental results of our TransCAR on the challenging nuScenes datasets
illustrate that our TransCAR outperforms state-of-the-art Camera-Radar
fusion-based 3D object detection approaches.",None,-1
01625b44-8e78-4a2d-8dbc-7c89a793b521,Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues,0.435928,"Discourse processing suffers from data sparsity, especially for dialogues. As
a result, we explore approaches to build discourse structures for dialogues,
based on attention matrices from Pre-trained Language Models (PLMs). We
investigate multiple tasks for fine-tuning and show that the dialogue-tailored
Sentence Ordering task performs best. To locate and exploit discourse
information in PLMs, we propose an unsupervised and a semi-supervised method.
Our proposals achieve encouraging results on the STAC corpus, with F1 scores of
57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When
restricted to projective trees, our scores improved to 63.3 and 68.1.",None,-1
1052e758-8531-4ebf-999d-41c63d8e79cc,Removing RLHF Protections in GPT-4 via Fine-Tuning,0.837009,"As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the
contrary: fine-tuning allows attackers to remove RLHF protections with as few
as 340 examples and a 95% success rate. These training examples can be
automatically generated with weaker models. We further show that removing RLHF
protections does not decrease usefulness on non-censored outputs, providing
evidence that our fine-tuning strategy does not decrease usefulness despite
using weaker models to generate training data. Our results show the need for
further research on protections on LLMs.",None,-1
8e0d4e04-5a60-4e3e-bf92-b0984dd37385,An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts,0.892396,"With a surge in identifying suicidal risk and its severity in social media
posts, we argue that a more consequential and explainable research is required
for optimal impact on clinical psychology practice and personalized mental
healthcare. The success of computational intelligence techniques for inferring
mental illness from social media resources, points to natural language
processing as a lens for determining Interpersonal Risk Factors (IRF) in human
writings. Motivated with limited availability of datasets for social NLP
research community, we construct and release a new annotated dataset with
human-labelled explanations and classification of IRF affecting mental
disturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)
Perceived Burdensomeness (PBu). We establish baseline models on our dataset
facilitating future research directions to develop real-time personalized AI
models by detecting patterns of TBe and PBu in emotional spectrum of user's
historical social media profile.",None,-1
ed522fd6-9513-49a3-a864-78abcbeab90c,Enhancing Low-resolution Face Recognition with Feature Similarity Knowledge Distillation,0.233339,"In this study, we introduce a feature knowledge distillation framework to
improve low-resolution (LR) face recognition performance using knowledge
obtained from high-resolution (HR) images. The proposed framework transfers
informative features from an HR-trained network to an LR-trained network by
reducing the distance between them. A cosine similarity measure was employed as
a distance metric to effectively align the HR and LR features. This approach
differs from conventional knowledge distillation frameworks, which use the L_p
distance metrics and offer the advantage of converging well when reducing the
distance between features of different resolutions. Our framework achieved a 3%
improvement over the previous state-of-the-art method on the AgeDB-30 benchmark
without bells and whistles, while maintaining a strong performance on HR
images. The effectiveness of cosine similarity as a distance metric was
validated through statistical analysis, making our approach a promising
solution for real-world applications in which LR images are frequently
encountered. The code and pretrained models are publicly available on
https://github.com/gist-ailab/feature-similarity-KD.",None,-1
5b87930f-9be7-43d5-997a-6d0da4730932,Building a Non-native Speech Corpus Featuring Chinese-English Bilingual Children: Compilation and Rationale,0.0321577,"This paper introduces a non-native speech corpus consisting of narratives
from fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5
hours of children taking a narrative comprehension test in English (L2) are
presented, along with human-rated scores and annotations of grammatical and
pronunciation errors. The children also completed the parallel MAIN tests in
Chinese (L1) for reference purposes. For all tests we recorded audio and video
with our innovative self-developed remote collection methods. The video
recordings serve to mitigate the challenge of low intelligibility in L2
narratives produced by young children during the transcription process. This
corpus offers valuable resources for second language teaching and has the
potential to enhance the overall performance of automatic speech recognition
(ASR).",None,-1
c570768f-2ebe-42a9-977f-ddf75e6de197,MoSFPAD: An end-to-end Ensemble of MobileNet and Support Vector Classifier for Fingerprint Presentation Attack Detection,0.756462,"Automatic fingerprint recognition systems are the most extensively used
systems for person authentication although they are vulnerable to Presentation
attacks. Artificial artifacts created with the help of various materials are
used to deceive these systems causing a threat to the security of
fingerprint-based applications. This paper proposes a novel end-to-end model to
detect fingerprint Presentation attacks. The proposed model incorporates
MobileNet as a feature extractor and a Support Vector Classifier as a
classifier to detect presentation attacks in cross-material and cross-sensor
paradigms. The feature extractor's parameters are learned with the loss
generated by the support vector classifier. The proposed model eliminates the
need for intermediary data preparation procedures, unlike other static hybrid
architectures. The performance of the proposed model has been validated on
benchmark LivDet 2011, 2013, 2015, 2017, and 2019 databases, and overall
accuracy of 98.64%, 99.50%, 97.23%, 95.06%, and 95.20% is achieved on these
databases, respectively. The performance of the proposed model is compared with
state-of-the-art methods and the proposed method outperforms in cross-material
and cross-sensor paradigms in terms of average classification error.",None,-1
40975231-0759-46a7-a01b-293656344f89,Just a Glimpse: Rethinking Temporal Information for Video Continual Learning,0.280267,"Class-incremental learning is one of the most important settings for the
study of Continual Learning, as it closely resembles real-world application
scenarios. With constrained memory sizes, catastrophic forgetting arises as the
number of classes/tasks increases. Studying continual learning in the video
domain poses even more challenges, as video data contains a large number of
frames, which places a higher burden on the replay memory. The current common
practice is to sub-sample frames from the video stream and store them in the
replay memory. In this paper, we propose SMILE a novel replay mechanism for
effective video continual learning based on individual/single frames. Through
extensive experimentation, we show that under extreme memory constraints, video
diversity plays a more significant role than temporal information. Therefore,
our method focuses on learning from a small number of frames that represent a
large number of unique videos. On three representative video datasets,
Kinetics, UCF101, and ActivityNet, the proposed method achieves
state-of-the-art performance, outperforming the previous state-of-the-art by up
to 21.49%.",None,-1
10569332-bd03-4c07-ada1-13d96ee29c12,Time Series as Images: Vision Transformer for Irregularly Sampled Time Series,0.738281,"Irregularly sampled time series are increasingly prevalent, particularly in
medical domains. While various specialized methods have been developed to
handle these irregularities, effectively modeling their complex dynamics and
pronounced sparsity remains a challenge. This paper introduces a novel
perspective by converting irregularly sampled time series into line graph
images, then utilizing powerful pre-trained vision transformers for time series
classification in the same way as image classification. This method not only
largely simplifies specialized algorithm designs but also presents the
potential to serve as a universal framework for time series modeling.
Remarkably, despite its simplicity, our approach outperforms state-of-the-art
specialized algorithms on several popular healthcare and human activity
datasets. Especially in the rigorous leave-sensors-out setting where a portion
of variables is omitted during testing, our method exhibits strong robustness
against varying degrees of missing observations, achieving an impressive
improvement of 42.8% in absolute F1 score points over leading specialized
baselines even with half the variables masked. Code and data are available at
https://github.com/Leezekun/ViTST",None,-1
8f96a4fd-b97b-4e7f-87d1-6a72645b4bd6,Discrete Prompt Compression with Reinforcement Learning,0.144887,"Compressed prompts aid instruction-tuned language models (LMs) in overcoming
context window limitations and reducing computational costs. Existing methods,
which primarily based on training embeddings, face various challenges
associated with interpretability, the fixed number of embedding tokens,
reusability across different LMs, and inapplicability when interacting with
black-box APIs. This study proposes prompt compression with reinforcement
learning (PCRL), which is a discrete prompt compression method that addresses
these issues. The proposed PCRL method utilizes a computationally efficient
policy network that edits prompts directly. The training approach employed in
the proposed PCRLs can be applied flexibly to various types of LMs, including
both decoder-only and encoder-decoder architecture and it can be trained
without gradient access to the LMs or labeled data. The proposed PCRL achieves
an average reduction of 24.6% in terms of the token count across various
instruction prompts while maintaining sufficient performance. In addition, we
demonstrate that the learned policy can be transferred to larger LMs, and
through a comprehensive analysis, we explore the token importance within the
prompts. Our code is accessible at
https://github.com/nenomigami/PromptCompressor.",None,-1
496c067c-783c-449f-bc4f-008a77cc67ee,Unleash the Potential of Image Branch for Cross-modal 3D Object Detection,0.386793,"To achieve reliable and precise scene understanding, autonomous vehicles
typically incorporate multiple sensing modalities to capitalize on their
complementary attributes. However, existing cross-modal 3D detectors do not
fully utilize the image domain information to address the bottleneck issues of
the LiDAR-based detectors. This paper presents a new cross-modal 3D object
detector, namely UPIDet, which aims to unleash the potential of the image
branch from two aspects. First, UPIDet introduces a new 2D auxiliary task
called normalized local coordinate map estimation. This approach enables the
learning of local spatial-aware features from the image modality to supplement
sparse point clouds. Second, we discover that the representational capability
of the point cloud backbone can be enhanced through the gradients
backpropagated from the training objectives of the image branch, utilizing a
succinct and effective point-to-pixel module. Extensive experiments and
ablation studies validate the effectiveness of our method. Notably, we achieved
the top rank in the highly competitive cyclist class of the KITTI benchmark at
the time of submission. The source code is available at
https://github.com/Eaphan/UPIDet.",None,-1
679ca2ec-bbfb-464a-b11b-7aa0c1f4a807,An Explainable Collaborative Dialogue System using a Theory of Mind,0.0787908,"Eva is a neuro-symbolic domain-independent multimodal collaborative dialogue
system that takes seriously that the purpose of task-oriented dialogue is to
assist the user. To do this, the system collaborates by inferring their
intentions and plans, detects obstacles to success, finds plans to overcome
them or to achieve higher-level goals, and plans its actions, including speech
acts, to help users accomplish those goals. In doing so, the system maintains
and reasons with its own declaratively-specified beliefs, goals and intentions,
and explicitly reasons about those of its user. Because Eva can track different
users' mental states, it can engage multiple agents in multi-party dialogues.
Reasoning is accomplished with a modal Horn-clause meta-interpreter that
enables computable inference within the subset of logic implemented. The system
employs both hierarchical and backward-chaining planning, operating over a rich
modal logic-based knowledge and action representation. The planning and
reasoning subsystems obey the principles of persistent goals and intentions
including: 1) The formation and decomposition of intentions to perform complex
actions, 2) the conditions under which persistent goals and intentions can be
given up, and 3) persistent goal and intention revision using the relativizing
formulas that are created during the planning process. The system treats its
speech acts just like its other actions. This general approach enables Eva to
plan a variety of speech acts, including requests, informs, questions,
confirmations, offers, acceptances, and emotive expressions. Because the
dialogue engine is a planner, as the dialogue proceeds, the system can flexibly
generate, execute, and potentially repair its plans using physical, digital,
and speech actions. Importantly, Eva can explain its utterances because it has
created a plan that caused it to utter them.",None,-1
6acc5b5b-6207-4321-8ebe-d8f2cb96c181,Align and Attend: Multimodal Summarization with Dual Contrastive Losses,0.449563,"The goal of multimodal summarization is to extract the most important
information from different modalities to form output summaries. Unlike the
unimodal summarization, the multimodal summarization task explicitly leverages
cross-modal information to help generate more reliable and high-quality
summaries. However, existing methods fail to leverage the temporal
correspondence between different modalities and ignore the intrinsic
correlation between different samples. To address this issue, we introduce
Align and Attend Multimodal Summarization (A2Summ), a unified multimodal
transformer-based model which can effectively align and attend the multimodal
input. In addition, we propose two novel contrastive losses to model both
inter-sample and intra-sample correlations. Extensive experiments on two
standard video summarization datasets (TVSum and SumMe) and two multimodal
summarization datasets (Daily Mail and CNN) demonstrate the superiority of
A2Summ, achieving state-of-the-art performances on all datasets. Moreover, we
collected a large-scale multimodal summarization dataset BLiSS, which contains
livestream videos and transcribed texts with annotated summaries. Our code and
dataset are publicly available at ~\url{https://boheumd.github.io/A2Summ/}.",None,-1
2a8b4dda-00dc-4bca-be54-413fd74e8e43,Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long Documents with LLMs,0.148151,"Large Language Models (LLMs) demonstrate exceptional performance in textual
understanding and tabular reasoning tasks. However, their ability to comprehend
and analyze hybrid text, containing textual and tabular data, remains
underexplored. In this research, we specialize in harnessing the potential of
LLMs to comprehend critical information from financial reports, which are
hybrid long-documents. We propose an Automated Financial Information Extraction
(AFIE) framework that enhances LLMs' ability to comprehend and extract
information from financial reports. To evaluate AFIE, we develop a Financial
Reports Numerical Extraction (FINE) dataset and conduct an extensive
experimental analysis. Our framework is effectively validated on GPT-3.5 and
GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,
compared to a naive method. These results suggest that the AFIE framework
offers accuracy for automated numerical extraction from complex, hybrid
documents.",None,-1
8e9eea42-9ac1-4a9c-9518-941e259b521d,AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning,0.558006,"Safety is a critical hurdle that limits the application of deep reinforcement
learning (RL) to real-world control tasks. To this end, constrained
reinforcement learning leverages cost functions to improve safety in
constrained Markov decision processes. However, such constrained RL methods
fail to achieve zero violation even when the cost limit is zero. This paper
analyzes the reason for such failure, which suggests that a proper cost
function plays an important role in constrained RL. Inspired by the analysis,
we propose AutoCost, a simple yet effective framework that automatically
searches for cost functions that help constrained RL to achieve zero-violation
performance. We validate the proposed method and the searched cost function on
the safe RL benchmark Safety Gym. We compare the performance of augmented
agents that use our cost function to provide additive intrinsic costs with
baseline agents that use the same policy learners but with only extrinsic
costs. Results show that the converged policies with intrinsic costs in all
environments achieve zero constraint violation and comparable performance with
baselines.",None,-1
75005647-a870-4954-83da-38116af62be9,MultiMediate'23: Engagement Estimation and Bodily Behaviour Recognition in Social Interactions,0.901612,"Automatic analysis of human behaviour is a fundamental prerequisite for the
creation of machines that can effectively interact with- and support humans in
social interactions. In MultiMediate'23, we address two key human social
behaviour analysis tasks for the first time in a controlled challenge:
engagement estimation and bodily behaviour recognition in social interactions.
This paper describes the MultiMediate'23 challenge and presents novel sets of
annotations for both tasks. For engagement estimation we collected novel
annotations on the NOvice eXpert Interaction (NOXI) database. For bodily
behaviour recognition, we annotated test recordings of the MPIIGroupInteraction
corpus with the BBSI annotation scheme. In addition, we present baseline
results for both challenge tasks.",None,-1
1d3a9768-5b9a-4855-92f4-5914c6622081,MODIFY: Model-driven Face Stylization without Style Images,0.074641,"Existing face stylization methods always acquire the presence of the target
(style) domain during the translation process, which violates privacy
regulations and limits their applicability in real-world systems. To address
this issue, we propose a new method called MODel-drIven Face stYlization
(MODIFY), which relies on the generative model to bypass the dependence of the
target images. Briefly, MODIFY first trains a generative model in the target
domain and then translates a source input to the target domain via the provided
style model. To preserve the multimodal style information, MODIFY further
introduces an additional remapping network, mapping a known continuous
distribution into the encoder's embedding space. During translation in the
source domain, MODIFY fine-tunes the encoder module within the target
style-persevering model to capture the content of the source input as precisely
as possible. Our method is extremely simple and satisfies versatile training
modes for face stylization. Experimental results on several different datasets
validate the effectiveness of MODIFY for unsupervised face stylization.",None,-1
ab4f7ba3-e87f-444e-b630-f6b0e8b26676,Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation,0.925784,"The automatic detection of hate speech online is an active research area in
NLP. Most of the studies to date are based on social media datasets that
contribute to the creation of hate speech detection models trained on them.
However, data creation processes contain their own biases, and models
inherently learn from these dataset-specific biases. In this paper, we perform
a large-scale cross-dataset comparison where we fine-tune language models on
different hate speech detection datasets. This analysis shows how some datasets
are more generalisable than others when used as training data. Crucially, our
experiments show how combining hate speech detection datasets can contribute to
the development of robust hate speech detection models. This robustness holds
even when controlling by data size and compared with the best individual
datasets.",None,-1
df17a8b6-ed75-400e-a8bc-d39452ec14eb,Exploring Linguistic Style Matching in Online Communities: The Role of Social Context and Conversation Dynamics,0.281873,"Linguistic style matching (LSM) in conversations can be reflective of several
aspects of social influence such as power or persuasion. However, how LSM
relates to the outcomes of online communication on platforms such as Reddit is
an unknown question. In this study, we analyze a large corpus of two-party
conversation threads in Reddit where we identify all occurrences of LSM using
two types of style: the use of function words and formality. Using this
framework, we examine how levels of LSM differ in conversations depending on
several social factors within Reddit: post and subreddit features, conversation
depth, user tenure, and the controversiality of a comment. Finally, we measure
the change of LSM following loss of status after community banning. Our
findings reveal the interplay of LSM in Reddit conversations with several
community metrics, suggesting the importance of understanding conversation
engagement when understanding community dynamics.",None,-1
b40537e0-4d82-4473-8dde-96a6a52bce25,Robust Unsupervised StyleGAN Image Restoration,0.683638,"GAN-based image restoration inverts the generative process to repair images
corrupted by known degradations. Existing unsupervised methods must be
carefully tuned for each task and degradation level. In this work, we make
StyleGAN image restoration robust: a single set of hyperparameters works across
a wide range of degradation levels. This makes it possible to handle
combinations of several degradations, without the need to retune. Our proposed
approach relies on a 3-phase progressive latent space extension and a
conservative optimizer, which avoids the need for any additional regularization
terms. Extensive experiments demonstrate robustness on inpainting, upsampling,
denoising, and deartifacting at varying degradations levels, outperforming
other StyleGAN-based inversion techniques. Our approach also favorably compares
to diffusion-based restoration by yielding much more realistic inversion
results. Code is available at https://lvsn.github.io/RobustUnsupervised/.",None,-1
120d1da2-2413-49e8-8a13-512759013be8,Automatic Readability Assessment for Closely Related Languages,0.906605,"In recent years, the main focus of research on automatic readability
assessment (ARA) has shifted towards using expensive deep learning-based
methods with the primary goal of increasing models' accuracy. This, however, is
rarely applicable for low-resource languages where traditional handcrafted
features are still widely used due to the lack of existing NLP tools to extract
deeper linguistic representations. In this work, we take a step back from the
technical component and focus on how linguistic aspects such as mutual
intelligibility or degree of language relatedness can improve ARA in a
low-resource setting. We collect short stories written in three languages in
the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment
models and explore the interaction of data and features in various
cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel
specialized feature exploiting n-gram overlap applied to languages with high
mutual intelligibility, significantly improves the performance of ARA models
compared to the use of off-the-shelf large multilingual language models alone.
Consequently, when both linguistic representations are combined, we achieve
state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA
in Bikol.",None,-1
09d7b46b-fa3d-46f0-b7b6-c1b5e77e3204,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,0.453804,"Sports video analysis is a widespread research topic. Its applications are
very diverse, like events detection during a match, video summary, or
fine-grained movement analysis of athletes. As part of the MediaEval 2022
benchmarking initiative, this task aims at detecting and classifying subtle
movements from sport videos. We focus on recordings of table tennis matches.
Conducted since 2019, this task provides a classification challenge from
untrimmed videos recorded under natural conditions with known temporal
boundaries for each stroke. Since 2021, the task also provides a stroke
detection challenge from unannotated, untrimmed videos. This year, the
training, validation, and test sets are enhanced to ensure that all strokes are
represented in each dataset. The dataset is now similar to the one used in [1,
2]. This research is intended to build tools for coaches and athletes who want
to further evaluate their sport performances.",None,-1
771966d7-269c-4603-9091-05ef4fa734fd,Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue,0.699011,"Open-domain dialogue system usually requires different sources of knowledge
to generate more informative and evidential responses. However, existing
knowledge-grounded dialogue systems either focus on a single knowledge source
or overlook the dependency between multiple sources of knowledge, which may
result in generating inconsistent or even paradoxical responses. To incorporate
multiple knowledge sources and dependencies between them, we propose SAFARI, a
novel framework that leverages the exceptional capabilities of large language
models (LLMs) in planning, understanding, and incorporating under both
supervised and unsupervised settings. Specifically, SAFARI decouples the
knowledge grounding into multiple sources and response generation, which allows
easy extension to various knowledge sources including the possibility of not
using any sources. To study the problem, we construct a personalized
knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind
\textbf{P}ersona}~(\textbf{KBP}), which is the first to consider the dependency
between persona and implicit knowledge. Experimental results on the KBP dataset
demonstrate that the SAFARI framework can effectively produce
persona-consistent and knowledge-enhanced responses.",None,-1
242d75d3-5a96-495a-8969-dee71795b50a,The Design Space of Generative Models,0.586113,"Card et al.'s classic paper ""The Design Space of Input Devices"" established
the value of design spaces as a tool for HCI analysis and invention. We posit
that developing design spaces for emerging pre-trained, generative AI models is
necessary for supporting their integration into human-centered systems and
practices. We explore what it means to develop an AI model design space by
proposing two design spaces relating to generative AI models: the first
considers how HCI can impact generative models (i.e., interfaces for models)
and the second considers how generative models can impact HCI (i.e., models as
an HCI prototyping material).",None,-1
4dba7994-908d-4b56-b7ac-8fc624713cf7,Gradient-Free Structured Pruning with Unlabeled Data,0.613165,"Large Language Models (LLMs) have achieved great success in solving difficult
tasks across many domains, but such success comes with a high computation cost,
and inference latency. As developers and third parties customize these models,
the need to provide efficient inference has increased. Many efforts have
attempted to reduce inference cost through model compression techniques such as
pruning and distillation. However, these techniques either require labeled
data, or are time-consuming as they require the compressed model to be
retrained to regain accuracy. In this paper, we propose a gradient-free
structured pruning framework that uses only unlabeled data. An evaluation on
the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates
the effectiveness of the proposed approach. By only using the weights of the
pre-trained model and unlabeled data, in a matter of a few minutes on a single
GPU, up to 40% of the original FLOP count can be reduced with less than a 4%
accuracy loss across all tasks considered.",None,-1
054ed535-28d9-4913-8753-7984ec5c06b1,Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications,0.0892316,"Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the
zero-shot capabilities of Large Language Models (LLMs), but in doing so induces
new evaluation metric requirements. We show LLM-based metrics to be well
adapted to these requirements, and leverage them to conduct an investigation of
task-specialization strategies, quantifying the trade-offs that emerge in
practical industrial settings. Our findings offer practitioners actionable
insights for real-world IFT model deployment.",None,-1
10886ace-3c1b-4fbb-8c16-3a615bea57d5,ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding,0.837592,"We introduce ZeroSCROLLS, a zero-shot benchmark for natural language
understanding over long texts, which contains only test and small validation
sets, without training data. We adapt six tasks from the SCROLLS benchmark, and
add four new datasets, including two novel information fusing tasks, such as
aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a
comprehensive evaluation of both open-source and closed large language models,
finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest
average score. However, there is still room for improvement on multiple open
challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to
pass the naive baseline. As the state of the art is a moving target, we invite
researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",None,-1
6a64b6f7-6f27-4c5f-ab62-070dd6fee3e8,Solar Irradiance Anticipative Transformer,0.522922,"This paper proposes an anticipative transformer-based model for short-term
solar irradiance forecasting. Given a sequence of sky images, our proposed
vision transformer encodes features of consecutive images, feeding into a
transformer decoder to predict irradiance values associated with future unseen
sky images. We show that our model effectively learns to attend only to
relevant features in images in order to forecast irradiance. Moreover, the
proposed anticipative transformer captures long-range dependencies between sky
images to achieve a forecasting skill of 21.45 % on a 15 minute ahead
prediction for a newly introduced dataset of all-sky images when compared to a
smart persistence model.",None,-1
d6bdedb1-02bf-4346-a752-fb966b41307e,Visualizing Skiers' Trajectories in Monocular Videos,0.0864937,"Trajectories are fundamental to winning in alpine skiing. Tools enabling the
analysis of such curves can enhance the training activity and enrich
broadcasting content. In this paper, we propose SkiTraVis, an algorithm to
visualize the sequence of points traversed by a skier during its performance.
SkiTraVis works on monocular videos and constitutes a pipeline of a visual
tracker to model the skier's motion and of a frame correspondence module to
estimate the camera's motion. The separation of the two motions enables the
visualization of the trajectory according to the moving camera's perspective.
We performed experiments on videos of real-world professional competitions to
quantify the visualization error, the computational efficiency, as well as the
applicability. Overall, the results achieved demonstrate the potential of our
solution for broadcasting media enhancement and coach assistance.",None,-1
9d05084a-29db-41e2-b37f-ae443a4e07fe,Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion,0.0687808,"Time Series Classification (TSC) is an important and challenging task for
many visual computing applications. Despite the extensive range of methods
developed for TSC, relatively few utilized Deep Neural Networks (DNNs). In this
paper, we propose two novel attention blocks (Global Temporal Attention and
Temporal Pseudo-Gaussian augmented Self-Attention) that can enhance deep
learning-based TSC approaches, even when such approaches are designed and
optimized for a specific dataset or task. We validate this claim by evaluating
multiple state-of-the-art deep learning-based TSC models on the University of
East Anglia (UEA) benchmark, a standardized collection of 30 Multivariate Time
Series Classification (MTSC) datasets. We show that adding the proposed
attention blocks improves base models' average accuracy by up to 3.6%.
Additionally, the proposed TPS block uses a new injection module to include the
relative positional information in transformers. As a standalone unit with less
computational complexity, it enables TPS to perform better than most of the
state-of-the-art DNN-based TSC methods. The source codes for our experimental
setups and proposed attention blocks are made publicly available.",None,-1
c4c75770-cc1a-4258-a1c4-4a7a4bf7211f,Word sense extension,0.756963,"Humans often make creative use of words to express novel senses. A
long-standing effort in natural language processing has been focusing on word
sense disambiguation (WSD), but little has been explored about how the sense
inventory of a word may be extended toward novel meanings. We present a
paradigm of word sense extension (WSE) that enables words to spawn new senses
toward novel context. We develop a framework that simulates novel word sense
extension by first partitioning a polysemous word type into two pseudo-tokens
that mark its different senses, and then inferring whether the meaning of a
pseudo-token can be extended to convey the sense denoted by the token
partitioned from the same word type. Our framework combines cognitive models of
chaining with a learning scheme that transforms a language model embedding
space to support various types of word sense extension. We evaluate our
framework against several competitive baselines and show that it is superior in
predicting plausible novel senses for over 7,500 English words. Furthermore, we
show that our WSE framework improves performance over a range of
transformer-based WSD models in predicting rare word senses with few or zero
mentions in the training data.",None,-1
c7910dcf-8dba-49f0-95de-dbf5d9a8bf55,What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations,0.755915,"We propose and address a new generalisation problem: can a model trained for
action recognition successfully classify actions when they are performed within
a previously unseen scenario and in a previously unseen location? To answer
this question, we introduce the Action Recognition Generalisation Over
scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from
the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We
demonstrate recognition models struggle to generalise over 10 proposed test
splits, each of an unseen scenario in an unseen location. We thus propose CIR,
a method to represent each video as a Cross-Instance Reconstruction of videos
from other domains. Reconstructions are paired with text narrations to guide
the learning of a domain generalisable representation. We provide extensive
analysis and ablations on ARGO1M that show CIR outperforms prior domain
generalisation works on all test splits. Code and data:
https://chiaraplizz.github.io/what-can-a-cook/.",None,-1
0ec1352e-141b-4074-8faf-b0be620f7580,ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,0.66683,"The Transformer structures have been widely used in computer vision and have
recently made an impact in the area of medical image registration. However, the
use of Transformer in most registration networks is straightforward. These
networks often merely use the attention mechanism to boost the feature learning
as the segmentation networks do, but do not sufficiently design to be adapted
for the registration task. In this paper, we propose a novel motion
decomposition Transformer (ModeT) to explicitly model multiple motion
modalities by fully exploiting the intrinsic capability of the Transformer
structure for deformation estimation. The proposed ModeT naturally transforms
the multi-head neighborhood attention relationship into the multi-coordinate
relationship to model multiple motion modes. Then the competitive weighting
module (CWM) fuses multiple deformation sub-fields to generate the resulting
deformation field. Extensive experiments on two public brain magnetic resonance
imaging (MRI) datasets show that our method outperforms current
state-of-the-art registration networks and Transformers, demonstrating the
potential of our ModeT for the challenging non-rigid deformation estimation
problem. The benchmarks and our code are publicly available at
https://github.com/ZAX130/SmileCode.",None,-1
83984d46-232b-4856-81d6-5387c9079106,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,0.559234,"Diffusion-based generative models have recently emerged as powerful solutions
for high-quality synthesis in multiple domains. Leveraging the bidirectional
Markov chains, diffusion probabilistic models generate samples by inferring the
reversed Markov chain based on the learned distribution mapping at the forward
diffusion process. In this work, we propose Modiff, a conditional paradigm that
benefits from the denoising diffusion probabilistic model (DDPM) to tackle the
problem of realistic and diverse action-conditioned 3D skeleton-based motion
generation. We are a pioneering attempt that uses DDPM to synthesize a variable
number of motion sequences conditioned on a categorical action. We evaluate our
approach on the large-scale NTU RGB+D dataset and show improvements over
state-of-the-art motion generation methods.",None,-1
30a4ef15-a090-466a-b532-01064258f95b,Adapting the adapters for code-switching in multilingual ASR,0.24436,"Recently, large pre-trained multilingual speech models have shown potential
in scaling Automatic Speech Recognition (ASR) to many low-resource languages.
Some of these models employ language adapters in their formulation, which helps
to improve monolingual performance and avoids some of the drawbacks of
multi-lingual modeling on resource-rich languages. However, this formulation
restricts the usability of these models on code-switched speech, where two
languages are mixed together in the same utterance. In this work, we propose
ways to effectively fine-tune such models on code-switched speech, by
assimilating information from both language adapters at each language
adaptation point in the network. We also model code-switching as a sequence of
latent binary sequences that can be used to guide the flow of information from
each language adapter at the frame level. The proposed approaches are evaluated
on three code-switched datasets encompassing Arabic, Mandarin, and Hindi
languages paired with English, showing consistent improvements in
code-switching performance with at least 10\% absolute reduction in CER across
all test sets.",None,-1
b0322439-f9f3-4da2-a90b-a9588251dd1c,What's the Meaning of Superhuman Performance in Today's NLU?,0.344702,"In the last five years, there has been a significant focus in Natural
Language Processing (NLP) on developing larger Pretrained Language Models
(PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their
abilities in language understanding, reasoning, and reading comprehension.
These PLMs have achieved impressive results on these benchmarks, even
surpassing human performance in some cases. This has led to claims of
superhuman capabilities and the provocative idea that certain tasks have been
solved. In this position paper, we take a critical look at these claims and ask
whether PLMs truly have superhuman abilities and what the current benchmarks
are really evaluating. We show that these benchmarks have serious limitations
affecting the comparison between humans and PLMs and provide recommendations
for fairer and more transparent benchmarks.",None,-1
54a3ace3-54fe-4f51-931a-4bd3ec6f8086,MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages in China,0.998497,"Current large language models demonstrate deficiencies in understanding
low-resource languages, particularly the minority languages in China. This
limitation stems from the scarcity of available pre-training data. To address
this accessibility challenge, we present MC$^2$, a Multilingual Corpus of
Minority Languages in China, which is the largest open-source corpus of its
kind so far. MC$^2$ includes four underrepresented languages: Tibetan, Uyghur,
Kazakh, and Mongolian. Notably, we focus on the less common writing systems of
Kazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian
script, respectively, which have been long neglected in previous corpus
construction efforts. Recognizing the prevalence of language contamination
within existing corpora, we adopt a quality-centric solution for collecting
MC$^2$, prioritizing accuracy while enhancing diversity. Furthermore, we
underscore the importance of attending to the multiplicity of writing systems,
which is closely related to the cultural awareness of the resulting models. The
MC$^2$ corpus and related models are made public to the community.",None,-1
abd3b4e1-aa34-4813-91e6-5af9b5687e3e,Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens,0.443767,"We argue that translation quality alone is not a sufficient metric for
measuring knowledge transfer in multilingual neural machine translation. To
support this claim, we introduce Representational Transfer Potential (RTP),
which measures representational similarities between languages. We show that
RTP can measure both positive and negative transfer (interference), and find
that RTP is strongly correlated with changes in translation quality, indicating
that transfer does occur. Furthermore, we investigate data and language
characteristics that are relevant for transfer, and find that multi-parallel
overlap is an important yet under-explored feature. Based on this, we develop a
novel training scheme, which uses an auxiliary similarity loss that encourages
representations to be more invariant across languages by taking advantage of
multi-parallel data. We show that our method yields increased translation
quality for low- and mid-resource languages across multiple data and model
setups.",None,-1
1f88ea5e-1359-459d-9398-390226afc49a,ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition,0.538763,"Class imbalance is a common challenge in real-world recognition tasks, where
the majority of classes have few samples, also known as tail classes. We
address this challenge with the perspective of generalization and empirically
find that the promising Sharpness-Aware Minimization (SAM) fails to address
generalization issues under the class-imbalanced setting. Through investigating
this specific type of task, we identify that its generalization bottleneck
primarily lies in the severe overfitting for tail classes with limited training
data. To overcome this bottleneck, we leverage class priors to restrict the
generalization scope of the class-agnostic SAM and propose a class-aware
smoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the
guidance of class priors, our ImbSAM specifically improves generalization
targeting tail classes. We also verify the efficacy of ImbSAM on two
prototypical applications of class-imbalanced recognition: long-tailed
classification and semi-supervised anomaly detection, where our ImbSAM
demonstrates remarkable performance improvements for tail classes and anomaly.
Our code implementation is available at
https://github.com/cool-xuan/Imbalanced_SAM.",None,-1
e017c3b5-0b21-42b8-9097-d4e8c63ee3ce,"NTIRE 2023 Challenge on Light Field Image Super-Resolution: Dataset, Methods and Results",0.99739,"In this report, we summarize the first NTIRE challenge on light field (LF)
image super-resolution (SR), which aims at super-resolving LF images under the
standard bicubic degradation with a magnification factor of 4. This challenge
develops a new LF dataset called NTIRE-2023 for validation and test, and
provides a toolbox called BasicLFSR to facilitate model development. Compared
with single image SR, the major challenge of LF image SR lies in how to exploit
complementary angular information from plenty of views with varying
disparities. In total, 148 participants have registered the challenge, and 11
teams have successfully submitted results with PSNR scores higher than the
baseline method LF-InterNet \cite{LF-InterNet}. These newly developed methods
have set new state-of-the-art in LF image SR, e.g., the winning method achieves
around 1 dB PSNR improvement over the existing state-of-the-art method DistgSSR
\cite{DistgLF}. We report the solutions proposed by the participants, and
summarize their common trends and useful tricks. We hope this challenge can
stimulate future research and inspire new ideas in LF image SR.",None,-1
57e2881a-6876-4a92-876c-f554e1c4d4d2,Improving Medical Dialogue Generation with Abstract Meaning Representations,0.618325,"Medical Dialogue Generation serves a critical role in telemedicine by
facilitating the dissemination of medical expertise to patients. Existing
studies focus on incorporating textual representations, which have limited
their ability to represent the semantics of text, such as ignoring important
medical entities. To enhance the model's understanding of the textual semantics
and the medical knowledge including entities and relations, we introduce the
use of Abstract Meaning Representations (AMR) to construct graphical
representations that delineate the roles of language constituents and medical
entities within the dialogues. In this paper, We propose a novel framework that
models dialogues between patients and healthcare professionals using AMR
graphs, where the neural networks incorporate textual and graphical knowledge
with a dual attention mechanism. Experimental results show that our framework
outperforms strong baseline models in medical dialogue generation,
demonstrating the effectiveness of AMR graphs in enhancing the representations
of medical knowledge and logical relationships. Furthermore, to support future
research in this domain, we provide the corresponding source code at
https://github.com/Bernard-Yang/MedDiaAMR.",None,-1
6cfd1927-8746-4503-9c8f-9174cb410e68,Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention,0.157164,"Most of the existing audio-driven 3D facial animation methods suffered from
the lack of detailed facial expression and head pose, resulting in
unsatisfactory experience of human-robot interaction. In this paper, a novel
pose-controllable 3D facial animation synthesis method is proposed by utilizing
hierarchical audio-vertex attention. To synthesize real and detailed
expression, a hierarchical decomposition strategy is proposed to encode the
audio signal into both a global latent feature and a local vertex-wise control
feature. Then the local and global audio features combined with vertex spatial
features are used to predict the final consistent facial animation via a graph
convolutional neural network by fusing the intrinsic spatial topology structure
of the face model and the corresponding semantic feature of the audio. To
accomplish pose-controllable animation, we introduce a novel pose attribute
augmentation method by utilizing the 2D talking face technique. Experimental
results indicate that the proposed method can produce more realistic facial
expressions and head posture movements. Qualitative and quantitative
experiments show that the proposed method achieves competitive performance
against state-of-the-art methods.",None,-1
549b646c-57bb-4550-b628-cfeaf5a94388,Automatically Identifying Relations Between Self-Admitted Technical Debt Across Different Sources,0.345399,"Self-Admitted Technical Debt or SATD can be found in various sources, such as
source code comments, commit messages, issue tracking systems, and pull
requests. Previous research has established the existence of relations between
SATD items in different sources; such relations can be useful for investigating
and improving SATD management. However, there is currently a lack of approaches
for automatically detecting these SATD relations. To address this, we proposed
and evaluated approaches for automatically identifying SATD relations across
different sources. Our findings show that our approach outperforms baseline
approaches by a large margin, achieving an average F1-score of 0.829 in
identifying relations between SATD items. Moreover, we explored the
characteristics of SATD relations in 103 open-source projects and describe nine
major cases in which related SATD is documented in a second source, and give a
quantitative overview of 26 kinds of relations.",None,-1
afde2460-3803-4bff-ade3-3c01b025a2c6,Testing System Intelligence,0.331622,"We discuss the adequacy of tests for intelligent systems and practical
problems raised by their implementation. We propose the replacement test as the
ability of a system to replace successfully another system performing a task in
a given context. We show how it can characterize salient aspects of human
intelligence that cannot be taken into account by the Turing test. We argue
that building intelligent systems passing the replacement test involves a
series of technical problems that are outside the scope of current AI. We
present a framework for implementing the proposed test and validating the
properties of the intelligent systems. We discuss the inherent limitations of
intelligent system validation and advocate new theoretical foundations for
extending existing rigorous test methods. We suggest that the replacement test,
based on the complementarity of skills between human and machine, can lead to a
multitude of intelligence concepts reflecting the ability to combine data-based
and symbolic knowledge to varying degrees.",None,-1
14973a54-41d9-4278-b937-14c8bc945a18,A2S-NAS: Asymmetric Spectral-Spatial Neural Architecture Search For Hyperspectral Image Classification,0.433445,"Existing deep learning-based hyperspectral image (HSI) classification works
still suffer from the limitation of the fixed-sized receptive field, leading to
difficulties in distinctive spectral-spatial features for ground objects with
various sizes and arbitrary shapes. Meanwhile, plenty of previous works ignore
asymmetric spectral-spatial dimensions in HSI. To address the above issues, we
propose a multi-stage search architecture in order to overcome asymmetric
spectral-spatial dimensions and capture significant features. First, the
asymmetric pooling on the spectral-spatial dimension maximally retains the
essential features of HSI. Then, the 3D convolution with a selectable range of
receptive fields overcomes the constraints of fixed-sized convolution kernels.
Finally, we extend these two searchable operations to different layers of each
stage to build the final architecture. Extensive experiments are conducted on
two challenging HSI benchmarks including Indian Pines and Houston University,
and results demonstrate the effectiveness of the proposed method with superior
performance compared with the related works.",None,-1
d4182153-1b07-433b-9655-db8cade90ede,Continual Learning as Computationally Constrained Reinforcement Learning,0.681186,"An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research.",None,-1
3466533f-b06a-4218-bcdc-9e5982b57778,Adversarial Alignment for Source Free Object Detection,0.601669,"Source-free object detection (SFOD) aims to transfer a detector pre-trained
on a label-rich source domain to an unlabeled target domain without seeing
source data. While most existing SFOD methods generate pseudo labels via a
source-pretrained model to guide training, these pseudo labels usually contain
high noises due to heavy domain discrepancy. In order to obtain better pseudo
supervisions, we divide the target domain into source-similar and
source-dissimilar parts and align them in the feature space by adversarial
learning. Specifically, we design a detection variance-based criterion to
divide the target domain. This criterion is motivated by a finding that larger
detection variances denote higher recall and larger similarity to the source
domain. Then we incorporate an adversarial module into a mean teacher framework
to drive the feature spaces of these two subsets indistinguishable. Extensive
experiments on multiple cross-domain object detection datasets demonstrate that
our proposed method consistently outperforms the compared SFOD methods.",None,-1
5217bd56-baa2-46f8-abcb-968150790ef7,Towards a performance analysis on pre-trained Visual Question Answering models for autonomous driving,0.0799811,"This short paper presents a preliminary analysis of three popular Visual
Question Answering (VQA) models, namely ViLBERT, ViLT, and LXMERT, in the
context of answering questions relating to driving scenarios. The performance
of these models is evaluated by comparing the similarity of responses to
reference answers provided by computer vision experts. Model selection is
predicated on the analysis of transformer utilization in multimodal
architectures. The results indicate that models incorporating cross-modal
attention and late fusion techniques exhibit promising potential for generating
improved answers within a driving perspective. This initial analysis serves as
a launchpad for a forthcoming comprehensive comparative study involving nine
VQA models and sets the scene for further investigations into the effectiveness
of VQA model queries in self-driving scenarios. Supplementary material is
available at
https://github.com/KaavyaRekanar/Towards-a-performance-analysis-on-pre-trained-VQA-models-for-autonomous-driving.",None,-1
e1d88613-9ad3-4d39-ae3c-1b51d0b506cd,NarrativePlay: Interactive Narrative Understanding,0.305824,"In this paper, we introduce NarrativePlay, a novel system that allows users
to role-play a fictional character and interact with other characters in
narratives such as novels in an immersive environment. We leverage Large
Language Models (LLMs) to generate human-like responses, guided by personality
traits extracted from narratives. The system incorporates auto-generated visual
display of narrative settings, character portraits, and character speech,
greatly enhancing user experience. Our approach eschews predefined sandboxes,
focusing instead on main storyline events extracted from narratives from the
perspective of a user-selected character. NarrativePlay has been evaluated on
two types of narratives, detective and adventure stories, where users can
either explore the world or improve their favorability with the narrative
characters through conversations.",None,-1
3a436ae9-c8e0-418c-b099-3dedd657e67e,Directional Connectivity-based Segmentation of Medical Images,0.350986,"Anatomical consistency in biomarker segmentation is crucial for many medical
image analysis tasks. A promising paradigm for achieving anatomically
consistent segmentation via deep networks is incorporating pixel connectivity,
a basic concept in digital topology, to model inter-pixel relationships.
However, previous works on connectivity modeling have ignored the rich
channel-wise directional information in the latent space. In this work, we
demonstrate that effective disentanglement of directional sub-space from the
shared latent space can significantly enhance the feature representation in the
connectivity-based network. To this end, we propose a directional connectivity
modeling scheme for segmentation that decouples, tracks, and utilizes the
directional information across the network. Experiments on various public
medical image segmentation benchmarks show the effectiveness of our model as
compared to the state-of-the-art methods. Code is available at
https://github.com/Zyun-Y/DconnNet.",None,-1
e3d5b709-0960-4100-9d4f-1ce009e71447,DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation,0.998551,"Text-to-image diffusion models pre-trained on billions of image-text pairs
have recently enabled 3D content creation by optimizing a randomly initialized
differentiable 3D representation with score distillation. However, the
optimization process suffers slow convergence and the resultant 3D models often
exhibit two limitations: (a) quality concerns such as missing attributes and
distorted shape and texture; (b) extremely low diversity comparing to
text-guided image synthesis. In this paper, we show that the conflict between
the 3D optimization process and uniform timestep sampling in score distillation
is the main reason for these limitations. To resolve this conflict, we propose
to prioritize timestep sampling with monotonically non-increasing functions,
which aligns the 3D optimization process with the sampling process of diffusion
model. Extensive experiments show that our simple redesign significantly
improves 3D content creation with faster convergence, better quality and
diversity.",None,-1
