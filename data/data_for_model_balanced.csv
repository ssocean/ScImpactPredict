title,TNCSI,abstract
Insights on Neural Representations for End-to-End Speech Recognition,0.00654443,"End-to-end automatic speech recognition (ASR) models aim to learn a
generalised speech representation. However, there are limited tools available
to understand the internal functions and the effect of hierarchical
dependencies within the model architecture. It is crucial to understand the
correlations between the layer-wise representations, to derive insights on the
relationship between neural representations and performance.
  Previous investigations of network similarities using correlation analysis
techniques have not been explored for End-to-End ASR models. This paper
analyses and explores the internal dynamics between layers during training with
CNN, LSTM and Transformer based approaches using Canonical correlation analysis
(CCA) and centered kernel alignment (CKA) for the experiments. It was found
that neural representations within CNN layers exhibit hierarchical correlation
dependencies as layer depth increases but this is mostly limited to cases where
neural representation correlates more closely. This behaviour is not observed
in LSTM architecture, however there is a bottom-up pattern observed across the
training process, while Transformer encoder layers exhibit irregular
coefficiency correlation as neural depth increases. Altogether, these results
provide new insights into the role that neural architectures have upon speech
recognition performance. More specifically, these techniques can be used as
indicators to build better performing speech recognition models."
TwistSLAM++: Fusing multiple modalities for accurate dynamic semantic SLAM,0.0446313,"Most classical SLAM systems rely on the static scene assumption, which limits
their applicability in real world scenarios. Recent SLAM frameworks have been
proposed to simultaneously track the camera and moving objects. However they
are often unable to estimate the canonical pose of the objects and exhibit a
low object tracking accuracy. To solve this problem we propose TwistSLAM++, a
semantic, dynamic, SLAM system that fuses stereo images and LiDAR information.
Using semantic information, we track potentially moving objects and associate
them to 3D object detections in LiDAR scans to obtain their pose and size.
Then, we perform registration on consecutive object scans to refine object pose
estimation. Finally, object scans are used to estimate the shape of the object
and constrain map points to lie on the estimated surface within the BA. We show
on classical benchmarks that this fusion approach based on multimodal
information improves the accuracy of object tracking."
Automatic Severity Classification of Dysarthric speech by using Self-supervised Model with Multi-task Learning,0.0426828,"Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect."
Atypical lexical abbreviations identification in Russian medical texts,0.00780499,"Abbreviation is a method of word formation that aims to construct the
shortened term from the first letters of the initial phrase. Implicit
abbreviations frequently cause the comprehension difficulties for unprepared
readers. In this paper, we propose an efficient ML-based algorithm which allows
to identify the abbreviations in Russian texts. The method achieves ROC AUC
score 0.926 and F1 score 0.706 which are confirmed as competitive in comparison
with the baselines. Along with the pipeline, we also establish first to our
knowledge Russian dataset that is relevant for the desired task."
Automated Audio Captioning with Epochal Difficult Captions for Curriculum Learning,0.0431766,"In this paper, we propose an algorithm, Epochal Difficult Captions, to
supplement the training of any model for the Automated Audio Captioning task.
Epochal Difficult Captions is an elegant evolution to the keyword estimation
task that previous work have used to train the encoder of the AAC model.
Epochal Difficult Captions modifies the target captions based on a curriculum
and a difficulty level determined as a function of current epoch. Epochal
Difficult Captions can be used with any model architecture and is a lightweight
function that does not increase training time. We test our results on three
systems and show that using Epochal Difficult Captions consistently improves
performance"
Data augmentation for efficient learning from parametric experts,0.0089734,"We present a simple, yet powerful data-augmentation technique to enable
data-efficient learning from parametric experts for reinforcement and imitation
learning. We focus on what we call the policy cloning setting, in which we use
online or offline queries of an expert or expert policy to inform the behavior
of a student policy. This setting arises naturally in a number of problems, for
instance as variants of behavior cloning, or as a component of other algorithms
such as DAGGER, policy distillation or KL-regularized RL. Our approach,
augmented policy cloning (APC), uses synthetic states to induce
feedback-sensitivity in a region around sampled trajectories, thus dramatically
reducing the environment interactions required for successful cloning of the
expert. We achieve highly data-efficient transfer of behavior from an expert to
a student policy for high-degrees-of-freedom control problems. We demonstrate
the benefit of our method in the context of several existing and widely used
algorithms that include policy cloning as a constituent part. Moreover, we
highlight the benefits of our approach in two practically relevant settings (a)
expert compression, i.e. transfer to a student with fewer parameters; and (b)
transfer from privileged experts, i.e. where the expert has a different
observation space than the student, usually including access to privileged
information."
Generating Explanations from Deep Reinforcement Learning Using Episodic Memory,0.0366845,"Deep Reinforcement Learning (RL) involves the use of Deep Neural Networks
(DNNs) to make sequential decisions in order to maximize reward. For many tasks
the resulting sequence of actions produced by a Deep RL policy can be long and
difficult to understand for humans. A crucial component of human explanations
is selectivity, whereby only key decisions and causes are recounted. Imbuing
Deep RL agents with such an ability would make their resulting policies easier
to understand from a human perspective and generate a concise set of
instructions to aid the learning of future agents. To this end we use a Deep RL
agent with an episodic memory system to identify and recount key decisions
during policy execution. We show that these decisions form a short, human
readable explanation that can also be used to speed up the learning of naive
Deep RL agents in an algorithm-independent manner."
BERT4Loc: BERT for Location -- POI Recommender System,0.0520889,"Recommending points of interest (POIs) is a challenging task that requires
extracting comprehensive location data from location-based social media
platforms. To provide effective location-based recommendations, it's important
to analyze users' historical behavior and preferences. In this study, we
present a sophisticated location-aware recommendation system that uses
Bidirectional Encoder Representations from Transformers (BERT) to offer
personalized location-based suggestions. Our model combines location
information and user preferences to provide more relevant recommendations
compared to models that predict the next POI in a sequence. Our experiments on
two benchmark dataset show that our BERT-based model outperforms various
state-of-the-art sequential models. Moreover, we see the effectiveness of the
proposed model for quality through additional experiments."
Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding,0.00501456,"In recent years, large pre-trained Transformer networks have demonstrated
dramatic improvements in many natural language understanding tasks. However,
the huge size of these models brings significant challenges to their
fine-tuning and online deployment due to latency and cost constraints. New
hardware supporting both N:M semi-structured sparsity and low-precision integer
computation is a promising solution to boost DNN model serving efficiency.
However, there have been very few studies that systematically investigate to
what extent pre-trained Transformer networks benefit from the combination of
these techniques, as well as how to best compress each component of the
Transformer. We propose a flexible compression framework NxMiFormer that
performs simultaneous sparsification and quantization using ADMM and STE-based
QAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm
that identifies promising heterogeneous compression configurations that meet a
compression ratio constraint. When evaluated across the GLUE suite of NLU
benchmarks, our approach can achieve up to 93% compression of the encoders of a
BERT model while retaining 98.2% of the original model accuracy and taking full
advantage of the hardware's capabilities. Heterogeneous configurations found
the by the search heuristic maintain 99.5% of the baseline accuracy while still
compressing the model by 87.5%."
Learning Visual Explanations for DCNN-Based Image Classifiers Using an Attention Mechanism,0.0101659,"In this paper two new learning-based eXplainable AI (XAI) methods for deep
convolutional neural network (DCNN) image classifiers, called L-CAM-Fm and
L-CAM-Img, are proposed. Both methods use an attention mechanism that is
inserted in the original (frozen) DCNN and is trained to derive class
activation maps (CAMs) from the last convolutional layer's feature maps. During
training, CAMs are applied to the feature maps (L-CAM-Fm) or the input image
(L-CAM-Img) forcing the attention mechanism to learn the image regions
explaining the DCNN's outcome. Experimental evaluation on ImageNet shows that
the proposed methods achieve competitive results while requiring a single
forward pass at the inference stage. Moreover, based on the derived
explanations a comprehensive qualitative analysis is performed providing
valuable insight for understanding the reasons behind classification errors,
including possible dataset biases affecting the trained classifier."
In-Vehicle Interface Adaptation to Environment-Induced Cognitive Workload,0.0412657,"Many car accidents are caused by human distractions, including cognitive
distractions. In-vehicle human-machine interfaces (HMIs) have evolved
throughout the years, providing more and more functions. Interaction with the
HMIs can, however, also lead to further distractions and, as a consequence,
accidents. To tackle this problem, we propose using adaptive HMIs that change
according to the mental workload of the driver. In this work, we present the
current status as well as preliminary results of a user study using
naturalistic secondary tasks while driving (i.e., the primary task) that
attempt to understand the effects of one such interface."
P$^3$LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training,0.0108149,"Conventional autoregressive left-to-right (L2R) sequence generation faces two
issues during decoding: limited to unidirectional target sequence modeling, and
constrained on strong local dependencies. To address the aforementioned
problem, we propose P$^3$LM, a probabilistically permuted prophet language
model, which strengthens the modeling of bidirectional information and long
token dependencies for sequence generation. Specifically, P$^3$LM learns to
generate tokens in permuted order upon an order-aware transformer decoder, as
well as to generate the corresponding future $N$ tokens with a multi-stream
attention mechanism. Extensive experiments are conducted on the GLGE benchmark,
which includes four datasets for summarization, two for question generation,
one for conversational question answering, and one for dialog response
generation, where P$^3$LM achieves state-of-the-art results compared with
strong publicly available generative pre-training methods."
Autoregressive GAN for Semantic Unconditional Head Motion Generation,0.00525344,"In this work, we address the task of unconditional head motion generation to
animate still human faces in a low-dimensional semantic space from a single
reference pose. Different from traditional audio-conditioned talking head
generation that seldom puts emphasis on realistic head motions, we devise a
GAN-based architecture that learns to synthesize rich head motion sequences
over long duration while maintaining low error accumulation levels.In
particular, the autoregressive generation of incremental outputs ensures smooth
trajectories, while a multi-scale discriminator on input pairs drives
generation toward better handling of high- and low-frequency signals and less
mode collapse.We experimentally demonstrate the relevance of the proposed
method and show its superiority compared to models that attained
state-of-the-art performances on similar tasks."
Recognition of Implicit Geographic Movement in Text,0.0282515,"Analyzing the geographic movement of humans, animals, and other phenomena is
a growing field of research. This research has benefited urban planning,
logistics, animal migration understanding, and much more. Typically, the
movement is captured as precise geographic coordinates and time stamps with
Global Positioning Systems (GPS). Although some research uses computational
techniques to take advantage of implicit movement in descriptions of route
directions, hiking paths, and historical exploration routes, innovation would
accelerate with a large and diverse corpus. We created a corpus of sentences
labeled as describing geographic movement or not and including the type of
entity moving. Creating this corpus proved difficult without any comparable
corpora to start with, high human labeling costs, and since movement can at
times be interpreted differently. To overcome these challenges, we developed an
iterative process employing hand labeling, crowd voting for confirmation, and
machine learning to predict more labels. By merging advances in word embeddings
with traditional machine learning models and model ensembling, prediction
accuracy is at an acceptable level to produce a large silver-standard corpus
despite the small gold-standard corpus training set. Our corpus will likely
benefit computational processing of geography in text and spatial cognition, in
addition to detection of movement."
Multi-level Latent Space Structuring for Generative Control,0.0072254,"Truncation is widely used in generative models for improving the quality of
the generated samples, at the expense of reducing their diversity. We propose
to leverage the StyleGAN generative architecture to devise a new truncation
technique, based on a decomposition of the latent space into clusters, enabling
customized truncation to be performed at multiple semantic levels. We do so by
learning to re-generate W-space, the extended intermediate latent space of
StyleGAN, using a learnable mixture of Gaussians, while simultaneously training
a classifier to identify, for each latent vector, the cluster that it belongs
to. The resulting truncation scheme is more faithful to the original
untruncated samples and allows a better trade-off between quality and
diversity. We compare our method to other truncation approaches for StyleGAN,
both qualitatively and quantitatively."
"Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks",0.0301609,"In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs, namely
word-consensus-networks, allows the SLU system to improve in comparison to the
1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,
learning from acoustic and text embeddings, obtains performance similar to the
oracle setup, a relative improvement of 17.8% over the 1-best configuration,
being a recommended alternative to overcome the limitations of working with
automatically generated transcripts."
Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability,0.0415954,"Great endeavors have been made to study AI's ability in abstract reasoning,
along with which different versions of RAVEN's progressive matrices (RPM) are
proposed as benchmarks. Previous works give inkling that without sophisticated
design or extra meta-data containing semantic information, neural networks may
still be indecisive in making decisions regarding RPM problems, after
relentless training. Evidenced by thorough experiments and ablation studies, we
showcase that end-to-end neural networks embodied with felicitous inductive
bias, intentionally design or serendipitously match, can solve RPM problems
elegantly, without the augment of any extra meta-data or preferences of any
specific backbone. Our work also reveals that multi-viewpoint with
multi-evaluation is a key learning strategy for successful reasoning. Finally,
potential explanations for the failure of connectionist models in
generalization are provided. We hope that these results will serve as
inspections of AI's ability beyond perception and toward abstract reasoning.
Source code can be found in https://github.com/QinglaiWeiCASIA/RavenSolver."
Incorporating Multi-armed Bandit with Local Search for MaxSAT,0.0120932,"Partial MaxSAT (PMS) and Weighted PMS (WPMS) are two practical
generalizations of the MaxSAT problem. In this paper, we propose a local search
algorithm for these problems, called BandHS, which applies two multi-armed
bandits to guide the search directions when escaping local optima. One bandit
is combined with all the soft clauses to help the algorithm select to satisfy
appropriate soft clauses, and the other bandit with all the literals in hard
clauses to help the algorithm select appropriate literals to satisfy the hard
clauses. These two bandits can improve the algorithm's search ability in both
feasible and infeasible solution spaces. We further propose an initialization
method for (W)PMS that prioritizes both unit and binary clauses when producing
the initial solutions. Extensive experiments demonstrate the excellent
performance and generalization capability of our proposed methods, that greatly
boost the state-of-the-art local search algorithm, SATLike3.0, and the
state-of-the-art SAT-based incomplete solver, NuWLS-c."
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction,0.0315331,"Relation extraction (RE), which has relied on structurally annotated corpora
for model training, has been particularly challenging in low-resource scenarios
and domains. Recent literature has tackled low-resource RE by self-supervised
learning, where the solution involves pretraining the entity pair embedding by
RE-based objective and finetuning on labeled data by classification-based
objective. However, a critical challenge to this approach is the gap in
objectives, which prevents the RE model from fully utilizing the knowledge in
pretrained representations. In this paper, we aim at bridging the gap and
propose to pretrain and finetune the RE model using consistent objectives of
contrastive learning. Since in this kind of representation learning paradigm,
one relation may easily form multiple clusters in the representation space, we
further propose a multi-center contrastive loss that allows one relation to
form multiple clusters to better align with pretraining. Experiments on two
document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness
of our method. Particularly, when using 1% end-task training data, our method
outperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets,
respectively."
Winning the CVPR'2022 AQTC Challenge: A Two-stage Function-centric Approach,0.0251057,"Affordance-centric Question-driven Task Completion for Egocentric
Assistant(AQTC) is a novel task which helps AI assistant learn from
instructional videos and scripts and guide the user step-by-step. In this
paper, we deal with the AQTC via a two-stage Function-centric approach, which
consists of Question2Function Module to ground the question with the related
function and Function2Answer Module to predict the action based on the
historical steps. We evaluated several possible solutions in each module and
obtained significant gains compared to the given baselines. Our code is
available at \url{https://github.com/starsholic/LOVEU-CVPR22-AQTC}."
"""Covid vaccine is against Covid but Oxford vaccine is made at Oxford!"" Semantic Interpretation of Proper Noun Compounds",0.0241587,"Proper noun compounds, e.g., ""Covid vaccine"", convey information in a
succinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against the
Covid disease""). These are commonly used in short-form domains, such as news
headlines, but are largely ignored in information-seeking applications. To
address this limitation, we release a new manually annotated dataset, ProNCI,
consisting of 22.5K proper noun compounds along with their free-form semantic
interpretations. ProNCI is 60 times larger than prior noun compound datasets
and also includes non-compositional examples, which have not been previously
explored. We experiment with various neural models for automatically generating
the semantic interpretations from proper noun compounds, ranging from few-shot
prompting to supervised learning, with varying degrees of knowledge about the
constituent nouns. We find that adding targeted knowledge, particularly about
the common noun, results in performance gains of upto 2.8%. Finally, we
integrate our model generated interpretations with an existing Open IE system
and observe an 7.5% increase in yield at a precision of 85%. The dataset and
code are available at https://github.com/dair-iitd/pronci."
Bias-Scalable Near-Memory CMOS Analog Processor for Machine Learning,0.0356846,"Bias-scalable analog computing is attractive for implementing machine
learning (ML) processors with distinct power-performance specifications. For
instance, ML implementations for server workloads are focused on higher
computational throughput for faster training, whereas ML implementations for
edge devices are focused on energy-efficient inference. In this paper, we
demonstrate the implementation of bias-scalable approximate analog computing
circuits using the generalization of the margin-propagation principle called
shape-based analog computing (S-AC). The resulting S-AC core integrates several
near-memory compute elements, which include: (a) non-linear activation
functions; (b) inner-product compute circuits; and (c) a mixed-signal
compressive memory, all of which can be scaled for performance or power while
preserving its functionality. Using measured results from prototypes fabricated
in a 180nm CMOS process, we demonstrate that the performance of computing
modules remains robust to transistor biasing and variations in temperature. In
this paper, we also demonstrate the effect of bias-scalability and
computational accuracy on a simple ML regression task."
Socially Intelligent Genetic Agents for the Emergence of Explicit Norms,0.0441994,"Norms help regulate a society. Norms may be explicit (represented in
structured form) or implicit. We address the emergence of explicit norms by
developing agents who provide and reason about explanations for norm violations
in deciding sanctions and identifying alternative norms. These agents use a
genetic algorithm to produce norms and reinforcement learning to learn the
values of these norms. We find that applying explanations leads to norms that
provide better cohesion and goal satisfaction for the agents. Our results are
stable for societies with differing attitudes of generosity."
Visual Information Guided Zero-Shot Paraphrase Generation,0.0211587,"Zero-shot paraphrase generation has drawn much attention as the large-scale
high-quality paraphrase corpus is limited. Back-translation, also known as the
pivot-based method, is typical to this end. Several works leverage different
information as ""pivot"" such as language, semantic representation and so on. In
this paper, we explore using visual information such as image as the ""pivot"" of
back-translation. Different with the pipeline back-translation method, we
propose visual information guided zero-shot paraphrase generation (ViPG) based
only on paired image-caption data. It jointly trains an image captioning model
and a paraphrasing model and leverage the image captioning model to guide the
training of the paraphrasing model. Both automatic evaluation and human
evaluation show our model can generate paraphrase with good relevancy, fluency
and diversity, and image is a promising kind of pivot for zero-shot paraphrase
generation."
RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments,0.0210986,"As a fundamental task for intelligent robots, visual SLAM has made great
progress over the past decades. However, robust SLAM under highly weak-textured
environments still remains very challenging. In this paper, we propose a novel
visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR
network which is able to produce dense point matching under low-textured scenes
to generate feature descriptors. To integrate the new features into the popular
ORB-SLAM framework, we develop feature masks to filter out the unreliable
features and employ KNN strategy to strengthen the matching robustness. We also
retrained visual vocabulary upon new descriptors for efficient loop closing.
The resulting RWT-SLAM is tested in various public datasets such as TUM and
OpenLORIS, as well as our own data. The results shows very promising
performance under highly weak-textured environments."
Analytical Solutions for the Inverse Problem within Gradual Semantics,0.0492012,"Gradual semantics within abstract argumentation associate a numeric score
with every argument in a system, which represents the level of acceptability of
this argument, and from which a preference ordering over arguments can be
derived. While some semantics operate over standard argumentation frameworks,
many utilise a weighted framework, where a numeric initial weight is associated
with each argument. Recent work has examined the inverse problem within gradual
semantics. Rather than determining a preference ordering given an argumentation
framework and a semantics, the inverse problem takes an argumentation
framework, a gradual semantics, and a preference ordering as inputs, and
identifies what weights are needed to over arguments in the framework to obtain
the desired preference ordering. Existing work has attacked the inverse problem
numerically, using a root finding algorithm (the bisection method) to identify
appropriate initial weights. In this paper we demonstrate that for a class of
gradual semantics, an analytical approach can be used to solve the inverse
problem. Unlike the current state-of-the-art, such an analytic approach can
rapidly find a solution, and is guaranteed to do so. In obtaining this result,
we are able to prove several important properties which previous work had posed
as conjectures."
Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding,0.0328939,"Pre-trained language models (PLM) have demonstrated their effectiveness for a
broad range of information retrieval and natural language processing tasks. As
the core part of PLM, multi-head self-attention is appealing for its ability to
jointly attend to information from different positions. However, researchers
have found that PLM always exhibits fixed attention patterns regardless of the
input (e.g., excessively paying attention to [CLS] or [SEP]), which we argue
might neglect important information in the other positions. In this work, we
propose a simple yet effective attention guiding mechanism to improve the
performance of PLM by encouraging attention towards the established goals.
Specifically, we propose two kinds of attention guiding methods, i.e., map
discrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).
The former definitely encourages the diversity among multiple self-attention
heads to jointly attend to information from different representation subspaces,
while the latter encourages self-attention to attend to as many different
positions of the input as possible. We conduct experiments with multiple
general pre-trained models (i.e., BERT, ALBERT, and Roberta) and
domain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and
SciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and
Cross-genre-IR). Extensive experimental results demonstrate that our proposed
MDG and PDG bring stable performance improvements on all datasets with high
efficiency and low cost."
Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem,0.0825292,"Math word problem solver requires both precise relation reasoning about
quantities in the text and reliable generation for the diverse equation.
Current sequence-to-tree or relation extraction methods regard this only from a
fixed view, struggling to simultaneously handle complex semantics and diverse
equations. However, human solving naturally involves two consistent reasoning
views: top-down and bottom-up, just as math equations also can be expressed in
multiple equivalent forms: pre-order and post-order. We propose a multi-view
consistent contrastive learning for a more complete semantics-to-equation
mapping. The entire process is decoupled into two independent but consistent
views: top-down decomposition and bottom-up construction, and the two reasoning
views are aligned in multi-granularity for consistency, enhancing global
generation and precise reasoning. Experiments on multiple datasets across two
languages show our approach significantly outperforms the existing baselines,
especially on complex problems. We also show after consistent alignment,
multi-view can absorb the merits of both views and generate more diverse
results consistent with the mathematical laws."
Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting,0.100494,"This paper presents work on novel machine translation (MT) systems between
spoken and signed languages, where signed languages are represented in
SignWriting, a sign language writing system. Our work seeks to address the lack
of out-of-the-box support for signed languages in current MT systems and is
based on the SignBank dataset, which contains pairs of spoken language text and
SignWriting content. We introduce novel methods to parse, factorize, decode,
and evaluate SignWriting, leveraging ideas from neural factored MT. In a
bilingual setup--translating from American Sign Language to (American)
English--our method achieves over 30 BLEU, while in two multilingual
setups--translating in both directions between spoken languages and signed
languages--we achieve over 20 BLEU. We find that common MT techniques used to
improve spoken language translation similarly affect the performance of sign
language translation. These findings validate our use of an intermediate text
representation for signed languages to include them in natural language
processing research."
Semiconductor Defect Pattern Classification by Self-Proliferation-and-Attention Neural Network,0.0740846,"Semiconductor manufacturing is on the cusp of a revolution: the Internet of
Things (IoT). With IoT we can connect all the equipment and feed information
back to the factory so that quality issues can be detected. In this situation,
more and more edge devices are used in wafer inspection equipment. This edge
device must have the ability to quickly detect defects. Therefore, how to
develop a high-efficiency architecture for automatic defect classification to
be suitable for edge devices is the primary task. In this paper, we present a
novel architecture that can perform defect classification in a more efficient
way. The first function is self-proliferation, using a series of linear
transformations to generate more feature maps at a cheaper cost. The second
function is self-attention, capturing the long-range dependencies of feature
map by the channel-wise and spatial-wise attention mechanism. We named this
method as self-proliferation-and-attention neural network. This method has been
successfully applied to various defect pattern classification tasks. Compared
with other latest methods, SP&A-Net has higher accuracy and lower computation
cost in many defect inspection tasks."
Leaf: Multiple-Choice Question Generation,0.0700979,"Testing with quiz questions has proven to be an effective way to assess and
improve the educational process. However, manually creating quizzes is tedious
and time-consuming. To address this challenge, we present Leaf, a system for
generating multiple-choice questions from factual text. In addition to being
very well suited for the classroom, Leaf could also be used in an industrial
setting, e.g., to facilitate onboarding and knowledge sharing, or as a
component of chatbots, question answering systems, or Massive Open Online
Courses (MOOCs). The code and the demo are available on
https://github.com/KristiyanVachev/Leaf-Question-Generation."
Data-driven prediction of Air Traffic Controllers reactions to resolving conflicts,0.100555,"With the aim to enhance automation in conflict detection and resolution
(CD&R) tasks in the Air Traffic Management domain, in this paper we propose
deep learning techniques (DL) that can learn models of Air Traffic Controllers'
(ATCO) reactions in resolving conflicts that can violate separation minimum
constraints among aircraft trajectories: This implies learning when the ATCO
will react towards resolving a conflict, and how he/she will react. Timely
reactions, to which this paper aims, focus on when do reactions happen, aiming
to predict the trajectory points, as the trajectory evolves, that the ATCO
issues a conflict resolution action, while also predicting the type of
resolution action (if any). Towards this goal, the paper formulates the ATCO
reactions prediction problem for CD&R, and presents DL methods that can model
ATCO timely reactions and evaluates these methods in real-world data sets,
showing their efficacy in prediction with very high accuracy."
The Neural Race Reduction: Dynamics of Abstraction in Gated Networks,0.0579237,"Our theoretical understanding of deep learning has not kept pace with its
empirical success. While network architecture is known to be critical, we do
not yet understand its effect on learned representations and network behavior,
or how this architecture should reflect task structure.In this work, we begin
to address this gap by introducing the Gated Deep Linear Network framework that
schematizes how pathways of information flow impact learning dynamics within an
architecture. Crucially, because of the gating, these networks can compute
nonlinear functions of their input. We derive an exact reduction and, for
certain cases, exact solutions to the dynamics of learning. Our analysis
demonstrates that the learning dynamics in structured networks can be
conceptualized as a neural race with an implicit bias towards shared
representations, which then govern the model's ability to systematically
generalize, multi-task, and transfer. We validate our key insights on
naturalistic datasets and with relaxed assumptions. Taken together, our work
gives rise to general hypotheses relating neural architecture to learning and
provides a mathematical approach towards understanding the design of more
complex architectures and the role of modularity and compositionality in
solving real-world problems. The code and results are available at
https://www.saxelab.org/gated-dln ."
A Deep-Discrete Learning Framework for Spherical Surface Registration,0.0584814,"Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology."
Dynamic Point Cloud Compression with Cross-Sectional Approach,0.063056,"The recent development of dynamic point clouds has introduced the possibility
of mimicking natural reality, and greatly assisting quality of life. However,
to broadcast successfully, the dynamic point clouds require higher compression
due to their huge volume of data compared to the traditional video. Recently,
MPEG finalized a Video-based Point Cloud Compression standard known as V-PCC.
However, V-PCC requires huge computational time due to expensive normal
calculation and segmentation, sacrifices some points to limit the number of 2D
patches, and cannot occupy all spaces in the 2D frame. The proposed method
addresses these limitations by using a novel cross-sectional approach. This
approach reduces expensive normal estimation and segmentation, retains more
points, and utilizes more spaces for 2D frame generation compared to the VPCC.
The experimental results using standard video sequences show that the proposed
technique can achieve better compression in both geometric and texture data
compared to the V-PCC standard."
Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages,0.0553044,"Translation Quality Estimation (QE) is the task of predicting the quality of
machine translation (MT) output without any reference. This task has gained
increasing attention as an important component in the practical applications of
MT. In this paper, we first propose XLMRScore, which is a cross-lingual
counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric
can be used as a simple unsupervised QE method, nevertheless facing two issues:
firstly, the untranslated tokens leading to unexpectedly high translation
scores, and secondly, the issue of mismatching errors between source and
hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate
these issues, we suggest replacing untranslated words with the unknown token
and the cross-lingual alignment of the pre-trained model to represent aligned
words closer to each other, respectively. We evaluate the proposed method on
four low-resource language pairs of the WMT21 QE shared task, as well as a new
English$\rightarrow$Persian (En-Fa) test dataset introduced in this paper.
Experiments show that our method could get comparable results with the
supervised baseline for two zero-shot scenarios, i.e., with less than 0.01
difference in Pearson correlation, while outperforming unsupervised rivals in
all the low-resource language pairs for above 8%, on average."
Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals,0.05609,"Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps."
Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance,0.0812181,"This paper presents a novel end-to-end method for the problem of
skeleton-based unsupervised human action recognition. We propose a new
architecture with a convolutional autoencoder that uses graph Laplacian
regularization to model the skeletal geometry across the temporal dynamics of
actions. Our approach is robust towards viewpoint variations by including a
self-supervised gradient reverse layer that ensures generalization across
camera views. The proposed method is validated on NTU-60 and NTU-120
large-scale datasets in which it outperforms all prior unsupervised
skeleton-based approaches on the cross-subject, cross-view, and cross-setup
protocols. Although unsupervised, our learnable representation allows our
method even to surpass a few supervised skeleton-based action recognition
methods. The code is available in:
www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian"
Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot Segmentation,0.0545921,"Few-shot learning allows machines to classify novel classes using only a few
labeled samples. Recently, few-shot segmentation aiming at semantic
segmentation on low sample data has also seen great interest. In this paper, we
propose a learnable module that can be placed on top of existing segmentation
networks for performing few-shot segmentation. This module, called the
task-adaptive feature transformer (TAFT), linearly transforms task-specific
high-level features to a set of task agnostic features well-suited to
conducting few-shot segmentation. The task-conditioned feature transformation
allows an effective utilization of the semantic information in novel classes to
generate tight segmentation masks. We also propose a semantic enrichment (SE)
module that utilizes a pixel-wise attention module for high-level feature and
an auxiliary loss from an auxiliary segmentation network conducting the
semantic segmentation for all training classes. Experiments on PASCAL-$5^i$ and
COCO-$20^i$ datasets confirm that the added modules successfully extend the
capability of existing segmentators to yield highly competitive few-shot
segmentation performances."
kpfriends at SemEval-2022 Task 2: NEAMER -- Named Entity Augmented Multi-word Expression Recognizer,0.0952839,"We present NEAMER -- Named Entity Augmented Multi-word Expression Recognizer.
This system is inspired by non-compositionality characteristics shared between
Named Entity and Idiomatic Expressions. We utilize transfer learning and
locality features to enhance idiom classification task. This system is our
submission for SemEval Task 2: Multilingual Idiomaticity Detection and Sentence
Embedding Subtask A OneShot shared task. We achieve SOTA with F1 0.9395 during
post-evaluation phase. We also observe improvement in training stability.
Lastly, we experiment with non-compositionality knowledge transfer,
cross-lingual fine-tuning and locality features, which we also introduce in
this paper."
Deep Model-Based Super-Resolution with Non-uniform Blur,0.055798,"We propose a state-of-the-art method for super-resolution with non-uniform
blur. Single-image super-resolution methods seek to restore a high-resolution
image from blurred, subsampled, and noisy measurements. Despite their
impressive performance, existing techniques usually assume a uniform blur
kernel. Hence, these techniques do not generalize well to the more general case
of non-uniform blur. Instead, in this paper, we address the more realistic and
computationally challenging case of spatially-varying blur. To this end, we
first propose a fast deep plug-and-play algorithm, based on linearized ADMM
splitting techniques, which can solve the super-resolution problem with
spatially-varying blur. Second, we unfold our iterative algorithm into a single
network and train it end-to-end. In this way, we overcome the intricacy of
manually tuning the parameters involved in the optimization scheme. Our
algorithm presents remarkable performance and generalizes well after a single
training to a large family of spatially-varying blur kernels, noise levels and
scale factors."
AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation,0.0600174,"As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties, the
capabilities of detecting novelties, characterizing them,
accommodating/adapting to them, gathering ground-truth training data and
incrementally learning the unknowns/novelties become critical in making the AI
agent more and more knowledgeable, powerful and self-sustainable over time. The
key challenge here is how to automate the process so that it is carried out
continually on the agent's own initiative and through its own interactions with
humans, other agents and the environment just like human on-the-job learning.
This paper proposes a framework (called SOLA) for this learning paradigm to
promote the research of building autonomous and continual learning enabled AI
agents. To show feasibility, an implemented agent is also described."
Controlling Bias Exposure for Fair Interpretable Predictions,0.101849,"Recent work on reducing bias in NLP models usually focuses on protecting or
isolating information related to a sensitive attribute (like gender or race).
However, when sensitive information is semantically entangled with the task
information of the input, e.g., gender information is predictive for a
profession, a fair trade-off between task performance and bias mitigation is
difficult to achieve. Existing approaches perform this trade-off by eliminating
bias information from the latent space, lacking control over how much bias is
necessarily required to be removed. We argue that a favorable debiasing method
should use sensitive information 'fairly', rather than blindly eliminating it
(Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we
provide a novel debiasing algorithm by adjusting the predictive model's belief
to (1) ignore the sensitive information if it is not useful for the task; (2)
use sensitive information minimally as necessary for the prediction (while also
incurring a penalty). Experimental results on two text classification tasks
(influenced by gender) and an open-ended generation task (influenced by race)
indicate that our model achieves a desirable trade-off between debiasing and
task performance along with producing debiased rationales as evidence."
Mathematical Cookbook for Snapshot Compressive Imaging,0.0734141,"The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction, conventional optimization, and deep
equilibrium models. The latest releases are strongly recommended! For any other
questions, suggestions, or comments, feel free to email the author."
InducT-GCN: Inductive Graph Convolutional Networks for Text Classification,0.0701222,"Text classification aims to assign labels to textual units by making use of
global information. Recent studies have applied graph neural network (GNN) to
capture the global word co-occurrence in a corpus. Existing approaches require
that all the nodes (training and test) in a graph are present during training,
which are transductive and do not naturally generalise to unseen nodes. To make
those models inductive, they use extra resources, like pretrained word
embedding. However, high-quality resource is not always available and hard to
train. Under the extreme settings with no extra resource and limited amount of
training set, can we still learn an inductive graph-based text classification
model? In this paper, we introduce a novel inductive graph-based text
classification framework, InducT-GCN (InducTive Graph Convolutional Networks
for Text classification). Compared to transductive models that require test
documents in training, we construct a graph based on the statistics of training
documents only and represent document vectors with a weighted sum of word
vectors. We then conduct one-directional GCN propagation during testing. Across
five text classification benchmarks, our InducT-GCN outperformed
state-of-the-art methods that are either transductive in nature or pre-trained
additional resources. We also conducted scalability testing by gradually
increasing the data size and revealed that our InducT-GCN can reduce the time
and space complexity. The code is available on:
https://github.com/usydnlp/InductTGCN."
Dual Progressive Transformations for Weakly Supervised Semantic Segmentation,0.0930372,"Weakly supervised semantic segmentation (WSSS), which aims to mine the object
regions by merely using class-level labels, is a challenging task in computer
vision. The current state-of-the-art CNN-based methods usually adopt
Class-Activation-Maps (CAMs) to highlight the potential areas of the object,
however, they may suffer from the part-activated issues. To this end, we try an
early attempt to explore the global feature attention mechanism of vision
transformer in WSSS task. However, since the transformer lacks the inductive
bias as in CNN models, it can not boost the performance directly and may yield
the over-activated problems. To tackle these drawbacks, we propose a
Convolutional Neural Networks Refined Transformer (CRT) to mine a globally
complete and locally accurate class activation maps in this paper. To validate
the effectiveness of our proposed method, extensive experiments are conducted
on PASCAL VOC 2012 and CUB-200-2011 datasets. Experimental evaluations show
that our proposed CRT achieves the new state-of-the-art performance on both the
weakly supervised semantic segmentation task the weakly supervised object
localization task, which outperform others by a large margin."
I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning,0.0633187,"Knowledge graph (KG) embedding seeks to learn vector representations for
entities and relations. Conventional models reason over graph structures, but
they suffer from the issues of graph incompleteness and long-tail entities.
Recent studies have used pre-trained language models to learn embeddings based
on the textual information of entities and relations, but they cannot take
advantage of graph structures. In the paper, we show empirically that these two
kinds of features are complementary for KG embedding. To this end, we propose
CoLE, a Co-distillation Learning method for KG Embedding that exploits the
complementarity of graph structures and text information. Its graph embedding
model employs Transformer to reconstruct the representation of an entity from
its neighborhood subgraph. Its text embedding model uses a pre-trained language
model to generate entity representations from the soft prompts of their names,
descriptions, and relational neighbors. To let the two model promote each
other, we propose co-distillation learning that allows them to distill
selective knowledge from each other's prediction logits. In our co-distillation
learning, each model serves as both a teacher and a student. Experiments on
benchmark datasets demonstrate that the two models outperform their related
baselines, and the ensemble method CoLE with co-distillation learning advances
the state-of-the-art of KG embedding."
Unpaired Image Translation via Vector Symbolic Architectures,0.0839373,"Image-to-image translation has played an important role in enabling synthetic
data for computer vision. However, if the source and target domains have a
large semantic mismatch, existing techniques often suffer from source content
corruption aka semantic flipping. To address this problem, we propose a new
paradigm for image-to-image translation using Vector Symbolic Architectures
(VSA), a theoretical framework which defines algebraic operations in a
high-dimensional vector (hypervector) space. We introduce VSA-based constraints
on adversarial learning for source-to-target translations by learning a
hypervector mapping that inverts the translation to ensure consistency with
source content. We show both qualitatively and quantitatively that our method
improves over other state-of-the-art techniques."
Exploration of Machine Learning Classification Models Used for Behavioral Biometrics Authentication,0.0927617,"Mobile devices have been manufactured and enhanced at growing rates in the
past decades. While this growth has significantly evolved the capability of
these devices, their security has been falling behind. This contrast in
development between capability and security of mobile devices is a significant
problem with the sensitive information of the public at risk. Continuing the
previous work in this field, this study identifies key Machine Learning
algorithms currently being used for behavioral biometric mobile authentication
schemes and aims to provide a comprehensive review of these algorithms when
used with touch dynamics and phone movement. Throughout this paper the
benefits, limitations, and recommendations for future work will be discussed."
"From Perception to Programs: Regularize, Overparameterize, and Amortize",0.0758003,"Toward combining inductive reasoning with perception abilities, we develop
techniques for neurosymbolic program synthesis where perceptual input is first
parsed by neural nets into a low-dimensional interpretable representation,
which is then processed by a synthesized program. We explore several techniques
for relaxing the problem and jointly learning all modules end-to-end with
gradient descent: multitask learning; amortized inference;
overparameterization; and a differentiable strategy for penalizing lengthy
programs. Collectedly this toolbox improves the stability of gradient-guided
program search, and suggests ways of learning both how to perceive input as
discrete abstractions, and how to symbolically process those abstractions as
programs."
Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning,0.0929713,"Controlled automated story generation seeks to generate natural language
stories satisfying constraints from natural language critiques or preferences.
Existing methods to control for story preference utilize prompt engineering
which is labor intensive and often inconsistent. They may also use
logit-manipulation methods which require annotated datasets to exist for the
desired attributes. To address these issues, we first train a contrastive
bi-encoder model to align stories with corresponding human critiques, named
CARP, building a general purpose preference model. This is subsequently used as
a reward function to fine-tune a generative language model via reinforcement
learning. However, simply fine-tuning a generative language model with a
contrastive reward model does not always reliably result in a story generation
system capable of generating stories that meet user preferences. To increase
story generation robustness we further fine-tune the contrastive reward model
using a prompt-learning technique. A human participant study is then conducted
comparing generations from our full system, ablations, and two baselines. We
show that the full fine-tuning pipeline results in a story generator preferred
over a LLM 20x as large as well as logit-based methods. This motivates the use
of contrastive learning for general purpose human preference modeling."
Planning Landscape Analysis for Self-Adaptive Systems,0.0877092,"To assure performance on the fly, planning is arguably one of the most
important steps for self-adaptive systems (SASs), especially when they are
highly configurable with a daunting number of adaptation options. However,
there has been little understanding of the planning landscape or ways by which
it can be analyzed. This inevitably creates barriers to the design of better
and tailored planners for SASs. In this paper, we showcase how the planning
landscapes of SASs can be quantified and reasoned, particularly with respect to
the different environments. By studying four diverse real-world SASs and 14
environments, we found that (1) the SAS planning landscapes often provide
strong guidance to the planner, but their ruggedness and multi-modality can be
the major obstacle; (2) the extents of guidance and number of global/local
optima are sensitive to the changing environment, but not the ruggedness of the
surface; (3) the local optima are often closer to the global optimum than other
random points; and (4) there are considerable (and useful) overlaps on the
global/local optima between landscapes under different environments. We then
discuss the potential implications to the future work of planner designs for
SASs."
Mono-surrogate vs Multi-surrogate in Multi-objective Bayesian Optimisation,0.0741982,"Bayesian optimisation (BO) has been widely used to solve problems with
expensive function evaluations. In multi-objective optimisation problems, BO
aims to find a set of approximated Pareto optimal solutions. There are
typically two ways to build surrogates in multi-objective BO: One surrogate by
aggregating objective functions (by using a scalarising function, also called
mono-surrogate approach) and multiple surrogates (for each objective function,
also called multi-surrogate approach). In both approaches, an acquisition
function (AF) is used to guide the search process. Mono-surrogate has the
advantage that only one model is used, however, the approach has two major
limitations. Firstly, the fitness landscape of the scalarising function and the
objective functions may not be similar. Secondly, the approach assumes that the
scalarising function distribution is Gaussian, and thus a closed-form
expression of the AF can be used. In this work, we overcome these limitations
by building a surrogate model for each objective function and show that the
scalarising function distribution is not Gaussian. We approximate the
distribution using Generalised extreme value distribution. The results and
comparison with existing approaches on standard benchmark and real-world
optimisation problems show the potential of the multi-surrogate approach."
CREATER: CTR-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning,0.0621283,"This paper focuses on automatically generating the text of an ad, and the
goal is that the generated text can capture user interest for achieving higher
click-through rate (CTR). We propose CREATER, a CTR-driven advertising text
generation approach, to generate ad texts based on high-quality user reviews.
To incorporate CTR objective, our model learns from online A/B test data with
contrastive learning, which encourages the model to generate ad texts that
obtain higher CTR. To alleviate the low-resource issue, we design a customized
self-supervised objective reducing the gap between pre-training and
fine-tuning. Experiments on industrial datasets show that CREATER significantly
outperforms current approaches. It has been deployed online in a leading
advertising platform and brings uplift on core online metrics."
A Reference Data Model for Process-Related User Interaction Logs,0.1161,"User interaction (UI) logs are high-resolution event logs that record
low-level activities performed by a user during the execution of a task in an
information system. Each event in a UI log corresponds to a single interaction
between the user and the interface, such as clicking a button or entering a
string into a text field. UI logs are used for purposes like task mining or
robotic process automation (RPA), but each study and tool relies on a different
conceptualization and implementation of the elements and attributes that
constitute user interactions. This lack of standardization makes it difficult
to integrate UI logs from different sources and to combine tools for UI data
collection with downstream analytics or automation solutions. To address this,
we propose a universally applicable reference data model for process-related UI
logs. Based on a review of scientific literature and industry solutions, this
model includes the core attributes of UI logs, but remains flexible with regard
to the scope, level of abstraction, and case notion. We provide an
implementation of the model as an extension to the XES interchange standard for
event logs and demonstrate its practical applicability in a real-life RPA
scenario."
Deep Vehicle Detection in Satellite Video,0.129491,"This work presents a deep learning approach for vehicle detection in
satellite video. Vehicle detection is perhaps impossible in single EO satellite
images due to the tininess of vehicles (4-10 pixel) and their similarity to the
background. Instead, we consider satellite video which overcomes the lack of
spatial information by temporal consistency of vehicle movement. A new
spatiotemporal model of a compact $3 \times 3$ convolutional, neural network is
proposed which neglects pooling layers and uses leaky ReLUs. Then we use a
reformulation of the output heatmap including Non-Maximum-Suppression (NMS) for
the final segmentation. Empirical results on two new annotated satellite videos
reconfirm the applicability of this approach for vehicle detection. They more
importantly indicate that pre-training on WAMI data and then fine-tuning on few
annotated video frames for a new video is sufficient. In our experiment only
five annotated images yield a $F_1$ score of 0.81 on a new video showing more
complex traffic patterns than the Las Vegas video. Our best result on Las Vegas
is a $F_1$ score of 0.87 which makes the proposed approach a leading method for
this benchmark."
Towards Robust k-Nearest-Neighbor Machine Translation,0.11967,"k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research
direction of NMT in recent years. Its main idea is to retrieve useful key-value
pairs from an additional datastore to modify translations without updating the
NMT model. However, the underlying retrieved noisy pairs will dramatically
deteriorate the model performance. In this paper, we conduct a preliminary
study and find that this problem results from not fully exploiting the
prediction of the NMT model. To alleviate the impact of noise, we propose a
confidence-enhanced kNN-MT model with robust training. Concretely, we introduce
the NMT confidence to refine the modeling of two important components of
kNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two
types of perturbations into the retrieved pairs for robust training.
Experimental results on four benchmark datasets demonstrate that our model not
only achieves significant improvements over current kNN-MT models, but also
exhibits better robustness. Our code is available at
https://github.com/DeepLearnXMU/Robust-knn-mt."
Exploiting Global and Local Hierarchies for Hierarchical Text Classification,0.128213,"Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets."
3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization,0.156893,"Although deep-learning based methods for monocular pedestrian detection have
made great progress, they are still vulnerable to heavy occlusions. Using
multi-view information fusion is a potential solution but has limited
applications, due to the lack of annotated training samples in existing
multi-view datasets, which increases the risk of overfitting. To address this
problem, a data augmentation method is proposed to randomly generate 3D
cylinder occlusions, on the ground plane, which are of the average size of
pedestrians and projected to multiple views, to relieve the impact of
overfitting in the training. Moreover, the feature map of each view is
projected to multiple parallel planes at different heights, by using
homographies, which allows the CNNs to fully utilize the features across the
height of each pedestrian to infer the locations of pedestrians on the ground
plane. The proposed 3DROM method has a greatly improved performance in
comparison with the state-of-the-art deep-learning based methods for multi-view
pedestrian detection."
Non-Autoregressive Machine Translation: It's Not as Fast as it Seems,0.140265,"Efficient machine translation models are commercially important as they can
increase inference speeds, and reduce costs and carbon emissions. Recently,
there has been much interest in non-autoregressive (NAR) models, which promise
faster translation. In parallel to the research on NAR models, there have been
successful attempts to create optimized autoregressive models as part of the
WMT shared task on efficient translation. In this paper, we point out flaws in
the evaluation methodology present in the literature on NAR models and we
provide a fair comparison between a state-of-the-art NAR model and the
autoregressive submissions to the shared task. We make the case for consistent
evaluation of NAR models, and also for the importance of comparing NAR models
with other widely used methods for improving efficiency. We run experiments
with a connectionist-temporal-classification-based (CTC) NAR model implemented
in C++ and compare it with AR models using wall clock times. Our results show
that, although NAR models are faster on GPUs, with small batch sizes, they are
almost always slower under more realistic usage conditions. We call for more
realistic and extensive evaluation of NAR models in future work."
Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection,0.139308,"Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of
aspect-based sentiment analysis, which aims to detect aspect categories
accurately with limited training instances. Recently, dominant works use the
prototypical network to accomplish this task, and employ the attention
mechanism to extract keywords of aspect category from the sentences to produce
the prototype for each aspect. However, they still suffer from serious noise
problems: (1) due to lack of sufficient supervised data, the previous methods
easily catch noisy words irrelevant to the current aspect category, which
largely affects the quality of the generated prototype; (2) the
semantically-close aspect categories usually generate similar prototypes, which
are mutually noisy and confuse the classifier seriously. In this paper, we
resort to the label information of each aspect to tackle the above problems,
along with proposing a novel Label-Driven Denoising Framework (LDF). Extensive
experimental results show that our framework achieves better performance than
other state-of-the-art methods."
Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations,0.132106,"We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh."
nerf2nerf: Pairwise Registration of Neural Radiance Fields,0.132844,"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io"
Towards customizable reinforcement learning agents: Enabling preference specification through online vocabulary expansion,0.138389,"There is a growing interest in developing automated agents that can work
alongside humans. In addition to completing the assigned task, such an agent
will undoubtedly be expected to behave in a manner that is preferred by the
human. This requires the human to communicate their preferences to the agent.
To achieve this, the current approaches either require the users to specify the
reward function or the preference is interactively learned from queries that
ask the user to compare behavior. The former approach can be challenging if the
internal representation used by the agent is inscrutable to the human while the
latter is unnecessarily cumbersome for the user if their preference can be
specified more easily in symbolic terms. In this work, we propose PRESCA
(PREference Specification through Concept Acquisition), a system that allows
users to specify their preferences in terms of concepts that they understand.
PRESCA maintains a set of such concepts in a shared vocabulary. If the relevant
concept is not in the shared vocabulary, then it is learned. To make learning a
new concept more feedback efficient, PRESCA leverages causal associations
between the target concept and concepts that are already known. In addition, we
use a novel data augmentation approach to further reduce required feedback. We
evaluate PRESCA by using it on a Minecraft environment and show that it can
effectively align the agent with the user's preference."
Rethinking Audio-visual Synchronization for Active Speaker Detection,0.106856,"Active speaker detection (ASD) systems are important modules for analyzing
multi-talker conversations. They aim to detect which speakers or none are
talking in a visual scene at any given time. Existing research on ASD does not
agree on the definition of active speakers. We clarify the definition in this
work and require synchronization between the audio and visual speaking
activities. This clarification of definition is motivated by our extensive
experiments, through which we discover that existing ASD methods fail in
modeling the audio-visual synchronization and often classify unsynchronized
videos as active speaking. To address this problem, we propose a cross-modal
contrastive learning strategy and apply positional encoding in attention
modules for supervised ASD models to leverage the synchronization cue.
Experimental results suggest that our model can successfully detect
unsynchronized speaking as not speaking, addressing the limitation of current
models."
PaCo: Parameter-Compositional Multi-Task Reinforcement Learning,0.127767,"The purpose of multi-task reinforcement learning (MTRL) is to train a single
policy that can be applied to a set of different tasks. Sharing parameters
allows us to take advantage of the similarities among tasks. However, the gaps
between contents and difficulties of different tasks bring us challenges on
both which tasks should share the parameters and what parameters should be
shared, as well as the optimization challenges due to parameter sharing. In
this work, we introduce a parameter-compositional approach (PaCo) as an attempt
to address these challenges. In this framework, a policy subspace represented
by a set of parameters is learned. Policies for all the single tasks lie in
this subspace and can be composed by interpolating with the learned set. It
allows not only flexible parameter sharing but also a natural way to improve
training. We demonstrate the state-of-the-art performance on Meta-World
benchmarks, verifying the effectiveness of the proposed approach."
Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma Segmentation and Koos Grade Prediction based on Semi-Supervised Contrastive Learning,0.121909,"Domain adaptation has been widely adopted to transfer styles across
multi-vendors and multi-centers, as well as to complement the missing
modalities. In this challenge, we proposed an unsupervised domain adaptation
framework for cross-modality vestibular schwannoma (VS) and cochlea
segmentation and Koos grade prediction. We learn the shared representation from
both ceT1 and hrT2 images and recover another modality from the latent
representation, and we also utilize proxy tasks of VS segmentation and brain
parcellation to restrict the consistency of image structures in domain
adaptation. After generating missing modalities, the nnU-Net model is utilized
for VS and cochlea segmentation, while a semi-supervised contrastive learning
pre-train approach is employed to improve the model performance for Koos grade
prediction. On CrossMoDA validation phase Leaderboard, our method received rank
4 in task1 with a mean Dice score of 0.8394 and rank 2 in task2 with
Macro-Average Mean Square Error of 0.3941. Our code is available at
https://github.com/fiy2W/cmda2022.superpolymerization."
Implicit Two-Tower Policies,0.106533,"We present a new class of structured reinforcement learning
policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are
chosen based on the attention scores of their learnable latent representations
with those of the input states. By explicitly disentangling action from state
processing in the policy stack, we achieve two main goals: substantial
computational gains and better performance. Our architectures are compatible
with both: discrete and continuous action spaces. By conducting tests on 15
environments from OpenAI Gym and DeepMind Control Suite, we show that
ITT-architectures are particularly suited for blackbox/evolutionary
optimization and the corresponding policy training algorithms outperform their
vanilla unstructured implicit counterparts as well as commonly used explicit
policies. We complement our analysis by showing how techniques such as hashing
and lazy tower updates, critically relying on the two-tower structure of ITTs,
can be applied to obtain additional computational improvements."
Automatic Generation of Factual News Headlines in Finnish,0.13934,"We present a novel approach to generating news headlines in Finnish for a
given news story. We model this as a summarization task where a model is given
a news article, and its task is to produce a concise headline describing the
main topic of the article. Because there are no openly available GPT-2 models
for Finnish, we will first build such a model using several corpora. The model
is then fine-tuned for the headline generation task using a massive news
corpus. The system is evaluated by 3 expert journalists working in a Finnish
media house. The results showcase the usability of the presented approach as a
headline suggestion tool to facilitate the news production process."
Seeing a Rose in Five Thousand Ways,0.114308,"What is a rose, visually? A rose comprises its intrinsics, including the
distribution of geometry, texture, and material specific to its object
category. With knowledge of these intrinsic properties, we may render roses of
different sizes and shapes, in different poses, and under different lighting
conditions. In this work, we build a generative model that learns to capture
such object intrinsics from a single image, such as a photo of a bouquet. Such
an image includes multiple instances of an object type. These instances all
share the same intrinsics, but appear different due to a combination of
variance within these intrinsics and differences in extrinsic factors, such as
pose and illumination. Experiments show that our model successfully learns
object intrinsics (distribution of geometry, texture, and material) for a wide
range of objects, each from a single Internet image. Our method achieves
superior results on multiple downstream tasks, including intrinsic image
decomposition, shape and image generation, view synthesis, and relighting."
Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR),0.157228,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys."
Few-shot Named Entity Recognition with Self-describing Networks,0.149295,"Few-shot NER needs to effectively capture information from limited instances
and transfer useful knowledge from external resources. In this paper, we
propose a self-describing mechanism for few-shot NER, which can effectively
leverage illustrative instances and precisely transfer knowledge from external
resources by describing both entity types and mentions using a universal
concept set. Specifically, we design Self-describing Networks (SDNet), a
Seq2Seq generation model which can universally describe mentions using
concepts, automatically map novel entity types to concepts, and adaptively
recognize entities on-demand. We pre-train SDNet with large-scale corpus, and
conduct experiments on 8 benchmarks from different domains. Experiments show
that SDNet achieves competitive performances on all benchmarks and achieves the
new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and
robustness."
Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models,0.111532,"Massively Multilingual Transformer based Language Models have been observed
to be surprisingly effective on zero-shot transfer across languages, though the
performance varies from language to language depending on the pivot language(s)
used for fine-tuning. In this work, we build upon some of the existing
techniques for predicting the zero-shot performance on a task, by modeling it
as a multi-task learning problem. We jointly train predictive models for
different tasks which helps us build more accurate predictors for tasks where
we have test data in very few languages to measure the actual performance of
the model. Our approach also lends us the ability to perform a much more robust
feature selection and identify a common set of features that influence
zero-shot performance across a variety of tasks."
Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis,0.115337,"Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net."
A Human-Centric Assessment Framework for AI,0.153976,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner."
Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval,0.110502,"Multi-document summarization (MDS) assumes a set of topic-related documents
are provided as input. In practice, this document set is not always available;
it would need to be retrieved given an information need, i.e. a question or
topic statement, a setting we dub ""open-domain"" MDS. We study this more
challenging setting by formalizing the task and bootstrapping it using existing
datasets, retrievers and summarizers. Via extensive automatic and human
evaluation, we determine: (1) state-of-the-art summarizers suffer large
reductions in performance when applied to open-domain MDS, (2) additional
training in the open-domain setting can reduce this sensitivity to imperfect
retrieval, and (3) summarizers are insensitive to the retrieval of duplicate
documents and the order of retrieved documents, but highly sensitive to other
errors, like the retrieval of irrelevant documents. Based on our results, we
provide practical guidelines to enable future work on open-domain MDS, e.g. how
to choose the number of retrieved documents to summarize. Our results suggest
that new retrieval and summarization methods and annotated resources for
training and evaluation are necessary for further progress in the open-domain
setting."
Low-complexity CNNs for Acoustic Scene Classification,0.108447,"This paper presents a low-complexity framework for acoustic scene
classification (ASC). Most of the frameworks designed for ASC use convolutional
neural networks (CNNs) due to their learning ability and improved performance
compared to hand-engineered features. However, CNNs are resource hungry due to
their large size and high computational complexity. Therefore, CNNs are
difficult to deploy on resource constrained devices. This paper addresses the
problem of reducing the computational complexity and memory requirement in
CNNs. We propose a low-complexity CNN architecture, and apply pruning and
quantization to further reduce the parameters and memory. We then propose an
ensemble framework that combines various low-complexity CNNs to improve the
overall performance. An experimental evaluation of the proposed framework is
performed on the publicly available DCASE 2022 Task 1 that focuses on ASC. The
proposed ensemble framework has approximately 60K parameters, requires 19M
multiply-accumulate operations and improves the performance by approximately
2-4 percentage points compared to the DCASE 2022 Task 1 baseline network."
That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory,0.12289,"The evolution of language follows the rule of gradual change. Grammar,
vocabulary, and lexical semantic shifts take place over time, resulting in a
diachronic linguistic gap. As such, a considerable amount of texts are written
in languages of different eras, which creates obstacles for natural language
processing tasks, such as word segmentation and machine translation. Although
the Chinese language has a long history, previous Chinese natural language
processing research has primarily focused on tasks within a specific era.
Therefore, we propose a cross-era learning framework for Chinese word
segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to
incorporate era-specific linguistic knowledge. Experiments on four corpora from
different eras show that the performance of each corpus significantly improves.
Further analyses also demonstrate that the SM can effectively integrate the
knowledge of the eras into the neural network."
Defending Black-box Skeleton-based Human Activity Classifiers,0.113883,"Skeletal motions have been heavily replied upon for human activity
recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR
has been identified across a variety of classifiers and data, calling for
mitigation. To this end, we propose the first black-box defense method for
skeleton-based HAR to our best knowledge. Our method is featured by full
Bayesian treatments of the clean data, the adversaries and the classifier,
leading to (1) a new Bayesian Energy-based formulation of robust discriminative
classifiers, (2) a new adversary sampling scheme based on natural motion
manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We
name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is
straightforward but elegant, which turns vulnerable black-box classifiers into
robust ones without sacrificing accuracy. It demonstrates surprising and
universal effectiveness across a wide range of skeletal HAR classifiers and
datasets, under various attacks. Code is available at
https://github.com/realcrane/RobustActionRecogniser."
RASR: Risk-Averse Soft-Robust MDPs with EVaR and Entropic Risk,0.107418,"Prior work on safe Reinforcement Learning (RL) has studied risk-aversion to
randomness in dynamics (aleatory) and to model uncertainty (epistemic) in
isolation. We propose and analyze a new framework to jointly model the risk
associated with epistemic and aleatory uncertainties in finite-horizon and
discounted infinite-horizon MDPs. We call this framework that combines
Risk-Averse and Soft-Robust methods RASR. We show that when the risk-aversion
is defined using either EVaR or the entropic risk, the optimal policy in RASR
can be computed efficiently using a new dynamic program formulation with a
time-dependent risk level. As a result, the optimal risk-averse policies are
deterministic but time-dependent, even in the infinite-horizon discounted
setting. We also show that particular RASR objectives reduce to risk-averse RL
with mean posterior transition probabilities. Our empirical results show that
our new algorithms consistently mitigate uncertainty as measured by EVaR and
other standard risk measures."
ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer,0.113318,"The problem of multimodal intent and trajectory prediction for human-driven
vehicles in parking lots is addressed in this paper. Using models designed with
CNN and Transformer networks, we extract temporal-spatial and contextual
information from trajectory history and local bird's eye view (BEV) semantic
images, and generate predictions about intent distribution and future
trajectory sequences. Our methods outperform existing models in accuracy, while
allowing an arbitrary number of modes, encoding complex multi-agent scenarios,
and adapting to different parking maps. To train and evaluate our method, we
present the first public 4K video dataset of human driving in parking lots with
accurate annotation, high frame rate, and rich traffic scenarios."
Exploring Target Representations for Masked Autoencoders,0.166271,"Masked autoencoders have become popular training paradigms for
self-supervised visual representation learning. These models randomly mask a
portion of the input and reconstruct the masked portion according to the target
representations. In this paper, we first show that a careful choice of the
target representation is unnecessary for learning good representations, since
different targets tend to derive similarly behaved models. Driven by this
observation, we propose a multi-stage masked distillation pipeline and use a
randomly initialized model as the teacher, enabling us to effectively train
high-capacity models without any efforts to carefully design target
representations. Interestingly, we further explore using teachers of larger
capacity, obtaining distilled students with remarkable transferring ability. On
different tasks of classification, transfer learning, object detection, and
semantic segmentation, the proposed method to perform masked knowledge
distillation with bootstrapped teachers (dBOT) outperforms previous
self-supervised methods by nontrivial margins. We hope our findings, as well as
the proposed method, could motivate people to rethink the roles of target
representations in pre-training masked autoencoders.The code and pre-trained
models are publicly available at https://github.com/liuxingbin/dbot."
A New Amharic Speech Emotion Dataset and Classification Benchmark,0.167651,"In this paper we present the Amharic Speech Emotion Dataset (ASED), which
covers four dialects (Gojjam, Wollo, Shewa and Gonder) and five different
emotions (neutral, fearful, happy, sad and angry). We believe it is the first
Speech Emotion Recognition (SER) dataset for the Amharic language. 65 volunteer
participants, all native speakers, recorded 2,474 sound samples, two to four
seconds in length. Eight judges assigned emotions to the samples with high
agreement level (Fleiss kappa = 0.8). The resulting dataset is freely available
for download. Next, we developed a four-layer variant of the well-known VGG
model which we call VGGb. Three experiments were then carried out using VGGb
for SER, using ASED. First, we investigated whether Mel-spectrogram features or
Mel-frequency Cepstral coefficient (MFCC) features work best for Amharic. This
was done by training two VGGb SER models on ASED, one using Mel-spectrograms
and the other using MFCC. Four forms of training were tried, standard
cross-validation, and three variants based on sentences, dialects and speaker
groups. Thus, a sentence used for training would not be used for testing, and
the same for a dialect and speaker group. The conclusion was that MFCC features
are superior under all four training schemes. MFCC was therefore adopted for
Experiment 2, where VGGb and three other existing models were compared on ASED:
RESNet50, Alex-Net and LSTM. VGGb was found to have very good accuracy (90.73%)
as well as the fastest training time. In Experiment 3, the performance of VGGb
was compared when trained on two existing SER datasets, RAVDESS (English) and
EMO-DB (German) as well as on ASED (Amharic). Results are comparable across
these languages, with ASED being the highest. This suggests that VGGb can be
successfully applied to other languages. We hope that ASED will encourage
researchers to experiment with other models for Amharic SER."
BBA-net: A bi-branch attention network for crowd counting,0.161621,"In the field of crowd counting, the current mainstream CNN-based regression
methods simply extract the density information of pedestrians without finding
the position of each person. This makes the output of the network often found
to contain incorrect responses, which may erroneously estimate the total number
and not conducive to the interpretation of the algorithm. To this end, we
propose a Bi-Branch Attention Network (BBA-NET) for crowd counting, which has
three innovation points. i) A two-branch architecture is used to estimate the
density information and location information separately. ii) Attention
mechanism is used to facilitate feature extraction, which can reduce false
responses. iii) A new density map generation method combining geometric
adaptation and Voronoi split is introduced. Our method can integrate the
pedestrian's head and body information to enhance the feature expression
ability of the density map. Extensive experiments performed on two public
datasets show that our method achieves a lower crowd counting error compared to
other state-of-the-art methods."
Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition,0.180669,"Subject-invariant facial action unit (AU) recognition remains challenging for
the reason that the data distribution varies among subjects. In this paper, we
propose a causal inference framework for subject-invariant facial action unit
recognition. To illustrate the causal effect existing in AU recognition task,
we formulate the causalities among facial images, subjects, latent AU semantic
relations, and estimated AU occurrence probabilities via a structural causal
model. By constructing such a causal diagram, we clarify the causal effect
among variables and propose a plug-in causal intervention module, CIS, to
deconfound the confounder \emph{Subject} in the causal diagram. Extensive
experiments conducted on two commonly used AU benchmark datasets, BP4D and
DISFA, show the effectiveness of our CIS, and the model with CIS inserted,
CISNet, has achieved state-of-the-art performance."
Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities,0.161478,"As for other forms of AI, speech recognition has recently been examined with
respect to performance disparities across different user cohorts. One approach
to achieve fairness in speech recognition is to (1) identify speaker cohorts
that suffer from subpar performance and (2) apply fairness mitigation measures
targeting the cohorts discovered. In this paper, we report on initial findings
with both discovery and mitigation of performance disparities using data from a
product-scale AI assistant speech recognition system. We compare cohort
discovery based on geographic and demographic information to a more scalable
method that groups speakers without human labels, using speaker embedding
technology. For fairness mitigation, we find that oversampling of
underrepresented cohorts, as well as modeling speaker cohort membership by
additional input variables, reduces the gap between top- and bottom-performing
cohorts, without deteriorating overall recognition accuracy."
Standing on the Shoulders of Giant Frozen Language Models,0.177088,"Huge pretrained language models (LMs) have demonstrated surprisingly good
zero-shot capabilities on a wide variety of tasks. This gives rise to the
appealing vision of a single, versatile model with a wide range of
functionalities across disparate applications. However, current leading
techniques for leveraging a ""frozen"" LM -- i.e., leaving its weights untouched
-- still often underperform fine-tuning approaches which modify these weights
in a task-dependent way. Those, in turn, suffer forgetfulness and compromise
versatility, suggesting a tradeoff between performance and versatility. The
main message of this paper is that current frozen-model techniques such as
prompt tuning are only the tip of the iceberg, and more powerful methods for
leveraging frozen LMs can do just as well as fine tuning in challenging domains
without sacrificing the underlying model's versatility. To demonstrate this, we
introduce three novel methods for leveraging frozen models: input-dependent
prompt tuning, frozen readers, and recursive LMs, each of which vastly improves
on current frozen-model approaches. Indeed, some of our methods even outperform
fine-tuning approaches in domains currently dominated by the latter. The
computational cost of each method is higher than that of existing frozen model
methods, but still negligible relative to a single pass through a huge frozen
LM. Each of these methods constitutes a meaningful contribution in its own
right, but by presenting these contributions together we aim to convince the
reader of a broader message that goes beyond the details of any given method:
that frozen models have untapped potential and that fine-tuning is often
unnecessary."
YORO -- Lightweight End to End Visual Grounding,0.207477,"We present YORO - a multi-modal transformer encoder-only architecture for the
Visual Grounding (VG) task. This task involves localizing, in an image, an
object referred via natural language. Unlike the recent trend in the literature
of using multi-stage approaches that sacrifice speed for accuracy, YORO seeks a
better trade-off between speed an accuracy by embracing a single-stage design,
without CNN backbone. YORO consumes natural language queries, image patches,
and learnable detection tokens and predicts coordinates of the referred object,
using a single transformer encoder. To assist the alignment between text and
visual objects, a novel patch-text alignment loss is proposed. Extensive
experiments are conducted on 5 different datasets with ablations on
architecture design choices. YORO is shown to support real-time inference and
outperform all approaches in this class (single-stage methods) by large
margins. It is also the fastest VG model and achieves the best speed/accuracy
trade-off in the literature."
Will Large-scale Generative Models Corrupt Future Datasets?,0.194836,"Recently proposed large-scale text-to-image generative models such as
DALL$\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and
realistic images from users' prompts. Not limited to the research community,
ordinary Internet users enjoy these generative models, and consequently, a
tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a
lot to images collected from the Internet. These trends lead us to a research
question: ""\textbf{will such generated images impact the quality of future
datasets and the performance of computer vision models positively or
negatively?}"" This paper empirically answers this question by simulating
contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using
a state-of-the-art generative model and evaluate models trained with
""contaminated"" datasets on various tasks, including image classification and
image generation. Throughout experiments, we conclude that generated images
negatively affect downstream performance, while the significance depends on
tasks and the amount of generated images. The generated datasets and the codes
for experiments will be publicly released for future research. Generated
datasets and source codes are available from
\url{https://github.com/moskomule/dataset-contamination}."
OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers,0.160982,"We present OSFormer, the first one-stage transformer framework for
camouflaged instance segmentation (CIS). OSFormer is based on two key designs.
First, we design a location-sensing transformer (LST) to obtain the location
label and instance-aware parameters by introducing the location-guided queries
and the blend-convolution feedforward network. Second, we develop a
coarse-to-fine fusion (CFF) to merge diverse context information from the LST
encoder and CNN backbone. Coupling these two components enables OSFormer to
efficiently blend local features and long-range context dependencies for
predicting camouflaged instances. Compared with two-stage frameworks, our
OSFormer reaches 41% AP and achieves good convergence efficiency without
requiring enormous training data, i.e., only 3,040 samples under 60 epochs.
Code link: https://github.com/PJLallen/OSFormer."
TensorIR: An Abstraction for Automatic Tensorized Program Optimization,0.192128,"Deploying deep learning models on various devices has become an important
topic. The wave of hardware specialization brings a diverse set of acceleration
primitives for multi-dimensional tensor computations. These new acceleration
primitives, along with the emerging machine learning models, bring tremendous
engineering challenges. In this paper, we present TensorIR, a compiler
abstraction for optimizing programs with these tensor computation primitives.
TensorIR generalizes the loop nest representation used in existing machine
learning compilers to bring tensor computation as the first-class citizen.
Finally, we build an end-to-end framework on top of our abstraction to
automatically optimize deep learning models for given tensor computation
primitives. Experimental results show that TensorIR compilation automatically
uses the tensor computation primitives for given hardware backends and delivers
performance that is competitive to state-of-art hand-optimized systems across
platforms."
VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension,0.183473,"One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference."
SQuId: Measuring Speech Naturalness in Many Languages,0.196008,"Much of text-to-speech research relies on human evaluation, which incurs
heavy costs and slows down the development process. The problem is particularly
acute in heavily multilingual applications, where recruiting and polling judges
can take weeks. We introduce SQuId (Speech Quality Identification), a
multilingual naturalness prediction model trained on over a million ratings and
tested in 65 locales-the largest effort of this type to date. The main insight
is that training one model on many locales consistently outperforms mono-locale
baselines. We present our task, the model, and show that it outperforms a
competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then
demonstrate the effectiveness of cross-locale transfer during fine-tuning and
highlight its effect on zero-shot locales, i.e., locales for which there is no
fine-tuning data. Through a series of analyses, we highlight the role of
non-linguistic effects such as sound artifacts in cross-locale transfer.
Finally, we present the effect of our design decision, e.g., model size,
pre-training diversity, and language rebalancing with several ablation
experiments."
AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,0.204335,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate."
PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation,0.160137,"In this paper, we propose a new deep learning-based method for estimating
room layout given a pair of 360 panoramas. Our system, called Position-aware
Stereo Merging Network or PSMNet, is an end-to-end joint layout-pose estimator.
PSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel
Cross-Perspective Projection (CP2) layer. The stereo-view SP2 transformer is
used to implicitly infer correspondences between views, and can handle noisy
poses. The pose-aware CP2 layer is designed to render features from the
adjacent view to the anchor (reference) view, in order to perform view fusion
and estimate the visible layout. Our experiments and analysis validate our
method, which significantly outperforms the state-of-the-art layout estimators,
especially for large and complex room spaces."
Matching Tweets With Applicable Fact-Checks Across Languages,0.16573,"An important challenge for news fact-checking is the effective dissemination
of existing fact-checks. This in turn brings the need for reliable methods to
detect previously fact-checked claims. In this paper, we focus on automatically
finding existing fact-checks for claims made in social media posts (tweets). We
conduct both classification and retrieval experiments, in monolingual (English
only), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)
settings using multilingual transformer models such as XLM-RoBERTa and
multilingual embeddings such as LaBSE and SBERT. We present promising results
for ""match"" classification (86% average accuracy) in four language pairs. We
also find that a BM25 baseline outperforms or is on par with state-of-the-art
multilingual embedding models for the retrieval task during our monolingual
experiments. We highlight and discuss NLP challenges while addressing this
problem in different languages, and we introduce a novel curated dataset of
fact-checks and corresponding tweets for future research."
On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,0.192512,"Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance."
Neural Cloth Simulation,0.190283,"We present a general framework for the garment animation problem through
unsupervised deep learning inspired in physically based simulation. Existing
trends in the literature already explore this possibility. Nonetheless, these
approaches do not handle cloth dynamics. Here, we propose the first methodology
able to learn realistic cloth dynamics unsupervisedly, and henceforth, a
general formulation for neural cloth simulation. The key to achieve this is to
adapt an existing optimization scheme for motion from simulation based
methodologies to deep learning. Then, analyzing the nature of the problem, we
devise an architecture able to automatically disentangle static and dynamic
cloth subspaces by design. We will show how this improves model performance.
Additionally, this opens the possibility of a novel motion augmentation
technique that greatly improves generalization. Finally, we show it also allows
to control the level of motion in the predictions. This is a useful, never seen
before, tool for artists. We provide of detailed analysis of the problem to
establish the bases of neural cloth simulation and guide future research into
the specifics of this domain."
Selective Annotation Makes Language Models Better Few-Shot Learners,0.20881,"Many recent approaches to natural language tasks are built on the remarkable
abilities of large language models. Large language models can perform
in-context learning, where they learn a new task from a few task
demonstrations, without any parameter updates. This work examines the
implications of in-context learning for the creation of datasets for new
natural language tasks. Departing from recent in-context learning methods, we
formulate an annotation-efficient, two-step framework: selective annotation
that chooses a pool of examples to annotate from unlabeled data in advance,
followed by prompt retrieval that retrieves task examples from the annotated
pool at test time. Based on this framework, we propose an unsupervised,
graph-based selective annotation method, voke-k, to select diverse,
representative examples to annotate. Extensive experiments on 10 datasets
(covering classification, commonsense reasoning, dialogue, and text/code
generation) demonstrate that our selective annotation method improves the task
performance by a large margin. On average, vote-k achieves a 12.9%/11.4%
relative gain under an annotation budget of 18/100, as compared to randomly
selecting examples to annotate. Compared to state-of-the-art supervised
finetuning approaches, it yields similar performance with 10-100x less
annotation cost across 10 tasks. We further analyze the effectiveness of our
framework in various scenarios: language models with varying sizes, alternative
selective annotation methods, and cases where there is a test data domain
shift. We hope that our studies will serve as a basis for data annotations as
large language models are increasingly applied to new tasks. Our code is
available at https://github.com/HKUNLP/icl-selective-annotation."
Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation,0.203218,"In recent years, a number of keyphrase generation (KPG) approaches were
proposed consisting of complex model architectures, dedicated training
paradigms and decoding strategies. In this work, we opt for simplicity and show
how a commonly used seq2seq language model, BART, can be easily adapted to
generate keyphrases from the text in a single batch computation using a simple
training procedure. Empirical results on five benchmarks show that our approach
is as good as the existing state-of-the-art KPG systems, but using a much
simpler and easy to deploy framework."
Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions,0.203454,"Pruning is one of the predominant approaches for compressing deep neural
networks (DNNs). Lately, coresets (provable data summarizations) were leveraged
for pruning DNNs, adding the advantage of theoretical guarantees on the
trade-off between the compression rate and the approximation error. However,
coresets in this domain were either data-dependent or generated under
restrictive assumptions on both the model's weights and inputs. In real-world
scenarios, such assumptions are rarely satisfied, limiting the applicability of
coresets. To this end, we suggest a novel and robust framework for computing
such coresets under mild assumptions on the model's weights and without any
assumption on the training data. The idea is to compute the importance of each
neuron in each layer with respect to the output of the following layer. This is
achieved by a combination of L\""{o}wner ellipsoid and Caratheodory theorem. Our
method is simultaneously data-independent, applicable to various networks and
datasets (due to the simplified assumptions), and theoretically supported.
Experimental results show that our method outperforms existing coreset based
neural pruning approaches across a wide range of networks and datasets. For
example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet
with $1.09\%$ drop in accuracy."
Spatial Transformer Network on Skeleton-based Gait Recognition,0.198598,"Skeleton-based gait recognition models usually suffer from the robustness
problem, as the Rank-1 accuracy varies from 90\% in normal walking cases to
70\% in walking with coats cases. In this work, we propose a state-of-the-art
robust skeleton-based gait recognition model called Gait-TR, which is based on
the combination of spatial transformer frameworks and temporal convolutional
networks. Gait-TR achieves substantial improvements over other skeleton-based
gait models with higher accuracy and better robustness on the well-known gait
dataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\%
Rank-1 gait recognition accuracy rate, which is higher than the best result of
silhouette-based models, which usually have higher accuracy than the
silhouette-based gait recognition models. Moreover, our experiment on CASIA-B
shows that the spatial transformer can extract gait features from the human
skeleton better than the widely used graph convolutional network."
Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages,0.179412,"Grammatical cues are sometimes redundant with word meanings in natural
language. For instance, English word order rules constrain the word order of a
sentence like ""The dog chewed the bone"" even though the status of ""dog"" as
subject and ""bone"" as object can be inferred from world knowledge and
plausibility. Quantifying how often this redundancy occurs, and how the level
of redundancy varies across typologically diverse languages, can shed light on
the function and evolution of grammar. To that end, we performed a behavioral
experiment in English and Russian and a cross-linguistic computational analysis
measuring the redundancy of grammatical cues in transitive clauses extracted
from corpus text. English and Russian speakers (n=484) were presented with
subjects, verbs, and objects (in random order and with morphological markings
removed) extracted from naturally occurring sentences and were asked to
identify which noun is the subject of the action. Accuracy was high in both
languages (~89% in English, ~87% in Russian). Next, we trained a neural network
machine classifier on a similar task: predicting which nominal in a
subject-verb-object triad is the subject. Across 30 languages from eight
language families, performance was consistently high: a median accuracy of 87%,
comparable to the accuracy observed in the human experiments. The conclusion is
that grammatical cues such as word order are necessary to convey subjecthood
and objecthood in a minority of naturally occurring transitive clauses;
nevertheless, they can (a) provide an important source of redundancy and (b)
are crucial for conveying intended meaning that cannot be inferred from the
words alone, including descriptions of human interactions, where roles are
often reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing
non-prototypical meanings (e.g., ""The bone chewed the dog."")."
Deep Generative Framework for Interactive 3D Terrain Authoring and Manipulation,0.181999,"Automated generation and (user) authoring of the realistic virtual terrain is
most sought for by the multimedia applications like VR models and gaming. The
most common representation adopted for terrain is Digital Elevation Model
(DEM). Existing terrain authoring and modeling techniques have addressed some
of these and can be broadly categorized as: procedural modeling, simulation
method, and example-based methods. In this paper, we propose a novel realistic
terrain authoring framework powered by a combination of VAE and generative
conditional GAN model. Our framework is an example-based method that attempts
to overcome the limitations of existing methods by learning a latent space from
a real-world terrain dataset. This latent space allows us to generate multiple
variants of terrain from a single input as well as interpolate between terrains
while keeping the generated terrains close to real-world data distribution. We
also developed an interactive tool, that lets the user generate diverse
terrains with minimalist inputs. We perform thorough qualitative and
quantitative analysis and provide comparisons with other SOTA methods. We
intend to release our code/tool to the academic community."
A Reinforcement Learning Approach for Electric Vehicle Routing Problem with Vehicle-to-Grid Supply,0.18992,"The use of electric vehicles (EV) in the last mile is appealing from both
sustainability and operational cost perspectives. In addition to the inherent
cost efficiency of EVs, selling energy back to the grid during peak grid
demand, is a potential source of additional revenue to a fleet operator. To
achieve this, EVs have to be at specific locations (discharge points) during
specific points in time (peak period), even while meeting their core purpose of
delivering goods to customers. In this work, we consider the problem of EV
routing with constraints on loading capacity; time window; vehicle-to-grid
energy supply (CEVRPTW-D); which not only satisfy multiple system objectives,
but also scale efficiently to large problem sizes involving hundreds of
customers and discharge stations. We present QuikRouteFinder that uses
reinforcement learning (RL) for EV routing to overcome these challenges. Using
Solomon datasets, results from RL are compared against exact formulations based
on mixed-integer linear program (MILP) and genetic algorithm (GA)
metaheuristics. On an average, the results show that RL is 24 times faster than
MILP and GA, while being close in quality (within 20%) to the optimal."
WR-ONE2SET: Towards Well-Calibrated Keyphrase Generation,0.166182,"Keyphrase generation aims to automatically generate short phrases summarizing
an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021)
generates keyphrases as a set and has achieved competitive performance.
Nevertheless, we observe serious calibration errors outputted by ONE2SET,
especially in the over-estimation of $\varnothing$ token (means ""no
corresponding keyphrase""). In this paper, we deeply analyze this limitation and
identify two main reasons behind: 1) the parallel generation has to introduce
excessive $\varnothing$ as padding tokens into training instances; and 2) the
training mechanism assigning target to each slot is unstable and further
aggravates the $\varnothing$ token over-estimation. To make the model
well-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive
instance-level cost Weighting strategy and a target Re-assignment mechanism.
The former dynamically penalizes the over-estimated slots for different
instances thus smoothing the uneven training distribution. The latter refines
the original inappropriate assignment and reduces the supervisory signals of
over-estimated slots. Experimental results on commonly-used datasets
demonstrate the effectiveness and generality of our proposed paradigm."
A Dynamic Graph Interactive Framework with Label-Semantic Injection for Spoken Language Understanding,0.172606,"Multi-intent detection and slot filling joint models are gaining increasing
traction since they are closer to complicated real-world scenarios. However,
existing approaches (1) focus on identifying implicit correlations between
utterances and one-hot encoded labels in both tasks while ignoring explicit
label characteristics; (2) directly incorporate multi-intent information for
each token, which could lead to incorrect slot prediction due to the
introduction of irrelevant intent. In this paper, we propose a framework termed
DGIF, which first leverages the semantic information of labels to give the
model additional signals and enriched priors. Then, a multi-grain interactive
graph is constructed to model correlations between intents and slots.
Specifically, we propose a novel approach to construct the interactive graph
based on the injection of label semantics, which can automatically update the
graph to better alleviate error propagation. Experimental results show that our
framework significantly outperforms existing approaches, obtaining a relative
improvement of 13.7% over the previous best model on the MixATIS dataset in
overall accuracy."
Read Top News First: A Document Reordering Approach for Multi-Document News Summarization,0.182287,"A common method for extractive multi-document news summarization is to
re-formulate it as a single-document summarization problem by concatenating all
documents as a single meta-document. However, this method neglects the relative
importance of documents. We propose a simple approach to reorder the documents
according to their relative importance before concatenating and summarizing
them. The reordering makes the salient content easier to learn by the
summarization model. Experiments show that our approach outperforms previous
state-of-the-art methods with more complex architectures."
Automatic Evaluation and Analysis of Idioms in Neural Machine Translation,0.213743,"A major open problem in neural machine translation (NMT) is the translation
of idiomatic expressions, such as ""under the weather"". The meaning of these
expressions is not composed by the meaning of their constituent words, and NMT
models tend to translate them literally (i.e., word-by-word), which leads to
confusing and nonsensical translations. Research on idioms in NMT is limited
and obstructed by the absence of automatic methods for quantifying these
errors. In this work, first, we propose a novel metric for automatically
measuring the frequency of literal translation errors without human
involvement. Equipped with this metric, we present controlled translation
experiments with models trained in different conditions (with/without the
test-set idioms) and across a wide range of (global and targeted) metrics and
test sets. We explore the role of monolingual pretraining and find that it
yields substantial targeted improvements, even without observing any
translation examples of the test-set idioms. In our analysis, we probe the role
of idiom context. We find that the randomly initialized models are more local
or ""myopic"" as they are relatively unaffected by variations of the idiom
context, unlike the pretrained ones."
Transformer Quality in Linear Time,0.233874,"We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling."
Training Language Models with Language Feedback,0.22071,"Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability."
Linearizing Transformer with Key-Value Memory,0.2387,"Efficient transformer variants with linear time complexity have been
developed to mitigate the quadratic computational overhead of the vanilla
transformer. Among them are low-rank projection methods such as Linformer and
kernel-based Transformers. Despite their unique merits, they usually suffer
from a performance drop comparing with the vanilla transformer on many sequence
generation tasks, and often fail to obtain computation gain when the generation
is short. We propose MemSizer, an approach towards closing the performance gap
while improving the efficiency even with short generation. It projects the
source sequences into lower dimension representations like Linformer, while
enjoying efficient recurrent-style incremental computation similar to
kernel-based transformers. This yields linear computation time and constant
memory complexity at inference time. MemSizer also employs a lightweight
multi-head mechanism which renders the computation as light as a single-head
model. We demonstrate that MemSizer provides an improved balance between
efficiency and accuracy over the vanilla transformer and other efficient
transformer variants in three typical sequence generation tasks, including
machine translation, abstractive text summarization, and language modeling."
Hyperspherical Consistency Regularization,0.224419,"Recent advances in contrastive learning have enlightened diverse applications
across various semi-supervised fields. Jointly training supervised learning and
unsupervised learning with a shared feature encoder becomes a common scheme.
Though it benefits from taking advantage of both feature-dependent information
from self-supervised learning and label-dependent information from supervised
learning, this scheme remains suffering from bias of the classifier. In this
work, we systematically explore the relationship between self-supervised
learning and supervised learning, and study how self-supervised learning helps
robust data-efficient deep learning. We propose hyperspherical consistency
regularization (HCR), a simple yet effective plug-and-play method, to
regularize the classifier using feature-dependent information and thus avoid
bias from labels. Specifically, HCR first projects logits from the classifier
and feature projections from the projection head on the respective hypersphere,
then it enforces data points on hyperspheres to have similar structures by
minimizing binary cross entropy of pairwise distances' similarity metrics.
Extensive experiments on semi-supervised and weakly-supervised learning
demonstrate the effectiveness of our method, by showing superior performance
with HCR."
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,0.221344,"Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation studies for summarization either exhibit a low inter-annotator
agreement or have insufficient scale, and an in-depth analysis of human
evaluation is lacking. Therefore, we address the shortcomings of existing
summarization evaluation along the following axes: (1) We propose a modified
summarization salience protocol, Atomic Content Units (ACUs), which is based on
fine-grained semantic units and allows for a high inter-annotator agreement.
(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large
human evaluation dataset consisting of 22,000 summary-level annotations over 28
top-performing systems on three datasets. (3) We conduct a comparative study of
four human evaluation protocols, underscoring potential confounding factors in
evaluation setups. (4) We evaluate 50 automatic metrics and their variants
using the collected human annotations across evaluation protocols and
demonstrate how our benchmark leads to more statistically stable and
significant results. The metrics we benchmarked include recent methods based on
large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings
have important implications for evaluating LLMs, as we show that LLMs adjusted
by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,
which is affected by the annotators' prior, input-agnostic preferences, calling
for more robust, targeted evaluation methods."
Deep Leaning-Based Ultra-Fast Stair Detection,0.234668,"Staircases are some of the most common building structures in urban
environments. Stair detection is an important task for various applications,
including the environmental perception of exoskeleton robots, humanoid robots,
and rescue robots and the navigation of visually impaired people. Most existing
stair detection algorithms have difficulty dealing with the diversity of stair
structure materials, extreme light and serious occlusion. Inspired by human
perception, we propose an end-to-end method based on deep learning.
Specifically, we treat the process of stair line detection as a multitask
involving coarse-grained semantic segmentation and object detection. The input
images are divided into cells, and a simple neural network is used to judge
whether each cell contains stair lines. For cells containing stair lines, the
locations of the stair lines relative to each cell are regressed. Extensive
experiments on our dataset show that our method can achieve high performance in
terms of both speed and accuracy. A lightweight version can even achieve 300+
frames per second with the same resolution. Our code and dataset will be soon
available at GitHub."
Few-Shot Table-to-Text Generation with Prefix-Controlled Generator,0.232355,"Neural table-to-text generation approaches are data-hungry, limiting their
adaptation for low-resource real-world applications. Previous works mostly
resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a
table. However, they often contain hallucinated contents due to the
uncontrolled nature of PLMs. Moreover, the topological differences between
tables and sequences are rarely studied. Last but not least, fine-tuning on
PLMs with a handful of instances may lead to over-fitting and catastrophic
forgetting. To alleviate these problems, we propose a prompt-based approach,
Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation.
We prepend a task-specific prefix for a PLM to make the table structure better
fit the pre-trained input. In addition, we generate an input-specific prefix to
control the factual contents and word order of the generated text. Both
automatic and human evaluations on different domains (humans, books and songs)
of the Wikibio dataset show substantial improvements over baseline approaches."
Learning to Detect Mobile Objects from LiDAR Scans Without Labels,0.253891,"Current 3D object detectors for autonomous driving are almost entirely
trained on human-annotated data. Although of high quality, the generation of
such data is laborious and costly, restricting them to a few specific locations
and object types. This paper proposes an alternative approach entirely based on
unlabeled data, which can be collected cheaply and in abundance almost
everywhere on earth. Our approach leverages several simple common sense
heuristics to create an initial set of approximate seed labels. For example,
relevant traffic participants are generally not persistent across multiple
traversals of the same route, do not fly, and are never under ground. We
demonstrate that these seed labels are highly effective to bootstrap a
surprisingly accurate detector through repeated self-training without a single
human annotated label."
Keypoint Cascade Voting for Point Cloud Based 6DoF Pose Estimation,0.220729,"We propose a novel keypoint voting 6DoF object pose estimation method, which
takes pure unordered point cloud geometry as input without RGB information. The
proposed cascaded keypoint voting method, called RCVPose3D, is based upon a
novel architecture which separates the task of semantic segmentation from that
of keypoint regression, thereby increasing the effectiveness of both and
improving the ultimate performance. The method also introduces a pairwise
constraint in between different keypoints to the loss function when regressing
the quantity for keypoint estimation, which is shown to be effective, as well
as a novel Voter Confident Score which enhances both the learning and inference
stages. Our proposed RCVPose3D achieves state-of-the-art performance on the
Occlusion LINEMOD (74.5%) and YCB-Video (96.9%) datasets, outperforming
existing pure RGB and RGB-D based methods, as well as being competitive with
RGB plus point cloud methods."
Revisiting End-to-End Speech-to-Text Translation From Scratch,0.259694,"End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining
its encoder and/or decoder using source transcripts via speech recognition or
text translation tasks, without which translation performance drops
substantially. However, transcripts are not always available, and how
significant such pretraining is for E2E ST has rarely been studied in the
literature. In this paper, we revisit this question and explore the extent to
which the quality of E2E ST trained on speech-translation pairs alone can be
improved. We reexamine several techniques proven beneficial to ST previously,
and offer a set of best practices that biases a Transformer-based E2E ST system
toward training from scratch. Besides, we propose parameterized distance
penalty to facilitate the modeling of locality in the self-attention model for
speech. On four benchmarks covering 23 languages, our experiments show that,
without using any transcripts or pretraining, the proposed system reaches and
even outperforms previous studies adopting pretraining, although the gap
remains in (extremely) low-resource settings. Finally, we discuss neural
acoustic feature modeling, where a neural model is designed to extract acoustic
features from raw speech signals directly, with the goal to simplify inductive
biases and add freedom to the model in describing speech. For the first time,
we demonstrate its feasibility and show encouraging results on ST tasks."
"Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022",0.230731,"The Event Causality Identification Shared Task of CASE 2022 involved two
subtasks working on the Causal News Corpus. Subtask 1 required participants to
predict if a sentence contains a causal relation or not. This is a supervised
binary classification task. Subtask 2 required participants to identify the
Cause, Effect and Signal spans per causal sentence. This could be seen as a
supervised sequence labeling task. For both subtasks, participants uploaded
their predictions for a held-out test set, and ranking was done based on binary
F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes
the work of the 17 teams that submitted their results to our competition and 12
system description papers that were received. The best F1 scores achieved for
Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing
approaches involved pre-trained language models fine-tuned to the targeted
task. We further discuss these approaches and analyze errors across
participants' systems in this paper."
Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian,0.236385,"We present the shared task on artificial text detection in Russian, which is
organized as a part of the Dialogue Evaluation initiative, held in 2022. The
shared task dataset includes texts from 14 text generators, i.e., one human
writer and 13 text generative models fine-tuned for one or more of the
following generation tasks: machine translation, paraphrase generation, text
summarization, text simplification. We also consider back-translation and
zero-shot generation approaches. The human-written texts are collected from
publicly available resources across multiple domains. The shared task consists
of two sub-tasks: (i) to determine if a given text is automatically generated
or written by a human; (ii) to identify the author of a given text. The first
task is framed as a binary classification problem. The second task is a
multi-class classification problem. We provide count-based and BERT-based
baselines, along with the human evaluation on the first sub-task. A total of 30
and 8 systems have been submitted to the binary and multi-class sub-tasks,
correspondingly. Most teams outperform the baselines by a wide margin. We
publicly release our codebase, human evaluation results, and other materials in
our GitHub repository (https://github.com/dialogue-evaluation/RuATD)."
Self-supervised models of audio effectively explain human cortical responses to speech,0.215203,"Self-supervised language models are very effective at predicting high-level
cortical responses during language comprehension. However, the best current
models of lower-level auditory processing in the human brain rely on either
hand-constructed acoustic filters or representations from supervised audio
neural networks. In this work, we capitalize on the progress of self-supervised
speech representation learning (SSL) to create new state-of-the-art models of
the human auditory system. Compared against acoustic baselines, phonemic
features, and supervised models, representations from the middle layers of
self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently
yield the best prediction performance for fMRI recordings within the auditory
cortex (AC). Brain areas involved in low-level auditory processing exhibit a
preference for earlier SSL model layers, whereas higher-level semantic areas
prefer later layers. We show that these trends are due to the models' ability
to encode information at multiple linguistic levels (acoustic, phonetic, and
lexical) along their representation depth. Overall, these results show that
self-supervised models effectively capture the hierarchy of information
relevant to different stages of speech processing in human cortex."
Cycle-Consistent Counterfactuals by Latent Transformations,0.248931,"CounterFactual (CF) visual explanations try to find images similar to the
query image that change the decision of a vision system to a specified outcome.
Existing methods either require inference-time optimization or joint training
with a generative adversarial model which makes them time-consuming and
difficult to use in practice. We propose a novel approach, Cycle-Consistent
Counterfactuals by Latent Transformations (C3LT), which learns a latent
transformation that automatically generates visual CFs by steering in the
latent space of generative models. Our method uses cycle consistency between
the query and CF latent representations which helps our training to find better
solutions. C3LT can be easily plugged into any state-of-the-art pretrained
generative network. This enables our method to generate high-quality and
interpretable CF images at high resolution such as those in ImageNet. In
addition to several established metrics for evaluating CF explanations, we
introduce a novel metric tailored to assess the quality of the generated CF
examples and validate the effectiveness of our method on an extensive set of
experiments."
DraftRec: Personalized Draft Recommendation for Winning in Multi-Player Online Battle Arena Games,0.214734,"This paper presents a personalized character recommendation system for
Multiplayer Online Battle Arena (MOBA) games which are considered as one of the
most popular online video game genres around the world. When playing MOBA
games, players go through a draft stage, where they alternately select a
virtual character to play. When drafting, players select characters by not only
considering their character preferences, but also the synergy and competence of
their team's character combination. However, the complexity of drafting induces
difficulties for beginners to choose the appropriate characters based on the
characters of their team while considering their own champion preferences. To
alleviate this problem, we propose DraftRec, a novel hierarchical model which
recommends characters by considering each player's champion preferences and the
interaction between the players. DraftRec consists of two networks: the player
network and the match network. The player network captures the individual
player's champion preference, and the match network integrates the complex
relationship between the players and their respective champions. We train and
evaluate our model from a manually collected 280,000 matches of League of
Legends and a publicly available 50,000 matches of Dota2. Empirically, our
method achieved state-of-the-art performance in character recommendation and
match outcome prediction task. Furthermore, a comprehensive user survey
confirms that DraftRec provides convincing and satisfying recommendations. Our
code and dataset are available at https://github.com/dojeon-ai/DraftRec."
Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments,0.223418,"Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement."
PIC4rl-gym: a ROS2 modular framework for Robots Autonomous Navigation with Deep Reinforcement Learning,0.240054,"Learning agents can optimize standard autonomous navigation improving
flexibility, efficiency, and computational cost of the system by adopting a
wide variety of approaches. This work introduces the \textit{PIC4rl-gym}, a
fundamental modular framework to enhance navigation and learning research by
mixing ROS2 and Gazebo, the standard tools of the robotics community, with Deep
Reinforcement Learning (DRL). The paper describes the whole structure of the
PIC4rl-gym, which fully integrates DRL agent's training and testing in several
indoor and outdoor navigation scenarios and tasks. A modular approach is
adopted to easily customize the simulation by selecting new platforms, sensors,
or models. We demonstrate the potential of our novel gym by benchmarking the
resulting policies, trained for different navigation tasks, with a complete set
of metrics."
Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches,0.251237,"Deep learning has substantially boosted the performance of Monocular Depth
Estimation (MDE), a critical component in fully vision-based autonomous driving
(AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack
against learning-based MDE. In particular, we use an optimization-based method
to systematically generate stealthy physical-object-oriented adversarial
patches to attack depth estimation. We balance the stealth and effectiveness of
our attack with object-oriented adversarial design, sensitive region
localization, and natural style camouflage. Using real-world driving scenarios,
we evaluate our attack on concurrent MDE models and a representative downstream
task for AD (i.e., 3D object detection). Experimental results show that our
method can generate stealthy, effective, and robust adversarial patches for
different target objects and models and achieves more than 6 meters mean depth
estimation error and 93% attack success rate (ASR) in object detection with a
patch of 1/9 of the vehicle's rear area. Field tests on three different driving
routes with a real vehicle indicate that we cause over 6 meters mean depth
estimation error and reduce the object detection rate from 90.70% to 5.16% in
continuous video frames."
Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,0.252805,"Previous work has shown that there exists a scaling law between the size of
Language Models (LMs) and their zero-shot performance on different downstream
NLP tasks. In this work, we show that this phenomenon does not hold when
evaluating large LMs on tasks with negated prompts, but instead shows an
inverse scaling law. We evaluate 9 different tasks with negated prompts on (1)
pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further
pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with
few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all
LM types perform worse on negated prompts as they scale and show a huge
performance gap between the human performance when comparing the average score
on both original and negated prompts. By highlighting a critical limitation of
existing LMs and methods, we urge the community to develop new approaches of
developing LMs that actually follow the given instructions. We provide the code
and the datasets to explore negated prompts at
https://github.com/joeljang/negated-prompts-for-llms"
Efficient Long Sequence Modeling via State Space Augmented Transformer,0.230885,"Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks."
On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry,0.257,"We introduce a framework of the equivariant convolutional algorithms which is
tailored for a number of machine-learning tasks on physical systems with
arbitrary SU($d$) symmetries. It allows us to enhance a natural model of
quantum computation--permutational quantum computing (PQC) [Quantum Inf.
Comput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. While
PQC was shown to be effectively classically simulatable, we exhibit a problem
which can be efficiently solved on PQC+ machine, whereas the best known
classical algorithms runs in $O(n!n^2)$ time, thus providing strong evidence
against PQC+ being classically simulatable. We further discuss practical
quantum machine learning algorithms which can be carried out in the paradigm of
PQC+."
Learning to Revise References for Faithful Summarization,0.220904,"In real-world scenarios with naturally occurring datasets, reference
summaries are noisy and may contain information that cannot be inferred from
the source text. On large news corpora, removing low quality samples has been
shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,
filtering is detrimental to performance. To improve reference quality while
retaining all data, we propose a new approach: to selectively re-write
unsupported reference sentences to better reflect source data. We automatically
generate a synthetic dataset of positive and negative revisions by corrupting
supported sentences and learn to revise reference sentences with contrastive
learning. The intensity of revisions is treated as a controllable attribute so
that, at inference, diverse candidates can be over-generated-then-rescored to
balance faithfulness and abstraction. To test our methods, we extract noisy
references from publicly available MIMIC-III discharge summaries for the task
of hospital-course summarization, and vary the data on which models are
trained. According to metrics and human evaluation, models trained on revised
clinical references are much more faithful, informative, and fluent than models
trained on original or filtered data."
ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification,0.230164,"Generating new events given context with correlated ones plays a crucial role
in many event-centric reasoning tasks. Existing works either limit their scope
to specific scenarios or overlook event-level correlations. In this paper, we
propose to pre-train a general Correlation-aware context-to-Event Transformer
(ClarET) for event-centric reasoning. To achieve this, we propose three novel
event-centric objectives, i.e., whole event recovering, contrastive
event-correlation encoding and prompt-based event locating, which highlight
event-level correlations with effective training. The proposed ClarET is
applicable to a wide range of event-centric reasoning scenarios, considering
its versatility of (i) event-correlation types (e.g., causal, temporal,
contrast), (ii) application formulations (i.e., generation and classification),
and (iii) reasoning types (e.g., abductive, counterfactual and ending
reasoning). Empirical fine-tuning results, as well as zero- and few-shot
learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4
reasoning types with diverse event correlations), verify its effectiveness and
generalization ability."
CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization,0.230085,"The quest for seeking health information has swamped the web with consumers'
health-related questions. Generally, consumers use overly descriptive and
peripheral information to express their medical condition or other healthcare
needs, contributing to the challenges of natural language understanding. One
way to address this challenge is to summarize the questions and distill the key
information of the original question. To address this issue, we introduce a new
dataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health
questions and corresponding summaries. The dataset is derived from the
community question-answering forum and therefore provides a valuable resource
for understanding consumer health-related posts on social media. We benchmark
the dataset on multiple state-of-the-art summarization models to show the
effectiveness of the dataset."
Addressing the Challenges of Cross-Lingual Hate Speech Detection,0.212605,"The goal of hate speech detection is to filter negative online content aiming
at certain groups of people. Due to the easy accessibility of social media
platforms it is crucial to protect everyone which requires building hate speech
detection systems for a wide range of languages. However, the available labeled
hate speech datasets are limited making it problematic to build systems for
many languages. In this paper we focus on cross-lingual transfer learning to
support hate speech detection in low-resource languages. We leverage
cross-lingual word embeddings to train our neural network systems on the source
language and apply it to the target language, which lacks labeled examples, and
show that good performance can be achieved. We then incorporate unlabeled
target language data for further model improvements by bootstrapping labels
using an ensemble of different model architectures. Furthermore, we investigate
the issue of label imbalance of hate speech datasets, since the high ratio of
non-hate examples compared to hate examples often leads to low model
performance. We test simple data undersampling and oversampling techniques and
show their effectiveness."
Efficient Visual Tracking via Hierarchical Cross-Attention Transformer,0.261071,"In recent years, target tracking has made great progress in accuracy. This
development is mainly attributed to powerful networks (such as transformers)
and additional modules (such as online update and refinement modules). However,
less attention has been paid to tracking speed. Most state-of-the-art trackers
are satisfied with the real-time speed on powerful GPUs. However, practical
applications necessitate higher requirements for tracking speed, especially
when edge platforms with limited resources are used. In this work, we present
an efficient tracking method via a hierarchical cross-attention transformer
named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on
the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our
HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,
UAV123, and VOT2020. Code and models are available at
https://github.com/chenxin-dlut/HCAT."
TweetNLP: Cutting-Edge Natural Language Processing for Social Media,0.267422,"In this paper we present TweetNLP, an integrated platform for Natural
Language Processing (NLP) in social media. TweetNLP supports a diverse set of
NLP tasks, including generic focus areas such as sentiment analysis and named
entity recognition, as well as social media-specific tasks such as emoji
prediction and offensive language identification. Task-specific systems are
powered by reasonably-sized Transformer-based language models specialized on
social media text (in particular, Twitter) which can be run without the need
for dedicated hardware or cloud services. The main contributions of TweetNLP
are: (1) an integrated Python library for a modern toolkit supporting social
media analysis using our various task-specific models adapted to the social
domain; (2) an interactive online demo for codeless experimentation using our
models; and (3) a tutorial covering a wide variety of typical social media
applications."
RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation,0.297934,"Category-level object pose estimation aims to predict the 6D pose as well as
the 3D metric size of arbitrary objects from a known set of categories. Recent
methods harness shape prior adaptation to map the observed point cloud into the
canonical space and apply Umeyama algorithm to recover the pose and size.
However, their shape prior integration strategy boosts pose estimation
indirectly, which leads to insufficient pose-sensitive feature extraction and
slow inference speed. To tackle this problem, in this paper, we propose a novel
geometry-guided Residual Object Bounding Box Projection network RBP-Pose that
jointly predicts object pose and residual vectors describing the displacements
from the shape-prior-indicated object surface projections on the bounding box
towards the real surface projections. Such definition of residual vectors is
inherently zero-mean and relatively small, and explicitly encapsulates spatial
cues of the 3D object for robust and accurate pose regression. We enforce
geometry-aware consistency terms to align the predicted pose and residual
vectors to further boost performance."
Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance,0.306752,"Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results."
Understanding Iterative Revision from Human-Written Text,0.267414,"Writing is, by nature, a strategic, adaptive, and more importantly, an
iterative process. A crucial part of writing is editing and revising the text.
Previous works on text revision have focused on defining edit intention
taxonomies within a single domain or developing computational models with a
single level of edit granularity, such as sentence-level edits, which differ
from human's revision cycles. This work describes IteraTeR: the first
large-scale, multi-domain, edit-intention annotated corpus of iteratively
revised text. In particular, IteraTeR is collected based on a new framework to
comprehensively model the iterative text revisions that generalize to various
domains of formal writing, edit intentions, revision depths, and granularities.
When we incorporate our annotated edit intentions, both generative and
edit-based text revision models significantly improve automatic evaluations.
Through our work, we better understand the text revision process, making vital
connections between edit intentions and writing quality, enabling the creation
of diverse corpora to support computational modeling of iterative text
revisions."
On Improving Cross-dataset Generalization of Deepfake Detectors,0.303505,"Facial manipulation by deep fake has caused major security risks and raised
severe societal concerns. As a countermeasure, a number of deep fake detection
methods have been proposed recently. Most of them model deep fake detection as
a binary classification problem using a backbone convolutional neural network
(CNN) architecture pretrained for the task. These CNN-based methods have
demonstrated very high efficacy in deep fake detection with the Area under the
Curve (AUC) as high as 0.99. However, the performance of these methods degrades
significantly when evaluated across datasets. In this paper, we formulate deep
fake detection as a hybrid combination of supervised and reinforcement learning
(RL) to improve its cross-dataset generalization performance. The proposed
method chooses the top-k augmentations for each test sample by an RL agent in
an image-specific manner. The classification scores, obtained using CNN, of all
the augmentations of each test image are averaged together for final real or
fake classification. Through extensive experimental validation, we demonstrate
the superiority of our method over existing published research in cross-dataset
generalization of deep fake detectors, thus obtaining state-of-the-art
performance."
Taylor Genetic Programming for Symbolic Regression,0.301101,"Genetic programming (GP) is a commonly used approach to solve symbolic
regression (SR) problems. Compared with the machine learning or deep learning
methods that depend on the pre-defined model and the training dataset for
solving SR problems, GP is more focused on finding the solution in a search
space. Although GP has good performance on large-scale benchmarks, it randomly
transforms individuals to search results without taking advantage of the
characteristics of the dataset. So, the search process of GP is usually slow,
and the final results could be unstable.To guide GP by these characteristics,
we propose a new method for SR, called Taylor genetic programming (TaylorGP)
(Code and appendix at https://kgae-cup.github.io/TaylorGP/). TaylorGP leverages
a Taylor polynomial to approximate the symbolic equation that fits the dataset.
It also utilizes the Taylor polynomial to extract the features of the symbolic
equation: low order polynomial discrimination, variable separability, boundary,
monotonic, and parity. GP is enhanced by these Taylor polynomial techniques.
Experiments are conducted on three kinds of benchmarks: classical SR, machine
learning, and physics. The experimental results show that TaylorGP not only has
higher accuracy than the nine baseline methods, but also is faster in finding
stable results."
Modern Machine-Learning Predictive Models for Diagnosing Infectious Diseases,0.300479,"Controlling infectious diseases is a major health priority because they can
spread and infect humans, thus evolving into epidemics or pandemics. Therefore,
early detection of infectious diseases is a significant need, and many
researchers have developed models to diagnose them in the early stages. This
paper reviewed research articles for recent machine-learning (ML) algorithms
applied to infectious disease diagnosis. We searched the Web of Science,
ScienceDirect, PubMed, Springer, and IEEE databases from 2015 to 2022,
identified the pros and cons of the reviewed ML models, and discussed the
possible recommendations to advance the studies in this field. We found that
most of the articles used small datasets, and few of them used real-time data.
Our results demonstrated that a suitable ML technique depends on the nature of
the dataset and the desired goal."
Zero-Shot Video Captioning with Evolving Pseudo-Tokens,0.281674,"We introduce a zero-shot video captioning method that employs two frozen
networks: the GPT-2 language model and the CLIP image-text matching model. The
matching score is used to steer the language model toward generating a sentence
that has a high average matching score to a subset of the video frames. Unlike
zero-shot image captioning methods, our work considers the entire sentence at
once. This is achieved by optimizing, during the generation process, part of
the prompt from scratch, by modifying the representation of all other tokens in
the prompt, and by repeating the process iteratively, gradually improving the
specificity and comprehensiveness of the generated sentence. Our experiments
show that the generated captions are coherent and display a broad range of
real-world knowledge. Our code is available at:
https://github.com/YoadTew/zero-shot-video-to-text"
UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture,0.297793,"We present UnrealEgo, i.e., a new large-scale naturalistic dataset for
egocentric 3D human pose estimation. UnrealEgo is based on an advanced concept
of eyeglasses equipped with two fisheye cameras that can be used in
unconstrained environments. We design their virtual prototype and attach them
to 3D human models for stereo view capture. We next generate a large corpus of
human motions. As a consequence, UnrealEgo is the first dataset to provide
in-the-wild stereo images with the largest variety of motions among existing
egocentric datasets. Furthermore, we propose a new benchmark method with a
simple but effective idea of devising a 2D keypoint estimation module for
stereo inputs to improve 3D human pose estimation. The extensive experiments
show that our approach outperforms the previous state-of-the-art methods
qualitatively and quantitatively. UnrealEgo and our source codes are available
on our project web page."
Multi-Scale Representation Learning on Proteins,0.311931,"Proteins are fundamental biological entities mediating key roles in cellular
function and disease. This paper introduces a multi-scale graph construction of
a protein -- HoloProt -- connecting surface to structure and sequence. The
surface captures coarser details of the protein, while sequence as primary
component and structure -- comprising secondary and tertiary components --
capture finer details. Our graph encoder then learns a multi-scale
representation by allowing each level to integrate the encoding from level(s)
below with the graph at that level. We test the learned representation on
different tasks, (i.) ligand binding affinity (regression), and (ii.) protein
function prediction (classification). On the regression task, contrary to
previous methods, our model performs consistently and reliably across different
dataset splits, outperforming all baselines on most splits. On the
classification task, it achieves a performance close to the top-performing
model while using 10x fewer parameters. To improve the memory efficiency of our
construction, we segment the multiplex protein surface manifold into molecular
superpixels and substitute the surface with these superpixels at little to no
performance loss."
MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors,0.296033,"In this paper, we propose MOTRv2, a simple yet effective pipeline to
bootstrap end-to-end multi-object tracking with a pretrained object detector.
Existing end-to-end methods, MOTR and TrackFormer are inferior to their
tracking-by-detection counterparts mainly due to their poor detection
performance. We aim to improve MOTR by elegantly incorporating an extra object
detector. We first adopt the anchor formulation of queries and then use an
extra object detector to generate proposals as anchors, providing detection
prior to MOTR. The simple modification greatly eases the conflict between joint
learning detection and association tasks in MOTR. MOTRv2 keeps the query
propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the
1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in
Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on
the BDD100K dataset. We hope this simple and effective pipeline can provide
some new insights to the end-to-end MOT community. Code is available at
\url{https://github.com/megvii-research/MOTRv2}."
Continual Learning with Recursive Gradient Optimization,0.271006,"Learning multiple tasks sequentially without forgetting previous knowledge,
called Continual Learning(CL), remains a long-standing challenge for neural
networks. Most existing methods rely on additional network capacity or data
replay. In contrast, we introduce a novel approach which we refer to as
Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated
optimizer that modifies the gradient to minimize forgetting without data replay
and a virtual Feature Encoding Layer(FEL) that represents different long-term
structures with only task descriptors. Experiments demonstrate that RGO has
significantly better performance on popular continual classification benchmarks
when compared to the baselines and achieves new state-of-the-art performance on
20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher
average accuracy than Single-Task Learning(STL), this method is flexible and
reliable to provide continual learning capabilities for learning models that
rely on gradient descent."
Style Matters! Investigating Linguistic Style in Online Communities,0.276514,"Content has historically been the primary lens used to study language in
online communities. This paper instead focuses on the linguistic style of
communities. While we know that individuals have distinguishable styles, here
we ask whether communities have distinguishable styles. Additionally, while
prior work has relied on a narrow definition of style, we employ a broad
definition involving 262 features to analyze the linguistic style of 9 online
communities from 3 social media platforms discussing politics, television and
travel. We find that communities indeed have distinct styles. Also, style is an
excellent predictor of group membership (F-score 0.952 and Accuracy 96.09%).
While on average it is statistically equivalent to predictions using content
alone, it is more resilient to reductions in training data."
Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,0.271236,"We introduce camouflaged data poisoning attacks, a new attack vector that
arises in the context of machine unlearning and other settings when model
retraining may be induced. An adversary first adds a few carefully crafted
points to the training dataset such that the impact on the model's predictions
is minimal. The adversary subsequently triggers a request to remove a subset of
the introduced points at which point the attack is unleashed and the model's
predictions are negatively affected. In particular, we consider clean-label
targeted attacks (in which the goal is to cause the model to misclassify a
specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.
This attack is realized by constructing camouflage datapoints that mask the
effect of a poisoned dataset."
CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability,0.297121,"Convolutional neural networks (CNNs) have dominated the field of computer
vision for nearly a decade due to their strong ability to learn local features.
However, due to their limited receptive field, CNNs fail to model the global
context. On the other hand, transformer, an attention-based architecture can
model the global context easily. Despite this, there are limited studies that
investigate the effectiveness of transformers in crowd counting. In addition,
the majority of the existing crowd counting methods are based on the regression
of density maps which requires point-level annotation of each person present in
the scene. This annotation task is laborious and also error-prone. This has led
to increased focus on weakly-supervised crowd counting methods which require
only the count-level annotations. In this paper, we propose a weakly-supervised
method for crowd counting using a pyramid vision transformer. We have conducted
extensive evaluations to validate the effectiveness of the proposed method. Our
method is comparable to the state-of-the-art on the benchmark crowd datasets.
More importantly, it shows remarkable generalizability."
Real-time Online Multi-Object Tracking in Compressed Domain,0.271508,"Recent online Multi-Object Tracking (MOT) methods have achieved desirable
tracking performance. However, the tracking speed of most existing methods is
rather slow. Inspired from the fact that the adjacent frames are highly
relevant and redundant, we divide the frames into key and non-key frames
respectively and track objects in the compressed domain. For the key frames,
the RGB images are restored for detection and data association. To make data
association more reliable, an appearance Convolutional Neural Network (CNN)
which can be jointly trained with the detector is proposed. For the non-key
frames, the objects are directly propagated by a tracking CNN based on the
motion information provided in the compressed domain. Compared with the
state-of-the-art online MOT methods,our tracker is about 6x faster while
maintaining a comparable tracking performance."
mGPT: Few-Shot Learners Go Multilingual,0.286719,"Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released."
Transformer based Urdu Handwritten Text Optical Character Reader,0.297542,"Extracting Handwritten text is one of the most important components of
digitizing information and making it available for large scale setting.
Handwriting Optical Character Reader (OCR) is a research problem in computer
vision and natural language processing computing, and a lot of work has been
done for English, but unfortunately, very little work has been done for low
resourced languages such as Urdu. Urdu language script is very difficult
because of its cursive nature and change of shape of characters based on it's
relative position, therefore, a need arises to propose a model which can
understand complex features and generalize it for every kind of handwriting
style. In this work, we propose a transformer based Urdu Handwritten text
extraction model. As transformers have been very successful in Natural Language
Understanding task, we explore them further to understand complex Urdu
Handwriting."
Text normalization for low-resource languages: the case of Ligurian,0.270937,"Text normalization is a crucial technology for low-resource languages which
lack rigid spelling conventions or that have undergone multiple spelling
reforms. Low-resource text normalization has so far relied upon hand-crafted
rules, which are perceived to be more data efficient than neural methods. In
this paper we examine the case of text normalization for Ligurian, an
endangered Romance language. We collect 4,394 Ligurian sentences paired with
their normalized versions, as well as the first open source monolingual corpus
for Ligurian. We show that, in spite of the small amounts of data available, a
compact transformer-based model can be trained to achieve very low error rates
by the use of backtranslation and appropriate tokenization."
e-CARE: a New Dataset for Exploring Explainable Causal Reasoning,0.311227,"Understanding causality has vital importance for various Natural Language
Processing (NLP) applications. Beyond the labeled instances, conceptual
explanations of the causality can provide deep understanding of the causal
facts to facilitate the causal reasoning process. However, such explanation
information still remains absent in existing causal reasoning resources. In
this paper, we fill this gap by presenting a human-annotated explainable CAusal
REasoning dataset (e-CARE), which contains over 21K causal reasoning questions,
together with natural language formed explanations of the causal questions.
Experimental results show that generating valid explanations for causal facts
still remains especially challenging for the state-of-the-art models, and the
explanation information can be helpful for promoting the accuracy and stability
of causal reasoning models."
EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points,0.287269,"Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/."
Point-Level Region Contrast for Object Detection Pre-Training,0.264829,"In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available."
Revisiting Grammatical Error Correction Evaluation and Beyond,0.289992,"Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore
and BARTScore) have been widely used in several sentence generation tasks
(e.g., machine translation and text summarization) due to their better
correlation with human judgments over traditional overlap-based methods.
Although PT-based methods have become the de facto standard for training
grammatical error correction (GEC) systems, GEC evaluation still does not
benefit from pretrained knowledge. This paper takes the first step towards
understanding and improving GEC evaluation with pretraining. We first find that
arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory
correlation results because of the excessive attention to inessential systems
outputs (e.g., unchanged parts). To alleviate the limitation, we propose a
novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2
which only uses PT-based metrics to score those corrected parts. Experimental
results on the CoNLL14 evaluation task show that PT-M2 significantly
outperforms existing methods, achieving a new state-of-the-art result of 0.949
Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate
competitive GEC systems. Source code and scripts are freely available at
https://github.com/pygongnlp/PT-M2."
Universal Conditional Masked Language Pre-training for Neural Machine Translation,0.279802,"Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching & masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low- to extremely high-resource
languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on
average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it
can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the
best of our knowledge, this is the first work to pre-train a unified model for
fine-tuning on both NMT tasks. Code, data, and pre-trained models are available
at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT."
DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation,0.288338,"In many real-world scenarios, we often deal with streaming data that is
sequentially collected over time. Due to the non-stationary nature of the
environment, the streaming data distribution may change in unpredictable ways,
which is known as concept drift. To handle concept drift, previous methods
first detect when/where the concept drift happens and then adapt models to fit
the distribution of the latest data. However, there are still many cases that
some underlying factors of environment evolution are predictable, making it
possible to model the future concept drift trend of the streaming data, while
such cases are not fully explored in previous work.
  In this paper, we propose a novel method DDG-DA, that can effectively
forecast the evolution of data distribution and improve the performance of
models. Specifically, we first train a predictor to estimate the future data
distribution, then leverage it to generate training samples, and finally train
models on the generated data. We conduct experiments on three real-world tasks
(forecasting on stock price trend, electricity load and solar irradiance) and
obtain significant improvement on multiple widely-used models."
Toward More Meaningful Resources for Lower-resourced Languages,0.291206,"In this position paper, we describe our perspective on how meaningful
resources for lower-resourced languages should be developed in connection with
the speakers of those languages. We first examine two massively multilingual
resources in detail. We explore the contents of the names stored in Wikidata
for a few lower-resourced languages and find that many of them are not in fact
in the languages they claim to be and require non-trivial effort to correct. We
discuss quality issues present in WikiAnn and evaluate whether it is a useful
supplement to hand annotated data. We then discuss the importance of creating
annotation for lower-resourced languages in a thoughtful and ethical way that
includes the languages' speakers as part of the development process. We
conclude with recommended guidelines for resource development."
Iterative Scene Graph Generation,0.299179,"The task of scene graph generation entails identifying object entities and
their corresponding interaction predicates in a given image (or video). Due to
the combinatorially large solution space, existing approaches to scene graph
generation assume certain factorization of the joint distribution to make the
estimation feasible (e.g., assuming that objects are conditionally independent
of predicate predictions). However, this fixed factorization is not ideal under
all scenarios (e.g., for images where an object entailed in interaction is
small and not discernible on its own). In this work, we propose a novel
framework for scene graph generation that addresses this limitation, as well as
introduces dynamic conditioning on the image, using message passing in a Markov
Random Field. This is implemented as an iterative refinement procedure wherein
each modification is conditioned on the graph generated in the previous
iteration. This conditioning across refinement steps allows joint reasoning
over entities and relations. This framework is realized via a novel and
end-to-end trainable transformer-based architecture. In addition, the proposed
framework can improve existing approach performance. Through extensive
experiments on Visual Genome and Action Genome benchmark datasets we show
improved performance on the scene graph generation."
IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty,0.343075,"Single image surface normal estimation and depth estimation are closely
related problems as the former can be calculated from the latter. However, the
surface normals computed from the output of depth estimation methods are
significantly less accurate than the surface normals directly estimated by
networks. To reduce such discrepancy, we introduce a novel framework that uses
surface normal and its uncertainty to recurrently refine the predicted
depth-map. The depth of each pixel can be propagated to a query pixel, using
the predicted surface normal as guidance. We thus formulate depth refinement as
a classification of choosing the neighboring pixel to propagate from. Then, by
propagating to sub-pixel points, we upsample the refined, low-resolution
output. The proposed method shows state-of-the-art performance on NYUv2 and
iBims-1 - both in terms of depth and normal. Our refinement module can also be
attached to the existing depth estimation methods to improve their accuracy. We
also show that our framework, only trained for depth estimation, can also be
used for depth completion. The code is available at
https://github.com/baegwangbin/IronDepth."
Intelligent problem-solving as integrated hierarchical reinforcement learning,0.348196,"According to cognitive psychology and related disciplines, the development of
complex problem-solving behaviour in biological agents depends on hierarchical
cognitive mechanisms. Hierarchical reinforcement learning is a promising
computational approach that may eventually yield comparable problem-solving
behaviour in artificial agents and robots. However, to date the problem-solving
abilities of many human and non-human animals are clearly superior to those of
artificial systems. Here, we propose steps to integrate biologically inspired
hierarchical mechanisms to enable advanced problem-solving skills in artificial
agents. Therefore, we first review the literature in cognitive psychology to
highlight the importance of compositional abstraction and predictive
processing. Then we relate the gained insights with contemporary hierarchical
reinforcement learning methods. Interestingly, our results suggest that all
identified cognitive mechanisms have been implemented individually in isolated
computational architectures, raising the question of why there exists no single
unifying architecture that integrates them. As our final contribution, we
address this question by providing an integrative perspective on the
computational challenges to develop such a unifying architecture. We expect our
results to guide the development of more sophisticated cognitively inspired
hierarchical machine learning architectures."
Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,0.355797,"With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI."
A Low-Shot Object Counting Network With Iterative Prototype Adaptation,0.353208,"We consider low-shot counting of arbitrary semantic categories in the image
using only few annotated exemplars (few-shot) or no exemplars (no-shot). The
standard few-shot pipeline follows extraction of appearance queries from
exemplars and matching them with image features to infer the object counts.
Existing methods extract queries by feature pooling which neglects the shape
information (e.g., size and aspect) and leads to a reduced object localization
accuracy and count estimates. We propose a Low-shot Object Counting network
with iterative prototype Adaptation (LOCA). Our main contribution is the new
object prototype extraction module, which iteratively fuses the exemplar shape
and appearance information with image features. The module is easily adapted to
zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot
counting problems. LOCA outperforms all recent state-of-the-art methods on
FSC147 benchmark by 20-30% in RMSE on one-shot and few-shot and achieves
state-of-the-art on zero-shot scenarios, while demonstrating better
generalization capabilities."
Rethinking Performance Gains in Image Dehazing Networks,0.344541,"Image dehazing is an active topic in low-level vision, and many image
dehazing networks have been proposed with the rapid development of deep
learning. Although these networks' pipelines work fine, the key mechanism to
improving image dehazing performance remains unclear. For this reason, we do
not target to propose a dehazing network with fancy modules; rather, we make
minimal modifications to popular U-Net to obtain a compact dehazing network.
Specifically, we swap out the convolutional blocks in U-Net for residual blocks
with the gating mechanism, fuse the feature maps of main paths and skip
connections using the selective kernel, and call the resulting U-Net variant
gUNet. As a result, with a significantly reduced overhead, gUNet is superior to
state-of-the-art methods on multiple image dehazing datasets. Finally, we
verify these key designs to the performance gain of image dehazing networks
through extensive ablation studies."
A Self-Guided Framework for Radiology Report Generation,0.323976,"Automatic radiology report generation is essential to computer-aided
diagnosis. Through the success of image captioning, medical report generation
has been achievable. However, the lack of annotated disease labels is still the
bottleneck of this area. In addition, the image-text data bias problem and
complex sentences make it more difficult to generate accurate reports. To
address these gaps, we pre-sent a self-guided framework (SGF), a suite of
unsupervised and supervised deep learning methods to mimic the process of human
learning and writing. In detail, our framework obtains the domain knowledge
from medical reports with-out extra disease labels and guides itself to extract
fined-grain visual features as-sociated with the text. Moreover, SGF
successfully improves the accuracy and length of medical report generation by
incorporating a similarity comparison mechanism that imitates the process of
human self-improvement through compar-ative practice. Extensive experiments
demonstrate the utility of our SGF in the majority of cases, showing its
superior performance over state-of-the-art meth-ods. Our results highlight the
capacity of the proposed framework to distinguish fined-grained visual details
between words and verify its advantage in generating medical reports."
IoV Scenario: Implementation of a Bandwidth Aware Algorithm in Wireless Network Communication Mode,0.344612,"The wireless network communication mode represented by the Internet of
vehicles (IoV) has been widely used. However, due to the limitations of
traditional network architecture, resource scheduling in wireless network
environment is still facing great challenges. This paper focuses on the
allocation of bandwidth resources in the virtual network environment. This
paper proposes a bandwidth aware multi domain virtual network embedding
algorithm (BA-VNE). The algorithm is mainly aimed at the problem that users
need a lot of bandwidth in wireless communication mode, and solves the problem
of bandwidth resource allocation from the perspective of virtual network
embedding (VNE). In order to improve the performance of the algorithm, we
introduce particle swarm optimization (PSO) algorithm to optimize the
performance of the algorithm. In order to verify the effectiveness of the
algorithm, we have carried out simulation experiments from link bandwidth,
mapping cost and virtual network request (VNR) acceptance rate. The final
results show that the proposed algorithm is better than other representative
algorithms in the above indicators."
Tools and Practices for Responsible AI Engineering,0.32918,"Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs."
Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint,0.344497,"Active learning is a promising alternative to alleviate the issue of high
annotation cost in the computer vision tasks by consciously selecting more
informative samples to label. Active learning for object detection is more
challenging and existing efforts on it are relatively rare. In this paper, we
propose a novel hybrid approach to address this problem, where the
instance-level uncertainty and diversity are jointly considered in a bottom-up
manner. To balance the computational complexity, the proposed approach is
designed as a two-stage procedure. At the first stage, an Entropy-based
Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of
every image, which performs NMS according to the entropy in the feature space
to remove predictions with redundant information gains. At the second stage, a
diverse prototype (DivProto) strategy is explored to ensure the diversity
across images by progressively converting it into the intra-class and
inter-class diversities of the entropy-based class-specific prototypes.
Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed
approach achieves state of the art results and significantly outperforms the
other counterparts, highlighting its superiority."
Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation,0.366949,"Document-level Relation Extraction (DocRE) is a more challenging task
compared to its sentence-level counterpart. It aims to extract relations from
multiple sentences at once. In this paper, we propose a semi-supervised
framework for DocRE with three novel components. Firstly, we use an axial
attention module for learning the interdependency among entity-pairs, which
improves the performance on two-hop relations. Secondly, we propose an adaptive
focal loss to tackle the class imbalance problem of DocRE. Lastly, we use
knowledge distillation to overcome the differences between human annotated data
and distantly supervised data. We conducted experiments on two DocRE datasets.
Our model consistently outperforms strong baselines and its performance exceeds
the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.
Our code and data will be released at https://github.com/tonytan48/KD-DocRE."
Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models,0.343444,"Automatically summarizing patients' main problems from daily progress notes
using natural language processing methods helps to battle against information
and cognitive overload in hospital settings and potentially assists providers
with computerized diagnostic decision support. Problem list summarization
requires a model to understand, abstract, and generate clinical documentation.
In this work, we propose a new NLP task that aims to generate a list of
problems in a patient's daily care plan using input from the provider's
progress notes during hospitalization. We investigate the performance of T5 and
BART, two state-of-the-art seq2seq transformer architectures, in solving this
problem. We provide a corpus built on top of progress notes from publicly
available electronic health record progress notes in the Medical Information
Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain
text, and we experiment with a data augmentation method and a domain adaptation
pre-training method to increase exposure to medical vocabulary and knowledge.
Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence
embedding, and F-score on medical concepts. Results show that T5 with domain
adaptive pre-training achieves significant performance gains compared to a
rule-based system and general domain pre-trained language models, indicating a
promising direction for tackling the problem summarization task."
Learning by Distilling Context,0.318681,"Language models significantly benefit from context tokens, such as prompts or
scratchpads. They perform better when prompted with informative instructions,
and they acquire new reasoning capabilities by generating a scratch-pad before
predicting the final answers. However, they do not \textit{internalize} these
performance gains, which disappear when the context tokens are gone. Our work
proposes to apply context distillation so that a language model can improve
itself by internalizing these gains. Concretely, given a synthetic unlabeled
input for the target task, we condition the model on ``[instructions] +
[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune
the same model to predict its own ``[final answer]'' conditioned on the
``[task-input]'', without seeing the ``[instructions]'' or using the
``[scratch-pad]''.
  We show that context distillation is a general method to train language
models, and it can effectively internalize 3 types of training signals. First,
it can internalize abstract task instructions and explanations, so we can
iteratively update the model parameters with new instructions and overwrite old
ones. Second, it can internalize step-by-step reasoning for complex tasks
(e.g., 8-digit addition), and such a newly acquired capability proves to be
useful for other downstream tasks. Finally, it can internalize concrete
training examples, and it outperforms directly learning with gradient descent
by 9\% on the SPIDER Text-to-SQL dataset; furthermore, combining context
distillation operations can internalize more training examples than the context
window size allows."
Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition,0.351796,"This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech
Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio
Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech
with a help of audio-visual correspondence. The proposed V-CAFE is designed to
capture the transition of lip movements, namely visual context and to generate
a noise reduction mask by considering the obtained visual context. Through
context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be
refined for mask generation. The noisy representations are masked out with the
noise reduction mask resulting in enhanced audio features. The enhanced audio
features are fused with the visual features and taken to an encoder-decoder
model composed of Conformer and Transformer for speech recognition. We show the
proposed end-to-end AVSR with the V-CAFE can further improve the
noise-robustness of AVSR. The effectiveness of the proposed method is evaluated
in noisy speech recognition and overlapped speech recognition experiments using
the two largest audio-visual datasets, LRS2 and LRS3."
An Online Semantic Mapping System for Extending and Enhancing Visual SLAM,0.323573,"We present a real-time semantic mapping approach for mobile vision systems
with a 2D to 3D object detection pipeline and rapid data association for
generated landmarks. Besides the semantic map enrichment the associated
detections are further introduced as semantic constraints into a simultaneous
localization and mapping (SLAM) system for pose correction purposes. This way,
we are able generate additional meaningful information that allows to achieve
higher-level tasks, while simultaneously leveraging the view-invariance of
object detections to improve the accuracy and the robustness of the odometry
estimation. We propose tracklets of locally associated object observations to
handle ambiguous and false predictions and an uncertainty-based greedy
association scheme for an accelerated processing time. Our system reaches
real-time capabilities with an average iteration duration of 65~ms and is able
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a
public dataset. Additionally, we implemented our approach as a modular ROS
package that makes it straightforward for integration in arbitrary graph-based
SLAM methods."
Image-based Automatic Dial Meter Reading in Unconstrained Scenarios,0.320399,"The replacement of analog meters with smart meters is costly, laborious, and
far from complete in developing countries. The Energy Company of Parana (Copel)
(Brazil) performs more than 4 million meter readings (almost entirely of
non-smart devices) per month, and we estimate that 850 thousand of them are
from dial meters. Therefore, an image-based automatic reading system can reduce
human errors, create a proof of reading, and enable the customers to perform
the reading themselves through a mobile application. We propose novel
approaches for Automatic Dial Meter Reading (ADMR) and introduce a new dataset
for ADMR in unconstrained scenarios, called UFPR-ADMR-v2. Our best-performing
method combines YOLOv4 with a novel regression approach (AngReg), and explores
several postprocessing techniques. Compared to previous works, it decreased the
Mean Absolute Error (MAE) from 1,343 to 129 and achieved a meter recognition
rate (MRR) of 98.90% -- with an error tolerance of 1 Kilowatt-hour (kWh)."
NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages,0.323579,"Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages."
OptG: Optimizing Gradient-driven Criteria in Network Sparsity,0.32968,"Network sparsity receives popularity mostly due to its capability to reduce
the network complexity. Extensive studies excavate gradient-driven sparsity.
Typically, these methods are constructed upon premise of weight independence,
which however, is contrary to the fact that weights are mutually influenced.
Thus, their performance remains to be improved. In this paper, we propose to
optimize gradient-driven sparsity (OptG) by solving this independence paradox.
Our motive comes from the recent advances in supermask training which shows
that high-performing sparse subnetworks can be located by simply updating mask
values without modifying any weight. We prove that supermask training is to
accumulate the criteria of gradient-driven sparsity for both removed and
preserved weights, and it can partly solve the independence paradox.
Consequently, OptG integrates supermask training into gradient-driven sparsity,
and a novel supermask optimizer is further proposed to comprehensively mitigate
the independence paradox. Experiments show that OptG can well surpass many
existing state-of-the-art competitors, especially at ultra-high sparsity
levels. Our code is available at \url{https://github.com/zyxxmu/OptG}."
SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer,0.346647,"Point cloud completion has become increasingly popular among generation tasks
of 3D point clouds, as it is a challenging yet indispensable problem to recover
the complete shape of a 3D object from its partial observation. In this paper,
we propose a novel SeedFormer to improve the ability of detail preservation and
recovery in point cloud completion. Unlike previous methods based on a global
feature vector, we introduce a new shape representation, namely Patch Seeds,
which not only captures general structures from partial inputs but also
preserves regional information of local patterns. Then, by integrating seed
features into the generation process, we can recover faithful details for
complete point clouds in a coarse-to-fine manner. Moreover, we devise an
Upsample Transformer by extending the transformer structure into basic
operations of point generators, which effectively incorporates spatial and
semantic relationships between neighboring points. Qualitative and quantitative
evaluations demonstrate that our method outperforms state-of-the-art completion
networks on several benchmark datasets. Our code is available at
https://github.com/hrzhou2/seedformer."
Aggregate effects of advertising decisions: a complex systems look at search engine advertising via an experimental study,0.365903,"Purpose: We model group advertising decisions, which are the collective
decisions of every single advertiser within the set of advertisers who are
competing in the same auction or vertical industry, and examine resulting
market outcomes, via a proposed simulation framework named EXP-SEA
(Experimental Platform for Search Engine Advertising) supporting experimental
studies of collective behaviors in the context of search engine advertising.
Design: We implement the EXP-SEA to validate the proposed simulation framework,
also conduct three experimental studies on the aggregate impact of electronic
word-of-mouth, the competition level, and strategic bidding behaviors. EXP-SEA
supports heterogeneous participants, various auction mechanisms, and also
ranking and pricing algorithms. Findings: Findings from our three experiments
show that (a) both the market profit and advertising indexes such as number of
impressions and number of clicks are larger when the eWOM effect presents,
meaning social media certainly has some effect on search engine advertising
outcomes, (b) the competition level has a monotonic increasing effect on the
market performance, thus search engines have an incentive to encourage both the
eWOM among search users and competition among advertisers, and (c) given the
market-level effect of the percentage of advertisers employing a dynamic greedy
bidding strategy, there is a cut-off point for strategic bidding behaviors.
Originality: This is one of the first research works to explore collective
group decisions and resulting phenomena in the complex context of search engine
advertising via developing and validating a simulation framework that supports
assessments of various advertising strategies and estimations of the impact of
mechanisms on the search market."
Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,0.361327,"Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small."
Detector-Free Weakly Supervised Group Activity Recognition,0.345982,"Group activity recognition is the task of understanding the activity
conducted by a group of people as a whole in a multi-person video. Existing
models for this task are often impractical in that they demand ground-truth
bounding box labels of actors even in testing or rely on off-the-shelf object
detectors. Motivated by this, we propose a novel model for group activity
recognition that depends neither on bounding box labels nor on object detector.
Our model based on Transformer localizes and encodes partial contexts of a
group activity by leveraging the attention mechanism, and represents a video
clip as a set of partial context embeddings. The embedding vectors are then
aggregated to form a single group representation that reflects the entire
context of an activity while capturing temporal evolution of each partial
context. Our method achieves outstanding performance on two benchmarks,
Volleyball and NBA datasets, surpassing not only the state of the art trained
with the same level of supervision, but also some of existing models relying on
stronger supervision."
Attentive Dual Stream Siamese U-net for Flood Detection on Multi-temporal Sentinel-1 Data,0.331598,"Due to climate and land-use change, natural disasters such as flooding have
been increasing in recent years. Timely and reliable flood detection and
mapping can help emergency response and disaster management. In this work, we
propose a flood detection network using bi-temporal SAR acquisitions. The
proposed segmentation network has an encoder-decoder architecture with two
Siamese encoders for pre and post-flood images. The network's feature maps are
fused and enhanced using attention blocks to achieve more accurate detection of
the flooded areas. Our proposed network is evaluated on publicly available
Sen1Flood11 benchmark dataset. The network outperformed the existing
state-of-the-art (uni-temporal) flood detection method by 6\% IOU. The
experiments highlight that the combination of bi-temporal SAR data with an
effective network architecture achieves more accurate flood detection than
uni-temporal methods."
Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion,0.347329,"Knowledge Graph Completion (KGC) has been recently extended to multiple
knowledge graph (KG) structures, initiating new research directions, e.g.
static KGC, temporal KGC and few-shot KGC. Previous works often design KGC
models closely coupled with specific graph structures, which inevitably results
in two drawbacks: 1) structure-specific KGC models are mutually incompatible;
2) existing KGC methods are not adaptable to emerging KGs. In this paper, we
propose KG-S2S, a Seq2Seq generative framework that could tackle different
verbalizable graph structures by unifying the representation of KG facts into
""flat"" text, regardless of their original form. To remedy the KG structure
information loss from the ""flat"" text, we further improve the input
representations of entities and relations, and the inference algorithm in
KG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many
competitive baselines, setting new state-of-the-art performance. Finally, we
analyze KG-S2S's ability on the different relations and the Non-entity
Generations."
Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,0.343467,"Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research."
Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss,0.336553,"We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the facial video PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost."
Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022,0.334039,"In this work, we present the winning solution for ORBIT Few-Shot Video Object
Recognition Challenge 2022. Built upon the ProtoNet baseline, the performance
of our method is improved with three effective techniques. These techniques
include the embedding adaptation, the uniform video clip sampler and the
invalid frame detection. In addition, we re-factor and re-implement the
official codebase to encourage modularity, compatibility and improved
performance. Our implementation accelerates the data loading in both training
and testing."
Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models,0.317428,"We present a novel way of conditioning a pretrained denoising diffusion
speech model to produce speech in the voice of a novel person unseen during
training. The method requires a short (~3 seconds) sample from the target
person, and generation is steered at inference time, without any training
steps. At the heart of the method lies a sampling process that combines the
estimation of the denoising model with a low-pass version of the new speaker's
sample. The objective and subjective evaluations show that our sampling method
can generate a voice similar to that of the target speaker in terms of
frequency, with an accuracy comparable to state-of-the-art methods, and without
training."
Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation,0.375568,"Federated learning (FL) can be essential in knowledge representation,
reasoning, and data mining applications over multi-source knowledge graphs
(KGs). A recent study FedE first proposes an FL framework that shares entity
embeddings of KGs across all clients. However, entity embedding sharing from
FedE would incur a severe privacy leakage. Specifically, the known entity
embedding can be used to infer whether a specific relation between two entities
exists in a private client. In this paper, we introduce a novel attack method
that aims to recover the original data based on the embedding information,
which is further used to evaluate the vulnerabilities of FedE. Furthermore, we
propose a Federated learning paradigm with privacy-preserving Relation
embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,
relation embedding sharing can significantly reduce the communication cost due
to its smaller size of queries. We conduct extensive experiments to evaluate
FedR with five different KG embedding models and three datasets. Compared to
FedE, FedR achieves similar utility and significant improvements regarding
privacy-preserving effect and communication efficiency on the link prediction
task."
Soft Diffusion: Score Matching for General Corruptions,0.392099,"We define a broader family of corruption processes that generalizes
previously known diffusion models. To reverse these general diffusions, we
propose a new objective called Soft Score Matching that provably learns the
score function for any linear corruption process and yields state of the art
results for CelebA. Soft Score Matching incorporates the degradation process in
the network. Our new loss trains the model to predict a clean image,
\textit{that after corruption}, matches the diffused observation. We show that
our objective learns the gradient of the likelihood under suitable regularity
conditions for a family of corruption processes. We further develop a
principled way to select the corruption levels for general diffusion processes
and a novel sampling method that we call Momentum Sampler. We show
experimentally that our framework works for general linear corruption
processes, such as Gaussian blur and masking. We achieve state-of-the-art FID
score $1.85$ on CelebA-64, outperforming all previous linear diffusion models.
We also show significant computational benefits compared to vanilla denoising
diffusion."
Practical Exposure Correction: Great Truths Are Always Simple,0.415105,"Improving the visual quality of the given degraded observation by correcting
exposure level is a fundamental task in the computer vision community. Existing
works commonly lack adaptability towards unknown scenes because of the
data-driven patterns (deep networks) and limited regularization (traditional
optimization), and they usually need time-consuming inference. These two points
heavily limit their practicability. In this paper, we establish a Practical
Exposure Corrector (PEC) that assembles the characteristics of efficiency and
performance. To be concrete, we rethink the exposure correction to provide a
linear solution with exposure-sensitive compensation. Around generating the
compensation, we introduce an exposure adversarial function as the key engine
to fully extract valuable information from the observation. By applying the
defined function, we construct a segmented shrinkage iterative scheme to
generate the desired compensation. Its shrinkage nature supplies powerful
support for algorithmic stability and robustness. Extensive experimental
evaluations fully reveal the superiority of our proposed PEC. The code is
available at https://rsliu.tech/PEC."
Offline RL Policies Should be Trained to be Adaptive,0.395708,"Offline RL algorithms must account for the fact that the dataset they are
provided may leave many facets of the environment unknown. The most common way
to approach this challenge is to employ pessimistic or conservative methods,
which avoid behaviors that are too dissimilar from those in the training
dataset. However, relying exclusively on conservatism has drawbacks:
performance is sensitive to the exact degree of conservatism, and conservative
objectives can recover highly suboptimal policies. In this work, we propose
that offline RL methods should instead be adaptive in the presence of
uncertainty. We show that acting optimally in offline RL in a Bayesian sense
involves solving an implicit POMDP. As a result, optimal policies for offline
RL must be adaptive, depending not just on the current state but rather all the
transitions seen so far during evaluation.We present a model-free algorithm for
approximating this optimal adaptive policy, and demonstrate the efficacy of
learning such adaptive policies in offline RL benchmarks."
MagicPony: Learning Articulated 3D Animals in the Wild,0.406881,"We consider the problem of predicting the 3D shape, articulation, viewpoint,
texture, and lighting of an articulated animal like a horse given a single test
image as input. We present a new method, dubbed MagicPony, that learns this
predictor purely from in-the-wild single-view images of the object category,
with minimal assumptions about the topology of deformation. At its core is an
implicit-explicit representation of articulated shape and appearance, combining
the strengths of neural fields and meshes. In order to help the model
understand an object's shape and pose, we distil the knowledge captured by an
off-the-shelf self-supervised vision transformer and fuse it into the 3D model.
To overcome local optima in viewpoint estimation, we further introduce a new
viewpoint sampling scheme that comes at no additional training cost. MagicPony
outperforms prior work on this challenging task and demonstrates excellent
generalisation in reconstructing art, despite the fact that it is only trained
on real images."
Fine-Grained Object Classification via Self-Supervised Pose Alignment,0.418873,"Semantic patterns of fine-grained objects are determined by subtle appearance
difference of local parts, which thus inspires a number of part-based methods.
However, due to uncontrollable object poses in images, distinctive details
carried by local regions can be spatially distributed or even self-occluded,
leading to a large variation on object representation. For discounting pose
variations, this paper proposes to learn a novel graph based object
representation to reveal a global configuration of local parts for
self-supervised pose alignment across classes, which is employed as an
auxiliary feature regularization on a deep representation learning
network.Moreover, a coarse-to-fine supervision together with the proposed
pose-insensitive constraint on shallow-to-deep sub-networks encourages
discriminative features in a curriculum learning manner. We evaluate our method
on three popular fine-grained object classification benchmarks, consistently
achieving the state-of-the-art performance. Source codes are available at
https://github.com/yangxh11/P2P-Net."
Models and Datasets for Cross-Lingual Summarisation,0.388606,"We present a cross-lingual summarisation corpus with long documents in a
source language associated with multi-sentence summaries in a target language.
The corpus covers twelve language pairs and directions for four European
languages, namely Czech, English, French and German, and the methodology for
its creation can be applied to several other languages. We derive cross-lingual
document-summary instances from Wikipedia by combining lead paragraphs and
articles' bodies from language aligned Wikipedia titles. We analyse the
proposed cross-lingual summarisation task with automatic metrics and validate
it with a human study. To illustrate the utility of our dataset we report
experiments with multi-lingual pre-trained models in supervised, zero- and
few-shot, and out-of-domain scenarios."
Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,0.412992,"Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for applying a
pre-trained language model to a new task: fine-tuning all parameters, prompt
tuning, and in-context learning. We observe that fine-tuning generally has flat
or negative scaling curves on out-of-distribution compositional generalization
in semantic parsing evaluations. In-context learning has positive scaling
curves, but is generally outperformed by much smaller fine-tuned models.
Prompt-tuning can outperform fine-tuning, suggesting further potential
improvements from scaling as it exhibits a more positive scaling curve.
Additionally, we identify several error trends that vary with model scale. For
example, larger models are generally better at modeling the syntax of the
output space, but are also more prone to certain types of overfitting. Overall,
our study highlights limitations of current techniques for effectively
leveraging model scale for compositional generalization, while our analysis
also suggests promising directions for future work."
QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars,0.401363,"Real-time tracking of human body motion is crucial for interactive and
immersive experiences in AR/VR. However, very limited sensor data about the
body is available from standalone wearable devices such as HMDs (Head Mounted
Devices) or AR glasses. In this work, we present a reinforcement learning
framework that takes in sparse signals from an HMD and two controllers, and
simulates plausible and physically valid full body motions. Using high quality
full body motion as dense supervision during training, a simple policy network
can learn to output appropriate torques for the character to balance, walk, and
jog, while closely following the input signals. Our results demonstrate
surprisingly similar leg motions to ground truth without any observations of
the lower body, even when the input is only the 6D transformations of the HMD.
We also show that a single policy can be robust to diverse locomotion styles,
different body sizes, and novel environments."
Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings,0.390131,"Automatic depression detection on Twitter can help individuals privately and
conveniently understand their mental health status in the early stages before
seeing mental health professionals. Most existing black-box-like deep learning
methods for depression detection largely focused on improving classification
performance. However, explaining model decisions is imperative in health
research because decision-making can often be high-stakes and life-and-death.
Reliable automatic diagnosis of mental health problems including depression
should be supported by credible explanations justifying models' predictions. In
this work, we propose a novel explainable model for depression detection on
Twitter. It comprises a novel encoder combining hierarchical attention
mechanisms and feed-forward neural networks. To support psycholinguistic
studies, our model leverages metaphorical concept mappings as input. Thus, it
not only detects depressed individuals, but also identifies features of such
users' tweets and associated metaphor concept mappings."
Quark: Controllable Text Generation with Reinforced Unlearning,0.382432,"Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives."
Voxel Field Fusion for 3D Object Detection,0.370128,"In this work, we present a conceptually simple yet effective framework for
cross-modality 3D object detection, named voxel field fusion. The proposed
approach aims to maintain cross-modality consistency by representing and fusing
augmented image features as a ray in the voxel field. To this end, the
learnable sampler is first designed to sample vital features from the image
plane that are projected to the voxel grid in a point-to-ray manner, which
maintains the consistency in feature representation with spatial context. In
addition, ray-wise fusion is conducted to fuse features with the supplemental
context in the constructed voxel field. We further develop mixed augmentor to
align feature-variant transformations, which bridges the modality gap in data
augmentation. The proposed framework is demonstrated to achieve consistent
gains in various benchmarks and outperforms previous fusion-based methods on
KITTI and nuScenes datasets. Code is made available at
https://github.com/dvlab-research/VFF."
EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance,0.401518,"Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process."
Consent as a Foundation for Responsible Autonomy,0.390681,"This paper focuses on a dynamic aspect of responsible autonomy, namely, to
make intelligent agents be responsible at run time. That is, it considers
settings where decision making by agents impinges upon the outcomes perceived
by other agents. For an agent to act responsibly, it must accommodate the
desires and other attitudes of its users and, through other agents, of their
users.
  The contribution of this paper is twofold. First, it provides a conceptual
analysis of consent, its benefits and misuses, and how understanding consent
can help achieve responsible autonomy. Second, it outlines challenges for AI
(in particular, for agents and multiagent systems) that merit investigation to
form as a basis for modeling consent in multiagent systems and applying consent
to achieve responsible autonomy."
Region-Aware Face Swapping,0.395715,"This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$."
User-Controllable Latent Transformer for StyleGAN Image Layout Editing,0.381482,"Latent space exploration is a technique that discovers interpretable latent
directions and manipulates latent codes to edit various attributes in images
generated by generative adversarial networks (GANs). However, in previous work,
spatial control is limited to simple transformations (e.g., translation and
rotation), and it is laborious to identify appropriate latent directions and
adjust their parameters. In this paper, we tackle the problem of editing the
StyleGAN image layout by annotating the image directly. To do so, we propose an
interactive framework for manipulating latent codes in accordance with the user
inputs. In our framework, the user annotates a StyleGAN image with locations
they want to move or not and specifies a movement direction by mouse dragging.
From these user inputs and initial latent codes, our latent transformer based
on a transformer encoder-decoder architecture estimates the output latent
codes, which are fed to the StyleGAN generator to obtain a result image. To
train our latent transformer, we utilize synthetic data and pseudo-user inputs
generated by off-the-shelf StyleGAN and optical flow models, without manual
supervision. Quantitative and qualitative evaluations demonstrate the
effectiveness of our method over existing methods."
Learning State-Aware Visual Representations from Audible Interactions,0.402428,"We propose a self-supervised algorithm to learn representations from
egocentric video data. Recently, significant efforts have been made to capture
humans interacting with their own environments as they go about their daily
activities. In result, several large egocentric datasets of interaction-rich
multi-modal data have emerged. However, learning representations from videos
can be challenging. First, given the uncurated nature of long-form continuous
videos, learning effective representations require focusing on moments in time
when interactions take place. Second, visual representations of daily
activities should be sensitive to changes in the state of the environment.
However, current successful multi-modal learning frameworks encourage
representation invariance over time. To address these challenges, we leverage
audio signals to identify moments of likely interactions which are conducive to
better learning. We also propose a novel self-supervised objective that learns
from audible state changes caused by interactions. We validate these
contributions extensively on two large-scale egocentric datasets,
EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on
several downstream tasks, including action recognition, long-term action
anticipation, and object state change classification."
"Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction",0.418725,"Motion prediction is highly relevant to the perception of dynamic objects and
static map elements in the scenarios of autonomous driving. In this work, we
propose PIP, the first end-to-end Transformer-based framework which jointly and
interactively performs online mapping, object detection and motion prediction.
PIP leverages map queries, agent queries and mode queries to encode the
instance-wise information of map elements, agents and motion intentions,
respectively. Based on the unified query representation, a differentiable
multi-task interaction scheme is proposed to exploit the correlation between
perception and prediction. Even without human-annotated HD map or agent's
historical tracking trajectory as guidance information, PIP realizes end-to-end
multi-agent motion prediction and achieves better performance than
tracking-based and HD-map-based methods. PIP provides comprehensive high-level
information of the driving scene (vectorized static map and dynamic objects
with motion information), and contributes to the downstream planning and
control. Code and models will be released for facilitating further research."
Online Continual Learning for Embedded Devices,0.396486,"Real-time on-device continual learning is needed for new applications such as
home robots, user personalization on smartphones, and augmented/virtual reality
headsets. However, this setting poses unique challenges: embedded devices have
limited memory and compute capacity and conventional machine learning models
suffer from catastrophic forgetting when updated on non-stationary data
streams. While several online continual learning models have been developed,
their effectiveness for embedded applications has not been rigorously studied.
In this paper, we first identify criteria that online continual learners must
meet to effectively perform real-time, on-device learning. We then study the
efficacy of several online continual learning methods when used with mobile
neural networks. We measure their performance, memory usage, compute
requirements, and ability to generalize to out-of-domain inputs."
Using Deep Mixture-of-Experts to Detect Word Meaning Shift for TempoWiC,0.416355,"This paper mainly describes the dma submission to the TempoWiC task, which
achieves a macro-F1 score of 77.05% and attains the first place in this task.
We first explore the impact of different pre-trained language models. Then we
adopt data cleaning, data augmentation, and adversarial training strategies to
enhance the model generalization and robustness. For further improvement, we
integrate POS information and word semantic representation using a
Mixture-of-Experts (MoE) approach. The experimental results show that MoE can
overcome the feature overuse issue and combine the context, POS, and word
semantic features well. Additionally, we use a model ensemble method for the
final prediction, which has been proven effective by many research works."
On the Effects of Image Quality Degradation on Minutiae- and Ridge-Based Automatic Fingerprint Recognition,0.398929,"The effect of image quality degradation on the verification performance of
automatic fingerprint recognition is investigated. We study the performance of
two fingerprint matchers based on minutiae and ridge information under varying
fingerprint image quality. The ridge-based system is found to be more robust to
image quality degradation than the minutiae-based system for a number of
different image quality criteria."
Audio-Adaptive Activity Recognition Across Video Domains,0.385063,"This paper strives for activity recognition under domain shift, for example
caused by change of scenery or camera viewpoint. The leading approaches reduce
the shift in activity appearance by adversarial training and self-supervised
learning. Different from these vision-focused works we leverage activity sounds
for domain adaptation as they have less variance across domains and can
reliably indicate which activities are not happening. We propose an
audio-adaptive encoder and associated learning methods that discriminatively
adjust the visual feature representation as well as addressing shifts in the
semantic distribution. To further eliminate domain-specific features and
include domain-invariant activity sounds for recognition, an audio-infused
recognizer is proposed, which effectively models the cross-modal interaction
across domains. We also introduce the new task of actor shift, with a
corresponding audio-visual dataset, to challenge our method with situations
where the activity appearance changes dramatically. Experiments on this
dataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach."
Describing Differences between Text Distributions with Natural Language,0.39926,"How do two distributions of texts differ? Humans are slow at answering this,
since discovering patterns might require tediously reading through hundreds of
samples. We propose to automatically summarize the differences by ""learning a
natural language hypothesis"": given two distributions $D_{0}$ and $D_{1}$, we
search for a description that is more often true for $D_{1}$, e.g., ""is
military-related."" To tackle this problem, we fine-tune GPT-3 to propose
descriptions with the prompt: ""[samples of $D_{0}$] + [samples of $D_{1}$] +
the difference between them is_____."" We then re-rank the descriptions by
checking how often they hold on a larger set of samples with a learned
verifier. On a benchmark of 54 real-world binary classification tasks, while
GPT-3 Curie (13B) only generates a description similar to human annotation 7%
of the time, the performance reaches 61% with fine-tuning and re-ranking, and
our best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to
describe distribution shifts, debug dataset shortcuts, summarize unknown tasks,
and label text clusters, and present analyses based on automatically generated
descriptions."
Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction,0.419326,"Compared with traditional sentence-level relation extraction, document-level
relation extraction is a more challenging task where an entity in a document
may be mentioned multiple times and associated with multiple relations.
However, most methods of document-level relation extraction do not distinguish
between mention-level features and entity-level features, and just apply simple
pooling operation for aggregating mention-level features into entity-level
features. As a result, the distinct semantics between the different mentions of
an entity are overlooked. To address this problem, we propose RSMAN in this
paper which performs selective attentions over different entity mentions with
respect to candidate relations. In this manner, the flexible and
relation-specific representations of entities are obtained which indeed benefit
relation classification. Our extensive experiments upon two benchmark datasets
show that our RSMAN can bring significant improvements for some backbone models
to achieve state-of-the-art performance, especially when an entity have
multiple mentions in the document."
Open Vocabulary Extreme Classification Using Generative Models,0.402445,"The extreme multi-label classification (XMC) task aims at tagging content
with a subset of labels from an extremely large label set. The label vocabulary
is typically defined in advance by domain experts and assumed to capture all
necessary tags. However in real world scenarios this label set, although large,
is often incomplete and experts frequently need to refine it. To develop
systems that simplify this process, we introduce the task of open vocabulary
XMC (OXMC): given a piece of content, predict a set of labels, some of which
may be outside of the known tag set. Hence, in addition to not having training
data for some labels - as is the case in zero-shot classification - models need
to invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model
for OXMC that generates the set of labels as a flat sequence and is trained
using a novel loss independent of predicted label order. We show the efficacy
of the approach, experimenting with popular XMC datasets for which GROOV is
able to predict meaningful labels outside the given vocabulary while performing
on par with state-of-the-art solutions for known labels."
The Quest for a Common Model of the Intelligent Decision Maker,0.373907,"The premise of the Multi-disciplinary Conference on Reinforcement Learning
and Decision Making is that multiple disciplines share an interest in
goal-directed decision making over time. The idea of this paper is to sharpen
and deepen this premise by proposing a perspective on the decision maker that
is substantive and widely held across psychology, artificial intelligence,
economics, control theory, and neuroscience, which I call the ""common model of
the intelligent agent"". The common model does not include anything specific to
any organism, world, or application domain. The common model does include
aspects of the decision maker's interaction with its world (there must be input
and output, and a goal) and internal components of the decision maker (for
perception, decision-making, internal evaluation, and a world model). I
identify these aspects and components, note that they are given different names
in different disciplines but refer essentially to the same ideas, and discuss
the challenges and benefits of devising a neutral terminology that can be used
across disciplines. It is time to recognize and build on the convergence of
multiple diverse disciplines on a substantive common model of the intelligent
agent."
Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,0.381437,"While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method."
IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes,0.446895,"Indoor scenes exhibit significant appearance variations due to myriad
interactions between arbitrarily diverse object shapes, spatially-changing
materials, and complex lighting. Shadows, highlights, and inter-reflections
caused by visible and invisible light sources require reasoning about
long-range interactions for inverse rendering, which seeks to recover the
components of image formation, namely, shape, material, and lighting. In this
work, our intuition is that the long-range attention learned by transformer
architectures is ideally suited to solve longstanding challenges in
single-image inverse rendering. We demonstrate with a specific instantiation of
a dense vision transformer, IRISformer, that excels at both single-task and
multi-task reasoning required for inverse rendering. Specifically, we propose a
transformer architecture to simultaneously estimate depths, normals,
spatially-varying albedo, roughness and lighting from a single image of an
indoor scene. Our extensive evaluations on benchmark datasets demonstrate
state-of-the-art results on each of the above tasks, enabling applications like
object insertion and material editing in a single unconstrained real image,
with greater photorealism than prior works. Code and data are publicly released
at https://github.com/ViLab-UCSD/IRISformer."
C-VTON: Context-Driven Image-Based Virtual Try-On Network,0.468031,"Image-based virtual try-on techniques have shown great promise for enhancing
the user-experience and improving customer satisfaction on fashion-oriented
e-commerce platforms. However, existing techniques are currently still limited
in the quality of the try-on results they are able to produce from input images
of diverse characteristics. In this work, we propose a Context-Driven Virtual
Try-On Network (C-VTON) that addresses these limitations and convincingly
transfers selected clothing items to the target subjects even under challenging
pose configurations and in the presence of self-occlusions. At the core of the
C-VTON pipeline are: (i) a geometric matching procedure that efficiently aligns
the target clothing with the pose of the person in the input images, and (ii) a
powerful image generator that utilizes various types of contextual information
when synthesizing the final try-on result. C-VTON is evaluated in rigorous
experiments on the VITON and MPV datasets and in comparison to state-of-the-art
techniques from the literature. Experimental results show that the proposed
approach is able to produce photo-realistic and visually convincing results and
significantly improves on the existing state-of-the-art."
Prompt Consistency for Zero-Shot Task Generalization,0.434231,"One of the most impressive results of recent NLP history is the ability of
pre-trained language models to solve new tasks in a zero-shot setting. To
achieve this, NLP tasks are framed as natural language prompts, generating a
response indicating the predicted output. Nonetheless, the performance in such
settings often lags far behind its supervised counterpart, suggesting a large
space for potential improvement. In this paper, we explore methods to utilize
unlabeled data to improve zero-shot performance. Specifically, we take
advantage of the fact that multiple prompts can be used to specify a single
task, and propose to regularize prompt consistency, encouraging consistent
predictions over this diverse set of prompts. Our method makes it possible to
fine-tune the model either with extra unlabeled training data, or directly on
test input at inference time in an unsupervised manner. In experiments, our
approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,
2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points
in terms of accuracy. The gains are often attained with a small number of
unlabeled examples."
Strategy Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic Games,0.424889,"Neuro-symbolic approaches to artificial intelligence, which combine neural
networks with classical symbolic techniques, are growing in prominence,
necessitating formal approaches to reason about their correctness. We propose a
novel modelling formalism called neuro-symbolic concurrent stochastic games
(NS-CSGs), which comprise two probabilistic finite-state agents interacting in
a shared continuous-state environment. Each agent observes the environment
using a neural perception mechanism, which converts inputs such as images into
symbolic percepts, and makes decisions symbolically. We focus on the class of
NS-CSGs with Borel state spaces and prove the existence and measurability of
the value function for zero-sum discounted cumulative rewards under
piecewise-constant restrictions on the components of this class of models. To
compute values and synthesise strategies, we present, for the first time,
practical value iteration (VI) and policy iteration (PI) algorithms to solve
this new subclass of continuous-state CSGs. These require a finite
decomposition of the environment induced by the neural perception mechanisms of
the agents and rely on finite abstract representations of value functions and
strategies closed under VI or PI. First, we introduce a Borel measurable
piecewise-constant (B-PWC) representation of value functions, extend minimax
backups to this representation and propose a value iteration algorithm called
B-PWC VI. Second, we introduce two novel representations for the value
functions and strategies, constant-piecewise-linear (CON-PWL) and
constant-piecewise-constant (CON-PWC) respectively, and propose
Minimax-action-free PI by extending a recent PI method based on alternating
player choices for finite state spaces to Borel state spaces, which does not
require normal-form games to be solved."
Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation,0.448647,"Although human action anticipation is a task which is inherently multi-modal,
state-of-the-art methods on well known action anticipation datasets leverage
this data by applying ensemble methods and averaging scores of unimodal
anticipation networks. In this work we introduce transformer based modality
fusion techniques, which unify multi-modal data at an early stage. Our
Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular
score fusion approaches and presents state-of-the-art results outperforming
previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily
extensible and allows for adding new modalities without architectural changes.
Consequently, we extracted audio features on EpicKitchens-100 which we add to
the set of commonly used features in the community."
GIFS: Neural Implicit Function for General Shape Representation,0.473087,"Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs ."
Extreme Compression for Pre-trained Transformers Made Simple and Efficient,0.455049,"Extreme compression, particularly ultra-low bit precision (binary/ternary)
quantization, has been proposed to fit large NLP models on resource-constraint
devices. However, to preserve the accuracy for such aggressive compression
schemes, cutting-edge methods usually introduce complicated compression
pipelines, e.g., multi-stage expensive knowledge distillation with extensive
hyperparameter tuning. Also, they oftentimes focus less on smaller transformer
models that have already been heavily compressed via knowledge distillation and
lack a systematic study to show the effectiveness of their methods. In this
paper, we perform a very comprehensive systematic study to measure the impact
of many key hyperparameters and training strategies from previous works. As a
result, we find out that previous baselines for ultra-low bit precision
quantization are significantly under-trained. Based on our study, we propose a
simple yet effective compression pipeline for extreme compression, named XTC.
XTC demonstrates that (1) we can skip the pre-training knowledge distillation
to obtain a 5-layer BERT while achieving better performance than previous
state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization
plus layer reduction is able to reduce the model size by 50x, resulting in new
state-of-the-art results on GLUE tasks."
Ditto: Building Digital Twins of Articulated Objects from Interaction,0.431631,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto"
ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations,0.446489,"Context is everything, even in commonsense moral reasoning. Changing contexts
can flip the moral judgment of an action; ""Lying to a friend"" is wrong in
general, but may be morally acceptable if it is intended to protect their life.
  We present ClarifyDelphi, an interactive system that learns to ask
clarification questions (e.g., why did you lie to your friend?) in order to
elicit additional salient contexts of a social or moral situation. We posit
that questions whose potential answers lead to diverging moral judgments are
the most informative. Thus, we propose a reinforcement learning framework with
a defeasibility reward that aims to maximize the divergence between moral
judgments of hypothetical answers to a question. Human evaluation demonstrates
that our system generates more relevant, informative and defeasible questions
compared to competitive baselines. Our work is ultimately inspired by studies
in cognitive science that have investigated the flexibility in moral cognition
(i.e., the diverse contexts in which moral rules can be bent), and we hope that
research in this direction can assist both cognitive and computational
investigations of moral judgments."
DKM: Dense Kernelized Feature Matching for Geometry Estimation,0.440571,"Feature matching is a challenging computer vision task that involves finding
correspondences between two images of a 3D scene. In this paper we consider the
dense approach instead of the more common sparse paradigm, thus striving to
find all correspondences. Perhaps counter-intuitively, dense methods have
previously shown inferior performance to their sparse and semi-sparse
counterparts for estimation of two-view geometry. This changes with our novel
dense method, which outperforms both dense and sparse methods on geometry
estimation. The novelty is threefold: First, we propose a kernel regression
global matcher. Secondly, we propose warp refinement through stacked feature
maps and depthwise convolution kernels. Thirdly, we propose learning dense
confidence through consistent depth and a balanced sampling approach for dense
confidence maps. Through extensive experiments we confirm that our proposed
dense method, \textbf{D}ense \textbf{K}ernelized Feature \textbf{M}atching,
sets a new state-of-the-art on multiple geometry estimation benchmarks. In
particular, we achieve an improvement on MegaDepth-1500 of +4.9 and +8.9
AUC$@5^{\circ}$ compared to the best previous sparse method and dense method
respectively. Our code is provided at https://github.com/Parskatt/dkm"
A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots,0.452555,"A slot value might be provided segment by segment over multiple-turn
interactions in a dialog, especially for some important information such as
phone numbers and names. It is a common phenomenon in daily life, but little
attention has been paid to it in previous work. To fill the gap, this paper
defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds
a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset
includes a total of 40K dialogs and 500K utterances from four different
domains: Chinese names, phone numbers, ID numbers and license plate numbers.
The data is well annotated with sub-slot values, slot values, dialog states and
actions. We find some new linguistic phenomena and interactive manners in SSTOD
which raise critical challenges of building dialog agents for the task. We test
three state-of-the-art dialog models on SSTOD and find they cannot handle the
task well on any of the four domains. We also investigate an improved model by
involving slot knowledge in a plug-in manner. More work should be done to meet
the new challenges raised from SSTOD which widely exists in real-life
applications. The dataset and code are publicly available via
https://github.com/shunjiu/SSTOD."
Ham2Pose: Animating Sign Language Notation into Pose Sequences,0.467542,"Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal by design, our proposed method offers a generic solution
invariant to the target Sign language. Our method gradually generates pose
predictions using transformer encoders that create meaningful representations
of the text and poses while considering their spatial and temporal information.
We use weak supervision for the training process and show that our method
succeeds in learning from partial and inaccurate data. Additionally, we offer a
new distance measurement that considers missing keypoints, to measure the
distance between pose sequences using DTW-MJE. We validate its correctness
using AUTSL, a large-scale Sign language dataset, show that it measures the
distance between pose sequences more accurately than existing measurements, and
use it to assess the quality of our generated pose sequences. Code for the data
pre-processing, the model, and the distance measurement is publicly released
for future research."
"Less Data, More Knowledge: Building Next Generation Semantic Communication Networks",0.443997,"Semantic communication is viewed as a revolutionary paradigm that can
potentially transform how we design and operate wireless communication systems.
However, despite a recent surge of research activities in this area, the
research landscape remains limited. In this tutorial, we present the first
rigorous vision of a scalable end-to-end semantic communication network that is
founded on novel concepts from artificial intelligence (AI), causal reasoning,
and communication theory. We first discuss how the design of semantic
communication networks requires a move from data-driven networks towards
knowledge-driven ones. Subsequently, we highlight the necessity of creating
semantic representations of data that satisfy the key properties of minimalism,
generalizability, and efficiency so as to do more with less. We then explain
how those representations can form the basis a so-called semantic language. By
using semantic representation and languages, we show that the traditional
transmitter and receiver now become a teacher and apprentice. Then, we define
the concept of reasoning by investigating the fundamentals of causal
representation learning and their role in designing semantic communication
networks. We demonstrate that reasoning faculties are majorly characterized by
the ability to capture causal and associational relationships in datastreams.
For such reasoning-driven networks, we propose novel and essential semantic
communication metrics that include new ""reasoning capacity"" measures that could
go beyond Shannon's bound to capture the convergence of computing and
communication. Finally, we explain how semantic communications can be scaled to
large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to
provide a comprehensive reference on how to properly build, analyze, and deploy
future semantic communication networks."
3D Dual-Fusion: Dual-Domain Dual-Query Camera-LiDAR Fusion for 3D Object Detection,0.426232,"Fusing data from cameras and LiDAR sensors is an essential technique to
achieve robust 3D object detection. One key challenge in camera-LiDAR fusion
involves mitigating the large domain gap between the two sensors in terms of
coordinates and data distribution when fusing their features. In this paper, we
propose a novel camera-LiDAR fusion architecture called, 3D Dual-Fusion, which
is designed to mitigate the gap between the feature representations of camera
and LiDAR data. The proposed method fuses the features of the camera-view and
3D voxel-view domain and models their interactions through deformable
attention. We redesign the transformer fusion encoder to aggregate the
information from the two domains. Two major changes include 1) dual query-based
deformable attention to fuse the dual-domain features interactively and 2) 3D
local self-attention to encode the voxel-domain queries prior to dual-query
decoding. The results of an experimental evaluation show that the proposed
camera-LiDAR fusion architecture achieved competitive performance on the KITTI
and nuScenes datasets, with state-of-the-art performances in some 3D object
detection benchmarks categories."
Transformer-based SAR Image Despeckling,0.424057,"Synthetic Aperture Radar (SAR) images are usually degraded by a
multiplicative noise known as speckle which makes processing and interpretation
of SAR images difficult. In this paper, we introduce a transformer-based
network for SAR image despeckling. The proposed despeckling network comprises
of a transformer-based encoder which allows the network to learn global
dependencies between different image regions - aiding in better despeckling.
The network is trained end-to-end with synthetically generated speckled images
using a composite loss function. Experiments show that the proposed method
achieves significant improvements over traditional and convolutional neural
network-based despeckling methods on both synthetic and real SAR images."
Unsupervised Segmentation of Hyperspectral Remote Sensing Images with Superpixels,0.453867,"In this paper, we propose an unsupervised method for hyperspectral remote
sensing image segmentation. The method exploits the mean-shift clustering
algorithm that takes as input a preliminary hyperspectral superpixels
segmentation together with the spectral pixel information. The proposed method
does not require the number of segmentation classes as input parameter, and it
does not exploit any a-priori knowledge about the type of land-cover or
land-use to be segmented (e.g. water, vegetation, building etc.). Experiments
on Salinas, SalinasA, Pavia Center and Pavia University datasets are carried
out. Performance are measured in terms of normalized mutual information,
adjusted Rand index and F1-score. Results demonstrate the validity of the
proposed method in comparison with the state of the art."
PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects,0.440829,"Object pose estimation is crucial for robotic applications and augmented
reality. Beyond instance level 6D object pose estimation methods, estimating
category-level pose and shape has become a promising trend. As such, a new
research field needs to be supported by well-designed datasets. To provide a
benchmark with high-quality ground truth annotations to the community, we
introduce a multimodal dataset for category-level object pose estimation with
photometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high
quality 3D models of household objects over 8 categories including highly
reflective, transparent and symmetric objects. We developed a novel
robot-supported multi-modal (RGB, depth, polarisation) data acquisition and
annotation process. It ensures sub-millimeter accuracy of the pose for opaque
textured, shiny and transparent objects, no motion blur and perfect camera
synchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and
monocular RGB methods are evaluated on the challenging scenes of PhoCaL."
Decoupling Makes Weakly Supervised Local Feature Better,0.459884,"Weakly supervised learning can help local feature methods to overcome the
obstacle of acquiring a large-scale dataset with densely labeled
correspondences. However, since weak supervision cannot distinguish the losses
caused by the detection and description steps, directly conducting weakly
supervised learning within a joint describe-then-detect pipeline suffers
limited performance. In this paper, we propose a decoupled describe-then-detect
pipeline tailored for weakly supervised local feature learning. Within our
pipeline, the detection step is decoupled from the description step and
postponed until discriminative and robust descriptors are learned. In addition,
we introduce a line-to-window search strategy to explicitly use the camera pose
information for better descriptor learning. Extensive experiments show that our
method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous
fully and weakly supervised methods and achieves state-of-the-art performance
on a wide range of downstream tasks."
SimVP: Simpler yet Better Video Prediction,0.422232,"From CNN, RNN, to ViT, we have witnessed remarkable advancements in video
prediction, incorporating auxiliary inputs, elaborate neural architectures, and
sophisticated training strategies. We admire these progresses but are confused
about the necessity: is there a simple method that can perform comparably well?
This paper proposes SimVP, a simple video prediction model that is completely
built upon CNN and trained by MSE loss in an end-to-end fashion. Without
introducing any additional tricks and complicated strategies, we can achieve
state-of-the-art performance on five benchmark datasets. Through extended
experiments, we demonstrate that SimVP has strong generalization and
extensibility on real-world datasets. The significant reduction of training
cost makes it easier to scale to complex scenarios. We believe SimVP can serve
as a solid baseline to stimulate the further development of video prediction.
The code is available at
\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}."
Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning,0.461305,"We introduce Patch Aligned Contrastive Learning (PACL), a modified
compatibility function for CLIP's contrastive loss, intending to train an
alignment between the patch tokens of the vision encoder and the CLS token of
the text encoder. With such an alignment, a model can identify regions of an
image corresponding to a given text input, and therefore transfer seamlessly to
the task of open vocabulary semantic segmentation without requiring any
segmentation annotations during training. Using pre-trained CLIP encoders with
PACL, we are able to set the state-of-the-art on the task of open vocabulary
zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC,
Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also
applicable to image-level predictions and when used with a CLIP backbone,
provides a general improvement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets."
High-resolution Face Swapping via Latent Semantics Disentanglement,0.42671,"We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes."
Long-tailed Instance Segmentation using Gumbel Optimized Loss,0.429659,"Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL"
Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature,0.465856,"Literary translation is a culturally significant task, but it is bottlenecked
by the small number of qualified literary translators relative to the many
untranslated works published around the world. Machine translation (MT) holds
potential to complement the work of human translators by improving both
training procedures and their overall efficiency. Literary translation is less
constrained than more traditional MT settings since translators must balance
meaning equivalence, readability, and critical interpretability in the target
language. This property, along with the complex discourse-level context present
in literary texts, also makes literary MT more challenging to computationally
model and evaluate. To explore this task, we collect a dataset (Par3) of
non-English language novels in the public domain, each aligned at the paragraph
level to both human and automatic English translations. Using Par3, we discover
that expert literary translators prefer reference human translations over
machine-translated paragraphs at a rate of 84%, while state-of-the-art
automatic MT metrics do not correlate with those preferences. The experts note
that MT outputs contain not only mistranslations, but also discourse-disrupting
errors and stylistic inconsistencies. To address these problems, we train a
post-editing model whose output is preferred over normal MT output at a rate of
69% by experts. We publicly release Par3 at
https://github.com/katherinethai/par3/ to spur future research into literary
MT."
Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras,0.423898,"We propose a novel and pragmatic framework for traffic scene perception with
roadside cameras. The proposed framework covers a full-stack of roadside
perception pipeline for infrastructure-assisted autonomous driving, including
object detection, object localization, object tracking, and multi-camera
information fusion. Unlike previous vision-based perception frameworks rely
upon depth offset or 3D annotation at training, we adopt a modular decoupling
design and introduce a landmark-based 3D localization method, where the
detection and localization can be well decoupled so that the model can be
easily trained based on only 2D annotations. The proposed framework applies to
either optical or thermal cameras with pinhole or fish-eye lenses. Our
framework is deployed at a two-lane roundabout located at Ellsworth Rd. and
State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring
and high-precision vehicle trajectory extraction. The whole system runs
efficiently on a low-power edge computing device with all-component end-to-end
delay of less than 20ms."
A sequence-to-sequence approach for document-level relation extraction,0.441497,"Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\url{https://github.com/johngiorgi/seq2rel}}. An
online demo is available at
{\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}."
"A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games",0.47237,"This work studies an algorithm, which we call magnetic mirror descent, that
is inspired by mirror descent and the non-Euclidean proximal gradient
algorithm. Our contribution is demonstrating the virtues of magnetic mirror
descent as both an equilibrium solver and as an approach to reinforcement
learning in two-player zero-sum games. These virtues include: 1) Being the
first quantal response equilibria solver to achieve linear convergence for
extensive-form games with first order feedback; 2) Being the first standard
reinforcement learning algorithm to achieve empirically competitive results
with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark
Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning
algorithm."
Mix and Localize: Localizing Sound Sources in Mixtures,0.454825,"We present a method for simultaneously localizing multiple sound sources
within a visual scene. This task requires a model to both group a sound mixture
into individual sources, and to associate them with a visual signal. Our method
jointly solves both tasks at once, using a formulation inspired by the
contrastive random walk of Jabri et al. We create a graph in which images and
separated sounds correspond to nodes, and train a random walker to transition
between nodes from different modalities with high return probability. The
transition probabilities for this walk are determined by an audio-visual
similarity metric that is learned by our model. We show through experiments
with musical instruments and human speech that our model can successfully
localize multiple sounds, outperforming other self-supervised methods. Project
site: https://hxixixh.github.io/mix-and-localize"
Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations,0.5027,"We present Neural Feature Fusion Fields (N3F), a method that improves dense
2D image feature extractors when the latter are applied to the analysis of
multiple images reconstructible as a 3D scene. Given an image feature
extractor, for example pre-trained using self-supervision, N3F uses it as a
teacher to learn a student network defined in 3D space. The 3D student network
is similar to a neural radiance field that distills said features and can be
trained with the usual differentiable rendering machinery. As a consequence,
N3F is readily applicable to most neural rendering formulations, including
vanilla NeRF and its extensions to complex dynamic scenes. We show that our
method not only enables semantic understanding in the context of scene-specific
neural fields without the use of manual labels, but also consistently improves
over the self-supervised 2D baselines. This is demonstrated by considering
various tasks, such as 2D object retrieval, 3D segmentation, and scene editing,
in diverse sequences, including long egocentric videos in the EPIC-KITCHENS
benchmark."
ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework,0.511731,"In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost."
SGPT: GPT Sentence Embeddings for Semantic Search,0.508077,"Decoder transformers have continued increasing in scale reaching hundreds of
billions of parameters. Due to their scale the same decoder sets
state-of-the-art results on various language tasks via prompting or
fine-tuning. Yet, these large foundation models remain unusable for the related
fields of semantic search and sentence embeddings. This prevents possibly new
state-of-the-art results and forces organizations to train and maintain
separate models. To this end, we propose SGPT to use decoders for sentence
embeddings and semantic search via prompting or fine-tuning. At 5.8 billion
parameters SGPT improves on the previously best sentence embeddings by a margin
of 7% and outperforms a concurrent method with 175 billion parameters as
measured on the BEIR search benchmark. Code, models and result files are freely
available at https://github.com/Muennighoff/sgpt."
Differentiable Point-Based Radiance Fields for Efficient View Synthesis,0.485825,"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers."
Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation,0.517173,"Research on Automatic Story Generation (ASG) relies heavily on human and
automatic evaluation. However, there is no consensus on which human evaluation
criteria to use, and no analysis of how well automatic criteria correlate with
them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a
set of 6 orthogonal and comprehensive human criteria, carefully motivated by
the social sciences literature. We also present HANNA, an annotated dataset of
1,056 stories produced by 10 different ASG systems. HANNA allows us to
quantitatively evaluate the correlations of 72 automatic metrics with human
criteria. Our analysis highlights the weaknesses of current metrics for ASG and
allows us to formulate practical recommendations for ASG evaluation."
Shadow-Background-Noise 3D Spatial Decomposition Using Sparse Low-Rank Gaussian Properties for Video-SAR Moving Target Shadow Enhancement,0.483617,"Moving target shadows among video synthetic aperture radar (Video-SAR) images
are always interfered by low scattering backgrounds and cluttered noises,
causing poor detec-tion-tracking accuracy. Thus, a shadow-background-noise 3D
spatial decomposition (SBN-3D-SD) model is proposed to enhance shadows for
higher detection-tracking accuracy. It leverages the sparse property of
shadows, the low-rank property of back-grounds, and the Gaussian property of
noises to perform 3D spatial three-decomposition. It separates shadows from
back-grounds and noises by the alternating direction method of multi-pliers
(ADMM). Results on the Sandia National Laboratories (SNL) data verify its
effectiveness. It boosts the shadow saliency from the qualitative and
quantitative evaluation. It boosts the shadow detection accuracy of Faster
R-CNN, RetinaNet and YOLOv3. It also boosts the shadow tracking accuracy of
TransTrack, FairMOT and ByteTrack."
Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning,0.474729,"Conversational recommender systems (CRS) aim to proactively elicit user
preference and recommend high-quality items through natural language
conversations. Typically, a CRS consists of a recommendation module to predict
preferred items for users and a conversation module to generate appropriate
responses. To develop an effective CRS, it is essential to seamlessly integrate
the two modules. Existing works either design semantic alignment strategies, or
share knowledge resources and representations between the two modules. However,
these approaches still rely on different architectures or techniques to develop
the two modules, making it difficult for effective module integration.
  To address this problem, we propose a unified CRS model named UniCRS based on
knowledge-enhanced prompt learning. Our approach unifies the recommendation and
conversation subtasks into the prompt learning paradigm, and utilizes
knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to
fulfill both subtasks in a unified approach. In the prompt design, we include
fused knowledge representations, task-specific soft tokens, and the dialogue
context, which can provide sufficient contextual information to adapt the PLM
for the CRS task. Besides, for the recommendation subtask, we also incorporate
the generated response template as an important part of the prompt, to enhance
the information interaction between the two subtasks. Extensive experiments on
two public CRS datasets have demonstrated the effectiveness of our approach."
MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation,0.480304,"Applying existing methods to emotional support conversation -- which provides
valuable assistance to people who are in need -- has two major limitations: (a)
they generally employ a conversation-level emotion label, which is too
coarse-grained to capture user's instant mental state; (b) most of them focus
on expressing empathy in the response(s) rather than gradually reducing user's
distress. To address the problems, we propose a novel model \textbf{MISC},
which firstly infers the user's fine-grained emotional status, and then
responds skillfully using a mixture of strategy. Experimental results on the
benchmark dataset demonstrate the effectiveness of our method and reveal the
benefits of fine-grained emotion understanding as well as mixed-up strategy
modeling. Our code and data could be found in
\url{https://github.com/morecry/MISC}."
A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction,0.499575,"Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment
analysis task that aims to extract triplets of aspect terms, sentiments, and
opinion terms from review sentences. Recently, span-level models achieve
gratifying results on ASTE task by taking advantage of the predictions of all
possible spans. Since all possible spans significantly increases the number of
potential aspect and opinion candidates, it is crucial and challenging to
efficiently extract the triplet elements among them. In this paper, we present
a span-level bidirectional network which utilizes all possible spans as input
and extracts triplets from spans bidirectionally. Specifically, we devise both
the aspect decoder and opinion decoder to decode the span representations and
extract triples from aspect-to-opinion and opinion-to-aspect directions. With
these two decoders complementing with each other, the whole network can extract
triplets from spans more comprehensively. Moreover, considering that mutual
exclusion cannot be guaranteed between the spans, we design a similar span
separation loss to facilitate the downstream task of distinguishing the correct
span by expanding the KL divergence of similar spans during the training
process; in the inference process, we adopt an inference strategy to remove
conflicting triplets from the results base on their confidence scores.
Experimental results show that our framework not only significantly outperforms
state-of-the-art methods, but achieves better performance in predicting
triplets with multi-token entities and extracting triplets in sentences contain
multi-triplets."
ProsocialDialog: A Prosocial Backbone for Conversational Agents,0.521271,"Most existing dialogue systems fail to respond properly to potentially unsafe
user utterances by either ignoring or passively agreeing with them. To address
this issue, we introduce ProsocialDialog, the first large-scale multi-turn
dialogue dataset to teach conversational agents to respond to problematic
content following social norms. Covering diverse unethical, problematic,
biased, and toxic situations, ProsocialDialog contains responses that encourage
prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,
RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists
of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue
safety labels accompanied by free-form rationales.
  With this dataset, we introduce a dialogue safety detection module, Canary,
capable of generating RoTs given conversational context, and a
socially-informed dialogue agent, Prost. Empirical results show that Prost
generates more socially acceptable dialogues compared to other state-of-the-art
language and dialogue models in both in-domain and out-of-domain settings.
Additionally, Canary effectively guides conversational agents and off-the-shelf
language models to generate significantly more prosocial responses. Our work
highlights the promise and importance of creating and steering conversational
AI to be socially responsible."
Pointillism: Accurate 3D bounding box estimation with multi-radars,0.50552,"Autonomous perception requires high-quality environment sensing in the form
of 3D bounding boxes of dynamic objects. The primary sensors used in automotive
systems are light-based cameras and LiDARs. However, they are known to fail in
adverse weather conditions. Radars can potentially solve this problem as they
are barely affected by adverse weather conditions. However, specular
reflections of wireless signals cause poor performance of radar point clouds.
We introduce Pointillism, a system that combines data from multiple spatially
separated radars with an optimal separation to mitigate these problems. We
introduce a novel concept of Cross Potential Point Clouds, which uses the
spatial diversity induced by multiple radars and solves the problem of noise
and sparsity in radar point clouds. Furthermore, we present the design of
RP-net, a novel deep learning architecture, designed explicitly for radar's
sparse data distribution, to enable accurate 3D bounding box estimation. The
spatial techniques designed and proposed in this paper are fundamental to
radars point cloud distribution and would benefit other radar sensing
applications."
Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning,0.480341,"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset."
Generative Cooperative Learning for Unsupervised Video Anomaly Detection,0.504041,"Video anomaly detection is well investigated in weakly-supervised and
one-class classification (OCC) settings. However, unsupervised video anomaly
detection methods are quite sparse, likely because anomalies are less frequent
in occurrence and usually not well-defined, which when coupled with the absence
of ground truth supervision, could adversely affect the performance of the
learning algorithms. This problem is challenging yet rewarding as it can
completely eradicate the costs of obtaining laborious annotations and enable
such systems to be deployed without human intervention. To this end, we propose
a novel unsupervised Generative Cooperative Learning (GCL) approach for video
anomaly detection that exploits the low frequency of anomalies towards building
a cross-supervision between a generator and a discriminator. In essence, both
networks get trained in a cooperative fashion, thereby allowing unsupervised
learning. We conduct extensive experiments on two large-scale video anomaly
detection datasets, UCF crime, and ShanghaiTech. Consistent improvement over
the existing state-of-the-art unsupervised and OCC methods corroborate the
effectiveness of our approach."
Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking,0.485456,"Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors. Recent researches start from the pretrained knowledge of language
models and take multimodal information into CSC models to improve the
performance. However, they overlook the rich knowledge in the dictionary, the
reference book where one can learn how one character should be pronounced,
written, and used. In this paper, we propose the LEAD framework, which renders
the CSC model to learn heterogeneous knowledge from the dictionary in terms of
phonetics, vision, and meaning. LEAD first constructs positive and negative
samples according to the knowledge of character phonetics, glyphs, and
definitions in the dictionary. Then a unified contrastive learning-based
training scheme is employed to refine the representations of the CSC models.
Extensive experiments and detailed analyses on the SIGHAN benchmark datasets
demonstrate the effectiveness of our proposed methods."
Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling,0.501251,"Existing research generally treats Chinese character as a minimum unit for
representation. However, such Chinese character representation will suffer two
bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich
internal features (e.g., radicals and strokes); and 2) Parameter bottleneck,
each individual character has to be represented by a unique vector. In this
paper, we introduce a novel representation method for Chinese characters to
break the bottlenecks, namely StrokeNet, which represents a Chinese character
by a Latinized stroke sequence (e.g., ""ao1 (concave)"" to ""ajaie"" and ""tu1
(convex)"" to ""aeaqe""). Specifically, StrokeNet maps each stroke to a specific
Latin character, thus allowing similar Chinese characters to have similar Latin
representations. With the introduction of StrokeNet to neural machine
translation (NMT), many powerful but not applicable techniques to non-Latin
languages (e.g., shared subword vocabulary learning and ciphertext-based data
augmentation) can now be perfectly implemented. Experiments on the widely-used
NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT
tasks show that StrokeNet can provide a significant performance boost over the
strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17
Chinese-English task which is better than any previously reported results
without using monolingual data. Code and scripts are freely available at
https://github.com/zjwang21/StrokeNet."
Evaluating Human-Language Model Interaction,0.499323,"Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation."
Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness,0.488539,"A notable challenge in Multi-Document Summarization (MDS) is the
extremely-long length of the input. In this paper, we present an
extract-then-abstract Transformer framework to overcome the problem.
Specifically, we leverage pre-trained language models to construct a
hierarchical extractor for salient sentence selection across documents and an
abstractor for rewriting the selected contents as summaries. However, learning
such a framework is challenging since the optimal contents for the abstractor
are generally unknown. Previous works typically create pseudo extraction oracle
to enable the supervised learning for both the extractor and the abstractor.
Nevertheless, we argue that the performance of such methods could be restricted
due to the insufficient information for prediction and inconsistent objectives
between training and testing. To this end, we propose a loss weighting
mechanism that makes the model aware of the unequal importance for the
sentences not in the pseudo extraction oracle, and leverage the fine-tuned
abstractor to generate summary references as auxiliary signals for learning the
extractor. Moreover, we propose a reinforcement learning method that can
efficiently apply to the extractor for harmonizing the optimization between
training and testing. Experiment results show that our framework substantially
outperforms strong baselines with comparable model sizes and achieves the best
results on the Multi-News, Multi-XScience, and WikiCatSum corpora."
Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning,0.499211,"Non-exemplar class-incremental learning is to recognize both the old and new
classes when old class samples cannot be saved. It is a challenging task since
representation optimization and feature retention can only be achieved under
supervision from new classes. To address this problem, we propose a novel
self-sustaining representation expansion scheme. Our scheme consists of a
structure reorganization strategy that fuses main-branch expansion and
side-branch updating to maintain the old features, and a main-branch
distillation scheme to transfer the invariant knowledge. Furthermore, a
prototype selection mechanism is proposed to enhance the discrimination between
the old and new classes by selectively incorporating new samples into the
distillation process. Extensive experiments on three benchmarks demonstrate
significant incremental performance, outperforming the state-of-the-art methods
by a margin of 3%, 3% and 6%, respectively."
Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning,0.497836,"Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods."
Multi-modal Contrastive Representation Learning for Entity Alignment,0.522093,"Multi-modal entity alignment aims to identify equivalent entities between two
different multi-modal knowledge graphs, which consist of structural triples and
images associated with entities. Most previous works focus on how to utilize
and encode information from different modalities, while it is not trivial to
leverage multi-modal knowledge in entity alignment because of the modality
heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive
Learning based Entity Alignment model, to obtain effective joint
representations for multi-modal entity alignment. Different from previous
works, MCLEA considers task-oriented modality and models the inter-modal
relationships for each entity representation. In particular, MCLEA firstly
learns multiple individual representations from multiple modalities, and then
performs contrastive learning to jointly model intra-modal and inter-modal
interactions. Extensive experimental results show that MCLEA outperforms
state-of-the-art baselines on public datasets under both supervised and
unsupervised settings."
Unified Vision and Language Prompt Learning,0.515446,"Prompt tuning, a parameter- and data-efficient transfer learning paradigm
that tunes only a small number of parameters in a model's input space, has
become a trend in the vision community since the emergence of large
vision-language models like CLIP. We present a systematic study on two
representative prompt tuning methods, namely text prompt tuning and visual
prompt tuning. A major finding is that none of the unimodal prompt tuning
methods performs consistently well: text prompt tuning fails on data with high
intra-class visual variances while visual prompt tuning cannot handle low
inter-class variances. To combine the best from both worlds, we propose a
simple approach called Unified Prompt Tuning (UPT), which essentially learns a
tiny neural network to jointly optimize prompts across different modalities.
Extensive experiments on over 11 vision datasets show that UPT achieves a
better trade-off than the unimodal counterparts on few-shot learning
benchmarks, as well as on domain generalization benchmarks. Code and models
will be released to facilitate future research."
Semantic Image Synthesis via Diffusion Models,0.490584,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS)."
CORL: Research-oriented Deep Offline Reinforcement Learning Library,0.482613,"CORL is an open-source library that provides thoroughly benchmarked
single-file implementations of both deep offline and offline-to-online
reinforcement learning algorithms. It emphasizes a simple developing experience
with a straightforward codebase and a modern analysis tracking tool. In CORL,
we isolate methods implementation into separate single files, making
performance-relevant details easier to recognize. Additionally, an experiment
tracking feature is available to help log metrics, hyperparameters,
dependencies, and more to the cloud. Finally, we have ensured the reliability
of the implementations by benchmarking commonly employed D4RL datasets
providing a transparent source of results that can be reused for robust
evaluation tools such as performance profiles, probability of improvement, or
expected online performance."
A Distributional Lens for Multi-Aspect Controllable Text Generation,0.498208,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach."
Imagination-Augmented Natural Language Understanding,0.486221,"Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances."
Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation,0.517115,"A diffusion model learns to predict a vector field of gradients. We propose
to apply chain rule on the learned gradients, and back-propagate the score of a
diffusion model through the Jacobian of a differentiable renderer, which we
instantiate to be a voxel radiance field. This setup aggregates 2D scores at
multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D
model for 3D data generation. We identify a technical challenge of distribution
mismatch that arises in this application, and propose a novel estimation
mechanism to resolve it. We run our algorithm on several off-the-shelf
diffusion image generative models, including the recently released Stable
Diffusion trained on the large-scale LAION dataset."
AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach,0.515258,"Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging."
BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis,0.527198,"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to align aspects and corresponding sentiments for
aspect-specific sentiment polarity inference. It is challenging because a
sentence may contain multiple aspects or complicated (e.g., conditional,
coordinating, or adversative) relations. Recently, exploiting dependency syntax
information with graph neural networks has been the most popular trend. Despite
its success, methods that heavily rely on the dependency tree pose challenges
in accurately modeling the alignment of the aspects and their words indicative
of sentiment, since the dependency tree may provide noisy signals of unrelated
associations (e.g., the ""conj"" relation between ""great"" and ""dreadful"" in
Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax
aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully
exploits the syntax information (e.g., phrase segmentation and hierarchical
structure) of the constituent tree of a sentence to model the sentiment-aware
context of every single aspect (called intra-context) and the sentiment
relations across aspects (called inter-context) for learning. Experiments on
four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the
state-of-the-art methods consistently."
One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones,0.575011,"We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and
52% relative improvement in unseen success rate over two competitive base
models."
"A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets",0.542205,"In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this article, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. We create a new large medical dialogue dataset with multi-level
finegrained annotations and establish five independent tasks, including named
entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies. Both code and data is available
from https://github.com/lemuria-wchen/imcs21."
Uncertainty-aware Panoptic Segmentation,0.551462,"Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective panoptic fusion module that
leverages the predicted uncertainties. Furthermore, we provide several strong
baselines combining state-of-the-art panoptic segmentation networks with
sampling-free uncertainty estimation techniques. Extensive evaluations show
that our EvPSNet achieves the new state-of-the-art for the standard Panoptic
Quality (PQ), as well as for our uncertainty-aware panoptic metrics. We make
the code available at: \url{https://github.com/kshitij3112/EvPSNet}"
Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning,0.532493,"With the rapid development of artificial intelligence and autonomous driving
technology, the demand for semiconductors is projected to rise substantially.
However, the massive expansion of semiconductor manufacturing and the
development of new technology will bring many defect wafers. If these defect
wafers have not been correctly inspected, the ineffective semiconductor
processing on these defect wafers will cause additional impact to our
environment, such as excessive carbon dioxide emission and energy consumption.
In this paper, we utilize the information processing advantages of quantum
computing to promote the defect learning defect review (DLDR). We propose a
classical-quantum hybrid algorithm for deep learning on near-term quantum
processors. By tuning parameters implemented on it, quantum circuit driven by
our framework learns a given DLDR task, include of wafer defect map
classification, defect pattern classification, and hotspot detection. In
addition, we explore parametrized quantum circuits with different
expressibility and entangling capacities. These results can be used to build a
future roadmap to develop circuit-based quantum deep learning for semiconductor
defect detection."
SeqOT: A Spatial-Temporal Transformer Network for Place Recognition Using Sequential LiDAR Data,0.552902,"Place recognition is an important component for autonomous vehicles to
achieve loop closing or global localization. In this paper, we tackle the
problem of place recognition based on sequential 3D LiDAR scans obtained by an
onboard LiDAR sensor. We propose a transformer-based network named SeqOT to
exploit the temporal and spatial information provided by sequential range
images generated from the LiDAR data. It uses multi-scale transformers to
generate a global descriptor for each sequence of LiDAR range images in an
end-to-end fashion. During online operation, our SeqOT finds similar places by
matching such descriptors between the current query sequence and those stored
in the map. We evaluate our approach on four datasets collected with different
types of LiDAR sensors in different environments. The experimental results show
that our method outperforms the state-of-the-art LiDAR-based place recognition
methods and generalizes well across different environments. Furthermore, our
method operates online faster than the frame rate of the sensor. The
implementation of our method is released as open source at:
https://github.com/BIT-MJY/SeqOT."
Anomaly Detection via Reverse Distillation from One-Class Embedding,0.559367,"Knowledge distillation (KD) achieves promising results on the challenging
problem of unsupervised anomaly detection (AD).The representation discrepancy
of anomalies in the teacher-student (T-S) model provides essential evidence for
AD. However, using similar or identical architectures to build the teacher and
student models in previous studies hinders the diversity of anomalous
representations. To tackle this problem, we propose a novel T-S model
consisting of a teacher encoder and a student decoder and introduce a simple
yet effective ""reverse distillation"" paradigm accordingly. Instead of receiving
raw images directly, the student network takes teacher model's one-class
embedding as input and targets to restore the teacher's multiscale
representations. Inherently, knowledge distillation in this study starts from
abstract, high-level presentations to low-level features. In addition, we
introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S
model. The obtained compact embedding effectively preserves essential
information on normal patterns, but abandons anomaly perturbations. Extensive
experimentation on AD and one-class novelty detection benchmarks shows that our
method surpasses SOTA performance, demonstrating our proposed approach's
effectiveness and generalizability."
Conformance Checking with Uncertainty via SMT (Extended Version),0.569054,"Logs of real-life processes often feature uncertainty pertaining the recorded
timestamps, data values, and/or events. We consider the problem of checking
conformance of uncertain logs against data-aware reference processes.
Specifically, we show how to solve it via SMT encodings, lifting previous work
on data-aware SMT-based conformance checking to this more sophisticated
setting. Our approach is modular, in that it homogeneously accommodates for
different types of uncertainty. Moreover, using appropriate cost functions,
different conformance checking tasks can be addressed. We show the correctness
of our approach and witness feasibility through a proof-of-concept
implementation."
Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models,0.566874,"Image restoration under adverse weather conditions has been of significant
interest for various computer vision applications. Recent successful methods
rely on the current progress in deep neural network architectural designs
(e.g., with vision transformers). Motivated by the recent progress achieved
with state-of-the-art conditional generative models, we present a novel
patch-based image restoration algorithm based on denoising diffusion
probabilistic models. Our patch-based diffusion modeling approach enables
size-agnostic image restoration by using a guided denoising process with
smoothed noise estimates across overlapping patches during inference. We
empirically evaluate our model on benchmark datasets for image desnowing,
combined deraining and dehazing, and raindrop removal. We demonstrate our
approach to achieve state-of-the-art performances on both weather-specific and
multi-weather image restoration, and experimentally show strong generalization
to real-world test images."
Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation,0.52901,"Scene flow estimation, which extracts point-wise motion between scenes, is
becoming a crucial task in many computer vision tasks. However, all of the
existing estimation methods utilize only the unidirectional features,
restricting the accuracy and generality. This paper presents a novel scene flow
estimation architecture using bidirectional flow embedding layers. The proposed
bidirectional layer learns features along both forward and backward directions,
enhancing the estimation performance. In addition, hierarchical feature
extraction and warping improve the performance and reduce computational
overhead. Experimental results show that the proposed architecture achieved a
new state-of-the-art record by outperforming other approaches with large margin
in both FlyingThings3D and KITTI benchmarks. Codes are available at
https://github.com/cwc1260/BiFlow."
"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",0.563233,"While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations ""detached"" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results."
TourBERT: A pretrained language model for the tourism industry,0.527633,"The Bidirectional Encoder Representations from Transformers (BERT) is
currently one of the most important and state-of-the-art models for natural
language. However, it has also been shown that for domain-specific tasks it is
helpful to pretrain BERT on a domain-specific corpus. In this paper, we present
TourBERT, a pretrained language model for tourism. We describe how TourBERT was
developed and evaluated. The evaluations show that TourBERT is outperforming
BERT in all tourism-specific tasks."
Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some benchmarks,0.540019,"The Donate Speech campaign has so far succeeded in gathering approximately
3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta
(Donate Speech) corpus. The corpus includes over twenty thousand speakers from
all the regions of Finland and from all age brackets. The primary goals of the
collection were to create a representative, large-scale resource to study
spontaneous spoken Finnish and to accelerate the development of language
technology and speech-based services. In this paper, we present the collection
process and the collected corpus, and showcase its versatility through multiple
use cases. The evaluated use cases include: automatic speech recognition of
spontaneous speech, detection of age, gender, dialect and topic and metadata
analysis. We provide benchmarks for the use cases, as well down loadable,
trained baseline systems with open-source code for reproducibility. One further
use case is to verify the metadata and transcripts given in this corpus itself,
and to suggest artificial metadata and transcripts for the part of the corpus
where it is missing."
PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices,0.551371,"Neural radiance-density field methods have become increasingly popular for
the task of novel-view rendering. Their recent extension to hash-based
positional encoding ensures fast training and inference with visually pleasing
results. However, density-based methods struggle with recovering accurate
surface geometry. Hybrid methods alleviate this issue by optimizing the density
based on an underlying SDF. However, current SDF methods are overly smooth and
miss fine geometric details. In this work, we combine the strengths of these
two lines of work in a novel hash-based implicit surface representation. We
propose improvements to the two areas by replacing the voxel hash encoding with
a permutohedral lattice which optimizes faster, especially for higher
dimensions. We additionally propose a regularization scheme which is crucial
for recovering high-frequency geometric detail. We evaluate our method on
multiple datasets and show that we can recover geometric detail at the level of
pores and wrinkles while using only RGB images for supervision. Furthermore,
using sphere tracing we can render novel views at 30 fps on an RTX 3090. Code
is publicly available at: https://radualexandru.github.io/permuto_sdf"
ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select,0.56112,"We study the problem of extracting N-ary relation tuples from scientific
articles. This task is challenging because the target knowledge tuples can
reside in multiple parts and modalities of the document. Our proposed method
ReSel decomposes this task into a two-stage procedure that first retrieves the
most relevant paragraph/table and then selects the target entity from the
retrieved component. For the high-level retrieval stage, ReSel designs a simple
and effective feature set, which captures multi-level lexical and semantic
similarities between the query and components. For the low-level selection
stage, ReSel designs a cross-modal entity correlation graph along with a
multi-view architecture, which models both semantic and document-structural
relations between entities. Our experiments on three scientific information
extraction datasets show that ReSel outperforms state-of-the-art baselines
significantly."
SALTED: A Framework for SAlient Long-Tail Translation Error Detection,0.568988,"Traditional machine translation (MT) metrics provide an average measure of
translation quality that is insensitive to the long tail of behavioral problems
in MT. Examples include translation of numbers, physical units, dropped content
and hallucinations. These errors, which occur rarely and unpredictably in
Neural Machine Translation (NMT), greatly undermine the reliability of
state-of-the-art MT systems. Consequently, it is important to have visibility
into these problems during model development. Towards this direction, we
introduce SALTED, a specifications-based framework for behavioral testing of MT
models that provides fine-grained views of salient long-tail errors, permitting
trustworthy visibility into previously invisible problems. At the core of our
approach is the development of high-precision detectors that flag errors (or
alternatively, verify output correctness) between a source sentence and a
system output. We demonstrate that such detectors could be used not just to
identify salient long-tail errors in MT systems, but also for higher-recall
filtering of the training data, fixing targeted errors with model fine-tuning
in NMT and generating novel data for metamorphic testing to elicit further bugs
in models."
Dataless Knowledge Fusion by Merging Weights of Language Models,0.528632,"Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios."
StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,0.547094,"Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance."
Rebellion and Disobedience as Useful Tools in Human-Robot Interaction Research -- The Handheld Robotics Case,0.527633,"This position paper argues on the utility of rebellion and disobedience (RaD)
in human-robot interaction (HRI). In general, we see two main opportunities in
the use of controlled and well designed rebellion and disobedience: i)
illuminate insight into the effectiveness of the collaboration (or lack of) and
ii) prevent mistakes and correct user actions when in the user's own interest.
Through the use of a close interaction modality, that of handheld robots, we
discuss use cases for utility of rebellion and disobedience that can be
applicable to other instances of HRI."
TCTrack: Temporal Contexts for Aerial Tracking,0.567683,"Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier."
Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models,0.578328,"We explore the idea of compressing the prompts used to condition language
models, and show that compressed prompts can retain a substantive amount of
information about the original prompt. For severely compressed prompts, while
fine-grained information is lost, abstract information and general sentiments
can be retained with surprisingly few parameters, which can be useful in the
context of decode-time algorithms for controllability and toxicity reduction.
We explore contrastive conditioning to steer language model generation towards
desirable text and away from undesirable text, and find that some complex
prompts can be effectively compressed into a single token to guide generation.
We also show that compressed prompts are largely compositional, and can be
constructed such that they can be used to control independent aspects of
generated text."
Stubborn: A Strong Baseline for Indoor Object Navigation,0.552096,"We present a strong baseline that surpasses the performance of previously
published methods on the Habitat Challenge task of navigating to a target
object in indoor environments. Our method is motivated from primary failure
modes of prior state-of-the-art: poor exploration, inaccurate object
identification, and agent getting trapped due to imprecise map construction. We
make three contributions to mitigate these issues: (i) First, we show that
existing map-based methods fail to effectively use semantic clues for
exploration. We present a semantic-agnostic exploration strategy (called
Stubborn) without any learning that surprisingly outperforms prior work. (ii)
We propose a strategy for integrating temporal information to improve object
identification. (iii) Lastly, due to inaccurate depth observation the agent
often gets trapped in small regions. We develop a multi-scale collision map for
obstacle identification that mitigates this issue."
Active Learning by Feature Mixing,0.57656,"The promise of active learning (AL) is to reduce labelling costs by selecting
the most valuable examples to annotate from a pool of unlabelled data.
Identifying these examples is especially challenging with high-dimensional data
(e.g. images, videos) and in low-data regimes. In this paper, we propose a
novel method for batch AL called ALFA-Mix. We identify unlabelled instances
with sufficiently-distinct features by seeking inconsistencies in predictions
resulting from interventions on their representations. We construct
interpolations between representations of labelled and unlabelled instances
then examine the predicted labels. We show that inconsistencies in these
predictions help discovering features that the model is unable to recognise in
the unlabelled instances. We derive an efficient implementation based on a
closed-form solution to the optimal interpolation causing changes in
predictions. Our method outperforms all recent AL approaches in 30 different
settings on 12 benchmarks of images, videos, and non-visual data. The
improvements are especially significant in low-data regimes and on self-trained
vision transformers, where ALFA-Mix outperforms the state-of-the-art in 59% and
43% of the experiments respectively."
DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,0.549166,"With the ever-growing size of pretrained models (PMs), fine-tuning them has
become more expensive and resource-hungry. As a remedy, low-rank adapters
(LoRA) keep the main pretrained weights of the model frozen and just introduce
some learnable truncated SVD modules (so-called LoRA blocks) to the model.
While LoRA blocks are parameter-efficient, they suffer from two major problems:
first, the size of these blocks is fixed and cannot be modified after training
(for example, if we need to change the rank of LoRA blocks, then we need to
re-train them from scratch); second, optimizing their rank requires an
exhaustive search and effort. In this work, we introduce a dynamic low-rank
adaptation (DyLoRA) technique to address these two problems together. Our
DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank
by sorting the representation learned by the adapter module at different ranks
during training. We evaluate our solution on different natural language
understanding (GLUE benchmark) and language generation tasks (E2E, DART and
WebNLG) using different pretrained models such as RoBERTa and GPT with
different sizes. Our results show that we can train dynamic search-free models
with DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA
without significantly compromising performance. Moreover, our models can
perform consistently well on a much larger range of ranks compared to LoRA."
Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation,0.565497,"Fashion attribute editing is a task that aims to convert the semantic
attributes of a given fashion image while preserving the irrelevant regions.
Previous works typically employ conditional GANs where the generator explicitly
learns the target attributes and directly execute the conversion. These
approaches, however, are neither scalable nor generic as they operate only with
few limited attributes and a separate generator is required for each dataset or
attribute set. Inspired by the recent advancement of diffusion models, we
explore the classifier-guided diffusion that leverages the off-the-shelf
diffusion model pretrained on general visual semantics such as Imagenet. In
order to achieve a generic editing pipeline, we pose this as multi-attribute
image manipulation task, where the attribute ranges from item category, fabric,
pattern to collar and neckline. We empirically show that conventional methods
fail in our challenging setting, and study efficient adaptation scheme that
involves recently introduced attention-pooling technique to obtain a
multi-attribute classifier guidance. Based on this, we present a mask-free
fashion attribute editing framework that leverages the classifier logits and
the cross-attention map for manipulation. We empirically demonstrate that our
framework achieves convincing sample quality and attribute alignments."
CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification,0.532568,"Existing computer vision research in artwork struggles with artwork's
fine-grained attributes recognition and lack of curated annotated datasets due
to their costly creation. To the best of our knowledge, we are one of the first
methods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural
network on a variety of artwork images and text descriptions pairs. CLIP is
able to learn directly from free-form art descriptions, or, if available,
curated fine-grained labels. Model's zero-shot capability allows predicting
accurate natural language description for a given image, without directly
optimizing for the task. Our approach aims to solve 2 challenges: instance
retrieval and fine-grained artwork attribute recognition. We use the iMet
Dataset, which we consider the largest annotated artwork dataset. In this
benchmark we achieved competitive results using only self-supervision."
Sparse2Dense: Learning to Densify 3D Features for 3D Object Detection,0.555502,"LiDAR-produced point clouds are the major source for most state-of-the-art 3D
object detectors. Yet, small, distant, and incomplete objects with sparse or
few points are often hard to detect. We present Sparse2Dense, a new framework
to efficiently boost 3D detection performance by learning to densify point
clouds in latent space. Specifically, we first train a dense point 3D detector
(DDet) with a dense point cloud as input and design a sparse point 3D detector
(SDet) with a regular point cloud as input. Importantly, we formulate the
lightweight plug-in S2D module and the point cloud reconstruction module in
SDet to densify 3D features and train SDet to produce 3D features, following
the dense 3D features in DDet. So, in inference, SDet can simulate dense 3D
features from regular (sparse) point cloud inputs without requiring dense
inputs. We evaluate our method on the large-scale Waymo Open Dataset and the
Waymo Domain Adaptation Dataset, showing its high performance and efficiency
over the state of the arts."
UC-OWOD: Unknown-Classified Open World Object Detection,0.601024,"Open World Object Detection (OWOD) is a challenging computer vision problem
that requires detecting unknown objects and gradually learning the identified
unknown classes. However, it cannot distinguish unknown instances as multiple
unknown classes. In this work, we propose a novel OWOD problem called
Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to
detect unknown instances and classify them into different unknown classes.
Besides, we formulate the problem and devise a two-stage object detector to
solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative
classification head are used to detect known and unknown objects. Then,
similarity-based unknown classification and unknown clustering refinement
modules are constructed to distinguish multiple unknown classes. Moreover, two
novel evaluation protocols are designed to evaluate unknown-class detection.
Abundant experiments and visualizations prove the effectiveness of the proposed
method. Code is available at https://github.com/JohnWuzh/UC-OWOD."
A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation,0.629889,"Early exiting allows instances to exit at different layers according to the
estimation of difficulty. Previous works usually adopt heuristic metrics such
as the entropy of internal outputs to measure instance difficulty, which
suffers from generalization and threshold-tuning. In contrast, learning to
exit, or learning to predict instance difficulty is a more appealing way.
Though some effort has been devoted to employing such ""learn-to-exit"" modules,
it is still unknown whether and how well the instance difficulty can be
learned. As a response, we first conduct experiments on the learnability of
instance difficulty, which demonstrates that modern neural models perform
poorly on predicting instance difficulty. Based on this observation, we propose
a simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces
the learn-to-exit modules with hash functions to assign each token to a fixed
exiting layer. Different from previous methods, HashEE requires no internal
classifiers nor extra parameters, and therefore is more efficient. Experimental
results on classification, regression, and generation tasks demonstrate that
HashEE can achieve higher performance with fewer FLOPs and inference time
compared with previous state-of-the-art early exiting methods."
ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval,0.589126,"Neural retrievers based on pre-trained language models (PLMs), such as
dual-encoders, have achieved promising performance on the task of open-domain
question answering (QA). Their effectiveness can further reach new
state-of-the-arts by incorporating cross-architecture knowledge distillation.
However, most of the existing studies just directly apply conventional
distillation methods. They fail to consider the particular situation where the
teacher and student have different structures. In this paper, we propose a
novel distillation method that significantly advances cross-architecture
distillation for dual-encoders. Our method 1) introduces a self on-the-fly
distillation method that can effectively distill late interaction (i.e.,
ColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation
process to further improve the performance with a cross-encoder teacher.
Extensive experiments are conducted to validate that our proposed solution
outperforms strong baselines and establish a new state-of-the-art on
open-domain QA benchmarks."
Offline RL for Natural Language Generation with Implicit Language Q Learning,0.586294,"Large language models distill broad knowledge from text corpora. However,
they can be inconsistent when it comes to completing user specified tasks. This
issue can be addressed by finetuning such models via supervised learning on
curated datasets, or via reinforcement learning. In this work, we propose a
novel offline RL method, implicit language Q-learning (ILQL), designed for use
on language models, that combines both the flexible utility maximization
framework of RL algorithms with the ability of supervised learning to leverage
previously collected data, as well as its simplicity and stability. Our method
employs a combination of value conservatism alongside an implicit dataset
support constraint in learning value functions, which are then used to guide
language model generations towards maximizing user-specified utility functions.
In addition to empirically validating ILQL, we present a detailed empirical
analysis of situations where offline RL can be useful in natural language
generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can
effectively optimize high variance reward functions based on subjective
judgement, such as whether to label a comment as toxic or not."
Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment,0.614641,"Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting."
Selective Residual M-Net for Real Image Denoising,0.617815,"Image restoration is a low-level vision task which is to restore degraded
images to noise-free images. With the success of deep neural networks, the
convolutional neural networks surpass the traditional restoration methods and
become the mainstream in the computer vision area. To advance the performanceof
denoising algorithms, we propose a blind real image denoising network (SRMNet)
by employing a hierarchical architecture improved from U-Net. Specifically, we
use a selective kernel with residual block on the hierarchical structure called
M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet
has competitive performance results on two synthetic and two real-world noisy
datasets in terms of quantitative metrics and visual quality. The source code
and pretrained model are available at
https://github.com/TentativeGitHub/SRMNet."
Medical Dataset Classification for Kurdish Short Text over Social Media,0.603448,"The Facebook application is used as a resource for collecting the comments of
this dataset, The dataset consists of 6756 comments to create a Medical Kurdish
Dataset (MKD). The samples are comments of users, which are gathered from
different posts of pages (Medical, News, Economy, Education, and Sport). Six
steps as a preprocessing technique are performed on the raw dataset to clean
and remove noise in the comments by replacing characters. The comments (short
text) are labeled for positive class (medical comment) and negative class
(non-medical comment) as text classification. The percentage ratio of the
negative class is 55% while the positive class is 45%."
Cross-Spectral Neural Radiance Fields,0.628712,"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations."
In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models,0.617843,"Given the success with in-context learning of large pre-trained language
models, we introduce in-context learning distillation to transfer in-context
few-shot learning ability from large models to smaller models. We propose to
combine in-context learning objectives with language modeling objectives to
distill both the ability to read in-context examples and task knowledge to the
smaller models. We perform in-context learning distillation under two different
few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask
In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask
few-shot learning but also requires more computation than Meta-ICT. Our method
shows consistent improvements for both Meta-ICT and Multitask-ICT on two
benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal
that in-context learning objectives and language modeling objectives are
complementary under the Multitask-ICT paradigm. In-context learning objectives
achieve the best performance when combined with language modeling objectives."
3D Common Corruptions and Data Augmentation,0.610361,"We introduce a set of image transformations that can be used as corruptions
to evaluate the robustness of models as well as data augmentation mechanisms
for training neural networks. The primary distinction of the proposed
transformations is that, unlike existing approaches such as Common Corruptions,
the geometry of the scene is incorporated in the transformations -- thus
leading to corruptions that are more likely to occur in the real world. We also
introduce a set of semantic corruptions (e.g. natural object occlusions). We
show these transformations are `efficient' (can be computed on-the-fly),
`extendable' (can be applied on most image datasets), expose vulnerability of
existing models, and can effectively make models more robust when employed as
`3D data augmentation' mechanisms. The evaluations on several tasks and
datasets suggest incorporating 3D information into benchmarking and training
opens up a promising direction for robustness research."
TRUST XAI: Model-Agnostic Explanations for AI With a Case Study on IIoT Security,0.62827,"Despite AI's significant growth, its ""black box"" nature creates challenges in
generating adequate trust. Thus, it is seldom utilized as a standalone unit in
IoT high-risk applications, such as critical industrial infrastructures,
medical systems, and financial applications, etc. Explainable AI (XAI) has
emerged to help with this problem. However, designing appropriately fast and
accurate XAI is still challenging, especially in numerical applications. Here,
we propose a universal XAI model named Transparency Relying Upon Statistical
Theory (TRUST), which is model-agnostic, high-performing, and suitable for
numerical applications. Simply put, TRUST XAI models the statistical behavior
of the AI's outputs in an AI-based system. Factor analysis is used to transform
the input features into a new set of latent variables. We use mutual
information to rank these variables and pick only the most influential ones on
the AI's outputs and call them ""representatives"" of the classes. Then we use
multi-modal Gaussian distributions to determine the likelihood of any new
sample belonging to each class. We demonstrate the effectiveness of TRUST in a
case study on cybersecurity of the industrial Internet of things (IIoT) using
three different cybersecurity datasets. As IIoT is a prominent application that
deals with numerical data. The results show that TRUST XAI provides
explanations for new random samples with an average success rate of 98%.
Compared with LIME, a popular XAI model, TRUST is shown to be superior in the
context of performance, speed, and the method of explainability. In the end, we
also show how TRUST is explained to the user."
Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks,0.619979,"The wide adoption and application of Masked language models~(MLMs) on
sensitive data (from legal to medical) necessitates a thorough quantitative
investigation into their privacy vulnerabilities -- to what extent do MLMs leak
information about their training data? Prior attempts at measuring leakage of
MLMs via membership inference attacks have been inconclusive, implying the
potential robustness of MLMs to privacy attacks. In this work, we posit that
prior attempts were inconclusive because they based their attack solely on the
MLM's model score. We devise a stronger membership inference attack based on
likelihood ratio hypothesis testing that involves an additional reference MLM
to more accurately quantify the privacy risks of memorization in MLMs. We show
that masked language models are extremely susceptible to likelihood ratio
membership inference attacks: Our empirical results, on models trained on
medical notes, show that our attack improves the AUC of prior membership
inference attacks from 0.66 to an alarmingly high 0.90 level, with a
significant improvement in the low-error region: at 1% false positive rate, our
attack is 51X more powerful than prior work."
MiQA: A Benchmark for Inference on Metaphorical Questions,0.625114,"We propose a benchmark to assess the capability of large language models to
reason with conventional metaphors. Our benchmark combines the previously
isolated topics of metaphor detection and commonsense reasoning into a single
task that requires a model to make inferences by accurately selecting between
the literal and metaphorical register. We examine the performance of
state-of-the-art pre-trained models on binary-choice tasks and find a large
discrepancy between the performance of small and very large models, going from
chance to near-human level. We also analyse the largest model in a generative
setting and find that although human performance is approached, careful
multiple-shot prompting is required."
PP-YOLOE: An evolved version of YOLO,0.62568,"In this report, we present PP-YOLOE, an industrial state-of-the-art object
detector with high performance and friendly deployment. We optimize on the
basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful
backbone and neck equipped with CSPRepResStage, ET-head and dynamic label
assignment algorithm TAL. We provide s/m/l/x models for different practice
scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1
FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed
up) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art
industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference
speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct
extensive experiments to verify the effectiveness of our designs. Source code
and pre-trained models are available at
https://github.com/PaddlePaddle/PaddleDetection."
Multimodal Token Fusion for Vision Transformers,0.589839,"Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images. Our
code is available at https://github.com/yikaiw/TokenFusion."
Perception Prioritized Training of Diffusion Models,0.616106,"Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies."
A Graph Neural Network Approach to Automated Model Building in Cryo-EM Maps,0.579403,"Electron cryo-microscopy (cryo-EM) produces three-dimensional (3D) maps of
the electrostatic potential of biological macromolecules, including proteins.
Along with knowledge about the imaged molecules, cryo-EM maps allow de novo
atomic modelling, which is typically done through a laborious manual process.
Taking inspiration from recent advances in machine learning applications to
protein structure prediction, we propose a graph neural network (GNN) approach
for automated model building of proteins in cryo-EM maps. The GNN acts on a
graph with nodes assigned to individual amino acids and edges representing the
protein chain. Combining information from the voxel-based cryo-EM data, the
amino acid sequence data and prior knowledge about protein geometries, the GNN
refines the geometry of the protein chain and classifies the amino acids for
each of its nodes. Application to 28 test cases shows that our approach
outperforms the state-of-the-art and approximates manual building for cryo-EM
maps with resolutions better than 3.5 \r{A}."
PromptBERT: Improving BERT Sentence Embeddings with Prompts,0.624511,"We propose PromptBERT, a novel contrastive learning method for learning
better sentence representation. We firstly analyze the drawback of current
sentence embedding from original BERT and find that it is mainly due to the
static token embedding bias and ineffective BERT layers. Then we propose the
first prompt-based sentence embeddings method and discuss two prompt
representing methods and three prompt searching methods to make BERT achieve
better sentence embeddings. Moreover, we propose a novel unsupervised training
objective by the technology of template denoising, which substantially shortens
the performance gap between the supervised and unsupervised settings. Extensive
experiments show the effectiveness of our method. Compared to SimCSE,
PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and
RoBERTa in the unsupervised setting."
Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense,0.605026,"We develop a novel optimization method for NLPbackdoor inversion. We leverage
a dynamically reducing temperature coefficient in the softmax function to
provide changing loss landscapes to the optimizer such that the process
gradually focuses on the ground truth trigger, which is denoted as a one-hot
value in a convex hull. Our method also features a temperature rollback
mechanism to step away from local optimals, exploiting the observation that
local optimals can be easily deter-mined in NLP trigger inversion (while not in
general optimization). We evaluate the technique on over 1600 models (with
roughly half of them having injected backdoors) on 3 prevailing NLP tasks, with
4 different backdoor attacks and 7 architectures. Our results show that the
technique is able to effectively and efficiently detect and remove backdoors,
outperforming 4 baseline methods."
HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences,0.589843,"In this paper, we tackle the important yet under-investigated problem of
making long-horizon prediction of event sequences. Existing state-of-the-art
models do not perform well at this task due to their autoregressive structure.
We propose HYPRO, a hybridly normalized probabilistic model that naturally fits
this task: its first part is an autoregressive base model that learns to
propose predictions; its second part is an energy function that learns to
reweight the proposals such that more realistic predictions end up with higher
probabilities. We also propose efficient training and inference algorithms for
this model. Experiments on multiple real-world datasets demonstrate that our
proposed HYPRO model can significantly outperform previous models at making
long-horizon predictions of future events. We also conduct a range of ablation
studies to investigate the effectiveness of each component of our proposed
methods."
DocEnTr: An End-to-End Document Image Enhancement Transformer,0.621617,"Document images can be affected by many degradation scenarios, which cause
recognition and processing difficulties. In this age of digitization, it is
important to denoise them for proper usage. To address this challenge, we
present a new encoder-decoder architecture based on vision transformers to
enhance both machine-printed and handwritten document images, in an end-to-end
fashion. The encoder operates directly on the pixel patches with their
positional information without the use of any convolutional layers, while the
decoder reconstructs a clean image from the encoded patches. Conducted
experiments show a superiority of the proposed model compared to the state-of
the-art methods on several DIBCO benchmarks. Code and models will be publicly
available at: \url{https://github.com/dali92002/DocEnTR}."
Biometric Signature Verification Using Recurrent Neural Networks,0.612743,"Architectures based on Recurrent Neural Networks (RNNs) have been
successfully applied to many different tasks such as speech or handwriting
recognition with state-of-the-art results. The main contribution of this work
is to analyse the feasibility of RNNs for on-line signature verification in
real practical scenarios. We have considered a system based on Long Short-Term
Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity
metric from pairs of signatures. For the experimental work, the BiosecurID
database comprised of 400 users and 4 separated acquisition sessions are
considered. Our proposed LSTM RNN system has outperformed the results of recent
published works on the BiosecurID benchmark in figures ranging from 17.76% to
28.00% relative verification performance improvement for skilled forgeries."
Automatic Depression Detection: An Emotional Audio-Textual Corpus and a GRU/BiLSTM-based Model,0.627124,"Depression is a global mental health problem, the worst case of which can
lead to suicide. An automatic depression detection system provides great help
in facilitating depression self-assessment and improving diagnostic accuracy.
In this work, we propose a novel depression detection approach utilizing speech
characteristics and linguistic contents from participants' interviews. In
addition, we establish an Emotional Audio-Textual Depression Corpus
(EATD-Corpus) which contains audios and extracted transcripts of responses from
depressed and non-depressed volunteers. To the best of our knowledge,
EATD-Corpus is the first and only public depression dataset that contains audio
and text data in Chinese. Evaluated on two depression datasets, the proposed
method achieves the state-of-the-art performances. The outperforming results
demonstrate the effectiveness and generalization ability of the proposed
method. The source code and EATD-Corpus are available at
https://github.com/speechandlanguageprocessing/ICASSP2022-Depression."
Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations,0.58123,"We propose an unsupervised method for 3D geometry-aware representation
learning of articulated objects, in which no image-pose pairs or foreground
masks are used for training. Though photorealistic images of articulated
objects can be rendered with explicit pose control through existing 3D neural
representations, these methods require ground truth 3D pose and foreground
masks for training, which are expensive to obtain. We obviate this need by
learning the representations with GAN training. The generator is trained to
produce realistic images of articulated objects from random poses and latent
vectors by adversarial training. To avoid a high computational cost for GAN
training, we propose an efficient neural representation for articulated objects
based on tri-planes and then present a GAN-based framework for its unsupervised
training. Experiments demonstrate the efficiency of our method and show that
GAN-based training enables the learning of controllable 3D representations
without paired supervision."
ByT5 model for massively multilingual grapheme-to-phoneme conversion,0.608394,"In this study, we tackle massively multilingual grapheme-to-phoneme
conversion through implementing G2P models based on ByT5. We have curated a G2P
dataset from various sources that covers around 100 languages and trained
large-scale multilingual G2P models based on ByT5. We found that ByT5 operating
on byte-level inputs significantly outperformed the token-based mT5 model in
terms of multilingual G2P. Pairwise comparison with monolingual models in these
languages suggests that multilingual ByT5 models generally lower the phone
error rate by jointly learning from a variety of languages. The pretrained
model can further benefit low resource G2P through zero-shot prediction on
unseen languages or provides pretrained weights for finetuning, which helps the
model converge to a lower phone error rate than randomly initialized weights.
To facilitate future research on multilingual G2P, we make available our code
and pretrained multilingual G2P models at:
https://github.com/lingjzhu/CharsiuG2P."
In-Hand 3D Object Scanning from an RGB Sequence,0.597316,"We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses."
Motron: Multimodal Probabilistic Human Motion Forecasting,0.625971,"Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms."
Cross-modal Contrastive Learning for Speech Translation,0.669684,"How can we learn unified representations for spoken utterances and their
written text? Learning similar representations for semantically similar speech
and text is important for speech translation. To this end, we propose ConST, a
cross-modal contrastive learning method for end-to-end speech-to-text
translation. We evaluate ConST and a variety of previous baselines on a popular
benchmark MuST-C. Experiments show that the proposed ConST consistently
outperforms the previous methods on, and achieves an average BLEU of 29.4. The
analysis further verifies that ConST indeed closes the representation gap of
different modalities -- its learned representation improves the accuracy of
cross-modal speech-text retrieval from 4% to 88%. Code and models are available
at https://github.com/ReneeYe/ConST."
WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation,0.683889,"Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN"
From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer,0.671794,"Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC."
4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation,0.638811,"In this work, we present a new paradigm, called 4D-StOP, to tackle the task
of 4D Panoptic LiDAR Segmentation. 4D-StOP first generates spatio-temporal
proposals using voting-based center predictions, where each point in the 4D
volume votes for a corresponding center. These tracklet proposals are further
aggregated using learned geometric features. The tracklet aggregation method
effectively generates a video-level 4D scene representation over the entire
space-time volume. This is in contrast to existing end-to-end trainable
state-of-the-art approaches which use spatio-temporal embeddings that are
represented by Gaussian probability distributions. Our voting-based tracklet
generation method followed by geometric feature-based aggregation generates
significantly improved panoptic LiDAR segmentation quality when compared to
modeling the entire 4D volume using Gaussian probability distributions. 4D-StOP
achieves a new state-of-the-art when applied to the SemanticKITTI test dataset
with a score of 63.9 LSTQ, which is a large (+7%) improvement compared to
current best-performing end-to-end trainable methods. The code and pre-trained
models are available at: https://github.com/LarsKreuzberg/4D-StOP."
Coarse-to-Fine Sparse Sequential Recommendation,0.648534,"Sequential recommendation aims to model dynamic user behavior from historical
interactions. Self-attentive methods have proven effective at capturing
short-term dynamics and long-term preferences. Despite their success, these
approaches still struggle to model sparse data, on which they struggle to learn
high-quality item representations. We propose to model user dynamics from
shopping intents and interacted items simultaneously. The learned intents are
coarse-grained and work as prior knowledge for item recommendation. To this
end, we present a coarse-to-fine self-attention framework, namely CaFe, which
explicitly learns coarse-grained and fine-grained sequential dynamics.
Specifically, CaFe first learns intents from coarse-grained sequences which are
dense and hence provide high-quality user intent representations. Then, CaFe
fuses intent representations into item encoder outputs to obtain improved item
representations. Finally, we infer recommended items based on representations
of items and corresponding intents. Experiments on sparse datasets show that
CaFe outperforms state-of-the-art self-attentive recommenders by 44.03% NDCG@5
on average."
Repairing Bugs in Python Assignments Using Large Language Models,0.642346,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average."
Variable Bitrate Neural Fields,0.68203,"Neural approximations of scalar and vector fields, such as signed distance
functions and radiance fields, have emerged as accurate, high-quality
representations. State-of-the-art results are obtained by conditioning a neural
approximation with a lookup from trainable feature grids that take on part of
the learning task and allow for smaller, more efficient neural networks.
Unfortunately, these feature grids usually come at the cost of significantly
increased memory consumption compared to stand-alone neural network models. We
present a dictionary method for compressing such feature grids, reducing their
memory consumption by up to 100x and permitting a multiresolution
representation which can be useful for out-of-core streaming. We formulate the
dictionary optimization as a vector-quantized auto-decoder problem which lets
us learn end-to-end discrete neural representations in a space where no direct
supervision is available and with dynamic topology and structure. Our source
code will be available at https://github.com/nv-tlabs/vqad."
Comparative layer-wise analysis of self-supervised speech models,0.678831,"Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive successes on downstream tasks, we still have a
limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models."
Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,0.663112,"Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning."
Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models,0.661245,"Pre-trained vision-language models (e.g., CLIP) have shown promising
zero-shot generalization in many downstream tasks with properly designed text
prompts. Instead of relying on hand-engineered prompts, recent works learn
prompts using the training data from downstream tasks. While effective,
training on domain-specific data reduces a model's generalization capability to
unseen new domains. In this work, we propose test-time prompt tuning (TPT), a
method that can learn adaptive prompts on the fly with a single test sample.
For image classification, TPT optimizes the prompt by minimizing the entropy
with confidence selection so that the model has consistent predictions across
different augmented views of each test sample. In evaluating generalization to
natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP
by 3.6% on average, surpassing previous prompt tuning approaches that require
additional task-specific training data. In evaluating cross-dataset
generalization with unseen categories, TPT performs on par with the
state-of-the-art approaches that use additional training data. Project page:
https://azshue.github.io/TPT."
MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations,0.68078,"Emotion Recognition in Conversations (ERC) has considerable prospects for
developing empathetic machines. For multimodal ERC, it is vital to understand
context and fuse modality information in conversations. Recent graph-based
fusion methods generally aggregate multimodal information by exploring unimodal
and cross-modal interactions in a graph. However, they accumulate redundant
information at each layer, limiting the context understanding between
modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network
(MM-DFN) to recognize emotions by fully understanding multimodal conversational
context. Specifically, we design a new graph-based dynamic fusion module to
fuse multimodal contextual features in a conversation. The module reduces
redundancy and enhances complementarity between modalities by capturing the
dynamics of contextual information in different semantic spaces. Extensive
experiments on two public benchmark datasets demonstrate the effectiveness and
superiority of MM-DFN."
S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers,0.656669,"The task of converting a natural language question into an executable SQL
query, known as text-to-SQL, is an important branch of semantic parsing. The
state-of-the-art graph-based encoder has been successfully used in this task
but does not model the question syntax well. In this paper, we propose
S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL
parsers, which effectively leverages the syntactic dependency information of
questions in text-to-SQL to improve the performance. We also employ the
decoupling constraint to induce diverse relational edge embedding, which
further improves the network's performance. Experiments on the Spider and
robustness setting Spider-Syn demonstrate that the proposed approach
outperforms all existing methods when pre-training models are used, resulting
in a performance ranks first on the Spider leaderboard."
Dynamic Global Memory for Document-level Argument Extraction,0.650797,"Extracting informative arguments of events from news articles is a
challenging problem in information extraction, which requires a global
contextual understanding of each document. While recent work on document-level
extraction has gone beyond single-sentence and increased the cross-sentence
inference capability of end-to-end models, they are still restricted by certain
input sequence length constraints and usually ignore the global context between
events. To tackle this issue, we introduce a new global neural generation-based
framework for document-level event argument extraction by constructing a
document memory store to record the contextual event information and leveraging
it to implicitly and explicitly help with decoding of arguments for later
events. Empirical results show that our framework outperforms prior methods
substantially and it is more robust to adversarially annotated examples with
our constrained decoding design. (Our code and resources are available at
https://github.com/xinyadu/memory_docie for research purpose.)"
"DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",0.647909,"Discriminative learning, restorative learning, and adversarial learning have
proven beneficial for self-supervised learning schemes in computer vision and
medical imaging. Existing efforts, however, omit their synergistic effects on
each other in a ternary setup, which, we envision, can significantly benefit
deep semantic representation learning. To realize this vision, we have
developed DiRA, the first framework that unites discriminative, restorative,
and adversarial learning in a unified manner to collaboratively glean
complementary visual information from unlabeled medical images for fine-grained
semantic representation learning. Our extensive experiments demonstrate that
DiRA (1) encourages collaborative learning among three learning ingredients,
resulting in more generalizable representation across organs, diseases, and
modalities; (2) outperforms fully supervised ImageNet models and increases
robustness in small data regimes, reducing annotation cost across multiple
medical imaging applications; (3) learns fine-grained semantic representation,
facilitating accurate lesion localization with only image-level annotation; and
(4) enhances state-of-the-art restorative approaches, revealing that DiRA is a
general mechanism for united representation learning. All code and pre-trained
models are available at https: //github.com/JLiangLab/DiRA."
Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs,0.659135,"Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly
challenging problem as traditional subgraph matching methods are not capable to
deal with noise and missing information. To address this problem, it has been
recently introduced a promising approach based on jointly embedding logical
queries and KGs into a low-dimensional space to identify answer entities.
However, existing proposals ignore critical semantic knowledge inherently
available in KGs, such as type information. To leverage type information, we
propose a novel TypE-aware Message Passing (TEMP) model, which enhances the
entity and relation representations in queries, and simultaneously improves
generalization, deductive and inductive reasoning. Remarkably, TEMP is a
plug-and-play model that can be easily incorporated into existing
embedding-based models to improve their performance. Extensive experiments on
three real-world datasets demonstrate TEMP's effectiveness."
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,0.637793,"This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner"
COEM: Cross-Modal Embedding for MetaCell Identification,0.64459,"Metacells are disjoint and homogeneous groups of single-cell profiles,
representing discrete and highly granular cell states. Existing metacell
algorithms tend to use only one modality to infer metacells, even though
single-cell multi-omics datasets profile multiple molecular modalities within
the same cell. Here, we present \textbf{C}ross-M\textbf{O}dal
\textbf{E}mbedding for \textbf{M}etaCell Identification (COEM), which utilizes
an embedded space leveraging the information of both scATAC-seq and scRNA-seq
to perform aggregation, balancing the trade-off between fine resolution and
sufficient sequencing coverage. COEM outperforms the state-of-the-art method
SEACells by efficiently identifying accurate and well-separated metacells
across datasets with continuous and discrete cell types. Furthermore, COEM
significantly improves peak-to-gene association analyses, and facilitates
complex gene regulatory inference tasks."
ON-DEMAND-FL: A Dynamic and Efficient Multi-Criteria Federated Learning Client Deployment Scheme,0.653756,"In this paper, we increase the availability and integration of devices in the
learning process to enhance the convergence of federated learning (FL) models.
To address the issue of having all the data in one location, federated
learning, which maintains the ability to learn over decentralized data sets,
combines privacy and technology. Until the model converges, the server combines
the updated weights obtained from each dataset over a number of rounds. The
majority of the literature suggested client selection techniques to accelerate
convergence and boost accuracy. However, none of the existing proposals have
focused on the flexibility to deploy and select clients as needed, wherever and
whenever that may be. Due to the extremely dynamic surroundings, some devices
are actually not available to serve as clients in FL, which affects the
availability of data for learning and the applicability of the existing
solution for client selection. In this paper, we address the aforementioned
limitations by introducing an On-Demand-FL, a client deployment approach for
FL, offering more volume and heterogeneity of data in the learning process. We
make use of the containerization technology such as Docker to build efficient
environments using IoT and mobile devices serving as volunteers. Furthermore,
Kubernetes is used for orchestration. The Genetic algorithm (GA) is used to
solve the multi-objective optimization problem due to its evolutionary
strategy. The performed experiments using the Mobile Data Challenge (MDC)
dataset and the Localfed framework illustrate the relevance of the proposed
approach and the efficiency of the on-the-fly deployment of clients whenever
and wherever needed with less discarded rounds and more available data."
Learning Non-target Knowledge for Few-shot Semantic Segmentation,0.663444,"Existing studies in few-shot semantic segmentation only focus on mining the
target object information, however, often are hard to tell ambiguous regions,
especially in non-target regions, which include background (BG) and Distracting
Objects (DOs). To alleviate this problem, we propose a novel framework, namely
Non-Target Region Eliminating (NTRE) network, to explicitly mine and eliminate
BG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to
extract the BG region via learning a general BG prototype. To this end, we
design a BG loss to supervise the learning of BGMM only using the known target
object segmentation ground truth. Then, a BG Eliminating Module and a DO
Eliminating Module are proposed to successively filter out the BG and DO
information from the query feature, based on which we can obtain a BG and
DO-free target object segmentation result. Furthermore, we propose a
prototypical contrastive learning algorithm to improve the model ability of
distinguishing the target object from DOs. Extensive experiments on both
PASCAL-5i and COCO-20i datasets show that our approach is effective despite its
simplicity."
TransBoost: Improving the Best ImageNet Performance using Deep Transduction,0.632121,"This paper deals with deep transductive learning, and proposes TransBoost as
a procedure for fine-tuning any deep neural model to improve its performance on
any (unlabeled) test set provided at training time. TransBoost is inspired by a
large margin principle and is efficient and simple to use. Our method
significantly improves the ImageNet classification performance on a wide range
of architectures, such as ResNets, MobileNetV3-L, EfficientNetB0, ViT-S, and
ConvNext-T, leading to state-of-the-art transductive performance. Additionally
we show that TransBoost is effective on a wide variety of image classification
datasets. The implementation of TransBoost is provided at:
https://github.com/omerb01/TransBoost ."
Experimental analysis regarding the influence of iris segmentation on the recognition rate,0.638874,"In this study the authors will look at the detection and segmentation of the
iris and its influence on the overall performance of the iris-biometric tool
chain. The authors will examine whether the segmentation accuracy, based on
conformance with a ground truth, can serve as a predictor for the overall
performance of the iris-biometric tool chain. That is: If the segmentation
accuracy is improved will this always improve the overall performance?
Furthermore, the authors will systematically evaluate the influence of
segmentation parameters, pupillary and limbic boundary and normalisation centre
(based on Daugman's rubbersheet model), on the rest of the iris-biometric tool
chain. The authors will investigate if accurately finding these parameters is
important and how consistency, that is, extracting the same exact region of the
iris during segmenting, influences the overall performance."
Unified Chinese License Plate Detection and Recognition with High Efficiency,0.638564,"Recently, deep learning-based methods have reached an excellent performance
on License Plate (LP) detection and recognition tasks. However, it is still
challenging to build a robust model for Chinese LPs since there are not enough
large and representative datasets. In this work, we propose a new dataset named
Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP
images as a supplement to the existing public benchmarks. The images are mainly
captured with electronic monitoring systems with detailed annotations. To our
knowledge, CRPD is the largest public multi-objective Chinese LP dataset with
annotations of vertices. With CRPD, a unified detection and recognition network
with high efficiency is presented as the baseline. The network is end-to-end
trainable with totally real-time inference efficiency (30 fps with 640p). The
experiments on several public benchmarks demonstrate that our method has
reached competitive performance. The code and dataset will be publicly
available at https://github.com/yxgong0/CRPD."
LEBP -- Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents,0.666676,"People always desire an embodied agent that can perform a task by
understanding language instruction. Moreover, they also want to monitor and
expect agents to understand commands the way they expected. But, how to build
such an embodied agent is still unclear. Recently, people can explore this
problem with the Vision-and-Language Interaction benchmark ALFRED, which
requires an agent to perform complicated daily household tasks following
natural language instructions in unseen scenes. In this paper, we propose LEBP
-- Language Expectation and Binding Policy Module to tackle the ALFRED. The
LEBP contains a two-stream process: 1) It first conducts a language expectation
module to generate an expectation describing how to perform tasks by
understanding the language instruction. The expectation consists of a sequence
of sub-steps for the task (e.g., Pick an apple). The expectation allows people
to access and check the understanding results of instructions before the agent
takes actual actions, in case the task might go wrong. 2) Then, it uses the
binding policy module to bind sub-steps in expectation to actual actions to
specific scenarios. Actual actions include navigation and object manipulation.
Experimental results suggest our approach achieves comparable performance to
currently published SOTA methods and can avoid large decay from seen scenarios
to unseen scenarios."
BotSIM: An End-to-End Bot Simulation Toolkit for Commercial Task-Oriented Dialog Systems,0.632121,"We introduce BotSIM, a modular, open-source Bot SIMulation environment with
dialog generation, user simulation and conversation analytics capabilities.
BotSIM aims to serve as a one-stop solution for large-scale data-efficient
end-to-end evaluation, diagnosis and remediation of commercial task-oriented
dialog (TOD) systems to significantly accelerate commercial bot development and
evaluation, reduce cost and time-to-market. BotSIM adopts a layered design
comprising the infrastructure layer, the adaptor layer and the application
layer. The infrastructure layer hosts key models and components to support
BotSIM's major functionalities via a streamlined
""generation-simulation-remediation"" pipeline. The adaptor layer is used to
extend BotSIM to accommodate new bot platforms. The application layer provides
a suite of command line tools and a Web App to significantly lower the entry
barrier for BotSIM users such as bot admins or practitioners. In this report,
we focus on the technical designs of various system components. A detailed case
study using Einstein BotBuilder is also presented to show how to apply BotSIM
pipeline for bot evaluation and remediation. The detailed system descriptions
can be found in our system demo paper. The toolkit is available at:
https://github.com/salesforce/BotSIM ."
DSI++: Updating Transformer Memory with New Documents,0.638969,"Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence."
Data Contamination: From Memorization to Exploitation,0.649438,"Pretrained language models are typically trained on massive web-based
datasets, which are often ""contaminated"" with downstream test sets. It is not
clear to what extent models exploit the contaminated data for downstream tasks.
We present a principled method to study this question. We pretrain BERT models
on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune
them on the relevant task. Comparing performance between samples seen and
unseen during pretraining enables us to define and quantify levels of
memorization and exploitation. Experiments with two models and three downstream
tasks show that exploitation exists in some cases, but in others the models
memorize the contaminated data, but do not exploit it. We show that these two
measures are affected by different factors such as the number of duplications
of the contaminated data and the model size. Our results highlight the
importance of analyzing massive web-scale datasets to verify that progress in
NLP is obtained by better language understanding and not better data
exploitation."
Gendered Mental Health Stigma in Masked Language Models,0.659273,"Mental health stigma prevents many individuals from receiving the appropriate
care, and social psychology studies have shown that mental health tends to be
overlooked in men. In this work, we investigate gendered mental health stigma
in masked language models. In doing so, we operationalize mental health stigma
by developing a framework grounded in psychology research: we use clinical
psychology literature to curate prompts, then evaluate the models' propensity
to generate gendered words. We find that masked language models capture
societal stigma about gender in mental health: models are consistently more
likely to predict female subjects than male in sentences about having a mental
health condition (32% vs. 19%), and this disparity is exacerbated for sentences
that indicate treatment-seeking behavior. Furthermore, we find that different
models capture dimensions of stigma differently for men and women, associating
stereotypes like anger, blame, and pity more with women with mental health
conditions than with men. In showing the complex nuances of models' gendered
mental health stigma, we demonstrate that context and overlapping dimensions of
identity are important considerations when assessing computational models'
social biases."
Questions Are All You Need to Train a Dense Passage Retriever,0.706879,"We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses."
Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding,0.731829,"Contrastive learning has become a new paradigm for unsupervised sentence
embeddings. Previous studies focus on instance-wise contrastive learning,
attempting to construct positive pairs with textual data augmentation. In this
paper, we propose a novel Contrastive learning method with Prompt-derived
Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts,
we construct virtual semantic prototypes to each instance, and derive negative
prototypes by using the negative form of the prompts. Using a prototypical
contrastive loss, we enforce the anchor sentence embedding to be close to its
corresponding semantic prototypes, and far apart from the negative prototypes
as well as the prototypes of other sentences. Extensive experimental results on
semantic textual similarity, transfer, and clustering tasks demonstrate the
effectiveness of our proposed model compared to strong baselines. Code is
available at https://github.com/lemon0830/promptCSE."
Towards a Cleaner Document-Oriented Multilingual Crawled Corpus,0.69908,"The need for raw large raw corpora has dramatically increased in recent years
with the introduction of transfer learning and semi-supervised learning methods
to Natural Language Processing. And while there have been some recent attempts
to manually curate the amount of data necessary to train large language models,
the main way to obtain this data is still through automatic web crawling. In
this paper we take the existing multilingual web corpus OSCAR and its pipeline
Ungoliant that extracts and classifies data from Common Crawl at the line
level, and propose a set of improvements and automatic annotations in order to
produce a new document-oriented version of OSCAR that could prove more suitable
to pre-train large generative language models as well as hopefully other
applications in Natural Language Processing and Digital Humanities."
How to Fine-Tune Vision Models with SGD,0.699011,"SGD and AdamW are the two most used optimizers for fine-tuning large neural
networks in computer vision. When the two methods perform the same, SGD is
preferable because it uses less memory (12 bytes/parameter with momentum and 8
bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite
of downstream tasks, especially those with distribution shifts, we find that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first ""embedding""
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: freezing the
embedding layer (less than 1% of the parameters) leads to SGD with or without
momentum performing slightly better than AdamW while using less memory (e.g.,
on ViT-L, SGD uses 33% less GPU memory). Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet."
DiGamma: Domain-aware Genetic Algorithm for HW-Mapping Co-optimization for DNN Accelerators,0.73134,"The design of DNN accelerators includes two key parts: HW resource
configuration and mapping strategy. Intensive research has been conducted to
optimize each of them independently. Unfortunately, optimizing for both
together is extremely challenging due to the extremely large cross-coupled
search space. To address this, in this paper, we propose a HW-Mapping
co-optimization framework, an efficient encoding of the immense design space
constructed by HW and Mapping, and a domain-aware genetic algorithm, named
DiGamma, with specialized operators for improving search efficiency. We
evaluate DiGamma with seven popular DNNs models with different properties. Our
evaluations show DiGamma can achieve (geomean) 3.0x and 10.0x speedup,
comparing to the best-performing baseline optimization algorithms, in edge and
cloud settings."
Learning Progressive Modality-shared Transformers for Effective Visible-Infrared Person Re-identification,0.708943,"Visible-Infrared Person Re-Identification (VI-ReID) is a challenging
retrieval task under complex modality changes. Existing methods usually focus
on extracting discriminative visual features while ignoring the reliability and
commonality of visual features between different modalities. In this paper, we
propose a novel deep learning framework named Progressive Modality-shared
Transformer (PMT) for effective VI-ReID. To reduce the negative effect of
modality gaps, we first take the gray-scale images as an auxiliary modality and
propose a progressive learning strategy. Then, we propose a Modality-Shared
Enhancement Loss (MSEL) to guide the model to explore more reliable identity
information from modality-shared features. Finally, to cope with the problem of
large intra-class differences and small inter-class differences, we propose a
Discriminative Center Loss (DCL) combined with the MSEL to further improve the
discrimination of reliable features. Extensive experiments on SYSU-MM01 and
RegDB datasets show that our proposed framework performs better than most
state-of-the-art methods. For model reproduction, we release the source code at
https://github.com/hulu88/PMT."
Event Transformer. A sparse-aware solution for efficient event data processing,0.696371,"Event cameras are sensors of great interest for many applications that run in
low-resource and challenging environments. They log sparse illumination changes
with high temporal resolution and high dynamic range, while they present
minimal power consumption. However, top-performing methods often ignore
specific event-data properties, leading to the development of generic but
computationally expensive algorithms. Efforts toward efficient solutions
usually do not achieve top-accuracy results for complex tasks. This work
proposes a novel framework, Event Transformer (EvT), that effectively takes
advantage of event-data properties to be highly efficient and accurate. We
introduce a new patch-based event representation and a compact transformer-like
architecture to process it. EvT is evaluated on different event-based
benchmarks for action and gesture recognition. Evaluation results show better
or comparable accuracy to the state-of-the-art while requiring significantly
less computation resources, which makes EvT able to work with minimal latency
both on GPU and CPU."
PLM-ICD: Automatic ICD Coding with Pretrained Language Models,0.718648,"Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD"
CINO: A Chinese Minority Pre-trained Language Model,0.7092,"Multilingual pre-trained language models have shown impressive performance on
cross-lingual tasks. It greatly facilitates the applications of natural
language processing on low-resource languages. However, there are still some
languages that the current multilingual models do not perform well on. In this
paper, we propose CINO (Chinese Minority Pre-trained Language Model), a
multilingual pre-trained language model for Chinese minority languages. It
covers Standard Chinese, Yue Chinese, and six other ethnic minority languages.
To evaluate the cross-lingual ability of the multilingual model on ethnic
minority languages, we collect documents from Wikipedia and news websites, and
construct two text classification datasets, WCM (Wiki-Chinese-Minority) and
CMNews (Chinese-Minority-News). We show that CINO notably outperforms the
baselines on various classification tasks. The CINO model and the datasets are
publicly available at http://cino.hfl-rc.com."
SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,0.69777,"We present SpeechMatrix, a large-scale multilingual corpus of
speech-to-speech translations mined from real speech of European Parliament
recordings. It contains speech alignments in 136 language pairs with a total of
418 thousand hours of speech. To evaluate the quality of this parallel speech,
we train bilingual speech-to-speech translation models on mined data only and
establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test
sets. Enabled by the multilinguality of SpeechMatrix, we also explore
multilingual speech-to-speech translation, a topic which was addressed by few
other works. We also demonstrate that model pre-training and sparse scaling
using Mixture-of-Experts bring large gains to translation performance. The
mined data and models are freely available."
Generative Modelling With Inverse Heat Dissipation,0.713086,"While diffusion models have shown great success in image generation, their
noise-inverting generative process does not explicitly consider the structure
of images, such as their inherent multi-scale nature. Inspired by diffusion
models and the empirical success of coarse-to-fine modelling, we propose a new
diffusion-like model that generates images through stochastically reversing the
heat equation, a PDE that locally erases fine-scale information when run over
the 2D plane of the image. We interpret the solution of the forward heat
equation with constant additive noise as a variational approximation in the
diffusion latent variable model. Our new model shows emergent qualitative
properties not seen in standard diffusion models, such as disentanglement of
overall colour and shape in images. Spectral analysis on natural images
highlights connections to diffusion models and reveals an implicit
coarse-to-fine inductive bias in them."
Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting,0.694871,"In this paper, we propose a novel end-to-end user-defined keyword spotting
method that utilizes linguistically corresponding patterns between speech and
text sequences. Unlike previous approaches requiring speech keyword enrollment,
our method compares input queries with an enrolled text keyword sequence. To
place the audio and text representations within a common latent space, we adopt
an attention-based cross-modal matching approach that is trained in an
end-to-end manner with monotonic matching loss and keyword classification loss.
We also utilize a de-noising loss for the acoustic embedding network to improve
robustness in noisy environments. Additionally, we introduce the LibriPhrase
dataset, a new short-phrase dataset based on LibriSpeech for efficiently
training keyword spotting models. Our proposed method achieves competitive
results on various evaluation sets compared to other single-modal and
cross-modal baselines."
Meta Spatio-Temporal Debiasing for Video Scene Graph Generation,0.694763,"Video scene graph generation (VidSGG) aims to parse the video content into
scene graphs, which involves modeling the spatio-temporal contextual
information in the video. However, due to the long-tailed training data in
datasets, the generalization performance of existing VidSGG models can be
affected by the spatio-temporal conditional bias problem. In this work, from
the perspective of meta-learning, we propose a novel Meta Video Scene Graph
Generation (MVSGG) framework to address such a bias problem. Specifically, to
handle various types of spatio-temporal conditional biases, our framework first
constructs a support set and a group of query sets from the training data,
where the data distribution of each query set is different from that of the
support set w.r.t. a type of conditional bias. Then, by performing a novel meta
training and testing process to optimize the model to obtain good testing
performance on these query sets after training on the support set, our
framework can effectively guide the model to learn to well generalize against
biases. Extensive experiments demonstrate the efficacy of our proposed
framework."
Delving into Out-of-Distribution Detection with Vision-Language Representations,0.693359,"Recognizing out-of-distribution (OOD) samples is critical for machine
learning systems deployed in the open world. The vast majority of OOD detection
methods are driven by a single modality (e.g., either vision or language),
leaving the rich information in multi-modal representations untapped. Inspired
by the recent success of vision-language pre-training, this paper enriches the
landscape of OOD detection from a single-modal to a multi-modal regime.
Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective
zero-shot OOD detection method based on aligning visual features with textual
concepts. We contribute in-depth analysis and theoretical insights to
understand the effectiveness of MCM. Extensive experiments demonstrate that MCM
achieves superior performance on a wide variety of real-world tasks. MCM with
vision-language features outperforms a common baseline with pure visual
features on a hard OOD task with semantically similar classes by 13.1% (AUROC).
Code is available at https://github.com/deeplearning-wisc/MCM."
Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss,0.734401,"In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings."
AutoAttention: Automatic Field Pair Selection for Attention in User Behavior Modeling,0.698806,"In Click-through rate (CTR) prediction models, a user's interest is usually
represented as a fixed-length vector based on her history behaviors. Recently,
several methods are proposed to learn an attentive weight for each user
behavior and conduct weighted sum pooling. However, these methods only manually
select several fields from the target item side as the query to interact with
the behaviors, neglecting the other target item fields, as well as user and
context fields. Directly including all these fields in the attention may
introduce noise and deteriorate the performance. In this paper, we propose a
novel model named AutoAttention, which includes all item/user/context side
fields as the query, and assigns a learnable weight for each field pair between
behavior fields and query fields. Pruning on these field pairs via these
learnable weights lead to automatic field pair selection, so as to identify and
remove noisy field pairs. Though including more fields, the computation cost of
AutoAttention is still low due to using a simple attention function and field
pair selection. Extensive experiments on the public dataset and Tencent's
production dataset demonstrate the effectiveness of the proposed approach."
Human-to-Robot Imitation in the Wild,0.712292,"We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io"
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning,0.686992,"Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt."
Semantic-aligned Fusion Transformer for One-shot Object Detection,0.692228,"One-shot object detection aims at detecting novel objects according to merely
one given instance. With extreme data scarcity, current approaches explore
various feature fusions to obtain directly transferable meta-knowledge. Yet,
their performances are often unsatisfactory. In this paper, we attribute this
to inappropriate correlation methods that misalign query-support semantics by
overlooking spatial structures and scale variances. Upon analysis, we leverage
the attention mechanism and propose a simple but effective architecture named
Semantic-aligned Fusion Transformer (SaFT) to resolve these issues.
Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale
semantic enhancement and a horizontal fusion module (HFM) for cross-sample
feature fusion. Together, they broaden the vision for each feature point from
the support to a whole augmented feature pyramid from the query, facilitating
semantic-aligned associations. Extensive experiments on multiple benchmarks
demonstrate the superiority of our framework. Without fine-tuning on novel
classes, it brings significant performance gains to one-stage baselines,
lifting state-of-the-art results to a higher level."
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing,0.701912,"We present PHORHUM, a novel, end-to-end trainable, deep neural network
methodology for photorealistic 3D human reconstruction given just a monocular
RGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the
first time, the unshaded surface color together with the scene illumination.
Observing that 3D supervision alone is not sufficient for high fidelity color
reconstruction, we introduce patch-based rendering losses that enable reliable
color reconstruction on visible parts of the human, and detailed and plausible
color estimation for the non-visible parts. Moreover, our method specifically
addresses methodological and practical limitations of prior work in terms of
representing geometry, albedo, and illumination effects, in an end-to-end model
where factors can be effectively disentangled. In extensive experiments, we
demonstrate the versatility and robustness of our approach. Our
state-of-the-art results validate the method qualitatively and for different
metrics, for both geometric and color reconstruction."
Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation,0.687172,"In this paper we present Mask DINO, a unified object detection and
segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising
Anchor Boxes) by adding a mask prediction branch which supports all image
segmentation tasks (instance, panoptic, and semantic). It makes use of the
query embeddings from DINO to dot-product a high-resolution pixel embedding map
to predict a set of binary masks. Some key components in DINO are extended for
segmentation through a shared architecture and training process. Mask DINO is
simple, efficient, and scalable, and it can benefit from joint large-scale
detection and segmentation datasets. Our experiments show that Mask DINO
significantly outperforms all existing specialized segmentation methods, both
on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably,
Mask DINO establishes the best results to date on instance segmentation (54.5
AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation
(60.8 mIoU on ADE20K) among models under one billion parameters. Code is
available at \url{https://github.com/IDEACVR/MaskDINO}."
SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations,0.721929,"Accurate mapping of large-scale environments is an essential building block
of most outdoor autonomous systems. Challenges of traditional mapping methods
include the balance between memory consumption and mapping accuracy. This paper
addresses the problem of achieving large-scale 3D reconstruction using implicit
representations built from 3D LiDAR measurements. We learn and store implicit
features through an octree-based, hierarchical structure, which is sparse and
extensible. The implicit features can be turned into signed distance values
through a shallow neural network. We leverage binary cross entropy loss to
optimize the local features with the 3D measurements as supervision. Based on
our implicit representation, we design an incremental mapping system with
regularization to tackle the issue of forgetting in continual learning. Our
experiments show that our 3D reconstructions are more accurate, complete, and
memory-efficient than current state-of-the-art 3D mapping methods."
MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing,0.728664,"Snow removal causes challenges due to its characteristic of complex
degradations. To this end, targeted treatment of multi-scale snow degradations
is critical for the network to learn effective snow removal. In order to handle
the diverse scenes, we propose a multi-scale projection transformer
(MSP-Former), which understands and covers a variety of snow degradation
features in a multi-path manner, and integrates comprehensive scene context
information for clean reconstruction via self-attention operation. For the
local details of various snow degradations, the local capture module is
introduced in parallel to assist in the rebuilding of a clean image. Such
design achieves the SOTA performance on three desnowing benchmark datasets
while costing the low parameters and computational complexity, providing a
guarantee of practicality."
iCaps: Iterative Category-level Object Pose and Shape Estimation,0.697521,"This paper proposes a category-level 6D object pose and shape estimation
approach iCaps, which allows tracking 6D poses of unseen objects in a category
and estimating their 3D shapes. We develop a category-level auto-encoder
network using depth images as input, where feature embeddings from the
auto-encoder encode poses of objects in a category. The auto-encoder can be
used in a particle filter framework to estimate and track 6D poses of objects
in a category. By exploiting an implicit shape representation based on signed
distance functions, we build a LatentNet to estimate a latent representation of
the 3D shape given the estimated pose of an object. Then the estimated pose and
shape can be used to update each other in an iterative way. Our category-level
6D object pose and shape estimation pipeline only requires 2D detection and
segmentation for initialization. We evaluate our approach on a publicly
available dataset and demonstrate its effectiveness. In particular, our method
achieves comparably high accuracy on shape estimation."
LISA: Learning Implicit Shape and Appearance of Hands,0.734463,"This paper proposes a do-it-all neural model of human hands, named LISA. The
model can capture accurate hand shape and appearance, generalize to arbitrary
hand subjects, provide dense surface correspondences, be reconstructed from
images in the wild and easily animated. We train LISA by minimizing the shape
and appearance losses on a large set of multi-view RGB image sequences
annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand
local coordinate, our model predicts the color and the signed distance with
respect to each hand bone independently, and then combines the per-bone
predictions using predicted skinning weights. The shape, color and pose
representations are disentangled by design, allowing to estimate or animate
only selected parameters. We experimentally demonstrate that LISA can
accurately reconstruct a dynamic hand from monocular or multi-view sequences,
achieving a noticeably higher quality of reconstructed hand shapes compared to
baseline approaches. Project page:
https://www.iri.upc.edu/people/ecorona/lisa/."
DeiT III: Revenge of the ViT,0.687369,"A Vision Transformer (ViT) is a simple neural architecture amenable to serve
several computer vision tasks. It has limited built-in architectural priors, in
contrast to more recent architectures that incorporate priors either about the
input data or of specific tasks. Recent works show that ViTs benefit from
self-supervised pre-training, in particular BerT-like pre-training like BeiT.
In this paper, we revisit the supervised training of ViTs. Our procedure builds
upon and simplifies a recipe introduced for training ResNet-50. It includes a
new simple data-augmentation procedure with only 3 augmentations, closer to the
practice in self-supervised learning. Our evaluations on Image classification
(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning
and semantic segmentation show that our procedure outperforms by a large margin
previous fully supervised training recipes for ViT. It also reveals that the
performance of our ViT trained with supervision is comparable to that of more
recent architectures. Our results could serve as better baselines for recent
self-supervised approaches demonstrated on ViT."
Latent Image Animator: Learning to Animate Images via Latent Space Navigation,0.70954,"Due to the remarkable progress of deep generative models, animating images
has become increasingly efficient, whereas associated results have become
increasingly realistic. Current animation-approaches commonly exploit structure
representation extracted from driving videos. Such structure representation is
instrumental in transferring motion from driving videos to still images.
However, such approaches fail in case the source image and driving video
encompass large appearance variation. Moreover, the extraction of structure
information requires additional modules that endow the animation-model with
increased complexity. Deviating from such models, we here introduce the Latent
Image Animator (LIA), a self-supervised autoencoder that evades need for
structure representation. LIA is streamlined to animate images by linear
navigation in the latent space. Specifically, motion in generated video is
constructed by linear displacement of codes in the latent space. Towards this,
we learn a set of orthogonal motion directions simultaneously, and use their
linear combination, in order to represent any displacement in the latent space.
Extensive quantitative and qualitative analysis suggests that our model
systematically and significantly outperforms state-of-art methods on VoxCeleb,
Taichi and TED-talk datasets w.r.t. generated quality."
CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,0.764101,"Constructing benchmarks that test the abilities of modern natural language
understanding models is difficult - pre-trained language models exploit
artifacts in benchmarks to achieve human parity, but still fail on adversarial
examples and make errors that demonstrate a lack of common sense. In this work,
we propose gamification as a framework for data construction. The goal of
players in the game is to compose questions that mislead a rival AI while using
specific phrases for extra points. The game environment leads to enhanced user
engagement and simultaneously gives the game designer control over the
collected data, allowing us to collect high-quality data at scale. Using our
method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and
demonstrate its difficulty for models that are orders-of-magnitude larger than
the AI used in the game itself. Our best baseline, the T5-based Unicorn with
11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3
(52.9%) in a few-shot inference setup. Both score well below human performance
which is at 94.1%."
CORE: Simple and Effective Session-based Recommendation within Consistent Representation Space,0.786109,"Session-based Recommendation (SBR) refers to the task of predicting the next
item based on short-term user behaviors within an anonymous session. However,
session embedding learned by a non-linear encoder is usually not in the same
representation space as item embeddings, resulting in the inconsistent
prediction issue while recommending items. To address this issue, we propose a
simple and effective framework named CORE, which can unify the representation
space for both the encoding and decoding processes. Firstly, we design a
representation-consistent encoder that takes the linear combination of input
item embeddings as session embedding, guaranteeing that sessions and items are
in the same representation space. Besides, we propose a robust distance
measuring method to prevent overfitting of embeddings in the consistent
representation space. Extensive experiments conducted on five public real-world
datasets demonstrate the effectiveness and efficiency of the proposed method.
The code is available at: https://github.com/RUCAIBox/CORE."
M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction,0.774122,"Predicting future motions of road participants is an important task for
driving autonomously in urban scenes. Existing models excel at predicting
marginal trajectories for single agents, yet it remains an open question to
jointly predict scene compliant trajectories over multiple agents. The
challenge is due to exponentially increasing prediction space as a function of
the number of agents. In this work, we exploit the underlying relations between
interacting agents and decouple the joint prediction problem into marginal
prediction problems. Our proposed approach M2I first classifies interacting
agents as pairs of influencers and reactors, and then leverages a marginal
prediction model and a conditional prediction model to predict trajectories for
the influencers and reactors, respectively. The predictions from interacting
agents are combined and selected according to their joint likelihoods.
Experiments show that our simple but effective approach achieves
state-of-the-art performance on the Waymo Open Motion Dataset interactive
prediction benchmark."
"Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus",0.771684,"The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the
performance of general artificial intelligence algorithms. The ARC's focus on
broad generalization and few-shot learning has made it difficult to solve using
pure machine learning. A more promising approach has been to perform program
synthesis within an appropriately designed Domain Specific Language (DSL).
However, these too have seen limited success. We propose Abstract Reasoning
with Graph Abstractions (ARGA), a new object-centric framework that first
represents images using graphs and then performs a search for a correct program
in a DSL that is based on the abstracted graph space. The complexity of this
combinatorial search is tamed through the use of constraint acquisition, state
hashing, and Tabu search. An extensive set of experiments demonstrates the
promise of ARGA in tackling some of the complicated object-centric tasks of the
ARC rather efficiently, producing programs that are correct and easy to
understand."
RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs,0.762052,"Blind face restoration is to recover a high-quality face image from unknown
degradations. As face image contains abundant contextual information, we
propose a method, RestoreFormer, which explores fully-spatial attentions to
model contextual information and surpasses existing works that use local
operators. RestoreFormer has several benefits compared to prior arts. First,
unlike the conventional multi-head self-attention in previous Vision
Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention
layer to learn fully-spatial interactions between corrupted queries and
high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are
sampled from a reconstruction-oriented high-quality dictionary, whose elements
are rich in high-quality facial features specifically aimed for face
reconstruction, leading to superior restoration results. Third, RestoreFormer
outperforms advanced state-of-the-art methods on one synthetic dataset and
three real-world datasets, as well as produces images with better visual
quality."
Training Language Models with Memory Augmentation,0.757998,"Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs."
LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding,0.78243,"Structured document understanding has attracted considerable attention and
made significant progress recently, owing to its crucial role in intelligent
document processing. However, most existing related models can only deal with
the document data of specific language(s) (typically English) included in the
pre-training collection, which is extremely limited. To address this issue, we
propose a simple yet effective Language-independent Layout Transformer (LiLT)
for structured document understanding. LiLT can be pre-trained on the
structured documents of a single language and then directly fine-tuned on other
languages with the corresponding off-the-shelf monolingual/multilingual
pre-trained textual models. Experimental results on eight languages have shown
that LiLT can achieve competitive or even superior performance on diverse
widely-used downstream benchmarks, which enables language-independent benefit
from the pre-training of document layout structure. Code and model are publicly
available at https://github.com/jpWang/LiLT."
AdaPrompt: Adaptive Model Training for Prompt-based NLP,0.758025,"Prompt-based learning, with its capability to tackle zero-shot and few-shot
NLP tasks, has gained much attention in community. The main idea is to bridge
the gap between NLP downstream tasks and language modeling (LM), by mapping
these tasks into natural language prompts, which are then filled by pre-trained
language models (PLMs). However, for prompt learning, there are still two
salient gaps between NLP tasks and pretraining. First, prompt information is
not necessarily sufficiently present during LM pretraining. Second,
task-specific data are not necessarily well represented during pretraining. We
address these two issues by proposing AdaPrompt, adaptively retrieving external
data for continual pretraining of PLMs by making use of both task and prompt
characteristics. In addition, we make use of knowledge in Natural Language
Inference models for deriving adaptive verbalizers. Experimental results on
five NLP benchmarks show that AdaPrompt can improve over standard PLMs in
few-shot settings. In addition, in zero-shot settings, our method outperforms
standard prompt-based methods by up to 26.35\% relative error reduction."
Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation,0.739853,"Transformers have revolutionized vision and natural language processing with
their ability to scale with large datasets. But in robotic manipulation, data
is both limited and expensive. Can manipulation still benefit from Transformers
with the right problem formulation? We investigate this question with PerAct, a
language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.
PerAct encodes language goals and RGB-D voxel observations with a Perceiver
Transformer, and outputs discretized actions by ``detecting the next best voxel
action''. Unlike frameworks that operate on 2D images, the voxelized 3D
observation and action space provides a strong structural prior for efficiently
learning 6-DoF actions. With this formulation, we train a single multi-task
Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks
(with 18 variations) from just a few demonstrations per task. Our results show
that PerAct significantly outperforms unstructured image-to-action agents and
3D ConvNet baselines for a wide range of tabletop tasks."
On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation,0.787022,"In recent years, pre-trained models have become dominant in most natural
language processing (NLP) tasks. However, in the area of Automated Essay
Scoring (AES), pre-trained models such as BERT have not been properly used to
outperform other deep learning models such as LSTM. In this paper, we introduce
a novel multi-scale essay representation for BERT that can be jointly learned.
We also employ multiple losses and transfer learning from out-of-domain essays
to further improve the performance. Experiment results show that our approach
derives much benefit from joint learning of multi-scale essay representation
and obtains almost the state-of-the-art result among all deep learning models
in the ASAP task. Our multi-scale essay representation also generalizes well to
CommonLit Readability Prize data set, which suggests that the novel text
representation proposed in this paper may be a new and effective choice for
long-text tasks."
IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images,0.767944,"We propose a neural inverse rendering pipeline called IRON that operates on
photometric images and outputs high-quality 3D content in the format of
triangle meshes and material textures readily deployable in existing graphics
pipelines. Our method adopts neural representations for geometry as signed
distance fields (SDFs) and materials during optimization to enjoy their
flexibility and compactness, and features a hybrid optimization scheme for
neural SDFs: first, optimize using a volumetric radiance field approach to
recover correct topology, then optimize further using edgeaware physics-based
surface rendering for geometry refinement and disentanglement of materials and
lighting. In the second stage, we also draw inspiration from mesh-based
differentiable rendering, and design a novel edge sampling algorithm for neural
SDFs to further improve performance. We show that our IRON achieves
significantly better inverse rendering quality compared to prior works. Our
project page is here: https://kai-46.github.io/IRON-website/"
Bangla hate speech detection on social media using attention-based recurrent neural network,0.75001,"Hate speech has spread more rapidly through the daily use of technology and,
most notably, by sharing your opinions or feelings on social media in a
negative aspect. Although numerous works have been carried out in detecting
hate speeches in English, German, and other languages, very few works have been
carried out in the context of the Bengali language. In contrast, millions of
people communicate on social media in Bengali. The few existing works that have
been carried out need improvements in both accuracy and interpretability. This
article proposed encoder decoder based machine learning model, a popular tool
in NLP, to classify user's Bengali comments on Facebook pages. A dataset of
7,425 Bengali comments, consisting of seven distinct categories of hate
speeches, was used to train and evaluate our model. For extracting and encoding
local features from the comments, 1D convolutional layers were used. Finally,
the attention mechanism, LSTM, and GRU based decoders have been used for
predicting hate speech categories. Among the three encoder decoder algorithms,
the attention-based decoder obtained the best accuracy (77%)."
Error Compensation Framework for Flow-Guided Video Inpainting,0.742475,"The key to video inpainting is to use correlation information from as many
reference frames as possible. Existing flow-based propagation methods split the
video synthesis process into multiple steps: flow completion -> pixel
propagation -> synthesis. However, there is a significant drawback that the
errors in each step continue to accumulate and amplify in the next step. To
this end, we propose an Error Compensation Framework for Flow-guided Video
Inpainting (ECFVI), which takes advantage of the flow-based method and offsets
its weaknesses. We address the weakness with the newly designed flow completion
module and the error compensation network that exploits the error guidance map.
Our approach greatly improves the temporal consistency and the visual quality
of the completed videos. Experimental results show the superior performance of
our proposed method with the speed up of x6, compared to the state-of-the-art
methods. In addition, we present a new benchmark dataset for evaluation by
supplementing the weaknesses of existing test datasets."
Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small,0.76108,"Research in mechanistic interpretability seeks to explain behaviors of
machine learning models in terms of their internal components. However, most
previous work either focuses on simple behaviors in small models, or describes
complicated behaviors in larger models with broad strokes. In this work, we
bridge this gap by presenting an explanation for how GPT-2 small performs a
natural language task called indirect object identification (IOI). Our
explanation encompasses 26 attention heads grouped into 7 main classes, which
we discovered using a combination of interpretability approaches relying on
causal interventions. To our knowledge, this investigation is the largest
end-to-end attempt at reverse-engineering a natural behavior ""in the wild"" in a
language model. We evaluate the reliability of our explanation using three
quantitative criteria--faithfulness, completeness and minimality. Though these
criteria support our explanation, they also point to remaining gaps in our
understanding. Our work provides evidence that a mechanistic understanding of
large ML models is feasible, opening opportunities to scale our understanding
to both larger models and more complex tasks."
The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts,0.781799,"Classroom discourse is a core medium of instruction - analyzing it can
provide a window into teaching and learning as well as driving the development
of new tools for improving instruction. We introduce the largest dataset of
mathematics classroom transcripts available to researchers, and demonstrate how
this data can help improve instruction. The dataset consists of 1,660 45-60
minute long 4th and 5th grade elementary mathematics observations collected by
the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The
anonymized transcripts represent data from 317 teachers across 4 school
districts that serve largely historically marginalized students. The
transcripts come with rich metadata, including turn-level annotations for
dialogic discourse moves, classroom observation scores, demographic
information, survey responses and student test scores. We demonstrate that our
natural language processing model, trained on our turn-level annotations, can
learn to identify dialogic discourse moves and these moves are correlated with
better classroom observation scores and learning outcomes. This dataset opens
up several possibilities for researchers, educators and policymakers to learn
about and improve K-12 instruction. The dataset can be found at
https://github.com/ddemszky/classroom-transcript-analysis."
Crosslingual Generalization through Multitask Finetuning,0.772832,"Multitask prompted finetuning (MTF) has been shown to help large language
models generalize to new tasks in a zero-shot setting, but so far explorations
of MTF have focused on English data and models. We apply MTF to the pretrained
multilingual BLOOM and mT5 model families to produce finetuned variants called
BLOOMZ and mT0. We find finetuning large multilingual language models on
English tasks with English prompts allows for task generalization to
non-English languages that appear only in the pretraining corpus. Finetuning on
multilingual tasks with English prompts further improves performance on English
and non-English tasks leading to various state-of-the-art zero-shot results. We
also investigate finetuning on multilingual tasks with prompts that have been
machine-translated from English to match the language of each dataset. We find
training on these machine-translated prompts leads to better performance on
human-written prompts in the respective languages. Surprisingly, we find models
are capable of zero-shot generalization to tasks in languages they have never
intentionally seen. We conjecture that the models are learning higher-level
capabilities that are both task- and language-agnostic. In addition, we
introduce xP3, a composite of supervised datasets in 46 languages with English
and machine-translated prompts. Our code, datasets and models are freely
available at https://github.com/bigscience-workshop/xmtf."
Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer,0.750273,"Learning Bird's Eye View (BEV) representation from surrounding-view cameras
is of great importance for autonomous driving. In this work, we propose a
Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation
learning mechanism. GKT leverages the geometric priors to guide the transformer
to focus on discriminative regions and unfolds kernel features to generate BEV
representation. For fast inference, we further introduce a look-up table (LUT)
indexing method to get rid of the camera's calibrated parameters at runtime.
GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust
to the camera deviation and the predefined BEV height. And GKT achieves the
state-of-the-art real-time segmentation results, i.e., 38.0 mIoU
(100m$\times$100m perception range at a 0.5m resolution) on the nuScenes val
set. Given the efficiency, effectiveness, and robustness, GKT has great
practical values in autopilot scenarios, especially for real-time running
systems. Code and models will be available at
\url{https://github.com/hustvl/GKT}."
"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",0.782825,"The formalization of existing mathematical proofs is a notoriously difficult
process. Despite decades of research on automation and proof assistants,
writing formal proofs remains arduous and only accessible to a few experts.
While previous studies to automate formalization focused on powerful search
algorithms, no attempts were made to take advantage of available informal
proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method
that maps informal proofs to formal proof sketches, and uses the sketches to
guide an automated prover by directing its search to easier sub-problems. We
investigate two relevant setups where informal proofs are either written by
humans or generated by a language model. Our experiments and ablation studies
show that large language models are able to produce well-structured formal
sketches that follow the same reasoning steps as the informal proofs. Guiding
an automated prover with these sketches enhances its performance from 20.9% to
39.3% on a collection of mathematical competition problems."
Fine-Grained Scene Graph Generation with Data Transfer,0.752083,"Scene graph generation (SGG) is designed to extract (subject, predicate,
object) triplets in images. Recent works have made a steady progress on SGG,
and provide useful tools for high-level vision and language understanding.
However, due to the data distribution problems including long-tail distribution
and semantic ambiguity, the predictions of current SGG models tend to collapse
to several frequent but uninformative predicates (e.g., on, at), which limits
practical application of these models in downstream tasks. To deal with the
problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a plug-and-play fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the enhanced dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The code and data
are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch."
AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,0.766493,"Knowledge distillation (KD) methods compress large models into smaller
students with manually-designed student architectures given pre-specified
computational cost. This requires several trials to find a viable student, and
further repeating the process for each student or computational budget change.
We use Neural Architecture Search (NAS) to automatically distill several
compressed students with variable cost from a large model. Current works train
a single SuperLM consisting of millions of subnetworks with weight-sharing,
resulting in interference between subnetworks of different sizes. Our framework
AutoDistil addresses above challenges with the following steps: (a)
Incorporates inductive bias and heuristics to partition Transformer search
space into K compact sub-spaces (K=3 for typical student sizes of base, small
and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic
objective (e.g., self-attention distillation) with weight-sharing of students;
(c) Lightweight search for the optimal student without re-training. Fully
task-agnostic training and search allow students to be reused for fine-tuning
on any downstream task. Experiments on GLUE benchmark against state-of-the-art
KD and NAS methods demonstrate AutoDistil to outperform leading compression
techniques with upto 2.7x reduction in computational cost and negligible loss
in task performance."
BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment,0.778042,"This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge."
A Generalist Framework for Panoptic Segmentation of Images and Videos,0.758526,"Panoptic segmentation assigns semantic and instance ID labels to every pixel
of an image. As permutations of instance IDs are also valid solutions, the task
requires learning of high-dimensional one-to-many mapping. As a result,
state-of-the-art approaches use customized architectures and task-specific loss
functions. We formulate panoptic segmentation as a discrete data generation
problem, without relying on inductive bias of the task. A diffusion model is
proposed to model panoptic masks, with a simple architecture and generic loss
function. By simply adding past predictions as a conditioning signal, our
method is capable of modeling video (in a streaming setting) and thereby learns
to track object instances automatically. With extensive experiments, we
demonstrate that our simple approach can perform competitively to
state-of-the-art specialist methods in similar settings."
Large Language Models Can Self-Improve,0.751607,"Large Language Models (LLMs) have achieved excellent performances in various
tasks. However, fine-tuning an LLM requires extensive supervision. Human, on
the other hand, may improve their reasoning abilities by self-thinking without
external inputs. In this work, we demonstrate that an LLM is also capable of
self-improving with only unlabeled datasets. We use a pre-trained LLM to
generate ""high-confidence"" rationale-augmented answers for unlabeled questions
using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM
using those self-generated solutions as target outputs. We show that our
approach improves the general reasoning ability of a 540B-parameter LLM
(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and
63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,
without any ground truth label. We conduct ablation studies and show that
fine-tuning on reasoning is critical for self-improvement."
"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",0.741303,"The past decade has witnessed dramatic gains in natural language processing
and an unprecedented scaling of large language models. These developments have
been accelerated by the advent of few-shot techniques such as chain of thought
(CoT) prompting. Specifically, CoT pushes the performance of large language
models in a few-shot setup by augmenting the prompts with intermediate steps.
Despite impressive results across various tasks, the reasons behind their
success have not been explored. This work uses counterfactual prompting to
develop a deeper understanding of CoT-based few-shot prompting mechanisms in
large language models. We first systematically identify and define the key
components of a prompt: symbols, patterns, and text. Then, we devise and
conduct an exhaustive set of experiments across four different tasks, by
querying the model with counterfactual prompts where only one of these
components is altered. Our experiments across three models (PaLM, GPT-3, and
CODEX) reveal several surprising findings and brings into question the
conventional wisdom around few-shot prompting. First, the presence of factual
patterns in a prompt is practically immaterial to the success of CoT. Second,
our results conclude that the primary role of intermediate steps may not be to
facilitate learning how to solve a task. The intermediate steps are rather a
beacon for the model to realize what symbols to replicate in the output to form
a factual answer. Further, text imbues patterns with commonsense knowledge and
meaning. Our empirical and qualitative analysis reveals that a symbiotic
relationship between text and patterns explains the success of few-shot
prompting: text helps extract commonsense from the question to help patterns,
and patterns enforce task understanding and direct text generation."
Is Conditional Generative Modeling all you need for Decision-Making?,0.76689,"Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making."
Online Decision Transformer,0.770206,"Recent work has shown that offline reinforcement learning (RL) can be
formulated as a sequence modeling problem (Chen et al., 2021; Janner et al.,
2021) and solved via approaches similar to large-scale language modeling.
However, any practical instantiation of RL also involves an online component,
where policies pretrained on passive offline datasets are finetuned via
taskspecific interactions with the environment. We propose Online Decision
Transformers (ODT), an RL algorithm based on sequence modeling that blends
offline pretraining with online finetuning in a unified framework. Our
framework uses sequence-level entropy regularizers in conjunction with
autoregressive modeling objectives for sample-efficient exploration and
finetuning. Empirically, we show that ODT is competitive with the
state-of-the-art in absolute performance on the D4RL benchmark but shows much
more significant gains during the finetuning procedure."
Can language models learn from explanations in context?,0.781143,"Language Models (LMs) can perform new tasks by adapting to a few in-context
examples. For humans, explanations that connect examples to task principles can
improve learning. We therefore investigate whether explanations of few-shot
examples can help LMs. We annotate questions from 40 challenging tasks with
answer explanations, and various matched control explanations. We evaluate how
different types of explanations, instructions, and controls affect zero- and
few-shot performance. We analyze these results using statistical multilevel
modeling techniques that account for the nested dependencies among conditions,
tasks, prompts, and models. We find that explanations can improve performance
-- even without tuning. Furthermore, explanations hand-tuned for performance on
a small validation set offer substantially larger benefits, and building a
prompt by selecting examples and explanations together substantially improves
performance over selecting examples alone. Finally, even untuned explanations
outperform carefully matched controls, suggesting that the benefits are due to
the link between an example and its explanation, rather than lower-level
features. However, only large models benefit. In summary, explanations can
support the in-context learning of large LMs on challenging tasks."
Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles,0.791707,"Video Anomaly Detection (VAD) is an important topic in computer vision.
Motivated by the recent advances in self-supervised learning, this paper
addresses VAD by solving an intuitive yet challenging pretext task, i.e.,
spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained
classification problem. Our method exhibits several advantages over existing
works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial
and temporal dimensions, responsible for capturing highly discriminative
appearance and motion features, respectively; 2) full permutations are used to
provide abundant jigsaw puzzles covering various difficulty levels, allowing
the network to distinguish subtle spatio-temporal differences between normal
and abnormal events; and 3) the pretext task is tackled in an end-to-end manner
without relying on any pre-trained models. Our method outperforms
state-of-the-art counterparts on three public benchmarks. Especially on
ShanghaiTech Campus, the result is superior to reconstruction and
prediction-based methods by a large margin."
Simple Open-Vocabulary Object Detection with Vision Transformers,0.824939,"Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub."
AUTOLEX: An Automatic Framework for Linguistic Exploration,0.798104,"Each language has its own complex systems of word, phrase, and sentence
construction, the guiding principles of which are often summarized in grammar
descriptions for the consumption of linguists or language learners. However,
manual creation of such descriptions is a fraught process, as creating
descriptions which describe the language in ""its own terms"" without bias or
error requires both a deep understanding of the language at hand and
linguistics as a whole. We propose an automatic framework AutoLEX that aims to
ease linguists' discovery and extraction of concise descriptions of linguistic
phenomena. Specifically, we apply this framework to extract descriptions for
three phenomena: morphological agreement, case marking, and word order, across
several languages. We evaluate the descriptions with the help of language
experts and propose a method for automated evaluation when human evaluation is
infeasible."
"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",0.841303,"Pretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations."
REGTR: End-to-end Point Cloud Correspondences with Transformers,0.79298,"Despite recent success in incorporating learning into point cloud
registration, many works focus on learning feature descriptors and continue to
rely on nearest-neighbor feature matching and outlier filtering through RANSAC
to obtain the final set of correspondences for pose estimation. In this work,
we conjecture that attention mechanisms can replace the role of explicit
feature matching and RANSAC, and thus propose an end-to-end framework to
directly predict the final set of correspondences. We use a network
architecture consisting primarily of transformer layers containing self and
cross attentions, and train it to predict the probability each point lies in
the overlapping region and its corresponding position in the other point cloud.
The required rigid transformation can then be estimated directly from the
predicted correspondences without further post-processing. Despite its
simplicity, our approach achieves state-of-the-art performance on 3DMatch and
ModelNet benchmarks. Our source code can be found at
https://github.com/yewzijian/RegTR ."
HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,0.827891,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex label hierarchy. Recently, the
pretrained language models (PLM)have been widely adopted in HTC through a
fine-tuning paradigm. However, in this paradigm, there exists a huge gap
between the classification tasks with sophisticated label hierarchy and the
masked language model (MLM) pretraining tasks of PLMs and thus the potentials
of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose
HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label
MLM perspective. Specifically, we construct a dynamic virtual template and
label words that take the form of soft prompts to fuse the label hierarchy
knowledge and introduce a zero-bounded multi-label cross entropy loss to
harmonize the objectives of HTC and MLM. Extensive experiments show HPT
achieves state-of-the-art performances on 3 popular HTC datasets and is adept
at handling the imbalance and low resource situations. Our code is available at
https://github.com/wzh9969/HPT."
RelTR: Relation Transformer for Scene Graph Generation,0.821438,"Different objects in the same scene are more or less related to each other,
but only a limited number of these relationships are noteworthy. Inspired by
DETR, which excels in object detection, we view scene graph generation as a set
prediction problem and propose an end-to-end scene graph generation model RelTR
which has an encoder-decoder architecture. The encoder reasons about the visual
feature context while the decoder infers a fixed-size set of triplets
subject-predicate-object using different types of attention mechanisms with
coupled subject and object queries. We design a set prediction loss performing
the matching between the ground truth and predicted triplets for the end-to-end
training. In contrast to most existing scene graph generation methods, RelTR is
a one-stage method that predicts a set of relationships directly only using
visual appearance without combining entities and labeling all possible
predicates. Extensive experiments on the Visual Genome and Open Images V6
datasets demonstrate the superior performance and fast inference of our model."
DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,0.793902,"Language-guided Embodied AI benchmarks requiring an agent to navigate an
environment and manipulate objects typically allow one-way communication: the
human user gives a natural language command to the agent, and the agent can
only follow the command passively. We present DialFRED, a dialogue-enabled
embodied instruction following benchmark based on the ALFRED benchmark.
DialFRED allows an agent to actively ask questions to the human user; the
additional information in the user's response is used by the agent to better
complete its task. We release a human-annotated dataset with 53K task-relevant
questions and answers and an oracle to answer questions. To solve DialFRED, we
propose a questioner-performer framework wherein the questioner is pre-trained
with the human-annotated data and fine-tuned with reinforcement learning. We
make DialFRED publicly available and encourage researchers to propose and
evaluate their solutions to building dialog-enabled embodied agents."
V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer,0.827607,"In this paper, we investigate the application of Vehicle-to-Everything (V2X)
communication to improve the perception performance of autonomous vehicles. We
present a robust cooperative perception framework with V2X communication using
a novel vision Transformer. Specifically, we build a holistic attention model,
namely V2X-ViT, to effectively fuse information across on-road agents (i.e.,
vehicles and infrastructure). V2X-ViT consists of alternating layers of
heterogeneous multi-agent self-attention and multi-scale window self-attention,
which captures inter-agent interaction and per-agent spatial relationships.
These key modules are designed in a unified Transformer architecture to handle
common V2X challenges, including asynchronous information sharing, pose errors,
and heterogeneity of V2X components. To validate our approach, we create a
large-scale V2X perception dataset using CARLA and OpenCDA. Extensive
experimental results demonstrate that V2X-ViT sets new state-of-the-art
performance for 3D object detection and achieves robust performance even under
harsh, noisy environments. The code is available at
https://github.com/DerrickXuNu/v2x-vit."
Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models,0.817992,"Motivations for methods in explainable artificial intelligence (XAI) often
include detecting, quantifying and mitigating bias, and contributing to making
machine learning models fairer. However, exactly how an XAI method can help in
combating biases is often left unspecified. In this paper, we briefly review
trends in explainability and fairness in NLP research, identify the current
practices in which explainability methods are applied to detect and mitigate
bias, and investigate the barriers preventing XAI methods from being used more
widely in tackling fairness issues."
FEAT: Face Editing with Attention,0.796232,"Employing the latent space of pretrained generators has recently been shown
to be an effective means for GAN-based face manipulation. The success of this
approach heavily relies on the innate disentanglement of the latent space axes
of the generator. However, face manipulation often intends to affect local
regions only, while common generators do not tend to have the necessary spatial
disentanglement. In this paper, we build on the StyleGAN generator, and present
a method that explicitly encourages face manipulation to focus on the intended
regions by incorporating learned attention maps. During the generation of the
edited image, the attention map serves as a mask that guides a blending between
the original features and the modified ones. The guidance for the latent space
edits is achieved by employing CLIP, which has recently been shown to be
effective for text-driven edits. We perform extensive experiments and show that
our method can perform disentangled and controllable face manipulations based
on text descriptions by attending to the relevant regions only. Both
qualitative and quantitative experimental results demonstrate the superiority
of our method for facial region editing over alternative methods."
TEMPERA: Test-Time Prompting via Reinforcement Learning,0.83425,"Careful prompt design is critical to the use of large language models in
zero-shot or few-shot learning. As a consequence, there is a growing interest
in automated methods to design optimal prompts. In this work, we propose
Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to
prior prompt generation methods, TEMPERA can efficiently leverage prior
knowledge, is adaptive to different queries and provides an interpretable
prompt for every query. To achieve this, we design a novel action space that
allows flexible editing of the initial prompts covering a wide set of
commonly-used components like instructions, few-shot exemplars, and
verbalizers. The proposed method achieves significant gains compared with
recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a
variety of tasks including sentiment analysis, topic classification, natural
language inference, and reading comprehension. Our method achieves 5.33x on
average improvement in sample efficiency when compared to the traditional
fine-tuning methods."
Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds,0.830527,"Despite impressive successes, deep reinforcement learning (RL) systems still
fall short of human performance on generalization to new tasks and environments
that differ from their training. As a benchmark tailored for studying RL
generalization, we introduce Avalon, a set of tasks in which embodied agents in
highly diverse procedural 3D worlds must survive by navigating terrain, hunting
or gathering food, and avoiding hazards. Avalon is unique among existing RL
benchmarks in that the reward function, world dynamics, and action space are
the same for every task, with tasks differentiated solely by altering the
environment; its 20 tasks, ranging in complexity from eat and throw to hunt and
navigate, each create worlds in which the agent must perform specific skills in
order to survive. This setup enables investigations of generalization within
tasks, between tasks, and to compositional tasks that require combining skills
learned from previous tasks. Avalon includes a highly efficient simulator, a
library of baselines, and a benchmark with scoring metrics evaluated against
hundreds of hours of human performance, all of which are open-source and
publicly available. We find that standard RL baselines make progress on most
tasks but are still far from human performance, suggesting Avalon is
challenging enough to advance the quest for generalizable RL."
TVLT: Textless Vision-Language Transformer,0.831402,"In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT"
DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,0.837716,"Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks."
Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,0.83482,"Prompting-based large language models (LLMs) are surprisingly powerful at
generating natural language reasoning steps or Chains-of-Thoughts (CoT) for
multi-step question answering (QA). They struggle, however, when the necessary
knowledge is either unavailable to the LLM or not up-to-date within its
parameters. While using the question to retrieve relevant text from an external
knowledge source helps LLMs, we observe that this one-step retrieve-and-read
approach is insufficient for multi-step QA. Here, \textit{what to retrieve}
depends on \textit{what has already been derived}, which in turn may depend on
\textit{what was previously retrieved}. To address this, we propose IRCoT, a
new approach for multi-step QA that interleaves retrieval with steps
(sentences) in a CoT, guiding the retrieval with CoT and in turn using
retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves
retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four
datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar
substantial gains in out-of-distribution (OOD) settings as well as with much
smaller models such as Flan-T5-large without additional training. IRCoT reduces
model hallucination, resulting in factually more accurate CoT reasoning. Code,
data, and prompts are available at \url{https://github.com/stonybrooknlp/ircot}"
PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions,0.832064,"Cross-entropy loss and focal loss are the most common choices when training
deep neural networks for classification problems. Generally speaking, however,
a good loss function can take on much more flexible forms, and should be
tailored for different tasks and datasets. Motivated by how functions can be
approximated via Taylor expansion, we propose a simple framework, named
PolyLoss, to view and design loss functions as a linear combination of
polynomial functions. Our PolyLoss allows the importance of different
polynomial bases to be easily adjusted depending on the targeting tasks and
datasets, while naturally subsuming the aforementioned cross-entropy loss and
focal loss as special cases. Extensive experimental results show that the
optimal choice within the PolyLoss is indeed dependent on the task and dataset.
Simply by introducing one extra hyperparameter and adding one line of code, our
Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D
image classification, instance segmentation, object detection, and 3D object
detection tasks, sometimes by a large margin."
RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization,0.817124,"6-DoF object pose estimation from a monocular image is challenging, and a
post-refinement procedure is generally needed for high-precision estimation. In
this paper, we propose a framework based on a recurrent neural network (RNN)
for object pose refinement, which is robust to erroneous initial poses and
occlusions. During the recurrent iterations, object pose refinement is
formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
enabling end-to-end training. The correspondence field estimation and pose
refinement are conducted alternatively in each iteration to recover the object
poses. Furthermore, to improve the robustness to occlusion, we introduce a
consistency-check mechanism based on the learned descriptors of the 3D model
and observed 2D images, which downweights the unreliable correspondences during
pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance."
Thin-Plate Spline Motion Model for Image Animation,0.840597,"Image animation brings life to the static object in the source image
according to the driving video. Recent works attempt to perform motion transfer
on arbitrary objects through unsupervised methods without using a priori
knowledge. However, it remains a significant challenge for current unsupervised
methods when there is a large pose gap between the objects in the source and
driving images. In this paper, a new end-to-end unsupervised motion transfer
framework is proposed to overcome such issue. Firstly, we propose thin-plate
spline motion estimation to produce a more flexible optical flow, which warps
the feature maps of the source image to the feature domain of the driving
image. Secondly, in order to restore the missing regions more realistically, we
leverage multi-resolution occlusion masks to achieve more effective feature
fusion. Finally, additional auxiliary loss functions are designed to ensure
that there is a clear division of labor in the network modules, encouraging the
network to generate high-quality images. Our method can animate a variety of
objects, including talking faces, human bodies, and pixel animations.
Experiments demonstrate that our method performs better on most benchmarks than
the state of the art with visible improvements in pose-related metrics."
Czech Grammar Error Correction with a Large and Diverse Corpus,0.838224,"We introduce a large and diverse Czech corpus annotated for grammatical error
correction (GEC) with the aim to contribute to the still scarce data resources
in this domain for languages other than English. The Grammar Error Correction
Corpus for Czech (GECCC) offers a variety of four domains, covering error
distributions ranging from high error density essays written by non-native
speakers, to website texts, where errors are expected to be much less common.
We compare several Czech GEC systems, including several Transformer-based ones,
setting a strong baseline to future research. Finally, we meta-evaluate common
GEC metrics against human judgements on our data. We make the new Czech GEC
corpus publicly available under the CC BY-SA 4.0 license at
http://hdl.handle.net/11234/1-4639 ."
SCAMPS: Synthetics for Camera Measurement of Physiological Signals,0.818984,"The use of cameras and computational algorithms for noninvasive, low-cost and
scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs
is very attractive. However, diverse data representing a range of environments,
body motions, illumination conditions and physiological states is laborious,
time consuming and expensive to obtain. Synthetic data have proven a valuable
tool in several areas of machine learning, yet are not widely available for
camera measurement of physiological states. Synthetic data offer ""perfect""
labels (e.g., without noise and with precise synchronization), labels that may
not be possible to obtain otherwise (e.g., precise pixel level segmentation
maps) and provide a high degree of control over variation and diversity in the
dataset. We present SCAMPS, a dataset of synthetics containing 2,800 videos
(1.68M frames) with aligned cardiac and respiratory signals and facial action
intensities. The RGB frames are provided alongside segmentation maps. We
provide precise descriptive statistics about the underlying waveforms,
including inter-beat interval, heart rate variability, and pulse arrival time.
Finally, we present baseline results training on these synthetic data and
testing on real-world datasets to illustrate generalizability."
When Is Partially Observable Reinforcement Learning Not Scary?,0.824867,"Applications of Reinforcement Learning (RL), in which agents learn to make a
sequence of decisions despite lacking complete information about the latent
states of the controlled system, that is, they act under partial observability
of the states, are ubiquitous. Partially observable RL can be notoriously
difficult -- well-known information-theoretic results show that learning
partially observable Markov decision processes (POMDPs) requires an exponential
number of samples in the worst case. Yet, this does not rule out the existence
of large subclasses of POMDPs over which learning is tractable.
  In this paper we identify such a subclass, which we call weakly revealing
POMDPs. This family rules out the pathological instances of POMDPs where
observations are uninformative to a degree that makes learning hard. We prove
that for weakly revealing POMDPs, a simple algorithm combining optimism and
Maximum Likelihood Estimation (MLE) is sufficient to guarantee polynomial
sample complexity. To the best of our knowledge, this is the first provably
sample-efficient result for learning from interactions in overcomplete POMDPs,
where the number of latent states can be larger than the number of
observations."
QuadTree Attention for Vision Transformers,0.809951,"Transformers have been successful in many vision tasks, thanks to their
capability of capturing long-range dependency. However, their quadratic
computational complexity poses a major obstacle for applying them to vision
tasks requiring dense predictions, such as object detection, feature matching,
stereo, etc. We introduce QuadTree Attention, which reduces the computational
complexity from quadratic to linear. Our quadtree transformer builds token
pyramids and computes attention in a coarse-to-fine manner. At each level, the
top K patches with the highest attention scores are selected, such that at the
next level, attention is only evaluated within the relevant regions
corresponding to these top K patches. We demonstrate that quadtree attention
achieves state-of-the-art performance in various vision tasks, e.g. with 4.0%
improvement in feature matching on ScanNet, about 50% flops reduction in stereo
matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,
1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on
semantic segmentation over previous state-of-the-art transformers. The codes
are available at https://github.com/Tangshitao/QuadtreeAttention."
HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crisis Response,0.811124,"Timely and effective response to humanitarian crises requires quick and
accurate analysis of large amounts of text data - a process that can highly
benefit from expert-assisted NLP systems trained on validated and annotated
data in the humanitarian response domain. To enable creation of such NLP
systems, we introduce and release HumSet, a novel and rich multilingual dataset
of humanitarian response documents annotated by experts in the humanitarian
response community. The dataset provides documents in three languages (English,
French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021
across the globe. For each document, HUMSET provides selected snippets
(entries) as well as assigned classes to each entry annotated using common
humanitarian information analysis frameworks. HUMSET also provides novel and
challenging entry extraction and multi-label entry classification tasks. In
this paper, we take a first step towards approaching these tasks and conduct a
set of experiments on Pre-trained Language Models (PLM) to establish strong
baselines for future research in this domain. The dataset is available at
https://blog.thedeep.io/humset/."
FS6D: Few-Shot 6D Pose Estimation of Novel Objects,0.817971,"6D object pose estimation networks are limited in their capability to scale
to large numbers of object instances due to the close-set assumption and their
reliance on high-fidelity object CAD models. In this work, we study a new open
set problem; the few-shot 6D object poses estimation: estimating the 6D pose of
an unknown object by a few support views without extra training. To tackle the
problem, we point out the importance of fully exploring the appearance and
geometric relationship between the given support views and query scene patches
and propose a dense prototypes matching framework by extracting and matching
dense RGBD prototypes with transformers. Moreover, we show that the priors from
diverse appearances and shapes are crucial to the generalization capability
under the problem setting and thus propose a large-scale RGBD photorealistic
dataset (ShapeNet6D) for network pre-training. A simple and effective online
texture blending approach is also introduced to eliminate the domain gap from
the synthesis dataset, which enriches appearance diversity at a low cost.
Finally, we discuss possible solutions to this problem and establish benchmarks
on popular datasets to facilitate future research. The project page is at
\url{https://fs6d.github.io/}."
EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start,0.795868,"We present EdiT5 - a novel semi-autoregressive text-editing model designed to
combine the strengths of non-autoregressive text-editing and autoregressive
decoding. EdiT5 is faster during inference than conventional
sequence-to-sequence (seq2seq) models, while being capable of modelling
flexible input-output transformations.
  This is achieved by decomposing the generation process into three sub-tasks:
(1) tagging to decide on the subset of input tokens to be preserved in the
output, (2) re-ordering to define their order in the output text, and (3)
insertion to infill the missing tokens that are not present in the input. The
tagging and re-ordering steps, which are responsible for generating the largest
portion of the output, are non-autoregressive, while the insertion step uses an
autoregressive decoder.
  Depending on the task, EdiT5 on average requires significantly fewer
autoregressive steps, demonstrating speedups of up to 25x when compared to
seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5
checkpoint yielding comparable performance to T5 in high-resource settings when
evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,
and Decontextualization while clearly outperforming T5 in low-resource
settings."
Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation,0.808628,"Sparsely annotated semantic segmentation (SASS) aims to train a segmentation
network with coarse-grained (i.e., point-, scribble-, and block-wise)
supervisions, where only a small proportion of pixels are labeled in each
image. In this paper, we propose a novel tree energy loss for SASS by providing
semantic guidance for unlabeled pixels. The tree energy loss represents images
as minimum spanning trees to model both low-level and high-level pair-wise
affinities. By sequentially applying these affinities to the network
prediction, soft pseudo labels for unlabeled pixels are generated in a
coarse-to-fine manner, achieving dynamic online self-training. The tree energy
loss is effective and easy to be incorporated into existing frameworks by
combining it with a traditional segmentation loss. Compared with previous SASS
methods, our method requires no multistage training strategies, alternating
optimization procedures, additional supervised data, or time-consuming
post-processing while outperforming them in all SASS settings. Code is
available at https://github.com/megvii-research/TreeEnergyLoss."
Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,0.864665,"This paper presents exploratory work on whether and to what extent biases
against queer and trans people are encoded in large language models (LLMs) such
as BERT. We also propose a method for reducing these biases in downstream
tasks: finetuning the models on data written by and/or about queer people. To
measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,
modeled after other bias-detection benchmarks but addressing homophobic and
transphobic biases. We found that BERT shows significant homophobic bias, but
this bias can be mostly mitigated by finetuning BERT on a natural language
corpus written by members of the LGBTQ+ community."
How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,0.854876,"Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection."
"GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction",0.863321,"We present GO-Surf, a direct feature grid optimization method for accurate
and fast surface reconstruction from RGB-D sequences. We model the underlying
scene with a learned hierarchical feature voxel grid that encapsulates
multi-level geometric and appearance local information. Feature vectors are
directly optimized such that after being tri-linearly interpolated, decoded by
two shallow MLPs into signed distance and radiance values, and rendered via
surface volume rendering, the discrepancy between synthesized and observed
RGB/depth values is minimized. Our supervision signals -- RGB, depth and
approximate SDF -- can be obtained directly from input images without any need
for fusion or post-processing. We formulate a novel SDF gradient regularization
term that encourages surface smoothness and hole filling while maintaining high
frequency details. GO-Surf can optimize sequences of $1$-$2$K frames in
$15$-$45$ minutes, a speedup of $\times60$ over NeuralRGB-D, the most related
approach based on an MLP representation, while maintaining on par performance
on standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/"
The Stack: 3 TB of permissively licensed source code,0.877409,"Large Language Models (LLMs) play an ever-increasing role in the field of
Artificial Intelligence (AI)--not only for natural language processing but also
for code understanding and generation. To stimulate open and responsible
research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting
of permissively licensed source code in 30 programming languages. We describe
how we collect the full dataset, construct a permissively licensed subset,
present a data governance plan, discuss limitations, and show promising results
on text2code benchmarks by training 350M-parameter decoders on different Python
subsets. We find that (1) near-deduplicating the data significantly boosts
performance across all experiments, and (2) it is possible to match previously
reported HumanEval and MBPP performance using only permissively licensed data.
We make the dataset available at https://hf.co/BigCode, provide a tool called
""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers
to search The Stack for copies of their code, and provide a process for code to
be removed from the dataset by following the instructions at
https://www.bigcode-project.org/docs/about/the-stack/."
Rationale-Augmented Ensembles in Language Models,0.868289,"Recent research has shown that rationales, or step-by-step chains of thought,
can be used to improve performance in multi-step reasoning tasks. We reconsider
rationale-augmented prompting for few-shot in-context learning, where (input ->
output) prompts are expanded to (input, rationale -> output) prompts. For
rationale-augmented prompting we demonstrate how existing approaches, which
rely on manual prompt engineering, are subject to sub-optimal rationales that
may harm performance. To mitigate this brittleness, we propose a unified
framework of rationale-augmented ensembles, where we identify rationale
sampling in the output space as the key component to robustly improve
performance. This framework is general and can easily be extended to common
natural language processing tasks, even those that do not traditionally
leverage intermediate steps, such as question answering, word sense
disambiguation, and sentiment analysis. We demonstrate that rationale-augmented
ensembles achieve more accurate and interpretable results than existing
prompting approaches--including standard prompting without rationales and
rationale-based chain-of-thought prompting--while simultaneously improving
interpretability of model predictions through the associated rationales."
CSL: A Large-scale Chinese Scientific Literature Dataset,0.851738,"Scientific literature serves as a high-quality corpus, supporting a lot of
Natural Language Processing (NLP) research. However, existing datasets are
centered around the English language, which restricts the development of
Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese
Scientific Literature dataset, which contains the titles, abstracts, keywords
and academic fields of 396k papers. To our knowledge, CSL is the first
scientific document dataset in Chinese. The CSL can serve as a Chinese corpus.
Also, this semi-structured data is a natural annotation that can constitute
many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the
performance of models across scientific domain tasks, i.e., summarization,
keyword generation and text classification. We analyze the behavior of existing
text-to-text models on the evaluation tasks and reveal the challenges for
Chinese scientific NLP tasks, which provides a valuable reference for future
research. Data and code are available at https://github.com/ydli-ai/CSL"
ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations,0.864665,"Precise representations of 3D faces are beneficial to various computer vision
and graphics applications. Due to the data discretization and model linearity,
however, it remains challenging to capture accurate identity and expression
clues in current studies. This paper presents a novel 3D morphable face model,
namely ImFace, to learn a nonlinear and continuous space with implicit neural
representations. It builds two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
and designs an improved learning strategy to extend embeddings of expressions
to allow more diverse changes. We further introduce a Neural Blend-Field to
learn sophisticated details by adaptively blending a series of local fields. In
addition to ImFace, an effective preprocessing pipeline is proposed to address
the issue of watertight input requirement in implicit representations, enabling
them to work with common facial surfaces for the first time. Extensive
experiments are performed to demonstrate the superiority of ImFace."
Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation,0.864665,"3D Human body pose and shape estimation within a temporal sequence can be
quite critical for understanding human behavior. Despite the significant
progress in human pose estimation in the recent years, which are often based on
single images or videos, human motion estimation on live stream videos is still
a rarely-touched area considering its special requirements for real-time output
and temporal consistency. To address this problem, we present a temporally
embedded 3D human body pose and shape estimation (TePose) method to improve the
accuracy and temporal consistency of pose estimation in live stream videos.
TePose uses previous predictions as a bridge to feedback the error for better
estimation in the current frame and to learn the correspondence between data
frames and predictions in the history. A multi-scale spatio-temporal graph
convolutional network is presented as the motion discriminator for adversarial
training using datasets without any 3D labeling. We propose a sequential data
loading strategy to meet the special start-to-end data processing requirement
of live stream. We demonstrate the importance of each proposed module with
extensive experiments. The results show the effectiveness of TePose on
widely-used human pose benchmarks with state-of-the-art performance."
Faithful Reasoning Using Large Language Models,0.86867,"Although contemporary large language models (LMs) demonstrate impressive
question-answering capabilities, their answers are typically the product of a
single call to the model. This entails an unwelcome degree of opacity and
compromises performance, especially on problems that are inherently multi-step.
To address these limitations, we show how LMs can be made to perform faithful
multi-step reasoning via a process whose causal structure mirrors the
underlying logical structure of the problem. Our approach works by chaining
together reasoning steps, where each step results from calls to two fine-tuned
LMs, one for selection and one for inference, to produce a valid reasoning
trace. Our method carries out a beam search through the space of reasoning
traces to improve reasoning quality. We demonstrate the effectiveness of our
model on multi-step logical deduction and scientific question-answering,
showing that it outperforms baselines on final answer accuracy, and generates
humanly interpretable reasoning traces whose validity can be checked by the
user."
TAFNet: A Three-Stream Adaptive Fusion Network for RGB-T Crowd Counting,0.843613,"In this paper, we propose a three-stream adaptive fusion network named
TAFNet, which uses paired RGB and thermal images for crowd counting.
Specifically, TAFNet is divided into one main stream and two auxiliary streams.
We combine a pair of RGB and thermal images to constitute the input of main
stream. Two auxiliary streams respectively exploit RGB image and thermal image
to extract modality-specific features. Besides, we propose an Information
Improvement Module (IIM) to fuse the modality-specific features into the main
stream adaptively. Experiment results on RGBT-CC dataset show that our method
achieves more than 20% improvement on mean average error and root mean squared
error compared with state-of-the-art method. The source code will be publicly
available at https://github.com/TANGHAIHAN/TAFNet."
Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors,0.848092,"The main question this work aims at answering is: ""can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?"".
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is publicly available
for research purposes."
Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video,0.864665,"We present HandAvatar, a novel representation for hand animation and
rendering, which can generate smoothly compositional geometry and
self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as
a high-resolution mesh topology to fit personalized hand shapes. Sequentially,
we decompose hand geometry into per-bone rigid parts, and then re-compose
paired geometry encodings to derive an across-part consistent occupancy field.
As for texture modeling, we propose a self-occlusion-aware shading field
(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record
albedo information under a wide variety of hand poses. Moreover, directed soft
occupancy is designed to describe the ray-to-surface relation, which is
leveraged to generate an illumination field for the disentanglement of
pose-independent albedo and pose-dependent illumination. Trained from monocular
video data, our HandAvatar can perform free-pose hand animation and rendering
while at the same time achieving superior appearance fidelity. We also
demonstrate that HandAvatar provides a route for hand appearance editing.
Project website: https://seanchenxy.github.io/HandAvatarWeb."
TripleRE: Knowledge Graph Embeddings via Tripled Relation Vectors,0.864665,"Translation-based knowledge graph embedding has been one of the most
important branches for knowledge representation learning since TransE came out.
Although many translation-based approaches have achieved some progress in
recent years, the performance was still unsatisfactory. This paper proposes a
novel knowledge graph embedding method named TripleRE with two versions. The
first version of TripleRE creatively divide the relationship vector into three
parts. The second version takes advantage of the concept of residual and
achieves better performance. In addition, attempts on using NodePiece to encode
entities achieved promising results in reducing the parametric size, and solved
the problems of scalability. Experiments show that our approach achieved
state-of-the-art performance on the large-scale knowledge graph dataset, and
competitive performance on other datasets."
FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,0.842614,"Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the
architecture used for FiD was chosen by making minimal modifications to a
standard T5 model, which our analysis shows to be highly suboptimal for a
retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to
the encoder, while the majority of inference time results from memory bandwidth
constraints in the decoder. We propose two simple changes to the FiD
architecture to alleviate memory bandwidth constraints, and speed up inference
by 7x. This allows us to use a much larger decoder at modest cost. We denote
FiD with the above modifications as FiDO, and show that it strongly improves
performance over existing FiD models for a wide range of inference budgets. For
example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves
better performance than FiD-Large."
PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch,0.864665,"Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and ""zeros out"" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types."
Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks,0.864665,"Representation learning on networks aims to derive a meaningful vector
representation for each node, thereby facilitating downstream tasks such as
link prediction, node classification, and node clustering. In heterogeneous
text-rich networks, this task is more challenging due to (1) presence or
absence of text: Some nodes are associated with rich textual information, while
others are not; (2) diversity of types: Nodes and edges of multiple types form
a heterogeneous network structure. As pretrained language models (PLMs) have
demonstrated their effectiveness in obtaining widely generalizable text
representations, a substantial amount of effort has been made to incorporate
PLMs into representation learning on text-rich networks. However, few of them
can jointly consider heterogeneous structure (network) information as well as
rich textual semantic information of each node effectively. In this paper, we
propose Heterformer, a Heterogeneous Network-Empowered Transformer that
performs contextualized text encoding and heterogeneous structure encoding in a
unified model. Specifically, we inject heterogeneous structure information into
each Transformer layer when encoding node texts. Meanwhile, Heterformer is
capable of characterizing node/edge type heterogeneity and encoding nodes with
or without texts. We conduct comprehensive experiments on three tasks (i.e.,
link prediction, node classification, and node clustering) on three large-scale
datasets from different domains, where Heterformer outperforms competitive
baselines significantly and consistently."
Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset,0.846832,"Speech is inherently continuous, where discrete words, phonemes and other
units are not clearly segmented, and so speech recognition has been an active
research problem for decades. In this work we have fine-tuned wav2vec 2.0 to
recognize and transcribe Bengali speech -- training it on the Bengali Common
Voice Speech Dataset. After training for 71 epochs, on a training set
consisting of 36919 mp3 files, we achieved a training loss of 0.3172 and WER of
0.2524 on a validation set of size 7,747. Using a 5-gram language model, the
Levenshtein Distance was 2.6446 on a test set of size 7,747. Then the training
set and validation set were combined, shuffled and split into 85-15 ratio.
Training for 7 more epochs on this combined dataset yielded an improved
Levenshtein Distance of 2.60753 on the test set. Our model was the best
performing one, achieving a Levenshtein Distance of 6.234 on a hidden dataset,
which was 1.1049 units lower than other competing submissions."
OTExtSum: Extractive Text Summarisation with Optimal Transport,0.864665,"Extractive text summarisation aims to select salient sentences from a
document to form a short yet informative summary. While learning-based methods
have achieved promising results, they have several limitations, such as
dependence on expensive training and lack of interpretability. Therefore, in
this paper, we propose a novel non-learning-based method by for the first time
formulating text summarisation as an Optimal Transport (OT) problem, namely
Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction
is conceptualised as obtaining an optimal summary that minimises the
transportation cost to a given document regarding their semantic distributions.
Such a cost is defined by the Wasserstein distance and used to measure the
summary's semantic coverage of the original document. Comprehensive experiments
on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and
CNN/DM demonstrate that our proposed method outperforms the state-of-the-art
non-learning-based methods and several recent learning-based methods in terms
of the ROUGE metric."
BOREx: Bayesian-Optimization--Based Refinement of Saliency Map for Image- and Video-Classification Models,0.864665,"Explaining a classification result produced by an image- and
video-classification model is one of the important but challenging issues in
computer vision. Many methods have been proposed for producing heat-map--based
explanations for this purpose, including ones based on the white-box approach
that uses the internal information of a model (e.g., LRP, Grad-CAM, and
Grad-CAM++) and ones based on the black-box approach that does not use any
internal information (e.g., LIME, SHAP, and RISE). We propose a new black-box
method BOREx (Bayesian Optimization for Refinement of visual model Explanation)
to refine a heat map produced by any method. Our observation is that a
heat-map--based explanation can be seen as a prior for an explanation method
based on Bayesian optimization. Based on this observation, BOREx conducts
Gaussian process regression (GPR) to estimate the saliency of each pixel in a
given image starting from the one produced by another explanation method. Our
experiments statistically demonstrate that the refinement by BOREx improves
low-quality heat maps for image- and video-classification results."
PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models,0.864665,"Generalizable 3D part segmentation is important but challenging in vision and
robotics. Training deep models via conventional supervised methods requires
large-scale 3D datasets with fine-grained part annotations, which are costly to
collect. This paper explores an alternative way for low-shot part segmentation
of 3D point clouds by leveraging a pretrained image-language model, GLIP, which
achieves superior performance on open-vocabulary 2D detection. We transfer the
rich knowledge from 2D to 3D through GLIP-based part detection on point cloud
rendering and a novel 2D-to-3D label lifting algorithm. We also utilize
multi-view 3D priors and few-shot prompt tuning to boost performance
significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets
shows that our method enables excellent zero-shot 3D part segmentation. Our
few-shot version not only outperforms existing few-shot approaches by a large
margin but also achieves highly competitive results compared to the fully
supervised counterpart. Furthermore, we demonstrate that our method can be
directly applied to iPhone-scanned point clouds without significant domain
gaps."
"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink",0.862472,"Machine Learning (ML) workloads have rapidly grown in importance, but raised
concerns about their carbon footprint. Four best practices can reduce ML
training energy by up to 100x and CO2 emissions up to 1000x. By following best
practices, overall ML energy use (across research, development, and production)
held steady at <15% of Google's total energy use for the past three years. If
the whole ML field were to adopt best practices, total carbon emissions from
training would reduce. Hence, we recommend that ML papers include emissions
explicitly to foster competition on more than just model quality. Estimates of
emissions in papers that omitted them have been off 100x-100,000x, so
publishing emissions has the added benefit of ensuring accurate accounting.
Given the importance of climate change, we must get the numbers right to make
certain that we work on its biggest challenges."
TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers,0.864665,"Mixup is a commonly adopted data augmentation technique for image
classification. Recent advances in mixup methods primarily focus on mixing
based on saliency. However, many saliency detectors require intense computation
and are especially burdensome for parameter-heavy transformer models. To this
end, we propose TokenMixup, an efficient attention-guided token-level data
augmentation method that aims to maximize the saliency of a mixed set of
tokens. TokenMixup provides x15 faster saliency-aware data augmentation
compared to gradient-based methods. Moreover, we introduce a variant of
TokenMixup which mixes tokens within a single instance, thereby enabling
multi-scale feature augmentation. Experiments show that our methods
significantly improve the baseline models' performance on CIFAR and
ImageNet-1K, while being more efficient than previous methods. We also reach
state-of-the-art performance on CIFAR-100 among from-scratch transformer
models. Code is available at https://github.com/mlvlab/TokenMixup."
CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,0.864665,"Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities."
Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization,0.889543,"Vision transformers (ViTs) are emerging with significantly improved accuracy
in computer vision tasks. However, their complex architecture and enormous
computation/storage demand impose urgent needs for new hardware accelerator
design methodology. This work proposes an FPGA-aware automatic ViT acceleration
framework based on the proposed mixed-scheme quantization. To the best of our
knowledge, this is the first FPGA-based ViT acceleration framework exploring
model quantization. Compared with state-of-the-art ViT quantization work
(algorithmic approach only without hardware acceleration), our quantization
achieves 0.47% to 1.36% higher Top-1 accuracy under the same bit-width.
Compared with the 32-bit floating-point baseline FPGA accelerator, our
accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS
vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base."
EfficientNeRF: Efficient Neural Radiance Fields,0.864665,"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications."
KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,0.864665,"Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets. Our implementation and pretrained checkpoints are released
at https://github.com/chijames/KERPLE.git."
Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic Process,0.864665,"We introduce FiLex, a self-reinforcing stochastic process which models finite
lexicons in emergent language experiments. The central property of FiLex is
that it is a self-reinforcing process, parallel to the intuition that the more
a word is used in a language, the more its use will continue. As a theoretical
model, FiLex serves as a way to both explain and predict the behavior of the
emergent language system. We empirically test FiLex's ability to capture the
relationship between the emergent language's hyperparameters and the lexicon's
Shannon entropy."
BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation,0.930517,"We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image."
"CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron",0.912505,"Cervical cancer is the seventh most common cancer among all the cancers
worldwide and the fourth most common cancer among women. Cervical cytopathology
image classification is an important method to diagnose cervical cancer. Manual
screening of cytopathology images is time-consuming and error-prone. The
emergence of the automatic computer-aided diagnosis system solves this problem.
This paper proposes a framework called CVM-Cervix based on deep learning to
perform cervical cell classification tasks. It can analyze pap slides quickly
and accurately. CVM-Cervix first proposes a Convolutional Neural Network module
and a Visual Transformer module for local and global feature extraction
respectively, then a Multilayer Perceptron module is designed to fuse the local
and global features for the final classification. Experimental results show the
effectiveness and potential of the proposed CVM-Cervix in the field of cervical
Pap smear image classification. In addition, according to the practical needs
of clinical work, we perform a lightweight post-processing to compress the
model."
Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,0.902191,"Large language models (LLMs) have shown remarkable reasoning capabilities
given chain-of-thought prompts (examples with intermediate reasoning steps).
Existing benchmarks measure reasoning ability indirectly, by evaluating
accuracy on downstream tasks such as mathematical reasoning. However, it is
unclear how these models obtain the answers and whether they rely on simple
heuristics rather than the generated chain-of-thought. To enable systematic
exploration of the reasoning ability of LLMs, we present a new synthetic
question-answering dataset called PrOntoQA, where each example is generated
from a synthetic world model represented in first-order logic. This allows us
to parse the generated chain-of-thought into symbolic proofs for formal
analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite
capable of making correct individual deduction steps, and so are generally
capable of reasoning, even in fictional contexts. However, they have difficulty
with proof planning: When multiple valid deduction steps are available, they
are not able to systematically explore the different options."
SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views,0.922119,"We introduce SparseNeuS, a novel neural rendering based method for the task
of surface reconstruction from multi-view images. This task becomes more
difficult when only sparse images are provided as input, a scenario where
existing neural reconstruction approaches usually produce incomplete or
distorted results. Moreover, their inability of generalizing to unseen new
scenes impedes their application in practice. Contrarily, SparseNeuS can
generalize to new scenes and work well with sparse images (as few as 2 or 3).
SparseNeuS adopts signed distance function (SDF) as the surface representation,
and learns generalizable priors from image features by introducing geometry
encoding volumes for generic surface prediction. Moreover, several strategies
are introduced to effectively leverage sparse views for high-quality
reconstruction, including 1) a multi-level geometry reasoning framework to
recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color
blending scheme for more reliable color prediction; 3) a consistency-aware
fine-tuning scheme to control the inconsistent regions caused by occlusion and
noise. Extensive experiments demonstrate that our approach not only outperforms
the state-of-the-art methods, but also exhibits good efficiency,
generalizability, and flexibility."
Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior,0.942709,"Learned locomotion policies can rapidly adapt to diverse environments similar
to those experienced during training but lack a mechanism for fast tuning when
they fail in an out-of-distribution test environment. This necessitates a slow
and iterative cycle of reward and environment redesign to achieve good
performance on a new task. As an alternative, we propose learning a single
policy that encodes a structured family of locomotion strategies that solve
training tasks in different ways, resulting in Multiplicity of Behavior (MoB).
Different strategies generalize differently and can be chosen in real-time for
new tasks or environments, bypassing the need for time-consuming retraining. We
release a fast, robust open-source MoB locomotion controller, Walk These Ways,
that can execute diverse gaits with variable footswing, posture, and speed,
unlocking diverse downstream tasks: crouching, hopping, high-speed running,
stair traversal, bracing against shoves, rhythmic dance, and more. Video and
code release: https://gmargo11.github.io/walk-these-ways/"
Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer,0.926157,"Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html."
COLD: A Benchmark for Chinese Offensive Language Detection,0.896621,"Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier."
PETR: Position Embedding Transformation for Multi-View 3D Object Detection,0.910751,"In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}."
VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments,0.900509,"Visual Place Recognition and Visual Localization are essential components in
navigation and mapping for autonomous vehicles especially in GNSS-denied
navigation scenarios. Recent work has focused on ground or close to ground
applications such as self-driving cars or indoor-scenarios and low-altitude
drone flights. However, applications such as Urban Air Mobility require
operations in large-scale outdoor environments at medium to high altitudes. We
present a new dataset named VPAIR. The dataset was recorded on board a light
aircraft flying at an altitude of more than 300 meters above ground capturing
images with a downwardfacing camera. Each image is paired with a high
resolution reference render including dense depth information and 6-DoF
reference poses. The dataset covers a more than one hundred kilometers long
trajectory over various types of challenging landscapes, e.g. urban, farmland
and forests. Experiments on this dataset illustrate the challenges introduced
by the change in perspective to a bird's eye view such as in-plane rotations."
Concept Bottleneck Model with Additional Unsupervised Concepts,0.901364,"With the increasing demands for accountability, interpretability is becoming
an essential capability for real-world AI applications. However, most methods
utilize post-hoc approaches rather than training the interpretable model. In
this article, we propose a novel interpretable model based on the concept
bottleneck model (CBM). CBM uses concept labels to train an intermediate layer
as the additional visible layer. However, because the number of concept labels
restricts the dimension of this layer, it is difficult to obtain high accuracy
with a small number of labels. To address this issue, we integrate supervised
concepts with unsupervised ones trained with self-explaining neural networks
(SENNs). By seamlessly training these two types of concepts while reducing the
amount of computation, we can obtain both supervised and unsupervised concepts
simultaneously, even for large-sized images. We refer to the proposed model as
the concept bottleneck model with additional unsupervised concepts (CBM-AUC).
We experimentally confirmed that the proposed model outperformed CBM and SENN.
We also visualized the saliency map of each concept and confirmed that it was
consistent with the semantic meanings."
UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes,0.930517,"We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision."
Flexible Diffusion Modeling of Long Videos,0.933456,"We present a framework for video modeling based on denoising diffusion
probabilistic models that produces long-duration video completions in a variety
of realistic environments. We introduce a generative model that can at
test-time sample any arbitrary subset of video frames conditioned on any other
subset and present an architecture adapted for this purpose. Doing so allows us
to efficiently compare and optimize a variety of schedules for the order in
which frames in a long video are sampled and use selective sparse and
long-range conditioning on previously sampled frames. We demonstrate improved
video modeling over prior work on a number of datasets and sample temporally
coherent videos over 25 minutes in length. We additionally release a new video
modeling dataset and semantically meaningful metrics based on videos generated
in the CARLA autonomous driving simulator."
Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection,0.909282,"Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec."
"Continual Learning, Fast and Slow",0.90107,"According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}."
Polysemanticity and Capacity in Neural Networks,0.933014,"Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons."
Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning,0.916967,"Wordle is an online word puzzle game that gained viral popularity in January
2022. The goal is to guess a hidden five letter word. After each guess, the
player gains information about whether the letters they guessed are present in
the word, and whether they are in the correct position. Numerous blogs have
suggested guessing strategies and starting word lists that improve the chance
of winning. Optimized algorithms can win 100% of games within five of the six
allowed trials. However, it is infeasible for human players to use these
algorithms due to an inability to perfectly recall all known 5-letter words and
perform complex calculations that optimize information gain. Here, we present
two different methods for choosing starting words along with a framework for
discovering the optimal human strategy based on reinforcement learning. Human
Wordle players can use the rules we discover to optimize their chance of
winning."
Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation,0.918898,"Capturing emotions within a conversation plays an essential role in modern
dialogue systems. However, the weak correlation between emotions and semantics
brings many challenges to emotion recognition in conversation (ERC). Even
semantically similar utterances, the emotion may vary drastically depending on
contexts or speakers. In this paper, we propose a Supervised Prototypical
Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical
Network, the SPCL targets at solving the imbalanced classification problem
through contrastive learning and does not require a large batch size.
Meanwhile, we design a difficulty measure function based on the distance
between classes and introduce curriculum learning to alleviate the impact of
extreme samples. We achieve state-of-the-art results on three widely used
benchmarks. Further, we conduct analytical experiments to demonstrate the
effectiveness of our proposed SPCL and curriculum learning strategy. We release
the code at https://github.com/caskcsg/SPCL."
Large Language Models Struggle to Learn Long-Tail Knowledge,0.939386,"The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail."
BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision,0.901585,"We present a novel bird's-eye-view (BEV) detector with perspective
supervision, which converges faster and better suits modern image backbones.
Existing state-of-the-art BEV detectors are often tied to certain depth
pre-trained backbones like VoVNet, hindering the synergy between booming image
backbones and BEV detectors. To address this limitation, we prioritize easing
the optimization of BEV detectors by introducing perspective space supervision.
To this end, we propose a two-stage BEV detector, where proposals from the
perspective head are fed into the bird's-eye-view head for final predictions.
To evaluate the effectiveness of our model, we conduct extensive ablation
studies focusing on the form of supervision and the generality of the proposed
detector. The proposed method is verified with a wide spectrum of traditional
and modern image backbones and achieves new SoTA results on the large-scale
nuScenes dataset. The code shall be released soon."
Evaluating the Text-to-SQL Capabilities of Large Language Models,0.90452,"We perform an empirical evaluation of Text-to-SQL capabilities of the Codex
language model. We find that, without any finetuning, Codex is a strong
baseline on the Spider benchmark; we also analyze the failure modes of Codex in
this setting. Furthermore, we demonstrate on the GeoQuery and Scholar
benchmarks that a small number of in-domain examples provided in the prompt
enables Codex to perform better than state-of-the-art models finetuned on such
few-shot examples."
PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Transformers for Patronizing and Condescending Language Detection,0.897292,"Patronizing and condescending language (PCL) has a large harmful impact and
is difficult to detect, both for human judges and existing NLP systems. At
SemEval-2022 Task 4, we propose a novel Transformer-based model and its
ensembles to accurately understand such language context for PCL detection. To
facilitate comprehension of the subtle and subjective nature of PCL, two
fine-tuning strategies are applied to capture discriminative features from
diverse linguistic behaviour and categorical distribution. The system achieves
remarkable results on the official ranking, including 1st in Subtask 1 and 5th
in Subtask 2. Extensive experiments on the task demonstrate the effectiveness
of our system and its strategies."
OneRel:Joint Entity and Relation Extraction with One Module in One Step,0.94397,"Joint entity and relation extraction is an essential task in natural language
processing and knowledge graph construction. Existing approaches usually
decompose the joint extraction task into several basic modules or processing
steps to make it easy to conduct. However, such a paradigm ignores the fact
that the three elements of a triple are interdependent and indivisible.
Therefore, previous joint methods suffer from the problems of cascading errors
and redundant information. To address these issues, in this paper, we propose a
novel joint entity and relation extraction model, named OneRel, which casts
joint extraction as a fine-grained triple classification problem. Specifically,
our model consists of a scoring-based classifier and a relation-specific horns
tagging strategy. The former evaluates whether a token pair and a relation
belong to a factual triple. The latter ensures a simple but effective decoding
process. Extensive experimental results on two widely used datasets demonstrate
that the proposed method performs better than the state-of-the-art baselines,
and delivers consistent performance gain on complex scenarios of various
overlapping patterns and multiple triples."
"An Exploratory Study of Tweets about the SARS-CoV-2 Omicron Variant: Insights from Sentiment Analysis, Language Interpretation, Source Tracking, Type Classification, and Embedded URL Detection",0.945253,"This paper presents the findings of an exploratory study on the continuously
generating Big Data on Twitter related to the sharing of information, news,
views, opinions, ideas, feedback, and experiences about the COVID-19 pandemic,
with a specific focus on the Omicron variant, which is the globally dominant
variant of SARS-CoV-2 at this time. A total of 12028 tweets about the Omicron
variant were studied, and the specific characteristics of tweets that were
analyzed include - sentiment, language, source, type, and embedded URLs. The
findings of this study are manifold. First, from sentiment analysis, it was
observed that 50.5% of tweets had a neutral emotion. The other emotions - bad,
good, terrible, and great were found in 15.6%, 14.0%, 12.5%, and 7.5% of the
tweets, respectively. Second, the findings of language interpretation showed
that 65.9% of the tweets were posted in English. It was followed by Spanish,
French, Italian, and other languages. Third, the findings from source tracking
showed that Twitter for Android was associated with 35.2% of tweets. It was
followed by Twitter Web App, Twitter for iPhone, Twitter for iPad, and other
sources. Fourth, studying the type of tweets revealed that retweets accounted
for 60.8% of the tweets, it was followed by original tweets and replies that
accounted for 19.8% and 19.4% of the tweets, respectively. Fifth, in terms of
embedded URL analysis, the most common domain embedded in the tweets was found
to be twitter.com, which was followed by biorxiv.org, nature.com, and other
domains. Finally, to support similar research in this field, we have developed
a Twitter dataset that comprises more than 500,000 tweets about the SARS-CoV-2
omicron variant since the first detected case of this variant on November 24,
2021."
Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images,0.910037,"In this paper, we present a generalizable model-free 6-DoF object pose
estimator called Gen6D. Existing generalizable pose estimators either need
high-quality object models or require additional depth maps or object masks in
test time, which significantly limits their application scope. In contrast, our
pose estimator only requires some posed images of the unseen object and is able
to accurately predict the poses of the object in arbitrary environments. Gen6D
consists of an object detector, a viewpoint selector and a pose refiner, all of
which do not require the 3D object model and can generalize to unseen objects.
Experiments show that Gen6D achieves state-of-the-art results on two model-free
datasets: the MOPED dataset and a new GenMOP dataset collected by us. In
addition, on the LINEMOD dataset, Gen6D achieves competitive results compared
with instance-specific pose estimators. Project page:
https://liuyuan-pal.github.io/Gen6D/."
Large Language Models are Few-Shot Clinical Information Extractors,0.904359,"A long-running goal of the clinical NLP community is the extraction of
important variables trapped in clinical notes. However, roadblocks have
included dataset shift from the general domain and a lack of public clinical
corpora and annotations. In this work, we show that large language models, such
as InstructGPT, perform well at zero- and few-shot information extraction from
clinical text despite not being trained specifically for the clinical domain.
Whereas text classification and generation performance have already been
studied extensively in such models, here we additionally demonstrate how to
leverage them to tackle a diverse set of NLP tasks which require more
structured outputs, including span identification, token-level sequence
classification, and relation extraction. Further, due to the dearth of
available data to evaluate these systems, we introduce new datasets for
benchmarking few-shot clinical information extraction based on a manual
re-annotation of the CASI dataset for new tasks. On the clinical extraction
tasks we studied, the GPT-3 systems significantly outperform existing zero- and
few-shot baselines."
PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition,0.903028,"3D Point cloud is becoming a critical data representation in many real-world
applications like autonomous driving, robotics, and medical imaging. Although
the success of deep learning further accelerates the adoption of 3D point
clouds in the physical world, deep learning is notorious for its vulnerability
to adversarial attacks. In this work, we first identify that the
state-of-the-art empirical defense, adversarial training, has a major
limitation in applying to 3D point cloud models due to gradient obfuscation. We
further propose PointDP, a purification strategy that leverages diffusion
models to defend against 3D adversarial attacks. We extensively evaluate
PointDP on six representative 3D point cloud architectures, and leverage 10+
strong and adaptive attacks to demonstrate its lower-bound robustness. Our
evaluation shows that PointDP achieves significantly better robustness than
state-of-the-art purification methods under strong attacks. Results of
certified defenses on randomized smoothing combined with PointDP will be
included in the near future."
"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",0.944582,"We introduce Mintaka, a complex, natural, and multilingual dataset designed
for experimenting with end-to-end question-answering models. Mintaka is
composed of 20,000 question-answer pairs collected in English, annotated with
Wikidata entities, and translated into Arabic, French, German, Hindi, Italian,
Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka
includes 8 types of complex questions, including superlative, intersection, and
multi-hop questions, which were naturally elicited from crowd workers. We run
baselines over Mintaka, the best of which achieves 38% hits@1 in English and
31% hits@1 multilingually, showing that existing models have room for
improvement. We release Mintaka at https://github.com/amazon-research/mintaka."
cosFormer: Rethinking Softmax in Attention,0.998054,"Transformer has shown great successes in natural language processing,
computer vision, and audio processing. As one of its core components, the
softmax attention helps to capture long-range dependencies yet prohibits its
scale-up due to the quadratic space and time complexity to the sequence length.
Kernel methods are often adopted to reduce the complexity by approximating the
softmax operator. Nevertheless, due to the approximation errors, their
performances vary in different tasks/corpus and suffer crucial performance
drops when compared with the vanilla softmax attention. In this paper, we
propose a linear transformer called cosFormer that can achieve comparable or
better accuracy to the vanilla transformer in both casual and cross attentions.
cosFormer is based on two key properties of softmax attention: i).
non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme
that can concentrate the distribution of the attention matrix. As its linear
substitute, cosFormer fulfills these properties with a linear operator and a
cosine-based distance re-weighting mechanism. Extensive experiments on language
modeling and text understanding tasks demonstrate the effectiveness of our
method. We further examine our method on long sequences and achieve
state-of-the-art performance on the Long-Range Arena benchmark. The source code
is available at https://github.com/OpenNLPLab/cosFormer."
The Role of ImageNet Classes in Frchet Inception Distance,0.964328,"Fr\'echet Inception Distance (FID) is the primary metric for ranking models
in data-driven generative modeling. While remarkably successful, the metric is
known to sometimes disagree with human judgement. We investigate a root cause
of these discrepancies, and visualize what FID ""looks at"" in generated images.
We show that the feature space that FID is (typically) computed in is so close
to the ImageNet classifications that aligning the histograms of Top-$N$
classifications between sets of generated and real images can reduce FID
substantially -- without actually improving the quality of results. Thus, we
conclude that FID is prone to intentional or accidental distortions. As a
practical example of an accidental distortion, we discuss a case where an
ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while
being worse in terms of human evaluation."
mSLAM: Massively multilingual joint pre-training for speech and text,0.955329,"We present mSLAM, a multilingual Speech and LAnguage Model that learns
cross-lingual cross-modal representations of speech and text by pre-training
jointly on large amounts of unlabeled speech and text in multiple languages.
mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on
character-level text, along with Connectionist Temporal Classification (CTC)
losses on paired speech and transcript data, to learn a single model capable of
learning from and representing both speech and text signals in a shared
representation space. We evaluate mSLAM on several downstream speech
understanding tasks and find that joint pre-training with text improves quality
on speech translation, speech intent classification and speech language-ID
while being competitive on multilingual ASR, when compared against speech-only
pre-training. Our speech translation model demonstrates zero-shot text
translation without seeing any text translation data, providing evidence for
cross-modal alignment of representations. mSLAM also benefits from multi-modal
fine-tuning, further improving the quality of speech translation by directly
leveraging text translation data during the fine-tuning process. Our empirical
analysis highlights several opportunities and challenges arising from
large-scale multimodal pre-training, suggesting directions for future research."
Visual Programming: Compositional visual reasoning without training,0.999999,"We present VISPROG, a neuro-symbolic approach to solving complex and
compositional visual tasks given natural language instructions. VISPROG avoids
the need for any task-specific training. Instead, it uses the in-context
learning ability of large language models to generate python-like modular
programs, which are then executed to get both the solution and a comprehensive
and interpretable rationale. Each line of the generated program may invoke one
of several off-the-shelf computer vision models, image processing routines, or
python functions to produce intermediate outputs that may be consumed by
subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4
diverse tasks - compositional visual question answering, zero-shot reasoning on
image pairs, factual knowledge object tagging, and language-guided image
editing. We believe neuro-symbolic approaches like VISPROG are an exciting
avenue to easily and effectively expand the scope of AI systems to serve the
long tail of complex tasks that people may wish to perform."
Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality,0.974503,"We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground."
MiniViT: Compressing Vision Transformers with Weight Multiplexing,0.967283,"Vision Transformer (ViT) models have recently drawn much attention in
computer vision due to their high model capability. However, ViT models suffer
from huge number of parameters, restricting their applicability on devices with
limited memory. To alleviate this problem, we propose MiniViT, a new
compression framework, which achieves parameter reduction in vision
transformers while retaining the same performance. The central idea of MiniViT
is to multiplex the weights of consecutive transformer blocks. More
specifically, we make the weights shared across layers, while imposing a
transformation on the weights to increase diversity. Weight distillation over
self-attention is also applied to transfer knowledge from large-scale ViT
models to weight-multiplexed compact models. Comprehensive experiments
demonstrate the efficacy of MiniViT, showing that it can reduce the size of the
pre-trained Swin-B transformer by 48\%, while achieving an increase of 1.0\% in
Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,
MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,
without seriously compromising the performance. Finally, we verify the
transferability of MiniViT by reporting its performance on downstream
benchmarks. Code and models are available at here."
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,1.0,"Chain-of-thought prompting has demonstrated remarkable performance on various
natural language reasoning tasks. However, it tends to perform poorly on tasks
which requires solving problems harder than the exemplars shown in the prompts.
To overcome this challenge of easy-to-hard generalization, we propose a novel
prompting strategy, least-to-most prompting. The key idea in this strategy is
to break down a complex problem into a series of simpler subproblems and then
solve them in sequence. Solving each subproblem is facilitated by the answers
to previously solved subproblems. Our experimental results on tasks related to
symbolic manipulation, compositional generalization, and math reasoning reveal
that least-to-most prompting is capable of generalizing to more difficult
problems than those seen in the prompts. A notable finding is that when the
GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve
the compositional generalization benchmark SCAN in any split (including length
split) with an accuracy of at least 99% using just 14 exemplars, compared to
only 16% accuracy with chain-of-thought prompting. This is particularly
noteworthy because neural-symbolic models in the literature that specialize in
solving SCAN are trained on the entire training set containing over 15,000
examples. We have included prompts for all the tasks in the Appendix."
CM3: A Causal Masked Multimodal Model of the Internet,0.988314,"We introduce CM3, a family of causally masked generative models trained over
a large corpus of structured multi-modal documents that can contain both text
and image tokens. Our new causally masked approach generates tokens left to
right while also masking out a small number of long token spans that are
generated at the end of the string, instead of their original positions. The
casual masking object provides a type of hybrid of the more common causal and
masked language models, by enabling full generative modeling while also
providing bidirectional context when generating the masked spans. We train
causally masked language-image models on large-scale web and Wikipedia
articles, where each document contains all of the text, hypertext markup,
hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they
appear in the original HTML source (before masking). The resulting CM3 models
can generate rich structured, multi-modal outputs while conditioning on
arbitrary masked document contexts, and thereby implicitly learn a wide range
of text, image, and cross modal tasks. They can be prompted to recover, in a
zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM.
We set the new state-of-the-art in zero-shot summarization, entity linking, and
entity disambiguation while maintaining competitive performance in the
fine-tuning setting. We can generate images unconditionally, conditioned on
text (like DALL-E) and do captioning all in a zero-shot setting with a single
model."
Third Time's the Charm? Image and Video Editing with StyleGAN3,0.99293,"StyleGAN is arguably one of the most intriguing and well-studied generative
models, demonstrating impressive performance in image generation, inversion,
and manipulation. In this work, we explore the recent StyleGAN3 architecture,
compare it to its predecessor, and investigate its unique advantages, as well
as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained
on unaligned data, one can still use aligned data for training, without
hindering the ability to generate unaligned imagery. Next, our analysis of the
disentanglement of the different latent spaces of StyleGAN3 indicates that the
commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,
underscoring the benefits of using the StyleSpace for fine-grained editing.
Considering image inversion, we observe that existing encoder-based techniques
struggle when trained on unaligned data. We therefore propose an encoding
scheme trained solely on aligned data, yet can still invert unaligned images.
Finally, we introduce a novel video inversion and editing workflow that
leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce
texture sticking and expand the field of view of the edited video."
SimSR: Simple Distance-based State Representation for Deep Reinforcement Learning,0.973652,"This work explores how to learn robust and generalizable state representation
from image-based observations with deep reinforcement learning methods.
Addressing the computational complexity, stringent assumptions and
representation collapse challenges in existing work of bisimulation metric, we
devise Simple State Representation (SimSR) operator. SimSR enables us to design
a stochastic approximation method that can practically learn the mapping
functions (encoders) from observations to latent representation space. In
addition to the theoretical analysis and comparison with the existing work, we
experimented and compared our work with recent state-of-the-art solutions in
visual MuJoCo tasks. The results shows that our model generally achieves better
performance and has better robustness and good generalization."
"""I think this is the most disruptive technology"": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data",0.968952,"Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users."
Demystifying Prompts in Language Models via Perplexity Estimation,0.978255,"Language models can be prompted to perform a wide variety of zero- and
few-shot learning problems. However, performance varies significantly with the
choice of prompt, and we do not yet understand why this happens or how to pick
the best prompts. In this work, we analyze the factors that contribute to this
variance and establish a new empirical hypothesis: the performance of a prompt
is coupled with the extent to which the model is familiar with the language it
contains. Over a wide range of tasks, we show that the lower the perplexity of
the prompt is, the better the prompt is able to perform the task. As a result,
we devise a method for creating prompts: (1) automatically extend a small seed
set of manually written prompts by paraphrasing using GPT3 and backtranslation
and (2) choose the lowest perplexity prompts to get significant gains in
performance."
Multi-Game Decision Transformers,0.967645,"A longstanding goal of the field of AI is a method for learning a highly
capable, generalist agent from diverse experience. In the subfields of vision
and language, this was largely achieved by scaling up transformer-based models
and training them on large, diverse datasets. Motivated by this progress, we
investigate whether the same strategy can be used to produce generalist
reinforcement learning agents. Specifically, we show that a single
transformer-based model - with a single set of weights - trained purely offline
can play a suite of up to 46 Atari games simultaneously at close-to-human
performance. When trained and evaluated appropriately, we find that the same
trends observed in language and vision hold, including scaling of performance
with model size and rapid adaptation to new games via fine-tuning. We compare
several approaches in this multi-game setting, such as online and offline RL
methods and behavioral cloning, and find that our Multi-Game Decision
Transformer models offer the best scalability and performance. We release the
pre-trained models and code to encourage further research in this direction."
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation,0.999976,"Vision-Language Pre-training (VLP) has advanced the performance for many
vision-language tasks. However, most existing pre-trained models only excel in
either understanding-based tasks or generation-based tasks. Furthermore,
performance improvement has been largely achieved by scaling up the dataset
with noisy image-text pairs collected from the web, which is a suboptimal
source of supervision. In this paper, we propose BLIP, a new VLP framework
which transfers flexibly to both vision-language understanding and generation
tasks. BLIP effectively utilizes the noisy web data by bootstrapping the
captions, where a captioner generates synthetic captions and a filter removes
the noisy ones. We achieve state-of-the-art results on a wide range of
vision-language tasks, such as image-text retrieval (+2.7% in average
recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).
BLIP also demonstrates strong generalization ability when directly transferred
to video-language tasks in a zero-shot manner. Code, models, and datasets are
released at https://github.com/salesforce/BLIP."
News Summarization and Evaluation in the Era of GPT-3,0.991461,"The recent success of prompting large language models like GPT-3 has led to a
paradigm shift in NLP research. In this paper, we study its impact on text
summarization, focusing on the classic benchmark domain of news summarization.
First, we investigate how GPT-3 compares against fine-tuned models trained on
large summarization datasets. We show that not only do humans overwhelmingly
prefer GPT-3 summaries, prompted using only a task description, but these also
do not suffer from common dataset-specific issues such as poor factuality.
Next, we study what this means for evaluation, particularly the role of gold
standard test sets. Our experiments show that both reference-based and
reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.
Finally, we evaluate models on a setting beyond generic summarization,
specifically keyword-based summarization, and show how dominant fine-tuning
approaches compare to prompting.
  To support further research, we release: (a) a corpus of 10K generated
summaries from fine-tuned and prompt-based models across 4 standard
summarization benchmarks, (b) 1K human preference judgments comparing different
systems for generic- and keyword-based summarization."
Omnivore: A Single Model for Many Visual Modalities,0.999928,"Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together."
Frontiers and Exact Learning of ELI Queries under DL-Lite Ontologies,0.977392,"We study ELI queries (ELIQs) in the presence of ontologies formulated in the
description logic DL-Lite. For the dialect DL-LiteH, we show that ELIQs have a
frontier (set of least general generalizations) that is of polynomial size and
can be computed in polynomial time. In the dialect DL-LiteF, in contrast,
frontiers may be infinite. We identify a natural syntactic restriction that
enables the same positive results as for DL-LiteH. We use out results on
frontiers to show that ELIQs are learnable in polynomial time in the presence
of a DL-LiteH / restricted DL-LiteF ontology in Angluin's framework of exact
learning with only membership queries."
SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection,0.981684,"Convolutional neural networks (CNNs) are good at extracting contexture
features within certain receptive fields, while transformers can model the
global long-range dependency features. By absorbing the advantage of
transformer and the merit of CNN, Swin Transformer shows strong feature
representation ability. Based on it, we propose a cross-modality fusion model
SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin
Transformer to extract the hierarchical features, boosted by attention
mechanism to bridge the gap between two modalities, and guided by edge
information to sharp the contour of salient object. To be specific, two-stream
Swin Transformer encoder first extracts multi-modality features, and then
spatial alignment and channel re-calibration module is presented to optimize
intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided
decoder achieves inter-level cross-modality fusion under the guidance of edge
features. The proposed model outperforms the state-of-the-art models on RGB-D
and RGB-T datasets, showing that it provides more insight into the
cross-modality complementarity task."
Dataset Distillation by Matching Training Trajectories,0.964366,"Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data."
Masked Generative Distillation,0.989691,"Knowledge distillation has been applied to various tasks successfully. The
current distillation algorithm usually improves students' performance by
imitating the output of the teacher. This paper shows that teachers can also
improve students' representation power by guiding students' feature recovery.
From this point of view, we propose Masked Generative Distillation (MGD), which
is simple: we mask random pixels of the student's feature and force it to
generate the teacher's full feature through a simple block. MGD is a truly
general feature-based distillation method, which can be utilized on various
tasks, including image classification, object detection, semantic segmentation
and instance segmentation. We experiment on different models with extensive
datasets and the results show that all the students achieve excellent
improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1
accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,
SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on
ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at
https://github.com/yzd-v/MGD."
Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic,0.991252,"Propaganda is the expression of an opinion or an action by an individual or a
group deliberately designed to influence the opinions or the actions of other
individuals or groups with reference to predetermined ends, which is achieved
by means of well-defined rhetorical and psychological devices. Propaganda
techniques are commonly used in social media to manipulate or to mislead users.
Thus, there has been a lot of recent research on automatic detection of
propaganda techniques in text as well as in memes. However, so far the focus
has been primarily on English. With the aim to bridge this language gap, we ran
a shared task on detecting propaganda techniques in Arabic tweets as part of
the WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to
identify the set of propaganda techniques used in a tweet, which is a
multilabel classification problem, while Subtask~2 asks to detect the
propaganda techniques used in a tweet together with the exact span(s) of text
in which each propaganda technique appears. The task attracted 63 team
registrations, and eventually 14 and 3 teams made submissions for subtask 1 and
2, respectively. Finally, 11 teams submitted system description papers."
SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D Shape Generation,0.970239,"We present a StyleGAN2-based deep learning approach for 3D shape generation,
called SDF-StyleGAN, with the aim of reducing visual and geometric
dissimilarity between generated shapes and a shape collection. We extend
StyleGAN2 to 3D generation and utilize the implicit signed distance function
(SDF) as the 3D shape representation, and introduce two novel global and local
shape discriminators that distinguish real and fake SDF values and gradients to
significantly improve shape geometry and visual quality. We further complement
the evaluation metrics of 3D generative models with the shading-image-based
Fr\'echet inception distance (FID) scores to better assess visual quality and
shape distribution of the generated shapes. Experiments on shape generation
demonstrate the superior performance of SDF-StyleGAN over the state-of-the-art.
We further demonstrate the efficacy of SDF-StyleGAN in various tasks based on
GAN inversion, including shape reconstruction, shape completion from partial
point clouds, single-view image-based shape generation, and shape style
editing. Extensive ablation studies justify the efficacy of our framework
design. Our code and trained models are available at
https://github.com/Zhengxinyang/SDF-StyleGAN."
UniCLIP: Unified Framework for Contrastive Language-Image Pre-training,0.976482,"Pre-training vision-language models with contrastive objectives has shown
promising results that are both scalable to large uncurated datasets and
transferable to many downstream applications. Some following works have
targeted to improve data efficiency by adding self-supervision terms, but
inter-domain (image-text) contrastive loss and intra-domain (image-image)
contrastive loss are defined on individual spaces in those works, so many
feasible combinations of supervision are overlooked. To overcome this issue, we
propose UniCLIP, a Unified framework for Contrastive Language-Image
Pre-training. UniCLIP integrates the contrastive loss of both inter-domain
pairs and intra-domain pairs into a single universal space. The discrepancies
that occur when integrating contrastive loss between different domains are
resolved by the three key components of UniCLIP: (1) augmentation-aware feature
embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure.
UniCLIP outperforms previous vision-language pre-training methods on various
single- and multi-modality downstream tasks. In our experiments, we show that
each component that comprises UniCLIP contributes well to the final
performance."
Exploring Plain Vision Transformer Backbones for Object Detection,0.994474,"We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone
network for object detection. This design enables the original ViT architecture
to be fine-tuned for object detection without needing to redesign a
hierarchical backbone for pre-training. With minimal adaptations for
fine-tuning, our plain-backbone detector can achieve competitive results.
Surprisingly, we observe: (i) it is sufficient to build a simple feature
pyramid from a single-scale feature map (without the common FPN design) and
(ii) it is sufficient to use window attention (without shifting) aided with
very few cross-window propagation blocks. With plain ViT backbones pre-trained
as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the
previous leading methods that were all based on hierarchical backbones,
reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K
pre-training. We hope our study will draw attention to research on
plain-backbone detectors. Code for ViTDet is available in Detectron2."
Watching the News: Towards VideoQA Models that can Read,0.950213,"Video Question Answering methods focus on commonsense reasoning and visual
cognition of objects or persons and their interactions over time. Current
VideoQA approaches ignore the textual information present in the video.
Instead, we argue that textual information is complementary to the action and
provides essential contextualisation cues to the reasoning process. To this
end, we propose a novel VideoQA task that requires reading and understanding
the text in the video. To explore this direction, we focus on news videos and
require QA systems to comprehend and answer questions about the topics
presented by combining visual and textual cues in the video. We introduce the
``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$
news videos obtained from diverse news channels from around the world. We
demonstrate the limitations of current Scene Text VQA and VideoQA methods and
propose ways to incorporate scene text information into VideoQA methods."
MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity Recognition,0.999854,"We present MultiCoNER, a large multilingual dataset for Named Entity
Recognition that covers 3 domains (Wiki sentences, questions, and search
queries) across 11 languages, as well as multilingual and code-mixing subsets.
This dataset is designed to represent contemporary challenges in NER, including
low-context scenarios (short and uncased text), syntactically complex entities
like movie titles, and long-tail entity distributions. The 26M token dataset is
compiled from public resources using techniques such as heuristic-based
sentence sampling, template extraction and slotting, and machine translation.
We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a
state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves
moderate performance (macro-F1=54%), highlighting the difficulty of our data.
GEMNET, which uses gazetteers, improvement significantly (average improvement
of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained
language models, and we believe that it can help further research in building
robust NER systems. MultiCoNER is publicly available at
https://registry.opendata.aws/multiconer/ and we hope that this resource will
help advance research in various aspects of NER."
"Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot Challenge on Conversational Task Assistance",0.993697,"Since its inception in 2016, the Alexa Prize program has enabled hundreds of
university students to explore and compete to develop conversational agents
through the SocialBot Grand Challenge. The goal of the challenge is to build
agents capable of conversing coherently and engagingly with humans on popular
topics for 20 minutes, while achieving an average rating of at least 4.0/5.0.
However, as conversational agents attempt to assist users with increasingly
complex tasks, new conversational AI techniques and evaluation platforms are
needed. The Alexa Prize TaskBot challenge, established in 2021, builds on the
success of the SocialBot challenge by introducing the requirements of
interactively assisting humans with real-world Cooking and Do-It-Yourself
tasks, while making use of both voice and visual modalities. This challenge
requires the TaskBots to identify and understand the user's need, identify and
integrate task and domain knowledge into the interaction, and develop new ways
of engaging the user without distracting them from the task at hand, among
other challenges. This paper provides an overview of the TaskBot challenge,
describes the infrastructure support provided to the teams with the CoBot
Toolkit, and summarizes the approaches the participating teams took to overcome
the research challenges. Finally, it analyzes the performance of the competing
TaskBots during the first year of the competition."
