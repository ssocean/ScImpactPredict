title,TNCSI,abstract
Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem,0.0825292,"Math word problem solver requires both precise relation reasoning about
quantities in the text and reliable generation for the diverse equation.
Current sequence-to-tree or relation extraction methods regard this only from a
fixed view, struggling to simultaneously handle complex semantics and diverse
equations. However, human solving naturally involves two consistent reasoning
views: top-down and bottom-up, just as math equations also can be expressed in
multiple equivalent forms: pre-order and post-order. We propose a multi-view
consistent contrastive learning for a more complete semantics-to-equation
mapping. The entire process is decoupled into two independent but consistent
views: top-down decomposition and bottom-up construction, and the two reasoning
views are aligned in multi-granularity for consistency, enhancing global
generation and precise reasoning. Experiments on multiple datasets across two
languages show our approach significantly outperforms the existing baselines,
especially on complex problems. We also show after consistent alignment,
multi-view can absorb the merits of both views and generate more diverse
results consistent with the mathematical laws."
Insights on Neural Representations for End-to-End Speech Recognition,0.00654443,"End-to-end automatic speech recognition (ASR) models aim to learn a
generalised speech representation. However, there are limited tools available
to understand the internal functions and the effect of hierarchical
dependencies within the model architecture. It is crucial to understand the
correlations between the layer-wise representations, to derive insights on the
relationship between neural representations and performance.
  Previous investigations of network similarities using correlation analysis
techniques have not been explored for End-to-End ASR models. This paper
analyses and explores the internal dynamics between layers during training with
CNN, LSTM and Transformer based approaches using Canonical correlation analysis
(CCA) and centered kernel alignment (CKA) for the experiments. It was found
that neural representations within CNN layers exhibit hierarchical correlation
dependencies as layer depth increases but this is mostly limited to cases where
neural representation correlates more closely. This behaviour is not observed
in LSTM architecture, however there is a bottom-up pattern observed across the
training process, while Transformer encoder layers exhibit irregular
coefficiency correlation as neural depth increases. Altogether, these results
provide new insights into the role that neural architectures have upon speech
recognition performance. More specifically, these techniques can be used as
indicators to build better performing speech recognition models."
TwistSLAM++: Fusing multiple modalities for accurate dynamic semantic SLAM,0.0446313,"Most classical SLAM systems rely on the static scene assumption, which limits
their applicability in real world scenarios. Recent SLAM frameworks have been
proposed to simultaneously track the camera and moving objects. However they
are often unable to estimate the canonical pose of the objects and exhibit a
low object tracking accuracy. To solve this problem we propose TwistSLAM++, a
semantic, dynamic, SLAM system that fuses stereo images and LiDAR information.
Using semantic information, we track potentially moving objects and associate
them to 3D object detections in LiDAR scans to obtain their pose and size.
Then, we perform registration on consecutive object scans to refine object pose
estimation. Finally, object scans are used to estimate the shape of the object
and constrain map points to lie on the estimated surface within the BA. We show
on classical benchmarks that this fusion approach based on multimodal
information improves the accuracy of object tracking."
Automatic Severity Classification of Dysarthric speech by using Self-supervised Model with Multi-task Learning,0.0426828,"Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect."
Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting,0.100494,"This paper presents work on novel machine translation (MT) systems between
spoken and signed languages, where signed languages are represented in
SignWriting, a sign language writing system. Our work seeks to address the lack
of out-of-the-box support for signed languages in current MT systems and is
based on the SignBank dataset, which contains pairs of spoken language text and
SignWriting content. We introduce novel methods to parse, factorize, decode,
and evaluate SignWriting, leveraging ideas from neural factored MT. In a
bilingual setup--translating from American Sign Language to (American)
English--our method achieves over 30 BLEU, while in two multilingual
setups--translating in both directions between spoken languages and signed
languages--we achieve over 20 BLEU. We find that common MT techniques used to
improve spoken language translation similarly affect the performance of sign
language translation. These findings validate our use of an intermediate text
representation for signed languages to include them in natural language
processing research."
Atypical lexical abbreviations identification in Russian medical texts,0.00780499,"Abbreviation is a method of word formation that aims to construct the
shortened term from the first letters of the initial phrase. Implicit
abbreviations frequently cause the comprehension difficulties for unprepared
readers. In this paper, we propose an efficient ML-based algorithm which allows
to identify the abbreviations in Russian texts. The method achieves ROC AUC
score 0.926 and F1 score 0.706 which are confirmed as competitive in comparison
with the baselines. Along with the pipeline, we also establish first to our
knowledge Russian dataset that is relevant for the desired task."
Automated Audio Captioning with Epochal Difficult Captions for Curriculum Learning,0.0431766,"In this paper, we propose an algorithm, Epochal Difficult Captions, to
supplement the training of any model for the Automated Audio Captioning task.
Epochal Difficult Captions is an elegant evolution to the keyword estimation
task that previous work have used to train the encoder of the AAC model.
Epochal Difficult Captions modifies the target captions based on a curriculum
and a difficulty level determined as a function of current epoch. Epochal
Difficult Captions can be used with any model architecture and is a lightweight
function that does not increase training time. We test our results on three
systems and show that using Epochal Difficult Captions consistently improves
performance"
Data augmentation for efficient learning from parametric experts,0.0089734,"We present a simple, yet powerful data-augmentation technique to enable
data-efficient learning from parametric experts for reinforcement and imitation
learning. We focus on what we call the policy cloning setting, in which we use
online or offline queries of an expert or expert policy to inform the behavior
of a student policy. This setting arises naturally in a number of problems, for
instance as variants of behavior cloning, or as a component of other algorithms
such as DAGGER, policy distillation or KL-regularized RL. Our approach,
augmented policy cloning (APC), uses synthetic states to induce
feedback-sensitivity in a region around sampled trajectories, thus dramatically
reducing the environment interactions required for successful cloning of the
expert. We achieve highly data-efficient transfer of behavior from an expert to
a student policy for high-degrees-of-freedom control problems. We demonstrate
the benefit of our method in the context of several existing and widely used
algorithms that include policy cloning as a constituent part. Moreover, we
highlight the benefits of our approach in two practically relevant settings (a)
expert compression, i.e. transfer to a student with fewer parameters; and (b)
transfer from privileged experts, i.e. where the expert has a different
observation space than the student, usually including access to privileged
information."
Generating Explanations from Deep Reinforcement Learning Using Episodic Memory,0.0366845,"Deep Reinforcement Learning (RL) involves the use of Deep Neural Networks
(DNNs) to make sequential decisions in order to maximize reward. For many tasks
the resulting sequence of actions produced by a Deep RL policy can be long and
difficult to understand for humans. A crucial component of human explanations
is selectivity, whereby only key decisions and causes are recounted. Imbuing
Deep RL agents with such an ability would make their resulting policies easier
to understand from a human perspective and generate a concise set of
instructions to aid the learning of future agents. To this end we use a Deep RL
agent with an episodic memory system to identify and recount key decisions
during policy execution. We show that these decisions form a short, human
readable explanation that can also be used to speed up the learning of naive
Deep RL agents in an algorithm-independent manner."
BERT4Loc: BERT for Location -- POI Recommender System,0.0520889,"Recommending points of interest (POIs) is a challenging task that requires
extracting comprehensive location data from location-based social media
platforms. To provide effective location-based recommendations, it's important
to analyze users' historical behavior and preferences. In this study, we
present a sophisticated location-aware recommendation system that uses
Bidirectional Encoder Representations from Transformers (BERT) to offer
personalized location-based suggestions. Our model combines location
information and user preferences to provide more relevant recommendations
compared to models that predict the next POI in a sequence. Our experiments on
two benchmark dataset show that our BERT-based model outperforms various
state-of-the-art sequential models. Moreover, we see the effectiveness of the
proposed model for quality through additional experiments."
Semiconductor Defect Pattern Classification by Self-Proliferation-and-Attention Neural Network,0.0740846,"Semiconductor manufacturing is on the cusp of a revolution: the Internet of
Things (IoT). With IoT we can connect all the equipment and feed information
back to the factory so that quality issues can be detected. In this situation,
more and more edge devices are used in wafer inspection equipment. This edge
device must have the ability to quickly detect defects. Therefore, how to
develop a high-efficiency architecture for automatic defect classification to
be suitable for edge devices is the primary task. In this paper, we present a
novel architecture that can perform defect classification in a more efficient
way. The first function is self-proliferation, using a series of linear
transformations to generate more feature maps at a cheaper cost. The second
function is self-attention, capturing the long-range dependencies of feature
map by the channel-wise and spatial-wise attention mechanism. We named this
method as self-proliferation-and-attention neural network. This method has been
successfully applied to various defect pattern classification tasks. Compared
with other latest methods, SP&A-Net has higher accuracy and lower computation
cost in many defect inspection tasks."
Leaf: Multiple-Choice Question Generation,0.0700979,"Testing with quiz questions has proven to be an effective way to assess and
improve the educational process. However, manually creating quizzes is tedious
and time-consuming. To address this challenge, we present Leaf, a system for
generating multiple-choice questions from factual text. In addition to being
very well suited for the classroom, Leaf could also be used in an industrial
setting, e.g., to facilitate onboarding and knowledge sharing, or as a
component of chatbots, question answering systems, or Massive Open Online
Courses (MOOCs). The code and the demo are available on
https://github.com/KristiyanVachev/Leaf-Question-Generation."
Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding,0.00501456,"In recent years, large pre-trained Transformer networks have demonstrated
dramatic improvements in many natural language understanding tasks. However,
the huge size of these models brings significant challenges to their
fine-tuning and online deployment due to latency and cost constraints. New
hardware supporting both N:M semi-structured sparsity and low-precision integer
computation is a promising solution to boost DNN model serving efficiency.
However, there have been very few studies that systematically investigate to
what extent pre-trained Transformer networks benefit from the combination of
these techniques, as well as how to best compress each component of the
Transformer. We propose a flexible compression framework NxMiFormer that
performs simultaneous sparsification and quantization using ADMM and STE-based
QAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm
that identifies promising heterogeneous compression configurations that meet a
compression ratio constraint. When evaluated across the GLUE suite of NLU
benchmarks, our approach can achieve up to 93% compression of the encoders of a
BERT model while retaining 98.2% of the original model accuracy and taking full
advantage of the hardware's capabilities. Heterogeneous configurations found
the by the search heuristic maintain 99.5% of the baseline accuracy while still
compressing the model by 87.5%."
Data-driven prediction of Air Traffic Controllers reactions to resolving conflicts,0.100555,"With the aim to enhance automation in conflict detection and resolution
(CD&R) tasks in the Air Traffic Management domain, in this paper we propose
deep learning techniques (DL) that can learn models of Air Traffic Controllers'
(ATCO) reactions in resolving conflicts that can violate separation minimum
constraints among aircraft trajectories: This implies learning when the ATCO
will react towards resolving a conflict, and how he/she will react. Timely
reactions, to which this paper aims, focus on when do reactions happen, aiming
to predict the trajectory points, as the trajectory evolves, that the ATCO
issues a conflict resolution action, while also predicting the type of
resolution action (if any). Towards this goal, the paper formulates the ATCO
reactions prediction problem for CD&R, and presents DL methods that can model
ATCO timely reactions and evaluates these methods in real-world data sets,
showing their efficacy in prediction with very high accuracy."
The Neural Race Reduction: Dynamics of Abstraction in Gated Networks,0.0579237,"Our theoretical understanding of deep learning has not kept pace with its
empirical success. While network architecture is known to be critical, we do
not yet understand its effect on learned representations and network behavior,
or how this architecture should reflect task structure.In this work, we begin
to address this gap by introducing the Gated Deep Linear Network framework that
schematizes how pathways of information flow impact learning dynamics within an
architecture. Crucially, because of the gating, these networks can compute
nonlinear functions of their input. We derive an exact reduction and, for
certain cases, exact solutions to the dynamics of learning. Our analysis
demonstrates that the learning dynamics in structured networks can be
conceptualized as a neural race with an implicit bias towards shared
representations, which then govern the model's ability to systematically
generalize, multi-task, and transfer. We validate our key insights on
naturalistic datasets and with relaxed assumptions. Taken together, our work
gives rise to general hypotheses relating neural architecture to learning and
provides a mathematical approach towards understanding the design of more
complex architectures and the role of modularity and compositionality in
solving real-world problems. The code and results are available at
https://www.saxelab.org/gated-dln ."
Learning Visual Explanations for DCNN-Based Image Classifiers Using an Attention Mechanism,0.0101659,"In this paper two new learning-based eXplainable AI (XAI) methods for deep
convolutional neural network (DCNN) image classifiers, called L-CAM-Fm and
L-CAM-Img, are proposed. Both methods use an attention mechanism that is
inserted in the original (frozen) DCNN and is trained to derive class
activation maps (CAMs) from the last convolutional layer's feature maps. During
training, CAMs are applied to the feature maps (L-CAM-Fm) or the input image
(L-CAM-Img) forcing the attention mechanism to learn the image regions
explaining the DCNN's outcome. Experimental evaluation on ImageNet shows that
the proposed methods achieve competitive results while requiring a single
forward pass at the inference stage. Moreover, based on the derived
explanations a comprehensive qualitative analysis is performed providing
valuable insight for understanding the reasons behind classification errors,
including possible dataset biases affecting the trained classifier."
A Deep-Discrete Learning Framework for Spherical Surface Registration,0.0584814,"Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology."
Dynamic Point Cloud Compression with Cross-Sectional Approach,0.063056,"The recent development of dynamic point clouds has introduced the possibility
of mimicking natural reality, and greatly assisting quality of life. However,
to broadcast successfully, the dynamic point clouds require higher compression
due to their huge volume of data compared to the traditional video. Recently,
MPEG finalized a Video-based Point Cloud Compression standard known as V-PCC.
However, V-PCC requires huge computational time due to expensive normal
calculation and segmentation, sacrifices some points to limit the number of 2D
patches, and cannot occupy all spaces in the 2D frame. The proposed method
addresses these limitations by using a novel cross-sectional approach. This
approach reduces expensive normal estimation and segmentation, retains more
points, and utilizes more spaces for 2D frame generation compared to the VPCC.
The experimental results using standard video sequences show that the proposed
technique can achieve better compression in both geometric and texture data
compared to the V-PCC standard."
In-Vehicle Interface Adaptation to Environment-Induced Cognitive Workload,0.0412657,"Many car accidents are caused by human distractions, including cognitive
distractions. In-vehicle human-machine interfaces (HMIs) have evolved
throughout the years, providing more and more functions. Interaction with the
HMIs can, however, also lead to further distractions and, as a consequence,
accidents. To tackle this problem, we propose using adaptive HMIs that change
according to the mental workload of the driver. In this work, we present the
current status as well as preliminary results of a user study using
naturalistic secondary tasks while driving (i.e., the primary task) that
attempt to understand the effects of one such interface."
P$^3$LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training,0.0108149,"Conventional autoregressive left-to-right (L2R) sequence generation faces two
issues during decoding: limited to unidirectional target sequence modeling, and
constrained on strong local dependencies. To address the aforementioned
problem, we propose P$^3$LM, a probabilistically permuted prophet language
model, which strengthens the modeling of bidirectional information and long
token dependencies for sequence generation. Specifically, P$^3$LM learns to
generate tokens in permuted order upon an order-aware transformer decoder, as
well as to generate the corresponding future $N$ tokens with a multi-stream
attention mechanism. Extensive experiments are conducted on the GLGE benchmark,
which includes four datasets for summarization, two for question generation,
one for conversational question answering, and one for dialog response
generation, where P$^3$LM achieves state-of-the-art results compared with
strong publicly available generative pre-training methods."
Autoregressive GAN for Semantic Unconditional Head Motion Generation,0.00525344,"In this work, we address the task of unconditional head motion generation to
animate still human faces in a low-dimensional semantic space from a single
reference pose. Different from traditional audio-conditioned talking head
generation that seldom puts emphasis on realistic head motions, we devise a
GAN-based architecture that learns to synthesize rich head motion sequences
over long duration while maintaining low error accumulation levels.In
particular, the autoregressive generation of incremental outputs ensures smooth
trajectories, while a multi-scale discriminator on input pairs drives
generation toward better handling of high- and low-frequency signals and less
mode collapse.We experimentally demonstrate the relevance of the proposed
method and show its superiority compared to models that attained
state-of-the-art performances on similar tasks."
Recognition of Implicit Geographic Movement in Text,0.0282515,"Analyzing the geographic movement of humans, animals, and other phenomena is
a growing field of research. This research has benefited urban planning,
logistics, animal migration understanding, and much more. Typically, the
movement is captured as precise geographic coordinates and time stamps with
Global Positioning Systems (GPS). Although some research uses computational
techniques to take advantage of implicit movement in descriptions of route
directions, hiking paths, and historical exploration routes, innovation would
accelerate with a large and diverse corpus. We created a corpus of sentences
labeled as describing geographic movement or not and including the type of
entity moving. Creating this corpus proved difficult without any comparable
corpora to start with, high human labeling costs, and since movement can at
times be interpreted differently. To overcome these challenges, we developed an
iterative process employing hand labeling, crowd voting for confirmation, and
machine learning to predict more labels. By merging advances in word embeddings
with traditional machine learning models and model ensembling, prediction
accuracy is at an acceptable level to produce a large silver-standard corpus
despite the small gold-standard corpus training set. Our corpus will likely
benefit computational processing of geography in text and spatial cognition, in
addition to detection of movement."
Multi-level Latent Space Structuring for Generative Control,0.0072254,"Truncation is widely used in generative models for improving the quality of
the generated samples, at the expense of reducing their diversity. We propose
to leverage the StyleGAN generative architecture to devise a new truncation
technique, based on a decomposition of the latent space into clusters, enabling
customized truncation to be performed at multiple semantic levels. We do so by
learning to re-generate W-space, the extended intermediate latent space of
StyleGAN, using a learnable mixture of Gaussians, while simultaneously training
a classifier to identify, for each latent vector, the cluster that it belongs
to. The resulting truncation scheme is more faithful to the original
untruncated samples and allows a better trade-off between quality and
diversity. We compare our method to other truncation approaches for StyleGAN,
both qualitatively and quantitatively."
"Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks",0.0301609,"In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs, namely
word-consensus-networks, allows the SLU system to improve in comparison to the
1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,
learning from acoustic and text embeddings, obtains performance similar to the
oracle setup, a relative improvement of 17.8% over the 1-best configuration,
being a recommended alternative to overcome the limitations of working with
automatically generated transcripts."
Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability,0.0415954,"Great endeavors have been made to study AI's ability in abstract reasoning,
along with which different versions of RAVEN's progressive matrices (RPM) are
proposed as benchmarks. Previous works give inkling that without sophisticated
design or extra meta-data containing semantic information, neural networks may
still be indecisive in making decisions regarding RPM problems, after
relentless training. Evidenced by thorough experiments and ablation studies, we
showcase that end-to-end neural networks embodied with felicitous inductive
bias, intentionally design or serendipitously match, can solve RPM problems
elegantly, without the augment of any extra meta-data or preferences of any
specific backbone. Our work also reveals that multi-viewpoint with
multi-evaluation is a key learning strategy for successful reasoning. Finally,
potential explanations for the failure of connectionist models in
generalization are provided. We hope that these results will serve as
inspections of AI's ability beyond perception and toward abstract reasoning.
Source code can be found in https://github.com/QinglaiWeiCASIA/RavenSolver."
Incorporating Multi-armed Bandit with Local Search for MaxSAT,0.0120932,"Partial MaxSAT (PMS) and Weighted PMS (WPMS) are two practical
generalizations of the MaxSAT problem. In this paper, we propose a local search
algorithm for these problems, called BandHS, which applies two multi-armed
bandits to guide the search directions when escaping local optima. One bandit
is combined with all the soft clauses to help the algorithm select to satisfy
appropriate soft clauses, and the other bandit with all the literals in hard
clauses to help the algorithm select appropriate literals to satisfy the hard
clauses. These two bandits can improve the algorithm's search ability in both
feasible and infeasible solution spaces. We further propose an initialization
method for (W)PMS that prioritizes both unit and binary clauses when producing
the initial solutions. Extensive experiments demonstrate the excellent
performance and generalization capability of our proposed methods, that greatly
boost the state-of-the-art local search algorithm, SATLike3.0, and the
state-of-the-art SAT-based incomplete solver, NuWLS-c."
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction,0.0315331,"Relation extraction (RE), which has relied on structurally annotated corpora
for model training, has been particularly challenging in low-resource scenarios
and domains. Recent literature has tackled low-resource RE by self-supervised
learning, where the solution involves pretraining the entity pair embedding by
RE-based objective and finetuning on labeled data by classification-based
objective. However, a critical challenge to this approach is the gap in
objectives, which prevents the RE model from fully utilizing the knowledge in
pretrained representations. In this paper, we aim at bridging the gap and
propose to pretrain and finetune the RE model using consistent objectives of
contrastive learning. Since in this kind of representation learning paradigm,
one relation may easily form multiple clusters in the representation space, we
further propose a multi-center contrastive loss that allows one relation to
form multiple clusters to better align with pretraining. Experiments on two
document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness
of our method. Particularly, when using 1% end-task training data, our method
outperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets,
respectively."
Winning the CVPR'2022 AQTC Challenge: A Two-stage Function-centric Approach,0.0251057,"Affordance-centric Question-driven Task Completion for Egocentric
Assistant(AQTC) is a novel task which helps AI assistant learn from
instructional videos and scripts and guide the user step-by-step. In this
paper, we deal with the AQTC via a two-stage Function-centric approach, which
consists of Question2Function Module to ground the question with the related
function and Function2Answer Module to predict the action based on the
historical steps. We evaluated several possible solutions in each module and
obtained significant gains compared to the given baselines. Our code is
available at \url{https://github.com/starsholic/LOVEU-CVPR22-AQTC}."
"""Covid vaccine is against Covid but Oxford vaccine is made at Oxford!"" Semantic Interpretation of Proper Noun Compounds",0.0241587,"Proper noun compounds, e.g., ""Covid vaccine"", convey information in a
succinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against the
Covid disease""). These are commonly used in short-form domains, such as news
headlines, but are largely ignored in information-seeking applications. To
address this limitation, we release a new manually annotated dataset, ProNCI,
consisting of 22.5K proper noun compounds along with their free-form semantic
interpretations. ProNCI is 60 times larger than prior noun compound datasets
and also includes non-compositional examples, which have not been previously
explored. We experiment with various neural models for automatically generating
the semantic interpretations from proper noun compounds, ranging from few-shot
prompting to supervised learning, with varying degrees of knowledge about the
constituent nouns. We find that adding targeted knowledge, particularly about
the common noun, results in performance gains of upto 2.8%. Finally, we
integrate our model generated interpretations with an existing Open IE system
and observe an 7.5% increase in yield at a precision of 85%. The dataset and
code are available at https://github.com/dair-iitd/pronci."
Bias-Scalable Near-Memory CMOS Analog Processor for Machine Learning,0.0356846,"Bias-scalable analog computing is attractive for implementing machine
learning (ML) processors with distinct power-performance specifications. For
instance, ML implementations for server workloads are focused on higher
computational throughput for faster training, whereas ML implementations for
edge devices are focused on energy-efficient inference. In this paper, we
demonstrate the implementation of bias-scalable approximate analog computing
circuits using the generalization of the margin-propagation principle called
shape-based analog computing (S-AC). The resulting S-AC core integrates several
near-memory compute elements, which include: (a) non-linear activation
functions; (b) inner-product compute circuits; and (c) a mixed-signal
compressive memory, all of which can be scaled for performance or power while
preserving its functionality. Using measured results from prototypes fabricated
in a 180nm CMOS process, we demonstrate that the performance of computing
modules remains robust to transistor biasing and variations in temperature. In
this paper, we also demonstrate the effect of bias-scalability and
computational accuracy on a simple ML regression task."
Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages,0.0553044,"Translation Quality Estimation (QE) is the task of predicting the quality of
machine translation (MT) output without any reference. This task has gained
increasing attention as an important component in the practical applications of
MT. In this paper, we first propose XLMRScore, which is a cross-lingual
counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric
can be used as a simple unsupervised QE method, nevertheless facing two issues:
firstly, the untranslated tokens leading to unexpectedly high translation
scores, and secondly, the issue of mismatching errors between source and
hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate
these issues, we suggest replacing untranslated words with the unknown token
and the cross-lingual alignment of the pre-trained model to represent aligned
words closer to each other, respectively. We evaluate the proposed method on
four low-resource language pairs of the WMT21 QE shared task, as well as a new
English$\rightarrow$Persian (En-Fa) test dataset introduced in this paper.
Experiments show that our method could get comparable results with the
supervised baseline for two zero-shot scenarios, i.e., with less than 0.01
difference in Pearson correlation, while outperforming unsupervised rivals in
all the low-resource language pairs for above 8%, on average."
Socially Intelligent Genetic Agents for the Emergence of Explicit Norms,0.0441994,"Norms help regulate a society. Norms may be explicit (represented in
structured form) or implicit. We address the emergence of explicit norms by
developing agents who provide and reason about explanations for norm violations
in deciding sanctions and identifying alternative norms. These agents use a
genetic algorithm to produce norms and reinforcement learning to learn the
values of these norms. We find that applying explanations leads to norms that
provide better cohesion and goal satisfaction for the agents. Our results are
stable for societies with differing attitudes of generosity."
Visual Information Guided Zero-Shot Paraphrase Generation,0.0211587,"Zero-shot paraphrase generation has drawn much attention as the large-scale
high-quality paraphrase corpus is limited. Back-translation, also known as the
pivot-based method, is typical to this end. Several works leverage different
information as ""pivot"" such as language, semantic representation and so on. In
this paper, we explore using visual information such as image as the ""pivot"" of
back-translation. Different with the pipeline back-translation method, we
propose visual information guided zero-shot paraphrase generation (ViPG) based
only on paired image-caption data. It jointly trains an image captioning model
and a paraphrasing model and leverage the image captioning model to guide the
training of the paraphrasing model. Both automatic evaluation and human
evaluation show our model can generate paraphrase with good relevancy, fluency
and diversity, and image is a promising kind of pivot for zero-shot paraphrase
generation."
RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments,0.0210986,"As a fundamental task for intelligent robots, visual SLAM has made great
progress over the past decades. However, robust SLAM under highly weak-textured
environments still remains very challenging. In this paper, we propose a novel
visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR
network which is able to produce dense point matching under low-textured scenes
to generate feature descriptors. To integrate the new features into the popular
ORB-SLAM framework, we develop feature masks to filter out the unreliable
features and employ KNN strategy to strengthen the matching robustness. We also
retrained visual vocabulary upon new descriptors for efficient loop closing.
The resulting RWT-SLAM is tested in various public datasets such as TUM and
OpenLORIS, as well as our own data. The results shows very promising
performance under highly weak-textured environments."
Analytical Solutions for the Inverse Problem within Gradual Semantics,0.0492012,"Gradual semantics within abstract argumentation associate a numeric score
with every argument in a system, which represents the level of acceptability of
this argument, and from which a preference ordering over arguments can be
derived. While some semantics operate over standard argumentation frameworks,
many utilise a weighted framework, where a numeric initial weight is associated
with each argument. Recent work has examined the inverse problem within gradual
semantics. Rather than determining a preference ordering given an argumentation
framework and a semantics, the inverse problem takes an argumentation
framework, a gradual semantics, and a preference ordering as inputs, and
identifies what weights are needed to over arguments in the framework to obtain
the desired preference ordering. Existing work has attacked the inverse problem
numerically, using a root finding algorithm (the bisection method) to identify
appropriate initial weights. In this paper we demonstrate that for a class of
gradual semantics, an analytical approach can be used to solve the inverse
problem. Unlike the current state-of-the-art, such an analytic approach can
rapidly find a solution, and is guaranteed to do so. In obtaining this result,
we are able to prove several important properties which previous work had posed
as conjectures."
Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals,0.05609,"Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps."
Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding,0.0328939,"Pre-trained language models (PLM) have demonstrated their effectiveness for a
broad range of information retrieval and natural language processing tasks. As
the core part of PLM, multi-head self-attention is appealing for its ability to
jointly attend to information from different positions. However, researchers
have found that PLM always exhibits fixed attention patterns regardless of the
input (e.g., excessively paying attention to [CLS] or [SEP]), which we argue
might neglect important information in the other positions. In this work, we
propose a simple yet effective attention guiding mechanism to improve the
performance of PLM by encouraging attention towards the established goals.
Specifically, we propose two kinds of attention guiding methods, i.e., map
discrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).
The former definitely encourages the diversity among multiple self-attention
heads to jointly attend to information from different representation subspaces,
while the latter encourages self-attention to attend to as many different
positions of the input as possible. We conduct experiments with multiple
general pre-trained models (i.e., BERT, ALBERT, and Roberta) and
domain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and
SciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and
Cross-genre-IR). Extensive experimental results demonstrate that our proposed
MDG and PDG bring stable performance improvements on all datasets with high
efficiency and low cost."
A two-step approach to leverage contextual data: speech recognition in air-traffic communications,0.0402931,"Automatic Speech Recognition (ASR), as the assistance of speech communication
between pilots and air-traffic controllers, can significantly reduce the
complexity of the task and increase the reliability of transmitted information.
ASR application can lead to a lower number of incidents caused by
misunderstanding and improve air traffic management (ATM) efficiency.
Evidently, high accuracy predictions, especially, of key information, i.e.,
callsigns and commands, are required to minimize the risk of errors. We prove
that combining the benefits of ASR and Natural Language Processing (NLP)
methods to make use of surveillance data (i.e. additional modality) helps to
considerably improve the recognition of callsigns (named entity). In this
paper, we investigate a two-step callsign boosting approach: (1) at the 1 step
(ASR), weights of probable callsign n-grams are reduced in G.fst and/or in the
decoding FST (lattices), (2) at the 2 step (NLP), callsigns extracted from the
improved recognition outputs with Named Entity Recognition (NER) are correlated
with the surveillance data to select the most suitable one. Boosting callsign
n-grams with the combination of ASR and NLP methods eventually leads up to
53.7% of an absolute, or 60.4% of a relative, improvement in callsign
recognition."
Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance,0.0812181,"This paper presents a novel end-to-end method for the problem of
skeleton-based unsupervised human action recognition. We propose a new
architecture with a convolutional autoencoder that uses graph Laplacian
regularization to model the skeletal geometry across the temporal dynamics of
actions. Our approach is robust towards viewpoint variations by including a
self-supervised gradient reverse layer that ensures generalization across
camera views. The proposed method is validated on NTU-60 and NTU-120
large-scale datasets in which it outperforms all prior unsupervised
skeleton-based approaches on the cross-subject, cross-view, and cross-setup
protocols. Although unsupervised, our learnable representation allows our
method even to surpass a few supervised skeleton-based action recognition
methods. The code is available in:
www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian"
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,0.044083,"Expressing natural language descriptions of structured facts or relations --
data-to-text generation (D2T) -- increases the accessibility of structured
knowledge repositories. Previous work shows that pre-trained language
models(PLMs) perform remarkably well on this task after fine-tuning on a
significant amount of task-specific training data. On the other hand, while
auto-regressive PLMs can generalize from a few task examples, their efficacy at
D2T is largely unexplored. Furthermore, we have an incomplete understanding of
the limits of PLMs on D2T.
  In this work, we conduct an empirical study of both fine-tuned and
auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their
performance as a function of the amount of task-specific data and how these
data are incorporated into the models: zero and few-shot learning, and
fine-tuning of model weights. In addition, we probe the limits of PLMs by
measuring performance on subsets of the evaluation data: novel predicates and
abstractive test examples. To improve the performance on these subsets, we
investigate two techniques: providing predicate descriptions in the context and
re-ranking generated candidates by information reflected in the source.
Finally, we conduct a human evaluation of model errors and show that D2T
generation tasks would benefit from datasets with more careful manual curation."
Self-directed Learning of Action Models using Exploratory Planning,0.0171364,"Complex, real-world domains may not be fully modeled for an agent, especially
if the agent has never operated in the domain before. The agent's ability to
effectively plan and act in such a domain is influenced by its knowledge of
when it can perform specific actions and the effects of those actions. We
describe a novel exploratory planning agent that is capable of learning action
preconditions and effects without expert traces or a given goal. The agent's
architecture allows it to perform both exploratory actions as well as
goal-directed actions, which opens up important considerations for how
exploratory planning and goal planning should be controlled, as well as how the
agent's behavior should be explained to any teammates it may have. The
contributions of this work include a new representation for contexts called
Lifted Linked Clauses, a novel exploration action selection approach using
these clauses, an exploration planner that uses lifted linked clauses as goals
in order to reach new states, and an empirical evaluation in a scenario from an
exploration-focused video game demonstrating that lifted linked clauses improve
exploration and action model learning against non-planning baseline agents."
WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition,0.00437056,"Named Entity Recognition task is one of the core tasks of information
extraction. Word ambiguity and word abbreviation are important reasons for the
low recognition rate of named entities. In this paper, we propose a novel named
entity recognition model WCL-BBCD (Word Contrastive Learning with
BERT-BiLSTM-CRF-DBpedia), which incorporates the idea of contrastive learning.
The model first trains the sentence pairs in the text, calculate similarity
between sentence pairs, and fine-tunes BERT used for the named entity
recognition task according to the similarity, so as to alleviate word
ambiguity. Then, the fine-tuned BERT is combined with BiLSTM-CRF to perform the
named entity recognition task. Finally, the recognition results are corrected
in combination with prior knowledge such as knowledge graphs, so as to
alleviate the low-recognition-rate problem caused by word abbreviations. The
results of experimentals conducted on the CoNLL-2003 English dataset and
OntoNotes V5 English dataset show that our model outperforms other similar
models on."
Classification Of Fake News Headline Based On Neural Networks,0.0449038,"Over the last few years, Text classification is one of the fundamental tasks
in natural language processing (NLP) in which the objective is to categorize
text documents into one of the predefined classes. The news is full of our
life. Therefore, news headlines classification is a crucial task to connect
users with the right news. The news headline classification is a kind of text
classification, which can be generally divided into three mainly parts: feature
extraction, classifier selection, and evaluations. In this article, we use the
dataset, containing news over a period of eighteen years provided by Kaggle
platform to classify news headlines. We choose TF-IDF to extract features and
neural network as the classifier, while the evaluation metrics is accuracy.
From the experiment result, it is obvious that our NN model has the best
performance among these models in the metrics of accuracy. The higher the
accuracy is, the better performance the model will gain. Our NN model owns the
accuracy 0.8622, which is highest accuracy among these four models. And it is
0.0134, 0.033, 0.080 higher than its of other models."
Multi-Spectral Image Classification with Ultra-Lean Complex-Valued Models,0.0410324,"Multi-spectral imagery is invaluable for remote sensing due to different
spectral signatures exhibited by materials that often appear identical in
greyscale and RGB imagery. Paired with modern deep learning methods, this
modality has great potential utility in a variety of remote sensing
applications, such as humanitarian assistance and disaster recovery efforts.
State-of-the-art deep learning methods have greatly benefited from large-scale
annotations like in ImageNet, but existing MSI image datasets lack annotations
at a similar scale. As an alternative to transfer learning on such data with
few annotations, we apply complex-valued co-domain symmetric models to classify
real-valued MSI images. Our experiments on 8-band xView data show that our
ultra-lean model trained on xView from scratch without data augmentations can
outperform ResNet with data augmentation and modified transfer learning on
xView. Our work is the first to demonstrate the value of complex-valued deep
learning on real-valued MSI data."
Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain,0.0512436,"As an essential task for the architecture, engineering, and construction
(AEC) industry, information retrieval (IR) from unstructured textual data based
on natural language processing (NLP) is gaining increasing attention. Although
various deep learning (DL) models for IR tasks have been investigated in the
AEC domain, it is still unclear how domain corpora and domain-specific
pretrained DL models can improve performance in various IR tasks. To this end,
this work systematically explores the impacts of domain corpora and various
transfer learning techniques on the performance of DL models for IR tasks and
proposes a pretrained domain-specific language model for the AEC domain. First,
both in-domain and close-domain corpora are developed. Then, two types of
pretrained models, including traditional wording embedding models and
BERT-based models, are pretrained based on various domain corpora and transfer
learning strategies. Finally, several widely used DL models for IR tasks are
further trained and tested based on various configurations and pretrained
models. The result shows that domain corpora have opposite effects on
traditional word embedding models for text classification and named entity
recognition tasks but can further improve the performance of BERT-based models
in all tasks. Meanwhile, BERT-based models dramatically outperform traditional
methods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1
score, respectively. This research contributes to the body of knowledge in two
ways: 1) demonstrating the advantages of domain corpora and pretrained DL
models and 2) opening the first domain-specific dataset and pretrained language
model for the AEC domain, to the best of our knowledge. Thus, this work sheds
light on the adoption and application of pretrained models in the AEC domain."
On the Paradox of Learning to Reason from Data,0.0416266,"Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be
trained end-to-end to solve logical reasoning problems presented in natural
language? We attempt to answer this question in a confined problem space where
there exists a set of parameters that perfectly simulates logical reasoning. We
make observations that seem to contradict each other: BERT attains near-perfect
accuracy on in-distribution test examples while failing to generalize to other
data distributions over the exact same problem space. Our study provides an
explanation for this paradox: instead of learning to emulate the correct
reasoning function, BERT has in fact learned statistical features that
inherently exist in logical reasoning problems. We also show that it is
infeasible to jointly remove statistical features from data, illustrating the
difficulty of learning to reason in general. Our result naturally extends to
other neural models and unveils the fundamental difference between learning to
reason and learning to achieve high performance on NLP benchmarks using
statistical features."
Ad Hoc Teamwork in the Presence of Adversaries,0.0384489,"Advances in ad hoc teamwork have the potential to create agents that
collaborate robustly in real-world applications. Agents deployed in the real
world, however, are vulnerable to adversaries with the intent to subvert them.
There has been little research in ad hoc teamwork that assumes the presence of
adversaries. We explain the importance of extending ad hoc teamwork to include
the presence of adversaries and clarify why this problem is difficult. We then
propose some directions for new research opportunities in ad hoc teamwork that
leads to more robust multi-agent cyber-physical infrastructure systems."
Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot Segmentation,0.0545921,"Few-shot learning allows machines to classify novel classes using only a few
labeled samples. Recently, few-shot segmentation aiming at semantic
segmentation on low sample data has also seen great interest. In this paper, we
propose a learnable module that can be placed on top of existing segmentation
networks for performing few-shot segmentation. This module, called the
task-adaptive feature transformer (TAFT), linearly transforms task-specific
high-level features to a set of task agnostic features well-suited to
conducting few-shot segmentation. The task-conditioned feature transformation
allows an effective utilization of the semantic information in novel classes to
generate tight segmentation masks. We also propose a semantic enrichment (SE)
module that utilizes a pixel-wise attention module for high-level feature and
an auxiliary loss from an auxiliary segmentation network conducting the
semantic segmentation for all training classes. Experiments on PASCAL-$5^i$ and
COCO-$20^i$ datasets confirm that the added modules successfully extend the
capability of existing segmentators to yield highly competitive few-shot
segmentation performances."
Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective,0.00906834,"One of the most fundamental questions in quantitative finance is the
existence of continuous-time diffusion models that fit market prices of a given
set of options. Traditionally, one employs a mix of intuition, theoretical and
empirical analysis to find models that achieve exact or approximate fits. Our
contribution is to show how a suitable game theoretical formulation of this
problem can help solve this question by leveraging existing developments in
modern deep multi-agent reinforcement learning to search in the space of
stochastic processes. Our experiments show that we are able to learn local
volatility, as well as path-dependence required in the volatility process to
minimize the price of a Bermudan option. Our algorithm can be seen as a
particle method \textit{\`{a} la} Guyon \textit{et} Henry-Labordere where
particles, instead of being designed to ensure $\sigma_{loc}(t,S_t)^2 =
\mathbb{E}[\sigma_t^2|S_t]$, are learning RL-driven agents cooperating towards
more general calibration targets."
First do not fall: learning to exploit a wall with a damaged humanoid robot,0.0219169,"Humanoid robots could replace humans in hazardous situations but most of such
situations are equally dangerous for them, which means that they have a high
chance of being damaged and falling. We hypothesize that humanoid robots would
be mostly used in buildings, which makes them likely to be close to a wall. To
avoid a fall, they can therefore lean on the closest wall, as a human would do,
provided that they find in a few milliseconds where to put the hand(s). This
article introduces a method, called D-Reflex, that learns a neural network that
chooses this contact position given the wall orientation, the wall distance,
and the posture of the robot. This contact position is then used by a
whole-body controller to reach a stable posture. We show that D-Reflex allows a
simulated TALOS robot (1.75m, 100kg, 30 degrees of freedom) to avoid more than
75% of the avoidable falls and can work on the real robot."
State Dropout-Based Curriculum Reinforcement Learning for Self-Driving at Unsignalized Intersections,0.0253503,"Traversing intersections is a challenging problem for autonomous vehicles,
especially when the intersections do not have traffic control. Recently deep
reinforcement learning has received massive attention due to its success in
dealing with autonomous driving tasks. In this work, we address the problem of
traversing unsignalized intersections using a novel curriculum for deep
reinforcement learning. The proposed curriculum leads to: 1) A faster training
process for the reinforcement learning agent, and 2) Better performance
compared to an agent trained without curriculum. Our main contribution is
two-fold: 1) Presenting a unique curriculum for training deep reinforcement
learning agents, and 2) showing the application of the proposed curriculum for
the unsignalized intersection traversal task. The framework expects processed
observations of the surroundings from the perception system of the autonomous
vehicle. We test our method in the CommonRoad motion planning simulator on
T-intersections and four-way intersections."
Who is we? Disambiguating the referents of first person plural pronouns in parliamentary debates,0.0363318,"This paper investigates the use of first person plural pronouns as a
rhetorical device in political speeches. We present an annotation schema for
disambiguating pronoun references and use our schema to create an annotated
corpus of debates from the German Bundestag. We then use our corpus to learn to
automatically resolve pronoun referents in parliamentary debates. We explore
the use of data augmentation with weak supervision to further expand our corpus
and report preliminary results."
Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems,0.0232073,"Automation of dialogue system evaluation is a driving force for the efficient
development of dialogue systems. This paper introduces the bipartite-play
method, a dialogue collection method for automating dialogue system evaluation.
It addresses the limitations of existing dialogue collection methods: (i)
inability to compare with systems that are not publicly available, and (ii)
vulnerability to cheating by intentionally selecting systems to be compared.
Experimental results show that the automatic evaluation using the
bipartite-play method mitigates these two drawbacks and correlates as strongly
with human subjectivity as existing methods."
Classifiers are Better Experts for Controllable Text Generation,0.00179473,"This paper proposes a simple method for controllable text generation based on
weighting logits with a free-form classifier, namely CAIF sampling. Using an
arbitrary text classifier, we adjust a small part of a language model's logits
and guide text generation towards or away from classifier prediction. We
experimented with toxicity avoidance and sentiment control tasks and showed
that the proposed method significantly outperforms recent PPLM, GeDi, and
DExperts on PPL and task accuracy metrics based on the external classifier of
generated texts. In addition, compared to other approaches, it is easier to
implement and tune and has significantly fewer restrictions and requirements."
Descartes: Generating Short Descriptions of Wikipedia Articles,0.00421647,"Wikipedia is one of the richest knowledge sources on the Web today. In order
to facilitate navigating, searching, and maintaining its content, Wikipedia's
guidelines state that all articles should be annotated with a so-called short
description indicating the article's topic (e.g., the short description of beer
is ""Alcoholic drink made from fermented cereal grains""). Nonetheless, a large
fraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no
short description yet, with detrimental effects for millions of Wikipedia
users. Motivated by this problem, we introduce the novel task of automatically
generating short descriptions for Wikipedia articles and propose Descartes, a
multilingual model for tackling it. Descartes integrates three sources of
information to generate an article description in a target language: the text
of the article in all its language versions, the already-existing descriptions
(if any) of the article in other languages, and semantic type information
obtained from a knowledge graph. We evaluate a Descartes model trained for
handling 25 languages simultaneously, showing that it beats baselines
(including a strong translation-based baseline) and performs on par with
monolingual models tailored for specific languages. A human evaluation on three
languages further shows that the quality of Descartes's descriptions is largely
indistinguishable from that of human-written descriptions; e.g., 91.3% of our
English descriptions (vs. 92.1% of human-written descriptions) pass the bar for
inclusion in Wikipedia, suggesting that Descartes is ready for production, with
the potential to support human editors in filling a major gap in today's
Wikipedia across languages."
kpfriends at SemEval-2022 Task 2: NEAMER -- Named Entity Augmented Multi-word Expression Recognizer,0.0952839,"We present NEAMER -- Named Entity Augmented Multi-word Expression Recognizer.
This system is inspired by non-compositionality characteristics shared between
Named Entity and Idiomatic Expressions. We utilize transfer learning and
locality features to enhance idiom classification task. This system is our
submission for SemEval Task 2: Multilingual Idiomaticity Detection and Sentence
Embedding Subtask A OneShot shared task. We achieve SOTA with F1 0.9395 during
post-evaluation phase. We also observe improvement in training stability.
Lastly, we experiment with non-compositionality knowledge transfer,
cross-lingual fine-tuning and locality features, which we also introduce in
this paper."
Far Away in the Deep Space: Dense Nearest-Neighbor-Based Out-of-Distribution Detection,0.0294703,"The key to out-of-distribution detection is density estimation of the
in-distribution data or of its feature representations. This is particularly
challenging for dense anomaly detection in domains where the in-distribution
data has a complex underlying structure. Nearest-Neighbors approaches have been
shown to work well in object-centric data domains, such as industrial
inspection and image classification. In this paper, we show that
nearest-neighbor approaches also yield state-of-the-art results on dense
novelty detection in complex driving scenes when working with an appropriate
feature representation. In particular, we find that transformer-based
architectures produce representations that yield much better similarity metrics
for the task. We identify the multi-head structure of these models as one of
the reasons, and demonstrate a way to transfer some of the improvements to
CNNs. Ultimately, the approach is simple and non-invasive, i.e., it does not
affect the primary segmentation performance, refrains from training on examples
of anomalies, and achieves state-of-the-art results on RoadAnomaly,
StreetHazards, and SegmentMeIfYouCan-Anomaly."
A Comparative Attention Framework for Better Few-Shot Object Detection on Aerial Images,0.0177603,"Few-Shot Object Detection (FSOD) methods are mainly designed and evaluated on
natural image datasets such as Pascal VOC and MS COCO. However, it is not clear
whether the best methods for natural images are also the best for aerial
images. Furthermore, direct comparison of performance between FSOD methods is
difficult due to the wide variety of detection frameworks and training
strategies. Therefore, we propose a benchmarking framework that provides a
flexible environment to implement and compare attention-based FSOD methods. The
proposed framework focuses on attention mechanisms and is divided into three
modules: spatial alignment, global attention, and fusion layer. To remain
competitive with existing methods, which often leverage complex training, we
propose new augmentation techniques designed for object detection. Using this
framework, several FSOD methods are reimplemented and compared. This comparison
highlights two distinct performance regimes on aerial and natural images: FSOD
performs worse on aerial images. Our experiments suggest that small objects,
which are harder to detect in the few-shot setting, account for the poor
performance. Finally, we develop a novel multiscale alignment method,
Cross-Scales Query-Support Alignment (XQSA) for FSOD, to improve the detection
of small objects. XQSA outperforms the state-of-the-art significantly on DOTA
and DIOR."
Domain-General Crowd Counting in Unseen Scenarios,0.048108,"Domain shift across crowd data severely hinders crowd counting models to
generalize to unseen scenarios. Although domain adaptive crowd counting
approaches close this gap to a certain extent, they are still dependent on the
target domain data to adapt (e.g. finetune) their models to the specific
domain. In this paper, we aim to train a model based on a single source domain
which can generalize well on any unseen domain. This falls into the realm of
domain generalization that remains unexplored in crowd counting. We first
introduce a dynamic sub-domain division scheme which divides the source domain
into multiple sub-domains such that we can initiate a meta-learning framework
for domain generalization. The sub-domain division is dynamically refined
during the meta-learning. Next, in order to disentangle domain-invariant
information from domain-specific information in image features, we design the
domain-invariant and -specific crowd memory modules to re-encode image
features. Two types of losses, i.e. feature reconstruction and orthogonal
losses, are devised to enable this disentanglement. Extensive experiments on
several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show
the strong generalizability of our method."
Moving Other Way: Exploring Word Mover Distance Extensions,0.00991219,"The word mover's distance (WMD) is a popular semantic similarity metric for
two texts. This position paper studies several possible extensions of WMD. We
experiment with the frequency of words in the corpus as a weighting factor and
the geometry of the word vector space. We validate possible extensions of WMD
on six document classification datasets. Some proposed extensions show better
results in terms of the k-nearest neighbor classification error than WMD."
Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution,0.0413059,"To achieve promising results on blind image super-resolution (SR), some
attempts leveraged the low resolution (LR) images to predict the kernel and
improve the SR performance. However, these Supervised Kernel Prediction (SKP)
methods are impractical due to the unavailable real-world blur kernels.
Although some Unsupervised Degradation Prediction (UDP) methods are proposed to
bypass this problem, the \textit{inconsistency} between degradation embedding
and SR feature is still challenging. By exploring the correlations between
degradation embedding and SR feature, we observe that jointly learning the
content and degradation aware feature is optimal. Based on this observation, a
Content and Degradation aware SR Network dubbed CDSR is proposed. Specifically,
CDSR contains three newly-established modules: (1) a Lightweight Patch-based
Encoder (LPE) is applied to jointly extract content and degradation features;
(2) a Domain Query Attention based module (DQA) is employed to adaptively
reduce the inconsistency; (3) a Codebook-based Space Compress module (CSC) that
can suppress the redundant information. Extensive experiments on several
benchmarks demonstrate that the proposed CDSR outperforms the existing UDP
models and achieves competitive performance on PSNR and SSIM even compared with
the state-of-the-art SKP methods."
Training-Free Robust Multimodal Learning via Sample-Wise Jacobian Regularization,0.00838028,"Multimodal fusion emerges as an appealing technique to improve model
performances on many tasks. Nevertheless, the robustness of such fusion methods
is rarely involved in the present literature. In this paper, we propose a
training-free robust late-fusion method by exploiting conditional independence
assumption and Jacobian regularization. Our key is to minimize the Frobenius
norm of a Jacobian matrix, where the resulting optimization problem is relaxed
to a tractable Sylvester equation. Furthermore, we provide a theoretical error
bound of our method and some insights about the function of the extra modality.
Several numerical experiments on AV-MNIST, RAVDESS, and VGGsound demonstrate
the efficacy of our method under both adversarial attacks and random
corruptions."
Task-specific Compression for Multi-task Language Models using Attribution-based Pruning,0.022262,"Multi-task language models show outstanding performance for various natural
language understanding tasks with only a single model. However, these language
models utilize an unnecessarily large number of model parameters, even when
used only for a specific task. This paper proposes a novel training-free
compression method for multi-task language models using a pruning method.
Specifically, we use an attribution method to determine which neurons are
essential for performing a specific task. We task-specifically prune
unimportant neurons and leave only task-specific parameters. Furthermore, we
extend our method to be applicable in low-resource and unsupervised settings.
Since our compression method is training-free, it uses few computing resources
and does not destroy the pre-trained knowledge of language models. Experimental
results on the six widely-used datasets show that our proposed pruning method
significantly outperforms baseline pruning methods. In addition, we demonstrate
that our method preserves performance even in an unseen domain setting."
Learning Explicit Object-Centric Representations with Vision Transformers,0.0137459,"With the recent successful adaptation of transformers to the vision domain,
particularly when trained in a self-supervised fashion, it has been shown that
vision transformers can learn impressive object-reasoning-like behaviour and
features expressive for the task of object segmentation in images. In this
paper, we build on the self-supervision task of masked autoencoding and explore
its effectiveness for explicitly learning object-centric representations with
transformers. To this end, we design an object-centric autoencoder using
transformers only and train it end-to-end to reconstruct full images from
unmasked patches. We show that the model efficiently learns to decompose simple
scenes as measured by segmentation metrics on several multi-object benchmarks."
Deep Model-Based Super-Resolution with Non-uniform Blur,0.055798,"We propose a state-of-the-art method for super-resolution with non-uniform
blur. Single-image super-resolution methods seek to restore a high-resolution
image from blurred, subsampled, and noisy measurements. Despite their
impressive performance, existing techniques usually assume a uniform blur
kernel. Hence, these techniques do not generalize well to the more general case
of non-uniform blur. Instead, in this paper, we address the more realistic and
computationally challenging case of spatially-varying blur. To this end, we
first propose a fast deep plug-and-play algorithm, based on linearized ADMM
splitting techniques, which can solve the super-resolution problem with
spatially-varying blur. Second, we unfold our iterative algorithm into a single
network and train it end-to-end. In this way, we overcome the intricacy of
manually tuning the parameters involved in the optimization scheme. Our
algorithm presents remarkable performance and generalizes well after a single
training to a large family of spatially-varying blur kernels, noise levels and
scale factors."
AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation,0.0600174,"As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties, the
capabilities of detecting novelties, characterizing them,
accommodating/adapting to them, gathering ground-truth training data and
incrementally learning the unknowns/novelties become critical in making the AI
agent more and more knowledgeable, powerful and self-sustainable over time. The
key challenge here is how to automate the process so that it is carried out
continually on the agent's own initiative and through its own interactions with
humans, other agents and the environment just like human on-the-job learning.
This paper proposes a framework (called SOLA) for this learning paradigm to
promote the research of building autonomous and continual learning enabled AI
agents. To show feasibility, an implemented agent is also described."
Controlling Bias Exposure for Fair Interpretable Predictions,0.101849,"Recent work on reducing bias in NLP models usually focuses on protecting or
isolating information related to a sensitive attribute (like gender or race).
However, when sensitive information is semantically entangled with the task
information of the input, e.g., gender information is predictive for a
profession, a fair trade-off between task performance and bias mitigation is
difficult to achieve. Existing approaches perform this trade-off by eliminating
bias information from the latent space, lacking control over how much bias is
necessarily required to be removed. We argue that a favorable debiasing method
should use sensitive information 'fairly', rather than blindly eliminating it
(Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we
provide a novel debiasing algorithm by adjusting the predictive model's belief
to (1) ignore the sensitive information if it is not useful for the task; (2)
use sensitive information minimally as necessary for the prediction (while also
incurring a penalty). Experimental results on two text classification tasks
(influenced by gender) and an open-ended generation task (influenced by race)
indicate that our model achieves a desirable trade-off between debiasing and
task performance along with producing debiased rationales as evidence."
Mathematical Cookbook for Snapshot Compressive Imaging,0.0734141,"The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction, conventional optimization, and deep
equilibrium models. The latest releases are strongly recommended! For any other
questions, suggestions, or comments, feel free to email the author."
N-RPN: Hard Example Learning for Region Proposal Networks,0.0134039,"The region proposal task is to generate a set of candidate regions that
contain an object. In this task, it is most important to propose as many
candidates of ground-truth as possible in a fixed number of proposals. In a
typical image, however, there are too few hard negative examples compared to
the vast number of easy negatives, so region proposal networks struggle to
train on hard negatives. Because of this problem, networks tend to propose hard
negatives as candidates, while failing to propose ground-truth candidates,
which leads to poor performance. In this paper, we propose a Negative Region
Proposal Network(nRPN) to improve Region Proposal Network(RPN). The nRPN learns
from the RPN's false positives and provide hard negative examples to the RPN.
Our proposed nRPN leads to a reduction in false positives and better RPN
performance. An RPN trained with an nRPN achieves performance improvements on
the PASCAL VOC 2007 dataset."
Interpretation Quality Score for Measuring the Quality of interpretability methods,0.0398972,"Machine learning (ML) models have been applied to a wide range of natural
language processing (NLP) tasks in recent years. In addition to making accurate
decisions, the necessity of understanding how models make their decisions has
become apparent in many applications. To that end, many interpretability
methods that help explain the decision processes of ML models have been
developed. Yet, there currently exists no widely-accepted metric to evaluate
the quality of explanations generated by these methods. As a result, there
currently is no standard way of measuring to what degree an interpretability
method achieves an intended objective. Moreover, there is no accepted standard
of performance by which we can compare and rank the current existing
interpretability methods. In this paper, we propose a novel metric for
quantifying the quality of explanations generated by interpretability methods.
We compute the metric on three NLP tasks using six interpretability methods and
present our results."
Multi-View Dreaming: Multi-View World Model with Contrastive Learning,0.041571,"In this paper, we propose Multi-View Dreaming, a novel reinforcement learning
agent for integrated recognition and control from multi-view observations by
extending Dreaming. Most current reinforcement learning method assumes a
single-view observation space, and this imposes limitations on the observed
data, such as lack of spatial information and occlusions. This makes obtaining
ideal observational information from the environment difficult and is a
bottleneck for real-world robotics applications. In this paper, we use
contrastive learning to train a shared latent space between different
viewpoints, and show how the Products of Experts approach can be used to
integrate and control the probability distributions of latent states for
multiple viewpoints. We also propose Multi-View DreamingV2, a variant of
Multi-View Dreaming that uses a categorical distribution to model the latent
state instead of the Gaussian distribution. Experiments show that the proposed
method outperforms simple extensions of existing methods in a realistic robot
control task."
"Embodied, Situated, and Grounded Intelligence: Implications for AI",0.0187008,"In April of 2022, the Santa Fe Institute hosted a workshop on embodied,
situated, and grounded intelligence as part of the Institute's Foundations of
Intelligence project. The workshop brought together computer scientists,
psychologists, philosophers, social scientists, and others to discuss the
science of embodiment and related issues in human intelligence, and its
implications for building robust, human-level AI. In this report, we summarize
each of the talks and the subsequent discussions. We also draw out a number of
key themes and identify important frontiers for future research."
InducT-GCN: Inductive Graph Convolutional Networks for Text Classification,0.0701222,"Text classification aims to assign labels to textual units by making use of
global information. Recent studies have applied graph neural network (GNN) to
capture the global word co-occurrence in a corpus. Existing approaches require
that all the nodes (training and test) in a graph are present during training,
which are transductive and do not naturally generalise to unseen nodes. To make
those models inductive, they use extra resources, like pretrained word
embedding. However, high-quality resource is not always available and hard to
train. Under the extreme settings with no extra resource and limited amount of
training set, can we still learn an inductive graph-based text classification
model? In this paper, we introduce a novel inductive graph-based text
classification framework, InducT-GCN (InducTive Graph Convolutional Networks
for Text classification). Compared to transductive models that require test
documents in training, we construct a graph based on the statistics of training
documents only and represent document vectors with a weighted sum of word
vectors. We then conduct one-directional GCN propagation during testing. Across
five text classification benchmarks, our InducT-GCN outperformed
state-of-the-art methods that are either transductive in nature or pre-trained
additional resources. We also conducted scalability testing by gradually
increasing the data size and revealed that our InducT-GCN can reduce the time
and space complexity. The code is available on:
https://github.com/usydnlp/InductTGCN."
PAMI-AD: An Activity Detector Exploiting Part-attention and Motion Information in Surveillance Videos,0.0132488,"Activity detection in surveillance videos is a challenging task caused by
small objects, complex activity categories, its untrimmed nature, etc. Existing
methods are generally limited in performance due to inaccurate proposals, poor
classifiers or inadequate post-processing method. In this work, we propose a
comprehensive and effective activity detection system in untrimmed surveillance
videos for person-centered and vehicle-centered activities. It consists of four
modules, i.e., object localizer, proposal filter, activity classifier and
activity refiner. For person-centered activities, a novel part-attention
mechanism is proposed to explore detailed features in different body parts. As
for vehicle-centered activities, we propose a localization masking method to
jointly encode motion and foreground attention features. We conduct experiments
on the large-scale activity detection datasets VIRAT, and achieve the best
results for both groups of activities. Furthermore, our team won the 1st place
in the TRECVID 2021 ActEV challenge."
Dual Progressive Transformations for Weakly Supervised Semantic Segmentation,0.0930372,"Weakly supervised semantic segmentation (WSSS), which aims to mine the object
regions by merely using class-level labels, is a challenging task in computer
vision. The current state-of-the-art CNN-based methods usually adopt
Class-Activation-Maps (CAMs) to highlight the potential areas of the object,
however, they may suffer from the part-activated issues. To this end, we try an
early attempt to explore the global feature attention mechanism of vision
transformer in WSSS task. However, since the transformer lacks the inductive
bias as in CNN models, it can not boost the performance directly and may yield
the over-activated problems. To tackle these drawbacks, we propose a
Convolutional Neural Networks Refined Transformer (CRT) to mine a globally
complete and locally accurate class activation maps in this paper. To validate
the effectiveness of our proposed method, extensive experiments are conducted
on PASCAL VOC 2012 and CUB-200-2011 datasets. Experimental evaluations show
that our proposed CRT achieves the new state-of-the-art performance on both the
weakly supervised semantic segmentation task the weakly supervised object
localization task, which outperform others by a large margin."
I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning,0.0633187,"Knowledge graph (KG) embedding seeks to learn vector representations for
entities and relations. Conventional models reason over graph structures, but
they suffer from the issues of graph incompleteness and long-tail entities.
Recent studies have used pre-trained language models to learn embeddings based
on the textual information of entities and relations, but they cannot take
advantage of graph structures. In the paper, we show empirically that these two
kinds of features are complementary for KG embedding. To this end, we propose
CoLE, a Co-distillation Learning method for KG Embedding that exploits the
complementarity of graph structures and text information. Its graph embedding
model employs Transformer to reconstruct the representation of an entity from
its neighborhood subgraph. Its text embedding model uses a pre-trained language
model to generate entity representations from the soft prompts of their names,
descriptions, and relational neighbors. To let the two model promote each
other, we propose co-distillation learning that allows them to distill
selective knowledge from each other's prediction logits. In our co-distillation
learning, each model serves as both a teacher and a student. Experiments on
benchmark datasets demonstrate that the two models outperform their related
baselines, and the ensemble method CoLE with co-distillation learning advances
the state-of-the-art of KG embedding."
The Shared Task on Gender Rewriting,0.0203204,"In this paper, we present the results and findings of the Shared Task on
Gender Rewriting, which was organized as part of the Seventh Arabic Natural
Language Processing Workshop. The task of gender rewriting refers to generating
alternatives of a given sentence to match different target user gender contexts
(e.g., female speaker with a male listener, a male speaker with a male
listener, etc.). This requires changing the grammatical gender (masculine or
feminine) of certain words referring to the users. In this task, we focus on
Arabic, a gender-marking morphologically rich language. A total of five teams
from four countries participated in the shared task."
Improving Multilingual Neural Machine Translation System for Indic Languages,0.0371719,"Machine Translation System (MTS) serves as an effective tool for
communication by translating text or speech from one language to another
language. The need of an efficient translation system becomes obvious in a
large multilingual environment like India, where English and a set of Indian
Languages (ILs) are officially used. In contrast with English, ILs are still
entreated as low-resource languages due to unavailability of corpora. In order
to address such asymmetric nature, multilingual neural machine translation
(MNMT) system evolves as an ideal approach in this direction. In this paper, we
propose a MNMT system to address the issues related to low-resource language
translation. Our model comprises of two MNMT systems i.e. for English-Indic
(one-to-many) and the other for Indic-English (many-to-one) with a shared
encoder-decoder containing 15 language pairs (30 translation directions). Since
most of IL pairs have scanty amount of parallel corpora, not sufficient for
training any machine translation model. We explore various augmentation
strategies to improve overall translation quality through the proposed model. A
state-of-the-art transformer architecture is used to realize the proposed
model. Trials over a good amount of data reveal its superiority over the
conventional models. In addition, the paper addresses the use of language
relationships (in terms of dialect, script, etc.), particularly about the role
of high-resource languages of the same family in boosting the performance of
low-resource languages. Moreover, the experimental results also show the
advantage of backtranslation and domain adaptation for ILs to enhance the
translation quality of both source and target languages. Using all these key
approaches, our proposed model emerges to be more efficient than the baseline
model in terms of evaluation metrics i.e BLEU (BiLingual Evaluation Understudy)
score for a set of ILs."
A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning,0.0352569,"Subword tokenization is a commonly used input pre-processing step in most
recent NLP models. However, it limits the models' ability to leverage
end-to-end task learning. Its frequency-based vocabulary creation compromises
tokenization in low-resource languages, leading models to produce suboptimal
representations. Additionally, the dependency on a fixed vocabulary limits the
subword models' adaptability across languages and domains. In this work, we
propose a vocabulary-free neural tokenizer by distilling segmentation
information from heuristic-based subword tokenization. We pre-train our
character-based tokenizer by processing unique words from multilingual corpus,
thereby extensively increasing word diversity across languages. Unlike the
predefined and fixed vocabularies in subword methods, our tokenizer allows
end-to-end task learning, resulting in optimal task-specific tokenization. The
experimental results show that replacing the subword tokenizer with our neural
tokenizer consistently improves performance on multilingual (NLI) and
code-switching (sentiment analysis) tasks, with larger gains in low-resource
languages. Additionally, our neural tokenizer exhibits a robust performance on
downstream tasks when adversarial noise is present (typos and misspelling),
further increasing the initial improvements over statistical subword
tokenizers."
HyperNST: Hyper-Networks for Neural Style Transfer,0.0400282,"We present HyperNST; a neural style transfer (NST) technique for the artistic
stylization of images, based on Hyper-networks and the StyleGAN2 architecture.
Our contribution is a novel method for inducing style transfer parameterized by
a metric space, pre-trained for style-based visual search (SBVS). We show for
the first time that such space may be used to drive NST, enabling the
application and interpolation of styles from an SBVS system. The technical
contribution is a hyper-network that predicts weight updates to a StyleGAN2
pre-trained over a diverse gamut of artistic content (portraits), tailoring the
style parameterization on a per-region basis using a semantic map of the facial
regions. We show HyperNST to exceed state of the art in content preservation
for our stylized content while retaining good style transfer performance."
EDU-level Extractive Summarization with Varying Summary Lengths,0.0112355,"Extractive models usually formulate text summarization as extracting fixed
top-$k$ salient sentences from the document as a summary. Few works exploited
extracting finer-grained Elementary Discourse Unit (EDU) with little analysis
and justification for the extractive unit selection. Further, the selection
strategy of the fixed top-$k$ salient sentences fits the summarization need
poorly, as the number of salient sentences in different documents varies and
therefore a common or best $k$ does not exist in reality. To fill these gaps,
this paper first conducts the comparison analysis of oracle summaries based on
EDUs and sentences, which provides evidence from both theoretical and
experimental perspectives to justify and quantify that EDUs make summaries with
higher automatic evaluation scores than sentences. Then, considering this merit
of EDUs, this paper further proposes an EDU-level extractive model with Varying
summary Lengths and develops the corresponding learning algorithm. EDU-VL
learns to encode and predict probabilities of EDUs in the document, generate
multiple candidate summaries with varying lengths based on various $k$ values,
and encode and score candidate summaries, in an end-to-end training manner.
Finally, EDU-VL is experimented on single and multi-document benchmark datasets
and shows improved performances on ROUGE scores in comparison with
state-of-the-art extractive models, and further human evaluation suggests that
EDU-constituent summaries maintain good grammaticality and readability."
Unpaired Image Translation via Vector Symbolic Architectures,0.0839373,"Image-to-image translation has played an important role in enabling synthetic
data for computer vision. However, if the source and target domains have a
large semantic mismatch, existing techniques often suffer from source content
corruption aka semantic flipping. To address this problem, we propose a new
paradigm for image-to-image translation using Vector Symbolic Architectures
(VSA), a theoretical framework which defines algebraic operations in a
high-dimensional vector (hypervector) space. We introduce VSA-based constraints
on adversarial learning for source-to-target translations by learning a
hypervector mapping that inverts the translation to ensure consistency with
source content. We show both qualitatively and quantitatively that our method
improves over other state-of-the-art techniques."
Exploration of Machine Learning Classification Models Used for Behavioral Biometrics Authentication,0.0927617,"Mobile devices have been manufactured and enhanced at growing rates in the
past decades. While this growth has significantly evolved the capability of
these devices, their security has been falling behind. This contrast in
development between capability and security of mobile devices is a significant
problem with the sensitive information of the public at risk. Continuing the
previous work in this field, this study identifies key Machine Learning
algorithms currently being used for behavioral biometric mobile authentication
schemes and aims to provide a comprehensive review of these algorithms when
used with touch dynamics and phone movement. Throughout this paper the
benefits, limitations, and recommendations for future work will be discussed."
Extending Process Discovery with Model Complexity Optimization and Cyclic States Identification: Application to Healthcare Processes,0.0121903,"Within Process mining, discovery techniques had made it possible to construct
business process models automatically from event logs. However, results often
do not achieve the balance between model complexity and its fitting accuracy,
so there is a need for manual model adjusting. The paper presents an approach
to process mining providing semi-automatic support to model optimization based
on the combined assessment of the model complexity and fitness. To balance
between the two ingredients, a model simplification approach is proposed, which
essentially abstracts the raw model at the desired granularity. Additionally,
we introduce a concept of meta-states, a cycle collapsing in the model, which
can potentially simplify the model and interpret it. We aim to demonstrate the
capabilities of the technological solution using three datasets from different
applications in the healthcare domain. They are remote monitoring process for
patients with arterial hypertension and workflows of healthcare workers during
the COVID-19 pandemic. A case study also investigates the use of various
complexity measures and different ways of solution application providing
insights on better practices in improving interpretability and
complexity/fitness balance in process models."
Evolution of a Web-Scale Near Duplicate Image Detection System,0.0207109,"Detecting near duplicate images is fundamental to the content ecosystem of
photo sharing web applications. However, such a task is challenging when
involving a web-scale image corpus containing billions of images. In this
paper, we present an efficient system for detecting near duplicate images
across 8 billion images. Our system consists of three stages: candidate
generation, candidate selection, and clustering. We also demonstrate that this
system can be used to greatly improve the quality of recommendations and search
results across a number of real-world applications.
  In addition, we include the evolution of the system over the course of six
years, bringing out experiences and lessons on how new systems are designed to
accommodate organic content growth as well as the latest technology. Finally,
we are releasing a human-labeled dataset of ~53,000 pairs of images introduced
in this paper."
Unsupervised Keyphrase Extraction via Interpretable Neural Networks,0.00976107,"Keyphrase extraction aims at automatically extracting a list of ""important""
phrases representing the key concepts in a document. Prior approaches for
unsupervised keyphrase extraction resorted to heuristic notions of phrase
importance via embedding clustering or graph centrality, requiring extensive
domain expertise. Our work presents a simple alternative approach which defines
keyphrases as document phrases that are salient for predicting the topic of the
document. To this end, we propose INSPECT -- an approach that uses
self-explaining models for identifying influential keyphrases in a document by
measuring the predictive impact of input phrases on the downstream task of the
document topic classification. We show that this novel method not only
alleviates the need for ad-hoc heuristics but also achieves state-of-the-art
results in unsupervised keyphrase extraction in four datasets across two
domains: scientific publications and news articles."
Font Representation Learning via Paired-glyph Matching,0.0062682,"Fonts can convey profound meanings of words in various forms of glyphs.
Without typography knowledge, manually selecting an appropriate font or
designing a new font is a tedious and painful task. To allow users to explore
vast font styles and create new font styles, font retrieval and font style
transfer methods have been proposed. These tasks increase the need for learning
high-quality font representations. Therefore, we propose a novel font
representation learning scheme to embed font styles into the latent space. For
the discriminative representation of a font from others, we propose a
paired-glyph matching-based font representation learning model that attracts
the representations of glyphs in the same font to one another, but pushes away
those of other fonts. Through evaluations on font retrieval with query glyphs
on new fonts, we show our font representation learning scheme achieves better
generalization performance than the existing font representation learning
techniques. Finally on the downstream font style transfer and generation tasks,
we confirm the benefits of transfer learning with the proposed method. The
source code is available at https://github.com/junhocho/paired-glyph-matching."
A Structure-Guided Diffusion Model for Large-Hole Image Completion,0.0152297,"Image completion techniques have made significant progress in filling missing
regions (i.e., holes) in images. However, large-hole completion remains
challenging due to limited structural information. In this paper, we address
this problem by integrating explicit structural guidance into diffusion-based
image completion, forming our structure-guided diffusion model (SGDM). It
consists of two cascaded diffusion probabilistic models: structure and texture
generators. The structure generator generates an edge image representing
plausible structures within the holes, which is then used for guiding the
texture generation process. To train both generators jointly, we devise a novel
strategy that leverages optimal Bayesian denoising, which denoises the output
of the structure generator in a single step and thus allows backpropagation.
Our diffusion-based approach enables a diversity of plausible completions,
while the editable edges allow for editing parts of an image. Our experiments
on natural scene (Places) and face (CelebA-HQ) datasets demonstrate that our
method achieves a superior or comparable visual quality compared to
state-of-the-art approaches. The code is available for research purposes at
https://github.com/UdonDa/Structure_Guided_Diffusion_Model."
Dreamento: an open-source dream engineering toolbox for sleep EEG wearables,0.022995,"We introduce Dreamento (Dream engineering toolbox), an open-source Python
package for dream engineering using sleep electroencephalography (EEG)
wearables. Dreamento main functions are (1) real-time recording, monitoring,
analysis, and sensory stimulation, and (2) offline post-processing of the
resulting data, both in a graphical user interface (GUI). In real-time,
Dreamento is capable of (1) data recording, visualization, and navigation, (2)
power-spectrum analysis, (3) automatic sleep scoring, (4) sensory stimulation
(visual, auditory, tactile), (5) establishing text-to-speech communication, and
(6) managing annotations of automatic and manual events. The offline functions
aid in post-processing the acquired data with features to reformat the wearable
data and integrate it with non-wearable recorded modalities such as
electromyography (EMG). While Dreamento was primarily developed for (lucid)
dreaming studies, its applications can be extended to other areas of sleep
research such as closed-loop auditory stimulation and targeted memory
reactivation."
"From Perception to Programs: Regularize, Overparameterize, and Amortize",0.0758003,"Toward combining inductive reasoning with perception abilities, we develop
techniques for neurosymbolic program synthesis where perceptual input is first
parsed by neural nets into a low-dimensional interpretable representation,
which is then processed by a synthesized program. We explore several techniques
for relaxing the problem and jointly learning all modules end-to-end with
gradient descent: multitask learning; amortized inference;
overparameterization; and a differentiable strategy for penalizing lengthy
programs. Collectedly this toolbox improves the stability of gradient-guided
program search, and suggests ways of learning both how to perceive input as
discrete abstractions, and how to symbolically process those abstractions as
programs."
Learning Enriched Illuminants for Cross and Single Sensor Color Constancy,0.00132158,"Color constancy aims to restore the constant colors of a scene under
different illuminants. However, due to the existence of camera spectral
sensitivity, the network trained on a certain sensor, cannot work well on
others. Also, since the training datasets are collected in certain
environments, the diversity of illuminants is limited for complex real world
prediction. In this paper, we tackle these problems via two aspects. First, we
propose cross-sensor self-supervised training to train the network. In detail,
we consider both the general sRGB images and the white-balanced RAW images from
current available datasets as the white-balanced agents. Then, we train the
network by randomly sampling the artificial illuminants in a sensor-independent
manner for scene relighting and supervision. Second, we analyze a previous
cascaded framework and present a more compact and accurate model by sharing the
backbone parameters with learning attention specifically. Experiments show that
our cross-sensor model and single-sensor model outperform other
state-of-the-art methods by a large margin on cross and single sensor
evaluations, respectively, with only 16% parameters of the previous best model."
FORCE: A Framework of Rule-Based Conversational Recommender System,0.0295994,"The conversational recommender systems (CRSs) have received extensive
attention in recent years. However, most of the existing works focus on various
deep learning models, which are largely limited by the requirement of
large-scale human-annotated datasets. Such methods are not able to deal with
the cold-start scenarios in industrial products. To alleviate the problem, we
propose FORCE, a Framework Of Rule-based Conversational Recommender system that
helps developers to quickly build CRS bots by simple configuration. We conduct
experiments on two datasets in different languages and domains to verify its
effectiveness and usability."
Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning,0.0929713,"Controlled automated story generation seeks to generate natural language
stories satisfying constraints from natural language critiques or preferences.
Existing methods to control for story preference utilize prompt engineering
which is labor intensive and often inconsistent. They may also use
logit-manipulation methods which require annotated datasets to exist for the
desired attributes. To address these issues, we first train a contrastive
bi-encoder model to align stories with corresponding human critiques, named
CARP, building a general purpose preference model. This is subsequently used as
a reward function to fine-tune a generative language model via reinforcement
learning. However, simply fine-tuning a generative language model with a
contrastive reward model does not always reliably result in a story generation
system capable of generating stories that meet user preferences. To increase
story generation robustness we further fine-tune the contrastive reward model
using a prompt-learning technique. A human participant study is then conducted
comparing generations from our full system, ablations, and two baselines. We
show that the full fine-tuning pipeline results in a story generator preferred
over a LLM 20x as large as well as logit-based methods. This motivates the use
of contrastive learning for general purpose human preference modeling."
Planning Landscape Analysis for Self-Adaptive Systems,0.0877092,"To assure performance on the fly, planning is arguably one of the most
important steps for self-adaptive systems (SASs), especially when they are
highly configurable with a daunting number of adaptation options. However,
there has been little understanding of the planning landscape or ways by which
it can be analyzed. This inevitably creates barriers to the design of better
and tailored planners for SASs. In this paper, we showcase how the planning
landscapes of SASs can be quantified and reasoned, particularly with respect to
the different environments. By studying four diverse real-world SASs and 14
environments, we found that (1) the SAS planning landscapes often provide
strong guidance to the planner, but their ruggedness and multi-modality can be
the major obstacle; (2) the extents of guidance and number of global/local
optima are sensitive to the changing environment, but not the ruggedness of the
surface; (3) the local optima are often closer to the global optimum than other
random points; and (4) there are considerable (and useful) overlaps on the
global/local optima between landscapes under different environments. We then
discuss the potential implications to the future work of planner designs for
SASs."
Mono-surrogate vs Multi-surrogate in Multi-objective Bayesian Optimisation,0.0741982,"Bayesian optimisation (BO) has been widely used to solve problems with
expensive function evaluations. In multi-objective optimisation problems, BO
aims to find a set of approximated Pareto optimal solutions. There are
typically two ways to build surrogates in multi-objective BO: One surrogate by
aggregating objective functions (by using a scalarising function, also called
mono-surrogate approach) and multiple surrogates (for each objective function,
also called multi-surrogate approach). In both approaches, an acquisition
function (AF) is used to guide the search process. Mono-surrogate has the
advantage that only one model is used, however, the approach has two major
limitations. Firstly, the fitness landscape of the scalarising function and the
objective functions may not be similar. Secondly, the approach assumes that the
scalarising function distribution is Gaussian, and thus a closed-form
expression of the AF can be used. In this work, we overcome these limitations
by building a surrogate model for each objective function and show that the
scalarising function distribution is not Gaussian. We approximate the
distribution using Generalised extreme value distribution. The results and
comparison with existing approaches on standard benchmark and real-world
optimisation problems show the potential of the multi-surrogate approach."
CREATER: CTR-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning,0.0621283,"This paper focuses on automatically generating the text of an ad, and the
goal is that the generated text can capture user interest for achieving higher
click-through rate (CTR). We propose CREATER, a CTR-driven advertising text
generation approach, to generate ad texts based on high-quality user reviews.
To incorporate CTR objective, our model learns from online A/B test data with
contrastive learning, which encourages the model to generate ad texts that
obtain higher CTR. To alleviate the low-resource issue, we design a customized
self-supervised objective reducing the gap between pre-training and
fine-tuning. Experiments on industrial datasets show that CREATER significantly
outperforms current approaches. It has been deployed online in a leading
advertising platform and brings uplift on core online metrics."
ALLSH: Active Learning Guided by Local Sensitivity and Hardness,0.0537378,"Active learning, which effectively collects informative unlabeled data for
annotation, reduces the demand for labeled data. In this work, we propose to
retrieve unlabeled samples with a local sensitivity and hardness-aware
acquisition function. The proposed method generates data copies through local
perturbations and selects data points whose predictive likelihoods diverge the
most from their copies. We further empower our acquisition function by
injecting the select-worst case perturbation. Our method achieves consistent
gains over the commonly used active learning strategies in various
classification tasks. Furthermore, we observe consistent improvements over the
baselines on the study of prompt selection in prompt-based few-shot learning.
These experiments demonstrate that our acquisition guided by local sensitivity
and hardness can be effective and beneficial for many NLP tasks."
NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs,0.0511849,"Complex query answering (CQA) is an essential task for multi-hop and logical
reasoning on knowledge graphs (KGs). Currently, most approaches are limited to
queries among binary relational facts and pay less attention to n-ary facts
(n>=2) containing more than two entities, which are more prevalent in the real
world. Moreover, previous CQA methods can only make predictions for a few given
types of queries and cannot be flexibly extended to more complex logical
queries, which significantly limits their applications. To overcome these
challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model
for CQA over hyper-relational knowledge graphs (HKGs), which include massive
n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and
fuzzy logic theory to satisfy all n-ary FOL queries, including existential
quantifiers, conjunction, disjunction, and negation. We also propose a parallel
processing algorithm that can train or predict arbitrary n-ary FOL queries in a
single batch, regardless of the kind of each query, with good flexibility and
extensibility. In addition, we generate a new CQA dataset WD50K-NFOL, including
diverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL and
other standard CQA datasets show that NQE is the state-of-the-art CQA method
over HKGs with good generalization capability. Our code and dataset are
publicly available."
Hierarchical Phrase-based Sequence-to-Sequence Learning,0.0478948,"We describe a neural transducer that maintains the flexibility of standard
sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases
as a source of inductive bias during training and as explicit constraints
during inference. Our approach trains two models: a discriminative parser based
on a bracketing transduction grammar whose derivation tree hierarchically
aligns source and target phrases, and a neural seq2seq model that learns to
translate the aligned phrases one-by-one. We use the same seq2seq model to
translate at all phrase scales, which results in two inference modes: one mode
in which the parser is discarded and only the seq2seq component is used at the
sequence-level, and another in which the parser is combined with the seq2seq
model. Decoding in the latter mode is done with the cube-pruned CKY algorithm,
which is more involved but can make use of new translation rules during
inference. We formalize our model as a source-conditioned synchronous grammar
and develop an efficient variational inference algorithm for training. When
applied on top of both randomly initialized and pretrained seq2seq models, we
find that both inference modes performs well compared to baselines on small
scale machine translation benchmarks."
Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes,0.0120638,"In many sequential tasks, a model needs to remember relevant events from the
distant past to make correct predictions. Unfortunately, a straightforward
application of gradient based training requires intermediate computations to be
stored for every element of a sequence. This requires to store prohibitively
large intermediate data if a sequence consists of thousands or even millions
elements, and as a result, makes learning of very long-term dependencies
infeasible. However, the majority of sequence elements can usually be predicted
by taking into account only temporally local information. On the other hand,
predictions affected by long-term dependencies are sparse and characterized by
high uncertainty given only local information. We propose MemUP, a new training
method that allows to learn long-term dependencies without backpropagating
gradients through the whole sequence at a time. This method can potentially be
applied to any recurrent architecture. LSTM network trained with MemUP performs
better or comparable to baselines while requiring to store less intermediate
data."
Data Augmentation by Selecting Mixed Classes Considering Distance Between Classes,0.00598679,"Data augmentation is an essential technique for improving recognition
accuracy in object recognition using deep learning. Methods that generate mixed
data from multiple data sets, such as mixup, can acquire new diversity that is
not included in the training data, and thus contribute significantly to
accuracy improvement. However, since the data selected for mixing are randomly
sampled throughout the training process, there are cases where appropriate
classes or data are not selected. In this study, we propose a data augmentation
method that calculates the distance between classes based on class
probabilities and can select data from suitable classes to be mixed in the
training process. Mixture data is dynamically adjusted according to the
training trend of each class to facilitate training. The proposed method is
applied in combination with conventional methods for generating mixed data.
Evaluation experiments show that the proposed method improves recognition
performance on general and long-tailed image recognition datasets."
Defending From Physically-Realizable Adversarial Attacks Through Internal Over-Activation Analysis,0.0831078,"This work presents Z-Mask, a robust and effective strategy to improve the
adversarial robustness of convolutional networks against physically-realizable
adversarial attacks. The presented defense relies on specific Z-score analysis
performed on the internal network features to detect and mask the pixels
corresponding to adversarial objects in the input image. To this end, spatially
contiguous activations are examined in shallow and deep layers to suggest
potential adversarial regions. Such proposals are then aggregated through a
multi-thresholding mechanism. The effectiveness of Z-Mask is evaluated with an
extensive set of experiments carried out on models for both semantic
segmentation and object detection. The evaluation is performed with both
digital patches added to the input images and printed patches positioned in the
real world. The obtained results confirm that Z-Mask outperforms the
state-of-the-art methods in terms of both detection accuracy and overall
performance of the networks under attack. Additional experiments showed that
Z-Mask is also robust against possible defense-aware attacks."
Motion Prediction via Joint Dependency Modeling in Phase Space,0.0941315,"Motion prediction is a classic problem in computer vision, which aims at
forecasting future motion given the observed pose sequence. Various deep
learning models have been proposed, achieving state-of-the-art performance on
motion prediction. However, existing methods typically focus on modeling
temporal dynamics in the pose space. Unfortunately, the complicated and high
dimensionality nature of human motion brings inherent challenges for dynamic
context capturing. Therefore, we move away from the conventional pose based
representation and present a novel approach employing a phase space trajectory
representation of individual joints. Moreover, current methods tend to only
consider the dependencies between physically connected joints. In this paper,
we introduce a novel convolutional neural model to effectively leverage
explicit prior knowledge of motion anatomy, and simultaneously capture both
spatial and temporal information of joint trajectory dynamics. We then propose
a global optimization module that learns the implicit relationships between
individual joint features.
  Empirically, our method is evaluated on large-scale 3D human motion benchmark
datasets (i.e., Human3.6M, CMU MoCap). These results demonstrate that our
method sets the new state-of-the-art on the benchmark datasets. Our code will
be available at https://github.com/Pose-Group/TEID."
"For Learning in Symmetric Teams, Local Optima are Global Nash Equilibria",0.01963,"Although it has been known since the 1970s that a globally optimal strategy
profile in a common-payoff game is a Nash equilibrium, global optimality is a
strict requirement that limits the result's applicability. In this work, we
show that any locally optimal symmetric strategy profile is also a (global)
Nash equilibrium. Furthermore, we show that this result is robust to
perturbations to the common payoff and to the local optimum. Applied to machine
learning, our result provides a global guarantee for any gradient method that
finds a local optimum in symmetric strategy space. While this result indicates
stability to unilateral deviation, we nevertheless identify broad classes of
games where mixed local optima are unstable under joint, asymmetric deviations.
We analyze the prevalence of instability by running learning algorithms in a
suite of symmetric games, and we conclude by discussing the applicability of
our results to multi-agent RL, cooperative inverse RL, and decentralized
POMDPs."
Agile Maneuvers in Legged Robots: a Predictive Control Approach,0.0954752,"Planning and execution of agile locomotion maneuvers have been a longstanding
challenge in legged robotics. It requires to derive motion plans and local
feedback policies in real-time to handle the nonholonomy of the kinetic
momenta. To achieve so, we propose a hybrid predictive controller that
considers the robot's actuation limits and full-body dynamics. It combines the
feedback policies with tactile information to locally predict future actions.
It converges within a few milliseconds thanks to a feasibility-driven approach.
Our predictive controller enables ANYmal robots to generate agile maneuvers in
realistic scenarios. A crucial element is to track the local feedback policies
as, in contrast to whole-body control, they achieve the desired angular
momentum. To the best of our knowledge, our predictive controller is the first
to handle actuation limits, generate agile locomotion maneuvers, and execute
optimal feedback policies for low level torque control without the use of a
separate whole-body controller."
Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations,0.0676316,"Visual Entailment with natural language explanations aims to infer the
relationship between a text-image pair and generate a sentence to explain the
decision-making process. Previous methods rely mainly on a pre-trained
vision-language model to perform the relation inference and a language model to
generate the corresponding explanation. However, the pre-trained
vision-language models mainly build token-level alignment between text and
image yet ignore the high-level semantic alignment between the phrases (chunks)
and visual contents, which is critical for vision-language reasoning. Moreover,
the explanation generator based only on the encoded joint representation does
not explicitly consider the critical decision-making points of relation
inference. Thus the generated explanations are less faithful to visual-language
reasoning. To mitigate these problems, we propose a unified Chunk-aware
Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a
Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical
Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence
structure inherent in language and various image regions to build chunk-aware
semantic alignment. Relation inferrer uses an attention-based reasoning network
to incorporate the token-level and chunk-level vision-language representations.
LeCG utilizes lexical constraints to expressly incorporate the words or chunks
focused by the relation inferrer into explanation generation, improving the
faithfulness and informativeness of the explanations. We conduct extensive
experiments on three datasets, and experimental results indicate that CALeC
significantly outperforms other competitor models on inference accuracy and
quality of generated explanations."
Mere Contrastive Learning for Cross-Domain Sentiment Analysis,0.0216626,"Cross-domain sentiment analysis aims to predict the sentiment of texts in the
target domain using the model trained on the source domain to cope with the
scarcity of labeled data. Previous studies are mostly cross-entropy-based
methods for the task, which suffer from instability and poor generalization. In
this paper, we explore contrastive learning on the cross-domain sentiment
analysis task. We propose a modified contrastive objective with in-batch
negative samples so that the sentence representations from the same class will
be pushed close while those from the different classes become further apart in
the latent space. Experiments on two widely used datasets show that our model
can achieve state-of-the-art performance in both cross-domain and multi-domain
sentiment analysis tasks. Meanwhile, visualizations demonstrate the
effectiveness of transferring knowledge learned in the source domain to the
target domain and the adversarial test verifies the robustness of our model."
Rethinking Audio-visual Synchronization for Active Speaker Detection,0.106856,"Active speaker detection (ASD) systems are important modules for analyzing
multi-talker conversations. They aim to detect which speakers or none are
talking in a visual scene at any given time. Existing research on ASD does not
agree on the definition of active speakers. We clarify the definition in this
work and require synchronization between the audio and visual speaking
activities. This clarification of definition is motivated by our extensive
experiments, through which we discover that existing ASD methods fail in
modeling the audio-visual synchronization and often classify unsynchronized
videos as active speaking. To address this problem, we propose a cross-modal
contrastive learning strategy and apply positional encoding in attention
modules for supervised ASD models to leverage the synchronization cue.
Experimental results suggest that our model can successfully detect
unsynchronized speaking as not speaking, addressing the limitation of current
models."
Towards Disentangled Speech Representations,0.0529856,"The careful construction of audio representations has become a dominant
feature in the design of approaches to many speech tasks. Increasingly, such
approaches have emphasized ""disentanglement"", where a representation contains
only parts of the speech signal relevant to transcription while discarding
irrelevant information. In this paper, we construct a representation learning
task based on joint modeling of ASR and TTS, and seek to learn a representation
of audio that disentangles that part of the speech signal that is relevant to
transcription from that part which is not. We present empirical evidence that
successfully finding such a representation is tied to the randomness inherent
in training. We then make the observation that these desired, disentangled
solutions to the optimization problem possess unique statistical properties.
Finally, we show that enforcing these properties during training improves WER
by 24.5% relative on average for our joint modeling task. These observations
motivate a novel approach to learning effective audio representations."
Implicit Two-Tower Policies,0.106533,"We present a new class of structured reinforcement learning
policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are
chosen based on the attention scores of their learnable latent representations
with those of the input states. By explicitly disentangling action from state
processing in the policy stack, we achieve two main goals: substantial
computational gains and better performance. Our architectures are compatible
with both: discrete and continuous action spaces. By conducting tests on 15
environments from OpenAI Gym and DeepMind Control Suite, we show that
ITT-architectures are particularly suited for blackbox/evolutionary
optimization and the corresponding policy training algorithms outperform their
vanilla unstructured implicit counterparts as well as commonly used explicit
policies. We complement our analysis by showing how techniques such as hashing
and lazy tower updates, critically relying on the two-tower structure of ITTs,
can be applied to obtain additional computational improvements."
Exploring Diversity-based Active Learning for 3D Object Detection in Autonomous Driving,0.0260712,"3D object detection has recently received much attention due to its great
potential in autonomous vehicle (AV). The success of deep learning based object
detectors relies on the availability of large-scale annotated datasets, which
is time-consuming and expensive to compile, especially for 3D bounding box
annotation. In this work, we investigate diversity-based active learning (AL)
as a potential solution to alleviate the annotation burden. Given limited
annotation budget, only the most informative frames and objects are
automatically selected for human to annotate. Technically, we take the
advantage of the multimodal information provided in an AV dataset, and propose
a novel acquisition function that enforces spatial and temporal diversity in
the selected samples. We benchmark the proposed method against other AL
strategies under realistic annotation cost measurement, where the realistic
costs for annotating a frame and a 3D bounding box are both taken into
consideration. We demonstrate the effectiveness of the proposed method on the
nuScenes dataset and show that it outperforms existing AL strategies
significantly."
Goal Recognition as a Deep Learning Task: the GRNet Approach,0.0224485,"In automated planning, recognising the goal of an agent from a trace of
observations is an important task with many applications. The state-of-the-art
approaches to goal recognition rely on the application of planning techniques,
which requires a model of the domain actions and of the initial domain state
(written, e.g., in PDDL). We study an alternative approach where goal
recognition is formulated as a classification task addressed by machine
learning. Our approach, called GRNet, is primarily aimed at making goal
recognition more accurate as well as faster by learning how to solve it in a
given domain. Given a planning domain specified by a set of propositions and a
set of action names, the goal classification instances in the domain are solved
by a Recurrent Neural Network (RNN). A run of the RNN processes a trace of
observed actions to compute how likely it is that each domain proposition is
part of the agent's goal, for the problem instance under considerations. These
predictions are then aggregated to choose one of the candidate goals. The only
information required as input of the trained RNN is a trace of action labels,
each one indicating just the name of an observed action. An experimental
analysis confirms that \our achieves good performance in terms of both goal
classification accuracy and runtime, obtaining better performance w.r.t. a
state-of-the-art goal recognition system over the considered benchmarks."
Towards Device Efficient Conditional Image Generation,0.0306799,"We present a novel algorithm to reduce tensor compute required by a
conditional image generation autoencoder without sacrificing quality of
photo-realistic image generation. Our method is device agnostic, and can
optimize an autoencoder for a given CPU-only, GPU compute device(s) in about
normal time it takes to train an autoencoder on a generic workstation. We
achieve this via a two-stage novel strategy where, first, we condense the
channel weights, such that, as few as possible channels are used. Then, we
prune the nearly zeroed out weight activations, and fine-tune the autoencoder.
To maintain image quality, fine-tuning is done via student-teacher training,
where we reuse the condensed autoencoder as the teacher. We show performance
gains for various conditional image generation tasks: segmentation mask to face
images, face images to cartoonization, and finally CycleGAN-based model over
multiple compute devices. We perform various ablation studies to justify the
claims and design choices, and achieve real-time versions of various
autoencoders on CPU-only devices while maintaining image quality, thus enabling
at-scale deployment of such autoencoders."
Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?,0.0734519,"The use of Deep Learning and Computer Vision in the Cultural Heritage domain
is becoming highly relevant in the last few years with lots of applications
about audio smart guides, interactive museums and augmented reality. All these
technologies require lots of data to work effectively and be useful for the
user. In the context of artworks, such data is annotated by experts in an
expensive and time consuming process. In particular, for each artwork, an image
of the artwork and a description sheet have to be collected in order to perform
common tasks like Visual Question Answering. In this paper we propose a method
for Visual Question Answering that allows to generate at runtime a description
sheet that can be used for answering both visual and contextual questions about
the artwork, avoiding completely the image and the annotation process. For this
purpose, we investigate on the use of GPT-3 for generating descriptions for
artworks analyzing the quality of generated descriptions through captioning
metrics. Finally we evaluate the performance for Visual Question Answering and
captioning tasks."
longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks,0.0287271,"Developing methods to adversarially challenge NLP systems is a promising
avenue for improving both model performance and interpretability. Here, we
describe the approach of the team ""longhorns"" on Task 1 of the The First
Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to
manually fool a model on an Extractive Question Answering task. Our team
finished first, with a model error rate of 62%. We advocate for a systematic,
linguistically informed approach to formulating adversarial questions, and we
describe the results of our pilot experiments, as well as our official
submission."
Controlling the Focus of Pretrained Language Generation Models,0.00477097,"The finetuning of pretrained transformer-based language generation models are
typically conducted in an end-to-end manner, where the model learns to attend
to relevant parts of the input by itself. However, there does not exist a
mechanism to directly control the model's focus. This work aims to develop a
control mechanism by which a user can select spans of context as ""highlights""
for the model to focus on, and generate relevant output. To achieve this goal,
we augment a pretrained model with trainable ""focus vectors"" that are directly
applied to the model's embeddings, while the model itself is kept fixed. These
vectors, trained on automatic annotations derived from attribution methods, act
as indicators for context importance. We test our approach on two core
generation tasks: dialogue response generation and abstractive summarization.
We also collect evaluation data where the highlight-generation pairs are
annotated by humans. Our experiments show that the trained focus vectors are
effective in steering the model to generate outputs that are relevant to
user-selected highlights."
Mental Health Assessment for the Chatbots,0.0246056,"Previous researches on dialogue system assessment usually focus on the
quality evaluation (e.g. fluency, relevance, etc) of responses generated by the
chatbots, which are local and technical metrics. For a chatbot which responds
to millions of online users including minors, we argue that it should have a
healthy mental tendency in order to avoid the negative psychological impact on
them. In this paper, we establish several mental health assessment dimensions
for chatbots (depression, anxiety, alcohol addiction, empathy) and introduce
the questionnaire-based mental health assessment methods. We conduct
assessments on some well-known open-domain chatbots and find that there are
severe mental health issues for all these chatbots. We consider that it is due
to the neglect of the mental health risks during the dataset building and the
model training procedures. We expect to attract researchers' attention to the
serious mental health problems of chatbots and improve the chatbots' ability in
positive emotional interaction."
A Comprehensive Study on Occlusion Invariant Face Recognition under Face Mask Occlusion,0.028006,"The face mask is an essential sanitaryware in daily lives growing during the
pandemic period and is a big threat to current face recognition systems. The
masks destroy a lot of details in a large area of face, and it makes it
difficult to recognize them even for humans. The evaluation report shows the
difficulty well when recognizing masked faces. Rapid development and
breakthrough of deep learning in the recent past have witnessed most promising
results from face recognition algorithms. But they fail to perform far from
satisfactory levels in the unconstrained environment during the challenges such
as varying lighting conditions, low resolution, facial expressions, pose
variation and occlusions. Facial occlusions are considered one of the most
intractable problems. Especially when the occlusion occupies a large region of
the face because it destroys lots of official features."
Shapley value-based approaches to explain the robustness of classifiers in machine learning,0.0305259,"The use of algorithm-agnostic approaches is an emerging area of research for
explaining the contribution of individual features towards the predicted
outcome. Whilst there is a focus on explaining the prediction itself, a little
has been done on explaining the robustness of these models, that is, how each
feature contributes towards achieving that robustness. In this paper, we
propose the use of Shapley values to explain the contribution of each feature
towards the model's robustness, measured in terms of Receiver-operating
Characteristics (ROC) curve and the Area under the ROC curve (AUC). With the
help of an illustrative example, we demonstrate the proposed idea of explaining
the ROC curve, and visualising the uncertainties in these curves. For
imbalanced datasets, the use of Precision-Recall Curve (PRC) is considered more
appropriate, therefore we also demonstrate how to explain the PRCs with the
help of Shapley values. The explanation of robustness can help analysts in a
number of ways, for example, it can help in feature selection by identifying
the irrelevant features that can be removed to reduce the computational
complexity. It can also help in identifying the features having critical
contributions or negative contributions towards robustness."
STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding,0.0226383,"In this technical report, we introduce our solution to human-centric
spatio-temporal video grounding task. We propose a concise and effective
framework named STVGFormer, which models spatiotemporal visual-linguistic
dependencies with a static branch and a dynamic branch. The static branch
performs cross-modal understanding in a single frame and learns to localize the
target object spatially according to intra-frame visual cues like object
appearances. The dynamic branch performs cross-modal understanding across
multiple frames. It learns to predict the starting and ending time of the
target moment according to dynamic visual cues like motions. Both the static
and dynamic branches are designed as cross-modal transformers. We further
design a novel static-dynamic interaction block to enable the static and
dynamic branches to transfer useful and complementary information from each
other, which is shown to be effective to improve the prediction on hard cases.
Our proposed method achieved 39.6% vIoU and won the first place in the HC-STVG
track of the 4th Person in Context Challenge."
Does Simultaneous Speech Translation need Simultaneous Models?,0.03624,"In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by the different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, generating high
computational costs. In this paper, motivated by the increased social and
environmental impact caused by these costs, we investigate whether a single
model trained offline can serve not only the offline but also the simultaneous
task without the need for any additional training or adaptation. Experiments on
en->{de, es} indicate that, aside from facilitating the adoption of
well-established offline techniques and architectures without affecting
latency, the offline solution achieves similar or better translation quality
compared to the same model trained in simultaneous settings, as well as being
competitive with the SimulST state of the art."
Multi-Task Learning for Visual Scene Understanding,0.00737487,"Despite the recent progress in deep learning, most approaches still go for a
silo-like solution, focusing on learning each task in isolation: training a
separate neural network for each individual task. Many real-world problems,
however, call for a multi-modal approach and, therefore, for multi-tasking
models. Multi-task learning (MTL) aims to leverage useful information across
tasks to improve the generalization capability of a model. This thesis is
concerned with multi-task learning in the context of computer vision. First, we
review existing approaches for MTL. Next, we propose several methods that
tackle important aspects of multi-task learning. The proposed methods are
evaluated on various benchmarks. The results show several advances in the
state-of-the-art of multi-task learning. Finally, we discuss several
possibilities for future work."
LiDAR-MIMO: Efficient Uncertainty Estimation for LiDAR-based 3D Object Detection,0.00550928,"The estimation of uncertainty in robotic vision, such as 3D object detection,
is an essential component in developing safe autonomous systems aware of their
own performance. However, the deployment of current uncertainty estimation
methods in 3D object detection remains challenging due to timing and
computational constraints. To tackle this issue, we propose LiDAR-MIMO, an
adaptation of the multi-input multi-output (MIMO) uncertainty estimation method
to the LiDAR-based 3D object detection task. Our method modifies the original
MIMO by performing multi-input at the feature level to ensure the detection,
uncertainty estimation, and runtime performance benefits are retained despite
the limited capacity of the underlying detector and the large computational
costs of point cloud processing. We compare LiDAR-MIMO with MC dropout and
ensembles as baselines and show comparable uncertainty estimation results with
only a small number of output heads. Further, LiDAR-MIMO can be configured to
be twice as fast as MC dropout and ensembles, while achieving higher mAP than
MC dropout and approaching that of ensembles."
ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language,0.0152438,"Most of the current task-oriented dialogue systems (ToD), despite having
interesting results, are designed for a handful of languages like Chinese and
English. Therefore, their performance in low-resource languages is still a
significant problem due to the absence of a standard dataset and evaluation
policy. To address this problem, we proposed ViWOZ, a fully-annotated
Vietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn,
multi-domain tasked oriented dataset in Vietnamese, a low-resource language.
The dataset consists of a total of 5,000 dialogues, including 60,946 fully
annotated utterances. Furthermore, we provide a comprehensive benchmark of both
modular and end-to-end models in low-resource language scenarios. With those
characteristics, the ViWOZ dataset enables future studies on creating a
multilingual task-oriented dialogue system."
Discrete Factorial Representations as an Abstraction for Goal Conditioned Reinforcement Learning,0.0142688,"Goal-conditioned reinforcement learning (RL) is a promising direction for
training agents that are capable of solving multiple tasks and reach a diverse
set of objectives. How to \textit{specify} and \textit{ground} these goals in
such a way that we can both reliably reach goals during training as well as
generalize to new goals during evaluation remains an open area of research.
Defining goals in the space of noisy and high-dimensional sensory inputs poses
a challenge for training goal-conditioned agents, or even for generalization to
novel goals. We propose to address this by learning factorial representations
of goals and processing the resulting representation via a discretization
bottleneck, for coarser goal specification, through an approach we call DGRL.
We show that applying a discretizing bottleneck can improve performance in
goal-conditioned RL setups, by experimentally evaluating this method on tasks
ranging from maze environments to complex robotic navigation and manipulation.
Additionally, we prove a theorem lower-bounding the expected return on
out-of-distribution goals, while still allowing for specifying goals with
expressive combinatorial structure."
Drawing Causal Inferences About Performance Effects in NLP,0.00458042,"This article emphasizes that NLP as a science seeks to make inferences about
the performance effects that result from applying one method (compared to
another method) in the processing of natural language. Yet NLP research in
practice usually does not achieve this goal: In NLP research articles,
typically only a few models are compared. Each model results from a specific
procedural pipeline (here named processing system) that is composed of a
specific collection of methods that are used in preprocessing, pretraining,
hyperparameter tuning, and training on the target task. To make generalizing
inferences about the performance effect that is caused by applying some method
A vs. another method B, it is not sufficient to compare a few specific models
that are produced by a few specific (probably incomparable) processing systems.
Rather, the following procedure would allow drawing inferences about methods'
performance effects: (1) A population of processing systems that researchers
seek to infer to has to be defined. (2) A random sample of processing systems
from this population is drawn. (The drawn processing systems in the sample will
vary with regard to the methods they apply along their procedural pipelines and
also will vary regarding the compositions of their training and test data sets
used for training and evaluation.) (3) Each processing system is applied once
with method A and once with method B. (4) Based on the sample of applied
processing systems, the expected generalization errors of method A and method B
are approximated. (5) The difference between the expected generalization errors
of method A and method B is the estimated average treatment effect due to
applying method A compared to method B in the population of processing systems."
Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?,0.0358988,"Even though Generative Adversarial Networks (GANs) have shown a remarkable
ability to generate high-quality images, GANs do not always guarantee the
generation of photorealistic images. Occasionally, they generate images that
have defective or unnatural objects, which are referred to as 'artifacts'.
Research to investigate why these artifacts emerge and how they can be detected
and removed has yet to be sufficiently carried out. To analyze this, we first
hypothesize that rarely activated neurons and frequently activated neurons have
different purposes and responsibilities for the progress of generating images.
In this study, by analyzing the statistics and the roles for those neurons, we
empirically show that rarely activated neurons are related to the failure
results of making diverse objects and inducing artifacts. In addition, we
suggest a correction method, called 'Sequential Ablation', to repair the
defective part of the generated images without high computational cost and
manual efforts."
SAT: Self-adaptive training for fashion compatibility prediction,0.0487522,"This paper presents a self-adaptive training (SAT) model for fashion
compatibility prediction. It focuses on the learning of some hard items, such
as those that share similar color, texture, and pattern features but are
considered incompatible due to the aesthetics or temporal shifts. Specifically,
we first design a method to define hard outfits and a difficulty score (DS) is
defined and assigned to each outfit based on the difficulty in recommending an
item for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS
of the outfit is considered. Finally, we propose a very simple conditional
similarity network combining the proposed SATL to achieve the learning of hard
items in the fashion compatibility prediction. Experiments on the publicly
available Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our
SAT's effectiveness in fashion compatibility prediction. Besides, our SATL can
be easily extended to other conditional similarity networks to improve their
performance."
Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization,0.0226756,"In zero-shot multilingual extractive text summarization, a model is typically
trained on English summarization dataset and then applied on summarization
datasets of other languages. Given English gold summaries and documents,
sentence-level labels for extractive summarization are usually generated using
heuristics. However, these monolingual labels created on English datasets may
not be optimal on datasets of other languages, for that there is the syntactic
or semantic discrepancy between different languages. In this way, it is
possible to translate the English dataset to other languages and obtain
different sets of labels again using heuristics. To fully leverage the
information of these different sets of labels, we propose NLSSum (Neural Label
Search for Summarization), which jointly learns hierarchical weights for these
different sets of labels together with our summarization model. We conduct
multilingual zero-shot summarization experiments on MLSUM and WikiLingua
datasets, and we achieve state-of-the-art results using both human and
automatic evaluations across these two datasets."
DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing,0.0917354,"In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary
attribute-value descriptions, which is considered more comprehensive and
specific than a triple-based fact. However, currently available
hyper-relational KG embedding methods in a single view are limited in
application because they weaken the hierarchical structure that represents the
affiliation between entities. To overcome this limitation, we propose a
dual-view hyper-relational KG structure (DH-KG) that contains a
hyper-relational instance view for entities and a hyper-relational ontology
view for concepts that are abstracted hierarchically from the entities. This
paper defines link prediction and entity typing tasks on DH-KG for the first
time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and
HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding
model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms
baseline models on DH-KG, according to experimental results. Finally, we
provide an example of how this technology can be used to treat hypertension.
Our model and new datasets are publicly available."
Detecting Driver Drowsiness as an Anomaly Using LSTM Autoencoders,0.0114765,"In this paper, an LSTM autoencoder-based architecture is utilized for
drowsiness detection with ResNet-34 as feature extractor. The problem is
considered as anomaly detection for a single subject; therefore, only the
normal driving representations are learned and it is expected that drowsiness
representations, yielding higher reconstruction losses, are to be distinguished
according to the knowledge of the network. In our study, the confidence levels
of normal and anomaly clips are investigated through the methodology of label
assignment such that training performance of LSTM autoencoder and
interpretation of anomalies encountered during testing are analyzed under
varying confidence rates. Our method is experimented on NTHU-DDD and
benchmarked with a state-of-the-art anomaly detection method for driver
drowsiness. Results show that the proposed model achieves detection rate of
0.8740 area under curve (AUC) and is able to provide significant improvements
on certain scenarios."
The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents,0.0021119,"Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings."
A multi-domain virtual network embedding algorithm with delay prediction,0.0305758,"Virtual network embedding (VNE) is an crucial part of network virtualization
(NV), which aims to map the virtual networks (VNs) to a shared substrate
network (SN). With the emergence of various delay-sensitive applications, how
to improve the delay performance of the system has become a hot topic in
academic circles. Based on extensive research, we proposed a multi-domain
virtual network embedding algorithm based on delay prediction (DP-VNE).
Firstly, the candidate physical nodes are selected by estimating the delay of
virtual requests, then particle swarm optimization (PSO) algorithm is used to
optimize the mapping process, so as to reduce the delay of the system. The
simulation results show that compared with the other three advanced algorithms,
the proposed algorithm can significantly reduce the system delay while keeping
other indicators unaffected."
Improving Data Driven Inverse Text Normalization using Data Augmentation,0.021946,"Inverse text normalization (ITN) is used to convert the spoken form output of
an automatic speech recognition (ASR) system to a written form. Traditional
handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile
neural modeling approaches require quality large-scale spoken-written pair
examples in the same or similar domain as the ASR system (in-domain data), to
train. Both these approaches require costly and complex annotations. In this
paper, we present a data augmentation technique that effectively generates rich
spoken-written numeric pairs from out-of-domain textual data with minimal human
annotation. We empirically demonstrate that ITN model trained using our data
augmentation technique consistently outperform ITN model trained using only
in-domain data across all numeric surfaces like cardinal, currency, and
fraction, by an overall accuracy of 14.44%."
Quantitative Method for Security Situation of the Power Information Network Based on the Evolutionary Neural Network,0.005721,"Cybersecurity is the security cornerstone of digital transformation of the
power grid and construction of new power systems. The traditional network
security situation quantification method only analyzes from the perspective of
network performance, ignoring the impact of various power application services
on the security situation, so the quantification results cannot fully reflect
the power information network risk state. This study proposes a method for
quantifying security situation of the power information network based on the
evolutionary neural network. First, the security posture system architecture is
designed by analyzing the business characteristics of power information network
applications. Second, combining the importance of power application business,
the spatial element index system of coupled interconnection is established from
three dimensions of network reliability, threat, and vulnerability. Then, the
BP neural network optimized by the genetic evolutionary algorithm is
incorporated into the element index calculation process, and the quantitative
model of security posture of the power information network based on the
evolutionary neural network is constructed. Finally, a simulation experiment
environment is built according to a power sector network topology, and the
effectiveness and robustness of the method proposed in the study are verified."
Transformers as Neural Augmentors: Class Conditional Sentence Generation via Variational Bayes,0.00721154,"Data augmentation methods for Natural Language Processing tasks are explored
in recent years, however they are limited and it is hard to capture the
diversity on sentence level. Besides, it is not always possible to perform data
augmentation on supervised tasks. To address those problems, we propose a
neural data augmentation method, which is a combination of Conditional
Variational Autoencoder and encoder-decoder Transformer model. While encoding
and decoding the input sentence, our model captures the syntactic and semantic
representation of the input language with its class condition. Following the
developments in the past years on pre-trained language models, we train and
evaluate our models on several benchmarks to strengthen the downstream tasks.
We compare our method with 3 different augmentation techniques. The presented
results show that, our model increases the performance of current models
compared to other data augmentation techniques with a small amount of
computation power."
Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian,0.0181975,"We propose a framework that can deform an object in a 2D image as it exists
in 3D space. Most existing methods for 3D-aware image manipulation are limited
to (1) only changing the global scene information or depth, or (2) manipulating
an object of specific categories. In this paper, we present a 3D-aware image
deformation method with minimal restrictions on shape category and deformation
type. While our framework leverages 2D-to-3D reconstruction, we argue that
reconstruction is not sufficient for realistic deformations due to the
vulnerability to topological errors. Thus, we propose to take a supervised
learning-based approach to predict the shape Laplacian of the underlying volume
of a 3D reconstruction represented as a point cloud. Given the deformation
energy calculated using the predicted shape Laplacian and user-defined
deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to
model plausible handle-based image deformation. In the experiments, we present
our results of deforming 2D character and clothed human images. We also
quantitatively show that our approach can produce more accurate deformation
weights compared to alternative methods (i.e., mesh reconstruction and point
cloud Laplacian methods)."
End-to-End Semantic Video Transformer for Zero-Shot Action Recognition,0.0282583,"While video action recognition has been an active area of research for
several years, zero-shot action recognition has only recently started gaining
traction. In this work, we propose a novel end-to-end trained transformer model
which is capable of capturing long range spatiotemporal dependencies
efficiently, contrary to existing approaches which use 3D-CNNs. Moreover, to
address a common ambiguity in the existing works about classes that can be
considered as previously unseen, we propose a new experimentation setup that
satisfies the zero-shot learning premise for action recognition by avoiding
overlap between the training and testing classes. The proposed approach
significantly outperforms the state of the arts in zero-shot action recognition
in terms of the the top-1 accuracy on UCF-101, HMDB-51 and ActivityNet
datasets. The code and proposed experimentation setup are available in GitHub:
https://github.com/Secure-and-Intelligent-Systems-Lab/SemanticVideoTransformer"
WinoDict: Probing language models for in-context word acquisition,0.0583544,"We introduce a new in-context learning paradigm to measure Large Language
Models' (LLMs) ability to learn novel words during inference. In particular, we
rewrite Winograd-style co-reference resolution problems by replacing the key
concept word with a synthetic but plausible word that the model must understand
to complete the task. Solving this task requires the model to make use of the
dictionary definition of the new word given in the prompt. This benchmark
addresses word acquisition, one important aspect of the diachronic degradation
known to afflict LLMs. As LLMs are frozen in time at the moment they are
trained, they are normally unable to reflect the way language changes over
time. We show that the accuracy of LLMs compared to the original Winograd tasks
decreases radically in our benchmark, thus identifying a limitation of current
models and providing a benchmark to measure future improvements in LLMs ability
to do in-context learning."
Non-iterative optimization of pseudo-labeling thresholds for training object detection models from multiple datasets,0.0144285,"We propose a non-iterative method to optimize pseudo-labeling thresholds for
learning object detection from a collection of low-cost datasets, each of which
is annotated for only a subset of all the object classes. A popular approach to
this problem is first to train teacher models and then to use their confident
predictions as pseudo ground-truth labels when training a student model. To
obtain the best result, however, thresholds for prediction confidence must be
adjusted. This process typically involves iterative search and repeated
training of student models and is time-consuming. Therefore, we develop a
method to optimize the thresholds without iterative optimization by maximizing
the $F_\beta$-score on a validation dataset, which measures the quality of
pseudo labels and can be measured without training a student model. We
experimentally demonstrate that our proposed method achieves an mAP comparable
to that of grid search on the COCO and VOC datasets."
Learning Program Synthesis for Integer Sequences from Scratch,0.0230053,"We present a self-learning approach for synthesizing programs from integer
sequences. Our method relies on a tree search guided by a learned policy. Our
system is tested on the On-Line Encyclopedia of Integer Sequences. There, it
discovers, on its own, solutions for 27987 sequences starting from basic
operators and without human-written training examples."
Interacting Hand-Object Pose Estimation via Dense Mutual Attention,0.0578264,"3D hand-object pose estimation is the key to the success of many computer
vision applications. The main focus of this task is to effectively model the
interaction between the hand and an object. To this end, existing works either
rely on interaction constraints in a computationally-expensive iterative
optimization, or consider only a sparse correlation between sampled hand and
object keypoints. In contrast, we propose a novel dense mutual attention
mechanism that is able to model fine-grained dependencies between the hand and
the object. Specifically, we first construct the hand and object graphs
according to their mesh structures. For each hand node, we aggregate features
from every object node by the learned attention and vice versa for each object
node. Thanks to such dense mutual attention, our method is able to produce
physically plausible poses with high quality and real-time inference speed.
Extensive quantitative and qualitative experiments on large benchmark datasets
show that our method outperforms state-of-the-art methods. The code is
available at https://github.com/rongakowang/DenseMutualAttention.git."
On three types of $L$-fuzzy $$-covering-based rough sets,0.0371745,"In this paper, we mainly construct three types of $L$-fuzzy
$\beta$-covering-based rough set models and study the axiom sets, matrix
representations and interdependency of these three pairs of $L$-fuzzy
$\beta$-covering-based rough approximation operators. Firstly, we propose three
pairs of $L$-fuzzy $\beta$-covering-based rough approximation operators by
introducing the concepts such as $\beta$-degree of intersection and
$\beta$-subsethood degree, which are generalizations of degree of intersection
and subsethood degree, respectively. And then, the axiom set for each of these
$L$-fuzzy $\beta$-covering-based rough approximation operator is investigated.
Thirdly, we give the matrix representations of three types of $L$-fuzzy
$\beta$-covering-based rough approximation operators, which make it valid to
calculate the $L$-fuzzy $\beta$-covering-based lower and upper rough
approximation operators through operations on matrices. Finally, the
interdependency of the three pairs of rough approximation operators based on
$L$-fuzzy $\beta$-covering is studied by using the notion of reducible elements
and independent elements. In other words, we present the necessary and
sufficient conditions under which two $L$-fuzzy $\beta$-coverings can generate
the same lower and upper rough approximation operations."
Subword Segmental Language Modelling for Nguni Languages,0.0498461,"Subwords have become the standard units of text in NLP, enabling efficient
open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword
segmentation is viewed as a preprocessing step applied to the corpus before
training. This can lead to sub-optimal segmentations for low-resource languages
with complex morphologies. We propose a subword segmental language model (SSLM)
that learns how to segment words while being trained for autoregressive
language modelling. By unifying subword segmentation and language modelling,
our model learns subwords that optimise LM performance. We train our model on
the 4 Nguni languages of South Africa. These are low-resource agglutinative
languages, so subword information is critical. As an LM, SSLM outperforms
existing approaches such as BPE-based models on average across the 4 languages.
Furthermore, it outperforms standard subword segmenters on unsupervised
morphological segmentation. We also train our model as a word-level sequence
model, resulting in an unsupervised morphological segmenter that outperforms
existing methods by a large margin for all 4 languages. Our results show that
learning subword segmentation is an effective alternative to existing subword
segmenters, enabling the model to discover morpheme-like subwords that improve
its LM capabilities."
Objects Matter: Learning Object Relation Graph for Robust Camera Relocalization,0.0748047,"Visual relocalization aims to estimate the pose of a camera from one or more
images. In recent years deep learning based pose regression methods have
attracted many attentions. They feature predicting the absolute poses without
relying on any prior built maps or stored images, making the relocalization
very efficient. However, robust relocalization under environments with complex
appearance changes and real dynamics remains very challenging. In this paper,
we propose to enhance the distinctiveness of the image features by extracting
the deep relationship among objects. In particular, we extract objects in the
image and construct a deep object relation graph (ORG) to incorporate the
semantic connections and relative spatial clues of the objects. We integrate
our ORG module into several popular pose regression models. Extensive
experiments on various public indoor and outdoor datasets demonstrate that our
method improves the performance significantly and outperforms the previous
approaches."
CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models,0.0816835,"Users often ask dialogue systems ambiguous questions that require
clarification. We show that current language models rarely ask users to clarify
ambiguous questions and instead provide incorrect answers. To address this, we
introduce CLAM: a framework for getting language models to selectively ask for
clarification about ambiguous user questions. In particular, we show that we
can prompt language models to detect whether a given question is ambiguous,
generate an appropriate clarifying question to ask the user, and give a final
answer after receiving clarification. We also show that we can simulate users
by providing language models with privileged information. This lets us
automatically evaluate multi-turn clarification dialogues. Finally, CLAM
significantly improves language models' accuracy on mixed ambiguous and
unambiguous questions relative to SotA."
SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media,0.028007,"The goal of sexism detection is to mitigate negative online content targeting
certain gender groups of people. However, the limited availability of labeled
sexism-related datasets makes it problematic to identify online sexism for
low-resource languages. In this paper, we address the task of automatic sexism
detection in social media for one low-resource language -- Chinese. Rather than
collecting new sexism data or building cross-lingual transfer learning models,
we develop a cross-lingual domain-aware semantic specialisation system in order
to make the most of existing data. Semantic specialisation is a technique for
retrofitting pre-trained distributional word vectors by integrating external
linguistic knowledge (such as lexico-semantic relations) into the specialised
feature space. To do this, we leverage semantic resources for sexism from a
high-resource language (English) to specialise pre-trained word vectors in the
target language (Chinese) to inject domain knowledge. We demonstrate the
benefit of our sexist word embeddings (SexWEs) specialised by our framework via
intrinsic evaluation of word similarity and extrinsic evaluation of sexism
detection. Compared with other specialisation approaches and Chinese baseline
word vectors, our SexWEs shows an average score improvement of 0.033 and 0.064
in both intrinsic and extrinsic evaluations, respectively. The ablative results
and visualisation of SexWEs also prove the effectiveness of our framework on
retrofitting word vectors in low-resource languages."
Automatic Evaluation and Analysis of Idioms in Neural Machine Translation,0.213743,"A major open problem in neural machine translation (NMT) is the translation
of idiomatic expressions, such as ""under the weather"". The meaning of these
expressions is not composed by the meaning of their constituent words, and NMT
models tend to translate them literally (i.e., word-by-word), which leads to
confusing and nonsensical translations. Research on idioms in NMT is limited
and obstructed by the absence of automatic methods for quantifying these
errors. In this work, first, we propose a novel metric for automatically
measuring the frequency of literal translation errors without human
involvement. Equipped with this metric, we present controlled translation
experiments with models trained in different conditions (with/without the
test-set idioms) and across a wide range of (global and targeted) metrics and
test sets. We explore the role of monolingual pretraining and find that it
yields substantial targeted improvements, even without observing any
translation examples of the test-set idioms. In our analysis, we probe the role
of idiom context. We find that the randomly initialized models are more local
or ""myopic"" as they are relatively unaffected by variations of the idiom
context, unlike the pretrained ones."
A Reference Data Model for Process-Related User Interaction Logs,0.1161,"User interaction (UI) logs are high-resolution event logs that record
low-level activities performed by a user during the execution of a task in an
information system. Each event in a UI log corresponds to a single interaction
between the user and the interface, such as clicking a button or entering a
string into a text field. UI logs are used for purposes like task mining or
robotic process automation (RPA), but each study and tool relies on a different
conceptualization and implementation of the elements and attributes that
constitute user interactions. This lack of standardization makes it difficult
to integrate UI logs from different sources and to combine tools for UI data
collection with downstream analytics or automation solutions. To address this,
we propose a universally applicable reference data model for process-related UI
logs. Based on a review of scientific literature and industry solutions, this
model includes the core attributes of UI logs, but remains flexible with regard
to the scope, level of abstraction, and case notion. We provide an
implementation of the model as an extension to the XES interchange standard for
event logs and demonstrate its practical applicability in a real-life RPA
scenario."
Training Language Models with Language Feedback,0.22071,"Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability."
Exploring Target Representations for Masked Autoencoders,0.166271,"Masked autoencoders have become popular training paradigms for
self-supervised visual representation learning. These models randomly mask a
portion of the input and reconstruct the masked portion according to the target
representations. In this paper, we first show that a careful choice of the
target representation is unnecessary for learning good representations, since
different targets tend to derive similarly behaved models. Driven by this
observation, we propose a multi-stage masked distillation pipeline and use a
randomly initialized model as the teacher, enabling us to effectively train
high-capacity models without any efforts to carefully design target
representations. Interestingly, we further explore using teachers of larger
capacity, obtaining distilled students with remarkable transferring ability. On
different tasks of classification, transfer learning, object detection, and
semantic segmentation, the proposed method to perform masked knowledge
distillation with bootstrapped teachers (dBOT) outperforms previous
self-supervised methods by nontrivial margins. We hope our findings, as well as
the proposed method, could motivate people to rethink the roles of target
representations in pre-training masked autoencoders.The code and pre-trained
models are publicly available at https://github.com/liuxingbin/dbot."
Deep Vehicle Detection in Satellite Video,0.129491,"This work presents a deep learning approach for vehicle detection in
satellite video. Vehicle detection is perhaps impossible in single EO satellite
images due to the tininess of vehicles (4-10 pixel) and their similarity to the
background. Instead, we consider satellite video which overcomes the lack of
spatial information by temporal consistency of vehicle movement. A new
spatiotemporal model of a compact $3 \times 3$ convolutional, neural network is
proposed which neglects pooling layers and uses leaky ReLUs. Then we use a
reformulation of the output heatmap including Non-Maximum-Suppression (NMS) for
the final segmentation. Empirical results on two new annotated satellite videos
reconfirm the applicability of this approach for vehicle detection. They more
importantly indicate that pre-training on WAMI data and then fine-tuning on few
annotated video frames for a new video is sufficient. In our experiment only
five annotated images yield a $F_1$ score of 0.81 on a new video showing more
complex traffic patterns than the Las Vegas video. Our best result on Las Vegas
is a $F_1$ score of 0.87 which makes the proposed approach a leading method for
this benchmark."
Towards Robust k-Nearest-Neighbor Machine Translation,0.11967,"k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research
direction of NMT in recent years. Its main idea is to retrieve useful key-value
pairs from an additional datastore to modify translations without updating the
NMT model. However, the underlying retrieved noisy pairs will dramatically
deteriorate the model performance. In this paper, we conduct a preliminary
study and find that this problem results from not fully exploiting the
prediction of the NMT model. To alleviate the impact of noise, we propose a
confidence-enhanced kNN-MT model with robust training. Concretely, we introduce
the NMT confidence to refine the modeling of two important components of
kNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two
types of perturbations into the retrieved pairs for robust training.
Experimental results on four benchmark datasets demonstrate that our model not
only achieves significant improvements over current kNN-MT models, but also
exhibits better robustness. Our code is available at
https://github.com/DeepLearnXMU/Robust-knn-mt."
A New Amharic Speech Emotion Dataset and Classification Benchmark,0.167651,"In this paper we present the Amharic Speech Emotion Dataset (ASED), which
covers four dialects (Gojjam, Wollo, Shewa and Gonder) and five different
emotions (neutral, fearful, happy, sad and angry). We believe it is the first
Speech Emotion Recognition (SER) dataset for the Amharic language. 65 volunteer
participants, all native speakers, recorded 2,474 sound samples, two to four
seconds in length. Eight judges assigned emotions to the samples with high
agreement level (Fleiss kappa = 0.8). The resulting dataset is freely available
for download. Next, we developed a four-layer variant of the well-known VGG
model which we call VGGb. Three experiments were then carried out using VGGb
for SER, using ASED. First, we investigated whether Mel-spectrogram features or
Mel-frequency Cepstral coefficient (MFCC) features work best for Amharic. This
was done by training two VGGb SER models on ASED, one using Mel-spectrograms
and the other using MFCC. Four forms of training were tried, standard
cross-validation, and three variants based on sentences, dialects and speaker
groups. Thus, a sentence used for training would not be used for testing, and
the same for a dialect and speaker group. The conclusion was that MFCC features
are superior under all four training schemes. MFCC was therefore adopted for
Experiment 2, where VGGb and three other existing models were compared on ASED:
RESNet50, Alex-Net and LSTM. VGGb was found to have very good accuracy (90.73%)
as well as the fastest training time. In Experiment 3, the performance of VGGb
was compared when trained on two existing SER datasets, RAVDESS (English) and
EMO-DB (German) as well as on ASED (Amharic). Results are comparable across
these languages, with ASED being the highest. This suggests that VGGb can be
successfully applied to other languages. We hope that ASED will encourage
researchers to experiment with other models for Amharic SER."
Exploiting Global and Local Hierarchies for Hierarchical Text Classification,0.128213,"Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets."
3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization,0.156893,"Although deep-learning based methods for monocular pedestrian detection have
made great progress, they are still vulnerable to heavy occlusions. Using
multi-view information fusion is a potential solution but has limited
applications, due to the lack of annotated training samples in existing
multi-view datasets, which increases the risk of overfitting. To address this
problem, a data augmentation method is proposed to randomly generate 3D
cylinder occlusions, on the ground plane, which are of the average size of
pedestrians and projected to multiple views, to relieve the impact of
overfitting in the training. Moreover, the feature map of each view is
projected to multiple parallel planes at different heights, by using
homographies, which allows the CNNs to fully utilize the features across the
height of each pedestrian to infer the locations of pedestrians on the ground
plane. The proposed 3DROM method has a greatly improved performance in
comparison with the state-of-the-art deep-learning based methods for multi-view
pedestrian detection."
Non-Autoregressive Machine Translation: It's Not as Fast as it Seems,0.140265,"Efficient machine translation models are commercially important as they can
increase inference speeds, and reduce costs and carbon emissions. Recently,
there has been much interest in non-autoregressive (NAR) models, which promise
faster translation. In parallel to the research on NAR models, there have been
successful attempts to create optimized autoregressive models as part of the
WMT shared task on efficient translation. In this paper, we point out flaws in
the evaluation methodology present in the literature on NAR models and we
provide a fair comparison between a state-of-the-art NAR model and the
autoregressive submissions to the shared task. We make the case for consistent
evaluation of NAR models, and also for the importance of comparing NAR models
with other widely used methods for improving efficiency. We run experiments
with a connectionist-temporal-classification-based (CTC) NAR model implemented
in C++ and compare it with AR models using wall clock times. Our results show
that, although NAR models are faster on GPUs, with small batch sizes, they are
almost always slower under more realistic usage conditions. We call for more
realistic and extensive evaluation of NAR models in future work."
Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection,0.139308,"Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of
aspect-based sentiment analysis, which aims to detect aspect categories
accurately with limited training instances. Recently, dominant works use the
prototypical network to accomplish this task, and employ the attention
mechanism to extract keywords of aspect category from the sentences to produce
the prototype for each aspect. However, they still suffer from serious noise
problems: (1) due to lack of sufficient supervised data, the previous methods
easily catch noisy words irrelevant to the current aspect category, which
largely affects the quality of the generated prototype; (2) the
semantically-close aspect categories usually generate similar prototypes, which
are mutually noisy and confuse the classifier seriously. In this paper, we
resort to the label information of each aspect to tackle the above problems,
along with proposing a novel Label-Driven Denoising Framework (LDF). Extensive
experimental results show that our framework achieves better performance than
other state-of-the-art methods."
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,0.221344,"Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation studies for summarization either exhibit a low inter-annotator
agreement or have insufficient scale, and an in-depth analysis of human
evaluation is lacking. Therefore, we address the shortcomings of existing
summarization evaluation along the following axes: (1) We propose a modified
summarization salience protocol, Atomic Content Units (ACUs), which is based on
fine-grained semantic units and allows for a high inter-annotator agreement.
(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large
human evaluation dataset consisting of 22,000 summary-level annotations over 28
top-performing systems on three datasets. (3) We conduct a comparative study of
four human evaluation protocols, underscoring potential confounding factors in
evaluation setups. (4) We evaluate 50 automatic metrics and their variants
using the collected human annotations across evaluation protocols and
demonstrate how our benchmark leads to more statistically stable and
significant results. The metrics we benchmarked include recent methods based on
large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings
have important implications for evaluating LLMs, as we show that LLMs adjusted
by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,
which is affected by the annotators' prior, input-agnostic preferences, calling
for more robust, targeted evaluation methods."
Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations,0.132106,"We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh."
BBA-net: A bi-branch attention network for crowd counting,0.161621,"In the field of crowd counting, the current mainstream CNN-based regression
methods simply extract the density information of pedestrians without finding
the position of each person. This makes the output of the network often found
to contain incorrect responses, which may erroneously estimate the total number
and not conducive to the interpretation of the algorithm. To this end, we
propose a Bi-Branch Attention Network (BBA-NET) for crowd counting, which has
three innovation points. i) A two-branch architecture is used to estimate the
density information and location information separately. ii) Attention
mechanism is used to facilitate feature extraction, which can reduce false
responses. iii) A new density map generation method combining geometric
adaptation and Voronoi split is introduced. Our method can integrate the
pedestrian's head and body information to enhance the feature expression
ability of the density map. Extensive experiments performed on two public
datasets show that our method achieves a lower crowd counting error compared to
other state-of-the-art methods."
Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition,0.180669,"Subject-invariant facial action unit (AU) recognition remains challenging for
the reason that the data distribution varies among subjects. In this paper, we
propose a causal inference framework for subject-invariant facial action unit
recognition. To illustrate the causal effect existing in AU recognition task,
we formulate the causalities among facial images, subjects, latent AU semantic
relations, and estimated AU occurrence probabilities via a structural causal
model. By constructing such a causal diagram, we clarify the causal effect
among variables and propose a plug-in causal intervention module, CIS, to
deconfound the confounder \emph{Subject} in the causal diagram. Extensive
experiments conducted on two commonly used AU benchmark datasets, BP4D and
DISFA, show the effectiveness of our CIS, and the model with CIS inserted,
CISNet, has achieved state-of-the-art performance."
Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities,0.161478,"As for other forms of AI, speech recognition has recently been examined with
respect to performance disparities across different user cohorts. One approach
to achieve fairness in speech recognition is to (1) identify speaker cohorts
that suffer from subpar performance and (2) apply fairness mitigation measures
targeting the cohorts discovered. In this paper, we report on initial findings
with both discovery and mitigation of performance disparities using data from a
product-scale AI assistant speech recognition system. We compare cohort
discovery based on geographic and demographic information to a more scalable
method that groups speakers without human labels, using speaker embedding
technology. For fairness mitigation, we find that oversampling of
underrepresented cohorts, as well as modeling speaker cohort membership by
additional input variables, reduces the gap between top- and bottom-performing
cohorts, without deteriorating overall recognition accuracy."
nerf2nerf: Pairwise Registration of Neural Radiance Fields,0.132844,"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io"
Standing on the Shoulders of Giant Frozen Language Models,0.177088,"Huge pretrained language models (LMs) have demonstrated surprisingly good
zero-shot capabilities on a wide variety of tasks. This gives rise to the
appealing vision of a single, versatile model with a wide range of
functionalities across disparate applications. However, current leading
techniques for leveraging a ""frozen"" LM -- i.e., leaving its weights untouched
-- still often underperform fine-tuning approaches which modify these weights
in a task-dependent way. Those, in turn, suffer forgetfulness and compromise
versatility, suggesting a tradeoff between performance and versatility. The
main message of this paper is that current frozen-model techniques such as
prompt tuning are only the tip of the iceberg, and more powerful methods for
leveraging frozen LMs can do just as well as fine tuning in challenging domains
without sacrificing the underlying model's versatility. To demonstrate this, we
introduce three novel methods for leveraging frozen models: input-dependent
prompt tuning, frozen readers, and recursive LMs, each of which vastly improves
on current frozen-model approaches. Indeed, some of our methods even outperform
fine-tuning approaches in domains currently dominated by the latter. The
computational cost of each method is higher than that of existing frozen model
methods, but still negligible relative to a single pass through a huge frozen
LM. Each of these methods constitutes a meaningful contribution in its own
right, but by presenting these contributions together we aim to convince the
reader of a broader message that goes beyond the details of any given method:
that frozen models have untapped potential and that fine-tuning is often
unnecessary."
Towards customizable reinforcement learning agents: Enabling preference specification through online vocabulary expansion,0.138389,"There is a growing interest in developing automated agents that can work
alongside humans. In addition to completing the assigned task, such an agent
will undoubtedly be expected to behave in a manner that is preferred by the
human. This requires the human to communicate their preferences to the agent.
To achieve this, the current approaches either require the users to specify the
reward function or the preference is interactively learned from queries that
ask the user to compare behavior. The former approach can be challenging if the
internal representation used by the agent is inscrutable to the human while the
latter is unnecessarily cumbersome for the user if their preference can be
specified more easily in symbolic terms. In this work, we propose PRESCA
(PREference Specification through Concept Acquisition), a system that allows
users to specify their preferences in terms of concepts that they understand.
PRESCA maintains a set of such concepts in a shared vocabulary. If the relevant
concept is not in the shared vocabulary, then it is learned. To make learning a
new concept more feedback efficient, PRESCA leverages causal associations
between the target concept and concepts that are already known. In addition, we
use a novel data augmentation approach to further reduce required feedback. We
evaluate PRESCA by using it on a Minecraft environment and show that it can
effectively align the agent with the user's preference."
YORO -- Lightweight End to End Visual Grounding,0.207477,"We present YORO - a multi-modal transformer encoder-only architecture for the
Visual Grounding (VG) task. This task involves localizing, in an image, an
object referred via natural language. Unlike the recent trend in the literature
of using multi-stage approaches that sacrifice speed for accuracy, YORO seeks a
better trade-off between speed an accuracy by embracing a single-stage design,
without CNN backbone. YORO consumes natural language queries, image patches,
and learnable detection tokens and predicts coordinates of the referred object,
using a single transformer encoder. To assist the alignment between text and
visual objects, a novel patch-text alignment loss is proposed. Extensive
experiments are conducted on 5 different datasets with ablations on
architecture design choices. YORO is shown to support real-time inference and
outperform all approaches in this class (single-stage methods) by large
margins. It is also the fastest VG model and achieves the best speed/accuracy
trade-off in the literature."
Will Large-scale Generative Models Corrupt Future Datasets?,0.194836,"Recently proposed large-scale text-to-image generative models such as
DALL$\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and
realistic images from users' prompts. Not limited to the research community,
ordinary Internet users enjoy these generative models, and consequently, a
tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a
lot to images collected from the Internet. These trends lead us to a research
question: ""\textbf{will such generated images impact the quality of future
datasets and the performance of computer vision models positively or
negatively?}"" This paper empirically answers this question by simulating
contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using
a state-of-the-art generative model and evaluate models trained with
""contaminated"" datasets on various tasks, including image classification and
image generation. Throughout experiments, we conclude that generated images
negatively affect downstream performance, while the significance depends on
tasks and the amount of generated images. The generated datasets and the codes
for experiments will be publicly released for future research. Generated
datasets and source codes are available from
\url{https://github.com/moskomule/dataset-contamination}."
PaCo: Parameter-Compositional Multi-Task Reinforcement Learning,0.127767,"The purpose of multi-task reinforcement learning (MTRL) is to train a single
policy that can be applied to a set of different tasks. Sharing parameters
allows us to take advantage of the similarities among tasks. However, the gaps
between contents and difficulties of different tasks bring us challenges on
both which tasks should share the parameters and what parameters should be
shared, as well as the optimization challenges due to parameter sharing. In
this work, we introduce a parameter-compositional approach (PaCo) as an attempt
to address these challenges. In this framework, a policy subspace represented
by a set of parameters is learned. Policies for all the single tasks lie in
this subspace and can be composed by interpolating with the learned set. It
allows not only flexible parameter sharing but also a natural way to improve
training. We demonstrate the state-of-the-art performance on Meta-World
benchmarks, verifying the effectiveness of the proposed approach."
Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma Segmentation and Koos Grade Prediction based on Semi-Supervised Contrastive Learning,0.121909,"Domain adaptation has been widely adopted to transfer styles across
multi-vendors and multi-centers, as well as to complement the missing
modalities. In this challenge, we proposed an unsupervised domain adaptation
framework for cross-modality vestibular schwannoma (VS) and cochlea
segmentation and Koos grade prediction. We learn the shared representation from
both ceT1 and hrT2 images and recover another modality from the latent
representation, and we also utilize proxy tasks of VS segmentation and brain
parcellation to restrict the consistency of image structures in domain
adaptation. After generating missing modalities, the nnU-Net model is utilized
for VS and cochlea segmentation, while a semi-supervised contrastive learning
pre-train approach is employed to improve the model performance for Koos grade
prediction. On CrossMoDA validation phase Leaderboard, our method received rank
4 in task1 with a mean Dice score of 0.8394 and rank 2 in task2 with
Macro-Average Mean Square Error of 0.3941. Our code is available at
https://github.com/fiy2W/cmda2022.superpolymerization."
OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers,0.160982,"We present OSFormer, the first one-stage transformer framework for
camouflaged instance segmentation (CIS). OSFormer is based on two key designs.
First, we design a location-sensing transformer (LST) to obtain the location
label and instance-aware parameters by introducing the location-guided queries
and the blend-convolution feedforward network. Second, we develop a
coarse-to-fine fusion (CFF) to merge diverse context information from the LST
encoder and CNN backbone. Coupling these two components enables OSFormer to
efficiently blend local features and long-range context dependencies for
predicting camouflaged instances. Compared with two-stage frameworks, our
OSFormer reaches 41% AP and achieves good convergence efficiency without
requiring enormous training data, i.e., only 3,040 samples under 60 epochs.
Code link: https://github.com/PJLallen/OSFormer."
TensorIR: An Abstraction for Automatic Tensorized Program Optimization,0.192128,"Deploying deep learning models on various devices has become an important
topic. The wave of hardware specialization brings a diverse set of acceleration
primitives for multi-dimensional tensor computations. These new acceleration
primitives, along with the emerging machine learning models, bring tremendous
engineering challenges. In this paper, we present TensorIR, a compiler
abstraction for optimizing programs with these tensor computation primitives.
TensorIR generalizes the loop nest representation used in existing machine
learning compilers to bring tensor computation as the first-class citizen.
Finally, we build an end-to-end framework on top of our abstraction to
automatically optimize deep learning models for given tensor computation
primitives. Experimental results show that TensorIR compilation automatically
uses the tensor computation primitives for given hardware backends and delivers
performance that is competitive to state-of-art hand-optimized systems across
platforms."
Automatic Generation of Factual News Headlines in Finnish,0.13934,"We present a novel approach to generating news headlines in Finnish for a
given news story. We model this as a summarization task where a model is given
a news article, and its task is to produce a concise headline describing the
main topic of the article. Because there are no openly available GPT-2 models
for Finnish, we will first build such a model using several corpora. The model
is then fine-tuned for the headline generation task using a massive news
corpus. The system is evaluated by 3 expert journalists working in a Finnish
media house. The results showcase the usability of the presented approach as a
headline suggestion tool to facilitate the news production process."
Keypoint Cascade Voting for Point Cloud Based 6DoF Pose Estimation,0.220729,"We propose a novel keypoint voting 6DoF object pose estimation method, which
takes pure unordered point cloud geometry as input without RGB information. The
proposed cascaded keypoint voting method, called RCVPose3D, is based upon a
novel architecture which separates the task of semantic segmentation from that
of keypoint regression, thereby increasing the effectiveness of both and
improving the ultimate performance. The method also introduces a pairwise
constraint in between different keypoints to the loss function when regressing
the quantity for keypoint estimation, which is shown to be effective, as well
as a novel Voter Confident Score which enhances both the learning and inference
stages. Our proposed RCVPose3D achieves state-of-the-art performance on the
Occlusion LINEMOD (74.5%) and YCB-Video (96.9%) datasets, outperforming
existing pure RGB and RGB-D based methods, as well as being competitive with
RGB plus point cloud methods."
VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension,0.183473,"One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference."
Seeing a Rose in Five Thousand Ways,0.114308,"What is a rose, visually? A rose comprises its intrinsics, including the
distribution of geometry, texture, and material specific to its object
category. With knowledge of these intrinsic properties, we may render roses of
different sizes and shapes, in different poses, and under different lighting
conditions. In this work, we build a generative model that learns to capture
such object intrinsics from a single image, such as a photo of a bouquet. Such
an image includes multiple instances of an object type. These instances all
share the same intrinsics, but appear different due to a combination of
variance within these intrinsics and differences in extrinsic factors, such as
pose and illumination. Experiments show that our model successfully learns
object intrinsics (distribution of geometry, texture, and material) for a wide
range of objects, each from a single Internet image. Our method achieves
superior results on multiple downstream tasks, including intrinsic image
decomposition, shape and image generation, view synthesis, and relighting."
SQuId: Measuring Speech Naturalness in Many Languages,0.196008,"Much of text-to-speech research relies on human evaluation, which incurs
heavy costs and slows down the development process. The problem is particularly
acute in heavily multilingual applications, where recruiting and polling judges
can take weeks. We introduce SQuId (Speech Quality Identification), a
multilingual naturalness prediction model trained on over a million ratings and
tested in 65 locales-the largest effort of this type to date. The main insight
is that training one model on many locales consistently outperforms mono-locale
baselines. We present our task, the model, and show that it outperforms a
competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then
demonstrate the effectiveness of cross-locale transfer during fine-tuning and
highlight its effect on zero-shot locales, i.e., locales for which there is no
fine-tuning data. Through a series of analyses, we highlight the role of
non-linguistic effects such as sound artifacts in cross-locale transfer.
Finally, we present the effect of our design decision, e.g., model size,
pre-training diversity, and language rebalancing with several ablation
experiments."
Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR),0.157228,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys."
Few-shot Named Entity Recognition with Self-describing Networks,0.149295,"Few-shot NER needs to effectively capture information from limited instances
and transfer useful knowledge from external resources. In this paper, we
propose a self-describing mechanism for few-shot NER, which can effectively
leverage illustrative instances and precisely transfer knowledge from external
resources by describing both entity types and mentions using a universal
concept set. Specifically, we design Self-describing Networks (SDNet), a
Seq2Seq generation model which can universally describe mentions using
concepts, automatically map novel entity types to concepts, and adaptively
recognize entities on-demand. We pre-train SDNet with large-scale corpus, and
conduct experiments on 8 benchmarks from different domains. Experiments show
that SDNet achieves competitive performances on all benchmarks and achieves the
new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and
robustness."
Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models,0.111532,"Massively Multilingual Transformer based Language Models have been observed
to be surprisingly effective on zero-shot transfer across languages, though the
performance varies from language to language depending on the pivot language(s)
used for fine-tuning. In this work, we build upon some of the existing
techniques for predicting the zero-shot performance on a task, by modeling it
as a multi-task learning problem. We jointly train predictive models for
different tasks which helps us build more accurate predictors for tasks where
we have test data in very few languages to measure the actual performance of
the model. Our approach also lends us the ability to perform a much more robust
feature selection and identify a common set of features that influence
zero-shot performance across a variety of tasks."
Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis,0.115337,"Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net."
AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,0.204335,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate."
PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation,0.160137,"In this paper, we propose a new deep learning-based method for estimating
room layout given a pair of 360 panoramas. Our system, called Position-aware
Stereo Merging Network or PSMNet, is an end-to-end joint layout-pose estimator.
PSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel
Cross-Perspective Projection (CP2) layer. The stereo-view SP2 transformer is
used to implicitly infer correspondences between views, and can handle noisy
poses. The pose-aware CP2 layer is designed to render features from the
adjacent view to the anchor (reference) view, in order to perform view fusion
and estimate the visible layout. Our experiments and analysis validate our
method, which significantly outperforms the state-of-the-art layout estimators,
especially for large and complex room spaces."
A Human-Centric Assessment Framework for AI,0.153976,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner."
That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory,0.12289,"The evolution of language follows the rule of gradual change. Grammar,
vocabulary, and lexical semantic shifts take place over time, resulting in a
diachronic linguistic gap. As such, a considerable amount of texts are written
in languages of different eras, which creates obstacles for natural language
processing tasks, such as word segmentation and machine translation. Although
the Chinese language has a long history, previous Chinese natural language
processing research has primarily focused on tasks within a specific era.
Therefore, we propose a cross-era learning framework for Chinese word
segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to
incorporate era-specific linguistic knowledge. Experiments on four corpora from
different eras show that the performance of each corpus significantly improves.
Further analyses also demonstrate that the SM can effectively integrate the
knowledge of the eras into the neural network."
Defending Black-box Skeleton-based Human Activity Classifiers,0.113883,"Skeletal motions have been heavily replied upon for human activity
recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR
has been identified across a variety of classifiers and data, calling for
mitigation. To this end, we propose the first black-box defense method for
skeleton-based HAR to our best knowledge. Our method is featured by full
Bayesian treatments of the clean data, the adversaries and the classifier,
leading to (1) a new Bayesian Energy-based formulation of robust discriminative
classifiers, (2) a new adversary sampling scheme based on natural motion
manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We
name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is
straightforward but elegant, which turns vulnerable black-box classifiers into
robust ones without sacrificing accuracy. It demonstrates surprising and
universal effectiveness across a wide range of skeletal HAR classifiers and
datasets, under various attacks. Code is available at
https://github.com/realcrane/RobustActionRecogniser."
ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer,0.113318,"The problem of multimodal intent and trajectory prediction for human-driven
vehicles in parking lots is addressed in this paper. Using models designed with
CNN and Transformer networks, we extract temporal-spatial and contextual
information from trajectory history and local bird's eye view (BEV) semantic
images, and generate predictions about intent distribution and future
trajectory sequences. Our methods outperform existing models in accuracy, while
allowing an arbitrary number of modes, encoding complex multi-agent scenarios,
and adapting to different parking maps. To train and evaluate our method, we
present the first public 4K video dataset of human driving in parking lots with
accurate annotation, high frame rate, and rich traffic scenarios."
Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations,0.13895,"Syntactically controlled paraphrase generation has become an emerging
research direction in recent years. Most existing approaches require annotated
paraphrase pairs for training and are thus costly to extend to new domains.
Unsupervised approaches, on the other hand, do not need paraphrase pairs but
suffer from relatively poor performance in terms of syntactic control and
quality of generated paraphrases. In this paper, we demonstrate that leveraging
Abstract Meaning Representations (AMR) can greatly improve the performance of
unsupervised syntactically controlled paraphrase generation. Our proposed
model, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR
graph and the constituency parse of the input sentence into two disentangled
semantic and syntactic embeddings. A decoder is then learned to reconstruct the
input sentence from the semantic and syntactic embeddings. Our experiments show
that AMRPG generates more accurate syntactically controlled paraphrases, both
quantitatively and qualitatively, compared to the existing unsupervised
approaches. We also demonstrate that the paraphrases generated by AMRPG can be
used for data augmentation to improve the robustness of NLP models."
Matching Tweets With Applicable Fact-Checks Across Languages,0.16573,"An important challenge for news fact-checking is the effective dissemination
of existing fact-checks. This in turn brings the need for reliable methods to
detect previously fact-checked claims. In this paper, we focus on automatically
finding existing fact-checks for claims made in social media posts (tweets). We
conduct both classification and retrieval experiments, in monolingual (English
only), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)
settings using multilingual transformer models such as XLM-RoBERTa and
multilingual embeddings such as LaBSE and SBERT. We present promising results
for ""match"" classification (86% average accuracy) in four language pairs. We
also find that a BM25 baseline outperforms or is on par with state-of-the-art
multilingual embedding models for the retrieval task during our monolingual
experiments. We highlight and discuss NLP challenges while addressing this
problem in different languages, and we introduce a novel curated dataset of
fact-checks and corresponding tweets for future research."
AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,0.149081,"Crowdsourced dialogue corpora are usually limited in scale and topic coverage
due to the expensive cost of data curation. This would hinder the
generalization of downstream dialogue models to open-domain topics. In this
work, we leverage large language models for dialogue augmentation in the task
of emotional support conversation (ESC). By treating dialogue augmentation as a
dialogue completion task, we prompt a fine-tuned language model to complete
full dialogues from available dialogue posts of various topics, which are then
postprocessed based on heuristics. Applying this approach, we construct AugESC,
an augmented dataset for the ESC task, which largely extends the scale and
topic coverage of the crowdsourced ESConv corpus. Through comprehensive human
evaluation, we demonstrate that our approach is superior to strong baselines of
dialogue augmentation and that AugESC has comparable dialogue quality to the
crowdsourced corpus. We also conduct human interactive evaluation and prove
that post-training on AugESC improves downstream dialogue models'
generalization ability to open-domain topics. These results suggest the utility
of AugESC and highlight the potential of large language models in improving
data-scarce dialogue generation tasks."
On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,0.192512,"Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance."
Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs,0.132687,"Most existing deep neural networks (DNNs) are easily disturbed by slight
noise. However, there are few researches on physical attacks by deploying
lighting equipment. The light-based physical attacks has excellent covertness,
which brings great security risks to many vision-based applications (such as
self-driving). Therefore, we propose a light-based physical attack, called
adversarial laser spot (AdvLS), which optimizes the physical parameters of
laser spots through genetic algorithm to perform physical attacks. It realizes
robust and covert physical attack by using low-cost laser equipment. As far as
we know, AdvLS is the first light-based physical attack that perform physical
attacks in the daytime. A large number of experiments in the digital and
physical environments show that AdvLS has excellent robustness and covertness.
In addition, through in-depth analysis of the experimental data, we find that
the adversarial perturbations generated by AdvLS have superior adversarial
attack migration. The experimental results show that AdvLS impose serious
interference to advanced DNNs, we call for the attention of the proposed AdvLS.
The code of AdvLS is available at: https://github.com/ChengYinHu/AdvLS"
Self-supervised Image Clustering from Multiple Incomplete Views via Constrastive Complementary Generation,0.130021,"Incomplete Multi-View Clustering aims to enhance clustering performance by
using data from multiple modalities. Despite the fact that several approaches
for studying this issue have been proposed, the following drawbacks still
persist: 1) It's difficult to learn latent representations that account for
complementarity yet consistency without using label information; 2) and thus
fails to take full advantage of the hidden information in incomplete data
results in suboptimal clustering performance when complete data is scarce. In
this paper, we propose Contrastive Incomplete Multi-View Image Clustering with
Generative Adversarial Networks (CIMIC-GAN), which uses GAN to fill in
incomplete data and uses double contrastive learning to learn consistency on
complete and incomplete data. More specifically, considering diversity and
complementary information among multiple modalities, we incorporate
autoencoding representation of complete and incomplete data into double
contrastive learning to achieve learning consistency. Integrating GANs into the
autoencoding process can not only take full advantage of new features of
incomplete data, but also better generalize the model in the presence of high
data missing rates. Experiments conducted on \textcolor{black}{four}
extensively-used datasets show that CIMIC-GAN outperforms state-of-the-art
incomplete multi-View clustering methods."
Arbitrary Point Cloud Upsampling with Spherical Mixture of Gaussians,0.12227,"Generating dense point clouds from sparse raw data benefits downstream 3D
understanding tasks, but existing models are limited to a fixed upsampling
ratio or to a short range of integer values. In this paper, we present
APU-SMOG, a Transformer-based model for Arbitrary Point cloud Upsampling (APU).
The sparse input is firstly mapped to a Spherical Mixture of Gaussians (SMOG)
distribution, from which an arbitrary number of points can be sampled. Then,
these samples are fed as queries to the Transformer decoder, which maps them
back to the target surface. Extensive qualitative and quantitative evaluations
show that APU-SMOG outperforms state-of-the-art fixed-ratio methods, while
effectively enabling upsampling with any scaling factor, including non-integer
values, with a single trained model. The code is available at
https://github.com/apusmog/apusmog/"
Active Self-Training for Weakly Supervised 3D Scene Semantic Segmentation,0.155596,"Since the preparation of labeled data for training semantic segmentation
networks of point clouds is a time-consuming process, weakly supervised
approaches have been introduced to learn from only a small fraction of data.
These methods are typically based on learning with contrastive losses while
automatically deriving per-point pseudo-labels from a sparse set of
user-annotated labels. In this paper, our key observation is that the selection
of what samples to annotate is as important as how these samples are used for
training. Thus, we introduce a method for weakly supervised segmentation of 3D
scenes that combines self-training with active learning. The active learning
selects points for annotation that likely result in performance improvements to
the trained model, while the self-training makes efficient use of the
user-provided labels for learning the model. We demonstrate that our approach
leads to an effective method that provides improvements in scene segmentation
over previous works and baselines, while requiring only a small number of user
annotations."
TYPIC: A Corpus of Template-Based Diagnostic Comments on Argumentation,0.115804,"Providing feedback on the argumentation of the learner is essential for
developing critical thinking skills, however, it requires a lot of time and
effort. To mitigate the overload on teachers, we aim to automate a process of
providing feedback, especially giving diagnostic comments which point out the
weaknesses inherent in the argumentation. It is recommended to give specific
diagnostic comments so that learners can recognize the diagnosis without
misinterpretation. However, it is not obvious how the task of providing
specific diagnostic comments should be formulated. We present a formulation of
the task as template selection and slot filling to make an automatic evaluation
easier and the behavior of the model more tractable. The key to the formulation
is the possibility of creating a template set that is sufficient for practical
use. In this paper, we define three criteria that a template set should
satisfy: expressiveness, informativeness, and uniqueness, and verify the
feasibility of creating a template set that satisfies these criteria as a first
trial. We will show that it is feasible through an annotation study that
converts diagnostic comments given in a text to a template format. The corpus
used in the annotation study is publicly available."
Features Fusion Framework for Multimodal Irregular Time-series Events,0.132188,"Some data from multiple sources can be modeled as multimodal time-series
events which have different sampling frequencies, data compositions, temporal
relations and characteristics. Different types of events have complex nonlinear
relationships, and the time of each event is irregular. Neither the classical
Recurrent Neural Network (RNN) model nor the current state-of-the-art
Transformer model can deal with these features well. In this paper, a features
fusion framework for multimodal irregular time-series events is proposed based
on the Long Short-Term Memory networks (LSTM). Firstly, the complex features
are extracted according to the irregular patterns of different events.
Secondly, the nonlinear correlation and complex temporal dependencies
relationship between complex features are captured and fused into a tensor.
Finally, a feature gate are used to control the access frequency of different
tensors. Extensive experiments on MIMIC-III dataset demonstrate that the
proposed framework significantly outperforms to the existing methods in terms
of AUC (the area under Receiver Operating Characteristic curve) and AP (Average
Precision)."
Neural Cloth Simulation,0.190283,"We present a general framework for the garment animation problem through
unsupervised deep learning inspired in physically based simulation. Existing
trends in the literature already explore this possibility. Nonetheless, these
approaches do not handle cloth dynamics. Here, we propose the first methodology
able to learn realistic cloth dynamics unsupervisedly, and henceforth, a
general formulation for neural cloth simulation. The key to achieve this is to
adapt an existing optimization scheme for motion from simulation based
methodologies to deep learning. Then, analyzing the nature of the problem, we
devise an architecture able to automatically disentangle static and dynamic
cloth subspaces by design. We will show how this improves model performance.
Additionally, this opens the possibility of a novel motion augmentation
technique that greatly improves generalization. Finally, we show it also allows
to control the level of motion in the predictions. This is a useful, never seen
before, tool for artists. We provide of detailed analysis of the problem to
establish the bases of neural cloth simulation and guide future research into
the specifics of this domain."
Generative Category-Level Shape and Pose Estimation with Semantic Primitives,0.13557,"Empowering autonomous agents with 3D understanding for daily objects is a
grand challenge in robotics applications. When exploring in an unknown
environment, existing methods for object pose estimation are still not
satisfactory due to the diversity of object shapes. In this paper, we propose a
novel framework for category-level object shape and pose estimation from a
single RGB-D image. To handle the intra-category variation, we adopt a semantic
primitive representation that encodes diverse shapes into a unified latent
space, which is the key to establish reliable correspondences between observed
point clouds and estimated shapes. Then, by using a SIM(3)-invariant shape
descriptor, we gracefully decouple the shape and pose of an object, thus
supporting latent shape optimization of target objects in arbitrary poses.
Extensive experiments show that the proposed method achieves SOTA pose
estimation performance and better generalization in the real-world dataset.
Code and video are available at https://zju3dv.github.io/gCasp."
Misspelling Semantics In Thai,0.135371,"User-generated content is full of misspellings. Rather than being just random
noise, we hypothesise that many misspellings contain hidden semantics that can
be leveraged for language understanding tasks. This paper presents a
fine-grained annotated corpus of misspelling in Thai, together with an analysis
of misspelling intention and its possible semantics to get a better
understanding of the misspelling patterns observed in the corpus. In addition,
we introduce two approaches to incorporate the semantics of misspelling:
Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST).
Experiments on a sentiment analysis task confirm our overall hypothesis:
additional semantics from misspelling can boost the micro F1 score up to
0.4-2%, while blindly normalising misspelling is harmful and suboptimal."
Skit-S2I: An Indian Accented Speech to Intent dataset,0.125586,"Conventional conversation assistants extract text transcripts from the speech
signal using automatic speech recognition (ASR) and then predict intent from
the transcriptions. Using end-to-end spoken language understanding (SLU), the
intents of the speaker are predicted directly from the speech signal without
requiring intermediate text transcripts. As a result, the model can optimize
directly for intent classification and avoid cascading errors from ASR. The
end-to-end SLU system also helps in reducing the latency of the intent
prediction model. Although many datasets are available publicly for
text-to-intent tasks, the availability of labeled speech-to-intent datasets is
limited, and there are no datasets available in the Indian accent. In this
paper, we release the Skit-S2I dataset, the first publicly available
Indian-accented SLU dataset in the banking domain in a conversational tonality.
We experiment with multiple baselines, compare different pretrained speech
encoder's representations, and find that SSL pretrained representations perform
slightly better than ASR pretrained representations lacking prosodic features
for speech-to-intent classification. The dataset and baseline code is available
at \url{https://github.com/skit-ai/speech-to-intent-dataset}"
C-Pack of IPAs: A C90 Program Benchmark of Introductory Programming Assignments,0.13291,"Due to the vast number of students enrolled in Massive Open Online Courses
(MOOCs), there has been an increasing number of automated program repair
techniques focused on introductory programming assignments (IPAs). Such
techniques take advantage of previous correct student implementations in order
to provide automated, comprehensive, and personalized feedback to students.
  This paper presents C-Pack-IPAs, a publicly available benchmark of students'
programs submitted for 25 different IPAs. C-Pack-IPAs contains semantically
correct, semantically incorrect, and syntactically incorrect programs plus a
test suite for each IPA. Hence, C-Pack-IPAs can be used to help evaluate the
development of novel semantic, as well as syntactic, automated program repair
frameworks, focused on providing feedback to novice programmers."
Selective Annotation Makes Language Models Better Few-Shot Learners,0.20881,"Many recent approaches to natural language tasks are built on the remarkable
abilities of large language models. Large language models can perform
in-context learning, where they learn a new task from a few task
demonstrations, without any parameter updates. This work examines the
implications of in-context learning for the creation of datasets for new
natural language tasks. Departing from recent in-context learning methods, we
formulate an annotation-efficient, two-step framework: selective annotation
that chooses a pool of examples to annotate from unlabeled data in advance,
followed by prompt retrieval that retrieves task examples from the annotated
pool at test time. Based on this framework, we propose an unsupervised,
graph-based selective annotation method, voke-k, to select diverse,
representative examples to annotate. Extensive experiments on 10 datasets
(covering classification, commonsense reasoning, dialogue, and text/code
generation) demonstrate that our selective annotation method improves the task
performance by a large margin. On average, vote-k achieves a 12.9%/11.4%
relative gain under an annotation budget of 18/100, as compared to randomly
selecting examples to annotate. Compared to state-of-the-art supervised
finetuning approaches, it yields similar performance with 10-100x less
annotation cost across 10 tasks. We further analyze the effectiveness of our
framework in various scenarios: language models with varying sizes, alternative
selective annotation methods, and cases where there is a test data domain
shift. We hope that our studies will serve as a basis for data annotations as
large language models are increasingly applied to new tasks. Our code is
available at https://github.com/HKUNLP/icl-selective-annotation."
Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation,0.149114,"Continual relation extraction (CRE) aims to continually learn new relations
from a class-incremental data stream. CRE model usually suffers from
catastrophic forgetting problem, i.e., the performance of old relations
seriously degrades when the model learns new relations. Most previous work
attributes catastrophic forgetting to the corruption of the learned
representations as new relations come, with an implicit assumption that the CRE
models have adequately learned the old relations. In this paper, through
empirical studies we argue that this assumption may not hold, and an important
reason for catastrophic forgetting is that the learned representations do not
have good robustness against the appearance of analogous relations in the
subsequent learning process. To address this issue, we encourage the model to
learn more precise and robust representations through a simple yet effective
adversarial class augmentation mechanism (ACA), which is easy to implement and
model-agnostic. Experimental results show that ACA can consistently improve the
performance of state-of-the-art CRE models on two popular benchmarks."
Self-supervised models of audio effectively explain human cortical responses to speech,0.215203,"Self-supervised language models are very effective at predicting high-level
cortical responses during language comprehension. However, the best current
models of lower-level auditory processing in the human brain rely on either
hand-constructed acoustic filters or representations from supervised audio
neural networks. In this work, we capitalize on the progress of self-supervised
speech representation learning (SSL) to create new state-of-the-art models of
the human auditory system. Compared against acoustic baselines, phonemic
features, and supervised models, representations from the middle layers of
self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently
yield the best prediction performance for fMRI recordings within the auditory
cortex (AC). Brain areas involved in low-level auditory processing exhibit a
preference for earlier SSL model layers, whereas higher-level semantic areas
prefer later layers. We show that these trends are due to the models' ability
to encode information at multiple linguistic levels (acoustic, phonetic, and
lexical) along their representation depth. Overall, these results show that
self-supervised models effectively capture the hierarchy of information
relevant to different stages of speech processing in human cortex."
Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling,0.116834,"To capture the relationship between samples and labels, conditional
generative models often inherit spurious correlations from the training
dataset. This can result in label-conditional distributions that are imbalanced
with respect to another latent attribute. To mitigate this issue, which we call
spurious causality of conditional generation, we propose a general two-step
strategy. (a) Fairness Intervention (FI): emphasize the minority samples that
are hard to generate due to the spurious correlation in the training dataset.
(b) Corrective Sampling (CS): explicitly filter the generated samples and
ensure that they follow the desired latent attribute distribution. We have
designed the fairness intervention to work for various degrees of supervision
on the spurious attribute, including unsupervised, weakly-supervised, and
semi-supervised scenarios. Our experimental results demonstrate that FICS can
effectively resolve spurious causality of conditional generation across various
datasets."
Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation,0.203218,"In recent years, a number of keyphrase generation (KPG) approaches were
proposed consisting of complex model architectures, dedicated training
paradigms and decoding strategies. In this work, we opt for simplicity and show
how a commonly used seq2seq language model, BART, can be easily adapted to
generate keyphrases from the text in a single batch computation using a simple
training procedure. Empirical results on five benchmarks show that our approach
is as good as the existing state-of-the-art KPG systems, but using a much
simpler and easy to deploy framework."
"Intelligent Computing: The Latest Advances, Challenges and Future",0.143315,"Computing is a critical driving force in the development of human
civilization. In recent years, we have witnessed the emergence of intelligent
computing, a new computing paradigm that is reshaping traditional computing and
promoting digital revolution in the era of big data, artificial intelligence
and internet-of-things with new computing theories, architectures, methods,
systems, and applications. Intelligent computing has greatly broadened the
scope of computing, extending it from traditional computing on data to
increasingly diverse computing paradigms such as perceptual intelligence,
cognitive intelligence, autonomous intelligence, and human-computer fusion
intelligence. Intelligence and computing have undergone paths of different
evolution and development for a long time but have become increasingly
intertwined in recent years: intelligent computing is not only
intelligence-oriented but also intelligence-driven. Such cross-fertilization
has prompted the emergence and rapid advancement of intelligent computing.
Intelligent computing is still in its infancy and an abundance of innovations
in the theories, systems, and applications of intelligent computing are
expected to occur soon. We present the first comprehensive survey of literature
on intelligent computing, covering its theory fundamentals, the technological
fusion of intelligence and computing, important applications, challenges, and
future perspectives. We believe that this survey is highly timely and will
provide a comprehensive reference and cast valuable insights into intelligent
computing for academic and industrial researchers and practitioners."
Bio-inspired Min-Nets Improve the Performance and Robustness of Deep Networks,0.115396,"Min-Nets are inspired by end-stopped cortical cells with units that output
the minimum of two learned filters. We insert such Min-units into
state-of-the-art deep networks, such as the popular ResNet and DenseNet, and
show that the resulting Min-Nets perform better on the Cifar-10 benchmark.
Moreover, we show that Min-Nets are more robust against JPEG compression
artifacts. We argue that the minimum operation is the simplest way of
implementing an AND operation on pairs of filters and that such AND operations
introduce a bias that is appropriate given the statistics of natural images."
Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions,0.203454,"Pruning is one of the predominant approaches for compressing deep neural
networks (DNNs). Lately, coresets (provable data summarizations) were leveraged
for pruning DNNs, adding the advantage of theoretical guarantees on the
trade-off between the compression rate and the approximation error. However,
coresets in this domain were either data-dependent or generated under
restrictive assumptions on both the model's weights and inputs. In real-world
scenarios, such assumptions are rarely satisfied, limiting the applicability of
coresets. To this end, we suggest a novel and robust framework for computing
such coresets under mild assumptions on the model's weights and without any
assumption on the training data. The idea is to compute the importance of each
neuron in each layer with respect to the output of the following layer. This is
achieved by a combination of L\""{o}wner ellipsoid and Caratheodory theorem. Our
method is simultaneously data-independent, applicable to various networks and
datasets (due to the simplified assumptions), and theoretically supported.
Experimental results show that our method outperforms existing coreset based
neural pruning approaches across a wide range of networks and datasets. For
example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet
with $1.09\%$ drop in accuracy."
Spatial Transformer Network on Skeleton-based Gait Recognition,0.198598,"Skeleton-based gait recognition models usually suffer from the robustness
problem, as the Rank-1 accuracy varies from 90\% in normal walking cases to
70\% in walking with coats cases. In this work, we propose a state-of-the-art
robust skeleton-based gait recognition model called Gait-TR, which is based on
the combination of spatial transformer frameworks and temporal convolutional
networks. Gait-TR achieves substantial improvements over other skeleton-based
gait models with higher accuracy and better robustness on the well-known gait
dataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\%
Rank-1 gait recognition accuracy rate, which is higher than the best result of
silhouette-based models, which usually have higher accuracy than the
silhouette-based gait recognition models. Moreover, our experiment on CASIA-B
shows that the spatial transformer can extract gait features from the human
skeleton better than the widely used graph convolutional network."
Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages,0.179412,"Grammatical cues are sometimes redundant with word meanings in natural
language. For instance, English word order rules constrain the word order of a
sentence like ""The dog chewed the bone"" even though the status of ""dog"" as
subject and ""bone"" as object can be inferred from world knowledge and
plausibility. Quantifying how often this redundancy occurs, and how the level
of redundancy varies across typologically diverse languages, can shed light on
the function and evolution of grammar. To that end, we performed a behavioral
experiment in English and Russian and a cross-linguistic computational analysis
measuring the redundancy of grammatical cues in transitive clauses extracted
from corpus text. English and Russian speakers (n=484) were presented with
subjects, verbs, and objects (in random order and with morphological markings
removed) extracted from naturally occurring sentences and were asked to
identify which noun is the subject of the action. Accuracy was high in both
languages (~89% in English, ~87% in Russian). Next, we trained a neural network
machine classifier on a similar task: predicting which nominal in a
subject-verb-object triad is the subject. Across 30 languages from eight
language families, performance was consistently high: a median accuracy of 87%,
comparable to the accuracy observed in the human experiments. The conclusion is
that grammatical cues such as word order are necessary to convey subjecthood
and objecthood in a minority of naturally occurring transitive clauses;
nevertheless, they can (a) provide an important source of redundancy and (b)
are crucial for conveying intended meaning that cannot be inferred from the
words alone, including descriptions of human interactions, where roles are
often reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing
non-prototypical meanings (e.g., ""The bone chewed the dog."")."
Multi-task Learning for Cross-Lingual Sentiment Analysis,0.132072,"This paper presents a cross-lingual sentiment analysis of news articles using
zero-shot and few-shot learning. The study aims to classify the Croatian news
articles with positive, negative, and neutral sentiments using the Slovene
dataset. The system is based on a trilingual BERT-based model trained in three
languages: English, Slovene, Croatian. The paper analyses different setups
using datasets in two languages and proposes a simple multi-task model to
perform sentiment classification. The evaluation is performed using the
few-shot and zero-shot scenarios in single-task and multi-task experiments for
Croatian and Slovene."
A context-aware knowledge transferring strategy for CTC-based ASR,0.130547,"Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method."
Deep Generative Framework for Interactive 3D Terrain Authoring and Manipulation,0.181999,"Automated generation and (user) authoring of the realistic virtual terrain is
most sought for by the multimedia applications like VR models and gaming. The
most common representation adopted for terrain is Digital Elevation Model
(DEM). Existing terrain authoring and modeling techniques have addressed some
of these and can be broadly categorized as: procedural modeling, simulation
method, and example-based methods. In this paper, we propose a novel realistic
terrain authoring framework powered by a combination of VAE and generative
conditional GAN model. Our framework is an example-based method that attempts
to overcome the limitations of existing methods by learning a latent space from
a real-world terrain dataset. This latent space allows us to generate multiple
variants of terrain from a single input as well as interpolate between terrains
while keeping the generated terrains close to real-world data distribution. We
also developed an interactive tool, that lets the user generate diverse
terrains with minimalist inputs. We perform thorough qualitative and
quantitative analysis and provide comparisons with other SOTA methods. We
intend to release our code/tool to the academic community."
DraftRec: Personalized Draft Recommendation for Winning in Multi-Player Online Battle Arena Games,0.214734,"This paper presents a personalized character recommendation system for
Multiplayer Online Battle Arena (MOBA) games which are considered as one of the
most popular online video game genres around the world. When playing MOBA
games, players go through a draft stage, where they alternately select a
virtual character to play. When drafting, players select characters by not only
considering their character preferences, but also the synergy and competence of
their team's character combination. However, the complexity of drafting induces
difficulties for beginners to choose the appropriate characters based on the
characters of their team while considering their own champion preferences. To
alleviate this problem, we propose DraftRec, a novel hierarchical model which
recommends characters by considering each player's champion preferences and the
interaction between the players. DraftRec consists of two networks: the player
network and the match network. The player network captures the individual
player's champion preference, and the match network integrates the complex
relationship between the players and their respective champions. We train and
evaluate our model from a manually collected 280,000 matches of League of
Legends and a publicly available 50,000 matches of Dota2. Empirically, our
method achieved state-of-the-art performance in character recommendation and
match outcome prediction task. Furthermore, a comprehensive user survey
confirms that DraftRec provides convincing and satisfying recommendations. Our
code and dataset are available at https://github.com/dojeon-ai/DraftRec."
A Reinforcement Learning Approach for Electric Vehicle Routing Problem with Vehicle-to-Grid Supply,0.18992,"The use of electric vehicles (EV) in the last mile is appealing from both
sustainability and operational cost perspectives. In addition to the inherent
cost efficiency of EVs, selling energy back to the grid during peak grid
demand, is a potential source of additional revenue to a fleet operator. To
achieve this, EVs have to be at specific locations (discharge points) during
specific points in time (peak period), even while meeting their core purpose of
delivering goods to customers. In this work, we consider the problem of EV
routing with constraints on loading capacity; time window; vehicle-to-grid
energy supply (CEVRPTW-D); which not only satisfy multiple system objectives,
but also scale efficiently to large problem sizes involving hundreds of
customers and discharge stations. We present QuikRouteFinder that uses
reinforcement learning (RL) for EV routing to overcome these challenges. Using
Solomon datasets, results from RL are compared against exact formulations based
on mixed-integer linear program (MILP) and genetic algorithm (GA)
metaheuristics. On an average, the results show that RL is 24 times faster than
MILP and GA, while being close in quality (within 20%) to the optimal."
WR-ONE2SET: Towards Well-Calibrated Keyphrase Generation,0.166182,"Keyphrase generation aims to automatically generate short phrases summarizing
an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021)
generates keyphrases as a set and has achieved competitive performance.
Nevertheless, we observe serious calibration errors outputted by ONE2SET,
especially in the over-estimation of $\varnothing$ token (means ""no
corresponding keyphrase""). In this paper, we deeply analyze this limitation and
identify two main reasons behind: 1) the parallel generation has to introduce
excessive $\varnothing$ as padding tokens into training instances; and 2) the
training mechanism assigning target to each slot is unstable and further
aggravates the $\varnothing$ token over-estimation. To make the model
well-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive
instance-level cost Weighting strategy and a target Re-assignment mechanism.
The former dynamically penalizes the over-estimated slots for different
instances thus smoothing the uneven training distribution. The latter refines
the original inappropriate assignment and reduces the supervisory signals of
over-estimated slots. Experimental results on commonly-used datasets
demonstrate the effectiveness and generality of our proposed paradigm."
A Dynamic Graph Interactive Framework with Label-Semantic Injection for Spoken Language Understanding,0.172606,"Multi-intent detection and slot filling joint models are gaining increasing
traction since they are closer to complicated real-world scenarios. However,
existing approaches (1) focus on identifying implicit correlations between
utterances and one-hot encoded labels in both tasks while ignoring explicit
label characteristics; (2) directly incorporate multi-intent information for
each token, which could lead to incorrect slot prediction due to the
introduction of irrelevant intent. In this paper, we propose a framework termed
DGIF, which first leverages the semantic information of labels to give the
model additional signals and enriched priors. Then, a multi-grain interactive
graph is constructed to model correlations between intents and slots.
Specifically, we propose a novel approach to construct the interactive graph
based on the injection of label semantics, which can automatically update the
graph to better alleviate error propagation. Experimental results show that our
framework significantly outperforms existing approaches, obtaining a relative
improvement of 13.7% over the previous best model on the MixATIS dataset in
overall accuracy."
Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity,0.117484,"This paper analyzes three formal models of Transformer encoders that differ
in the form of their self-attention mechanism: unique hard attention (UHAT);
generalized unique hard attention (GUHAT), which generalizes UHAT; and
averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers,
viewed as string acceptors, can only recognize formal languages in the
complexity class AC$^0$, the class of languages recognizable by families of
Boolean circuits of constant depth and polynomial size. This upper bound
subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages
or the PARITY language, since those languages are outside AC$^0$ (Furst et al.,
1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are
recognizable by AHAT networks, implying that AHAT can recognize languages that
UHAT and GUHAT cannot."
Read Top News First: A Document Reordering Approach for Multi-Document News Summarization,0.182287,"A common method for extractive multi-document news summarization is to
re-formulate it as a single-document summarization problem by concatenating all
documents as a single meta-document. However, this method neglects the relative
importance of documents. We propose a simple approach to reorder the documents
according to their relative importance before concatenating and summarizing
them. The reordering makes the salient content easier to learn by the
summarization model. Experiments show that our approach outperforms previous
state-of-the-art methods with more complex architectures."
Panoramic Human Activity Recognition,0.111816,"To obtain a more comprehensive activity understanding for a crowded scene, in
this paper, we propose a new problem of panoramic human activity recognition
(PAR), which aims to simultaneous achieve the individual action, social group
activity, and global activity recognition. This is a challenging yet practical
problem in real-world applications. For this problem, we develop a novel
hierarchical graph neural network to progressively represent and model the
multi-granularity human activities and mutual social relations for a crowd of
people. We further build a benchmark to evaluate the proposed method and other
existing related methods. Experimental results verify the rationality of the
proposed PAR problem, the effectiveness of our method and the usefulness of the
benchmark. We will release the source code and benchmark to the public for
promoting the study on this problem."
Black-box Few-shot Knowledge Distillation,0.11866,"Knowledge distillation (KD) is an efficient approach to transfer the
knowledge from a large ""teacher"" network to a smaller ""student"" network.
Traditional KD methods require lots of labeled training samples and a white-box
teacher (parameters are accessible) to train a good student. However, these
resources are not always available in real-world applications. The distillation
process often happens at an external party side where we do not have access to
much data, and the teacher does not disclose its parameters due to security and
privacy concerns. To overcome these challenges, we propose a black-box few-shot
KD method to train the student with few unlabeled training samples and a
black-box teacher. Our main idea is to expand the training set by generating a
diverse set of out-of-distribution synthetic images using MixUp and a
conditional variational auto-encoder. These synthetic images along with their
labels obtained from the teacher are used to train the student. We conduct
extensive experiments to show that our method significantly outperforms recent
SOTA few/zero-shot KD methods on image classification tasks. The code and
models are available at: https://github.com/nphdang/FS-BBT"
Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study,0.147878,"This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench."
MACSum: Controllable Summarization with Mixed Attributes,0.141442,"Controllable summarization allows users to generate customized summaries with
specified attributes. However, due to the lack of designated annotations of
controlled summaries, existing works have to craft pseudo datasets by adapting
generic summarization benchmarks. Furthermore, most research focuses on
controlling single attributes individually (e.g., a short summary or a highly
abstractive summary) rather than controlling a mix of attributes together
(e.g., a short and highly abstractive summary). In this paper, we propose
MACSum, the first human-annotated summarization dataset for controlling mixed
attributes. It contains source texts from two domains, news articles and
dialogues, with human-annotated summaries controlled by five designed
attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We
propose two simple and effective parameter-efficient approaches for the new
task of mixed controllable summarization based on hard prompt tuning and soft
prefix tuning. Results and analysis demonstrate that hard prompt models yield
the best performance on all metrics and human evaluations. However,
mixed-attribute control is still challenging for summarization tasks. Our
dataset and code are available at https://github.com/psunlpgroup/MACSum."
LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling,0.138399,"Compared with traditional task-irrelevant downsampling methods, task-oriented
neural networks have shown improved performance in point cloud downsampling
range. Recently, Transformer family of networks has shown a more powerful
learning capacity in visual tasks. However, Transformer-based architectures
potentially consume too many resources which are usually worthless for low
overhead task networks in downsampling range. This paper proposes a novel
light-weight Transformer network (LighTN) for task-oriented point cloud
downsampling, as an end-to-end and plug-and-play solution. In LighTN, a
single-head self-correlation module is presented to extract refined global
contextual features, where three projection matrices are simultaneously
eliminated to save resource overhead, and the output of symmetric matrix
satisfies the permutation invariant. Then, we design a novel downsampling loss
function to guide LighTN focuses on critical point cloud regions with more
uniform distribution and prominent points coverage. Furthermore, We introduce a
feed-forward network scaling mechanism to enhance the learnable capacity of
LighTN according to the expand-reduce strategy. The result of extensive
experiments on classification and registration tasks demonstrates LighTN can
achieve state-of-the-art performance with limited resource overhead."
Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization,0.159083,"In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D ."
Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings,0.145973,"Although contextualized embeddings generated from large-scale pre-trained
models perform well in many tasks, traditional static embeddings (e.g.,
Skip-gram, Word2Vec) still play an important role in low-resource and
lightweight settings due to their low computational cost, ease of deployment,
and stability. In this paper, we aim to improve word embeddings by 1)
incorporating more contextual information from existing pre-trained models into
the Skip-gram framework, which we call Context-to-Vec; 2) proposing a
post-processing retrofitting method for static embeddings independent of
training by employing priori synonym knowledge and weighted vector
distribution. Through extrinsic and intrinsic tasks, our methods are well
proven to outperform the baselines by a large margin."
Leveraging Graph-based Cross-modal Information Fusion for Neural Sign Language Translation,0.112966,"Sign Language (SL), as the mother tongue of the deaf community, is a special
visual language that most hearing people cannot understand. In recent years,
neural Sign Language Translation (SLT), as a possible way for bridging
communication gap between the deaf and the hearing people, has attracted
widespread academic attention. We found that the current mainstream end-to-end
neural SLT models, which tries to learning language knowledge in a weakly
supervised manner, could not mine enough semantic information under the
condition of low data resources. Therefore, we propose to introduce additional
word-level semantic knowledge of sign language linguistics to assist in
improving current end-to-end neural SLT models. Concretely, we propose a novel
neural SLT model with multi-modal feature fusion based on the dynamic graph, in
which the cross-modal information, i.e. text and video, is first assembled as a
dynamic graph according to their correlation, and then the graph is processed
by a multi-modal graph encoder to generate the multi-modal embeddings for
further usage in the subsequent neural translation models. To the best of our
knowledge, we are the first to introduce graph neural networks, for fusing
multi-modal information, into neural sign language translation models.
Moreover, we conducted experiments on a publicly available popular SLT dataset
RWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our
method can improve the model."
TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation,0.161524,"We present TetGAN, a convolutional neural network designed to generate
tetrahedral meshes. We represent shapes using an irregular tetrahedral grid
which encodes an occupancy and displacement field. Our formulation enables
defining tetrahedral convolution, pooling, and upsampling operations to
synthesize explicit mesh connectivity with variable topological genus. The
proposed neural network layers learn deep features over each tetrahedron and
learn to extract patterns within spatial regions across multiple scales. We
illustrate the capabilities of our technique to encode tetrahedral meshes into
a semantically meaningful latent-space which can be used for shape editing and
synthesis. Our project page is at https://threedle.github.io/tetGAN/."
Detecting Shortcuts in Medical Images -- A Case Study in Chest X-rays,0.199263,"The availability of large public datasets and the increased amount of
computing power have shifted the interest of the medical community to
high-performance algorithms. However, little attention is paid to the quality
of the data and their annotations. High performance on benchmark datasets may
be reported without considering possible shortcuts or artifacts in the data,
besides, models are not tested on subpopulation groups. With this work, we aim
to raise awareness about shortcuts problems. We validate previous findings, and
present a case study on chest X-rays using two publicly available datasets. We
share annotations for a subset of pneumothorax images with drains. We conclude
with general recommendations for medical image classification."
Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild,0.158744,"Previous methods for dynamic facial expression in the wild are mainly based
on Convolutional Neural Networks (CNNs), whose local operations ignore the
long-range dependencies in videos. To solve this problem, we propose the
spatio-temporal Transformer (STT) to capture discriminative features within
each frame and model contextual relationships among frames. Spatio-temporal
dependencies are captured and integrated by our unified Transformer.
Specifically, given an image sequence consisting of multiple frames as input,
we utilize the CNN backbone to translate each frame into a visual feature
sequence. Subsequently, the spatial attention and the temporal attention within
each block are jointly applied for learning spatio-temporal representations at
the sequence level. In addition, we propose the compact softmax cross entropy
loss to further encourage the learned features have the minimum intra-class
distance and the maximum inter-class distance. Experiments on two in-the-wild
dynamic facial expression datasets (i.e., DFEW and AFEW) indicate that our
method provides an effective way to make use of the spatial and temporal
dependencies for dynamic facial expression recognition. The source code and the
training logs will be made publicly available."
Monotonic Differentiable Sorting Networks,0.141674,"Differentiable sorting algorithms allow training with sorting and ranking
supervision, where only the ordering or ranking of samples is known. Various
methods have been proposed to address this challenge, ranging from optimal
transport-based differentiable Sinkhorn sorting algorithms to making classic
sorting networks differentiable. One problem of current differentiable sorting
methods is that they are non-monotonic. To address this issue, we propose a
novel relaxation of conditional swap operations that guarantees monotonicity in
differentiable sorting networks. We introduce a family of sigmoid functions and
prove that they produce differentiable sorting networks that are monotonic.
Monotonicity ensures that the gradients always have the correct sign, which is
an advantage in gradient-based optimization. We demonstrate that monotonic
differentiable sorting networks improve upon previous differentiable sorting
methods."
From Multi-agent to Multi-robot: A Scalable Training and Evaluation Platform for Multi-robot Reinforcement Learning,0.154587,"Multi-agent reinforcement learning (MARL) has been gaining extensive
attention from academia and industries in the past few decades. One of the
fundamental problems in MARL is how to evaluate different approaches
comprehensively. Most existing MARL methods are evaluated in either video games
or simplistic simulated scenarios. It remains unknown how these methods perform
in real-world scenarios, especially multi-robot systems. This paper introduces
a scalable emulation platform for multi-robot reinforcement learning (MRRL)
called SMART to meet this need. Precisely, SMART consists of two components: 1)
a simulation environment that provides a variety of complex interaction
scenarios for training and 2) a real-world multi-robot system for realistic
performance evaluation. Besides, SMART offers agent-environment APIs that are
plug-and-play for algorithm implementation. To illustrate the practicality of
our platform, we conduct a case study on the cooperative driving lane change
scenario. Building off the case study, we summarize several unique challenges
of MRRL, which are rarely considered previously. Finally, we open-source the
simulation environments, associated benchmark tasks, and state-of-the-art
baselines to encourage and empower MRRL research."
Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema Linking Graph,0.147112,"The generalizability to new databases is of vital importance to Text-to-SQL
systems which aim to parse human utterances into SQL statements. Existing works
achieve this goal by leveraging the exact matching method to identify the
lexical matching between the question words and the schema items. However,
these methods fail in other challenging scenarios, such as the synonym
substitution in which the surface form differs between the corresponding
question words and schema items. In this paper, we propose a framework named
ISESL-SQL to iteratively build a semantic enhanced schema-linking graph between
question tokens and database schemas. First, we extract a schema linking graph
from PLMs through a probing procedure in an unsupervised manner. Then the
schema linking graph is further optimized during the training process through a
deep graph learning method. Meanwhile, we also design an auxiliary task called
graph regularization to improve the schema information mentioned in the
schema-linking graph. Extensive experiments on three benchmarks demonstrate
that ISESL-SQL could consistently outperform the baselines and further
investigations show its generalizability and robustness."
MPANet: Multi-Patch Attention For Infrared Small Target object Detection,0.140356,"Infrared small target detection (ISTD) has attracted widespread attention and
been applied in various fields. Due to the small size of infrared targets and
the noise interference from complex backgrounds, the performance of ISTD using
convolutional neural networks (CNNs) is restricted. Moreover, the constriant
that long-distance dependent features can not be encoded by the vanilla CNNs
also impairs the robustness of capturing targets' shapes and locations in
complex scenarios. To this end, a multi-patch attention network (MPANet) based
on the axial-attention encoder and the multi-scale patch branch (MSPB)
structure is proposed. Specially, an axial-attention-improved encoder
architecture is designed to highlight the effective features of small targets
and suppress background noises. Furthermore, the developed MSPB structure fuses
the coarse-grained and fine-grained features from different semantic scales.
Extensive experiments on the SIRST dataset show the superiority performance and
effectiveness of the proposed MPANet compared to the state-of-the-art methods."
GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding,0.139762,"Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset."
Scale-free and Task-agnostic Attack: Generating Photo-realistic Adversarial Patterns with Patch Quilting Generator,0.134751,"\noindent Traditional L_p norm-restricted image attack algorithms suffer from
poor transferability to black box scenarios and poor robustness to defense
algorithms. Recent CNN generator-based attack approaches can synthesize
unrestricted and semantically meaningful entities to the image, which is shown
to be transferable and robust. However, such methods attack images by either
synthesizing local adversarial entities, which are only suitable for attacking
specific contents or performing global attacks, which are only applicable to a
specific image scale. In this paper, we propose a novel Patch Quilting
Generative Adversarial Networks (PQ-GAN) to learn the first scale-free CNN
generator that can be applied to attack images with arbitrary scales for
various computer vision tasks. The principal investigation on transferability
of the generated adversarial examples, robustness to defense frameworks, and
visual quality assessment show that the proposed PQG-based attack framework
outperforms the other nine state-of-the-art adversarial attack approaches when
attacking the neural networks trained on two standard evaluation datasets
(i.e., ImageNet and CityScapes)."
Instance Segmentation of Unlabeled Modalities via Cyclic Segmentation GAN,0.115086,"Instance segmentation for unlabeled imaging modalities is a challenging but
essential task as collecting expert annotation can be expensive and
time-consuming. Existing works segment a new modality by either deploying a
pre-trained model optimized on diverse training data or conducting domain
translation and image segmentation as two independent steps. In this work, we
propose a novel Cyclic Segmentation Generative Adversarial Network (CySGAN)
that conducts image translation and instance segmentation jointly using a
unified framework. Besides the CycleGAN losses for image translation and
supervised losses for the annotated source domain, we introduce additional
self-supervised and segmentation-based adversarial objectives to improve the
model performance by leveraging unlabeled target domain images. We benchmark
our approach on the task of 3D neuronal nuclei segmentation with annotated
electron microscopy (EM) images and unlabeled expansion microscopy (ExM) data.
Our CySGAN outperforms both pretrained generalist models and the baselines that
sequentially conduct image translation and segmentation. Our implementation and
the newly collected, densely annotated ExM nuclei dataset, named NucExM, are
available at https://connectomics-bazaar.github.io/proj/CySGAN/index.html."
"Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities",0.151826,"As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection."
VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web,0.113647,"The DarkWeb represents a hotbed for illicit activity, where users communicate
on different market forums in order to exchange goods and services. Law
enforcement agencies benefit from forensic tools that perform authorship
analysis, in order to identify and profile users based on their textual
content. However, authorship analysis has been traditionally studied using
corpora featuring literary texts such as fragments from novels or fan fiction,
which may not be suitable in a cybercrime context. Moreover, the few works that
employ authorship analysis tools for cybercrime prevention usually employ
ad-hoc experimental setups and datasets. To address these issues, we release
VeriDark: a benchmark comprised of three large scale authorship verification
datasets and one authorship identification dataset obtained from user activity
from either Dark Web related Reddit communities or popular illicit Dark Web
market forums. We evaluate competitive NLP baselines on the three datasets and
perform an analysis of the predictions to better understand the limitations of
such approaches. We make the datasets and baselines publicly available at
https://github.com/bit-ml/VeriDark"
Learning to Fold Real Garments with One Arm: A Case Study in Cloud-Based Robotics Research,0.195379,"Autonomous fabric manipulation is a longstanding challenge in robotics, but
evaluating progress is difficult due to the cost and diversity of robot
hardware. Using Reach, a cloud robotics platform that enables low-latency
remote execution of control policies on physical robots, we present the first
systematic benchmarking of fabric manipulation algorithms on physical hardware.
We develop 4 novel learning-based algorithms that model expert actions,
keypoints, reward functions, and dynamic motions, and we compare these against
4 learning-free and inverse dynamics algorithms on the task of folding a
crumpled T-shirt with a single robot arm. The entire lifecycle of data
collection, model training, and policy evaluation is performed remotely without
physical access to the robot workcell. Results suggest a new algorithm
combining imitation learning with analytic methods achieves 84% of human-level
performance on the folding task. See
https://sites.google.com/berkeley.edu/cloudfolding for all data, code, models,
and supplemental material."
Few-shot Learning with Noisy Labels,0.149947,"Few-shot learning (FSL) methods typically assume clean support sets with
accurately labeled samples when training on novel classes. This assumption can
often be unrealistic: support sets, no matter how small, can still include
mislabeled samples. Robustness to label noise is therefore essential for FSL
methods to be practical, but this problem surprisingly remains largely
unexplored. To address mislabeled samples in FSL settings, we make several
technical contributions. (1) We offer simple, yet effective, feature
aggregation methods, improving the prototypes used by ProtoNet, a popular FSL
technique. (2) We describe a novel Transformer model for Noisy Few-Shot
Learning (TraNFS). TraNFS leverages a transformer's attention mechanism to
weigh mislabeled versus correct samples. (3) Finally, we extensively test these
methods on noisy versions of MiniImageNet and TieredImageNet. Our results show
that TraNFS is on-par with leading FSL methods on clean support sets, yet
outperforms them, by far, in the presence of label noise."
Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar,0.164914,"Since the beginning of the COVID-19 pandemic, remote conferencing and
school-teaching have become important tools. The previous applications aim to
save the commuting cost with real-time interactions. However, our application
is going to lower the production and reproduction costs when preparing the
communication materials. This paper proposes a system called Pre-Avatar,
generating a presentation video with a talking face of a target speaker with 1
front-face photo and a 3-minute voice recording. Technically, the system
consists of three main modules, user experience interface (UEI), talking face
module and few-shot text-to-speech (TTS) module. The system firstly clones the
target speaker's voice, and then generates the speech, and finally generate an
avatar with appropriate lip and head movements. Under any scenario, users only
need to replace slides with different notes to generate another new video. The
demo has been released here and will be published as free software for use."
Enhanced Training of Query-Based Object Detection via Selective Query Recollection,0.20579,"This paper investigates a phenomenon where query-based object detectors
mispredict at the last decoding stage while predicting correctly at an
intermediate stage. We review the training process and attribute the overlooked
phenomenon to two limitations: lack of training emphasis and cascading errors
from decoding sequence. We design and present Selective Query Recollection
(SQR), a simple and effective training strategy for query-based object
detectors. It cumulatively collects intermediate queries as decoding stages go
deeper and selectively forwards the queries to the downstream stages aside from
the sequential structure. Such-wise, SQR places training emphasis on later
stages and allows later stages to work with intermediate queries from earlier
stages directly. SQR can be easily plugged into various query-based object
detectors and significantly enhances their performance while leaving the
inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,
and Deformable-DETR across various settings (backbone, number of queries,
schedule) and consistently brings 1.4-2.8 AP improvement."
Region Embedding with Intra and Inter-View Contrastive Learning,0.111176,"Unsupervised region representation learning aims to extract dense and
effective features from unlabeled urban data. While some efforts have been made
for solving this problem based on multiple views, existing methods are still
insufficient in extracting representations in a view and/or incorporating
representations from different views. Motivated by the success of contrastive
learning for representation learning, we propose to leverage it for multi-view
region representation learning and design a model called ReMVC (Region
Embedding with Multi-View Contrastive Learning) by following two guidelines: i)
comparing a region with others within each view for effective representation
extraction and ii) comparing a region with itself across different views for
cross-view information sharing. We design the intra-view contrastive learning
module which helps to learn distinguished region embeddings and the inter-view
contrastive learning module which serves as a soft co-regularizer to constrain
the embedding parameters and transfer knowledge across multi-views. We exploit
the learned region embeddings in two downstream tasks named land usage
clustering and region popularity prediction. Extensive experiments demonstrate
that our model achieves impressive improvements compared with seven
state-of-the-art baseline methods, and the margins are over 30% in the land
usage clustering task."
Pyramid Frequency Network with Spatial Attention Residual Refinement Module for Monocular Depth Estimation,0.114659,"Deep-learning-based approaches to depth estimation are rapidly advancing,
offering superior performance over existing methods. To estimate the depth in
real-world scenarios, depth estimation models require the robustness of various
noise environments. In this work, a Pyramid Frequency Network(PFN) with Spatial
Attention Residual Refinement Module(SARRM) is proposed to deal with the weak
robustness of existing deep-learning methods. To reconstruct depth maps with
accurate details, the SARRM constructs a residual fusion method with an
attention mechanism to refine the blur depth. The frequency division strategy
is designed, and the frequency pyramid network is developed to extract features
from multiple frequency bands. With the frequency strategy, PFN achieves better
visual accuracy than state-of-the-art methods in both indoor and outdoor scenes
on Make3D, KITTI depth, and NYUv2 datasets. Additional experiments on the noisy
NYUv2 dataset demonstrate that PFN is more reliable than existing deep-learning
methods in high-noise scenes."
Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis,0.168528,"Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition
of the esophagus associated with elevated esophageal eosinophils. Second only
to gastroesophageal reflux disease, EoE is one of the leading causes of chronic
refractory dysphagia in adults and children. EoE diagnosis requires enumerating
the density of esophageal eosinophils in esophageal biopsies, a somewhat
subjective task that is time-consuming, thus reducing the ability to process
the complex tissue structure. Previous artificial intelligence (AI) approaches
that aimed to improve histology-based diagnosis focused on recapitulating
identification and quantification of the area of maximal eosinophil density.
However, this metric does not account for the distribution of eosinophils or
other histological features, over the whole slide image. Here, we developed an
artificial intelligence platform that infers local and spatial biomarkers based
on semantic segmentation of intact eosinophils and basal zone distributions.
Besides the maximal density of eosinophils (referred to as Peak Eosinophil
Count [PEC]) and a maximal basal zone fraction, we identify two additional
metrics that reflect the distribution of eosinophils and basal zone fractions.
This approach enables a decision support system that predicts EoE activity and
classifies the histological severity of EoE patients. We utilized a cohort that
includes 1066 biopsy slides from 400 subjects to validate the system's
performance and achieved a histological severity classification accuracy of
86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach
highlights the importance of systematically analyzing the distribution of
biopsy features over the entire slide and paves the way towards a personalized
decision support system that will assist not only in counting cells but can
also potentially improve diagnosis and provide treatment prediction."
Multi-level Consistency Learning for Semi-supervised Domain Adaptation,0.17978,"Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from
a fully labeled source domain to a scarcely labeled target domain. In this
paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.
Specifically, our MCL regularizes the consistency of different views of target
domain samples at three levels: (i) at inter-domain level, we robustly and
accurately align the source and target domains using a prototype-based optimal
transport method that utilizes the pros and cons of different views of target
samples; (ii) at intra-domain level, we facilitate the learning of both
discriminative and compact target feature representations by proposing a novel
class-wise contrastive clustering loss; (iii) at sample level, we follow
standard practice and improve the prediction accuracy by conducting a
consistency-based self-training. Empirically, we verified the effectiveness of
our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,
and Office-Home datasets, and the experimental results demonstrate that our MCL
framework achieves the state-of-the-art performance."
Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI,0.124548,"As artificial intelligence becomes more powerful and a ubiquitous presence in
daily life, it is imperative to understand and manage the impact of AI systems
on our lives and decisions. Modern ML systems often change user behavior (e.g.
personalized recommender systems learn user preferences to deliver
recommendations that change online behavior). An externality of behavior change
is preference change. This article argues for the establishment of a
multidisciplinary endeavor focused on understanding how AI systems change
preference: Preference Science. We operationalize preference to incorporate
concepts from various disciplines, outlining the importance of meta-preferences
and preference-change preferences, and proposing a preliminary framework for
how preferences change. We draw a distinction between preference change,
permissible preference change, and outright preference manipulation. A
diversity of disciplines contribute unique insights to this framework."
CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions,0.159289,"While deep learning based approaches have demonstrated expert-level
performance in dermatological diagnosis tasks, they have also been shown to
exhibit biases toward certain demographic attributes, particularly skin types
(e.g., light versus dark), a fairness concern that must be addressed. We
propose CIRCLe, a skin color invariant deep representation learning method for
improving fairness in skin lesion classification. CIRCLe is trained to classify
images by utilizing a regularization loss that encourages images with the same
diagnosis but different skin types to have similar latent representations.
Through extensive evaluation and ablation studies, we demonstrate CIRCLe's
superior performance over the state-of-the-art when evaluated on 16k+ images
spanning 6 Fitzpatrick skin types and 114 diseases, using classification
accuracy, equal opportunity difference (for light versus dark groups), and
normalized accuracy range, a new measure we propose to assess fairness on
multiple skin type groups."
"Can counterfactual explanations of AI systems' predictions skew lay users' causal intuitions about the world? If so, can we correct for that?",0.135174,"Counterfactual (CF) explanations have been employed as one of the modes of
explainability in explainable AI-both to increase the transparency of AI
systems and to provide recourse. Cognitive science and psychology, however,
have pointed out that people regularly use CFs to express causal relationships.
Most AI systems are only able to capture associations or correlations in data
so interpreting them as casual would not be justified. In this paper, we
present two experiment (total N = 364) exploring the effects of CF explanations
of AI system's predictions on lay people's causal beliefs about the real world.
In Experiment 1 we found that providing CF explanations of an AI system's
predictions does indeed (unjustifiably) affect people's causal beliefs
regarding factors/features the AI uses and that people are more likely to view
them as causal factors in the real world. Inspired by the literature on
misinformation and health warning messaging, Experiment 2 tested whether we can
correct for the unjustified change in causal beliefs. We found that pointing
out that AI systems capture correlations and not necessarily causal
relationships can attenuate the effects of CF explanations on people's causal
beliefs."
NICO++: Towards Better Benchmarking for Domain Generalization,0.189952,"Despite the remarkable performance that modern deep neural networks have
achieved on independent and identically distributed (I.I.D.) data, they can
crash under distribution shifts. Most current evaluation methods for domain
generalization (DG) adopt the leave-one-out strategy as a compromise on the
limited number of domains. We propose a large-scale benchmark with extensive
labeled domains named NICO++ along with more rational evaluation methods for
comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose
two metrics to quantify covariate shift and concept shift, respectively. Two
novel generalization bounds from the perspective of data construction are
proposed to prove that limited concept shift and significant covariate shift
favor the evaluation capability for generalization. Through extensive
experiments, NICO++ shows its superior evaluation capability compared with
current DG datasets and its contribution in alleviating unfairness caused by
the leak of oracle knowledge in model selection."
Learning to Revise References for Faithful Summarization,0.220904,"In real-world scenarios with naturally occurring datasets, reference
summaries are noisy and may contain information that cannot be inferred from
the source text. On large news corpora, removing low quality samples has been
shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,
filtering is detrimental to performance. To improve reference quality while
retaining all data, we propose a new approach: to selectively re-write
unsupported reference sentences to better reflect source data. We automatically
generate a synthetic dataset of positive and negative revisions by corrupting
supported sentences and learn to revise reference sentences with contrastive
learning. The intensity of revisions is treated as a controllable attribute so
that, at inference, diverse candidates can be over-generated-then-rescored to
balance faithfulness and abstraction. To test our methods, we extract noisy
references from publicly available MIMIC-III discharge summaries for the task
of hospital-course summarization, and vary the data on which models are
trained. According to metrics and human evaluation, models trained on revised
clinical references are much more faithful, informative, and fluent than models
trained on original or filtered data."
Learning Equivariant Segmentation with Instance-Unique Querying,0.157033,"Prevalent state-of-the-art instance segmentation methods fall into a
query-based scheme, in which instance masks are derived by querying the image
feature using a set of instance-aware embeddings. In this work, we devise a new
training framework that boosts query-based models through discriminative query
embedding learning. It explores two essential properties, namely dataset-level
uniqueness and transformation equivariance, of the relation between queries and
instances. First, our algorithm uses the queries to retrieve the corresponding
instances from the whole training dataset, instead of only searching within
individual scenes. As querying instances across scenes is more challenging, the
segmenters are forced to learn more discriminative queries for effective
instance separation. Second, our algorithm encourages both image (instance)
representations and queries to be equivariant against geometric
transformations, leading to more robust, instance-query matching. On top of
four famous, query-based models ($i.e.,$ CondInst, SOLOv2, SOTR, and
Mask2Former), our training algorithm provides significant performance gains
($e.g.,$ +1.6 - 3.2 AP) on COCO dataset. In addition, our algorithm promotes
the performance of SOLOv2 by 2.7 AP, on LVISv1 dataset."
The SIGMORPHON 2022 Shared Task on Morpheme Segmentation,0.11548,"The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems
to decompose a word into a sequence of morphemes and covered most types of
morphology: compounds, derivations, and inflections. Subtask 1, word-level
morpheme segmentation, covered 5 million words in 9 languages (Czech, English,
Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13
system submissions from 7 teams and the best system averaged 97.29% F1 score
across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,
sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages
(Czech, English, Mongolian), received 10 system submissions from 3 teams, and
the best systems outperformed all three state-of-the-art subword tokenization
methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis
and support any type of future studies, we released all system predictions, the
evaluation script, and all gold standard datasets."
AnyMorph: Learning Transferable Polices By Inferring Agent Morphology,0.148696,"The prototypical approach to reinforcement learning involves training
policies tailored to a particular agent from scratch for every new morphology.
Recent work aims to eliminate the re-training of policies by investigating
whether a morphology-agnostic policy, trained on a diverse set of agents with
similar task objectives, can be transferred to new agents with unseen
morphologies without re-training. This is a challenging problem that required
previous approaches to use hand-designed descriptions of the new agent's
morphology. Instead of hand-designing this description, we propose a
data-driven method that learns a representation of morphology directly from the
reinforcement learning objective. Ours is the first reinforcement learning
algorithm that can train a policy to generalize to new agent morphologies
without requiring a description of the agent's morphology in advance. We
evaluate our approach on the standard benchmark for agent-agnostic control, and
improve over the current state of the art in zero-shot generalization to new
agents. Importantly, our method attains good performance without an explicit
description of morphology."
A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss,0.11342,"For readability assessment, traditional methods mainly employ machine
learning classifiers with hundreds of linguistic features. Although the deep
learning model has become the prominent approach for almost all NLP tasks, it
is less explored for readability assessment. In this paper, we propose a
BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)
for readability assessment. Specially, we present a new difficulty knowledge
guided semi-supervised method to extract topic features to complement the
traditional linguistic features. From the linguistic features, we employ
projection filtering to extract orthogonal features to supplement BERT
representations. Furthermore, we design a new length-balanced loss to handle
the greatly varying length distribution of data. Our model achieves
state-of-the-art performances on two English benchmark datasets and one dataset
of Chinese textbooks, and also achieves the near-perfect accuracy of 99\% on
one English dataset. Moreover, our proposed model obtains comparable results
with human experts in consistency test."
Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar,0.134609,"We introduce NSEdit (neural-symbolic edit), a novel Transformer-based code
repair method. Given only the source code that contains bugs, NSEdit predicts
an editing sequence that can fix the bugs. The edit grammar is formulated as a
regular language, and the Transformer uses it as a neural-symbolic scripting
interface to generate editing programs. We modify the Transformer and add a
pointer network to select the edit locations. An ensemble of rerankers are
trained to re-rank the editing sequences generated by beam search. We fine-tune
the rerankers on the validation set to reduce over-fitting. NSEdit is evaluated
on various code repair datasets and achieved a new state-of-the-art accuracy
($24.04\%$) on the Tufano small dataset of the CodeXGLUE benchmark. NSEdit
performs robustly when programs vary from packages to packages and when buggy
programs are concrete. We conduct detailed analysis on our methods and
demonstrate the effectiveness of each component."
Learning to Adapt Domain Shifts of Moral Values via Instance Weighting,0.148904,"Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines."
Addressing the Challenges of Cross-Lingual Hate Speech Detection,0.212605,"The goal of hate speech detection is to filter negative online content aiming
at certain groups of people. Due to the easy accessibility of social media
platforms it is crucial to protect everyone which requires building hate speech
detection systems for a wide range of languages. However, the available labeled
hate speech datasets are limited making it problematic to build systems for
many languages. In this paper we focus on cross-lingual transfer learning to
support hate speech detection in low-resource languages. We leverage
cross-lingual word embeddings to train our neural network systems on the source
language and apply it to the target language, which lacks labeled examples, and
show that good performance can be achieved. We then incorporate unlabeled
target language data for further model improvements by bootstrapping labels
using an ensemble of different model architectures. Furthermore, we investigate
the issue of label imbalance of hate speech datasets, since the high ratio of
non-hate examples compared to hate examples often leads to low model
performance. We test simple data undersampling and oversampling techniques and
show their effectiveness."
$\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing,0.140208,"This paper presents a generic probabilistic framework for estimating the
statistical dependency and finding the anatomical correspondences among an
arbitrary number of medical images. The method builds on a novel formulation of
the $N$-dimensional joint intensity distribution by representing the common
anatomy as latent variables and estimating the appearance model with
nonparametric estimators. Through connection to maximum likelihood and the
expectation-maximization algorithm, an information\hyp{}theoretic metric called
$\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg
are induced, allowing groupwise registration of the $N$ observed images with
computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally
extends for a weakly-supervised scenario where anatomical labels of certain
images are provided. This leads to a combined\hyp{}computing framework
implemented with deep learning, which performs registration and segmentation
simultaneously and collaboratively in an end-to-end fashion. Extensive
experiments were conducted to demonstrate the versatility and applicability of
our model, including multimodal groupwise registration, motion correction for
dynamic contrast enhanced magnetic resonance images, and deep combined
computing for multimodal medical images. Results show the superiority of our
method in various applications in terms of both accuracy and efficiency,
highlighting the advantage of the proposed representation of the imaging
process."
CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations,0.119469,"While Self-Supervised Learning has helped reap the benefit of the scale from
the available unlabeled data, the learning paradigms are continuously being
bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which
uses clustering and an augmentation-based cross-contrastive loss as its
self-supervised objective. Through the clustering module, we scale down the
influence of those negative examples that are highly similar to the positive.
The Cross-Contrastive loss is computed between the encoder output of the
original sample and the quantizer output of its augmentation and vice-versa,
bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up
to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on
the test-clean and test-other sets, respectively, of LibriSpeech, without the
use of any language model. The proposed method also achieves up to 14.9%
relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on
Switchboard data. We make all our codes publicly available on GitHub."
Deep Virtual-to-Real Distillation for Pedestrian Crossing Prediction,0.165789,"Pedestrian crossing is one of the most typical behavior which conflicts with
natural driving behavior of vehicles. Consequently, pedestrian crossing
prediction is one of the primary task that influences the vehicle planning for
safe driving. However, current methods that rely on the practically collected
data in real driving scenes cannot depict and cover all kinds of scene
condition in real traffic world. To this end, we formulate a deep virtual to
real distillation framework by introducing the synthetic data that can be
generated conveniently, and borrow the abundant information of pedestrian
movement in synthetic videos for the pedestrian crossing prediction in real
data with a simple and lightweight implementation. In order to verify this
framework, we construct a benchmark with 4667 virtual videos owning about 745k
frames (called Virtual-PedCross-4667), and evaluate the proposed method on two
challenging datasets collected in real driving situations, i.e., JAAD and PIE
datasets. State-of-the-art performance of this framework is demonstrated by
exhaustive experiment analysis. The dataset and code can be downloaded from the
website \url{http://www.lotvs.net/code_data/}."
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,0.184984,"This work presents a detailed linguistic analysis into why larger
Transformer-based pre-trained language models with more parameters and lower
perplexity nonetheless yield surprisal estimates that are less predictive of
human reading times. First, regression analyses show a strictly monotonic,
positive log-linear relationship between perplexity and fit to reading times
for the more recently released five GPT-Neo variants and eight OPT variants on
two separate datasets, replicating earlier results limited to just GPT-2 (Oh et
al., 2022). Subsequently, analysis of residual errors reveals a systematic
deviation of the larger variants, such as underpredicting reading times of
named entities and making compensatory overpredictions for reading times of
function words such as modals and conjunctions. These results suggest that the
propensity of larger Transformer-based models to 'memorize' sequences during
training makes their surprisal estimates diverge from humanlike expectations,
which warrants caution in using pre-trained language models to study human
language processing."
Processing the structure of documents: Logical Layout Analysis of historical newspapers in French,0.125131,"Background. In recent years, libraries and archives led important
digitisation campaigns that opened the access to vast collections of historical
documents. While such documents are often available as XML ALTO documents, they
lack information about their logical structure. In this paper, we address the
problem of Logical Layout Analysis applied to historical documents in French.
We propose a rule-based method, that we evaluate and compare with two
Machine-Learning models, namely RIPPER and Gradient Boosting. Our data set
contains French newspapers, periodicals and magazines, published in the first
half of the twentieth century in the Franche-Comt\'e Region. Results. Our
rule-based system outperforms the two other models in nearly all evaluations.
It has especially better Recall results, indicating that our system covers more
types of every logical label than the other two models. When comparing RIPPER
with Gradient Boosting, we can observe that Gradient Boosting has better
Precision scores but RIPPER has better Recall scores. Conclusions. The
evaluation shows that our system outperforms the two Machine Learning models,
and provides significantly higher Recall. It also confirms that our system can
be used to produce annotated data sets that are large enough to envisage
Machine Learning or Deep Learning approaches for the task of Logical Layout
Analysis. Combining rules and Machine Learning models into hybrid systems could
potentially provide even better performances. Furthermore, as the layout in
historical documents evolves rapidly, one possible solution to overcome this
problem would be to apply Rule Learning algorithms to bootstrap rule sets
adapted to different publication periods."
Multi-task Active Learning for Pre-trained Transformer-based Models,0.179688,"Multi-task learning, in which several tasks are jointly learned by a single
model, allows NLP models to share information from multiple annotations and may
facilitate better predictions when the tasks are inter-related. This technique,
however, requires annotating the same text with multiple annotation schemes
which may be costly and laborious. Active learning (AL) has been demonstrated
to optimize annotation processes by iteratively selecting unlabeled examples
whose annotation is most valuable for the NLP model. Yet, multi-task active
learning (MT-AL) has not been applied to state-of-the-art pre-trained
Transformer-based NLP models. This paper aims to close this gap. We explore
various multi-task selection criteria in three realistic multi-task scenarios,
reflecting different relations between the participating tasks, and demonstrate
the effectiveness of multi-task compared to single-task selection. Our results
suggest that MT-AL can be effectively used in order to minimize annotation
efforts for multi-task NLP models."
Differentiable Agent-based Epidemiology,0.173609,"Mechanistic simulators are an indispensable tool for epidemiology to explore
the behavior of complex, dynamic infections under varying conditions and
navigate uncertain environments. Agent-based models (ABMs) are an increasingly
popular simulation paradigm that can represent the heterogeneity of contact
interactions with granular detail and agency of individual behavior. However,
conventional ABM frameworks are not differentiable and present challenges in
scalability; due to which it is non-trivial to connect them to auxiliary data
sources. In this paper, we introduce GradABM: a scalable, differentiable design
for agent-based modeling that is amenable to gradient-based learning with
automatic differentiation. GradABM can quickly simulate million-size
populations in few seconds on commodity hardware, integrate with deep neural
networks and ingest heterogeneous data sources. This provides an array of
practical benefits for calibration, forecasting, and evaluating policy
interventions. We demonstrate the efficacy of GradABM via extensive experiments
with real COVID-19 and influenza datasets."
Deep Speech Based End-to-End Automated Speech Recognition (ASR) for Indian-English Accents,0.186141,"Automated Speech Recognition (ASR) is an interdisciplinary application of
computer science and linguistics that enable us to derive the transcription
from the uttered speech waveform. It finds several applications in Military
like High-performance fighter aircraft, helicopters, air-traffic controller.
Other than military speech recognition is used in healthcare, persons with
disabilities and many more. ASR has been an active research area. Several
models and algorithms for speech to text (STT) have been proposed. One of the
most recent is Mozilla Deep Speech, it is based on the Deep Speech research
paper by Baidu. Deep Speech is a state-of-art speech recognition system is
developed using end-to-end deep learning, it is trained using well-optimized
Recurrent Neural Network (RNN) training system utilizing multiple Graphical
Processing Units (GPUs). This training is mostly done using American-English
accent datasets, which results in poor generalizability to other English
accents. India is a land of vast diversity. This can even be seen in the
speech, there are several English accents which vary from state to state. In
this work, we have used transfer learning approach using most recent Deep
Speech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition
system for Indian-English accents. This work utilizes fine-tuning and data
argumentation to further optimize and improve the Deep Speech ASR system. Indic
TTS data of Indian-English accents is used for transfer learning and
fine-tuning the pre-trained Deep Speech model. A general comparison is made
among the untrained model, our trained model and other available speech
recognition services for Indian-English Accents."
Grounding Aleatoric Uncertainty for Unsupervised Environment Design,0.156145,"Adaptive curricula in reinforcement learning (RL) have proven effective for
producing policies robust to discrepancies between the train and test
environment. Recently, the Unsupervised Environment Design (UED) framework
generalized RL curricula to generating sequences of entire environments,
leading to new methods with robust minimax regret properties. Problematically,
in partially-observable or stochastic settings, optimal policies may depend on
the ground-truth distribution over aleatoric parameters of the environment in
the intended deployment setting, while curriculum learning necessarily shifts
the training distribution. We formalize this phenomenon as curriculum-induced
covariate shift (CICS), and describe how its occurrence in aleatoric parameters
can lead to suboptimal policies. Directly sampling these parameters from the
ground-truth distribution avoids the issue, but thwarts curriculum learning. We
propose SAMPLR, a minimax regret UED method that optimizes the ground-truth
utility function, even when the underlying training data is biased due to CICS.
We prove, and validate on challenging domains, that our approach preserves
optimality under the ground-truth distribution, while promoting robustness
across the full range of environment settings."
DETR++: Taming Your Multi-Scale Detection Transformer,0.136939,"Convolutional Neural Networks (CNN) have dominated the field of detection
ever since the success of AlexNet in ImageNet classification [12]. With the
sweeping reform of Transformers [27] in natural language processing, Carion et
al. [2] introduce the Transformer-based detection method, i.e., DETR. However,
due to the quadratic complexity in the self-attention mechanism in the
Transformer, DETR is never able to incorporate multi-scale features as
performed in existing CNN-based detectors, leading to inferior results in small
object detection. To mitigate this issue and further improve performance of
DETR, in this work, we investigate different methods to incorporate multi-scale
features and find that a Bi-directional Feature Pyramid (BiFPN) works best with
DETR in further raising the detection precision. With this discovery, we
propose DETR++, a new architecture that improves detection results by 1.9% AP
on MS COCO 2017, 11.5% AP on RICO icon detection, and 9.1% AP on RICO layout
extraction over existing baselines."
Towards Open Set Video Anomaly Detection,0.190223,"Open Set Video Anomaly Detection (OpenVAD) aims to identify abnormal events
from video data where both known anomalies and novel ones exist in testing.
Unsupervised models learned solely from normal videos are applicable to any
testing anomalies but suffer from a high false positive rate. In contrast,
weakly supervised methods are effective in detecting known anomalies but could
fail in an open world. We develop a novel weakly supervised method for the
OpenVAD problem by integrating evidential deep learning (EDL) and normalizing
flows (NFs) into a multiple instance learning (MIL) framework. Specifically, we
propose to use graph neural networks and triplet loss to learn discriminative
features for training the EDL classifier, where the EDL is capable of
identifying the unknown anomalies by quantifying the uncertainty. Moreover, we
develop an uncertainty-aware selection strategy to obtain clean anomaly
instances and a NFs module to generate the pseudo anomalies. Our method is
superior to existing approaches by inheriting the advantages of both the
unsupervised NFs and the weakly-supervised MIL framework. Experimental results
on multiple real-world video datasets show the effectiveness of our method."
Egocentric Prediction of Action Target in 3D,0.188295,"We are interested in anticipating as early as possible the target location of
a person's object manipulation action in a 3D workspace from egocentric vision.
It is important in fields like human-robot collaboration, but has not yet
received enough attention from vision and learning communities. To stimulate
more research on this challenging egocentric vision task, we propose a large
multimodality dataset of more than 1 million frames of RGB-D and IMU streams,
and provide evaluation metrics based on our high-quality 2D and 3D labels from
semi-automatic annotation. Meanwhile, we design baseline methods using
recurrent neural networks and conduct various ablation studies to validate
their effectiveness. Our results demonstrate that this new task is worthy of
further study by researchers in robotics, vision, and learning communities."
TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter,0.178343,"Pre-trained language models (PLMs) are fundamental for natural language
processing applications. Most existing PLMs are not tailored to the noisy
user-generated text on social media, and the pre-training does not factor in
the valuable social engagement logs available in a social network. We present
TwHIN-BERT, a multilingual language model productionized at Twitter, trained on
in-domain data from the popular social network. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages,
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on various multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We open-source TwHIN-BERT and our
curated hashtag prediction and social engagement benchmark datasets to the
research community."
CNSNet: A Cleanness-Navigated-Shadow Network for Shadow Removal,0.191235,"The key to shadow removal is recovering the contents of the shadow regions
with the guidance of the non-shadow regions. Due to the inadequate long-range
modeling, the CNN-based approaches cannot thoroughly investigate the
information from the non-shadow regions. To solve this problem, we propose a
novel cleanness-navigated-shadow network (CNSNet), with a shadow-oriented
adaptive normalization (SOAN) module and a shadow-aware aggregation with
transformer (SAAT) module based on the shadow mask. Under the guidance of the
shadow mask, the SOAN module formulates the statistics from the non-shadow
region and adaptively applies them to the shadow region for region-wise
restoration. The SAAT module utilizes the shadow mask to precisely guide the
restoration of each shadowed pixel by considering the highly relevant pixels
from the shadow-free regions for global pixel-wise restoration. Extensive
experiments on three benchmark datasets (ISTD, ISTD+, and SRD) show that our
method achieves superior de-shadowing performance."
Smoothing Entailment Graphs with Language Models,0.20607,"The diversity and Zipfian frequency distribution of natural language
predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by
Open Relation Extraction (ORE). EGs are computationally efficient and
explainable models of natural language inference, but as symbolic models, they
fail if a novel premise or hypothesis vertex is missing at test-time. We
present theory and methodology for overcoming such sparsity in symbolic models.
First, we introduce a theory of optimal smoothing of EGs by constructing
transitive chains. We then demonstrate an efficient, open-domain, and
unsupervised smoothing method using an off-the-shelf Language Model to find
approximations of missing premise predicates. This improves recall by 25.1 and
16.3 percentage points on two difficult directional entailment datasets, while
raising average precision and maintaining model explainability. Further, in a
QA task we show that EG smoothing is most useful for answering questions with
lesser supporting text, where missing premise predicates are more costly.
Finally, controlled experiments with WordNet confirm our theory and show that
hypothesis smoothing is difficult, but possible in principle."
User-Centric Gender Rewriting,0.151459,"In this paper, we define the task of gender rewriting in contexts involving
two users (I and/or You) - first and second grammatical persons with
independent grammatical gender preferences. We focus on Arabic, a
gender-marking morphologically rich language. We develop a multi-step system
that combines the positive aspects of both rule-based and neural rewriting
models. Our results successfully demonstrate the viability of this approach on
a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5
on a blind test set. Our proposed system improves over previous work on the
first-person-only version of this task, by 3.05 absolute increase in M2 F0.5.
We demonstrate a use case of our gender rewriting system by using it to
post-edit the output of a commercial MT system to provide personalized outputs
based on the users' grammatical gender preferences. We make our code, data, and
models publicly available."
Bias-Eliminated Semantic Refinement for Any-Shot Learning,0.134599,"When training samples are scarce, the semantic embedding technique, ie,
describing class labels with attributes, provides a condition to generate
visual features for unseen objects by transferring the knowledge from seen
objects. However, semantic descriptions are usually obtained in an external
paradigm, such as manual annotation, resulting in weak consistency between
descriptions and visual features. In this paper, we refine the coarse-grained
semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),
generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new
model, namely, the semantic refinement Wasserstein generative adversarial
network (SRWGAN) model, is designed with the proposed multihead representation
and hierarchical alignment techniques. Unlike conventional methods, semantic
refinement is performed with the aim of identifying a bias-eliminated condition
for disjoint-class feature generation and is applicable in both inductive and
transductive settings. We extensively evaluate model performance on six
benchmark datasets and observe state-of-the-art results for any-shot learning;
eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset
and 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the
standard GZSL setting. Various visualizations are also provided to show the
bias-eliminated generation of SRWGAN. Our code is available."
Learning Parameters for a Generalized Vidale-Wolfe Response Model with Flexible Ad Elasticity and Word-of-Mouth,0.219286,"In this research, we investigate a generalized form of Vidale-Wolfe (GVW)
model. One key element of our modeling work is that the GVW model contains two
useful indexes representing advertiser's elasticity and the word-of-mouth (WoM)
effect, respectively. Moreover, we discuss some desirable properties of the GVW
model, and present a deep neural network (DNN)-based estimation method to learn
its parameters. Furthermore, based on three realworld datasets, we conduct
computational experiments to validate the GVW model and identified properties.
In addition, we also discuss potential advantages of the GVW model over
econometric models. The research outcome shows that both the ad elasticity
index and the WoM index have significant influences on advertising responses,
and the GVW model has potential advantages over econometric models of
advertising, in terms of several interesting phenomena drawn from practical
advertising situations. The GVW model and its deep learning-based estimation
method provide a basis to support big data-driven advertising analytics and
decision makings; in the meanwhile, identified properties and experimental
findings of this research illuminate critical managerial insights for
advertisers in various advertising forms."
"The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",0.111428,"Construction Grammar (CxG) is a paradigm from cognitive linguistics
emphasising the connection between syntax and semantics. Rather than rules that
operate on lexical items, it posits constructions as the central building
blocks of language, i.e., linguistic units of different granularity that
combine syntax and semantics. As a first step towards assessing the
compatibility of CxG with the syntactic and semantic knowledge demonstrated by
state-of-the-art pretrained language models (PLMs), we present an investigation
of their capability to classify and understand one of the most commonly studied
constructions, the English comparative correlative (CC). We conduct experiments
examining the classification accuracy of a syntactic probe on the one hand and
the models' behaviour in a semantic application task on the other, with BERT,
RoBERTa, and DeBERTa as the example PLMs. Our results show that all three
investigated PLMs are able to recognise the structure of the CC but fail to use
its meaning. While human-like performance of PLMs on many NLP tasks has been
alleged, this indicates that PLMs still suffer from substantial shortcomings in
central domains of linguistic knowledge."
On the Transformation of Latent Space in Fine-Tuned NLP Models,0.140874,"We study the evolution of latent space in fine-tuned NLP models. Different
from the commonly used probing-framework, we opt for an unsupervised method to
analyze representations. More specifically, we discover latent concepts in the
representational space using hierarchical clustering. We then use an alignment
function to gauge the similarity between the latent space of a pre-trained
model and its fine-tuned version. We use traditional linguistic concepts to
facilitate our understanding and also study how the model space transforms
towards task-specific information. We perform a thorough analysis, comparing
pre-trained and fine-tuned models across three models and three downstream
tasks. The notable findings of our work are: i) the latent space of the higher
layers evolve towards task-specific concepts, ii) whereas the lower layers
retain generic concepts acquired in the pre-trained model, iii) we discovered
that some concepts in the higher layers acquire polarity towards the output
class, and iv) that these concepts can be used for generating adversarial
triggers."
Vector Quantized Semantic Communication System,0.180754,"Although analog semantic communication systems have received considerable
attention in the literature, there is less work on digital semantic
communication systems. In this paper, we develop a deep learning (DL)-enabled
vector quantized (VQ) semantic communication system for image transmission,
named VQ-DeepSC. Specifically, we propose a convolutional neural network
(CNN)-based transceiver to extract multi-scale semantic features of images and
introduce multi-scale semantic embedding spaces to perform semantic feature
quantization, rendering the data compatible with digital communication systems.
Furthermore, we employ adversarial training to improve the quality of received
images by introducing a PatchGAN discriminator. Experimental results
demonstrate that the proposed VQ-DeepSC is more robustness than BPG in digital
communication systems and has comparable MS-SSIM performance to the DeepJSCC
method."
Low-resource Neural Machine Translation with Cross-modal Alignment,0.181888,"How to achieve neural machine translation with limited parallel data?
Existing techniques often rely on large-scale monolingual corpora, which is
impractical for some low-resource languages. In this paper, we turn to connect
several low-resource languages to a particular high-resource one by additional
visual modality. Specifically, we propose a cross-modal contrastive learning
method to learn a shared space for all languages, where both a coarse-grained
sentence-level objective and a fine-grained token-level one are introduced.
Experimental results and further analysis show that our method can effectively
learn the cross-modal and cross-lingual alignment with a small amount of
image-text pairs and achieves significant improvements over the text-only
baseline under both zero-shot and few-shot scenarios."
Semantic Similarity Computing Model Based on Multi Model Fine-Grained Nonlinear Fusion,0.147768,"Natural language processing (NLP) task has achieved excellent performance in
many fields, including semantic understanding, automatic summarization, image
recognition and so on. However, most of the neural network models for NLP
extract the text in a fine-grained way, which is not conducive to grasp the
meaning of the text from a global perspective. To alleviate the problem, the
combination of the traditional statistical method and deep learning model as
well as a novel model based on multi model nonlinear fusion are proposed in
this paper. The model uses the Jaccard coefficient based on part of speech,
Term Frequency-Inverse Document Frequency (TF-IDF) and word2vec-CNN algorithm
to measure the similarity of sentences respectively. According to the
calculation accuracy of each model, the normalized weight coefficient is
obtained and the calculation results are compared. The weighted vector is input
into the fully connected neural network to give the final classification
results. As a result, the statistical sentence similarity evaluation algorithm
reduces the granularity of feature extraction, so it can grasp the sentence
features globally. Experimental results show that the matching of sentence
similarity calculation method based on multi model nonlinear fusion is 84%, and
the F1 value of the model is 75%."
Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning,0.119566,"In this paper, we study the named entity recognition (NER) problem under
distant supervision. Due to the incompleteness of the external dictionaries
and/or knowledge bases, such distantly annotated training data usually suffer
from a high false negative rate. To this end, we formulate the Distantly
Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)
learning and propose a theoretically and practically novel CONFidence-based MPU
(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of
two steps. First, a confidence score is estimated for each token of being an
entity token. Then, the proposed Conf-MPU risk estimation is applied to train a
multi-class classifier for the NER task. Thorough experiments on two benchmark
datasets labeled by various external knowledge demonstrate the superiority of
the proposed Conf-MPU over existing DS-NER methods."
A new Hyper-heuristic based on Adaptive Simulated Annealing and Reinforcement Learning for the Capacitated Electric Vehicle Routing Problem,0.213073,"Electric vehicles (EVs) have been adopted in urban areas to reduce
environmental pollution and global warming as a result of the increasing number
of freight vehicles. However, there are still deficiencies in routing the
trajectories of last-mile logistics that continue to impact social and economic
sustainability. For that reason, in this paper, a hyper-heuristic (HH) approach
called Hyper-heuristic Adaptive Simulated Annealing with Reinforcement Learning
(HHASA$_{RL}$) is proposed. It is composed of a multi-armed bandit method and
the self-adaptive Simulated Annealing (SA) metaheuristic algorithm for solving
the problem called Capacitated Electric Vehicle Routing Problem (CEVRP). Due to
the limited number of charging stations and the travel range of EVs, the EVs
must require battery recharging moments in advance and reduce travel times and
costs. The HH implemented improves multiple minimum best-known solutions and
obtains the best mean values for some high-dimensional instances for the
proposed benchmark for the IEEE WCCI2020 competition."
Balancing Discriminability and Transferability for Source-Free Domain Adaptation,0.18218,"Conventional domain adaptation (DA) techniques aim to improve domain
transferability by learning domain-invariant representations; while
concurrently preserving the task-discriminability knowledge gathered from the
labeled source data. However, the requirement of simultaneous access to labeled
source and unlabeled target renders them unsuitable for the challenging
source-free DA setting. The trivial solution of realizing an effective original
to generic domain mapping improves transferability but degrades task
discriminability. Upon analyzing the hurdles from both theoretical and
empirical standpoints, we derive novel insights to show that a mixup between
original and corresponding translated generic samples enhances the
discriminability-transferability trade-off while duly respecting the
privacy-oriented source-free setting. A simple but effective realization of the
proposed insights on top of the existing source-free DA approaches yields
state-of-the-art performance with faster convergence. Beyond single-source, we
also outperform multi-source prior-arts across both classification and semantic
segmentation benchmarks."
Czech Dataset for Cross-lingual Subjectivity Classification,0.1393,"In this paper, we introduce a new Czech subjectivity dataset of 10k manually
annotated subjective and objective sentences from movie reviews and
descriptions. Our prime motivation is to provide a reliable dataset that can be
used with the existing English dataset as a benchmark to test the ability of
pre-trained multilingual models to transfer knowledge between Czech and English
and vice versa. Two annotators annotated the dataset reaching 0.83 of the
Cohen's \k{appa} inter-annotator agreement. To the best of our knowledge, this
is the first subjectivity dataset for the Czech language. We also created an
additional dataset that consists of 200k automatically labeled sentences. Both
datasets are freely available for research purposes. Furthermore, we fine-tune
five pre-trained BERT-like models to set a monolingual baseline for the new
dataset and we achieve 93.56% of accuracy. We fine-tune models on the existing
English dataset for which we obtained results that are on par with the current
state-of-the-art results. Finally, we perform zero-shot cross-lingual
subjectivity classification between Czech and English to verify the usability
of our dataset as the cross-lingual benchmark. We compare and discuss the
cross-lingual and monolingual results and the ability of multilingual models to
transfer knowledge between languages."
Multi-sensor large-scale dataset for multi-view 3D reconstruction,0.119322,"We present a new multi-sensor dataset for multi-view 3D surface
reconstruction. It includes registered RGB and depth data from sensors of
different resolutions and modalities: smartphones, Intel RealSense, Microsoft
Kinect, industrial cameras, and structured-light scanner. The scenes are
selected to emphasize a diverse set of material properties challenging for
existing algorithms. We provide around 1.4 million images of 107 different
scenes acquired from 100 viewing directions under 14 lighting conditions. We
expect our dataset will be useful for evaluation and training of 3D
reconstruction algorithms and for related tasks. The dataset is available at
skoltech3d.appliedai.tech."
Identifying Electrocardiogram Abnormalities Using a Handcrafted-Rule-Enhanced Neural Network,0.139359,"A large number of people suffer from life-threatening cardiac abnormalities,
and electrocardiogram (ECG) analysis is beneficial to determining whether an
individual is at risk of such abnormalities. Automatic ECG classification
methods, especially the deep learning based ones, have been proposed to detect
cardiac abnormalities using ECG records, showing good potential to improve
clinical diagnosis and help early prevention of cardiovascular diseases.
However, the predictions of the known neural networks still do not
satisfactorily meet the needs of clinicians, and this phenomenon suggests that
some information used in clinical diagnosis may not be well captured and
utilized by these methods. In this paper, we introduce some rules into
convolutional neural networks, which help present clinical knowledge to deep
learning based ECG analysis, in order to improve automated ECG diagnosis
performance. Specifically, we propose a Handcrafted-Rule-enhanced Neural
Network (called HRNN) for ECG classification with standard 12-lead ECG input,
which consists of a rule inference module and a deep learning module.
Experiments on two large-scale public ECG datasets show that our new approach
considerably outperforms existing state-of-the-art methods. Further, our
proposed approach not only can improve the diagnosis performance, but also can
assist in detecting mislabelled ECG samples. Our codes are available at
https://github.com/alwaysbyx/ecg_processing."
Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search,0.141384,"Abstractive summarization systems today produce fluent and relevant output,
but often ""hallucinate"" statements not supported by the source text. We analyze
the connection between hallucinations and training data, and find evidence that
models hallucinate because they train on target summaries that are unsupported
by the source. Based on our findings, we present PINOCCHIO, a new decoding
method that improves the consistency of a transformer-based abstractive
summarizer by constraining beam search to avoid hallucinations. Given the model
states and outputs at a given step, PINOCCHIO detects likely model
hallucinations based on various measures of attribution to the source text.
PINOCCHIO backtracks to find more consistent output, and can opt to produce no
summary at all when no consistent generation can be found. In experiments, we
find that PINOCCHIO improves the consistency of generation (in terms of F1) by
an average of~67% on two abstractive summarization datasets."
R2D2: Robust Data-to-Text with Replacement Detection,0.187147,"Unfaithful text generation is a common problem for text generation systems.
In the case of Data-to-Text (D2T) systems, the factuality of the generated text
is particularly crucial for any real-world applications. We introduce R2D2, a
training framework that addresses unfaithful Data-to-Text generation by
training a system both as a generator and a faithfulness discriminator with
additional replacement detection and unlikelihood learning tasks. To facilitate
such training, we propose two methods for sampling unfaithful sentences. We
argue that the poor entity retrieval capability of D2T systems is one of the
primary sources of unfaithfulness, so in addition to the existing metrics, we
further propose NER-based metrics to evaluate the fidelity of D2T generations.
Our experimental results show that R2D2 systems could effectively mitigate the
unfaithful text generation, and they achieve new state-of-the-art results on
FeTaQA, LogicNLG, and ToTTo, all with significant improvements."
Visually-Augmented Language Modeling,0.209329,"Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on text-only self-supervised training with massive text
data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel latent text-image
alignment method via an image retrieval module to fetch corresponding images
given a textual context. With the visually-augmented context, VaLM uses a
visual knowledge fusion layer to enable multimodal grounded language modeling
by attending to both text context and visual knowledge in images. We evaluate
VaLM on various visual knowledge-intensive commonsense reasoning tasks, which
require visual information to excel. The experimental results illustrate that
VaLM outperforms all strong language-only and vision-language baselines with
substantial gains in reasoning object commonsense including color, size, and
shape. Our code is available at https://github.com/Victorwz/VaLM."
"Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",0.133078,"Natural language processing technology has rapidly improved automated
grammatical error correction tasks, and the community begins to explore
document-level revision as one of the next challenges. To go beyond
sentence-level automated grammatical error correction to NLP-based
document-level revision assistant, there are two major obstacles: (1) there are
few public corpora with document-level revisions being annotated by
professional editors, and (2) it is not feasible to elicit all possible
references and evaluate the quality of revision with such references because
there are infinite possibilities of revision. This paper tackles these
challenges. First, we introduce a new document-revision corpus, TETRA, where
professional editors revised academic papers sampled from the ACL anthology
which contain few trivial grammatical errors that enable us to focus more on
document- and paragraph-level edits such as coherence and consistency. Second,
we explore reference-less and interpretable methods for meta-evaluation that
can detect quality improvements by document revision. We show the uniqueness of
TETRA compared with existing document revision corpora and demonstrate that a
fine-tuned pre-trained language model can discriminate the quality of documents
after revision even when the difference is subtle. This promising result will
encourage the community to further explore automated document revision models
and metrics in future."
Transformer Quality in Linear Time,0.233874,"We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling."
Linearizing Transformer with Key-Value Memory,0.2387,"Efficient transformer variants with linear time complexity have been
developed to mitigate the quadratic computational overhead of the vanilla
transformer. Among them are low-rank projection methods such as Linformer and
kernel-based Transformers. Despite their unique merits, they usually suffer
from a performance drop comparing with the vanilla transformer on many sequence
generation tasks, and often fail to obtain computation gain when the generation
is short. We propose MemSizer, an approach towards closing the performance gap
while improving the efficiency even with short generation. It projects the
source sequences into lower dimension representations like Linformer, while
enjoying efficient recurrent-style incremental computation similar to
kernel-based transformers. This yields linear computation time and constant
memory complexity at inference time. MemSizer also employs a lightweight
multi-head mechanism which renders the computation as light as a single-head
model. We demonstrate that MemSizer provides an improved balance between
efficiency and accuracy over the vanilla transformer and other efficient
transformer variants in three typical sequence generation tasks, including
machine translation, abstractive text summarization, and language modeling."
Hyperspherical Consistency Regularization,0.224419,"Recent advances in contrastive learning have enlightened diverse applications
across various semi-supervised fields. Jointly training supervised learning and
unsupervised learning with a shared feature encoder becomes a common scheme.
Though it benefits from taking advantage of both feature-dependent information
from self-supervised learning and label-dependent information from supervised
learning, this scheme remains suffering from bias of the classifier. In this
work, we systematically explore the relationship between self-supervised
learning and supervised learning, and study how self-supervised learning helps
robust data-efficient deep learning. We propose hyperspherical consistency
regularization (HCR), a simple yet effective plug-and-play method, to
regularize the classifier using feature-dependent information and thus avoid
bias from labels. Specifically, HCR first projects logits from the classifier
and feature projections from the projection head on the respective hypersphere,
then it enforces data points on hyperspheres to have similar structures by
minimizing binary cross entropy of pairwise distances' similarity metrics.
Extensive experiments on semi-supervised and weakly-supervised learning
demonstrate the effectiveness of our method, by showing superior performance
with HCR."
TweetNLP: Cutting-Edge Natural Language Processing for Social Media,0.267422,"In this paper we present TweetNLP, an integrated platform for Natural
Language Processing (NLP) in social media. TweetNLP supports a diverse set of
NLP tasks, including generic focus areas such as sentiment analysis and named
entity recognition, as well as social media-specific tasks such as emoji
prediction and offensive language identification. Task-specific systems are
powered by reasonably-sized Transformer-based language models specialized on
social media text (in particular, Twitter) which can be run without the need
for dedicated hardware or cloud services. The main contributions of TweetNLP
are: (1) an integrated Python library for a modern toolkit supporting social
media analysis using our various task-specific models adapted to the social
domain; (2) an interactive online demo for codeless experimentation using our
models; and (3) a tutorial covering a wide variety of typical social media
applications."
RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation,0.297934,"Category-level object pose estimation aims to predict the 6D pose as well as
the 3D metric size of arbitrary objects from a known set of categories. Recent
methods harness shape prior adaptation to map the observed point cloud into the
canonical space and apply Umeyama algorithm to recover the pose and size.
However, their shape prior integration strategy boosts pose estimation
indirectly, which leads to insufficient pose-sensitive feature extraction and
slow inference speed. To tackle this problem, in this paper, we propose a novel
geometry-guided Residual Object Bounding Box Projection network RBP-Pose that
jointly predicts object pose and residual vectors describing the displacements
from the shape-prior-indicated object surface projections on the bounding box
towards the real surface projections. Such definition of residual vectors is
inherently zero-mean and relatively small, and explicitly encapsulates spatial
cues of the 3D object for robust and accurate pose regression. We enforce
geometry-aware consistency terms to align the predicted pose and residual
vectors to further boost performance."
Deep Leaning-Based Ultra-Fast Stair Detection,0.234668,"Staircases are some of the most common building structures in urban
environments. Stair detection is an important task for various applications,
including the environmental perception of exoskeleton robots, humanoid robots,
and rescue robots and the navigation of visually impaired people. Most existing
stair detection algorithms have difficulty dealing with the diversity of stair
structure materials, extreme light and serious occlusion. Inspired by human
perception, we propose an end-to-end method based on deep learning.
Specifically, we treat the process of stair line detection as a multitask
involving coarse-grained semantic segmentation and object detection. The input
images are divided into cells, and a simple neural network is used to judge
whether each cell contains stair lines. For cells containing stair lines, the
locations of the stair lines relative to each cell are regressed. Extensive
experiments on our dataset show that our method can achieve high performance in
terms of both speed and accuracy. A lightweight version can even achieve 300+
frames per second with the same resolution. Our code and dataset will be soon
available at GitHub."
Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance,0.306752,"Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results."
Understanding Iterative Revision from Human-Written Text,0.267414,"Writing is, by nature, a strategic, adaptive, and more importantly, an
iterative process. A crucial part of writing is editing and revising the text.
Previous works on text revision have focused on defining edit intention
taxonomies within a single domain or developing computational models with a
single level of edit granularity, such as sentence-level edits, which differ
from human's revision cycles. This work describes IteraTeR: the first
large-scale, multi-domain, edit-intention annotated corpus of iteratively
revised text. In particular, IteraTeR is collected based on a new framework to
comprehensively model the iterative text revisions that generalize to various
domains of formal writing, edit intentions, revision depths, and granularities.
When we incorporate our annotated edit intentions, both generative and
edit-based text revision models significantly improve automatic evaluations.
Through our work, we better understand the text revision process, making vital
connections between edit intentions and writing quality, enabling the creation
of diverse corpora to support computational modeling of iterative text
revisions."
On Improving Cross-dataset Generalization of Deepfake Detectors,0.303505,"Facial manipulation by deep fake has caused major security risks and raised
severe societal concerns. As a countermeasure, a number of deep fake detection
methods have been proposed recently. Most of them model deep fake detection as
a binary classification problem using a backbone convolutional neural network
(CNN) architecture pretrained for the task. These CNN-based methods have
demonstrated very high efficacy in deep fake detection with the Area under the
Curve (AUC) as high as 0.99. However, the performance of these methods degrades
significantly when evaluated across datasets. In this paper, we formulate deep
fake detection as a hybrid combination of supervised and reinforcement learning
(RL) to improve its cross-dataset generalization performance. The proposed
method chooses the top-k augmentations for each test sample by an RL agent in
an image-specific manner. The classification scores, obtained using CNN, of all
the augmentations of each test image are averaged together for final real or
fake classification. Through extensive experimental validation, we demonstrate
the superiority of our method over existing published research in cross-dataset
generalization of deep fake detectors, thus obtaining state-of-the-art
performance."
Few-Shot Table-to-Text Generation with Prefix-Controlled Generator,0.232355,"Neural table-to-text generation approaches are data-hungry, limiting their
adaptation for low-resource real-world applications. Previous works mostly
resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a
table. However, they often contain hallucinated contents due to the
uncontrolled nature of PLMs. Moreover, the topological differences between
tables and sequences are rarely studied. Last but not least, fine-tuning on
PLMs with a handful of instances may lead to over-fitting and catastrophic
forgetting. To alleviate these problems, we propose a prompt-based approach,
Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation.
We prepend a task-specific prefix for a PLM to make the table structure better
fit the pre-trained input. In addition, we generate an input-specific prefix to
control the factual contents and word order of the generated text. Both
automatic and human evaluations on different domains (humans, books and songs)
of the Wikibio dataset show substantial improvements over baseline approaches."
Learning to Detect Mobile Objects from LiDAR Scans Without Labels,0.253891,"Current 3D object detectors for autonomous driving are almost entirely
trained on human-annotated data. Although of high quality, the generation of
such data is laborious and costly, restricting them to a few specific locations
and object types. This paper proposes an alternative approach entirely based on
unlabeled data, which can be collected cheaply and in abundance almost
everywhere on earth. Our approach leverages several simple common sense
heuristics to create an initial set of approximate seed labels. For example,
relevant traffic participants are generally not persistent across multiple
traversals of the same route, do not fly, and are never under ground. We
demonstrate that these seed labels are highly effective to bootstrap a
surprisingly accurate detector through repeated self-training without a single
human annotated label."
Taylor Genetic Programming for Symbolic Regression,0.301101,"Genetic programming (GP) is a commonly used approach to solve symbolic
regression (SR) problems. Compared with the machine learning or deep learning
methods that depend on the pre-defined model and the training dataset for
solving SR problems, GP is more focused on finding the solution in a search
space. Although GP has good performance on large-scale benchmarks, it randomly
transforms individuals to search results without taking advantage of the
characteristics of the dataset. So, the search process of GP is usually slow,
and the final results could be unstable.To guide GP by these characteristics,
we propose a new method for SR, called Taylor genetic programming (TaylorGP)
(Code and appendix at https://kgae-cup.github.io/TaylorGP/). TaylorGP leverages
a Taylor polynomial to approximate the symbolic equation that fits the dataset.
It also utilizes the Taylor polynomial to extract the features of the symbolic
equation: low order polynomial discrimination, variable separability, boundary,
monotonic, and parity. GP is enhanced by these Taylor polynomial techniques.
Experiments are conducted on three kinds of benchmarks: classical SR, machine
learning, and physics. The experimental results show that TaylorGP not only has
higher accuracy than the nine baseline methods, but also is faster in finding
stable results."
A Self-Guided Framework for Radiology Report Generation,0.323976,"Automatic radiology report generation is essential to computer-aided
diagnosis. Through the success of image captioning, medical report generation
has been achievable. However, the lack of annotated disease labels is still the
bottleneck of this area. In addition, the image-text data bias problem and
complex sentences make it more difficult to generate accurate reports. To
address these gaps, we pre-sent a self-guided framework (SGF), a suite of
unsupervised and supervised deep learning methods to mimic the process of human
learning and writing. In detail, our framework obtains the domain knowledge
from medical reports with-out extra disease labels and guides itself to extract
fined-grain visual features as-sociated with the text. Moreover, SGF
successfully improves the accuracy and length of medical report generation by
incorporating a similarity comparison mechanism that imitates the process of
human self-improvement through compar-ative practice. Extensive experiments
demonstrate the utility of our SGF in the majority of cases, showing its
superior performance over state-of-the-art meth-ods. Our results highlight the
capacity of the proposed framework to distinguish fined-grained visual details
between words and verify its advantage in generating medical reports."
Modern Machine-Learning Predictive Models for Diagnosing Infectious Diseases,0.300479,"Controlling infectious diseases is a major health priority because they can
spread and infect humans, thus evolving into epidemics or pandemics. Therefore,
early detection of infectious diseases is a significant need, and many
researchers have developed models to diagnose them in the early stages. This
paper reviewed research articles for recent machine-learning (ML) algorithms
applied to infectious disease diagnosis. We searched the Web of Science,
ScienceDirect, PubMed, Springer, and IEEE databases from 2015 to 2022,
identified the pros and cons of the reviewed ML models, and discussed the
possible recommendations to advance the studies in this field. We found that
most of the articles used small datasets, and few of them used real-time data.
Our results demonstrated that a suitable ML technique depends on the nature of
the dataset and the desired goal."
Zero-Shot Video Captioning with Evolving Pseudo-Tokens,0.281674,"We introduce a zero-shot video captioning method that employs two frozen
networks: the GPT-2 language model and the CLIP image-text matching model. The
matching score is used to steer the language model toward generating a sentence
that has a high average matching score to a subset of the video frames. Unlike
zero-shot image captioning methods, our work considers the entire sentence at
once. This is achieved by optimizing, during the generation process, part of
the prompt from scratch, by modifying the representation of all other tokens in
the prompt, and by repeating the process iteratively, gradually improving the
specificity and comprehensiveness of the generated sentence. Our experiments
show that the generated captions are coherent and display a broad range of
real-world knowledge. Our code is available at:
https://github.com/YoadTew/zero-shot-video-to-text"
Revisiting End-to-End Speech-to-Text Translation From Scratch,0.259694,"End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining
its encoder and/or decoder using source transcripts via speech recognition or
text translation tasks, without which translation performance drops
substantially. However, transcripts are not always available, and how
significant such pretraining is for E2E ST has rarely been studied in the
literature. In this paper, we revisit this question and explore the extent to
which the quality of E2E ST trained on speech-translation pairs alone can be
improved. We reexamine several techniques proven beneficial to ST previously,
and offer a set of best practices that biases a Transformer-based E2E ST system
toward training from scratch. Besides, we propose parameterized distance
penalty to facilitate the modeling of locality in the self-attention model for
speech. On four benchmarks covering 23 languages, our experiments show that,
without using any transcripts or pretraining, the proposed system reaches and
even outperforms previous studies adopting pretraining, although the gap
remains in (extremely) low-resource settings. Finally, we discuss neural
acoustic feature modeling, where a neural model is designed to extract acoustic
features from raw speech signals directly, with the goal to simplify inductive
biases and add freedom to the model in describing speech. For the first time,
we demonstrate its feasibility and show encouraging results on ST tasks."
UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture,0.297793,"We present UnrealEgo, i.e., a new large-scale naturalistic dataset for
egocentric 3D human pose estimation. UnrealEgo is based on an advanced concept
of eyeglasses equipped with two fisheye cameras that can be used in
unconstrained environments. We design their virtual prototype and attach them
to 3D human models for stereo view capture. We next generate a large corpus of
human motions. As a consequence, UnrealEgo is the first dataset to provide
in-the-wild stereo images with the largest variety of motions among existing
egocentric datasets. Furthermore, we propose a new benchmark method with a
simple but effective idea of devising a 2D keypoint estimation module for
stereo inputs to improve 3D human pose estimation. The extensive experiments
show that our approach outperforms the previous state-of-the-art methods
qualitatively and quantitatively. UnrealEgo and our source codes are available
on our project web page."
"Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022",0.230731,"The Event Causality Identification Shared Task of CASE 2022 involved two
subtasks working on the Causal News Corpus. Subtask 1 required participants to
predict if a sentence contains a causal relation or not. This is a supervised
binary classification task. Subtask 2 required participants to identify the
Cause, Effect and Signal spans per causal sentence. This could be seen as a
supervised sequence labeling task. For both subtasks, participants uploaded
their predictions for a held-out test set, and ranking was done based on binary
F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes
the work of the 17 teams that submitted their results to our competition and 12
system description papers that were received. The best F1 scores achieved for
Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing
approaches involved pre-trained language models fine-tuned to the targeted
task. We further discuss these approaches and analyze errors across
participants' systems in this paper."
Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian,0.236385,"We present the shared task on artificial text detection in Russian, which is
organized as a part of the Dialogue Evaluation initiative, held in 2022. The
shared task dataset includes texts from 14 text generators, i.e., one human
writer and 13 text generative models fine-tuned for one or more of the
following generation tasks: machine translation, paraphrase generation, text
summarization, text simplification. We also consider back-translation and
zero-shot generation approaches. The human-written texts are collected from
publicly available resources across multiple domains. The shared task consists
of two sub-tasks: (i) to determine if a given text is automatically generated
or written by a human; (ii) to identify the author of a given text. The first
task is framed as a binary classification problem. The second task is a
multi-class classification problem. We provide count-based and BERT-based
baselines, along with the human evaluation on the first sub-task. A total of 30
and 8 systems have been submitted to the binary and multi-class sub-tasks,
correspondingly. Most teams outperform the baselines by a wide margin. We
publicly release our codebase, human evaluation results, and other materials in
our GitHub repository (https://github.com/dialogue-evaluation/RuATD)."
Multi-Scale Representation Learning on Proteins,0.311931,"Proteins are fundamental biological entities mediating key roles in cellular
function and disease. This paper introduces a multi-scale graph construction of
a protein -- HoloProt -- connecting surface to structure and sequence. The
surface captures coarser details of the protein, while sequence as primary
component and structure -- comprising secondary and tertiary components --
capture finer details. Our graph encoder then learns a multi-scale
representation by allowing each level to integrate the encoding from level(s)
below with the graph at that level. We test the learned representation on
different tasks, (i.) ligand binding affinity (regression), and (ii.) protein
function prediction (classification). On the regression task, contrary to
previous methods, our model performs consistently and reliably across different
dataset splits, outperforming all baselines on most splits. On the
classification task, it achieves a performance close to the top-performing
model while using 10x fewer parameters. To improve the memory efficiency of our
construction, we segment the multiplex protein surface manifold into molecular
superpixels and substitute the surface with these superpixels at little to no
performance loss."
MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors,0.296033,"In this paper, we propose MOTRv2, a simple yet effective pipeline to
bootstrap end-to-end multi-object tracking with a pretrained object detector.
Existing end-to-end methods, MOTR and TrackFormer are inferior to their
tracking-by-detection counterparts mainly due to their poor detection
performance. We aim to improve MOTR by elegantly incorporating an extra object
detector. We first adopt the anchor formulation of queries and then use an
extra object detector to generate proposals as anchors, providing detection
prior to MOTR. The simple modification greatly eases the conflict between joint
learning detection and association tasks in MOTR. MOTRv2 keeps the query
propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the
1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in
Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on
the BDD100K dataset. We hope this simple and effective pipeline can provide
some new insights to the end-to-end MOT community. Code is available at
\url{https://github.com/megvii-research/MOTRv2}."
Tools and Practices for Responsible AI Engineering,0.32918,"Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs."
Continual Learning with Recursive Gradient Optimization,0.271006,"Learning multiple tasks sequentially without forgetting previous knowledge,
called Continual Learning(CL), remains a long-standing challenge for neural
networks. Most existing methods rely on additional network capacity or data
replay. In contrast, we introduce a novel approach which we refer to as
Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated
optimizer that modifies the gradient to minimize forgetting without data replay
and a virtual Feature Encoding Layer(FEL) that represents different long-term
structures with only task descriptors. Experiments demonstrate that RGO has
significantly better performance on popular continual classification benchmarks
when compared to the baselines and achieves new state-of-the-art performance on
20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher
average accuracy than Single-Task Learning(STL), this method is flexible and
reliable to provide continual learning capabilities for learning models that
rely on gradient descent."
Style Matters! Investigating Linguistic Style in Online Communities,0.276514,"Content has historically been the primary lens used to study language in
online communities. This paper instead focuses on the linguistic style of
communities. While we know that individuals have distinguishable styles, here
we ask whether communities have distinguishable styles. Additionally, while
prior work has relied on a narrow definition of style, we employ a broad
definition involving 262 features to analyze the linguistic style of 9 online
communities from 3 social media platforms discussing politics, television and
travel. We find that communities indeed have distinct styles. Also, style is an
excellent predictor of group membership (F-score 0.952 and Accuracy 96.09%).
While on average it is statistically equivalent to predictions using content
alone, it is more resilient to reductions in training data."
Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,0.271236,"We introduce camouflaged data poisoning attacks, a new attack vector that
arises in the context of machine unlearning and other settings when model
retraining may be induced. An adversary first adds a few carefully crafted
points to the training dataset such that the impact on the model's predictions
is minimal. The adversary subsequently triggers a request to remove a subset of
the introduced points at which point the attack is unleashed and the model's
predictions are negatively affected. In particular, we consider clean-label
targeted attacks (in which the goal is to cause the model to misclassify a
specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.
This attack is realized by constructing camouflage datapoints that mask the
effect of a poisoned dataset."
CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability,0.297121,"Convolutional neural networks (CNNs) have dominated the field of computer
vision for nearly a decade due to their strong ability to learn local features.
However, due to their limited receptive field, CNNs fail to model the global
context. On the other hand, transformer, an attention-based architecture can
model the global context easily. Despite this, there are limited studies that
investigate the effectiveness of transformers in crowd counting. In addition,
the majority of the existing crowd counting methods are based on the regression
of density maps which requires point-level annotation of each person present in
the scene. This annotation task is laborious and also error-prone. This has led
to increased focus on weakly-supervised crowd counting methods which require
only the count-level annotations. In this paper, we propose a weakly-supervised
method for crowd counting using a pyramid vision transformer. We have conducted
extensive evaluations to validate the effectiveness of the proposed method. Our
method is comparable to the state-of-the-art on the benchmark crowd datasets.
More importantly, it shows remarkable generalizability."
Real-time Online Multi-Object Tracking in Compressed Domain,0.271508,"Recent online Multi-Object Tracking (MOT) methods have achieved desirable
tracking performance. However, the tracking speed of most existing methods is
rather slow. Inspired from the fact that the adjacent frames are highly
relevant and redundant, we divide the frames into key and non-key frames
respectively and track objects in the compressed domain. For the key frames,
the RGB images are restored for detection and data association. To make data
association more reliable, an appearance Convolutional Neural Network (CNN)
which can be jointly trained with the detector is proposed. For the non-key
frames, the objects are directly propagated by a tracking CNN based on the
motion information provided in the compressed domain. Compared with the
state-of-the-art online MOT methods,our tracker is about 6x faster while
maintaining a comparable tracking performance."
mGPT: Few-Shot Learners Go Multilingual,0.286719,"Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released."
Cycle-Consistent Counterfactuals by Latent Transformations,0.248931,"CounterFactual (CF) visual explanations try to find images similar to the
query image that change the decision of a vision system to a specified outcome.
Existing methods either require inference-time optimization or joint training
with a generative adversarial model which makes them time-consuming and
difficult to use in practice. We propose a novel approach, Cycle-Consistent
Counterfactuals by Latent Transformations (C3LT), which learns a latent
transformation that automatically generates visual CFs by steering in the
latent space of generative models. Our method uses cycle consistency between
the query and CF latent representations which helps our training to find better
solutions. C3LT can be easily plugged into any state-of-the-art pretrained
generative network. This enables our method to generate high-quality and
interpretable CF images at high resolution such as those in ImageNet. In
addition to several established metrics for evaluating CF explanations, we
introduce a novel metric tailored to assess the quality of the generated CF
examples and validate the effectiveness of our method on an extensive set of
experiments."
Transformer based Urdu Handwritten Text Optical Character Reader,0.297542,"Extracting Handwritten text is one of the most important components of
digitizing information and making it available for large scale setting.
Handwriting Optical Character Reader (OCR) is a research problem in computer
vision and natural language processing computing, and a lot of work has been
done for English, but unfortunately, very little work has been done for low
resourced languages such as Urdu. Urdu language script is very difficult
because of its cursive nature and change of shape of characters based on it's
relative position, therefore, a need arises to propose a model which can
understand complex features and generalize it for every kind of handwriting
style. In this work, we propose a transformer based Urdu Handwritten text
extraction model. As transformers have been very successful in Natural Language
Understanding task, we explore them further to understand complex Urdu
Handwriting."
Text normalization for low-resource languages: the case of Ligurian,0.270937,"Text normalization is a crucial technology for low-resource languages which
lack rigid spelling conventions or that have undergone multiple spelling
reforms. Low-resource text normalization has so far relied upon hand-crafted
rules, which are perceived to be more data efficient than neural methods. In
this paper we examine the case of text normalization for Ligurian, an
endangered Romance language. We collect 4,394 Ligurian sentences paired with
their normalized versions, as well as the first open source monolingual corpus
for Ligurian. We show that, in spite of the small amounts of data available, a
compact transformer-based model can be trained to achieve very low error rates
by the use of backtranslation and appropriate tokenization."
e-CARE: a New Dataset for Exploring Explainable Causal Reasoning,0.311227,"Understanding causality has vital importance for various Natural Language
Processing (NLP) applications. Beyond the labeled instances, conceptual
explanations of the causality can provide deep understanding of the causal
facts to facilitate the causal reasoning process. However, such explanation
information still remains absent in existing causal reasoning resources. In
this paper, we fill this gap by presenting a human-annotated explainable CAusal
REasoning dataset (e-CARE), which contains over 21K causal reasoning questions,
together with natural language formed explanations of the causal questions.
Experimental results show that generating valid explanations for causal facts
still remains especially challenging for the state-of-the-art models, and the
explanation information can be helpful for promoting the accuracy and stability
of causal reasoning models."
EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points,0.287269,"Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/."
Learning by Distilling Context,0.318681,"Language models significantly benefit from context tokens, such as prompts or
scratchpads. They perform better when prompted with informative instructions,
and they acquire new reasoning capabilities by generating a scratch-pad before
predicting the final answers. However, they do not \textit{internalize} these
performance gains, which disappear when the context tokens are gone. Our work
proposes to apply context distillation so that a language model can improve
itself by internalizing these gains. Concretely, given a synthetic unlabeled
input for the target task, we condition the model on ``[instructions] +
[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune
the same model to predict its own ``[final answer]'' conditioned on the
``[task-input]'', without seeing the ``[instructions]'' or using the
``[scratch-pad]''.
  We show that context distillation is a general method to train language
models, and it can effectively internalize 3 types of training signals. First,
it can internalize abstract task instructions and explanations, so we can
iteratively update the model parameters with new instructions and overwrite old
ones. Second, it can internalize step-by-step reasoning for complex tasks
(e.g., 8-digit addition), and such a newly acquired capability proves to be
useful for other downstream tasks. Finally, it can internalize concrete
training examples, and it outperforms directly learning with gradient descent
by 9\% on the SPIDER Text-to-SQL dataset; furthermore, combining context
distillation operations can internalize more training examples than the context
window size allows."
Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments,0.223418,"Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement."
Point-Level Region Contrast for Object Detection Pre-Training,0.264829,"In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available."
Revisiting Grammatical Error Correction Evaluation and Beyond,0.289992,"Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore
and BARTScore) have been widely used in several sentence generation tasks
(e.g., machine translation and text summarization) due to their better
correlation with human judgments over traditional overlap-based methods.
Although PT-based methods have become the de facto standard for training
grammatical error correction (GEC) systems, GEC evaluation still does not
benefit from pretrained knowledge. This paper takes the first step towards
understanding and improving GEC evaluation with pretraining. We first find that
arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory
correlation results because of the excessive attention to inessential systems
outputs (e.g., unchanged parts). To alleviate the limitation, we propose a
novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2
which only uses PT-based metrics to score those corrected parts. Experimental
results on the CoNLL14 evaluation task show that PT-M2 significantly
outperforms existing methods, achieving a new state-of-the-art result of 0.949
Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate
competitive GEC systems. Source code and scripts are freely available at
https://github.com/pygongnlp/PT-M2."
PIC4rl-gym: a ROS2 modular framework for Robots Autonomous Navigation with Deep Reinforcement Learning,0.240054,"Learning agents can optimize standard autonomous navigation improving
flexibility, efficiency, and computational cost of the system by adopting a
wide variety of approaches. This work introduces the \textit{PIC4rl-gym}, a
fundamental modular framework to enhance navigation and learning research by
mixing ROS2 and Gazebo, the standard tools of the robotics community, with Deep
Reinforcement Learning (DRL). The paper describes the whole structure of the
PIC4rl-gym, which fully integrates DRL agent's training and testing in several
indoor and outdoor navigation scenarios and tasks. A modular approach is
adopted to easily customize the simulation by selecting new platforms, sensors,
or models. We demonstrate the potential of our novel gym by benchmarking the
resulting policies, trained for different navigation tasks, with a complete set
of metrics."
Universal Conditional Masked Language Pre-training for Neural Machine Translation,0.279802,"Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching & masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low- to extremely high-resource
languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on
average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it
can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the
best of our knowledge, this is the first work to pre-train a unified model for
fine-tuning on both NMT tasks. Code, data, and pre-trained models are available
at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT."
DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation,0.288338,"In many real-world scenarios, we often deal with streaming data that is
sequentially collected over time. Due to the non-stationary nature of the
environment, the streaming data distribution may change in unpredictable ways,
which is known as concept drift. To handle concept drift, previous methods
first detect when/where the concept drift happens and then adapt models to fit
the distribution of the latest data. However, there are still many cases that
some underlying factors of environment evolution are predictable, making it
possible to model the future concept drift trend of the streaming data, while
such cases are not fully explored in previous work.
  In this paper, we propose a novel method DDG-DA, that can effectively
forecast the evolution of data distribution and improve the performance of
models. Specifically, we first train a predictor to estimate the future data
distribution, then leverage it to generate training samples, and finally train
models on the generated data. We conduct experiments on three real-world tasks
(forecasting on stock price trend, electricity load and solar irradiance) and
obtain significant improvement on multiple widely-used models."
Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches,0.251237,"Deep learning has substantially boosted the performance of Monocular Depth
Estimation (MDE), a critical component in fully vision-based autonomous driving
(AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack
against learning-based MDE. In particular, we use an optimization-based method
to systematically generate stealthy physical-object-oriented adversarial
patches to attack depth estimation. We balance the stealth and effectiveness of
our attack with object-oriented adversarial design, sensitive region
localization, and natural style camouflage. Using real-world driving scenarios,
we evaluate our attack on concurrent MDE models and a representative downstream
task for AD (i.e., 3D object detection). Experimental results show that our
method can generate stealthy, effective, and robust adversarial patches for
different target objects and models and achieves more than 6 meters mean depth
estimation error and 93% attack success rate (ASR) in object detection with a
patch of 1/9 of the vehicle's rear area. Field tests on three different driving
routes with a real vehicle indicate that we cause over 6 meters mean depth
estimation error and reduce the object detection rate from 90.70% to 5.16% in
continuous video frames."
Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,0.252805,"Previous work has shown that there exists a scaling law between the size of
Language Models (LMs) and their zero-shot performance on different downstream
NLP tasks. In this work, we show that this phenomenon does not hold when
evaluating large LMs on tasks with negated prompts, but instead shows an
inverse scaling law. We evaluate 9 different tasks with negated prompts on (1)
pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further
pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with
few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all
LM types perform worse on negated prompts as they scale and show a huge
performance gap between the human performance when comparing the average score
on both original and negated prompts. By highlighting a critical limitation of
existing LMs and methods, we urge the community to develop new approaches of
developing LMs that actually follow the given instructions. We provide the code
and the datasets to explore negated prompts at
https://github.com/joeljang/negated-prompts-for-llms"
Toward More Meaningful Resources for Lower-resourced Languages,0.291206,"In this position paper, we describe our perspective on how meaningful
resources for lower-resourced languages should be developed in connection with
the speakers of those languages. We first examine two massively multilingual
resources in detail. We explore the contents of the names stored in Wikidata
for a few lower-resourced languages and find that many of them are not in fact
in the languages they claim to be and require non-trivial effort to correct. We
discuss quality issues present in WikiAnn and evaluate whether it is a useful
supplement to hand annotated data. We then discuss the importance of creating
annotation for lower-resourced languages in a thoughtful and ethical way that
includes the languages' speakers as part of the development process. We
conclude with recommended guidelines for resource development."
Efficient Long Sequence Modeling via State Space Augmented Transformer,0.230885,"Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks."
An Online Semantic Mapping System for Extending and Enhancing Visual SLAM,0.323573,"We present a real-time semantic mapping approach for mobile vision systems
with a 2D to 3D object detection pipeline and rapid data association for
generated landmarks. Besides the semantic map enrichment the associated
detections are further introduced as semantic constraints into a simultaneous
localization and mapping (SLAM) system for pose correction purposes. This way,
we are able generate additional meaningful information that allows to achieve
higher-level tasks, while simultaneously leveraging the view-invariance of
object detections to improve the accuracy and the robustness of the odometry
estimation. We propose tracklets of locally associated object observations to
handle ambiguous and false predictions and an uncertainty-based greedy
association scheme for an accelerated processing time. Our system reaches
real-time capabilities with an average iteration duration of 65~ms and is able
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a
public dataset. Additionally, we implemented our approach as a modular ROS
package that makes it straightforward for integration in arbitrary graph-based
SLAM methods."
On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry,0.257,"We introduce a framework of the equivariant convolutional algorithms which is
tailored for a number of machine-learning tasks on physical systems with
arbitrary SU($d$) symmetries. It allows us to enhance a natural model of
quantum computation--permutational quantum computing (PQC) [Quantum Inf.
Comput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. While
PQC was shown to be effectively classically simulatable, we exhibit a problem
which can be efficiently solved on PQC+ machine, whereas the best known
classical algorithms runs in $O(n!n^2)$ time, thus providing strong evidence
against PQC+ being classically simulatable. We further discuss practical
quantum machine learning algorithms which can be carried out in the paradigm of
PQC+."
Iterative Scene Graph Generation,0.299179,"The task of scene graph generation entails identifying object entities and
their corresponding interaction predicates in a given image (or video). Due to
the combinatorially large solution space, existing approaches to scene graph
generation assume certain factorization of the joint distribution to make the
estimation feasible (e.g., assuming that objects are conditionally independent
of predicate predictions). However, this fixed factorization is not ideal under
all scenarios (e.g., for images where an object entailed in interaction is
small and not discernible on its own). In this work, we propose a novel
framework for scene graph generation that addresses this limitation, as well as
introduces dynamic conditioning on the image, using message passing in a Markov
Random Field. This is implemented as an iterative refinement procedure wherein
each modification is conditioned on the graph generated in the previous
iteration. This conditioning across refinement steps allows joint reasoning
over entities and relations. This framework is realized via a novel and
end-to-end trainable transformer-based architecture. In addition, the proposed
framework can improve existing approach performance. Through extensive
experiments on Visual Genome and Action Genome benchmark datasets we show
improved performance on the scene graph generation."
Learning Deformable Object Manipulation from Expert Demonstrations,0.300517,"We present a novel Learning from Demonstration (LfD) method, Deformable
Manipulation from Demonstrations (DMfD), to solve deformable manipulation tasks
using states or images as inputs, given expert demonstrations. Our method uses
demonstrations in three different ways, and balances the trade-off between
exploring the environment online and using guidance from experts to explore
high dimensional spaces effectively. We test DMfD on a set of representative
manipulation tasks for a 1-dimensional rope and a 2-dimensional cloth from the
SoftGym suite of tasks, each with state and image observations. Our method
exceeds baseline performance by up to 12.9% for state-based tasks and up to
33.44% on image-based tasks, with comparable or better robustness to
randomness. Additionally, we create two challenging environments for folding a
2D cloth using image-based observations, and set a performance benchmark for
them. We deploy DMfD on a real robot with a minimal loss in normalized
performance during real-world execution compared to simulation (~6%). Source
code is on github.com/uscresl/dmfd"
GazeOnce: Real-Time Multi-Person Gaze Estimation,0.290627,"Appearance-based gaze estimation aims to predict the 3D eye gaze direction
from a single image. While recent deep learning-based approaches have
demonstrated excellent performance, they usually assume one calibrated face in
each input image and cannot output multi-person gaze in real time. However,
simultaneous gaze estimation for multiple people in the wild is necessary for
real-world applications. In this paper, we propose the first one-stage
end-to-end gaze estimation method, GazeOnce, which is capable of simultaneously
predicting gaze directions for multiple faces (>10) in an image. In addition,
we design a sophisticated data generation pipeline and propose a new dataset,
MPSGaze, which contains full images of multiple people with 3D gaze ground
truth. Experimental results demonstrate that our unified framework not only
offers a faster speed, but also provides a lower gaze estimation error compared
with state-of-the-art methods. This technique can be useful in real-time
applications with multiple users."
Event Collapse in Contrast Maximization Frameworks,0.314508,"Contrast maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models."
ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification,0.230164,"Generating new events given context with correlated ones plays a crucial role
in many event-centric reasoning tasks. Existing works either limit their scope
to specific scenarios or overlook event-level correlations. In this paper, we
propose to pre-train a general Correlation-aware context-to-Event Transformer
(ClarET) for event-centric reasoning. To achieve this, we propose three novel
event-centric objectives, i.e., whole event recovering, contrastive
event-correlation encoding and prompt-based event locating, which highlight
event-level correlations with effective training. The proposed ClarET is
applicable to a wide range of event-centric reasoning scenarios, considering
its versatility of (i) event-correlation types (e.g., causal, temporal,
contrast), (ii) application formulations (i.e., generation and classification),
and (iii) reasoning types (e.g., abductive, counterfactual and ending
reasoning). Empirical fine-tuning results, as well as zero- and few-shot
learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4
reasoning types with diverse event correlations), verify its effectiveness and
generalization ability."
Diversity Enhanced Table-to-Text Generation via Type Control,0.271898,"Generating natural language statements to convey logical inferences from
tabular data (i.e., Logical NLG) is a process with one input and a variety of
valid outputs. This characteristic underscores the need for a method to produce
a diverse set of valid outputs, presenting different perspectives of the input
data. We propose a simple yet effective diversity-enhancing scheme that builds
upon an inherent property of the statements, their logic-types, by using a
type-controlled table-to-text generation model. We demonstrate, through
extensive automatic and human evaluations over the two publicly available
Logical NLG datasets, that our proposed method both facilitates the ability to
effectively control the generated statement type, and produces results superior
to the strongest baselines in terms of quality and factuality-diversity
trade-off."
CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization,0.230085,"The quest for seeking health information has swamped the web with consumers'
health-related questions. Generally, consumers use overly descriptive and
peripheral information to express their medical condition or other healthcare
needs, contributing to the challenges of natural language understanding. One
way to address this challenge is to summarize the questions and distill the key
information of the original question. To address this issue, we introduce a new
dataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health
questions and corresponding summaries. The dataset is derived from the
community question-answering forum and therefore provides a valuable resource
for understanding consumer health-related posts on social media. We benchmark
the dataset on multiple state-of-the-art summarization models to show the
effectiveness of the dataset."
Efficient Visual Tracking via Hierarchical Cross-Attention Transformer,0.261071,"In recent years, target tracking has made great progress in accuracy. This
development is mainly attributed to powerful networks (such as transformers)
and additional modules (such as online update and refinement modules). However,
less attention has been paid to tracking speed. Most state-of-the-art trackers
are satisfied with the real-time speed on powerful GPUs. However, practical
applications necessitate higher requirements for tracking speed, especially
when edge platforms with limited resources are used. In this work, we present
an efficient tracking method via a hierarchical cross-attention transformer
named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on
the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our
HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,
UAV123, and VOT2020. Code and models are available at
https://github.com/chenxin-dlut/HCAT."
D2A-BSP: Distilled Data Association Belief Space Planning with Performance Guarantees Under Budget Constraints,0.23873,"Unresolved data association in ambiguous and perceptually aliased
environments leads to multi-modal hypotheses on both the robot's and the
environment state. To avoid catastrophic results, when operating in such
ambiguous environments, it is crucial to reason about data association within
Belief Space Planning (BSP). However, explicitly considering all possible data
associations, the number of hypotheses grows exponentially with the planning
horizon and determining the optimal action sequence quickly becomes
intractable. Moreover, with hard budget constraints where some non-negligible
hypotheses must be pruned, achieving performance guarantees is crucial. In this
work we present a computationally efficient novel approach that utilizes only a
distilled subset of hypotheses to solve BSP problems while reasoning about data
association. Furthermore, to provide performance guarantees, we derive error
bounds with respect to the optimal solution. We then demonstrate our approach
in an extremely aliased environment, where we manage to significantly reduce
computation time without compromising on the quality of the solution."
Generation-Augmented Query Expansion For Code Retrieval,0.225686,"Pre-trained language models have achieved promising success in code retrieval
tasks, where a natural language documentation query is given to find the most
relevant existing code snippet. However, existing models focus only on
optimizing the documentation code pairs by embedding them into latent space,
without the association of external knowledge. In this paper, we propose a
generation-augmented query expansion framework. Inspired by the human retrieval
process - sketching an answer before searching, in this work, we utilize the
powerful code generation model to benefit the code retrieval task.
Specifically, we demonstrate that rather than merely retrieving the target code
snippet according to the documentation query, it would be helpful to augment
the documentation query with its generation counterpart - generated code
snippets from the code generation model. To the best of our knowledge, this is
the first attempt that leverages the code generation model to enhance the code
retrieval task. We achieve new state-of-the-art results on the CodeSearchNet
benchmark and surpass the baselines significantly."
LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs,0.268584,"Recent advance in 2D CNNs has revealed that large kernels are important.
However, when directly applying large convolutional kernels in 3D CNNs, severe
difficulties are met, where those successful module designs in 2D become
surprisingly ineffective on 3D networks, including the popular depth-wise
convolution. To address this vital challenge, we instead propose the
spatial-wise partition convolution and its large-kernel module. As a result, it
avoids the optimization and efficiency issues of naive 3D large kernels. Our
large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D
tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on
the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection
benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance
further boosts to 74.2% NDS with a simple multi-modal fusion. In addition,
LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object
detection. For the first time, we show that large kernels are feasible and
essential for 3D visual tasks."
FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment,0.242347,"We present Face Swapping GAN (FSGAN) for face swapping and reenactment.
Unlike previous work, we offer a subject agnostic swapping scheme that can be
applied to pairs of faces without requiring training on those faces. We derive
a novel iterative deep learning--based approach for face reenactment which
adjusts significant pose and expression variations that can be applied to a
single image or a video sequence. For video sequences, we introduce a
continuous interpolation of the face views based on reenactment, Delaunay
Triangulation, and barycentric coordinates. Occluded face regions are handled
by a face completion network. Finally, we use a face blending network for
seamless blending of the two faces while preserving the target skin color and
lighting conditions. This network uses a novel Poisson blending loss combining
Poisson optimization with a perceptual loss. We compare our approach to
existing state-of-the-art systems and show our results to be both qualitatively
and quantitatively superior. This work describes extensions of the FSGAN
method, proposed in an earlier conference version of our work, as well as
additional experiments and results."
An attention mechanism based convolutional network for satellite precipitation downscaling over China,0.267941,"Precipitation is a key part of hydrological circulation and is a sensitive
indicator of climate change. The Integrated Multi-satellitE Retrievals for the
Global Precipitation Measurement (GPM) mission (IMERG) datasets are widely used
for global and regional precipitation investigations. However, their local
application is limited by the relatively coarse spatial resolution. Therefore,
in this paper, an attention mechanism based convolutional network (AMCN) is
proposed to downscale GPM IMERG monthly precipitation data. The proposed method
is an end-to-end network, which consists of a global cross-attention module, a
multi-factor cross-attention module, and a residual convolutional module,
comprehensively considering the potential relationships between precipitation
and complicated surface characteristics. In addition, a degradation loss
function based on low-resolution precipitation is designed to physically
constrain the network training, to improve the robustness of the proposed
network under different time and scale variations. The experiments demonstrate
that the proposed network significantly outperforms three baseline methods.
Finally, a geographic difference analysis method is introduced to further
improve the downscaled results by incorporating in-situ measurements for
high-quality and fine-scale precipitation estimation."
Image-based Automatic Dial Meter Reading in Unconstrained Scenarios,0.320399,"The replacement of analog meters with smart meters is costly, laborious, and
far from complete in developing countries. The Energy Company of Parana (Copel)
(Brazil) performs more than 4 million meter readings (almost entirely of
non-smart devices) per month, and we estimate that 850 thousand of them are
from dial meters. Therefore, an image-based automatic reading system can reduce
human errors, create a proof of reading, and enable the customers to perform
the reading themselves through a mobile application. We propose novel
approaches for Automatic Dial Meter Reading (ADMR) and introduce a new dataset
for ADMR in unconstrained scenarios, called UFPR-ADMR-v2. Our best-performing
method combines YOLOv4 with a novel regression approach (AngReg), and explores
several postprocessing techniques. Compared to previous works, it decreased the
Mean Absolute Error (MAE) from 1,343 to 129 and achieved a meter recognition
rate (MRR) of 98.90% -- with an error tolerance of 1 Kilowatt-hour (kWh)."
Bayesian Learning to Discover Mathematical Operations in Governing Equations of Dynamic Systems,0.243772,"Discovering governing equations from data is critical for diverse scientific
disciplines as they can provide insights into the underlying phenomenon of
dynamic systems. This work presents a new representation for governing
equations by designing the Mathematical Operation Network (MathONet) with a
deep neural network-like hierarchical structure. Specifically, the MathONet is
stacked by several layers of unary operations (e.g., sin, cos, log) and binary
operations (e.g., +,-), respectively. An initialized MathONet is typically
regarded as a super-graph with a redundant structure, a sub-graph of which can
yield the governing equation. We develop a sparse group Bayesian learning
algorithm to extract the sub-graph by employing structurally constructed priors
over the redundant mathematical operations. By demonstrating the chaotic Lorenz
system, Lotka-Volterra system, and Kolmogorov-Petrovsky-Piskunov system, the
proposed method can discover the ordinary differential equations (ODEs) and
partial differential equations (PDEs) from the observations given limited
mathematical operations, without any prior knowledge on possible expressions of
the ODEs and PDEs."
NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages,0.323579,"Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages."
Standby-Based Deadlock Avoidance Method for Multi-Agent Pickup and Delivery Tasks,0.300571,"The multi-agent pickup and delivery (MAPD) problem, in which multiple agents
iteratively carry materials without collisions, has received significant
attention. However, many conventional MAPD algorithms assume a specifically
designed grid-like environment, such as an automated warehouse. Therefore, they
have many pickup and delivery locations where agents can stay for a lengthy
period, as well as plentiful detours to avoid collisions owing to the freedom
of movement in a grid. By contrast, because a maze-like environment such as a
search-and-rescue or construction site has fewer pickup/delivery locations and
their numbers may be unbalanced, many agents concentrate on such locations
resulting in inefficient operations, often becoming stuck or deadlocked. Thus,
to improve the transportation efficiency even in a maze-like restricted
environment, we propose a deadlock avoidance method, called standby-based
deadlock avoidance (SBDA). SBDA uses standby nodes determined in real-time
using the articulation-point-finding algorithm, and the agent is guaranteed to
stay there for a finite amount of time. We demonstrated that our proposed
method outperforms a conventional approach. We also analyzed how the parameters
used for selecting standby nodes affect the performance."
Whodunit? Learning to Contrast for Authorship Attribution,0.233312,"Authorship attribution is the task of identifying the author of a given text.
The key is finding representations that can differentiate between authors.
Existing approaches typically use manually designed features that capture a
dataset's content and style, but these approaches are dataset-dependent and
yield inconsistent performance across corpora. In this work, we propose
\textit{learning} author-specific representations by fine-tuning pre-trained
generic language representations with a contrastive objective (Contra-X). We
show that Contra-X learns representations that form highly separable clusters
for different authors. It advances the state-of-the-art on multiple human and
machine authorship attribution benchmarks, enabling improvements of up to 6.8%
over cross-entropy fine-tuning. However, we find that Contra-X improves overall
accuracy at the cost of sacrificing performance for some authors. Resolving
this tension will be an important direction for future work. To the best of our
knowledge, we are the first to integrate contrastive learning with pre-trained
language model fine-tuning for authorship attribution."
Motion Policy Networks,0.278777,"Collision-free motion generation in unknown environments is a core building
block for robot manipulation. Generating such motions is challenging due to
multiple objectives; not only should the solutions be optimal, the motion
generator itself must be fast enough for real-time performance and reliable
enough for practical deployment. A wide variety of methods have been proposed
ranging from local controllers to global planners, often being combined to
offset their shortcomings. We present an end-to-end neural model called Motion
Policy Networks (M$\pi$Nets) to generate collision-free, smooth motion from
just a single depth camera observation. M$\pi$Nets are trained on over 3
million motion planning problems in over 500,000 environments. Our experiments
show that M$\pi$Nets are significantly faster than global planners while
exhibiting the reactivity needed to deal with dynamic scenes. They are 46%
better than prior neural planners and more robust than local control policies.
Despite being only trained in simulation, M$\pi$Nets transfer well to the real
robot with noisy partial point clouds. Code and data are publicly available at
https://mpinets.github.io."
Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems,0.247393,"We prove fast mixing and characterize the stationary distribution of the
Langevin Algorithm for inverting random weighted DNN generators. This result
extends the work of Hand and Voroninski from efficient inversion to efficient
posterior sampling. In practice, to allow for increased expressivity, we
propose to do posterior sampling in the latent space of a pre-trained
generative model. To achieve that, we train a score-based model in the latent
space of a StyleGAN-2 and we use it to solve inverse problems. Our framework,
Score-Guided Intermediate Layer Optimization (SGILO), extends prior work by
replacing the sparsity regularization with a generative prior in the
intermediate layer. Experimentally, we obtain significant improvements over the
previous state-of-the-art, especially in the low measurement regime."
OptG: Optimizing Gradient-driven Criteria in Network Sparsity,0.32968,"Network sparsity receives popularity mostly due to its capability to reduce
the network complexity. Extensive studies excavate gradient-driven sparsity.
Typically, these methods are constructed upon premise of weight independence,
which however, is contrary to the fact that weights are mutually influenced.
Thus, their performance remains to be improved. In this paper, we propose to
optimize gradient-driven sparsity (OptG) by solving this independence paradox.
Our motive comes from the recent advances in supermask training which shows
that high-performing sparse subnetworks can be located by simply updating mask
values without modifying any weight. We prove that supermask training is to
accumulate the criteria of gradient-driven sparsity for both removed and
preserved weights, and it can partly solve the independence paradox.
Consequently, OptG integrates supermask training into gradient-driven sparsity,
and a novel supermask optimizer is further proposed to comprehensively mitigate
the independence paradox. Experiments show that OptG can well surpass many
existing state-of-the-art competitors, especially at ultra-high sparsity
levels. Our code is available at \url{https://github.com/zyxxmu/OptG}."
3D Equivariant Graph Implicit Functions,0.247297,"In recent years, neural implicit representations have made remarkable
progress in modeling of 3D shapes with arbitrary topology. In this work, we
address two key limitations of such representations, in failing to capture
local 3D geometric fine details, and to learn from and generalize to shapes
with unseen 3D transformations. To this end, we introduce a novel family of
graph implicit functions with equivariant layers that facilitates modeling fine
local details and guaranteed robustness to various groups of geometric
transformations, through local $k$-NN graph embeddings with sparse point set
observations at multiple resolutions. Our method improves over the existing
rotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet
reconstruction task. We also show that our equivariant implicit function can be
extended to other types of similarity transformations and generalizes to unseen
translations and scaling."
Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System,0.233649,"Contextually aware intelligent agents are often required to understand the
users and their surroundings in real-time. Our goal is to build Artificial
Intelligence (AI) systems that can assist children in their learning process.
Within such complex frameworks, Spoken Dialogue Systems (SDS) are crucial
building blocks to handle efficient task-oriented communication with children
in game-based learning settings. We are working towards a multimodal dialogue
system for younger kids learning basic math concepts. Our focus is on improving
the Natural Language Understanding (NLU) module of the task-oriented SDS
pipeline with limited datasets. This work explores the potential benefits of
data augmentation with paraphrase generation for the NLU models trained on
small task-specific datasets. We also investigate the effects of extracting
entities for conceivably further data expansion. We have shown that
paraphrasing with model-in-the-loop (MITL) strategies using small seed data is
a promising approach yielding improved performance results for the Intent
Recognition task."
Procedural Image Programs for Representation Learning,0.22317,"Learning image representations using synthetic data allows training neural
networks without some of the concerns associated with real images, such as
privacy and bias. Existing work focuses on a handful of curated generative
processes which require expert knowledge to design, making it hard to scale up.
To overcome this, we propose training with a large dataset of twenty-one
thousand programs, each one generating a diverse set of synthetic images. These
programs are short code snippets, which are easy to modify and fast to execute
using OpenGL. The proposed dataset can be used for both supervised and
unsupervised representation learning, and reduces the gap between pre-training
with real and procedurally generated images by 38%."
Optimizing Relevance Maps of Vision Transformers Improves Robustness,0.304706,"It has been observed that visual classification models often rely mostly on
the image background, neglecting the foreground, which hurts their robustness
to distribution changes. To alleviate this shortcoming, we propose to monitor
the model's relevancy signal and manipulate it such that the model is focused
on the foreground object. This is done as a finetuning step, involving
relatively few samples consisting of pairs of images and their associated
foreground masks. Specifically, we encourage the model's relevancy map (i) to
assign lower relevance to background regions, (ii) to consider as much
information as possible from the foreground, and (iii) we encourage the
decisions to have high confidence. When applied to Vision Transformer (ViT)
models, a marked improvement in robustness to domain shifts is observed.
Moreover, the foreground masks can be obtained automatically, from a
self-supervised variant of the ViT model itself; therefore no additional
supervision is required."
VALHALLA: Visual Hallucination for Machine Translation,0.251167,"Designing better machine translation systems by considering auxiliary inputs
such as images has attracted much attention in recent years. While existing
methods show promising performance over the conventional text-only translation
systems, they typically require paired text and image as input during
inference, which limits their applicability to real-world scenarios. In this
paper, we introduce a visual hallucination framework, called VALHALLA, which
requires only source sentences at inference time and instead uses hallucinated
visual representations for multimodal machine translation. In particular, given
a source sentence an autoregressive hallucination transformer is used to
predict a discrete visual representation from the input text, and the combined
text and hallucinated representations are utilized to obtain the target
translation. We train the hallucination transformer jointly with the
translation transformer using standard backpropagation with cross-entropy
losses while being guided by an additional loss that encourages consistency
between predictions using either ground-truth or hallucinated visual
representations. Extensive experiments on three standard translation datasets
with a diverse set of language pairs demonstrate the effectiveness of our
approach over both text-only baselines and state-of-the-art methods. Project
page: http://www.svcl.ucsd.edu/projects/valhalla."
Not always about you: Prioritizing community needs when developing endangered language technology,0.22662,"Languages are classified as low-resource when they lack the quantity of data
necessary for training statistical and machine learning tools and models.
Causes of resource scarcity vary but can include poor access to technology for
developing these resources, a relatively small population of speakers, or a
lack of urgency for collecting such resources in bilingual populations where
the second language is high-resource. As a result, the languages described as
low-resource in the literature are as different as Finnish on the one hand,
with millions of speakers using it in every imaginable domain, and Seneca, with
only a small-handful of fluent speakers using the language primarily in a
restricted domain. While issues stemming from the lack of resources necessary
to train models unite this disparate group of languages, many other issues cut
across the divide between widely-spoken low resource languages and endangered
languages. In this position paper, we discuss the unique technological,
cultural, practical, and ethical challenges that researchers and indigenous
speech community members face when working together to develop language
technology to support endangered language documentation and revitalization. We
report the perspectives of language teachers, Master Speakers and elders from
indigenous communities, as well as the point of view of academics. We describe
an ongoing fruitful collaboration and make recommendations for future
partnerships between academic researchers and language community stakeholders."
Can language models handle recursively nested grammatical structures? A case study on comparing models and humans,0.258604,"How should we compare the capabilities of language models (LMs) and humans? I
draw inspiration from comparative psychology to highlight some challenges. In
particular, I consider a case study: processing of recursively nested
grammatical structures. Prior work suggests that LMs cannot handle these
structures as reliably as humans can. However, the humans were provided with
instructions and training, while the LMs were evaluated zero-shot. I therefore
match the evaluation more closely. Providing large LMs with a simple prompt --
substantially less content than the human training -- allows the LMs to
consistently outperform the human results, and even to extrapolate to more
deeply nested conditions than were tested with humans. Further, reanalyzing the
prior human data suggests that the humans may not perform above chance at the
difficult structures initially. Thus, large LMs may indeed process recursively
nested grammatical structures as reliably as humans. This case study highlights
how discrepancies in the evaluation can confound comparisons of language models
and humans. I therefore reflect on the broader challenge of comparing human and
model capabilities, and highlight an important difference between evaluating
cognitive models and foundation models."
Geographic Adaptation of Pretrained Language Models,0.29902,"While pretrained language models (PLMs) have been shown to possess a plethora
of linguistic knowledge, the existing body of research has largely neglected
extralinguistic knowledge, which is generally difficult to obtain by
pretraining on text alone. Here, we contribute to closing this gap by examining
geolinguistic knowledge, i.e., knowledge about geographic variation in
language. We introduce geoadaptation, an intermediate training step that
couples language modeling with geolocation prediction in a multi-task learning
setup. We geoadapt four PLMs, covering language groups from three geographic
areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised)
geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction,
fine-tuned language identification, zero-shot language identification, and
zero-shot prediction of dialect features. Geoadaptation is very successful at
injecting geolinguistic knowledge into the PLMs: the geoadapted PLMs
consistently outperform PLMs adapted using only language modeling (by
especially wide margins on zero-shot prediction tasks), and we obtain new
state-of-the-art results on two benchmarks for geolocation prediction and
language identification. Furthermore, we show that the effectiveness of
geoadaptation stems from its ability to geographically retrofit the
representation space of the PLMs."
A Policy Driven AI-Assisted PoW Framework,0.240664,"Proof of Work (PoW) based cyberdefense systems require incoming network
requests to expend effort solving an arbitrary mathematical puzzle. Current
state of the art is unable to differentiate between trustworthy and
untrustworthy connections, requiring all to solve complex puzzles. In this
paper, we introduce an Artificial Intelligence (AI)-assisted PoW framework that
utilizes IP traffic based features to inform an adaptive issuer which can then
generate puzzles with varying hardness. The modular framework uses these
capabilities to ensure that untrustworthy clients solve harder puzzles thereby
incurring longer latency than authentic requests to receive a response from the
server. Our preliminary findings reveal our approach effectively throttles
untrustworthy traffic."
The Gender Gap in Face Recognition Accuracy Is a Hairy Problem,0.239406,"It is broadly accepted that there is a ""gender gap"" in face recognition
accuracy, with females having higher false match and false non-match rates.
However, relatively little is known about the cause(s) of this gender gap. Even
the recent NIST report on demographic effects lists ""analyze cause and effect""
under ""what we did not do"". We first demonstrate that female and male
hairstyles have important differences that impact face recognition accuracy. In
particular, compared to females, male facial hair contributes to creating a
greater average difference in appearance between different male faces. We then
demonstrate that when the data used to estimate recognition accuracy is
balanced across gender for how hairstyles occlude the face, the initially
observed gender gap in accuracy largely disappears. We show this result for two
different matchers, and analyzing images of Caucasians and of
African-Americans. These results suggest that future research on demographic
variation in accuracy should include a check for balanced quality of the test
data as part of the problem formulation. To promote reproducible research,
matchers, attribute classifiers, and datasets used in this research are/will be
publicly available."
Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems,0.226315,"Users interacting with voice assistants today need to phrase their requests
in a very specific manner to elicit an appropriate response. This limits the
user experience, and is partly due to the lack of reasoning capabilities of
dialogue platforms and the hand-crafted rules that require extensive labor. One
possible way to improve user experience and relieve the manual efforts of
designers is to build an end-to-end dialogue system that can do reasoning
itself while perceiving user's utterances. In this work, we propose a novel
method to incorporate the knowledge reasoning capability into dialogue systems
in a more scalable and generalizable manner. Our proposed method allows a
single transformer model to directly walk on a large-scale knowledge graph to
generate responses. To the best of our knowledge, this is the first work to
have transformer models generate responses by reasoning over differentiable
knowledge graphs. We investigate the reasoning abilities of the proposed method
on both task-oriented and domain-specific chit-chat dialogues. Empirical
results show that this method can effectively and efficiently incorporate a
knowledge graph into a dialogue system with fully-interpretable reasoning
paths."
Readability Controllable Biomedical Document Summarization,0.2999,"Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task."
Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges,0.260691,"This is a report on the NSF Future Directions Workshop on Automatic
Evaluation of Dialog. The workshop explored the current state of the art along
with its limitations and suggested promising directions for future work in this
important and very rapidly changing area of research."
HieNet: Bidirectional Hierarchy Framework for Automated ICD Coding,0.295195,"International Classification of Diseases (ICD) is a set of classification
codes for medical records. Automated ICD coding, which assigns unique
International Classification of Diseases codes with each medical record, is
widely used recently for its efficiency and error-prone avoidance. However,
there are challenges that remain such as heterogeneity, label unbalance, and
complex relationships between ICD codes. In this work, we proposed a novel
Bidirectional Hierarchy Framework(HieNet) to address the challenges.
Specifically, a personalized PageRank routine is developed to capture the
co-relation of codes, a bidirectional hierarchy passage encoder to capture the
codes' hierarchical representations, and a progressive predicting method is
then proposed to narrow down the semantic searching space of prediction. We
validate our method on two widely used datasets. Experimental results on two
authoritative public datasets demonstrate that our proposed method boosts
state-of-the-art performance by a large margin."
End-to-End Image-Based Fashion Recommendation,0.259308,"In fashion-based recommendation settings, incorporating the item image
features is considered a crucial factor, and it has shown significant
improvements to many traditional models, including but not limited to matrix
factorization, auto-encoders, and nearest neighbor models. While there are
numerous image-based recommender approaches that utilize dedicated deep neural
networks, comparisons to attribute-aware models are often disregarded despite
their ability to be easily extended to leverage items' image features. In this
paper, we propose a simple yet effective attribute-aware model that
incorporates image features for better item representation learning in item
recommendation tasks. The proposed model utilizes items' image features
extracted by a calibrated ResNet50 component. We present an ablation study to
compare incorporating the image features using three different techniques into
the recommender system component that can seamlessly leverage any available
items' attributes. Experiments on two image-based real-world recommender
systems datasets show that the proposed model significantly outperforms all
state-of-the-art image-based models."
NAFSSR: Stereo Image Super-Resolution Using NAFNet,0.242941,"Stereo image super-resolution aims at enhancing the quality of
super-resolution results by utilizing the complementary information provided by
binocular systems. To obtain reasonable performance, most methods focus on
finely designing modules, loss functions, and etc. to exploit information from
another viewpoint. This has the side effect of increasing system complexity,
making it difficult for researchers to evaluate new ideas and compare methods.
This paper inherits a strong and simple image restoration model, NAFNet, for
single-view feature extraction and extends it by adding cross attention modules
to fuse features between views to adapt to binocular scenarios. The proposed
baseline for stereo image super-resolution is noted as NAFSSR. Furthermore,
training/testing strategies are proposed to fully exploit the performance of
NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In
particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012,
KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place
in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models
will be released at https://github.com/megvii-research/NAFNet."
Attentive Dual Stream Siamese U-net for Flood Detection on Multi-temporal Sentinel-1 Data,0.331598,"Due to climate and land-use change, natural disasters such as flooding have
been increasing in recent years. Timely and reliable flood detection and
mapping can help emergency response and disaster management. In this work, we
propose a flood detection network using bi-temporal SAR acquisitions. The
proposed segmentation network has an encoder-decoder architecture with two
Siamese encoders for pre and post-flood images. The network's feature maps are
fused and enhanced using attention blocks to achieve more accurate detection of
the flooded areas. Our proposed network is evaluated on publicly available
Sen1Flood11 benchmark dataset. The network outperformed the existing
state-of-the-art (uni-temporal) flood detection method by 6\% IOU. The
experiments highlight that the combination of bi-temporal SAR data with an
effective network architecture achieves more accurate flood detection than
uni-temporal methods."
Towards Summary Candidates Fusion,0.230277,"Sequence-to-sequence deep neural models fine-tuned for abstractive
summarization can achieve great performance on datasets with enough human
annotations. Yet, it has been shown that they have not reached their full
potential, with a wide gap between the top beam search output and the oracle
beam. Recently, re-ranking methods have been proposed, to learn to select a
better summary candidate. However, such methods are limited by the summary
quality aspects captured by the first-stage candidates. To bypass this
limitation, we propose a new paradigm in second-stage abstractive summarization
called SummaFusion that fuses several summary candidates to produce a novel
abstractive second-stage summary. Our method works well on several
summarization datasets, improving both the ROUGE scores and qualitative
properties of fused summaries. It is especially good when the candidates to
fuse are worse, such as in the few-shot setup where we set a new
state-of-the-art. We will make our code and checkpoints available at
https://github.com/ntunlp/SummaFusion/."
JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,0.222885,"Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes."
EventGraph: Event Extraction as Semantic Graph Parsing,0.313955,"Event extraction involves the detection and extraction of both the event
triggers and corresponding event arguments. Existing systems often decompose
event extraction into multiple subtasks, without considering their possible
interactions. In this paper, we propose EventGraph, a joint framework for event
extraction, which encodes events as graphs. We represent event triggers and
arguments as nodes in a semantic graph. Event extraction therefore becomes a
graph parsing problem, which provides the following advantages: 1) performing
event detection and argument extraction jointly; 2) detecting and extracting
multiple events from a piece of text; and 3) capturing the complicated
interaction between event arguments and triggers. Experimental results on
ACE2005 show that our model is competitive to state-of-the-art systems and has
substantially improved the results on argument extraction. Additionally, we
create two new datasets from ACE2005 where we keep the entire text spans for
event arguments, instead of just the head word(s). Our code and models are
released as open-source."
Identifying Moments of Change from Longitudinal User Text,0.295369,"Identifying changes in individuals' behaviour and mood, as observed via
content shared on online platforms, is increasingly gaining importance. Most
research to-date on this topic focuses on either: (a) identifying individuals
at risk or with a certain mental health condition given a batch of posts or (b)
providing equivalent labels at the post level. A disadvantage of such work is
the lack of a strong temporal component and the inability to make longitudinal
assessments following an individual's trajectory and allowing timely
interventions. Here we define a new task, that of identifying moments of change
in individuals on the basis of their shared content online. The changes we
consider are sudden shifts in mood (switches) or gradual mood progression
(escalations). We have created detailed guidelines for capturing moments of
change and a corpus of 500 manually annotated user timelines (18.7K posts). We
have developed a variety of baseline models drawing inspiration from related
tasks and show that the best performance is obtained through context aware
sequential modelling. We also introduce new metrics for capturing rare events
in temporal windows."
Submodularity In Machine Learning and Artificial Intelligence,0.239109,"In this manuscript, we offer a gentle review of submodularity and
supermodularity and their properties. We offer a plethora of submodular
definitions; a full description of a number of example submodular functions and
their generalizations; example discrete constraints; a discussion of basic
algorithms for maximization, minimization, and other operations; a brief
overview of continuous submodular extensions; and some historical applications.
We then turn to how submodularity is useful in machine learning and artificial
intelligence. This includes summarization, and we offer a complete account of
the differences between and commonalities amongst sketching, coresets,
extractive and abstractive summarization in NLP, data distillation and
condensation, and data subset selection and feature selection. We discuss a
variety of ways to produce a submodular function useful for machine learning,
including heuristic hand-crafting, learning or approximately learning a
submodular function or aspects thereof, and some advantages of the use of a
submodular function as a coreset producer. We discuss submodular combinatorial
information functions, and how submodularity is useful for clustering, data
partitioning, parallel machine learning, active and semi-supervised learning,
probabilistic modeling, and structured norms and loss functions."
Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing,0.228815,"We introduce two techniques, length perturbation and n-best based label
smoothing, to improve generalization of deep neural network (DNN) acoustic
models for automatic speech recognition (ASR). Length perturbation is a data
augmentation algorithm that randomly drops and inserts frames of an utterance
to alter the length of the speech feature sequence. N-best based label
smoothing randomly injects noise to ground truth labels during training in
order to avoid overfitting, where the noisy labels are generated from n-best
hypotheses. We evaluate these two techniques extensively on the 300-hour
Switchboard (SWB300) dataset and an in-house 500-hour Japanese (JPN500) dataset
using recurrent neural network transducer (RNNT) acoustic models for ASR. We
show that both techniques improve the generalization of RNNT models
individually and they can also be complementary. In particular, they yield good
improvements over a strong SWB300 baseline and give state-of-art performance on
SWB300 using RNNT models."
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts,0.243894,"A common approach to transfer learning under distribution shift is to
fine-tune the last few layers of a pre-trained model, preserving learned
features while also adapting to the new task. This paper shows that in such
settings, selectively fine-tuning a subset of layers (which we term surgical
fine-tuning) matches or outperforms commonly used fine-tuning approaches.
Moreover, the type of distribution shift influences which subset is more
effective to tune: for example, for image corruptions, fine-tuning only the
first few layers works best. We validate our findings systematically across
seven real-world data tasks spanning three types of distribution shifts.
Theoretically, we prove that for two-layer neural networks in an idealized
setting, first-layer tuning can outperform fine-tuning all layers. Intuitively,
fine-tuning more parameters on a small target dataset can cause information
learned during pre-training to be forgotten, and the relevant information
depends on the type of shift."
Towards Climate Awareness in NLP Research,0.270496,"The climate impact of AI, and NLP research in particular, has become a
serious issue given the enormous amount of energy that is increasingly being
used for training and running computational models. Consequently, increasing
focus is placed on efficient NLP. However, this important initiative lacks
simple guidelines that would allow for systematic climate reporting of NLP
research. We argue that this deficiency is one of the reasons why very few
publications in NLP report key figures that would allow a more thorough
examination of environmental impact. As a remedy, we propose a climate
performance model card with the primary purpose of being practically usable
with only limited information about experiments and the underlying computer
hardware. We describe why this step is essential to increase awareness about
the environmental impact of NLP research and, thereby, paving the way for more
thorough discussions."
Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models,0.317428,"We present a novel way of conditioning a pretrained denoising diffusion
speech model to produce speech in the voice of a novel person unseen during
training. The method requires a short (~3 seconds) sample from the target
person, and generation is steered at inference time, without any training
steps. At the heart of the method lies a sampling process that combines the
estimation of the denoising model with a low-pass version of the new speaker's
sample. The objective and subjective evaluations show that our sampling method
can generate a voice similar to that of the target speaker in terms of
frequency, with an accuracy comparable to state-of-the-art methods, and without
training."
A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning,0.235829,"Deep reinforcement learning (DRL) has made significant achievements in many
real-world applications. But these real-world applications typically can only
provide partial observations for making decisions due to occlusions and noisy
sensors. However, partial state observability can be used to hide malicious
behaviors for backdoors. In this paper, we explore the sequential nature of DRL
and propose a novel temporal-pattern backdoor attack to DRL, whose trigger is a
set of temporal constraints on a sequence of observations rather than a single
observation, and effect can be kept in a controllable duration rather than in
the instant. We validate our proposed backdoor attack to a typical job
scheduling task in cloud computing. Numerous experimental results show that our
backdoor can achieve excellent effectiveness, stealthiness, and sustainability.
Our backdoor's average clean data accuracy and attack success rate can reach
97.8% and 97.5%, respectively."
AbductionRules: Training Transformers to Explain Unexpected Inputs,0.281833,"Transformers have recently been shown to be capable of reliably performing
logical reasoning over facts and rules expressed in natural language, but
abductive reasoning - inference to the best explanation of an unexpected
observation - has been underexplored despite significant applications to
scientific discovery, common-sense reasoning, and model interpretability.
  We present AbductionRules, a group of natural language datasets designed to
train and test generalisable abduction over natural-language knowledge bases.
We use these datasets to finetune pretrained Transformers and discuss their
performance, finding that our models learned generalisable abductive techniques
but also learned to exploit the structure of our data. Finally, we discuss the
viability of this approach to abductive reasoning and ways in which it may be
improved in future work."
VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification,0.243268,"Multimodal learning from document data has achieved great success lately as
it allows to pre-train semantically meaningful features as a prior into a
learnable downstream task. In this paper, we approach the document
classification problem by learning cross-modal representations through language
and vision cues, considering intra- and inter-modality relationships. Instead
of merging features from different modalities into a joint representation
space, the proposed method exploits high-level interactions and learns relevant
semantic information from effective attention flows within and across
modalities. The proposed learning objective is devised between intra- and
inter-modality alignment tasks, where the similarity distribution per task is
computed by contracting positive sample pairs while simultaneously contrasting
negative ones in the joint representation space}. Extensive experiments on
public document classification datasets demonstrate the effectiveness and the
generality of our model on low-scale and large-scale datasets."
Unobserved Local Structures Make Compositional Generalization Hard,0.304974,"While recent work has convincingly showed that sequence-to-sequence models
struggle to generalize to new compositions (termed compositional
generalization), little is known on what makes compositional generalization
hard on a particular test instance. In this work, we investigate what are the
factors that make generalization to certain test instances challenging. We
first substantiate that indeed some examples are more difficult than others by
showing that different models consistently fail or succeed on the same test
instances. Then, we propose a criterion for the difficulty of an example: a
test instance is hard if it contains a local structure that was not observed at
training time. We formulate a simple decision rule based on this criterion and
empirically show it predicts instance-level generalization well across 5
different semantic parsing datasets, substantially better than alternative
decision rules. Last, we show local structures can be leveraged for creating
difficult adversarial compositional splits and also to improve compositional
generalization under limited training budgets by strategically selecting
examples for the training set."
Gaussian Multi-head Attention for Simultaneous Machine Translation,0.299599,"Simultaneous machine translation (SiMT) outputs translation while receiving
the streaming source inputs, and hence needs a policy to determine where to
start translating. The alignment between target and source words often implies
the most informative source word for each target word, and hence provides the
unified control over translation quality and latency, but unfortunately the
existing SiMT methods do not explicitly model the alignment to perform the
control. In this paper, we propose Gaussian Multi-head Attention (GMA) to
develop a new SiMT policy by modeling alignment and translation in a unified
manner. For SiMT policy, GMA models the aligned source position of each target
word, and accordingly waits until its aligned position to start translating. To
integrate the learning of alignment into the translation model, a Gaussian
distribution centered on predicted aligned position is introduced as an
alignment-related prior, which cooperates with translation-related soft
attention to determine the final attention. Experiments on En-Vi and De-En
tasks show that our method outperforms strong baselines on the trade-off
between translation and latency."
From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,0.224685,"Multi-hop question answering (QA) is a challenging task requiring QA systems
to perform complex reasoning over multiple documents and provide supporting
facts together with the exact answer. Existing works tend to utilize
graph-based reasoning and question decomposition to obtain the reasoning chain,
which inevitably introduces additional complexity and cumulative error to the
system. To address the above issue, we propose a simple yet effective novel
framework, From Easy to Hard (FE2H), to remove distracting information and
obtain better contextual representations for the multi-hop QA task. Inspired by
the iterative document selection process and the progressive learning custom of
humans, FE2H divides both the document selector and reader into two stages
following an easy-to-hard manner. Specifically, we first select the document
most relevant to the question and then utilize the question together with this
document to select other pertinent documents. As for the QA phase, our reader
is first trained on a single-hop QA dataset and then transferred into the
multi-hop QA task. We comprehensively evaluate our model on the popular
multi-hop QA benchmark HotpotQA. Experimental results demonstrate that our
method ourperforms all other methods in the leaderboard of HotpotQA (distractor
setting)."
HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance,0.240826,"Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/."
"Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices",0.325607,"In this paper, we consider the problem of real-time video-based facial
emotion analytics, namely, facial expression recognition, prediction of valence
and arousal and detection of action unit points. We propose the novel
frame-level emotion recognition algorithm by extracting facial features with
the single EfficientNet model pre-trained on AffectNet. As a result, our
approach may be implemented even for video analytics on mobile devices.
Experimental results for the large scale Aff-Wild2 database from the third
Affective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our
simple model is significantly better when compared to the VggFace baseline. In
particular, our method is characterized by 0.15-0.2 higher performance measures
for validation sets in uni-task Expression Classification, Valence-Arousal
Estimation and Expression Classification. Due to simplicity, our approach may
be considered as a new baseline for all four sub-challenges."
SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding,0.256158,"In this paper, we investigate how to achieve better visual grounding with
modern vision-language transformers, and propose a simple yet powerful
Selective Retraining (SiRi) mechanism for this challenging task. Particularly,
SiRi conveys a significant principle to the research of visual grounding, i.e.,
a better initialized vision-language encoder would help the model converge to a
better local minimum, advancing the performance accordingly. In specific, we
continually update the parameters of the encoder as the training goes on, while
periodically re-initialize rest of the parameters to compel the model to be
better optimized based on an enhanced encoder. SiRi can significantly
outperform previous approaches on three popular benchmarks. Specifically, our
method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the
state-of-the-art approaches (training from scratch) by more than 10.21%.
Additionally, we reveal that SiRi performs surprisingly superior even with
limited training data. We also extend it to transformer-based visual grounding
models and other vision-language tasks to verify the validity."
Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation,0.320553,"Attribute-based Controlled Text Generation (CTG) refers to generating
sentences that satisfy desirable attributes (e.g., emotions and topics).
Existing works often utilize fine-tuning or resort to extra attribute
classifiers, yet suffer from storage and inference time increases. To address
these concerns, we explore attribute-based CTG in a prompt-based manner. In
short, the proposed Tailor represents each attribute as a pre-trained
continuous vector (i.e., single-attribute prompt) and guides the generation of
a fixed PLM switch to a pre-specified attribute. We experimentally find that
these prompts can be simply concatenated as a whole to multi-attribute CTG
without any re-training, yet raises problems of fluency decrease and position
sensitivity. To this end, Tailor provides a multi-attribute prompt mask and a
re-indexing position-ids sequence to bridge the gap between the training (one
prompt for each task) and testing stage (concatenating more than one prompt).
To further enhance such single-attribute prompt combinations, Tailor also
introduces a trainable prompt connector, which can be concatenated with any two
single-attribute prompts to multi-attribute text generation. Experiments on 11
attribute-specific generation tasks demonstrate strong performances of Tailor
on both single-attribute and multi-attribute CTG, with 0.08\% training
parameters of a GPT-2."
Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection,0.289019,"Over the past few years there has been major progress in the field of
synthetic data generation using simulation based techniques. These methods use
high-end graphics engines and physics-based ray-tracing rendering in order to
represent the world in 3D and create highly realistic images. Datagen has
specialized in the generation of high-quality 3D humans, realistic 3D
environments and generation of realistic human motion. This technology has been
developed into a data generation platform which we used for these experiments.
This work demonstrates the use of synthetic photo-realistic in-cabin data to
train a Driver Monitoring System that uses a lightweight neural network to
detect whether the driver's hands are on the wheel. We demonstrate that when
only a small amount of real data is available, synthetic data can be a simple
way to boost performance. Moreover, we adopt the data-centric approach and show
how performing error analysis and generating the missing edge-cases in our
platform boosts performance. This showcases the ability of human-centric
synthetic data to generalize well to the real world, and help train algorithms
in computer vision settings where data from the target domain is scarce or hard
to collect."
SVG Vector Font Generation for Chinese Characters with Transformer,0.269779,"Designing fonts for Chinese characters is highly labor-intensive and
time-consuming. While the latest methods successfully generate the English
alphabet vector font, despite the high demand for automatic font generation,
Chinese vector font generation has been an unsolved problem owing to its
complex shape and numerous characters. This study addressed the problem of
automatically generating Chinese vector fonts from only a single style and
content reference. We proposed a novel network architecture with Transformer
and loss functions to capture structural features without differentiable
rendering. Although the dataset range was still limited to the sans-serif
family, we successfully generated the Chinese vector font for the first time
using the proposed method."
PolyHope: Two-Level Hope Speech Detection from Tweets,0.243655,"Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into ""Hope"" and ""Not Hope"",
then into three fine-grained hope categories: ""Generalized Hope"", ""Realistic
Hope"", and ""Unrealistic Hope"" (along with ""Not Hope""). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset."
Towards Better Chinese-centric Neural Machine Translation for Low-resource Languages,0.267489,"The last decade has witnessed enormous improvements in science and
technology, stimulating the growing demand for economic and cultural exchanges
in various countries. Building a neural machine translation (NMT) system has
become an urgent trend, especially in the low-resource setting. However, recent
work tends to study NMT systems for low-resource languages centered on English,
while few works focus on low-resource NMT systems centered on other languages
such as Chinese. To achieve this, the low-resource multilingual translation
challenge of the 2021 iFLYTEK AI Developer Competition provides the
Chinese-centric multilingual low-resource NMT tasks, where participants are
required to build NMT systems based on the provided low-resource samples. In
this paper, we present the winner competition system that leverages monolingual
word embeddings data enhancement, bilingual curriculum learning, and
contrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss
function is proposed to replace the traditional cross-entropy loss when
training. The experimental results demonstrate that the implementation of these
ideas leads better performance than other state-of-the-art methods. All the
experimental codes are released at:
https://github.com/WENGSYX/Low-resource-text-translation."
MMNet: Muscle motion-guided network for micro-expression recognition,0.232883,"Facial micro-expressions (MEs) are involuntary facial motions revealing
peoples real feelings and play an important role in the early intervention of
mental illness, the national security, and many human-computer interaction
systems. However, existing micro-expression datasets are limited and usually
pose some challenges for training good classifiers. To model the subtle facial
muscle motions, we propose a robust micro-expression recognition (MER)
framework, namely muscle motion-guided network (MMNet). Specifically, a
continuous attention (CA) block is introduced to focus on modeling local subtle
muscle motion patterns with little identity information, which is different
from most previous methods that directly extract features from complete video
frames with much identity information. Besides, we design a position
calibration (PC) module based on the vision transformer. By adding the position
embeddings of the face generated by PC module at the end of the two branches,
the PC module can help to add position information to facial muscle motion
pattern features for the MER. Extensive experiments on three public
micro-expression datasets demonstrate that our approach outperforms
state-of-the-art methods by a large margin."
Generate rather than Retrieve: Large Language Models are Strong Context Generators,0.282701,"Knowledge-intensive tasks, such as open-domain question answering (QA),
require access to a large amount of world or domain knowledge. A common
approach for knowledge-intensive tasks is to employ a retrieve-then-read
pipeline that first retrieves a handful of relevant contextual documents from
an external corpus such as Wikipedia and then predicts an answer conditioned on
the retrieved documents. In this paper, we present a novel perspective for
solving knowledge-intensive tasks by replacing document retrievers with large
language model generators. We call our method generate-then-read (GenRead),
which first prompts a large language model to generate contextutal documents
based on a given question, and then reads the generated documents to produce
the final answer. Furthermore, we propose a novel clustering-based prompting
method that selects distinct prompts, resulting in the generated documents that
cover different perspectives, leading to better recall over acceptable answers.
We conduct extensive experiments on three different knowledge-intensive tasks,
including open-domain QA, fact checking, and dialogue system. Notably, GenRead
achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly
outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0
and +3.9, without retrieving any documents from any external knowledge source.
Lastly, we demonstrate the model performance can be further improved by
combining retrieval and generation. Our code and generated documents can be
found at https://github.com/wyu97/GenRead."
MIRROR: Differentiable Deep Social Projection for Assistive Human-Robot Communication,0.231766,"Communication is a hallmark of intelligence. In this work, we present MIRROR,
an approach to (i) quickly learn human models from human demonstrations, and
(ii) use the models for subsequent communication planning in assistive
shared-control settings. MIRROR is inspired by social projection theory, which
hypothesizes that humans use self-models to understand others. Likewise, MIRROR
leverages self-models learned using reinforcement learning to bootstrap human
modeling. Experiments with simulated humans show that this approach leads to
rapid learning and more robust models compared to existing behavioral cloning
and state-of-the-art imitation learning methods. We also present a
human-subject study using the CARLA simulator which shows that (i) MIRROR is
able to scale to complex domains with high-dimensional observations and
complicated world physics and (ii) provides effective assistive communication
that enabled participants to drive more safely in adverse weather conditions."
A Variational Hierarchical Model for Neural Cross-Lingual Summarization,0.29947,"The goal of the cross-lingual summarization (CLS) is to convert a document in
one language (e.g., English) to a summary in another one (e.g., Chinese).
Essentially, the CLS task is the combination of machine translation (MT) and
monolingual summarization (MS), and thus there exists the hierarchical
relationship between MT\&MS and CLS. Existing studies on CLS mainly focus on
utilizing pipeline methods or jointly training an end-to-end model through an
auxiliary MT or MS objective. However, it is very challenging for the model to
directly conduct CLS as it requires both the abilities to translate and
summarize. To address this issue, we propose a hierarchical model for the CLS
task, based on the conditional variational auto-encoder. The hierarchical model
contains two kinds of latent variables at the local and global levels,
respectively. At the local level, there are two latent variables, one for
translation and the other for summarization. As for the global level, there is
another latent variable for cross-lingual summarization conditioned on the two
local-level variables. Experiments on two language directions (English-Chinese)
verify the effectiveness and superiority of the proposed approach. In addition,
we show that our model is able to generate better cross-lingual summaries than
comparison models in the few-shot setting."
Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models,0.326028,"Constructive studies on symbol emergence systems seek to investigate
computational models that can better explain human language evolution, the
creation of symbol systems, and the construction of internal representations.
This study provides a new model for emergent communication, which is based on a
probabilistic generative model (PGM) instead of a discriminative model based on
deep reinforcement learning. We define the Metropolis-Hastings (MH) naming game
by generalizing previously proposed models. It is not a referential game with
explicit feedback, as assumed by many emergent communication studies. Instead,
it is a game based on joint attention without explicit feedback.
Mathematically, the MH naming game is proved to be a type of MH algorithm for
an integrative PGM that combines two agents that play the naming game. From
this viewpoint, symbol emergence is regarded as decentralized Bayesian
inference, and semiotic communication is regarded as inter-personal cross-modal
inference. This notion leads to the collective predictive coding hypothesis}
regarding language evolution and, in general, the emergence of symbols. We also
propose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE),
a deep generative model for emergent communication based on the MH naming game.
The model has been validated on MNIST and Fruits 360 datasets. Experimental
findings demonstrate that categories are formed from real images observed by
agents, and signs are correctly shared across agents by successfully utilizing
both of the observations of agents via the MH naming game. Furthermore,
scholars verified that visual images were recalled from signs uttered by
agents. Notably, emergent communication without supervision and reward feedback
improved the performance of the unsupervised representation learning of agents."
Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors,0.230276,"Robustness of machine learning models on ever-changing real-world data is
critical, especially for applications affecting human well-being such as
content moderation. New kinds of abusive language continually emerge in online
discussions in response to current events (e.g., COVID-19), and the deployed
abuse detection systems should be updated regularly to remain accurate. In this
paper, we show that general abusive language classifiers tend to be fairly
reliable in detecting out-of-domain explicitly abusive utterances but fail to
detect new types of more subtle, implicit abuse. Next, we propose an
interpretability technique, based on the Testing Concept Activation Vector
(TCAV) method from computer vision, to quantify the sensitivity of a trained
model to the human-defined concepts of explicit and implicit abusive language,
and use that to explain the generalizability of the model on new data, in this
case, COVID-related anti-Asian hate speech. Extending this technique, we
introduce a novel metric, Degree of Explicitness, for a single instance and
show that the new metric is beneficial in suggesting out-of-domain unlabeled
examples to effectively enrich the training data with informative, implicitly
abusive texts."
Unified Pretraining Framework for Document Understanding,0.299123,"Document intelligence automates the extraction of information from documents
and supports many business applications. Recent self-supervised learning
methods on large-scale unlabeled document datasets have opened up promising
directions towards reducing annotation efforts by training models with
self-supervised objectives. However, most of the existing document pretraining
methods are still language-dominated. We present UDoc, a new unified
pretraining framework for document understanding. UDoc is designed to support
most document understanding tasks, extending the Transformer to take multimodal
embeddings as input. Each input element is composed of words and visual
features from a semantic region of the input document image. An important
feature of UDoc is that it learns a generic representation by making use of
three self-supervised losses, encouraging the representation to model
sentences, learn similarities, and align modalities. Extensive empirical
analysis demonstrates that the pretraining procedure learns better joint
representations and leads to improvements in downstream tasks."
Delving Deeper into Cross-lingual Visual Question Answering,0.265901,"Visual question answering (VQA) is one of the crucial vision-and-language
tasks. Yet, existing VQA research has mostly focused on the English language,
due to a lack of suitable evaluation resources. Previous work on cross-lingual
VQA has reported poor zero-shot transfer performance of current multilingual
multimodal Transformers with large gaps to monolingual performance, without any
deeper analysis. In this work, we delve deeper into the different aspects of
cross-lingual VQA, aiming to understand the impact of 1) modeling methods and
choices, including architecture, inductive bias, fine-tuning; 2) learning
biases: including question types and modality biases in cross-lingual setups.
The key results of our analysis are: 1) We show that simple modifications to
the standard training setup can substantially reduce the transfer gap to
monolingual English performance, yielding +10 accuracy points over existing
methods. 2) We analyze cross-lingual VQA across different question types of
varying complexity for different multilingual multimodal Transformers, and
identify question types that are the most difficult to improve on. 3) We
provide an analysis of modality biases present in training data and models,
revealing why zero-shot performance gaps remain for certain question types and
languages."
Face Relighting with Geometrically Consistent Shadows,0.260233,"Most face relighting methods are able to handle diffuse shadows, but struggle
to handle hard shadows, such as those cast by the nose. Methods that propose
techniques for handling hard shadows often do not produce geometrically
consistent shadows since they do not directly leverage the estimated face
geometry while synthesizing them. We propose a novel differentiable algorithm
for synthesizing hard shadows based on ray tracing, which we incorporate into
training our face relighting model. Our proposed algorithm directly utilizes
the estimated face geometry to synthesize geometrically consistent hard
shadows. We demonstrate through quantitative and qualitative experiments on
Multi-PIE and FFHQ that our method produces more geometrically consistent
shadows than previous face relighting methods while also achieving
state-of-the-art face relighting performance under directional lighting. In
addition, we demonstrate that our differentiable hard shadow modeling improves
the quality of the estimated face geometry over diffuse shading models."
Empowering Graph Representation Learning with Test-Time Graph Transformation,0.283025,"As powerful tools for representation learning on graphs, graph neural
networks (GNNs) have facilitated various applications from drug discovery to
recommender systems. Nevertheless, the effectiveness of GNNs is immensely
challenged by issues related to data quality, such as distribution shift,
abnormal features and adversarial attacks. Recent efforts have been made on
tackling these issues from a modeling perspective which requires additional
cost of changing model architectures or re-training model parameters. In this
work, we provide a data-centric view to tackle these issues and propose a graph
transformation framework named GTrans which adapts and refines graph data at
test time to achieve better performance. We provide theoretical analysis on the
design of the framework and discuss why adapting graph data works better than
adapting the model. Extensive experiments have demonstrated the effectiveness
of GTrans on three distinct scenarios for eight benchmark datasets where
suboptimal data is presented. Remarkably, GTrans performs the best in most
cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on
three experimental settings. Code is released at
https://github.com/ChandlerBang/GTrans."
Adversarial Examples for Good: Adversarial Examples Guided Imbalanced Learning,0.277374,"Adversarial examples are inputs for machine learning models that have been
designed by attackers to cause the model to make mistakes. In this paper, we
demonstrate that adversarial examples can also be utilized for good to improve
the performance of imbalanced learning. We provide a new perspective on how to
deal with imbalanced data: adjust the biased decision boundary by training with
Guiding Adversarial Examples (GAEs). Our method can effectively increase the
accuracy of minority classes while sacrificing little accuracy on majority
classes. We empirically show, on several benchmark datasets, our proposed
method is comparable to the state-of-the-art method. To our best knowledge, we
are the first to deal with imbalanced learning with adversarial examples."
Analogical Math Word Problems Solving with Enhanced Problem-Solution Association,0.270317,"Math word problem (MWP) solving is an important task in question answering
which requires human-like reasoning ability. Analogical reasoning has long been
used in mathematical education, as it enables students to apply common
relational structures of mathematical situations to solve new problems. In this
paper, we propose to build a novel MWP solver by leveraging analogical MWPs,
which advance the solver's generalization ability across different kinds of
MWPs. The key idea, named analogy identification, is to associate the
analogical MWP pairs in a latent space, i.e., encoding an MWP close to another
analogical MWP, while moving away from the non-analogical ones. Moreover, a
solution discriminator is integrated into the MWP solver to enhance the
association between the representations of MWPs and their true solutions. The
evaluation results verify that our proposed analogical learning strategy
promotes the performance of MWP-BERT on Math23k over the state-of-the-art model
Generate2Rank, with 5 times fewer parameters in the encoder. We also find that
our model has a stronger generalization ability in solving difficult MWPs due
to the analogical learning from easy MWPs."
On Linking Level Segments,0.264245,"An increasingly common area of study in procedural content generation is the
creation of level segments: short pieces that can be used to form larger
levels. Previous work has used basic concatenation to form these larger levels.
However, even if the segments themselves are completable and well-formed,
concatenation can fail to produce levels that are completable and can cause
broken in-game structures (e.g. malformed pipes in Mario). We show this with
three tile-based games: a side-scrolling platformer, a vertical platformer, and
a top-down roguelike. Additionally, we present a Markov chain and a tree search
algorithm that finds a link between two level segments, which uses filters to
ensure completability and unbroken in-game structures in the linked segments.
We further show that these links work well for multi-segment levels. We find
that this method reliably finds links between segments and is customizable to
meet a designer's needs."
Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,0.282368,"Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,
QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in
which word similarities estimated by cosine over BERT embeddings are
understated and trace this effect to training data frequency. We find that
relative to human judgements, cosine similarity underestimates the similarity
of frequent words with other instances of the same word or other words across
contexts, even after controlling for polysemy and other factors. We conjecture
that this underestimation of similarity for high frequency words is due to
differences in the representational geometry of high and low frequency words
and provide a formal argument for the two-dimensional case."
On the focusing of thermal images,0.315678,"In this paper we present a new thermographic image database suitable for the
analysis of automatic focus measures. This database consists of 8 different
sets of scenes, where each scene contains one image for 96 different focus
positions. Using this database we evaluate the usefulness of six focus measures
with the goal to determine the optimal focus position. Experimental results
reveal that an accurate automatic detection of optimal focus position is
possible, even with a low computational burden. We also present an acquisition
tool able to help the acquisition of thermal images. To the best of our
knowledge, this is the first study about automatic focus of thermal images."
Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography,0.274827,"Although robust PCA has been increasingly adopted to extract vessels from
X-ray coronary angiography (XCA) images, challenging problems such as
inefficient vessel-sparsity modelling, noisy and dynamic background artefacts,
and high computational cost still remain unsolved. Therefore, we propose a
novel robust PCA unrolling network with sparse feature selection for
super-resolution XCA vessel imaging. Being embedded within a patch-wise
spatiotemporal super-resolution framework that is built upon a pooling layer
and a convolutional long short-term memory network, the proposed network can
not only gradually prune complex vessel-like artefacts and noisy backgrounds in
XCA during network training but also iteratively learn and select the
high-level spatiotemporal semantic information of moving contrast agents
flowing in the XCA-imaged vessels. The experimental results show that the
proposed method significantly outperforms state-of-the-art methods, especially
in the imaging of the vessel network and its distal vessels, by restoring the
intensity and geometry profiles of heterogeneous vessels against complex and
dynamic backgrounds."
FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation,0.292841,"Recent model-based reference-free metrics for open-domain dialogue evaluation
exhibit promising correlations with human judgment. However, they either
perform turn-level evaluation or look at a single dialogue quality dimension.
One would expect a good evaluation metric to assess multiple quality dimensions
at the dialogue level. To this end, we are motivated to propose a
multi-dimensional dialogue-level metric, which consists of three sub-metrics
with each targeting a specific dimension. The sub-metrics are trained with
novel self-supervised objectives and exhibit strong correlations with human
judgment for their respective dimensions. Moreover, we explore two approaches
to combine the sub-metrics: metric ensemble and multitask learning. Both
approaches yield a holistic metric that significantly outperforms individual
sub-metrics. Compared to the existing state-of-the-art metric, the combined
metrics achieve around 16% relative improvement on average across three
high-quality dialogue-level evaluation benchmarks."
Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives,0.320536,"This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a
new member in the family of InfoNCE losses that preserves a ranked ordering of
positive samples. In contrast to the standard InfoNCE loss, which requires a
strict binary separation of the training pairs into similar and dissimilar
samples, RINCE can exploit information about a similarity ranking for learning
a corresponding embedding space. We show that the proposed loss function learns
favorable embeddings compared to the standard InfoNCE whenever at least noisy
ranking information can be obtained or when the definition of positives and
negatives is blurry. We demonstrate this for a supervised classification task
with additional superclass labels and noisy similarity scores. Furthermore, we
show that RINCE can also be applied to unsupervised training with experiments
on unsupervised representation learning from videos. In particular, the
embedding yields higher classification accuracy, retrieval rates and performs
better in out-of-distribution detection than the standard InfoNCE loss."
MoDi: Unconditional Motion Synthesis from Diverse Data,0.246144,"The emergence of neural networks has revolutionized the field of motion
synthesis. Yet, learning to unconditionally synthesize motions from a given
distribution remains challenging, especially when the motions are highly
diverse. In this work, we present MoDi -- a generative model trained in an
unsupervised setting from an extremely diverse, unstructured and unlabeled
dataset. During inference, MoDi can synthesize high-quality, diverse motions.
Despite the lack of any structure in the dataset, our model yields a
well-behaved and highly structured latent space, which can be semantically
clustered, constituting a strong motion prior that facilitates various
applications including semantic editing and crowd simulation. In addition, we
present an encoder that inverts real motions into MoDi's natural motion
manifold, issuing solutions to various ill-posed challenges such as completion
from prefix and spatial editing. Our qualitative and quantitative experiments
achieve state-of-the-art results that outperform recent SOTA techniques. Code
and trained models are available at https://sigal-raab.github.io/MoDi."
Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks,0.232683,"In the pursuit of explaining implicit regularization in deep learning,
prominent focus was given to matrix and tensor factorizations, which correspond
to simplified neural networks. It was shown that these models exhibit an
implicit tendency towards low matrix and tensor ranks, respectively. Drawing
closer to practical deep learning, the current paper theoretically analyzes the
implicit regularization in hierarchical tensor factorization, a model
equivalent to certain deep convolutional neural networks. Through a dynamical
systems lens, we overcome challenges associated with hierarchy, and establish
implicit regularization towards low hierarchical tensor rank. This translates
to an implicit regularization towards locality for the associated convolutional
networks. Inspired by our theory, we design explicit regularization
discouraging locality, and demonstrate its ability to improve the performance
of modern convolutional networks on non-local tasks, in defiance of
conventional wisdom by which architectural changes are needed. Our work
highlights the potential of enhancing neural networks via theoretical analysis
of their implicit regularization."
SLiDE: Self-supervised LiDAR De-snowing through Reconstruction Difficulty,0.295948,"LiDAR is widely used to capture accurate 3D outdoor scene structures.
However, LiDAR produces many undesirable noise points in snowy weather, which
hamper analyzing meaningful 3D scene structures. Semantic segmentation with
snow labels would be a straightforward solution for removing them, but it
requires laborious point-wise annotation. To address this problem, we propose a
novel self-supervised learning framework for snow points removal in LiDAR point
clouds. Our method exploits the structural characteristic of the noise points:
low spatial correlation with their neighbors. Our method consists of two deep
neural networks: Point Reconstruction Network (PR-Net) reconstructs each point
from its neighbors; Reconstruction Difficulty Network (RD-Net) predicts
point-wise difficulty of the reconstruction by PR-Net, which we call
reconstruction difficulty. With simple post-processing, our method effectively
detects snow points without any label. Our method achieves the state-of-the-art
performance among label-free approaches and is comparable to the
fully-supervised method. Moreover, we demonstrate that our method can be
exploited as a pretext task to improve label-efficiency of supervised training
of de-snowing."
Segmentation Enhanced Lameness Detection in Dairy Cows from RGB and Depth Video,0.270865,"Cow lameness is a severe condition that affects the life cycle and life
quality of dairy cows and results in considerable economic losses. Early
lameness detection helps farmers address illnesses early and avoid negative
effects caused by the degeneration of cows' condition. We collected a dataset
of short clips of cows passing through a hallway exiting a milking station and
annotated the degree of lameness of the cows. This paper explores the resulting
dataset and provides a detailed description of the data collection process.
Additionally, we proposed a lameness detection method that leverages
pre-trained neural networks to extract discriminative features from videos and
assign a binary score to each cow indicating its condition: ""healthy"" or
""lame."" We improve this approach by forcing the model to focus on the structure
of the cow, which we achieve by substituting the RGB videos with binary
segmentation masks predicted with a trained segmentation model. This work aims
to encourage research and provide insights into the applicability of computer
vision models for cow lameness detection on farms."
Volume Rendering Digest (for NeRF),0.333182,"Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation."
Heterogeneous-Agent Mirror Learning: A Continuum of Solutions to Cooperative MARL,0.314076,"The necessity for cooperation among intelligent machines has popularised
cooperative multi-agent reinforcement learning (MARL) in the artificial
intelligence (AI) research community. However, many research endeavors have
been focused on developing practical MARL algorithms whose effectiveness has
been studied only empirically, thereby lacking theoretical guarantees. As
recent studies have revealed, MARL methods often achieve performance that is
unstable in terms of reward monotonicity or suboptimal at convergence. To
resolve these issues, in this paper, we introduce a novel framework named
Heterogeneous-Agent Mirror Learning (HAML) that provides a general template for
MARL algorithmic designs. We prove that algorithms derived from the HAML
template satisfy the desired properties of the monotonic improvement of the
joint reward and the convergence to Nash equilibrium. We verify the
practicality of HAML by proving that the current state-of-the-art cooperative
MARL algorithms, HATRPO and HAPPO, are in fact HAML instances. Next, as a
natural outcome of our theory, we propose HAML extensions of two well-known RL
algorithms, HAA2C (for A2C) and HADDPG (for DDPG), and demonstrate their
effectiveness against strong baselines on StarCraftII and Multi-Agent MuJoCo
tasks."
Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes,0.265738,"We study offline reinforcement learning (RL) in partially observable Markov
decision processes. In particular, we aim to learn an optimal policy from a
dataset collected by a behavior policy which possibly depends on the latent
state. Such a dataset is confounded in the sense that the latent state
simultaneously affects the action and the observation, which is prohibitive for
existing offline RL algorithms. To this end, we propose the \underline{P}roxy
variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization
(\texttt{P3O}) algorithm, which addresses the confounding bias and the
distributional shift between the optimal and behavior policies in the context
of general function approximation. At the core of \texttt{P3O} is a coupled
sequence of pessimistic confidence regions constructed via proximal causal
inference, which is formulated as minimax estimation. Under a partial coverage
assumption on the confounded dataset, we prove that \texttt{P3O} achieves a
$n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the
dataset. To our best knowledge, \texttt{P3O} is the first provably efficient
offline RL algorithm for POMDPs with a confounded dataset."
What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation,0.278736,"Accurate automatic evaluation metrics for open-domain dialogs are in high
demand. Existing model-based metrics for system response evaluation are trained
on human annotated data, which is cumbersome to collect. In this work, we
propose to use information that can be automatically extracted from the next
user utterance, such as its sentiment or whether the user explicitly ends the
conversation, as a proxy to measure the quality of the previous system
response. This allows us to train on a massive set of dialogs with weak
supervision, without requiring manual system turn quality annotations.
Experiments show that our model is comparable to models trained on human
annotated data. Furthermore, our model generalizes across both spoken and
written open-domain dialog corpora collected from real and paid users."
Multi-label Transformer for Action Unit Detection,0.300782,"Action Unit (AU) Detection is the branch of affective computing that aims at
recognizing unitary facial muscular movements. It is key to unlock unbiased
computational face representations and has therefore aroused great interest in
the past few years. One of the main obstacles toward building efficient deep
learning based AU detection system is the lack of wide facial image databases
annotated by AU experts. In that extent the ABAW challenge paves the way toward
better AU detection as it involves a 2M frames AU annotated dataset. In this
paper, we present our submission to the ABAW3 challenge. In a nutshell, we
applied a multi-label detection transformer that leverage multi-head attention
to learn which part of the face image is the most relevant to predict each AU."
Dialog Inpainting: Turning Documents into Dialogs,0.267734,"Many important questions (e.g. ""How to eat healthier?"") require conversation
to establish context and explore in depth. However, conversational question
answering (ConvQA) systems have long been stymied by scarce training data that
is expensive to collect. To address this problem, we propose a new technique
for synthetically generating diverse and high-quality dialog data: dialog
inpainting. Our approach takes the text of any document and transforms it into
a two-person dialog between the writer and an imagined reader: we treat
sentences from the article as utterances spoken by the writer, and then use a
dialog inpainter to predict what the imagined reader asked or said in between
each of the writer's utterances. By applying this approach to passages from
Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets
totalling 19 million diverse information-seeking dialogs -- 1,000x larger than
the largest existing ConvQA dataset. Furthermore, human raters judge the answer
adequacy and conversationality of WikiDialog to be as good or better than
existing manually-collected datasets. Using our inpainted data to pre-train
ConvQA retrieval systems, we significantly advance state-of-the-art across
three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains
on standard evaluation metrics."
Intelligent analysis of EEG signals to assess consumer decisions: A Study on Neuromarketing,0.30263,"Neuromarketing is an emerging field that combines neuroscience and marketing
to understand the factors that influence consumer decisions better. The study
proposes a method to understand consumers' positive and negative reactions to
advertisements (ads) and products by analysing electroencephalogram (EEG)
signals. These signals are recorded using a low-cost single electrode headset
from volunteers belonging to the ages 18-22. A detailed subject dependent (SD)
and subject independent (SI) analysis was performed employing machine learning
methods like Naive Bayes (NB), Support Vector Machine (SVM), k-nearest
neighbour and Decision Tree and the proposed deep learning (DL) model. SVM and
NB yielded an accuracy (Acc.) of 0.63 for the SD analysis. In SI analysis, SVM
performed better for the advertisement, product and gender-based analysis.
Furthermore, the performance of the DL model was on par with that of SVM,
especially, in product and ads-based analysis."
LEVEN: A Large-Scale Chinese Legal Event Detection Dataset,0.272608,"Recognizing facts is the most fundamental step in making judgments, hence
detecting events in the legal documents is important to legal case analysis
tasks. However, existing Legal Event Detection (LED) datasets only concern
incomprehensive event types and have limited annotated data, which restricts
the development of LED methods and their downstream applications. To alleviate
these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection
dataset, with 8,116 legal documents and 150,977 human-annotated event mentions
in 108 event types. Not only charge-related events, LEVEN also covers general
events, which are critical for legal case understanding but neglected in
existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and
has dozens of times the data scale of others, which shall significantly promote
the training and evaluation of LED methods. The results of extensive
experiments indicate that LED is challenging and needs further effort.
Moreover, we simply utilize legal events as side information to promote
downstream applications. The method achieves improvements of average 2.2 points
precision in low-resource judgment prediction, and 1.5 points mean average
precision in unsupervised case retrieval, which suggests the fundamentality of
LED. The source code and dataset can be obtained from
https://github.com/thunlp/LEVEN."
Quantification of emotions in decision making,0.294678,"The problem of quantification of emotions in the choice between alternatives
is considered. The alternatives are evaluated in a dual manner. From one side,
they are characterized by rational features defining the utility of each
alternative. From the other side, the choice is affected by emotions labeling
the alternatives as attractive or repulsive, pleasant or unpleasant. A decision
maker needs to make a choice taking into account both these features, the
utility of alternatives and their attractiveness. The notion of utility is
based on rational grounds, while the notion of attractiveness is vague and
rather is based on irrational feelings. A general method, allowing for the
quantification of the choice combining rational and emotional features is
described. Despite that emotions seem to avoid precise quantification, their
quantitative evaluation is possible at the aggregate level. The analysis of a
series of empirical data demonstrates the efficiency of the approach, including
the realistic behavioral problems that cannot be treated by the standard
expected utility theory."
Planning Assembly Sequence with Graph Transformer,0.233167,"Assembly sequence planning (ASP) is the essential process for modern
manufacturing, proven to be NP-complete thus its effective and efficient
solution has been a challenge for researchers in the field. In this paper, we
present a graph-transformer based framework for the ASP problem which is
trained and demonstrated on a self-collected ASP database. The ASP database
contains a self-collected set of LEGO models. The LEGO model is abstracted to a
heterogeneous graph structure after a thorough analysis of the original
structure and feature extraction. The ground truth assembly sequence is first
generated by brute-force search and then adjusted manually to in line with
human rational habits. Based on this self-collected ASP dataset, we propose a
heterogeneous graph-transformer framework to learn the latent rules for
assembly planning. We evaluated the proposed framework in a series of
experiment. The results show that the similarity of the predicted and ground
truth sequences can reach 0.44, a medium correlation measured by Kendall's
$\tau$. Meanwhile, we compared the different effects of node features and edge
features and generated a feasible and reasonable assembly sequence as a
benchmark for further research. Our data set and code is available on
https://github.com/AIR-DISCOVER/ICRA\_ASP."
Disentangling Visual Embeddings for Attributes and Objects,0.232578,"We study the problem of compositional zero-shot learning for object-attribute
recognition. Prior works use visual features extracted with a backbone network,
pre-trained for object classification and thus do not capture the subtly
distinct features associated with attributes. To overcome this challenge, these
studies employ supervision from the linguistic space, and use pre-trained word
embeddings to better separate and compose attribute-object pairs for
recognition. Analogous to linguistic embedding space, which already has unique
and agnostic embeddings for object and attribute, we shift the focus back to
the visual space and propose a novel architecture that can disentangle
attribute and object features in the visual space. We use visual decomposed
features to hallucinate embeddings that are representative for the seen and
novel compositions to better regularize the learning of our model. Extensive
experiments show that our method outperforms existing work with significant
margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created
based on VAW. The code, models, and dataset splits are publicly available at
https://github.com/nirat1606/OADis."
Overview of the HASOC Subtrack at FIRE 2022: Offensive Language Identification in Marathi,0.2687,"The widespread of offensive content online has become a reason for great
concern in recent years, motivating researchers to develop robust systems
capable of identifying such content automatically. With the goal of carrying
out a fair evaluation of these systems, several international competitions have
been organized, providing the community with important benchmark data and
evaluation methods for various languages. Organized since 2019, the HASOC (Hate
Speech and Offensive Content Identification) shared task is one of these
initiatives. In its fourth iteration, HASOC 2022 included three subtracks for
English, Hindi, and Marathi. In this paper, we report the results of the HASOC
2022 Marathi subtrack which provided participants with a dataset containing
data from Twitter manually annotated using the popular OLID taxonomy. The
Marathi track featured three additional subtracks, each corresponding to one
level of the taxonomy: Task A - offensive content identification (offensive vs.
non-offensive); Task B - categorization of offensive types (targeted vs.
untargeted), and Task C - offensive target identification (individual vs. group
vs. others). Overall, 59 runs were submitted by 10 teams. The best systems
obtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and
F1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of
traditional and deep learning approaches."
KSG: Knowledge and Skill Graph,0.231202,"The knowledge graph (KG) is an essential form of knowledge representation
that has grown in prominence in recent years. Because it concentrates on
nominal entities and their relationships, traditional knowledge graphs are
static and encyclopedic in nature. On this basis, event knowledge graph (Event
KG) models the temporal and spatial dynamics by text processing to facilitate
downstream applications, such as question-answering, recommendation and
intelligent search. Existing KG research, on the other hand, mostly focuses on
text processing and static facts, ignoring the vast quantity of dynamic
behavioral information included in photos, movies, and pre-trained neural
networks. In addition, no effort has been done to include behavioral
intelligence information into the knowledge graph for deep reinforcement
learning (DRL) and robot learning. In this paper, we propose a novel dynamic
knowledge and skill graph (KSG), and then we develop a basic and specific KSG
based on CN-DBpedia. The nodes are divided into entity and attribute nodes,
with entity nodes containing the agent, environment, and skill (DRL policy or
policy representation), and attribute nodes containing the entity description,
pre-train network, and offline dataset. KSG can search for different agents'
skills in various environments and provide transferable information for
acquiring new skills. This is the first study that we are aware of that looks
into dynamic KSG for skill retrieval and learning. Extensive experimental
results on new skill learning show that KSG boosts new skill learning
efficiency."
Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,0.282364,"Pre-trained language models (LMs) are shown to easily generate toxic
language. In this work, we systematically explore domain-adaptive training to
reduce the toxicity of language models. We conduct this study on three
dimensions: training corpus, model size, and parameter efficiency. For the
training corpus, we propose to leverage the generative power of LMs and
generate nontoxic datasets for domain-adaptive training, which mitigates the
exposure bias and is shown to be more data-efficient than using a curated
pre-training corpus. We demonstrate that the self-generation method
consistently outperforms the existing baselines across various model sizes on
both automatic and human evaluations, even when it uses a 1/3 smaller training
corpus. We then comprehensively study detoxifying LMs with parameter sizes
ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never
been studied before. We find that i) large LMs have similar toxicity levels as
smaller ones given the same pre-training corpus, and ii) large LMs require more
endeavor to detoxify. We also explore parameter-efficient training methods for
detoxification. We demonstrate that adding and training adapter-only layers in
LMs not only saves a lot of parameters but also achieves a better trade-off
between toxicity and perplexity than whole model adaptation for the large-scale
models."
Graph Coloring with Physics-Inspired Graph Neural Networks,0.333077,"We show how graph neural networks can be used to solve the canonical graph
coloring problem. We frame graph coloring as a multi-class node classification
problem and utilize an unsupervised training strategy based on the statistical
physics Potts model. Generalizations to other multi-class problems such as
community detection, data clustering, and the minimum clique cover problem are
straightforward. We provide numerical benchmark results and illustrate our
approach with an end-to-end application for a real-world scheduling use case
within a comprehensive encode-process-decode framework. Our optimization
approach performs on par or outperforms existing solvers, with the ability to
scale to problems with millions of variables."
TemporalUV: Capturing Loose Clothing with Temporally Coherent UV Coordinates,0.249708,"We propose a novel approach to generate temporally coherent UV coordinates
for loose clothing. Our method is not constrained by human body outlines and
can capture loose garments and hair. We implemented a differentiable pipeline
to learn UV mapping between a sequence of RGB inputs and textures via UV
coordinates. Instead of treating the UV coordinates of each frame separately,
our data generation approach connects all UV coordinates via feature matching
for temporal stability. Subsequently, a generative model is trained to balance
the spatial quality and temporal stability. It is driven by supervised and
unsupervised losses in both UV and image spaces. Our experiments show that the
trained models output high-quality UV coordinates and generalize to new poses.
Once a sequence of UV coordinates has been inferred by our model, it can be
used to flexibly synthesize new looks and modified visual styles. Compared to
existing methods, our approach reduces the computational workload to animate
new outfits by several orders of magnitude."
Detecting Arbitrary Keypoints on Limbs and Skis with Sparse Partly Correct Segmentation Masks,0.323584,"Analyses based on the body posture are crucial for top-class athletes in many
sports disciplines. If at all, coaches label only the most important keypoints,
since manual annotations are very costly. This paper proposes a method to
detect arbitrary keypoints on the limbs and skis of professional ski jumpers
that requires a few, only partly correct segmentation masks during training.
Our model is based on the Vision Transformer architecture with a special design
for the input tokens to query for the desired keypoints. Since we use
segmentation masks only to generate ground truth labels for the freely
selectable keypoints, partly correct segmentation masks are sufficient for our
training procedure. Hence, there is no need for costly hand-annotated
segmentation masks. We analyze different training techniques for freely
selected and standard keypoints, including pseudo labels, and show in our
experiments that only a few partly correct segmentation masks are sufficient
for learning to detect arbitrary keypoints on limbs and skis."
TAPE: Task-Agnostic Prior Embedding for Image Restoration,0.234432,"Learning a generalized prior for natural image restoration is an important
yet challenging task. Early methods mostly involved handcrafted priors
including normalized sparsity, l_0 gradients, dark channel priors, etc.
Recently, deep neural networks have been used to learn various image priors but
do not guarantee to generalize. In this paper, we propose a novel approach that
embeds a task-agnostic prior into a transformer. Our approach, named
Task-Agnostic Prior Embedding (TAPE), consists of two stages, namely,
task-agnostic pre-training and task-specific fine-tuning, where the first stage
embeds prior knowledge about natural images into the transformer and the second
stage extracts the knowledge to assist downstream image restoration.
Experiments on various types of degradation validate the effectiveness of TAPE.
The image restoration performance in terms of PSNR is improved by as much as
1.45dB and even outperforms task-specific algorithms. More importantly, TAPE
shows the ability of disentangling generalized image priors from degraded
images, which enjoys favorable transfer ability to unknown downstream tasks."
Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation,0.375568,"Federated learning (FL) can be essential in knowledge representation,
reasoning, and data mining applications over multi-source knowledge graphs
(KGs). A recent study FedE first proposes an FL framework that shares entity
embeddings of KGs across all clients. However, entity embedding sharing from
FedE would incur a severe privacy leakage. Specifically, the known entity
embedding can be used to infer whether a specific relation between two entities
exists in a private client. In this paper, we introduce a novel attack method
that aims to recover the original data based on the embedding information,
which is further used to evaluate the vulnerabilities of FedE. Furthermore, we
propose a Federated learning paradigm with privacy-preserving Relation
embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,
relation embedding sharing can significantly reduce the communication cost due
to its smaller size of queries. We conduct extensive experiments to evaluate
FedR with five different KG embedding models and three datasets. Compared to
FedE, FedR achieves similar utility and significant improvements regarding
privacy-preserving effect and communication efficiency on the link prediction
task."
Soft Diffusion: Score Matching for General Corruptions,0.392099,"We define a broader family of corruption processes that generalizes
previously known diffusion models. To reverse these general diffusions, we
propose a new objective called Soft Score Matching that provably learns the
score function for any linear corruption process and yields state of the art
results for CelebA. Soft Score Matching incorporates the degradation process in
the network. Our new loss trains the model to predict a clean image,
\textit{that after corruption}, matches the diffused observation. We show that
our objective learns the gradient of the likelihood under suitable regularity
conditions for a family of corruption processes. We further develop a
principled way to select the corruption levels for general diffusion processes
and a novel sampling method that we call Momentum Sampler. We show
experimentally that our framework works for general linear corruption
processes, such as Gaussian blur and masking. We achieve state-of-the-art FID
score $1.85$ on CelebA-64, outperforming all previous linear diffusion models.
We also show significant computational benefits compared to vanilla denoising
diffusion."
IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty,0.343075,"Single image surface normal estimation and depth estimation are closely
related problems as the former can be calculated from the latter. However, the
surface normals computed from the output of depth estimation methods are
significantly less accurate than the surface normals directly estimated by
networks. To reduce such discrepancy, we introduce a novel framework that uses
surface normal and its uncertainty to recurrently refine the predicted
depth-map. The depth of each pixel can be propagated to a query pixel, using
the predicted surface normal as guidance. We thus formulate depth refinement as
a classification of choosing the neighboring pixel to propagate from. Then, by
propagating to sub-pixel points, we upsample the refined, low-resolution
output. The proposed method shows state-of-the-art performance on NYUv2 and
iBims-1 - both in terms of depth and normal. Our refinement module can also be
attached to the existing depth estimation methods to improve their accuracy. We
also show that our framework, only trained for depth estimation, can also be
used for depth completion. The code is available at
https://github.com/baegwangbin/IronDepth."
Intelligent problem-solving as integrated hierarchical reinforcement learning,0.348196,"According to cognitive psychology and related disciplines, the development of
complex problem-solving behaviour in biological agents depends on hierarchical
cognitive mechanisms. Hierarchical reinforcement learning is a promising
computational approach that may eventually yield comparable problem-solving
behaviour in artificial agents and robots. However, to date the problem-solving
abilities of many human and non-human animals are clearly superior to those of
artificial systems. Here, we propose steps to integrate biologically inspired
hierarchical mechanisms to enable advanced problem-solving skills in artificial
agents. Therefore, we first review the literature in cognitive psychology to
highlight the importance of compositional abstraction and predictive
processing. Then we relate the gained insights with contemporary hierarchical
reinforcement learning methods. Interestingly, our results suggest that all
identified cognitive mechanisms have been implemented individually in isolated
computational architectures, raising the question of why there exists no single
unifying architecture that integrates them. As our final contribution, we
address this question by providing an integrative perspective on the
computational challenges to develop such a unifying architecture. We expect our
results to guide the development of more sophisticated cognitively inspired
hierarchical machine learning architectures."
Practical Exposure Correction: Great Truths Are Always Simple,0.415105,"Improving the visual quality of the given degraded observation by correcting
exposure level is a fundamental task in the computer vision community. Existing
works commonly lack adaptability towards unknown scenes because of the
data-driven patterns (deep networks) and limited regularization (traditional
optimization), and they usually need time-consuming inference. These two points
heavily limit their practicability. In this paper, we establish a Practical
Exposure Corrector (PEC) that assembles the characteristics of efficiency and
performance. To be concrete, we rethink the exposure correction to provide a
linear solution with exposure-sensitive compensation. Around generating the
compensation, we introduce an exposure adversarial function as the key engine
to fully extract valuable information from the observation. By applying the
defined function, we construct a segmented shrinkage iterative scheme to
generate the desired compensation. Its shrinkage nature supplies powerful
support for algorithmic stability and robustness. Extensive experimental
evaluations fully reveal the superiority of our proposed PEC. The code is
available at https://rsliu.tech/PEC."
Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,0.355797,"With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI."
Offline RL Policies Should be Trained to be Adaptive,0.395708,"Offline RL algorithms must account for the fact that the dataset they are
provided may leave many facets of the environment unknown. The most common way
to approach this challenge is to employ pessimistic or conservative methods,
which avoid behaviors that are too dissimilar from those in the training
dataset. However, relying exclusively on conservatism has drawbacks:
performance is sensitive to the exact degree of conservatism, and conservative
objectives can recover highly suboptimal policies. In this work, we propose
that offline RL methods should instead be adaptive in the presence of
uncertainty. We show that acting optimally in offline RL in a Bayesian sense
involves solving an implicit POMDP. As a result, optimal policies for offline
RL must be adaptive, depending not just on the current state but rather all the
transitions seen so far during evaluation.We present a model-free algorithm for
approximating this optimal adaptive policy, and demonstrate the efficacy of
learning such adaptive policies in offline RL benchmarks."
MagicPony: Learning Articulated 3D Animals in the Wild,0.406881,"We consider the problem of predicting the 3D shape, articulation, viewpoint,
texture, and lighting of an articulated animal like a horse given a single test
image as input. We present a new method, dubbed MagicPony, that learns this
predictor purely from in-the-wild single-view images of the object category,
with minimal assumptions about the topology of deformation. At its core is an
implicit-explicit representation of articulated shape and appearance, combining
the strengths of neural fields and meshes. In order to help the model
understand an object's shape and pose, we distil the knowledge captured by an
off-the-shelf self-supervised vision transformer and fuse it into the 3D model.
To overcome local optima in viewpoint estimation, we further introduce a new
viewpoint sampling scheme that comes at no additional training cost. MagicPony
outperforms prior work on this challenging task and demonstrates excellent
generalisation in reconstructing art, despite the fact that it is only trained
on real images."
Fine-Grained Object Classification via Self-Supervised Pose Alignment,0.418873,"Semantic patterns of fine-grained objects are determined by subtle appearance
difference of local parts, which thus inspires a number of part-based methods.
However, due to uncontrollable object poses in images, distinctive details
carried by local regions can be spatially distributed or even self-occluded,
leading to a large variation on object representation. For discounting pose
variations, this paper proposes to learn a novel graph based object
representation to reveal a global configuration of local parts for
self-supervised pose alignment across classes, which is employed as an
auxiliary feature regularization on a deep representation learning
network.Moreover, a coarse-to-fine supervision together with the proposed
pose-insensitive constraint on shallow-to-deep sub-networks encourages
discriminative features in a curriculum learning manner. We evaluate our method
on three popular fine-grained object classification benchmarks, consistently
achieving the state-of-the-art performance. Source codes are available at
https://github.com/yangxh11/P2P-Net."
Models and Datasets for Cross-Lingual Summarisation,0.388606,"We present a cross-lingual summarisation corpus with long documents in a
source language associated with multi-sentence summaries in a target language.
The corpus covers twelve language pairs and directions for four European
languages, namely Czech, English, French and German, and the methodology for
its creation can be applied to several other languages. We derive cross-lingual
document-summary instances from Wikipedia by combining lead paragraphs and
articles' bodies from language aligned Wikipedia titles. We analyse the
proposed cross-lingual summarisation task with automatic metrics and validate
it with a human study. To illustrate the utility of our dataset we report
experiments with multi-lingual pre-trained models in supervised, zero- and
few-shot, and out-of-domain scenarios."
A Low-Shot Object Counting Network With Iterative Prototype Adaptation,0.353208,"We consider low-shot counting of arbitrary semantic categories in the image
using only few annotated exemplars (few-shot) or no exemplars (no-shot). The
standard few-shot pipeline follows extraction of appearance queries from
exemplars and matching them with image features to infer the object counts.
Existing methods extract queries by feature pooling which neglects the shape
information (e.g., size and aspect) and leads to a reduced object localization
accuracy and count estimates. We propose a Low-shot Object Counting network
with iterative prototype Adaptation (LOCA). Our main contribution is the new
object prototype extraction module, which iteratively fuses the exemplar shape
and appearance information with image features. The module is easily adapted to
zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot
counting problems. LOCA outperforms all recent state-of-the-art methods on
FSC147 benchmark by 20-30% in RMSE on one-shot and few-shot and achieves
state-of-the-art on zero-shot scenarios, while demonstrating better
generalization capabilities."
Prompt Consistency for Zero-Shot Task Generalization,0.434231,"One of the most impressive results of recent NLP history is the ability of
pre-trained language models to solve new tasks in a zero-shot setting. To
achieve this, NLP tasks are framed as natural language prompts, generating a
response indicating the predicted output. Nonetheless, the performance in such
settings often lags far behind its supervised counterpart, suggesting a large
space for potential improvement. In this paper, we explore methods to utilize
unlabeled data to improve zero-shot performance. Specifically, we take
advantage of the fact that multiple prompts can be used to specify a single
task, and propose to regularize prompt consistency, encouraging consistent
predictions over this diverse set of prompts. Our method makes it possible to
fine-tune the model either with extra unlabeled training data, or directly on
test input at inference time in an unsupervised manner. In experiments, our
approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,
2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points
in terms of accuracy. The gains are often attained with a small number of
unlabeled examples."
Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,0.412992,"Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for applying a
pre-trained language model to a new task: fine-tuning all parameters, prompt
tuning, and in-context learning. We observe that fine-tuning generally has flat
or negative scaling curves on out-of-distribution compositional generalization
in semantic parsing evaluations. In-context learning has positive scaling
curves, but is generally outperformed by much smaller fine-tuned models.
Prompt-tuning can outperform fine-tuning, suggesting further potential
improvements from scaling as it exhibits a more positive scaling curve.
Additionally, we identify several error trends that vary with model scale. For
example, larger models are generally better at modeling the syntax of the
output space, but are also more prone to certain types of overfitting. Overall,
our study highlights limitations of current techniques for effectively
leveraging model scale for compositional generalization, while our analysis
also suggests promising directions for future work."
QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars,0.401363,"Real-time tracking of human body motion is crucial for interactive and
immersive experiences in AR/VR. However, very limited sensor data about the
body is available from standalone wearable devices such as HMDs (Head Mounted
Devices) or AR glasses. In this work, we present a reinforcement learning
framework that takes in sparse signals from an HMD and two controllers, and
simulates plausible and physically valid full body motions. Using high quality
full body motion as dense supervision during training, a simple policy network
can learn to output appropriate torques for the character to balance, walk, and
jog, while closely following the input signals. Our results demonstrate
surprisingly similar leg motions to ground truth without any observations of
the lower body, even when the input is only the 6D transformations of the HMD.
We also show that a single policy can be robust to diverse locomotion styles,
different body sizes, and novel environments."
Rethinking Performance Gains in Image Dehazing Networks,0.344541,"Image dehazing is an active topic in low-level vision, and many image
dehazing networks have been proposed with the rapid development of deep
learning. Although these networks' pipelines work fine, the key mechanism to
improving image dehazing performance remains unclear. For this reason, we do
not target to propose a dehazing network with fancy modules; rather, we make
minimal modifications to popular U-Net to obtain a compact dehazing network.
Specifically, we swap out the convolutional blocks in U-Net for residual blocks
with the gating mechanism, fuse the feature maps of main paths and skip
connections using the selective kernel, and call the resulting U-Net variant
gUNet. As a result, with a significantly reduced overhead, gUNet is superior to
state-of-the-art methods on multiple image dehazing datasets. Finally, we
verify these key designs to the performance gain of image dehazing networks
through extensive ablation studies."
Strategy Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic Games,0.424889,"Neuro-symbolic approaches to artificial intelligence, which combine neural
networks with classical symbolic techniques, are growing in prominence,
necessitating formal approaches to reason about their correctness. We propose a
novel modelling formalism called neuro-symbolic concurrent stochastic games
(NS-CSGs), which comprise two probabilistic finite-state agents interacting in
a shared continuous-state environment. Each agent observes the environment
using a neural perception mechanism, which converts inputs such as images into
symbolic percepts, and makes decisions symbolically. We focus on the class of
NS-CSGs with Borel state spaces and prove the existence and measurability of
the value function for zero-sum discounted cumulative rewards under
piecewise-constant restrictions on the components of this class of models. To
compute values and synthesise strategies, we present, for the first time,
practical value iteration (VI) and policy iteration (PI) algorithms to solve
this new subclass of continuous-state CSGs. These require a finite
decomposition of the environment induced by the neural perception mechanisms of
the agents and rely on finite abstract representations of value functions and
strategies closed under VI or PI. First, we introduce a Borel measurable
piecewise-constant (B-PWC) representation of value functions, extend minimax
backups to this representation and propose a value iteration algorithm called
B-PWC VI. Second, we introduce two novel representations for the value
functions and strategies, constant-piecewise-linear (CON-PWL) and
constant-piecewise-constant (CON-PWC) respectively, and propose
Minimax-action-free PI by extending a recent PI method based on alternating
player choices for finite state spaces to Borel state spaces, which does not
require normal-form games to be solved."
Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings,0.390131,"Automatic depression detection on Twitter can help individuals privately and
conveniently understand their mental health status in the early stages before
seeing mental health professionals. Most existing black-box-like deep learning
methods for depression detection largely focused on improving classification
performance. However, explaining model decisions is imperative in health
research because decision-making can often be high-stakes and life-and-death.
Reliable automatic diagnosis of mental health problems including depression
should be supported by credible explanations justifying models' predictions. In
this work, we propose a novel explainable model for depression detection on
Twitter. It comprises a novel encoder combining hierarchical attention
mechanisms and feed-forward neural networks. To support psycholinguistic
studies, our model leverages metaphorical concept mappings as input. Thus, it
not only detects depressed individuals, but also identifies features of such
users' tweets and associated metaphor concept mappings."
IoV Scenario: Implementation of a Bandwidth Aware Algorithm in Wireless Network Communication Mode,0.344612,"The wireless network communication mode represented by the Internet of
vehicles (IoV) has been widely used. However, due to the limitations of
traditional network architecture, resource scheduling in wireless network
environment is still facing great challenges. This paper focuses on the
allocation of bandwidth resources in the virtual network environment. This
paper proposes a bandwidth aware multi domain virtual network embedding
algorithm (BA-VNE). The algorithm is mainly aimed at the problem that users
need a lot of bandwidth in wireless communication mode, and solves the problem
of bandwidth resource allocation from the perspective of virtual network
embedding (VNE). In order to improve the performance of the algorithm, we
introduce particle swarm optimization (PSO) algorithm to optimize the
performance of the algorithm. In order to verify the effectiveness of the
algorithm, we have carried out simulation experiments from link bandwidth,
mapping cost and virtual network request (VNR) acceptance rate. The final
results show that the proposed algorithm is better than other representative
algorithms in the above indicators."
Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint,0.344497,"Active learning is a promising alternative to alleviate the issue of high
annotation cost in the computer vision tasks by consciously selecting more
informative samples to label. Active learning for object detection is more
challenging and existing efforts on it are relatively rare. In this paper, we
propose a novel hybrid approach to address this problem, where the
instance-level uncertainty and diversity are jointly considered in a bottom-up
manner. To balance the computational complexity, the proposed approach is
designed as a two-stage procedure. At the first stage, an Entropy-based
Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of
every image, which performs NMS according to the entropy in the feature space
to remove predictions with redundant information gains. At the second stage, a
diverse prototype (DivProto) strategy is explored to ensure the diversity
across images by progressively converting it into the intra-class and
inter-class diversities of the entropy-based class-specific prototypes.
Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed
approach achieves state of the art results and significantly outperforms the
other counterparts, highlighting its superiority."
Ditto: Building Digital Twins of Articulated Objects from Interaction,0.431631,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto"
Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation,0.366949,"Document-level Relation Extraction (DocRE) is a more challenging task
compared to its sentence-level counterpart. It aims to extract relations from
multiple sentences at once. In this paper, we propose a semi-supervised
framework for DocRE with three novel components. Firstly, we use an axial
attention module for learning the interdependency among entity-pairs, which
improves the performance on two-hop relations. Secondly, we propose an adaptive
focal loss to tackle the class imbalance problem of DocRE. Lastly, we use
knowledge distillation to overcome the differences between human annotated data
and distantly supervised data. We conducted experiments on two DocRE datasets.
Our model consistently outperforms strong baselines and its performance exceeds
the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.
Our code and data will be released at https://github.com/tonytan48/KD-DocRE."
Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models,0.343444,"Automatically summarizing patients' main problems from daily progress notes
using natural language processing methods helps to battle against information
and cognitive overload in hospital settings and potentially assists providers
with computerized diagnostic decision support. Problem list summarization
requires a model to understand, abstract, and generate clinical documentation.
In this work, we propose a new NLP task that aims to generate a list of
problems in a patient's daily care plan using input from the provider's
progress notes during hospitalization. We investigate the performance of T5 and
BART, two state-of-the-art seq2seq transformer architectures, in solving this
problem. We provide a corpus built on top of progress notes from publicly
available electronic health record progress notes in the Medical Information
Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain
text, and we experiment with a data augmentation method and a domain adaptation
pre-training method to increase exposure to medical vocabulary and knowledge.
Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence
embedding, and F-score on medical concepts. Results show that T5 with domain
adaptive pre-training achieves significant performance gains compared to a
rule-based system and general domain pre-trained language models, indicating a
promising direction for tackling the problem summarization task."
Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition,0.351796,"This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech
Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio
Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech
with a help of audio-visual correspondence. The proposed V-CAFE is designed to
capture the transition of lip movements, namely visual context and to generate
a noise reduction mask by considering the obtained visual context. Through
context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be
refined for mask generation. The noisy representations are masked out with the
noise reduction mask resulting in enhanced audio features. The enhanced audio
features are fused with the visual features and taken to an encoder-decoder
model composed of Conformer and Transformer for speech recognition. We show the
proposed end-to-end AVSR with the V-CAFE can further improve the
noise-robustness of AVSR. The effectiveness of the proposed method is evaluated
in noisy speech recognition and overlapped speech recognition experiments using
the two largest audio-visual datasets, LRS2 and LRS3."
DKM: Dense Kernelized Feature Matching for Geometry Estimation,0.440571,"Feature matching is a challenging computer vision task that involves finding
correspondences between two images of a 3D scene. In this paper we consider the
dense approach instead of the more common sparse paradigm, thus striving to
find all correspondences. Perhaps counter-intuitively, dense methods have
previously shown inferior performance to their sparse and semi-sparse
counterparts for estimation of two-view geometry. This changes with our novel
dense method, which outperforms both dense and sparse methods on geometry
estimation. The novelty is threefold: First, we propose a kernel regression
global matcher. Secondly, we propose warp refinement through stacked feature
maps and depthwise convolution kernels. Thirdly, we propose learning dense
confidence through consistent depth and a balanced sampling approach for dense
confidence maps. Through extensive experiments we confirm that our proposed
dense method, \textbf{D}ense \textbf{K}ernelized Feature \textbf{M}atching,
sets a new state-of-the-art on multiple geometry estimation benchmarks. In
particular, we achieve an improvement on MegaDepth-1500 of +4.9 and +8.9
AUC$@5^{\circ}$ compared to the best previous sparse method and dense method
respectively. Our code is provided at https://github.com/Parskatt/dkm"
Quark: Controllable Text Generation with Reinforced Unlearning,0.382432,"Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives."
Voxel Field Fusion for 3D Object Detection,0.370128,"In this work, we present a conceptually simple yet effective framework for
cross-modality 3D object detection, named voxel field fusion. The proposed
approach aims to maintain cross-modality consistency by representing and fusing
augmented image features as a ray in the voxel field. To this end, the
learnable sampler is first designed to sample vital features from the image
plane that are projected to the voxel grid in a point-to-ray manner, which
maintains the consistency in feature representation with spatial context. In
addition, ray-wise fusion is conducted to fuse features with the supplemental
context in the constructed voxel field. We further develop mixed augmentor to
align feature-variant transformations, which bridges the modality gap in data
augmentation. The proposed framework is demonstrated to achieve consistent
gains in various benchmarks and outperforms previous fusion-based methods on
KITTI and nuScenes datasets. Code is made available at
https://github.com/dvlab-research/VFF."
EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance,0.401518,"Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process."
"Less Data, More Knowledge: Building Next Generation Semantic Communication Networks",0.443997,"Semantic communication is viewed as a revolutionary paradigm that can
potentially transform how we design and operate wireless communication systems.
However, despite a recent surge of research activities in this area, the
research landscape remains limited. In this tutorial, we present the first
rigorous vision of a scalable end-to-end semantic communication network that is
founded on novel concepts from artificial intelligence (AI), causal reasoning,
and communication theory. We first discuss how the design of semantic
communication networks requires a move from data-driven networks towards
knowledge-driven ones. Subsequently, we highlight the necessity of creating
semantic representations of data that satisfy the key properties of minimalism,
generalizability, and efficiency so as to do more with less. We then explain
how those representations can form the basis a so-called semantic language. By
using semantic representation and languages, we show that the traditional
transmitter and receiver now become a teacher and apprentice. Then, we define
the concept of reasoning by investigating the fundamentals of causal
representation learning and their role in designing semantic communication
networks. We demonstrate that reasoning faculties are majorly characterized by
the ability to capture causal and associational relationships in datastreams.
For such reasoning-driven networks, we propose novel and essential semantic
communication metrics that include new ""reasoning capacity"" measures that could
go beyond Shannon's bound to capture the convergence of computing and
communication. Finally, we explain how semantic communications can be scaled to
large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to
provide a comprehensive reference on how to properly build, analyze, and deploy
future semantic communication networks."
Consent as a Foundation for Responsible Autonomy,0.390681,"This paper focuses on a dynamic aspect of responsible autonomy, namely, to
make intelligent agents be responsible at run time. That is, it considers
settings where decision making by agents impinges upon the outcomes perceived
by other agents. For an agent to act responsibly, it must accommodate the
desires and other attitudes of its users and, through other agents, of their
users.
  The contribution of this paper is twofold. First, it provides a conceptual
analysis of consent, its benefits and misuses, and how understanding consent
can help achieve responsible autonomy. Second, it outlines challenges for AI
(in particular, for agents and multiagent systems) that merit investigation to
form as a basis for modeling consent in multiagent systems and applying consent
to achieve responsible autonomy."
Region-Aware Face Swapping,0.395715,"This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$."
3D Dual-Fusion: Dual-Domain Dual-Query Camera-LiDAR Fusion for 3D Object Detection,0.426232,"Fusing data from cameras and LiDAR sensors is an essential technique to
achieve robust 3D object detection. One key challenge in camera-LiDAR fusion
involves mitigating the large domain gap between the two sensors in terms of
coordinates and data distribution when fusing their features. In this paper, we
propose a novel camera-LiDAR fusion architecture called, 3D Dual-Fusion, which
is designed to mitigate the gap between the feature representations of camera
and LiDAR data. The proposed method fuses the features of the camera-view and
3D voxel-view domain and models their interactions through deformable
attention. We redesign the transformer fusion encoder to aggregate the
information from the two domains. Two major changes include 1) dual query-based
deformable attention to fuse the dual-domain features interactively and 2) 3D
local self-attention to encode the voxel-domain queries prior to dual-query
decoding. The results of an experimental evaluation show that the proposed
camera-LiDAR fusion architecture achieved competitive performance on the KITTI
and nuScenes datasets, with state-of-the-art performances in some 3D object
detection benchmarks categories."
User-Controllable Latent Transformer for StyleGAN Image Layout Editing,0.381482,"Latent space exploration is a technique that discovers interpretable latent
directions and manipulates latent codes to edit various attributes in images
generated by generative adversarial networks (GANs). However, in previous work,
spatial control is limited to simple transformations (e.g., translation and
rotation), and it is laborious to identify appropriate latent directions and
adjust their parameters. In this paper, we tackle the problem of editing the
StyleGAN image layout by annotating the image directly. To do so, we propose an
interactive framework for manipulating latent codes in accordance with the user
inputs. In our framework, the user annotates a StyleGAN image with locations
they want to move or not and specifies a movement direction by mouse dragging.
From these user inputs and initial latent codes, our latent transformer based
on a transformer encoder-decoder architecture estimates the output latent
codes, which are fed to the StyleGAN generator to obtain a result image. To
train our latent transformer, we utilize synthetic data and pseudo-user inputs
generated by off-the-shelf StyleGAN and optical flow models, without manual
supervision. Quantitative and qualitative evaluations demonstrate the
effectiveness of our method over existing methods."
Learning State-Aware Visual Representations from Audible Interactions,0.402428,"We propose a self-supervised algorithm to learn representations from
egocentric video data. Recently, significant efforts have been made to capture
humans interacting with their own environments as they go about their daily
activities. In result, several large egocentric datasets of interaction-rich
multi-modal data have emerged. However, learning representations from videos
can be challenging. First, given the uncurated nature of long-form continuous
videos, learning effective representations require focusing on moments in time
when interactions take place. Second, visual representations of daily
activities should be sensitive to changes in the state of the environment.
However, current successful multi-modal learning frameworks encourage
representation invariance over time. To address these challenges, we leverage
audio signals to identify moments of likely interactions which are conducive to
better learning. We also propose a novel self-supervised objective that learns
from audible state changes caused by interactions. We validate these
contributions extensively on two large-scale egocentric datasets,
EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on
several downstream tasks, including action recognition, long-term action
anticipation, and object state change classification."
Transformer-based SAR Image Despeckling,0.424057,"Synthetic Aperture Radar (SAR) images are usually degraded by a
multiplicative noise known as speckle which makes processing and interpretation
of SAR images difficult. In this paper, we introduce a transformer-based
network for SAR image despeckling. The proposed despeckling network comprises
of a transformer-based encoder which allows the network to learn global
dependencies between different image regions - aiding in better despeckling.
The network is trained end-to-end with synthetically generated speckled images
using a composite loss function. Experiments show that the proposed method
achieves significant improvements over traditional and convolutional neural
network-based despeckling methods on both synthetic and real SAR images."
SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer,0.346647,"Point cloud completion has become increasingly popular among generation tasks
of 3D point clouds, as it is a challenging yet indispensable problem to recover
the complete shape of a 3D object from its partial observation. In this paper,
we propose a novel SeedFormer to improve the ability of detail preservation and
recovery in point cloud completion. Unlike previous methods based on a global
feature vector, we introduce a new shape representation, namely Patch Seeds,
which not only captures general structures from partial inputs but also
preserves regional information of local patterns. Then, by integrating seed
features into the generation process, we can recover faithful details for
complete point clouds in a coarse-to-fine manner. Moreover, we devise an
Upsample Transformer by extending the transformer structure into basic
operations of point generators, which effectively incorporates spatial and
semantic relationships between neighboring points. Qualitative and quantitative
evaluations demonstrate that our method outperforms state-of-the-art completion
networks on several benchmark datasets. Our code is available at
https://github.com/hrzhou2/seedformer."
"Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction",0.418725,"Motion prediction is highly relevant to the perception of dynamic objects and
static map elements in the scenarios of autonomous driving. In this work, we
propose PIP, the first end-to-end Transformer-based framework which jointly and
interactively performs online mapping, object detection and motion prediction.
PIP leverages map queries, agent queries and mode queries to encode the
instance-wise information of map elements, agents and motion intentions,
respectively. Based on the unified query representation, a differentiable
multi-task interaction scheme is proposed to exploit the correlation between
perception and prediction. Even without human-annotated HD map or agent's
historical tracking trajectory as guidance information, PIP realizes end-to-end
multi-agent motion prediction and achieves better performance than
tracking-based and HD-map-based methods. PIP provides comprehensive high-level
information of the driving scene (vectorized static map and dynamic objects
with motion information), and contributes to the downstream planning and
control. Code and models will be released for facilitating further research."
Aggregate effects of advertising decisions: a complex systems look at search engine advertising via an experimental study,0.365903,"Purpose: We model group advertising decisions, which are the collective
decisions of every single advertiser within the set of advertisers who are
competing in the same auction or vertical industry, and examine resulting
market outcomes, via a proposed simulation framework named EXP-SEA
(Experimental Platform for Search Engine Advertising) supporting experimental
studies of collective behaviors in the context of search engine advertising.
Design: We implement the EXP-SEA to validate the proposed simulation framework,
also conduct three experimental studies on the aggregate impact of electronic
word-of-mouth, the competition level, and strategic bidding behaviors. EXP-SEA
supports heterogeneous participants, various auction mechanisms, and also
ranking and pricing algorithms. Findings: Findings from our three experiments
show that (a) both the market profit and advertising indexes such as number of
impressions and number of clicks are larger when the eWOM effect presents,
meaning social media certainly has some effect on search engine advertising
outcomes, (b) the competition level has a monotonic increasing effect on the
market performance, thus search engines have an incentive to encourage both the
eWOM among search users and competition among advertisers, and (c) given the
market-level effect of the percentage of advertisers employing a dynamic greedy
bidding strategy, there is a cut-off point for strategic bidding behaviors.
Originality: This is one of the first research works to explore collective
group decisions and resulting phenomena in the complex context of search engine
advertising via developing and validating a simulation framework that supports
assessments of various advertising strategies and estimations of the impact of
mechanisms on the search market."
Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,0.361327,"Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small."
PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects,0.440829,"Object pose estimation is crucial for robotic applications and augmented
reality. Beyond instance level 6D object pose estimation methods, estimating
category-level pose and shape has become a promising trend. As such, a new
research field needs to be supported by well-designed datasets. To provide a
benchmark with high-quality ground truth annotations to the community, we
introduce a multimodal dataset for category-level object pose estimation with
photometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high
quality 3D models of household objects over 8 categories including highly
reflective, transparent and symmetric objects. We developed a novel
robot-supported multi-modal (RGB, depth, polarisation) data acquisition and
annotation process. It ensures sub-millimeter accuracy of the pose for opaque
textured, shiny and transparent objects, no motion blur and perfect camera
synchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and
monocular RGB methods are evaluated on the challenging scenes of PhoCaL."
Online Continual Learning for Embedded Devices,0.396486,"Real-time on-device continual learning is needed for new applications such as
home robots, user personalization on smartphones, and augmented/virtual reality
headsets. However, this setting poses unique challenges: embedded devices have
limited memory and compute capacity and conventional machine learning models
suffer from catastrophic forgetting when updated on non-stationary data
streams. While several online continual learning models have been developed,
their effectiveness for embedded applications has not been rigorously studied.
In this paper, we first identify criteria that online continual learners must
meet to effectively perform real-time, on-device learning. We then study the
efficacy of several online continual learning methods when used with mobile
neural networks. We measure their performance, memory usage, compute
requirements, and ability to generalize to out-of-domain inputs."
Using Deep Mixture-of-Experts to Detect Word Meaning Shift for TempoWiC,0.416355,"This paper mainly describes the dma submission to the TempoWiC task, which
achieves a macro-F1 score of 77.05% and attains the first place in this task.
We first explore the impact of different pre-trained language models. Then we
adopt data cleaning, data augmentation, and adversarial training strategies to
enhance the model generalization and robustness. For further improvement, we
integrate POS information and word semantic representation using a
Mixture-of-Experts (MoE) approach. The experimental results show that MoE can
overcome the feature overuse issue and combine the context, POS, and word
semantic features well. Additionally, we use a model ensemble method for the
final prediction, which has been proven effective by many research works."
Detector-Free Weakly Supervised Group Activity Recognition,0.345982,"Group activity recognition is the task of understanding the activity
conducted by a group of people as a whole in a multi-person video. Existing
models for this task are often impractical in that they demand ground-truth
bounding box labels of actors even in testing or rely on off-the-shelf object
detectors. Motivated by this, we propose a novel model for group activity
recognition that depends neither on bounding box labels nor on object detector.
Our model based on Transformer localizes and encodes partial contexts of a
group activity by leveraging the attention mechanism, and represents a video
clip as a set of partial context embeddings. The embedding vectors are then
aggregated to form a single group representation that reflects the entire
context of an activity while capturing temporal evolution of each partial
context. Our method achieves outstanding performance on two benchmarks,
Volleyball and NBA datasets, surpassing not only the state of the art trained
with the same level of supervision, but also some of existing models relying on
stronger supervision."
SimVP: Simpler yet Better Video Prediction,0.422232,"From CNN, RNN, to ViT, we have witnessed remarkable advancements in video
prediction, incorporating auxiliary inputs, elaborate neural architectures, and
sophisticated training strategies. We admire these progresses but are confused
about the necessity: is there a simple method that can perform comparably well?
This paper proposes SimVP, a simple video prediction model that is completely
built upon CNN and trained by MSE loss in an end-to-end fashion. Without
introducing any additional tricks and complicated strategies, we can achieve
state-of-the-art performance on five benchmark datasets. Through extended
experiments, we demonstrate that SimVP has strong generalization and
extensibility on real-world datasets. The significant reduction of training
cost makes it easier to scale to complex scenarios. We believe SimVP can serve
as a solid baseline to stimulate the further development of video prediction.
The code is available at
\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}."
Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion,0.347329,"Knowledge Graph Completion (KGC) has been recently extended to multiple
knowledge graph (KG) structures, initiating new research directions, e.g.
static KGC, temporal KGC and few-shot KGC. Previous works often design KGC
models closely coupled with specific graph structures, which inevitably results
in two drawbacks: 1) structure-specific KGC models are mutually incompatible;
2) existing KGC methods are not adaptable to emerging KGs. In this paper, we
propose KG-S2S, a Seq2Seq generative framework that could tackle different
verbalizable graph structures by unifying the representation of KG facts into
""flat"" text, regardless of their original form. To remedy the KG structure
information loss from the ""flat"" text, we further improve the input
representations of entities and relations, and the inference algorithm in
KG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many
competitive baselines, setting new state-of-the-art performance. Finally, we
analyze KG-S2S's ability on the different relations and the Non-entity
Generations."
High-resolution Face Swapping via Latent Semantics Disentanglement,0.42671,"We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes."
Long-tailed Instance Segmentation using Gumbel Optimized Loss,0.429659,"Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL"
Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,0.343467,"Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research."
On the Effects of Image Quality Degradation on Minutiae- and Ridge-Based Automatic Fingerprint Recognition,0.398929,"The effect of image quality degradation on the verification performance of
automatic fingerprint recognition is investigated. We study the performance of
two fingerprint matchers based on minutiae and ridge information under varying
fingerprint image quality. The ridge-based system is found to be more robust to
image quality degradation than the minutiae-based system for a number of
different image quality criteria."
Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss,0.336553,"We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the facial video PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost."
Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022,0.334039,"In this work, we present the winning solution for ORBIT Few-Shot Video Object
Recognition Challenge 2022. Built upon the ProtoNet baseline, the performance
of our method is improved with three effective techniques. These techniques
include the embedding adaptation, the uniform video clip sampler and the
invalid frame detection. In addition, we re-factor and re-implement the
official codebase to encourage modularity, compatibility and improved
performance. Our implementation accelerates the data loading in both training
and testing."
Audio-Adaptive Activity Recognition Across Video Domains,0.385063,"This paper strives for activity recognition under domain shift, for example
caused by change of scenery or camera viewpoint. The leading approaches reduce
the shift in activity appearance by adversarial training and self-supervised
learning. Different from these vision-focused works we leverage activity sounds
for domain adaptation as they have less variance across domains and can
reliably indicate which activities are not happening. We propose an
audio-adaptive encoder and associated learning methods that discriminatively
adjust the visual feature representation as well as addressing shifts in the
semantic distribution. To further eliminate domain-specific features and
include domain-invariant activity sounds for recognition, an audio-infused
recognizer is proposed, which effectively models the cross-modal interaction
across domains. We also introduce the new task of actor shift, with a
corresponding audio-visual dataset, to challenge our method with situations
where the activity appearance changes dramatically. Experiments on this
dataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach."
Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras,0.423898,"We propose a novel and pragmatic framework for traffic scene perception with
roadside cameras. The proposed framework covers a full-stack of roadside
perception pipeline for infrastructure-assisted autonomous driving, including
object detection, object localization, object tracking, and multi-camera
information fusion. Unlike previous vision-based perception frameworks rely
upon depth offset or 3D annotation at training, we adopt a modular decoupling
design and introduce a landmark-based 3D localization method, where the
detection and localization can be well decoupled so that the model can be
easily trained based on only 2D annotations. The proposed framework applies to
either optical or thermal cameras with pinhole or fish-eye lenses. Our
framework is deployed at a two-lane roundabout located at Ellsworth Rd. and
State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring
and high-precision vehicle trajectory extraction. The whole system runs
efficiently on a low-power edge computing device with all-component end-to-end
delay of less than 20ms."
A sequence-to-sequence approach for document-level relation extraction,0.441497,"Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\url{https://github.com/johngiorgi/seq2rel}}. An
online demo is available at
{\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}."
VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction,0.337467,"The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark."
PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization,0.3437,"Few-shot abstractive summarization has become a challenging task in natural
language generation. To support it, we designed a novel soft prompts
architecture coupled with a prompt pre-training plus fine-tuning paradigm that
is effective and tunes only extremely light parameters. The soft prompts
include continuous input embeddings across an encoder and a decoder to fit the
structure of the generation models. Importantly, a novel inner-prompt placed in
the text is introduced to capture document-level information. The aim is to
devote attention to understanding the document that better prompts the model to
generate document-related content. The first step in the summarization
procedure is to conduct prompt pre-training with self-supervised pseudo-data.
This teaches the model basic summarizing capabilities. The model is then
fine-tuned with few-shot examples. Experimental results on the CNN/DailyMail
and XSum datasets show that our method, with only 0.1% of the parameters,
outperforms full-model tuning where all model parameters are tuned. It also
surpasses Prompt Tuning by a large margin and delivers competitive results
against Prefix-Tuning with 3% of the parameters."
Finite Entailment of UCRPQs over ALC Ontologies,0.35882,"We investigate the problem of finite entailment of ontology-mediated queries.
We consider the expressive query language, unions of conjunctive regular path
queries (UCRPQs), extending the well-known class of union of conjunctive
queries, with regular expressions over roles. We look at ontologies formulated
using the description logic ALC, and show a tight 2EXPTIME upper bound for
entailment of UCRPQs. At the core of our decision procedure, there is a novel
automata-based technique introducing a stratification of interpretations
induced by the deterministic finite automaton underlying the input UCRPQ"
VS-CAM: Vertex Semantic Class Activation Mapping to Interpret Vision Graph Neural Network,0.425036,"Graph convolutional neural network (GCN) has drawn increasing attention and
attained good performance in various computer vision tasks, however, there
lacks a clear interpretation of GCN's inner mechanism. For standard
convolutional neural networks (CNNs), class activation mapping (CAM) methods
are commonly used to visualize the connection between CNN's decision and image
region by generating a heatmap. Nonetheless, such heatmap usually exhibits
semantic-chaos when these CAMs are applied to GCN directly. In this paper, we
proposed a novel visualization method particularly applicable to GCN, Vertex
Semantic Class Activation Mapping (VS-CAM). VS-CAM includes two independent
pipelines to produce a set of semantic-probe maps and a semantic-base map,
respectively. Semantic-probe maps are used to detect the semantic information
from semantic-base map to aggregate a semantic-aware heatmap. Qualitative
results show that VS-CAM can obtain heatmaps where the highlighted regions
match the objects much more precisely than CNN-based CAM. The quantitative
evaluation further demonstrates the superiority of VS-CAM."
UniDU: Towards A Unified Generative Dialogue Understanding Framework,0.362007,"With the development of pre-trained language models, remarkable success has
been witnessed in dialogue understanding (DU). However, current DU approaches
usually employ independent models for each distinct DU task without considering
shared knowledge across different DU tasks. In this paper, we propose a unified
generative dialogue understanding framework, named {\em UniDU}, to achieve
effective information exchange across diverse DU tasks. Here, we reformulate
all DU tasks into a unified prompt-based generative model paradigm. More
importantly, a novel model-agnostic multi-task training strategy (MATS) is
introduced to dynamically adapt the weights of diverse tasks for best knowledge
sharing during training, based on the nature and available data of each task.
Experiments on ten DU datasets covering five fundamental DU tasks show that the
proposed UniDU framework largely outperforms task-specific well-designed
methods on all tasks. MATS also reveals the knowledge-sharing structure of
these tasks. Finally, UniDU obtains promising performance in the unseen
dialogue domain, showing the great potential for generalization."
Describing Differences between Text Distributions with Natural Language,0.39926,"How do two distributions of texts differ? Humans are slow at answering this,
since discovering patterns might require tediously reading through hundreds of
samples. We propose to automatically summarize the differences by ""learning a
natural language hypothesis"": given two distributions $D_{0}$ and $D_{1}$, we
search for a description that is more often true for $D_{1}$, e.g., ""is
military-related."" To tackle this problem, we fine-tune GPT-3 to propose
descriptions with the prompt: ""[samples of $D_{0}$] + [samples of $D_{1}$] +
the difference between them is_____."" We then re-rank the descriptions by
checking how often they hold on a larger set of samples with a learned
verifier. On a benchmark of 54 real-world binary classification tasks, while
GPT-3 Curie (13B) only generates a description similar to human annotation 7%
of the time, the performance reaches 61% with fine-tuning and re-ranking, and
our best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to
describe distribution shifts, debug dataset shortcuts, summarize unknown tasks,
and label text clusters, and present analyses based on automatically generated
descriptions."
Retrieval Augmentation for Commonsense Reasoning: A Unified Approach,0.349357,"A common thread of retrieval-augmented methods in the existing literature
focuses on retrieving encyclopedic knowledge, such as Wikipedia, which
facilitates well-defined entity and relation spaces that can be modeled.
However, applying such methods to commonsense reasoning tasks faces two unique
challenges, i.e., the lack of a general large-scale corpus for retrieval and a
corresponding effective commonsense retriever. In this paper, we systematically
investigate how to leverage commonsense knowledge retrieval to improve
commonsense reasoning tasks. We proposed a unified framework of
retrieval-augmented commonsense reasoning (called RACo), including a newly
constructed commonsense corpus with over 20 million documents and novel
strategies for training a commonsense retriever. We conducted experiments on
four different commonsense reasoning tasks. Extensive evaluation results showed
that our proposed RACo can significantly outperform other knowledge-enhanced
method counterparts, achieving new SoTA performance on the CommonGen and CREAK
leaderboards."
Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Deep Reinforcement Learning Through Environmental Generalization,0.357525,"Previous works showed that Deep-RL can be applied to perform mapless
navigation, including the medium transition of Hybrid Unmanned Aerial
Underwater Vehicles (HUAUVs). This paper presents new approaches based on the
state-of-the-art actor-critic algorithms to address the navigation and medium
transition problems for a HUAUV. We show that a double critic Deep-RL with
Recurrent Neural Networks improves the navigation performance of HUAUVs using
solely range data and relative localization. Our Deep-RL approaches achieved
better navigation and transitioning capabilities with a solid generalization of
learning through distinct simulated scenarios, outperforming previous
approaches."
Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction,0.419326,"Compared with traditional sentence-level relation extraction, document-level
relation extraction is a more challenging task where an entity in a document
may be mentioned multiple times and associated with multiple relations.
However, most methods of document-level relation extraction do not distinguish
between mention-level features and entity-level features, and just apply simple
pooling operation for aggregating mention-level features into entity-level
features. As a result, the distinct semantics between the different mentions of
an entity are overlooked. To address this problem, we propose RSMAN in this
paper which performs selective attentions over different entity mentions with
respect to candidate relations. In this manner, the flexible and
relation-specific representations of entities are obtained which indeed benefit
relation classification. Our extensive experiments upon two benchmark datasets
show that our RSMAN can bring significant improvements for some backbone models
to achieve state-of-the-art performance, especially when an entity have
multiple mentions in the document."
Boundary-Guided Camouflaged Object Detection,0.356937,"Camouflaged object detection (COD), segmenting objects that are elegantly
blended into their surroundings, is a valuable yet challenging task. Existing
deep-learning methods often fall into the difficulty of accurately identifying
the camouflaged object with complete and fine object structure. To this end, in
this paper, we propose a novel boundary-guided network (BGNet) for camouflaged
object detection. Our method explores valuable and extra object-related edge
semantics to guide representation learning of COD, which forces the model to
generate features that highlight object structure, thereby promoting
camouflaged object detection of accurate boundary localization. Extensive
experiments on three challenging benchmark datasets demonstrate that our BGNet
significantly outperforms the existing 18 state-of-the-art methods under four
widely-used evaluation metrics. Our code is publicly available at:
https://github.com/thograce/BGNet."
Open Vocabulary Extreme Classification Using Generative Models,0.402445,"The extreme multi-label classification (XMC) task aims at tagging content
with a subset of labels from an extremely large label set. The label vocabulary
is typically defined in advance by domain experts and assumed to capture all
necessary tags. However in real world scenarios this label set, although large,
is often incomplete and experts frequently need to refine it. To develop
systems that simplify this process, we introduce the task of open vocabulary
XMC (OXMC): given a piece of content, predict a set of labels, some of which
may be outside of the known tag set. Hence, in addition to not having training
data for some labels - as is the case in zero-shot classification - models need
to invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model
for OXMC that generates the set of labels as a flat sequence and is trained
using a novel loss independent of predicted label order. We show the efficacy
of the approach, experimenting with popular XMC datasets for which GROOV is
able to predict meaningful labels outside the given vocabulary while performing
on par with state-of-the-art solutions for known labels."
The Quest for a Common Model of the Intelligent Decision Maker,0.373907,"The premise of the Multi-disciplinary Conference on Reinforcement Learning
and Decision Making is that multiple disciplines share an interest in
goal-directed decision making over time. The idea of this paper is to sharpen
and deepen this premise by proposing a perspective on the decision maker that
is substantive and widely held across psychology, artificial intelligence,
economics, control theory, and neuroscience, which I call the ""common model of
the intelligent agent"". The common model does not include anything specific to
any organism, world, or application domain. The common model does include
aspects of the decision maker's interaction with its world (there must be input
and output, and a goal) and internal components of the decision maker (for
perception, decision-making, internal evaluation, and a world model). I
identify these aspects and components, note that they are given different names
in different disciplines but refer essentially to the same ideas, and discuss
the challenges and benefits of devising a neutral terminology that can be used
across disciplines. It is time to recognize and build on the convergence of
multiple diverse disciplines on a substantive common model of the intelligent
agent."
Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation,0.437349,"Summarization of legal case judgement documents is a challenging problem in
Legal NLP. However, not much analyses exist on how different families of
summarization models (e.g., extractive vs. abstractive) perform when applied to
legal case documents. This question is particularly important since many recent
transformer-based abstractive summarization models have restrictions on the
number of input tokens, and legal documents are known to be very long. Also, it
is an open question on how best to evaluate legal case document summarization
systems. In this paper, we carry out extensive experiments with several
extractive and abstractive summarization methods (both supervised and
unsupervised) over three legal summarization datasets that we have developed.
Our analyses, that includes evaluation by law practitioners, lead to several
interesting insights on legal summarization in specific and long document
summarization in general."
Self-Supervised Leaf Segmentation under Complex Lighting Conditions,0.342774,"As an essential prerequisite task in image-based plant phenotyping, leaf
segmentation has garnered increasing attention in recent years. While
self-supervised learning is emerging as an effective alternative to various
computer vision tasks, its adaptation for image-based plant phenotyping remains
rather unexplored. In this work, we present a self-supervised leaf segmentation
framework consisting of a self-supervised semantic segmentation model, a
color-based leaf segmentation algorithm, and a self-supervised color correction
model. The self-supervised semantic segmentation model groups the semantically
similar pixels by iteratively referring to the self-contained information,
allowing the pixels of the same semantic object to be jointly considered by the
color-based leaf segmentation algorithm for identifying the leaf regions.
Additionally, we propose to use a self-supervised color correction model for
images taken under complex illumination conditions. Experimental results on
datasets of different plant species demonstrate the potential of the proposed
self-supervised framework in achieving effective and generalizable leaf
segmentation."
Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,0.381437,"While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method."
Lagrangian Manifold Monte Carlo on Monge Patches,0.349657,"The efficiency of Markov Chain Monte Carlo (MCMC) depends on how the
underlying geometry of the problem is taken into account. For distributions
with strongly varying curvature, Riemannian metrics help in efficient
exploration of the target distribution. Unfortunately, they have significant
computational overhead due to e.g. repeated inversion of the metric tensor, and
current geometric MCMC methods using the Fisher information matrix to induce
the manifold are in practice slow. We propose a new alternative Riemannian
metric for MCMC, by embedding the target distribution into a higher-dimensional
Euclidean space as a Monge patch and using the induced metric determined by
direct geometric reasoning. Our metric only requires first-order gradient
information and has fast inverse and determinants, and allows reducing the
computational complexity of individual iterations from cubic to quadratic in
the problem dimensionality. We demonstrate how Lagrangian Monte Carlo in this
metric efficiently explores the target distributions."
Evaluating Long-Term Memory in 3D Mazes,0.346024,"Intelligent agents need to remember salient information to reason in
partially-observed environments. For example, agents with a first-person view
should remember the positions of relevant objects even if they go out of view.
Similarly, to effectively navigate through rooms agents need to remember the
floor plan of how rooms are connected. However, most benchmark tasks in
reinforcement learning do not test long-term memory in agents, slowing down
progress in this important research direction. In this paper, we introduce the
Memory Maze, a 3D domain of randomized mazes specifically designed for
evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze
measures long-term memory separate from confounding agent abilities and
requires the agent to localize itself by integrating information over time.
With Memory Maze, we propose an online reinforcement learning benchmark, a
diverse offline dataset, and an offline probing evaluation. Recording a human
player establishes a strong baseline and verifies the need to build up and
retain memories, which is reflected in their gradually increasing rewards
within each episode. We find that current algorithms benefit from training with
truncated backpropagation through time and succeed on small mazes, but fall
short of human performance on the large mazes, leaving room for future
algorithmic designs to be evaluated on the Memory Maze."
SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,0.439925,"Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), similar efforts for discrete domains such as text have yet to
match the performance of autoregressive language models. In this work, we
present SSD-LM -- a diffusion-based language model with two key design choices.
First, SSD-LM is semi-autoregressive, iteratively generating blocks of text,
allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control using
off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on
unconstrained text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 models across standard quality and
diversity metrics, while vastly outperforming diffusion-based baselines. On
controlled text generation, SSD-LM also outperforms competitive baselines, with
an extra advantage in modularity."
Self-supervised AutoFlow,0.431763,"Recently, AutoFlow has shown promising results on learning a training set for
optical flow, but requires ground truth labels in the target domain to compute
its search metric. Observing a strong correlation between the ground truth
search metric and self-supervised losses, we introduce self-supervised AutoFlow
to handle real-world videos without ground truth labels. Using self-supervised
loss as the search metric, our self-supervised AutoFlow performs on par with
AutoFlow on Sintel and KITTI where ground truth is available, and performs
better on the real-world DAVIS dataset. We further explore using
self-supervised AutoFlow in the (semi-)supervised setting and obtain
competitive results against the state of the art."
Unsupervised Face Morphing Attack Detection via Self-paced Anomaly Detection,0.366079,"The supervised-learning-based morphing attack detection (MAD) solutions
achieve outstanding success in dealing with attacks from known morphing
techniques and known data sources. However, given variations in the morphing
attacks, the performance of supervised MAD solutions drops significantly due to
the insufficient diversity and quantity of the existing MAD datasets. To
address this concern, we propose a completely unsupervised MAD solution via
self-paced anomaly detection (SPL-MAD) by leveraging the existing large-scale
face recognition (FR) datasets and the unsupervised nature of convolutional
autoencoders. Using general FR datasets that might contain unintentionally and
unlabeled manipulated samples to train an autoencoder can lead to a diverse
reconstruction behavior of attack and bona fide samples. We analyze this
behavior empirically to provide a solid theoretical ground for designing our
unsupervised MAD solution. This also results in proposing to integrate our
adapted modified self-paced learning paradigm to enhance the reconstruction
error separability between the bona fide and attack samples in a completely
unsupervised manner. Our experimental results on a diverse set of MAD
evaluation datasets show that the proposed unsupervised SPL-MAD solution
outperforms the overall performance of a wide range of supervised MAD solutions
and provides higher generalizability on unknown attacks."
CITRIS: Causal Identifiability from Temporal Intervened Sequences,0.400436,"Understanding the latent causal factors of a dynamical system from visual
observations is considered a crucial step towards agents reasoning in complex
environments. In this paper, we propose CITRIS, a variational autoencoder
framework that learns causal representations from temporal sequences of images
in which underlying causal factors have possibly been intervened upon. In
contrast to the recent literature, CITRIS exploits temporality and observing
intervention targets to identify scalar and multidimensional causal factors,
such as 3D rotation angles. Furthermore, by introducing a normalizing flow,
CITRIS can be easily extended to leverage and disentangle representations
obtained by already pretrained autoencoders. Extending previous results on
scalar causal factors, we prove identifiability in a more general setting, in
which only some components of a causal factor are affected by interventions. In
experiments on 3D rendered image sequences, CITRIS outperforms previous methods
on recovering the underlying causal variables. Moreover, using pretrained
autoencoders, CITRIS can even generalize to unseen instantiations of causal
factors, opening future research areas in sim-to-real generalization for causal
representation learning."
"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends",0.408797,"This paper surveys vision-language pre-training (VLP) methods for multimodal
intelligence that have been developed in the last few years. We group these
approaches into three categories: ($i$) VLP for image-text tasks, such as image
captioning, image-text retrieval, visual question answering, and visual
grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image
classification, object detection, and segmentation; and ($iii$) VLP for
video-text tasks, such as video captioning, video-text retrieval, and video
question answering. For each category, we present a comprehensive review of
state-of-the-art methods, and discuss the progress that has been made and
challenges still being faced, using specific systems and models as case
studies. In addition, for each category, we discuss advanced topics being
actively explored in the research community, such as big foundation models,
unified modeling, in-context few-shot learning, knowledge, robustness, and
computer vision in the wild, to name a few."
Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds,0.384472,"Discovering latent topics from text corpora has been studied for decades.
Many existing topic models adopt a fully unsupervised setting, and their
discovered topics may not cater to users' particular interests due to their
inability of leveraging user guidance. Although there exist seed-guided topic
discovery approaches that leverage user-provided seeds to discover
topic-representative terms, they are less concerned with two factors: (1) the
existence of out-of-vocabulary seeds and (2) the power of pre-trained language
models (PLMs). In this paper, we generalize the task of seed-guided topic
discovery to allow out-of-vocabulary seeds. We propose a novel framework, named
SeeTopic, wherein the general knowledge of PLMs and the local semantics learned
from the input corpus can mutually benefit each other. Experiments on three
real datasets from different domains demonstrate the effectiveness of SeeTopic
in terms of topic coherence, accuracy, and diversity."
McQueen: a Benchmark for Multimodal Conversational Query Rewrite,0.417651,"The task of query rewrite aims to convert an in-context query to its
fully-specified version where ellipsis and coreference are completed and
referred-back according to the history context. Although much progress has been
made, less efforts have been paid to real scenario conversations that involve
drawing information from more than one modalities. In this paper, we propose
the task of multimodal conversational query rewrite (McQR), which performs
query rewrite under the multimodal visual conversation setting. We collect a
large-scale dataset named McQueen based on manual annotation, which contains
15k visual conversations and over 80k queries where each one is associated with
a fully-specified rewrite version. In addition, for entities appearing in the
rewrite, we provide the corresponding image box annotation. We then use the
McQueen dataset to benchmark a state-of-the-art method for effectively tackling
the McQR task, which is based on a multimodal pre-trained model with pointer
generator. Extensive experiments are performed to demonstrate the effectiveness
of our model on this task\footnote{The dataset and code of this paper are both
available in \url{https://github.com/yfyuan01/MQR}"
Distillation-Resistant Watermarking for Model Protection in NLP,0.422039,"How can we protect the intellectual property of trained NLP models? Modern
NLP models are prone to stealing by querying and distilling from their publicly
exposed APIs. However, existing protection methods such as watermarking only
work for images but are not applicable to text. We propose
Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP
models from being stolen via distillation. DRW protects a model by injecting
watermarks into the victim's prediction probability corresponding to a secret
key and is able to detect such a key by probing a suspect model. We prove that
a protected model still retains the original accuracy within a certain bound.
We evaluate DRW on a diverse set of NLP tasks including text classification,
part-of-speech tagging, and named entity recognition. Experiments show that DRW
protects the original model and detects stealing suspects at 100% mean average
precision for all four tasks while the prior method fails on two."
Attribution-aware Weight Transfer: A Warm-Start Initialization for Class-Incremental Semantic Segmentation,0.375914,"In class-incremental semantic segmentation (CISS), deep learning
architectures suffer from the critical problems of catastrophic forgetting and
semantic background shift. Although recent works focused on these issues,
existing classifier initialization methods do not address the background shift
problem and assign the same initialization weights to both background and new
foreground class classifiers. We propose to address the background shift with a
novel classifier initialization method which employs gradient-based attribution
to identify the most relevant weights for new classes from the classifier's
weights for the previous background and transfers these weights to the new
classifier. This warm-start weight initialization provides a general solution
applicable to several CISS methods. Furthermore, it accelerates learning of new
classes while mitigating forgetting. Our experiments demonstrate significant
improvement in mIoU compared to the state-of-the-art CISS methods on the
Pascal-VOC 2012, ADE20K and Cityscapes datasets."
Action Conditioned Tactile Prediction: a case study on slip prediction,0.36801,"Tactile predictive models can be useful across several robotic manipulation
tasks, e.g. robotic pushing, robotic grasping, slip avoidance, and in-hand
manipulation. However, available tactile prediction models are mostly studied
for image-based tactile sensors and there is no comparison study indicating the
best performing models. In this paper, we presented two novel data-driven
action-conditioned models for predicting tactile signals during real-world
physical robot interaction tasks (1) action condition tactile prediction and
(2) action conditioned tactile-video prediction models. We use a magnetic-based
tactile sensor that is challenging to analyse and test state-of-the-art
predictive models and the only existing bespoke tactile prediction model. We
compare the performance of these models with those of our proposed models. We
perform the comparison study using our novel tactile enabled dataset containing
51,000 tactile frames of a real-world robotic manipulation task with 11
flat-surfaced household objects. Our experimental results demonstrate the
superiority of our proposed tactile prediction models in terms of qualitative,
quantitative and slip prediction scores."
Weakly Supervised 3D Point Cloud Segmentation via Multi-Prototype Learning,0.365344,"Addressing the annotation challenge in 3D Point Cloud segmentation has
inspired research into weakly supervised learning. Existing approaches mainly
focus on exploiting manifold and pseudo-labeling to make use of large unlabeled
data points. A fundamental challenge here lies in the large intra-class
variations of local geometric structure, resulting in subclasses within a
semantic class. In this work, we leverage this intuition and opt for
maintaining an individual classifier for each subclass. Technically, we design
a multi-prototype classifier, each prototype serves as the classifier weights
for one subclass. To enable effective updating of multi-prototype classifier
weights, we propose two constraints respectively for updating the prototypes
w.r.t. all point features and for encouraging the learning of diverse
prototypes. Experiments on weakly supervised 3D point cloud segmentation tasks
validate the efficacy of proposed method in particular at low-label regime. Our
hypothesis is also verified given the consistent discovery of semantic
subclasses at no cost of additional annotations."
Motion Transformer with Global Intention Localization and Local Movement Refinement,0.435482,"Predicting multimodal future behavior of traffic participants is essential
for robotic vehicles to make safe decisions. Existing works explore to directly
predict future trajectories based on latent features or utilize dense goal
candidates to identify agent's destinations, where the former strategy
converges slowly since all motion modes are derived from the same feature while
the latter strategy has efficiency issue since its performance highly relies on
the density of goal candidates. In this paper, we propose Motion TRansformer
(MTR) framework that models motion prediction as the joint optimization of
global intention localization and local movement refinement. Instead of using
goal candidates, MTR incorporates spatial intention priors by adopting a small
set of learnable motion query pairs. Each motion query pair takes charge of
trajectory prediction and refinement for a specific motion mode, which
stabilizes the training process and facilitates better multimodal predictions.
Experiments show that MTR achieves state-of-the-art performance on both the
marginal and joint motion prediction challenges, ranking 1st on the
leaderboards of Waymo Open Motion Dataset. The source code is available at
https://github.com/sshaoshuai/MTR."
Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification,0.348833,"Self-supervised learning (SSL) has drawn increasing attention in
histopathological image analysis in recent years. Compared to contrastive
learning which is troubled with the false negative problem, i.e., semantically
similar images are selected as negative samples, masked autoencoders (MAE)
building SSL from a generative paradigm is probably a more appropriate
pre-training. In this paper, we introduce MAE and verify the effect of visible
patches for histopathological image understanding. Moreover, a novel SD-MAE
model is proposed to enable a self-distillation augmented MAE. Besides the
reconstruction loss on masked image patches, SD-MAE further imposes the
self-distillation loss on visible patches to enhance the representational
capacity of the encoder located shallow layer. We apply SD-MAE to
histopathological image classification, cell segmentation and object detection.
Experiments demonstrate that SD-MAE shows highly competitive performance when
compared with other SSL methods in these tasks."
Multimodal Hate Speech Detection from Bengali Memes and Texts,0.367947,"Numerous machine learning (ML) and deep learning (DL)-based approaches have
been proposed to utilize textual data from social media for anti-social
behavior analysis like cyberbullying, fake news detection, and identification
of hate speech mainly for highly-resourced languages such as English. However,
despite having a lot of diversity and millions of native speakers, some
languages like Bengali are under-resourced, which is due to a lack of
computational resources for natural language processing (NLP). Similar to other
languages, Bengali social media contents also include images along with texts
(e.g., multimodal memes are posted by embedding short texts into images on
Facebook). Therefore, only the textual data is not enough to judge them since
images might give extra context to make a proper judgement. This paper is about
hate speech detection from multimodal Bengali memes and texts. We prepared the
only multimodal hate speech dataset for-a-kind of problem for Bengali, which we
use to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM
with word embeddings, ConvNets + pre-trained language models, e.g., monolingual
Bangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly
analyze textual and visual information for hate speech detection. Conv-LSTM and
XLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and
0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1
scores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +
DenseNet-161 performed the best, yielding an F1 score of 0.83. Our study
suggests that text modality is most useful for hate speech detection, while
memes are moderately useful."
Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs,0.41179,"Question answering over temporal knowledge graphs (KGs) efficiently uses
facts contained in a temporal KG, which records entity relations and when they
occur in time, to answer natural language questions (e.g., ""Who was the
president of the US before Obama?""). These questions often involve three
time-related challenges that previous work fail to adequately address: 1)
questions often do not specify exact timestamps of interest (e.g., ""Obama""
instead of 2000); 2) subtle lexical differences in time relations (e.g.,
""before"" vs ""after""); 3) off-the-shelf temporal KG embeddings that previous
work builds on ignore the temporal order of timestamps, which is crucial for
answering temporal-order related questions. In this paper, we propose a
time-sensitive question answering (TSQA) framework to tackle these problems.
TSQA features a timestamp estimation module to infer the unwritten timestamp
from the question. We also employ a time-sensitive KG encoder to inject
ordering information into the temporal KG embeddings that TSQA is based on.
With the help of techniques to reduce the search space for potential answers,
TSQA significantly outperforms the previous state of the art on a new benchmark
for question answering over temporal KGs, especially achieving a 32% (absolute)
error reduction on complex questions that require multiple steps of reasoning
over facts in the temporal KG."
OPD: Single-view 3D Openable Part Detection,0.386091,"We address the task of predicting what parts of an object can open and how
they move when they do so. The input is a single image of an object, and as
output we detect what parts of the object can open, and the motion parameters
describing the articulation of each openable part. To tackle this task, we
create two datasets of 3D objects: OPDSynth based on existing synthetic
objects, and OPDReal based on RGBD reconstructions of real objects. We then
design OPDRCNN, a neural architecture that detects openable parts and predicts
their motion parameters. Our experiments show that this is a challenging task
especially when considering generalization across object categories, and the
limited amount of information in a single image. Our architecture outperforms
baselines and prior work especially for RGB image inputs. Short video summary
at https://www.youtube.com/watch?v=P85iCaD0rfc"
Synthetic Disinformation Attacks on Automated Fact Verification Systems,0.369925,"Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
  In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers."
ELSR: Extreme Low-Power Super Resolution Network For Mobile Devices,0.365264,"With the popularity of mobile devices, e.g., smartphone and wearable devices,
lighter and faster model is crucial for the application of video super
resolution. However, most previous lightweight models tend to concentrate on
reducing lantency of model inference on desktop GPU, which may be not energy
efficient in current mobile devices. In this paper, we proposed Extreme
Low-Power Super Resolution (ELSR) network which only consumes a small amount of
energy in mobile devices. Pretraining and finetuning methods are applied to
boost the performance of the extremely tiny model. Extensive experiments show
that our method achieves a excellent balance between restoration quality and
power consumption. Finally, we achieve a competitive score of 90.9 with PSNR
27.34 dB and power 0.09 W/30FPS on the target MediaTek Dimensity 9000
plantform, ranking 1st place in the Mobile AI & AIM 2022 Real-Time Video
Super-Resolution Challenge."
Nonparametric Masked Language Modeling,0.351153,"Existing language models (LMs) predict tokens with a softmax over a finite
vocabulary, which can make it difficult to predict rare tokens or phrases. We
introduce NPM, the first nonparametric masked language model that replaces this
softmax with a nonparametric distribution over every phrase in a reference
corpus. NPM fills in the [MASK] solely from retrieving a token from a text
corpus. We show that NPM can be efficiently trained with a contrastive
objective and an in-batch approximation to full corpus retrieval. Zero-shot
evaluation on 16 tasks including classification, fact probing and question
answering demonstrates that NPM outperforms significantly larger parametric
models, either with or without a retrieve-and-generate approach. It is
particularly better at dealing with rare patterns (word senses or facts) and
predicting rare or nearly unseen words (e.g., non-Latin script). We release the
model and code at github.com/facebookresearch/NPM."
Creativity in translation: machine translation as a constraint for literary texts,0.374209,"This article presents the results of a study involving the translation of a
short story by Kurt Vonnegut from English to Catalan and Dutch using three
modalities: machine-translation (MT), post-editing (PE) and translation without
aid (HT). Our aim is to explore creativity, understood to involve novelty and
acceptability, from a quantitative perspective. The results show that HT has
the highest creativity score, followed by PE, and lastly, MT, and this is
unanimous from all reviewers. A neural MT system trained on literary data does
not currently have the necessary capabilities for a creative translation; it
renders literal solutions to translation problems. More importantly, using MT
to post-edit raw output constrains the creativity of translators, resulting in
a poorer translation often not fit for publication, according to experts."
TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval,0.336976,"Text-Video retrieval is a task of great practical value and has received
increasing attention, among which learning spatial-temporal video
representation is one of the research hotspots. The video encoders in the
state-of-the-art video retrieval models usually directly adopt the pre-trained
vision backbones with the network structure fixed, they therefore can not be
further improved to produce the fine-grained spatial-temporal video
representation. In this paper, we propose Token Shift and Selection Network
(TS2-Net), a novel token shift and selection transformer architecture, which
dynamically adjusts the token sequence and selects informative tokens in both
temporal and spatial dimensions from input video samples. The token shift
module temporally shifts the whole token features back-and-forth across
adjacent frames, to preserve the complete token representation and capture
subtle movements. Then the token selection module selects tokens that
contribute most to local spatial semantics. Based on thorough experiments, the
proposed TS2-Net achieves state-of-the-art performance on major text-video
retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC,
ActivityNet, and DiDeMo."
Generating Natural Language Proofs with Verifier-Guided Search,0.378094,"Reasoning over natural language is a challenging problem in NLP. In this
work, we focus on proof generation: Given a hypothesis and a set of supporting
facts, the model generates a proof tree indicating how to derive the hypothesis
from supporting facts. Compared to generating the entire proof in one shot,
stepwise generation can better exploit the compositionality and generalize to
longer proofs but has achieved limited success on real-world data. Existing
stepwise methods struggle to generate proof steps that are both logically valid
and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps
given the hypothesis. In this paper, we present a novel stepwise method,
NLProofS (Natural Language Proof Search), which learns to generate relevant
steps conditioning on the hypothesis. At the core of our approach, we train an
independent verifier to check the validity of the proof steps to prevent
hallucination. Instead of generating steps greedily, we search for proofs
maximizing a global proof score judged by the verifier. NLProofS achieves
state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it
improves the correctness of predicted proofs from 27.7% to 33.3% in the
distractor setting of EntailmentBank, demonstrating the effectiveness of
NLProofS in generating challenging human-authored proofs."
Layer or Representation Space: What makes BERT-based Evaluation Metrics Robust?,0.356865,"The evaluation of recent embedding-based evaluation metrics for text
generation is primarily based on measuring their correlation with human
evaluations on standard benchmarks. However, these benchmarks are mostly from
similar domains to those used for pretraining word embeddings. This raises
concerns about the (lack of) generalization of embedding-based metrics to new
and noisy domains that contain a different vocabulary than the pretraining
data. In this paper, we examine the robustness of BERTScore, one of the most
popular embedding-based metrics for text generation. We show that (a) an
embedding-based metric that has the highest correlation with human evaluations
on a standard benchmark can have the lowest correlation if the amount of input
noise or unknown tokens increases, (b) taking embeddings from the first layer
of pretrained models improves the robustness of all metrics, and (c) the
highest robustness is achieved when using character-level embeddings, instead
of token-based embeddings, from the first layer of the pretrained model."
Learning a General Clause-to-Clause Relationships for Enhancing Emotion-Cause Pair Extraction,0.39477,"Emotion-cause pair extraction (ECPE) is an emerging task aiming to extract
potential pairs of emotions and corresponding causes from documents. Previous
approaches have focused on modeling the pair-to-pair relationship and achieved
promising results. However, the clause-to-clause relationship, which
fundamentally symbolizes the underlying structure of a document, has still been
in its research infancy. In this paper, we define a novel clause-to-clause
relationship. To learn it applicably, we propose a general clause-level
encoding model named EA-GAT comprising E-GAT and Activation Sort. E-GAT is
designed to aggregate information from different types of clauses; Activation
Sort leverages the individual emotion/cause prediction and the sort-based
mapping to propel the clause to a more favorable representation. Since EA-GAT
is a clause-level encoding model, it can be broadly integrated with any
previous approach. Experimental results show that our approach has a
significant advantage over all current approaches on the Chinese and English
benchmark corpus, with an average of $2.1\%$ and $1.03\%$."
Reducing the Vision and Language Bias for Temporal Sentence Grounding,0.349208,"Temporal sentence grounding (TSG) is an important yet challenging task in
multimedia information retrieval. Although previous TSG methods have achieved
decent performance, they tend to capture the selection biases of frequently
appeared video-query pairs in the dataset rather than present robust multimodal
reasoning abilities, especially for the rarely appeared pairs. In this paper,
we study the above issue of selection biases and accordingly propose a
Debiasing-TSG (D-TSG) model to filter and remove the negative biases in both
vision and language modalities for enhancing the model generalization ability.
Specifically, we propose to alleviate the issue from two perspectives: 1)
Feature distillation. We built a multi-modal debiasing branch to firstly
capture the vision and language biases, and then apply a bias identification
module to explicitly recognize the true negative biases and remove them from
the benign multi-modal representations. 2) Contrastive sample generation. We
construct two types of negative samples to enforce the model to accurately
learn the aligned multi-modal semantics and make complete semantic reasoning.
We apply the proposed model to both commonly and rarely appeared TSG cases, and
demonstrate its effectiveness by achieving the state-of-the-art performance on
three benchmark datasets (ActivityNet Caption, TACoS, and Charades-STA)."
Towards automatic generation of Piping and Instrumentation Diagrams (P&IDs) with Artificial Intelligence,0.367751,"Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step
during the development of chemical processes. Currently, this is a tedious,
manual, and time-consuming task. We propose a novel, completely data-driven
method for the prediction of control structures. Our methodology is inspired by
end-to-end transformer-based human language translation models. We cast the
control structure prediction as a translation task where Process Flow Diagrams
(PFDs) are translated to P&IDs. To use established transformer-based language
translation models, we represent the P&IDs and PFDs as strings using our
recently proposed SFILES 2.0 notation. Model training is performed in a
transfer learning approach. Firstly, we pre-train our model using generated
P&IDs to learn the grammatical structure of the process diagrams. Thereafter,
the model is fine-tuned leveraging transfer learning on real P&IDs. The model
achieved a top-5 accuracy of 74.8% on 10,000 generated P&IDs and 89.2% on
100,000 generated P&IDs. These promising results show great potential for
AI-assisted process engineering. The tests on a dataset of 312 real P&IDs
indicate the need of a larger P&IDs dataset for industry applications."
HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition,0.409595,"Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)
and wav2vec 2.0, has brought significant improvements in automatic speech
recognition (ASR). However, these models usually require an expensive
computational cost to achieve outstanding performance, slowing down the
inference speed. To improve the model efficiency, we propose an early exit
scheme for ASR, namely HuBERT-EE, that allows the model to stop the inference
dynamically. In HuBERT-EE, multiple early exit branches are added at the
intermediate layers, and each branch is used to decide whether a prediction can
be exited early. Experimental results on the LibriSpeech dataset show that
HuBERT-EE can accelerate the inference of a large-scale HuBERT model while
simultaneously balancing the trade-off between the word error rate (WER)
performance and the latency."
What Do We Maximize in Self-Supervised Learning?,0.411248,"In this paper, we examine self-supervised learning methods, particularly
VICReg, to provide an information-theoretical understanding of their
construction. As a first step, we demonstrate how information-theoretic
quantities can be obtained for a deterministic network, offering a possible
alternative to prior work that relies on stochastic models. This enables us to
demonstrate how VICReg can be (re)discovered from first principles and its
assumptions about data distribution. Furthermore, we empirically demonstrate
the validity of our assumptions, confirming our novel understanding of VICReg.
Finally, we believe that the derivation and insights we obtain can be
generalized to many other SSL methods, opening new avenues for theoretical and
practical understanding of SSL and transfer learning."
Splicing ViT Features for Semantic Appearance Transfer,0.405992,"We present a method for semantically transferring the visual appearance of
one natural image to another. Specifically, our goal is to generate an image in
which objects in a source structure image are ""painted"" with the visual
appearance of their semantically related objects in a target appearance image.
Our method works by training a generator given only a single
structure/appearance image pair as input. To integrate semantic information
into our framework - a pivotal component in tackling this task - our key idea
is to leverage a pre-trained and fixed Vision Transformer (ViT) model which
serves as an external semantic prior. Specifically, we derive novel
representations of structure and appearance extracted from deep ViT features,
untwisting them from the learned self-attention modules. We then establish an
objective function that splices the desired structure and appearance
representations, interweaving them together in the space of ViT features. Our
framework, which we term ""Splice"", does not involve adversarial training, nor
does it require any additional input information such as semantic segmentation
or correspondences, and can generate high-resolution results, e.g., work in HD.
We demonstrate high quality results on a variety of in-the-wild image pairs,
under significant variations in the number of objects, their pose and
appearance."
DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation,0.377627,"Deep learning approaches achieve prominent success in 3D semantic
segmentation. However, collecting densely annotated real-world 3D datasets is
extremely time-consuming and expensive. Training models on synthetic data and
generalizing on real-world scenarios becomes an appealing alternative, but
unfortunately suffers from notorious domain shifts. In this work, we propose a
Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and
context gaps caused by different sensing mechanisms and layout placements
across domains. Our DODA encompasses virtual scan simulation to imitate
real-world point cloud patterns and tail-aware cuboid mixing to alleviate the
interior context gap with a cuboid-based intermediate domain. The first
unsupervised sim-to-real adaptation benchmark on 3D indoor semantic
segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular
Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA
approaches by over 13% on both 3D-FRONT -> ScanNet and 3D-FRONT -> S3DIS. Code
is available at https://github.com/CVMI-Lab/DODA."
Instance-Specific Feature Propagation for Referring Segmentation,0.389129,"Referring segmentation aims to generate a segmentation mask for the target
instance indicated by a natural language expression. There are typically two
kinds of existing methods: one-stage methods that directly perform segmentation
on the fused vision and language features; and two-stage methods that first
utilize an instance segmentation model for instance proposal and then select
one of these instances via matching them with language features. In this work,
we propose a novel framework that simultaneously detects the target-of-interest
via feature propagation and generates a fine-grained segmentation mask. In our
framework, each instance is represented by an Instance-Specific Feature (ISF),
and the target-of-referring is identified by exchanging information among all
ISFs using our proposed Feature Propagation Module (FPM). Our instance-aware
approach learns the relationship among all objects, which helps to better
locate the target-of-interest than one-stage methods. Comparing to two-stage
methods, our approach collaboratively and interactively utilizes both vision
and language information for synchronous identification and segmentation. In
the experimental tests, our method outperforms previous state-of-the-art
methods on all three RefCOCO series datasets."
Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,0.355005,"We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
Covariance Matrix Adaptation MAP-Annealing,0.353541,"Single-objective optimization algorithms search for the single
highest-quality solution with respect to an objective. Quality diversity (QD)
optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites
(CMA-ME), search for a collection of solutions that are both high-quality with
respect to an objective and diverse with respect to specified measure
functions. However, CMA-ME suffers from three major limitations highlighted by
the QD community: prematurely abandoning the objective in favor of exploration,
struggling to explore flat objectives, and having poor performance for
low-resolution archives. We propose a new quality diversity algorithm,
Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), that addresses all three
limitations. We provide theoretical justifications for the new algorithm with
respect to each limitation. Our theory informs our experiments, which support
the theory and show that CMA-MAE achieves state-of-the-art performance and
robustness."
Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery,0.416829,"Discovering out-of-domain (OOD) intent is important for developing new skills
in task-oriented dialogue systems. The key challenges lie in how to transfer
prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD
representations and cluster assignments. Previous methods suffer from in-domain
overfitting problem, and there is a natural gap between representation learning
and clustering objectives. In this paper, we propose a unified K-nearest
neighbor contrastive learning framework to discover OOD intents. Specifically,
for IND pre-training stage, we propose a KCL objective to learn inter-class
discriminative features, while maintaining intra-class diversity, which
alleviates the in-domain overfitting problem. For OOD clustering stage, we
propose a KCC method to form compact clusters by mining true hard negative
samples, which bridges the gap between clustering and representation learning.
Extensive experiments on three benchmark datasets show that our method achieves
substantial improvements over the state-of-the-art methods."
A general-purpose material property data extraction pipeline from large polymer corpora using Natural Language Processing,0.391596,"The ever-increasing number of materials science articles makes it hard to
infer chemistry-structure-property relations from published literature. We used
natural language processing (NLP) methods to automatically extract material
property data from the abstracts of polymer literature. As a component of our
pipeline, we trained MaterialsBERT, a language model, using 2.4 million
materials science abstracts, which outperforms other baseline models in three
out of five named entity recognition datasets when used as the encoder for
text. Using this pipeline, we obtained ~300,000 material property records from
~130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse
range of applications such as fuel cells, supercapacitors, and polymer solar
cells to recover non-trivial insights. The data extracted through our pipeline
is made available through a web platform at https://polymerscholar.org which
can be used to locate material property data recorded in abstracts
conveniently. This work demonstrates the feasibility of an automatic pipeline
that starts from published literature and ends with a complete set of extracted
material property information."
Receding Horizon Inverse Reinforcement Learning,0.375268,"Inverse reinforcement learning (IRL) seeks to infer a cost function that
explains the underlying goals and preferences of expert demonstrations. This
paper presents receding horizon inverse reinforcement learning (RHIRL), a new
IRL algorithm for high-dimensional, noisy, continuous systems with black-box
dynamic models. RHIRL addresses two key challenges of IRL: scalability and
robustness. To handle high-dimensional continuous systems, RHIRL matches the
induced optimal trajectories with expert demonstrations locally in a receding
horizon manner and 'stitches' together the local solutions to learn the cost;
it thereby avoids the 'curse of dimensionality'. This contrasts sharply with
earlier algorithms that match with expert demonstrations globally over the
entire high-dimensional state space. To be robust against imperfect expert
demonstrations and control noise, RHIRL learns a state-dependent cost function
'disentangled' from system dynamics under mild conditions. Experiments on
benchmark tasks show that RHIRL outperforms several leading IRL algorithms in
most instances. We also prove that the cumulative error of RHIRL grows linearly
with the task duration."
On the Effectiveness of Compact Biomedical Transformers,0.368471,"Language models pre-trained on biomedical corpora, such as BioBERT, have
recently shown promising results on downstream biomedical tasks. Many existing
pre-trained models, on the other hand, are resource-intensive and
computationally heavy owing to factors such as embedding size, hidden
dimension, and number of layers. The natural language processing (NLP)
community has developed numerous strategies to compress these models utilising
techniques such as pruning, quantisation, and knowledge distillation, resulting
in models that are considerably faster, smaller, and subsequently easier to use
in practice. By the same token, in this paper we introduce six lightweight
models, namely, BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT,
TinyBioBERT, and CompactBioBERT which are obtained either by knowledge
distillation from a biomedical teacher or continual learning on the Pubmed
dataset via the Masked Language Modelling (MLM) objective. We evaluate all of
our models on three biomedical tasks and compare them with BioBERT-v1.1 to
create efficient lightweight models that perform on par with their larger
counterparts. All the models will be publicly available on our Huggingface
profile at https://huggingface.co/nlpie and the codes used to run the
experiments will be available at
https://github.com/nlpie-research/Compact-Biomedical-Transformers."
"YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception",0.388144,"Over the last decade, multi-tasking learning approaches have achieved
promising results in solving panoptic driving perception problems, providing
both high-precision and high-efficiency performance. It has become a popular
paradigm when designing networks for real-time practical autonomous driving
system, where computation resources are limited. This paper proposed an
effective and efficient multi-task learning network to simultaneously perform
the task of traffic object detection, drivable road area segmentation and lane
detection. Our model achieved the new state-of-the-art (SOTA) performance in
terms of accuracy and speed on the challenging BDD100K dataset. Especially, the
inference time is reduced by half compared to the previous SOTA model. Code
will be released in the near future."
Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation,0.352096,"We present Referee, a novel framework for sentence summarization that can be
trained reference-free (i.e., requiring no gold summaries for supervision),
while allowing direct control for compression ratio. Our work is the first to
demonstrate that reference-free, controlled sentence summarization is feasible
via the conceptual framework of Symbolic Knowledge Distillation (West et al.,
2022), where latent knowledge in pre-trained language models is distilled via
explicit examples sampled from the teacher models, further purified with three
types of filters: length, fidelity, and Information Bottleneck. Moreover, we
uniquely propose iterative distillation of knowledge, where student models from
the previous iteration of distillation serve as teacher models in the next
iteration. Starting off from a relatively modest set of GPT3-generated
summaries, we demonstrate how iterative knowledge distillation can lead to
considerably smaller, but better summarizers with sharper controllability. A
useful by-product of this iterative distillation process is a high-quality
dataset of sentence-summary pairs with varying degrees of compression ratios.
Empirical results demonstrate that the final student models vastly outperform
the much larger GPT3-Instruct model in terms of the controllability of
compression ratios, without compromising the quality of resulting
summarization."
"""This is Fake! Shared it by Mistake"": Assessing the Intent of Fake News Spreaders",0.390708,"Individuals can be misled by fake news and spread it unintentionally without
knowing it is false. This phenomenon has been frequently observed but has not
been investigated. Our aim in this work is to assess the intent of fake news
spreaders. To distinguish between intentional versus unintentional spreading,
we study the psychological explanations of unintentional spreading. With this
foundation, we then propose an influence graph, using which we assess the
intent of fake news spreaders. Our extensive experiments show that the assessed
intent can help significantly differentiate between intentional and
unintentional fake news spreaders. Furthermore, the estimated intent can
significantly improve the current techniques that detect fake news. To our best
knowledge, this is the first work to model individuals' intent in fake news
spreading."
Mitigating Artifacts in Real-World Video Super-Resolution Models,0.436746,"The recurrent structure is a prevalent framework for the task of video
super-resolution, which models the temporal dependency between frames via
hidden states. When applied to real-world scenarios with unknown and complex
degradations, hidden states tend to contain unpleasant artifacts and propagate
them to restored frames. In this circumstance, our analyses show that such
artifacts can be largely alleviated when the hidden state is replaced with a
cleaner counterpart. Based on the observations, we propose a Hidden State
Attention (HSA) module to mitigate artifacts in real-world video
super-resolution. Specifically, we first adopt various cheap filters to produce
a hidden state pool. For example, Gaussian blur filters are for smoothing
artifacts while sharpening filters are for enhancing details. To aggregate a
new hidden state that contains fewer artifacts from the hidden state pool, we
devise a Selective Cross Attention (SCA) module, in which the attention between
input features and each hidden state is calculated. Equipped with HSA, our
proposed method, namely FastRealVSR, is able to achieve 2x speedup while
obtaining better performance than Real-BasicVSR. Codes will be available at
https://github.com/TencentARC/FastRealVSR"
Knowledge Removal in Sampling-based Bayesian Inference,0.344163,"The right to be forgotten has been legislated in many countries, but its
enforcement in the AI industry would cause unbearable costs. When single data
deletion requests come, companies may need to delete the whole models learned
with massive resources. Existing works propose methods to remove knowledge
learned from data for explicitly parameterized models, which however are not
appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte
Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we
propose the first machine unlearning algorithm for MCMC. We first convert the
MCMC unlearning problem into an explicit optimization problem. Based on this
problem conversion, an {\it MCMC influence function} is designed to provably
characterize the learned knowledge from data, which then delivers the MCMC
unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not
compromise the generalizability of the MCMC models. Experiments on Gaussian
mixture models and Bayesian neural networks confirm the effectiveness of the
proposed algorithm. The code is available at
\url{https://github.com/fshp971/mcmc-unlearning}."
Walk this Way! Entity Walks and Property Walks for RDF2vec,0.349561,"RDF2vec is a knowledge graph embedding mechanism which first extracts
sequences from knowledge graphs by performing random walks, then feeds those
into the word embedding algorithm word2vec for computing vector representations
for entities. In this poster, we introduce two new flavors of walk extraction
coined e-walks and p-walks, which put an emphasis on the structure or the
neighborhood of an entity respectively, and thereby allow for creating
embeddings which focus on similarity or relatedness. By combining the walk
strategies with order-aware and classic RDF2vec, as well as CBOW and skip-gram
word2vec embeddings, we conduct a preliminary evaluation with a total of 12
RDF2vec variants."
Uncertainty-aware deep learning methods for robust diabetic retinopathy classification,0.407944,"Automatic classification of diabetic retinopathy from retinal images has been
widely studied using deep neural networks with impressive results. However,
there is a clinical need for estimation of the uncertainty in the
classifications, a shortcoming of modern neural networks. Recently, approximate
Bayesian deep learning methods have been proposed for the task but the studies
have only considered the binary referable/non-referable diabetic retinopathy
classification applied to benchmark datasets. We present novel results by
systematically investigating a clinical dataset and a clinically relevant
5-class classification scheme, in addition to benchmark datasets and the binary
classification scheme. Moreover, we derive a connection between uncertainty
measures and classifier risk, from which we develop a new uncertainty measure.
We observe that the previously proposed entropy-based uncertainty measure
generalizes to the clinical dataset on the binary classification scheme but not
on the 5-class scheme, whereas our new uncertainty measure generalizes to the
latter case."
Focal Length and Object Pose Estimation via Render and Compare,0.352617,"We introduce FocalPose, a neural render-and-compare method for jointly
estimating the camera-object 6D pose and camera focal length given a single RGB
input image depicting a known object. The contributions of this work are
twofold. First, we derive a focal length update rule that extends an existing
state-of-the-art render-and-compare 6D pose estimator to address the joint
estimation task. Second, we investigate several different loss functions for
jointly estimating the object pose and focal length. We find that a combination
of direct focal length regression with a reprojection loss disentangling the
contribution of translation, rotation, and focal length leads to improved
results. We show results on three challenging benchmark datasets that depict
known 3D models in uncontrolled settings. We demonstrate that our focal length
and 6D pose estimates have lower error than the existing state-of-the-art
methods."
Hyperbolic Vision Transformers: Combining Improvements in Metric Learning,0.370638,"Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric."
DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation,0.347625,"Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets."
CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds,0.418867,"We present a novel two-stage fully sparse convolutional 3D object detection
framework, named CAGroup3D. Our proposed method first generates some
high-quality 3D proposals by leveraging the class-aware local group strategy on
the object surface voxels with the same semantic predictions, which considers
semantic consistency and diverse locality abandoned in previous bottom-up
approaches. Then, to recover the features of missed voxels due to incorrect
voxel-wise segmentation, we build a fully sparse convolutional RoI pooling
module to directly aggregate fine-grained spatial information from backbone for
further proposal refinement. It is memory-and-computation efficient and can
better encode the geometry-specific features of each 3D proposal. Our model
achieves state-of-the-art 3D detection performance with remarkable gains of
+\textit{3.6\%} on ScanNet V2 and +\textit{2.6}\% on SUN RGB-D in term of
mAP@0.25. Code will be available at https://github.com/Haiyang-W/CAGroup3D."
Multi-Granularity Prediction for Scene Text Recognition,0.341716,"Scene text recognition (STR) has been an active research topic in computer
vision for years. To tackle this challenging problem, numerous innovative
methods have been successively proposed and incorporating linguistic knowledge
into STR models has recently become a prominent trend. In this work, we first
draw inspiration from the recent progress in Vision Transformer (ViT) to
construct a conceptually simple yet powerful vision STR model, which is built
upon ViT and outperforms previous state-of-the-art models for scene text
recognition, including both pure vision models and language-augmented methods.
To integrate linguistic knowledge, we further propose a Multi-Granularity
Prediction strategy to inject information from the language modality into the
model in an implicit way, i.e. , subword representations (BPE and WordPiece)
widely-used in NLP are introduced into the output space, in addition to the
conventional character level representation, while no independent language
model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push
the performance envelop of STR to an even higher level. Specifically, it
achieves an average recognition accuracy of 93.35% on standard benchmarks. Code
is available at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR."
Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations,0.394693,"Neural networks have achieved tremendous success in a large variety of
applications. However, their memory footprint and computational demand can
render them impractical in application settings with limited hardware or energy
resources. In this work, we propose a novel algorithm to find efficient
low-rank subnetworks. Remarkably, these subnetworks are determined and adapted
already during the training phase and the overall time and memory resources
required by both training and evaluating them are significantly reduced. The
main idea is to restrict the weight matrices to a low-rank manifold and to
update the low-rank factors rather than the full matrix during training. To
derive training updates that are restricted to the prescribed manifold, we
employ techniques from dynamic model order reduction for matrix differential
equations. This allows us to provide approximation, stability, and descent
guarantees. Moreover, our method automatically and dynamically adapts the ranks
during training to achieve the desired approximation accuracy. The efficiency
of the proposed method is demonstrated through a variety of numerical
experiments on fully-connected and convolutional networks."
Learning Object Placement via Dual-path Graph Completion,0.341582,"Object placement aims to place a foreground object over a background image
with a suitable location and size. In this work, we treat object placement as a
graph completion problem and propose a novel graph completion module (GCM). The
background scene is represented by a graph with multiple nodes at different
spatial locations with various receptive fields. The foreground object is
encoded as a special node that should be inserted at a reasonable place in this
graph. We also design a dual-path framework upon the structure of GCM to fully
exploit annotated composite images. With extensive experiments on OPA dataset,
our method proves to significantly outperform existing methods in generating
plausible object placement without loss of diversity."
Self-Repetition in Abstractive Neural Summarizers,0.404598,"We provide a quantitative and qualitative analysis of self-repetition in the
output of neural summarizers. We measure self-repetition as the number of
n-grams of length four or longer that appear in multiple outputs of the same
system. We analyze the behavior of three popular architectures (BART, T5, and
Pegasus), fine-tuned on five datasets. In a regression analysis, we find that
the three architectures have different propensities for repeating content
across output summaries for inputs, with BART being particularly prone to
self-repetition. Fine-tuning on more abstractive data, and on data featuring
formulaic language, is associated with a higher rate of self-repetition. In
qualitative analysis we find systems produce artefacts such as ads and
disclaimers unrelated to the content being summarized, as well as formulaic
phrases common in the fine-tuning domain. Our approach to corpus-level analysis
of self-repetition may help practitioners clean up training data for
summarizers and ultimately support methods for minimizing the amount of
self-repetition."
Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization,0.388926,"Current 3D scene stylization methods transfer textures and colors as styles
using arbitrary style references, lacking meaningful semantic correspondences.
We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) to
address this limitation. This controllable method stylizes a 3D scene using
radiance fields with a single stylized 2D view as a reference. We propose a ray
registration process based on the stylized reference view to obtain pseudo-ray
supervision in novel views. Then we exploit semantic correspondences in content
images to fill occluded regions with perceptually similar styles, resulting in
non-photorealistic and continuous novel view sequences. Our experimental
results demonstrate that Ref-NPR outperforms existing scene and video
stylization methods regarding visual quality and semantic correspondence. The
code and data are publicly available on the project page at
https://ref-npr.github.io."
Few-Shot Stance Detection via Target-Aware Prompt Distillation,0.33597,"Stance detection aims to identify whether the author of a text is in favor
of, against, or neutral to a given target. The main challenge of this task
comes two-fold: few-shot learning resulting from the varying targets and the
lack of contextual information of the targets. Existing works mainly focus on
solving the second issue by designing attention-based models or introducing
noisy external knowledge, while the first issue remains under-explored. In this
paper, inspired by the potential capability of pre-trained language models
(PLMs) serving as knowledge bases and few-shot learners, we propose to
introduce prompt-based fine-tuning for stance detection. PLMs can provide
essential contextual information for the targets and enable few-shot learning
via prompts. Considering the crucial role of the target in stance detection
task, we design target-aware prompts and propose a novel verbalizer. Instead of
mapping each label to a concrete word, our verbalizer maps each label to a
vector and picks the label that best captures the correlation between the
stance and the target. Moreover, to alleviate the possible defect of dealing
with varying targets with a single hand-crafted prompt, we propose to distill
the information learned from multiple prompts. Experimental results show the
superior performance of our proposed model in both full-data and few-shot
scenarios."
Spatio-Temporal Deformable Attention Network for Video Deblurring,0.4301,"The key success factor of the video deblurring methods is to compensate for
the blurry pixels of the mid-frame with the sharp pixels of the adjacent video
frames. Therefore, mainstream methods align the adjacent frames based on the
estimated optical flows and fuse the alignment frames for restoration. However,
these methods sometimes generate unsatisfactory results because they rarely
consider the blur levels of pixels, which may introduce blurry pixels from
video frames. Actually, not all the pixels in the video frames are sharp and
beneficial for deblurring. To address this problem, we propose the
spatio-temporal deformable attention network (STDANet) for video delurring,
which extracts the information of sharp pixels by considering the pixel-wise
blur levels of the video frames. Specifically, STDANet is an encoder-decoder
network combined with the motion estimator and spatio-temporal deformable
attention (STDA) module, where motion estimator predicts coarse optical flows
that are used as base offsets to find the corresponding sharp pixels in STDA
module. Experimental results indicate that the proposed STDANet performs
favorably against state-of-the-art methods on the GoPro, DVD, and BSD datasets."
META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI,0.396694,"Task-oriented dialogue (TOD) systems have been widely used by mobile phone
intelligent assistants to accomplish tasks such as calendar scheduling or hotel
reservation. Current TOD systems usually focus on multi-turn text/speech
interaction, then they would call back-end APIs designed for TODs to perform
the task. However, this API-based architecture greatly limits the
information-searching capability of intelligent assistants and may even lead to
task failure if TOD-specific APIs are not available or the task is too
complicated to be executed by the provided APIs. In this paper, we propose a
new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A
GUI-TOD system can directly perform GUI operations on real APPs and execute
tasks without invoking TOD-specific backend APIs. Furthermore, we release
META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile
GUI. We also propose a multi-model action prediction and response model, which
show promising results on META-GUI. The dataset, codes and leaderboard are
publicly available."
CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval,0.396022,"Fact-checking has gained increasing attention due to the widespread of
falsified information. Most fact-checking approaches focus on claims made in
English only due to the data scarcity issue in other languages. The lack of
fact-checking datasets in low-resource languages calls for an effective
cross-lingual transfer technique for fact-checking. Additionally, trustworthy
information in different languages can be complementary and helpful in
verifying facts. To this end, we present the first fact-checking framework
augmented with cross-lingual retrieval that aggregates evidence retrieved from
multiple languages through a cross-lingual retriever. Given the absence of
cross-lingual information retrieval datasets with claim-like queries, we train
the retriever with our proposed Cross-lingual Inverse Cloze Task (X-ICT), a
self-supervised algorithm that creates training instances by translating the
title of a passage. The goal for X-ICT is to learn cross-lingual retrieval in
which the model learns to identify the passage corresponding to a given
translated title. On the X-Fact dataset, our approach achieves 2.23% absolute
F1 improvement in the zero-shot cross-lingual setup over prior systems. The
source code and data are publicly available at
https://github.com/khuangaf/CONCRETE."
MatteFormer: Transformer-Based Image Matting via Prior-Tokens,0.359677,"In this paper, we propose a transformer-based image matting model called
MatteFormer, which takes full advantage of trimap information in the
transformer block. Our method first introduces a prior-token which is a global
representation of each trimap region (e.g. foreground, background and unknown).
These prior-tokens are used as global priors and participate in the
self-attention mechanism of each block. Each stage of the encoder is composed
of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin
Transformer block, but differs in a couple of aspects: 1) It has PA-WSA
(Prior-Attentive Window Self-Attention) layer, performing self-attention not
only with spatial-tokens but also with prior-tokens. 2) It has prior-memory
which saves prior-tokens accumulatively from the previous blocks and transfers
them to the next block. We evaluate our MatteFormer on the commonly used image
matting datasets: Composition-1k and Distinctions-646. Experiment results show
that our proposed method achieves state-of-the-art performance with a large
margin. Our codes are available at https://github.com/webtoon/matteformer."
DePlot: One-shot visual language reasoning by plot-to-table translation,0.365707,"Visual language such as charts and plots is ubiquitous in the human world.
Comprehending plots and charts requires strong reasoning skills. Prior
state-of-the-art (SOTA) models require at least tens of thousands of training
examples and their reasoning capabilities are still much limited, especially on
complex human-written queries. This paper presents the first one-shot solution
to visual language reasoning. We decompose the challenge of visual language
reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over
the translated text. The key in this method is a modality conversion module,
named as DePlot, which translates the image of a plot or chart to a linearized
table. The output of DePlot can then be directly used to prompt a pretrained
large language model (LLM), exploiting the few-shot reasoning capabilities of
LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing
unified task formats and metrics, and train DePlot end-to-end on this task.
DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
fashion. Compared with a SOTA model finetuned on more than >28k data points,
DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over
finetuned SOTA on human-written queries from the task of chart QA."
Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild,0.420928,"Talking face generation with great practical significance has attracted more
attention in recent audio-visual studies. How to achieve accurate lip
synchronization is a long-standing challenge to be further investigated.
Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by
incorporating spatial attention module and channel attention module into
lip-syncing strategy. Rather than focusing on the unimportant regions of the
face image, the proposed AttnWav2Lip model is able to pay more attention on the
lip region reconstruction. To our limited knowledge, this is the first attempt
to introduce attention mechanism to the scheme of talking face generation. An
extensive experiments have been conducted to evaluate the effectiveness of the
proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a
superior performance has been demonstrated on the benchmark lip synthesis
datasets, including LRW, LRS2 and LRS3."
Quantitative AI Risk Assessments: Opportunities and Challenges,0.371023,"Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations."
A Large Scale Search Dataset for Unbiased Learning to Rank,0.40558,"The unbiased learning to rank (ULTR) problem has been greatly advanced by
recent deep learning techniques and well-designed debias algorithms. However,
promising results on the existing benchmark datasets may not be extended to the
practical scenario due to the following disadvantages observed from those
popular benchmark datasets: (1) outdated semantic feature extraction where
state-of-the-art large scale pre-trained language models like BERT cannot be
exploited due to the missing of the original text;(2) incomplete display
features for in-depth study of ULTR, e.g., missing the displayed abstract of
documents for analyzing the click necessary bias; (3) lacking real-world user
feedback, leading to the prevalence of synthetic datasets in the empirical
study. To overcome the above disadvantages, we introduce the Baidu-ULTR
dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008
expert annotated queries, which is orders of magnitude larger than the existing
ones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained
language model for easy usage; (2) sufficient display information such as
position, displayed height, and displayed abstract, enabling the comprehensive
study of different biases with advanced techniques such as causal discovery and
meta-learning; and (3) rich user feedback on search result pages (SERPs) like
dwelling time, allowing for user engagement optimization and promoting the
exploration of multi-task learning in ULTR. In this paper, we present the
design principle of Baidu-ULTR and the performance of benchmark ULTR algorithms
on this new data resource, favoring the exploration of ranking for long-tail
queries and pre-training tasks for ranking. The Baidu-ULTR dataset and
corresponding baseline implementation are available at
https://github.com/ChuXiaokai/baidu_ultr_dataset."
CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation,0.394304,"3D human pose estimation can be handled by encoding the geometric
dependencies between the body parts and enforcing the kinematic constraints.
Recently, Transformer has been adopted to encode the long-range dependencies
between the joints in the spatial and temporal domains. While they had shown
excellence in long-range dependencies, studies have noted the need for
improving the locality of vision Transformers. In this direction, we propose a
novel pose estimation Transformer featuring rich representations of body joints
critical for capturing subtle changes across frames (i.e., inter-feature
representation). Specifically, through two novel interaction modules;
Cross-Joint Interaction and Cross-Frame Interaction, the model explicitly
encodes the local and global dependencies between the body joints. The proposed
architecture achieved state-of-the-art performance on two popular 3D human pose
estimation datasets, Human3.6 and MPI-INF-3DHP. In particular, our proposed
CrossFormer method boosts performance by 0.9% and 0.3%, compared to the closest
counterpart, PoseFormer, using the detected 2D poses and ground-truth settings
respectively."
Uncertainty-aware Personal Assistant for Making Personalized Privacy Decisions,0.430717,"Many software systems, such as online social networks enable users to share
information about themselves. While the action of sharing is simple, it
requires an elaborate thought process on privacy: what to share, with whom to
share, and for what purposes. Thinking about these for each piece of content to
be shared is tedious. Recent approaches to tackle this problem build personal
assistants that can help users by learning what is private over time and
recommending privacy labels such as private or public to individual content
that a user considers sharing. However, privacy is inherently ambiguous and
highly personal. Existing approaches to recommend privacy decisions do not
address these aspects of privacy sufficiently. Ideally, a personal assistant
should be able to adjust its recommendation based on a given user, considering
that user's privacy understanding. Moreover, the personal assistant should be
able to assess when its recommendation would be uncertain and let the user make
the decision on her own. Accordingly, this paper proposes a personal assistant
that uses evidential deep learning to classify content based on its privacy
label. An important characteristic of the personal assistant is that it can
model its uncertainty in its decisions explicitly, determine that it does not
know the answer, and delegate from making a recommendation when its uncertainty
is high. By factoring in the user's own understanding of privacy, such as risk
factors or own labels, the personal assistant can personalize its
recommendations per user. We evaluate our proposed personal assistant using a
well-known data set. Our results show that our personal assistant can
accurately identify uncertain cases, personalize them to its user's needs, and
thus helps users preserve their privacy well."
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion,0.349541,"Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task."
Multi-Behavior Enhanced Recommendation with Cross-Interaction Collaborative Relation Modeling,0.410196,"Many previous studies aim to augment collaborative filtering with deep neural
network techniques, so as to achieve better recommendation performance.
However, most existing deep learning-based recommender systems are designed for
modeling singular type of user-item interaction behavior, which can hardly
distill the heterogeneous relations between user and item. In practical
recommendation scenarios, there exist multityped user behaviors, such as browse
and purchase. Due to the overlook of user's multi-behavioral patterns over
different items, existing recommendation methods are insufficient to capture
heterogeneous collaborative signals from user multi-behavior data. Inspired by
the strength of graph neural networks for structured data modeling, this work
proposes a Graph Neural Multi-Behavior Enhanced Recommendation (GNMR) framework
which explicitly models the dependencies between different types of user-item
interactions under a graph-based message passing architecture. GNMR devises a
relation aggregation network to model interaction heterogeneity, and
recursively performs embedding propagation between neighboring nodes over the
user-item interaction graph. Experiments on real-world recommendation datasets
show that our GNMR consistently outperforms state-of-the-art methods. The
source code is available at https://github.com/akaxlh/GNMR."
NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,0.335746,"Given the ubiquitous nature of numbers in text, reasoning with numbers to
perform simple calculations is an important skill of AI systems. While many
datasets and models have been developed to this end, state-of-the-art AI
systems are brittle; failing to perform the underlying mathematical reasoning
when they appear in a slightly different scenario. Drawing inspiration from
GLUE that was proposed in the context of natural language understanding, we
propose NumGLUE, a multi-task benchmark that evaluates the performance of AI
systems on eight different tasks, that at their core require simple arithmetic
understanding. We show that this benchmark is far from being solved with neural
models including state-of-the-art large-scale language models performing
significantly worse than humans (lower by 46.4%). Further, NumGLUE promotes
sharing knowledge across tasks, especially those with limited training data as
evidenced by the superior performance (average gain of 3.4% on each task) when
a model is jointly trained on all the tasks as opposed to task-specific
modeling. Finally, we hope that NumGLUE will encourage systems that perform
robust and general arithmetic reasoning within language, a first step towards
being able to perform more complex mathematical reasoning."
The Inverse of Exact Renormalization Group Flows as Statistical Inference,0.352224,"We build on the view of the Exact Renormalization Group (ERG) as an
instantiation of Optimal Transport described by a functional
convection-diffusion equation. We provide a new information theoretic
perspective for understanding the ERG through the intermediary of Bayesian
Statistical Inference. This connection is facilitated by the Dynamical Bayesian
Inference scheme, which encodes Bayesian inference in the form of a one
parameter family of probability distributions solving an integro-differential
equation derived from Bayes' law. In this note, we demonstrate how the
Dynamical Bayesian Inference equation is, itself, equivalent to a diffusion
equation which we dub Bayesian Diffusion. Identifying the features that define
Bayesian Diffusion, and mapping them onto the features that define the ERG, we
obtain a dictionary outlining how renormalization can be understood as the
inverse of statistical inference."
"GigaST: A 10,000-hour Pseudo Speech Translation Corpus",0.410083,"This paper introduces GigaST, a large-scale pseudo speech translation (ST)
corpus. We create the corpus by translating the text in GigaSpeech, an English
ASR corpus, into German and Chinese. The training set is translated by a strong
machine translation system and the test set is translated by human. ST models
trained with an addition of our corpus obtain new state-of-the-art results on
the MuST-C English-German benchmark test set. We provide a detailed description
of the translation process and verify its quality. We make the translated text
data public and hope to facilitate research in speech translation.
Additionally, we also release the training scripts on NeurST to make it easy to
replicate our systems. GigaST dataset is available at
https://st-benchmark.github.io/resources/GigaST."
Automatic Fine-grained Glomerular Lesion Recognition in Kidney Pathology,0.366728,"Recognition of glomeruli lesions is the key for diagnosis and treatment
planning in kidney pathology; however, the coexisting glomerular structures
such as mesangial regions exacerbate the difficulties of this task. In this
paper, we introduce a scheme to recognize fine-grained glomeruli lesions from
whole slide images. First, a focal instance structural similarity loss is
proposed to drive the model to locate all types of glomeruli precisely. Then an
Uncertainty Aided Apportionment Network is designed to carry out the
fine-grained visual classification without bounding-box annotations. This
double branch-shaped structure extracts common features of the child class from
the parent class and produces the uncertainty factor for reconstituting the
training dataset. Results of slide-wise evaluation illustrate the effectiveness
of the entire scheme, with an 8-22% improvement of the mean Average Precision
compared with remarkable detection methods. The comprehensive results clearly
demonstrate the effectiveness of the proposed method."
Explainable Action Advising for Multi-Agent Reinforcement Learning,0.416594,"Action advising is a knowledge transfer technique for reinforcement learning
based on the teacher-student paradigm. An expert teacher provides advice to a
student during training in order to improve the student's sample efficiency and
policy performance. Such advice is commonly given in the form of state-action
pairs. However, it makes it difficult for the student to reason with and apply
to novel states. We introduce Explainable Action Advising, in which the teacher
provides action advice as well as associated explanations indicating why the
action was chosen. This allows the student to self-reflect on what it has
learned, enabling advice generalization and leading to improved sample
efficiency and learning performance - even in environments where the teacher is
sub-optimal. We empirically show that our framework is effective in both
single-agent and multi-agent scenarios, yielding improved policy returns and
convergence rates when compared to state-of-the-art methods"
Reinforcement Learning with Prior Policy Guidance for Motion Planning of Dual-Arm Free-Floating Space Robot,0.391433,"Reinforcement learning methods as a promising technique have achieved
superior results in the motion planning of free-floating space robots. However,
due to the increase in planning dimension and the intensification of system
dynamics coupling, the motion planning of dual-arm free-floating space robots
remains an open challenge. In particular, the current study cannot handle the
task of capturing a non-cooperative object due to the lack of the pose
constraint of the end-effectors. To address the problem, we propose a novel
algorithm, EfficientLPT, to facilitate RL-based methods to improve planning
accuracy efficiently. Our core contributions are constructing a mixed policy
with prior knowledge guidance and introducing infinite norm to build a more
reasonable reward function. Furthermore, our method successfully captures a
rotating object with different spinning speeds."
Label Semantics for Few Shot Named Entity Recognition,0.372126,"We study the problem of few shot learning for named entity recognition.
Specifically, we leverage the semantic information in the names of the labels
as a way of giving the model additional signal and enriched priors. We propose
a neural architecture that consists of two BERT encoders, one to encode the
document and its tokens and another one to encode each of the labels in natural
language format. Our model learns to match the representations of named
entities computed by the first encoder with label representations computed by
the second encoder. The label semantics signal is shown to support improved
state-of-the-art results in multiple few shot NER benchmarks and on-par
performance in standard benchmarks. Our model is especially effective in low
resource settings."
News Headlines Dataset For Sarcasm Detection,0.33946,"Past studies in Sarcasm Detection mostly make use of Twitter datasets
collected using hashtag-based supervision but such datasets are noisy in terms
of labels and language. Furthermore, many tweets are replies to other tweets,
and detecting sarcasm in these requires the availability of contextual tweets.
To overcome the limitations related to noise in Twitter datasets, we curate
News Headlines Dataset from two news websites: TheOnion aims at producing
sarcastic versions of current events, whereas HuffPost publishes real news. The
dataset contains about 28K headlines out of which 13K are sarcastic. To make it
more useful, we have included the source links of the news articles so that
more data can be extracted as needed. In this paper, we describe various
details about the dataset and potential use cases apart from Sarcasm Detection."
Detecting Methane Plumes using PRISMA: Deep Learning Model and Data Augmentation,0.367364,"The new generation of hyperspectral imagers, such as PRISMA, has improved
significantly our detection capability of methane (CH4) plumes from space at
high spatial resolution (30m). We present here a complete framework to identify
CH4 plumes using images from the PRISMA satellite mission and a deep learning
model able to detect plumes over large areas. To compensate for the relative
scarcity of PRISMA images, we trained our model by transposing high resolution
plumes from Sentinel-2 to PRISMA. Our methodology thus avoids computationally
expensive synthetic plume generation from Large Eddy Simulations by generating
a broad and realistic training database, and paves the way for large-scale
detection of methane plumes using future hyperspectral sensors (EnMAP, EMIT,
CarbonMapper)."
Coupled Iterative Refinement for 6D Multi-Object Pose Estimation,0.376839,"We address the task of 6D multi-object pose: given a set of known 3D objects
and an RGB or RGB-D input image, we detect and estimate the 6D pose of each
object. We propose a new approach to 6D object pose estimation which consists
of an end-to-end differentiable architecture that makes use of geometric
knowledge. Our approach iteratively refines both pose and correspondence in a
tightly coupled manner, allowing us to dynamically remove outliers to improve
accuracy. We use a novel differentiable layer to perform pose refinement by
solving an optimization problem we refer to as Bidirectional Depth-Augmented
Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on
standard 6D Object Pose benchmarks. Code is available at
https://github.com/princeton-vl/Coupled-Iterative-Refinement."
BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog,0.431889,"A typical end-to-end task-oriented dialog system transfers context into
dialog state, and upon which generates a response, which usually faces the
problem of error propagation from both previously generated inaccurate dialog
states and responses, especially in low-resource scenarios. To alleviate these
issues, we propose BORT, a back and denoising reconstruction approach for
end-to-end task-oriented dialog system. Squarely, to improve the accuracy of
dialog states, back reconstruction is used to reconstruct the original input
context from the generated dialog states since inaccurate dialog states cannot
recover the corresponding input context. To enhance the denoising capability of
the model to reduce the impact of error propagation, denoising reconstruction
is used to reconstruct the corrupted dialog state and response. Extensive
experiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of
BORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot
domain and low-resource scenarios."
BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis,0.527198,"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to align aspects and corresponding sentiments for
aspect-specific sentiment polarity inference. It is challenging because a
sentence may contain multiple aspects or complicated (e.g., conditional,
coordinating, or adversative) relations. Recently, exploiting dependency syntax
information with graph neural networks has been the most popular trend. Despite
its success, methods that heavily rely on the dependency tree pose challenges
in accurately modeling the alignment of the aspects and their words indicative
of sentiment, since the dependency tree may provide noisy signals of unrelated
associations (e.g., the ""conj"" relation between ""great"" and ""dreadful"" in
Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax
aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully
exploits the syntax information (e.g., phrase segmentation and hierarchical
structure) of the constituent tree of a sentence to model the sentiment-aware
context of every single aspect (called intra-context) and the sentiment
relations across aspects (called inter-context) for learning. Experiments on
four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the
state-of-the-art methods consistently."
IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes,0.446895,"Indoor scenes exhibit significant appearance variations due to myriad
interactions between arbitrarily diverse object shapes, spatially-changing
materials, and complex lighting. Shadows, highlights, and inter-reflections
caused by visible and invisible light sources require reasoning about
long-range interactions for inverse rendering, which seeks to recover the
components of image formation, namely, shape, material, and lighting. In this
work, our intuition is that the long-range attention learned by transformer
architectures is ideally suited to solve longstanding challenges in
single-image inverse rendering. We demonstrate with a specific instantiation of
a dense vision transformer, IRISformer, that excels at both single-task and
multi-task reasoning required for inverse rendering. Specifically, we propose a
transformer architecture to simultaneously estimate depths, normals,
spatially-varying albedo, roughness and lighting from a single image of an
indoor scene. Our extensive evaluations on benchmark datasets demonstrate
state-of-the-art results on each of the above tasks, enabling applications like
object insertion and material editing in a single unconstrained real image,
with greater photorealism than prior works. Code and data are publicly released
at https://github.com/ViLab-UCSD/IRISformer."
C-VTON: Context-Driven Image-Based Virtual Try-On Network,0.468031,"Image-based virtual try-on techniques have shown great promise for enhancing
the user-experience and improving customer satisfaction on fashion-oriented
e-commerce platforms. However, existing techniques are currently still limited
in the quality of the try-on results they are able to produce from input images
of diverse characteristics. In this work, we propose a Context-Driven Virtual
Try-On Network (C-VTON) that addresses these limitations and convincingly
transfers selected clothing items to the target subjects even under challenging
pose configurations and in the presence of self-occlusions. At the core of the
C-VTON pipeline are: (i) a geometric matching procedure that efficiently aligns
the target clothing with the pose of the person in the input images, and (ii) a
powerful image generator that utilizes various types of contextual information
when synthesizing the final try-on result. C-VTON is evaluated in rigorous
experiments on the VITON and MPV datasets and in comparison to state-of-the-art
techniques from the literature. Experimental results show that the proposed
approach is able to produce photo-realistic and visually convincing results and
significantly improves on the existing state-of-the-art."
"A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets",0.542205,"In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this article, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. We create a new large medical dialogue dataset with multi-level
finegrained annotations and establish five independent tasks, including named
entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies. Both code and data is available
from https://github.com/lemuria-wchen/imcs21."
Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation,0.448647,"Although human action anticipation is a task which is inherently multi-modal,
state-of-the-art methods on well known action anticipation datasets leverage
this data by applying ensemble methods and averaging scores of unimodal
anticipation networks. In this work we introduce transformer based modality
fusion techniques, which unify multi-modal data at an early stage. Our
Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular
score fusion approaches and presents state-of-the-art results outperforming
previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily
extensible and allows for adding new modalities without architectural changes.
Consequently, we extracted audio features on EpicKitchens-100 which we add to
the set of commonly used features in the community."
Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations,0.5027,"We present Neural Feature Fusion Fields (N3F), a method that improves dense
2D image feature extractors when the latter are applied to the analysis of
multiple images reconstructible as a 3D scene. Given an image feature
extractor, for example pre-trained using self-supervision, N3F uses it as a
teacher to learn a student network defined in 3D space. The 3D student network
is similar to a neural radiance field that distills said features and can be
trained with the usual differentiable rendering machinery. As a consequence,
N3F is readily applicable to most neural rendering formulations, including
vanilla NeRF and its extensions to complex dynamic scenes. We show that our
method not only enables semantic understanding in the context of scene-specific
neural fields without the use of manual labels, but also consistently improves
over the self-supervised 2D baselines. This is demonstrated by considering
various tasks, such as 2D object retrieval, 3D segmentation, and scene editing,
in diverse sequences, including long egocentric videos in the EPIC-KITCHENS
benchmark."
GIFS: Neural Implicit Function for General Shape Representation,0.473087,"Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs ."
Extreme Compression for Pre-trained Transformers Made Simple and Efficient,0.455049,"Extreme compression, particularly ultra-low bit precision (binary/ternary)
quantization, has been proposed to fit large NLP models on resource-constraint
devices. However, to preserve the accuracy for such aggressive compression
schemes, cutting-edge methods usually introduce complicated compression
pipelines, e.g., multi-stage expensive knowledge distillation with extensive
hyperparameter tuning. Also, they oftentimes focus less on smaller transformer
models that have already been heavily compressed via knowledge distillation and
lack a systematic study to show the effectiveness of their methods. In this
paper, we perform a very comprehensive systematic study to measure the impact
of many key hyperparameters and training strategies from previous works. As a
result, we find out that previous baselines for ultra-low bit precision
quantization are significantly under-trained. Based on our study, we propose a
simple yet effective compression pipeline for extreme compression, named XTC.
XTC demonstrates that (1) we can skip the pre-training knowledge distillation
to obtain a 5-layer BERT while achieving better performance than previous
state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization
plus layer reduction is able to reduce the model size by 50x, resulting in new
state-of-the-art results on GLUE tasks."
Uncertainty-aware Panoptic Segmentation,0.551462,"Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective panoptic fusion module that
leverages the predicted uncertainties. Furthermore, we provide several strong
baselines combining state-of-the-art panoptic segmentation networks with
sampling-free uncertainty estimation techniques. Extensive evaluations show
that our EvPSNet achieves the new state-of-the-art for the standard Panoptic
Quality (PQ), as well as for our uncertainty-aware panoptic metrics. We make
the code available at: \url{https://github.com/kshitij3112/EvPSNet}"
ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations,0.446489,"Context is everything, even in commonsense moral reasoning. Changing contexts
can flip the moral judgment of an action; ""Lying to a friend"" is wrong in
general, but may be morally acceptable if it is intended to protect their life.
  We present ClarifyDelphi, an interactive system that learns to ask
clarification questions (e.g., why did you lie to your friend?) in order to
elicit additional salient contexts of a social or moral situation. We posit
that questions whose potential answers lead to diverging moral judgments are
the most informative. Thus, we propose a reinforcement learning framework with
a defeasibility reward that aims to maximize the divergence between moral
judgments of hypothetical answers to a question. Human evaluation demonstrates
that our system generates more relevant, informative and defeasible questions
compared to competitive baselines. Our work is ultimately inspired by studies
in cognitive science that have investigated the flexibility in moral cognition
(i.e., the diverse contexts in which moral rules can be bent), and we hope that
research in this direction can assist both cognitive and computational
investigations of moral judgments."
Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning,0.532493,"With the rapid development of artificial intelligence and autonomous driving
technology, the demand for semiconductors is projected to rise substantially.
However, the massive expansion of semiconductor manufacturing and the
development of new technology will bring many defect wafers. If these defect
wafers have not been correctly inspected, the ineffective semiconductor
processing on these defect wafers will cause additional impact to our
environment, such as excessive carbon dioxide emission and energy consumption.
In this paper, we utilize the information processing advantages of quantum
computing to promote the defect learning defect review (DLDR). We propose a
classical-quantum hybrid algorithm for deep learning on near-term quantum
processors. By tuning parameters implemented on it, quantum circuit driven by
our framework learns a given DLDR task, include of wafer defect map
classification, defect pattern classification, and hotspot detection. In
addition, we explore parametrized quantum circuits with different
expressibility and entangling capacities. These results can be used to build a
future roadmap to develop circuit-based quantum deep learning for semiconductor
defect detection."
ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework,0.511731,"In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost."
SeqOT: A Spatial-Temporal Transformer Network for Place Recognition Using Sequential LiDAR Data,0.552902,"Place recognition is an important component for autonomous vehicles to
achieve loop closing or global localization. In this paper, we tackle the
problem of place recognition based on sequential 3D LiDAR scans obtained by an
onboard LiDAR sensor. We propose a transformer-based network named SeqOT to
exploit the temporal and spatial information provided by sequential range
images generated from the LiDAR data. It uses multi-scale transformers to
generate a global descriptor for each sequence of LiDAR range images in an
end-to-end fashion. During online operation, our SeqOT finds similar places by
matching such descriptors between the current query sequence and those stored
in the map. We evaluate our approach on four datasets collected with different
types of LiDAR sensors in different environments. The experimental results show
that our method outperforms the state-of-the-art LiDAR-based place recognition
methods and generalizes well across different environments. Furthermore, our
method operates online faster than the frame rate of the sensor. The
implementation of our method is released as open source at:
https://github.com/BIT-MJY/SeqOT."
A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots,0.452555,"A slot value might be provided segment by segment over multiple-turn
interactions in a dialog, especially for some important information such as
phone numbers and names. It is a common phenomenon in daily life, but little
attention has been paid to it in previous work. To fill the gap, this paper
defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds
a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset
includes a total of 40K dialogs and 500K utterances from four different
domains: Chinese names, phone numbers, ID numbers and license plate numbers.
The data is well annotated with sub-slot values, slot values, dialog states and
actions. We find some new linguistic phenomena and interactive manners in SSTOD
which raise critical challenges of building dialog agents for the task. We test
three state-of-the-art dialog models on SSTOD and find they cannot handle the
task well on any of the four domains. We also investigate an improved model by
involving slot knowledge in a plug-in manner. More work should be done to meet
the new challenges raised from SSTOD which widely exists in real-life
applications. The dataset and code are publicly available via
https://github.com/shunjiu/SSTOD."
Ham2Pose: Animating Sign Language Notation into Pose Sequences,0.467542,"Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal by design, our proposed method offers a generic solution
invariant to the target Sign language. Our method gradually generates pose
predictions using transformer encoders that create meaningful representations
of the text and poses while considering their spatial and temporal information.
We use weak supervision for the training process and show that our method
succeeds in learning from partial and inaccurate data. Additionally, we offer a
new distance measurement that considers missing keypoints, to measure the
distance between pose sequences using DTW-MJE. We validate its correctness
using AUTSL, a large-scale Sign language dataset, show that it measures the
distance between pose sequences more accurately than existing measurements, and
use it to assess the quality of our generated pose sequences. Code for the data
pre-processing, the model, and the distance measurement is publicly released
for future research."
SGPT: GPT Sentence Embeddings for Semantic Search,0.508077,"Decoder transformers have continued increasing in scale reaching hundreds of
billions of parameters. Due to their scale the same decoder sets
state-of-the-art results on various language tasks via prompting or
fine-tuning. Yet, these large foundation models remain unusable for the related
fields of semantic search and sentence embeddings. This prevents possibly new
state-of-the-art results and forces organizations to train and maintain
separate models. To this end, we propose SGPT to use decoders for sentence
embeddings and semantic search via prompting or fine-tuning. At 5.8 billion
parameters SGPT improves on the previously best sentence embeddings by a margin
of 7% and outperforms a concurrent method with 175 billion parameters as
measured on the BEIR search benchmark. Code, models and result files are freely
available at https://github.com/Muennighoff/sgpt."
Differentiable Point-Based Radiance Fields for Efficient View Synthesis,0.485825,"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers."
Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation,0.517173,"Research on Automatic Story Generation (ASG) relies heavily on human and
automatic evaluation. However, there is no consensus on which human evaluation
criteria to use, and no analysis of how well automatic criteria correlate with
them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a
set of 6 orthogonal and comprehensive human criteria, carefully motivated by
the social sciences literature. We also present HANNA, an annotated dataset of
1,056 stories produced by 10 different ASG systems. HANNA allows us to
quantitatively evaluate the correlations of 72 automatic metrics with human
criteria. Our analysis highlights the weaknesses of current metrics for ASG and
allows us to formulate practical recommendations for ASG evaluation."
Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation,0.52901,"Scene flow estimation, which extracts point-wise motion between scenes, is
becoming a crucial task in many computer vision tasks. However, all of the
existing estimation methods utilize only the unidirectional features,
restricting the accuracy and generality. This paper presents a novel scene flow
estimation architecture using bidirectional flow embedding layers. The proposed
bidirectional layer learns features along both forward and backward directions,
enhancing the estimation performance. In addition, hierarchical feature
extraction and warping improve the performance and reduce computational
overhead. Experimental results show that the proposed architecture achieved a
new state-of-the-art record by outperforming other approaches with large margin
in both FlyingThings3D and KITTI benchmarks. Codes are available at
https://github.com/cwc1260/BiFlow."
TourBERT: A pretrained language model for the tourism industry,0.527633,"The Bidirectional Encoder Representations from Transformers (BERT) is
currently one of the most important and state-of-the-art models for natural
language. However, it has also been shown that for domain-specific tasks it is
helpful to pretrain BERT on a domain-specific corpus. In this paper, we present
TourBERT, a pretrained language model for tourism. We describe how TourBERT was
developed and evaluated. The evaluations show that TourBERT is outperforming
BERT in all tourism-specific tasks."
Shadow-Background-Noise 3D Spatial Decomposition Using Sparse Low-Rank Gaussian Properties for Video-SAR Moving Target Shadow Enhancement,0.483617,"Moving target shadows among video synthetic aperture radar (Video-SAR) images
are always interfered by low scattering backgrounds and cluttered noises,
causing poor detec-tion-tracking accuracy. Thus, a shadow-background-noise 3D
spatial decomposition (SBN-3D-SD) model is proposed to enhance shadows for
higher detection-tracking accuracy. It leverages the sparse property of
shadows, the low-rank property of back-grounds, and the Gaussian property of
noises to perform 3D spatial three-decomposition. It separates shadows from
back-grounds and noises by the alternating direction method of multi-pliers
(ADMM). Results on the Sandia National Laboratories (SNL) data verify its
effectiveness. It boosts the shadow saliency from the qualitative and
quantitative evaluation. It boosts the shadow detection accuracy of Faster
R-CNN, RetinaNet and YOLOv3. It also boosts the shadow tracking accuracy of
TransTrack, FairMOT and ByteTrack."
Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some benchmarks,0.540019,"The Donate Speech campaign has so far succeeded in gathering approximately
3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta
(Donate Speech) corpus. The corpus includes over twenty thousand speakers from
all the regions of Finland and from all age brackets. The primary goals of the
collection were to create a representative, large-scale resource to study
spontaneous spoken Finnish and to accelerate the development of language
technology and speech-based services. In this paper, we present the collection
process and the collected corpus, and showcase its versatility through multiple
use cases. The evaluated use cases include: automatic speech recognition of
spontaneous speech, detection of age, gender, dialect and topic and metadata
analysis. We provide benchmarks for the use cases, as well down loadable,
trained baseline systems with open-source code for reproducibility. One further
use case is to verify the metadata and transcripts given in this corpus itself,
and to suggest artificial metadata and transcripts for the part of the corpus
where it is missing."
Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning,0.474729,"Conversational recommender systems (CRS) aim to proactively elicit user
preference and recommend high-quality items through natural language
conversations. Typically, a CRS consists of a recommendation module to predict
preferred items for users and a conversation module to generate appropriate
responses. To develop an effective CRS, it is essential to seamlessly integrate
the two modules. Existing works either design semantic alignment strategies, or
share knowledge resources and representations between the two modules. However,
these approaches still rely on different architectures or techniques to develop
the two modules, making it difficult for effective module integration.
  To address this problem, we propose a unified CRS model named UniCRS based on
knowledge-enhanced prompt learning. Our approach unifies the recommendation and
conversation subtasks into the prompt learning paradigm, and utilizes
knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to
fulfill both subtasks in a unified approach. In the prompt design, we include
fused knowledge representations, task-specific soft tokens, and the dialogue
context, which can provide sufficient contextual information to adapt the PLM
for the CRS task. Besides, for the recommendation subtask, we also incorporate
the generated response template as an important part of the prompt, to enhance
the information interaction between the two subtasks. Extensive experiments on
two public CRS datasets have demonstrated the effectiveness of our approach."
MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation,0.480304,"Applying existing methods to emotional support conversation -- which provides
valuable assistance to people who are in need -- has two major limitations: (a)
they generally employ a conversation-level emotion label, which is too
coarse-grained to capture user's instant mental state; (b) most of them focus
on expressing empathy in the response(s) rather than gradually reducing user's
distress. To address the problems, we propose a novel model \textbf{MISC},
which firstly infers the user's fine-grained emotional status, and then
responds skillfully using a mixture of strategy. Experimental results on the
benchmark dataset demonstrate the effectiveness of our method and reveal the
benefits of fine-grained emotion understanding as well as mixed-up strategy
modeling. Our code and data could be found in
\url{https://github.com/morecry/MISC}."
Unsupervised Segmentation of Hyperspectral Remote Sensing Images with Superpixels,0.453867,"In this paper, we propose an unsupervised method for hyperspectral remote
sensing image segmentation. The method exploits the mean-shift clustering
algorithm that takes as input a preliminary hyperspectral superpixels
segmentation together with the spectral pixel information. The proposed method
does not require the number of segmentation classes as input parameter, and it
does not exploit any a-priori knowledge about the type of land-cover or
land-use to be segmented (e.g. water, vegetation, building etc.). Experiments
on Salinas, SalinasA, Pavia Center and Pavia University datasets are carried
out. Performance are measured in terms of normalized mutual information,
adjusted Rand index and F1-score. Results demonstrate the validity of the
proposed method in comparison with the state of the art."
A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction,0.499575,"Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment
analysis task that aims to extract triplets of aspect terms, sentiments, and
opinion terms from review sentences. Recently, span-level models achieve
gratifying results on ASTE task by taking advantage of the predictions of all
possible spans. Since all possible spans significantly increases the number of
potential aspect and opinion candidates, it is crucial and challenging to
efficiently extract the triplet elements among them. In this paper, we present
a span-level bidirectional network which utilizes all possible spans as input
and extracts triplets from spans bidirectionally. Specifically, we devise both
the aspect decoder and opinion decoder to decode the span representations and
extract triples from aspect-to-opinion and opinion-to-aspect directions. With
these two decoders complementing with each other, the whole network can extract
triplets from spans more comprehensively. Moreover, considering that mutual
exclusion cannot be guaranteed between the spans, we design a similar span
separation loss to facilitate the downstream task of distinguishing the correct
span by expanding the KL divergence of similar spans during the training
process; in the inference process, we adopt an inference strategy to remove
conflicting triplets from the results base on their confidence scores.
Experimental results show that our framework not only significantly outperforms
state-of-the-art methods, but achieves better performance in predicting
triplets with multi-token entities and extracting triplets in sentences contain
multi-triplets."
ProsocialDialog: A Prosocial Backbone for Conversational Agents,0.521271,"Most existing dialogue systems fail to respond properly to potentially unsafe
user utterances by either ignoring or passively agreeing with them. To address
this issue, we introduce ProsocialDialog, the first large-scale multi-turn
dialogue dataset to teach conversational agents to respond to problematic
content following social norms. Covering diverse unethical, problematic,
biased, and toxic situations, ProsocialDialog contains responses that encourage
prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,
RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists
of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue
safety labels accompanied by free-form rationales.
  With this dataset, we introduce a dialogue safety detection module, Canary,
capable of generating RoTs given conversational context, and a
socially-informed dialogue agent, Prost. Empirical results show that Prost
generates more socially acceptable dialogues compared to other state-of-the-art
language and dialogue models in both in-domain and out-of-domain settings.
Additionally, Canary effectively guides conversational agents and off-the-shelf
language models to generate significantly more prosocial responses. Our work
highlights the promise and importance of creating and steering conversational
AI to be socially responsible."
Pointillism: Accurate 3D bounding box estimation with multi-radars,0.50552,"Autonomous perception requires high-quality environment sensing in the form
of 3D bounding boxes of dynamic objects. The primary sensors used in automotive
systems are light-based cameras and LiDARs. However, they are known to fail in
adverse weather conditions. Radars can potentially solve this problem as they
are barely affected by adverse weather conditions. However, specular
reflections of wireless signals cause poor performance of radar point clouds.
We introduce Pointillism, a system that combines data from multiple spatially
separated radars with an optimal separation to mitigate these problems. We
introduce a novel concept of Cross Potential Point Clouds, which uses the
spatial diversity induced by multiple radars and solves the problem of noise
and sparsity in radar point clouds. Furthermore, we present the design of
RP-net, a novel deep learning architecture, designed explicitly for radar's
sparse data distribution, to enable accurate 3D bounding box estimation. The
spatial techniques designed and proposed in this paper are fundamental to
radars point cloud distribution and would benefit other radar sensing
applications."
Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning,0.480341,"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset."
Decoupling Makes Weakly Supervised Local Feature Better,0.459884,"Weakly supervised learning can help local feature methods to overcome the
obstacle of acquiring a large-scale dataset with densely labeled
correspondences. However, since weak supervision cannot distinguish the losses
caused by the detection and description steps, directly conducting weakly
supervised learning within a joint describe-then-detect pipeline suffers
limited performance. In this paper, we propose a decoupled describe-then-detect
pipeline tailored for weakly supervised local feature learning. Within our
pipeline, the detection step is decoupled from the description step and
postponed until discriminative and robust descriptors are learned. In addition,
we introduce a line-to-window search strategy to explicitly use the camera pose
information for better descriptor learning. Extensive experiments show that our
method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous
fully and weakly supervised methods and achieves state-of-the-art performance
on a wide range of downstream tasks."
PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices,0.551371,"Neural radiance-density field methods have become increasingly popular for
the task of novel-view rendering. Their recent extension to hash-based
positional encoding ensures fast training and inference with visually pleasing
results. However, density-based methods struggle with recovering accurate
surface geometry. Hybrid methods alleviate this issue by optimizing the density
based on an underlying SDF. However, current SDF methods are overly smooth and
miss fine geometric details. In this work, we combine the strengths of these
two lines of work in a novel hash-based implicit surface representation. We
propose improvements to the two areas by replacing the voxel hash encoding with
a permutohedral lattice which optimizes faster, especially for higher
dimensions. We additionally propose a regularization scheme which is crucial
for recovering high-frequency geometric detail. We evaluate our method on
multiple datasets and show that we can recover geometric detail at the level of
pores and wrinkles while using only RGB images for supervision. Furthermore,
using sphere tracing we can render novel views at 30 fps on an RTX 3090. Code
is publicly available at: https://radualexandru.github.io/permuto_sdf"
Generative Cooperative Learning for Unsupervised Video Anomaly Detection,0.504041,"Video anomaly detection is well investigated in weakly-supervised and
one-class classification (OCC) settings. However, unsupervised video anomaly
detection methods are quite sparse, likely because anomalies are less frequent
in occurrence and usually not well-defined, which when coupled with the absence
of ground truth supervision, could adversely affect the performance of the
learning algorithms. This problem is challenging yet rewarding as it can
completely eradicate the costs of obtaining laborious annotations and enable
such systems to be deployed without human intervention. To this end, we propose
a novel unsupervised Generative Cooperative Learning (GCL) approach for video
anomaly detection that exploits the low frequency of anomalies towards building
a cross-supervision between a generator and a discriminator. In essence, both
networks get trained in a cooperative fashion, thereby allowing unsupervised
learning. We conduct extensive experiments on two large-scale video anomaly
detection datasets, UCF crime, and ShanghaiTech. Consistent improvement over
the existing state-of-the-art unsupervised and OCC methods corroborate the
effectiveness of our approach."
Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking,0.485456,"Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors. Recent researches start from the pretrained knowledge of language
models and take multimodal information into CSC models to improve the
performance. However, they overlook the rich knowledge in the dictionary, the
reference book where one can learn how one character should be pronounced,
written, and used. In this paper, we propose the LEAD framework, which renders
the CSC model to learn heterogeneous knowledge from the dictionary in terms of
phonetics, vision, and meaning. LEAD first constructs positive and negative
samples according to the knowledge of character phonetics, glyphs, and
definitions in the dictionary. Then a unified contrastive learning-based
training scheme is employed to refine the representations of the CSC models.
Extensive experiments and detailed analyses on the SIGHAN benchmark datasets
demonstrate the effectiveness of our proposed methods."
Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling,0.501251,"Existing research generally treats Chinese character as a minimum unit for
representation. However, such Chinese character representation will suffer two
bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich
internal features (e.g., radicals and strokes); and 2) Parameter bottleneck,
each individual character has to be represented by a unique vector. In this
paper, we introduce a novel representation method for Chinese characters to
break the bottlenecks, namely StrokeNet, which represents a Chinese character
by a Latinized stroke sequence (e.g., ""ao1 (concave)"" to ""ajaie"" and ""tu1
(convex)"" to ""aeaqe""). Specifically, StrokeNet maps each stroke to a specific
Latin character, thus allowing similar Chinese characters to have similar Latin
representations. With the introduction of StrokeNet to neural machine
translation (NMT), many powerful but not applicable techniques to non-Latin
languages (e.g., shared subword vocabulary learning and ciphertext-based data
augmentation) can now be perfectly implemented. Experiments on the widely-used
NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT
tasks show that StrokeNet can provide a significant performance boost over the
strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17
Chinese-English task which is better than any previously reported results
without using monolingual data. Code and scripts are freely available at
https://github.com/zjwang21/StrokeNet."
Evaluating Human-Language Model Interaction,0.499323,"Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation."
Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning,0.461305,"We introduce Patch Aligned Contrastive Learning (PACL), a modified
compatibility function for CLIP's contrastive loss, intending to train an
alignment between the patch tokens of the vision encoder and the CLS token of
the text encoder. With such an alignment, a model can identify regions of an
image corresponding to a given text input, and therefore transfer seamlessly to
the task of open vocabulary semantic segmentation without requiring any
segmentation annotations during training. Using pre-trained CLIP encoders with
PACL, we are able to set the state-of-the-art on the task of open vocabulary
zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC,
Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also
applicable to image-level predictions and when used with a CLIP backbone,
provides a general improvement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets."
Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness,0.488539,"A notable challenge in Multi-Document Summarization (MDS) is the
extremely-long length of the input. In this paper, we present an
extract-then-abstract Transformer framework to overcome the problem.
Specifically, we leverage pre-trained language models to construct a
hierarchical extractor for salient sentence selection across documents and an
abstractor for rewriting the selected contents as summaries. However, learning
such a framework is challenging since the optimal contents for the abstractor
are generally unknown. Previous works typically create pseudo extraction oracle
to enable the supervised learning for both the extractor and the abstractor.
Nevertheless, we argue that the performance of such methods could be restricted
due to the insufficient information for prediction and inconsistent objectives
between training and testing. To this end, we propose a loss weighting
mechanism that makes the model aware of the unequal importance for the
sentences not in the pseudo extraction oracle, and leverage the fine-tuned
abstractor to generate summary references as auxiliary signals for learning the
extractor. Moreover, we propose a reinforcement learning method that can
efficiently apply to the extractor for harmonizing the optimization between
training and testing. Experiment results show that our framework substantially
outperforms strong baselines with comparable model sizes and achieves the best
results on the Multi-News, Multi-XScience, and WikiCatSum corpora."
Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning,0.499211,"Non-exemplar class-incremental learning is to recognize both the old and new
classes when old class samples cannot be saved. It is a challenging task since
representation optimization and feature retention can only be achieved under
supervision from new classes. To address this problem, we propose a novel
self-sustaining representation expansion scheme. Our scheme consists of a
structure reorganization strategy that fuses main-branch expansion and
side-branch updating to maintain the old features, and a main-branch
distillation scheme to transfer the invariant knowledge. Furthermore, a
prototype selection mechanism is proposed to enhance the discrimination between
the old and new classes by selectively incorporating new samples into the
distillation process. Extensive experiments on three benchmarks demonstrate
significant incremental performance, outperforming the state-of-the-art methods
by a margin of 3%, 3% and 6%, respectively."
Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning,0.497836,"Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods."
Multi-modal Contrastive Representation Learning for Entity Alignment,0.522093,"Multi-modal entity alignment aims to identify equivalent entities between two
different multi-modal knowledge graphs, which consist of structural triples and
images associated with entities. Most previous works focus on how to utilize
and encode information from different modalities, while it is not trivial to
leverage multi-modal knowledge in entity alignment because of the modality
heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive
Learning based Entity Alignment model, to obtain effective joint
representations for multi-modal entity alignment. Different from previous
works, MCLEA considers task-oriented modality and models the inter-modal
relationships for each entity representation. In particular, MCLEA firstly
learns multiple individual representations from multiple modalities, and then
performs contrastive learning to jointly model intra-modal and inter-modal
interactions. Extensive experimental results show that MCLEA outperforms
state-of-the-art baselines on public datasets under both supervised and
unsupervised settings."
Unified Vision and Language Prompt Learning,0.515446,"Prompt tuning, a parameter- and data-efficient transfer learning paradigm
that tunes only a small number of parameters in a model's input space, has
become a trend in the vision community since the emergence of large
vision-language models like CLIP. We present a systematic study on two
representative prompt tuning methods, namely text prompt tuning and visual
prompt tuning. A major finding is that none of the unimodal prompt tuning
methods performs consistently well: text prompt tuning fails on data with high
intra-class visual variances while visual prompt tuning cannot handle low
inter-class variances. To combine the best from both worlds, we propose a
simple approach called Unified Prompt Tuning (UPT), which essentially learns a
tiny neural network to jointly optimize prompts across different modalities.
Extensive experiments on over 11 vision datasets show that UPT achieves a
better trade-off than the unimodal counterparts on few-shot learning
benchmarks, as well as on domain generalization benchmarks. Code and models
will be released to facilitate future research."
Semantic Image Synthesis via Diffusion Models,0.490584,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS)."
Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature,0.465856,"Literary translation is a culturally significant task, but it is bottlenecked
by the small number of qualified literary translators relative to the many
untranslated works published around the world. Machine translation (MT) holds
potential to complement the work of human translators by improving both
training procedures and their overall efficiency. Literary translation is less
constrained than more traditional MT settings since translators must balance
meaning equivalence, readability, and critical interpretability in the target
language. This property, along with the complex discourse-level context present
in literary texts, also makes literary MT more challenging to computationally
model and evaluate. To explore this task, we collect a dataset (Par3) of
non-English language novels in the public domain, each aligned at the paragraph
level to both human and automatic English translations. Using Par3, we discover
that expert literary translators prefer reference human translations over
machine-translated paragraphs at a rate of 84%, while state-of-the-art
automatic MT metrics do not correlate with those preferences. The experts note
that MT outputs contain not only mistranslations, but also discourse-disrupting
errors and stylistic inconsistencies. To address these problems, we train a
post-editing model whose output is preferred over normal MT output at a rate of
69% by experts. We publicly release Par3 at
https://github.com/katherinethai/par3/ to spur future research into literary
MT."
"A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games",0.47237,"This work studies an algorithm, which we call magnetic mirror descent, that
is inspired by mirror descent and the non-Euclidean proximal gradient
algorithm. Our contribution is demonstrating the virtues of magnetic mirror
descent as both an equilibrium solver and as an approach to reinforcement
learning in two-player zero-sum games. These virtues include: 1) Being the
first quantal response equilibria solver to achieve linear convergence for
extensive-form games with first order feedback; 2) Being the first standard
reinforcement learning algorithm to achieve empirically competitive results
with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark
Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning
algorithm."
Dataless Knowledge Fusion by Merging Weights of Language Models,0.528632,"Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios."
StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,0.547094,"Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance."
Rebellion and Disobedience as Useful Tools in Human-Robot Interaction Research -- The Handheld Robotics Case,0.527633,"This position paper argues on the utility of rebellion and disobedience (RaD)
in human-robot interaction (HRI). In general, we see two main opportunities in
the use of controlled and well designed rebellion and disobedience: i)
illuminate insight into the effectiveness of the collaboration (or lack of) and
ii) prevent mistakes and correct user actions when in the user's own interest.
Through the use of a close interaction modality, that of handheld robots, we
discuss use cases for utility of rebellion and disobedience that can be
applicable to other instances of HRI."
CORL: Research-oriented Deep Offline Reinforcement Learning Library,0.482613,"CORL is an open-source library that provides thoroughly benchmarked
single-file implementations of both deep offline and offline-to-online
reinforcement learning algorithms. It emphasizes a simple developing experience
with a straightforward codebase and a modern analysis tracking tool. In CORL,
we isolate methods implementation into separate single files, making
performance-relevant details easier to recognize. Additionally, an experiment
tracking feature is available to help log metrics, hyperparameters,
dependencies, and more to the cloud. Finally, we have ensured the reliability
of the implementations by benchmarking commonly employed D4RL datasets
providing a transparent source of results that can be reused for robust
evaluation tools such as performance profiles, probability of improvement, or
expected online performance."
Mix and Localize: Localizing Sound Sources in Mixtures,0.454825,"We present a method for simultaneously localizing multiple sound sources
within a visual scene. This task requires a model to both group a sound mixture
into individual sources, and to associate them with a visual signal. Our method
jointly solves both tasks at once, using a formulation inspired by the
contrastive random walk of Jabri et al. We create a graph in which images and
separated sounds correspond to nodes, and train a random walker to transition
between nodes from different modalities with high return probability. The
transition probabilities for this walk are determined by an audio-visual
similarity metric that is learned by our model. We show through experiments
with musical instruments and human speech that our model can successfully
localize multiple sounds, outperforming other self-supervised methods. Project
site: https://hxixixh.github.io/mix-and-localize"
Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages,0.449836,"Human languages are full of metaphorical expressions. Metaphors help people
understand the world by connecting new concepts and domains to more familiar
ones. Large pre-trained language models (PLMs) are therefore assumed to encode
metaphorical knowledge useful for NLP systems. In this paper, we investigate
this hypothesis for PLMs, by probing metaphoricity information in their
encodings, and by measuring the cross-lingual and cross-dataset generalization
of this information. We present studies in multiple metaphor detection datasets
and in four languages (i.e., English, Spanish, Russian, and Farsi). Our
extensive experiments suggest that contextual representations in PLMs do encode
metaphorical knowledge, and mostly in their middle layers. The knowledge is
transferable between languages and datasets, especially when the annotation is
consistent across training and testing sets. Our findings give helpful insights
for both cognitive and NLP scientists."
Speciesist Language and Nonhuman Animal Bias in English Masked Language Models,0.453595,"Various existing studies have analyzed what social biases are inherited by
NLP models. These biases may directly or indirectly harm people, therefore
previous studies have focused only on human attributes. However, until recently
no research on social biases in NLP regarding nonhumans existed. In this paper,
we analyze biases to nonhuman animals, i.e. speciesist bias, inherent in
English Masked Language Models such as BERT. We analyzed speciesist bias
against 46 animal names using template-based and corpus-extracted sentences
containing speciesist (or non-speciesist) language. We found that pre-trained
masked language models tend to associate harmful words with nonhuman animals
and have a bias toward using speciesist language for some nonhuman animal
names. Our code for reproducing the experiments will be made available on
GitHub."
A Distributional Lens for Multi-Aspect Controllable Text Generation,0.498208,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach."
Imagination-Augmented Natural Language Understanding,0.486221,"Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances."
Stubborn: A Strong Baseline for Indoor Object Navigation,0.552096,"We present a strong baseline that surpasses the performance of previously
published methods on the Habitat Challenge task of navigating to a target
object in indoor environments. Our method is motivated from primary failure
modes of prior state-of-the-art: poor exploration, inaccurate object
identification, and agent getting trapped due to imprecise map construction. We
make three contributions to mitigate these issues: (i) First, we show that
existing map-based methods fail to effectively use semantic clues for
exploration. We present a semantic-agnostic exploration strategy (called
Stubborn) without any learning that surprisingly outperforms prior work. (ii)
We propose a strategy for integrating temporal information to improve object
identification. (iii) Lastly, due to inaccurate depth observation the agent
often gets trapped in small regions. We develop a multi-scale collision map for
obstacle identification that mitigates this issue."
Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation,0.517115,"A diffusion model learns to predict a vector field of gradients. We propose
to apply chain rule on the learned gradients, and back-propagate the score of a
diffusion model through the Jacobian of a differentiable renderer, which we
instantiate to be a voxel radiance field. This setup aggregates 2D scores at
multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D
model for 3D data generation. We identify a technical challenge of distribution
mismatch that arises in this application, and propose a novel estimation
mechanism to resolve it. We run our algorithm on several off-the-shelf
diffusion image generative models, including the recently released Stable
Diffusion trained on the large-scale LAION dataset."
AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach,0.515258,"Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging."
DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,0.549166,"With the ever-growing size of pretrained models (PMs), fine-tuning them has
become more expensive and resource-hungry. As a remedy, low-rank adapters
(LoRA) keep the main pretrained weights of the model frozen and just introduce
some learnable truncated SVD modules (so-called LoRA blocks) to the model.
While LoRA blocks are parameter-efficient, they suffer from two major problems:
first, the size of these blocks is fixed and cannot be modified after training
(for example, if we need to change the rank of LoRA blocks, then we need to
re-train them from scratch); second, optimizing their rank requires an
exhaustive search and effort. In this work, we introduce a dynamic low-rank
adaptation (DyLoRA) technique to address these two problems together. Our
DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank
by sorting the representation learned by the adapter module at different ranks
during training. We evaluate our solution on different natural language
understanding (GLUE benchmark) and language generation tasks (E2E, DART and
WebNLG) using different pretrained models such as RoBERTa and GPT with
different sizes. Our results show that we can train dynamic search-free models
with DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA
without significantly compromising performance. Moreover, our models can
perform consistently well on a much larger range of ranks compared to LoRA."
CrackSeg9k: A Collection and Benchmark for Crack Segmentation Datasets and Frameworks,0.524288,"The detection of cracks is a crucial task in monitoring structural health and
ensuring structural safety. The manual process of crack detection is
time-consuming and subjective to the inspectors. Several researchers have tried
tackling this problem using traditional Image Processing or learning-based
techniques. However, their scope of work is limited to detecting cracks on a
single type of surface (walls, pavements, glass, etc.). The metrics used to
evaluate these methods are also varied across the literature, making it
challenging to compare techniques. This paper addresses these problems by
combining previously available datasets and unifying the annotations by
tackling the inherent problems within each dataset, such as noise and
distortions. We also present a pipeline that combines Image Processing and Deep
Learning models. Finally, we benchmark the results of proposed models on these
metrics on our new dataset and compare them with state-of-the-art models in the
literature."
Interpretability for Language Learners Using Example-Based Grammatical Error Correction,0.451257,"Grammatical Error Correction (GEC) should not focus only on high accuracy of
corrections but also on interpretability for language learning. However,
existing neural-based GEC models mainly aim at improving accuracy, and their
interpretability has not been explored. A promising approach for improving
interpretability is an example-based method, which uses similar retrieved
examples to generate corrections. In addition, examples are beneficial in
language learning, helping learners understand the basis of grammatically
incorrect/correct texts and improve their confidence in writing. Therefore, we
hypothesize that incorporating an example-based method into GEC can improve
interpretability as well as support language learners. In this study, we
introduce an Example-Based GEC (EB-GEC) that presents examples to language
learners as a basis for a correction result. The examples consist of pairs of
correct and incorrect sentences similar to a given input and its predicted
correction. Experiments demonstrate that the examples presented by EB-GEC help
language learners decide to accept or refuse suggestions from the GEC output.
Furthermore, the experiments also show that retrieved examples improve the
accuracy of corrections."
CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification,0.532568,"Existing computer vision research in artwork struggles with artwork's
fine-grained attributes recognition and lack of curated annotated datasets due
to their costly creation. To the best of our knowledge, we are one of the first
methods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural
network on a variety of artwork images and text descriptions pairs. CLIP is
able to learn directly from free-form art descriptions, or, if available,
curated fine-grained labels. Model's zero-shot capability allows predicting
accurate natural language description for a given image, without directly
optimizing for the task. Our approach aims to solve 2 challenges: instance
retrieval and fine-grained artwork attribute recognition. We use the iMet
Dataset, which we consider the largest annotated artwork dataset. In this
benchmark we achieved competitive results using only self-supervision."
SpA-Former: Transformer image shadow detection and removal via spatial attention,0.451937,"In this paper, we propose an end-to-end SpA-Former to recover a shadow-free
image from a single shaded image. Unlike traditional methods that require two
steps for shadow detection and then shadow removal, the SpA-Former unifies
these steps into one, which is a one-stage network capable of directly learning
the mapping function between shadows and no shadows, it does not require a
separate shadow detection. Thus, SpA-former is adaptable to real image
de-shadowing for shadows projected on different semantic regions. SpA-Former
consists of transformer layer and a series of joint Fourier transform residual
blocks and two-wheel joint spatial attention. The network in this paper is able
to handle the task while achieving a very fast processing efficiency.
  Our code is relased on
https://github.com/zhangbaijin/SpA-Former-shadow-removal"
Wireless Ad Hoc Federated Learning: A Fully Distributed Cooperative Machine Learning,0.508806,"Privacy-sensitive data is stored in autonomous vehicles, smart devices, or
sensor nodes that can move around with making opportunistic contact with each
other. Federation among such nodes was mainly discussed in the context of
federated learning with a centralized mechanism in many works. However, because
of multi-vendor issues, those nodes do not want to rely on a specific server
operated by a third party for this purpose. In this paper, we propose a
wireless ad hoc federated learning (WAFL) -- a fully distributed cooperative
machine learning organized by the nodes physically nearby. WAFL can develop
generalized models from Non-IID datasets stored in distributed nodes locally by
exchanging and aggregating them with each other over opportunistic node-to-node
contacts. In our benchmark-based evaluation with various opportunistic
networks, WAFL has achieved higher accuracy of 94.8-96.3% than the
self-training case of 84.7%. All our evaluation results show that WAFL can
train and converge the model parameters from highly-partitioned Non-IID
datasets over opportunistic networks without any centralized mechanisms."
Concrete Score Matching: Generalized Score Matching for Discrete Data,0.516264,"Representing probability distributions by the gradient of their density
functions has proven effective in modeling a wide range of continuous data
modalities. However, this representation is not applicable in discrete domains
where the gradient is undefined. To this end, we propose an analogous score
function called the ""Concrete score"", a generalization of the (Stein) score for
discrete settings. Given a predefined neighborhood structure, the Concrete
score of any input is defined by the rate of change of the probabilities with
respect to local directional changes of the input. This formulation allows us
to recover the (Stein) score in continuous domains when measuring such changes
by the Euclidean distance, while using the Manhattan distance leads to our
novel score function in discrete domains. Finally, we introduce a new framework
to learn such scores from samples called Concrete Score Matching (CSM), and
propose an efficient training objective to scale our approach to high
dimensions. Empirically, we demonstrate the efficacy of CSM on density
estimation tasks on a mixture of synthetic, tabular, and high-dimensional image
datasets, and demonstrate that it performs favorably relative to existing
baselines for modeling discrete data."
Towards High-Fidelity Single-view Holistic Reconstruction of Indoor Scenes,0.448684,"We present a new framework to reconstruct holistic 3D indoor scenes including
both room background and indoor objects from single-view images. Existing
methods can only produce 3D shapes of indoor objects with limited geometry
quality because of the heavy occlusion of indoor scenes. To solve this, we
propose an instance-aligned implicit function (InstPIFu) for detailed object
reconstruction. Combining with instance-aligned attention module, our method is
empowered to decouple mixed local features toward the occluded instances.
Additionally, unlike previous methods that simply represents the room
background as a 3D bounding box, depth map or a set of planes, we recover the
fine geometry of the background via implicit representation. Extensive
experiments on the SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets
demonstrate that our method outperforms existing approaches in both background
and foreground object reconstruction. Our code and model will be made publicly
available."
Compositional Semantic Parsing with Large Language Models,0.516877,"Humans can reason compositionally when presented with new tasks. Previous
research shows that appropriate prompting techniques enable large language
models (LLMs) to solve artificial compositional generalization tasks such as
SCAN. In this work, we identify additional challenges in more realistic
semantic parsing tasks with larger vocabulary and refine these prompting
techniques to address them. Our best method is based on least-to-most
prompting: it decomposes the problem using prompting-based syntactic parsing,
then uses this decomposition to select appropriate exemplars and to
sequentially generate the semantic parse. This method allows us to set a new
state of the art for CFQ while requiring only 1% of the training data used by
traditional approaches. Due to the general nature of our approach, we expect
similar efforts will lead to new results in other tasks and domains, especially
for knowledge-intensive applications."
Goal-Conditioned Reinforcement Learning: Problems and Solutions,0.457539,"Goal-conditioned reinforcement learning (GCRL), related to a set of complex
RL problems, trains an agent to achieve different goals under particular
scenarios. Compared to the standard RL solutions that learn a policy solely
depending on the states or observations, GCRL additionally requires the agent
to make decisions according to different goals. In this survey, we provide a
comprehensive overview of the challenges and algorithms for GCRL. Firstly, we
answer what the basic problems are studied in this field. Then, we explain how
goals are represented and present how existing solutions are designed from
different points of view. Finally, we make the conclusion and discuss potential
future prospects that recent researches focus on."
Combining Attention Module and Pixel Shuffle for License Plate Super-Resolution,0.495009,"The License Plate Recognition (LPR) field has made impressive advances in the
last decade due to novel deep learning approaches combined with the increased
availability of training data. However, it still has some open issues,
especially when the data come from low-resolution (LR) and low-quality
images/videos, as in surveillance systems. This work focuses on license plate
(LP) reconstruction in LR and low-quality images. We present a Single-Image
Super-Resolution (SISR) approach that extends the attention/transformer module
concept by exploiting the capabilities of PixelShuffle layers and that has an
improved loss function based on LPR predictions. For training the proposed
architecture, we use synthetic images generated by applying heavy Gaussian
noise in terms of Structural Similarity Index Measure (SSIM) to the original
high-resolution (HR) images. In our experiments, the proposed method
outperformed the baselines both quantitatively and qualitatively. The datasets
we created for this work are publicly available to the research community at
https://github.com/valfride/lpr-rsr/"
Continual Learning Based on OOD Detection and Task Masking,0.514738,"Existing continual learning techniques focus on either task incremental
learning (TIL) or class incremental learning (CIL) problem, but not both. CIL
and TIL differ mainly in that the task-id is provided for each test sample
during testing for TIL, but not provided for CIL. Continual learning methods
intended for one problem have limitations on the other problem. This paper
proposes a novel unified approach based on out-of-distribution (OOD) detection
and task masking, called CLOM, to solve both problems. The key novelty is that
each task is trained as an OOD detection model rather than a traditional
supervised learning model, and a task mask is trained to protect each task to
prevent forgetting. Our evaluation shows that CLOM outperforms existing
state-of-the-art baselines by large margins. The average TIL/CIL accuracy of
CLOM over six experiments is 87.6/67.9% while that of the best baselines is
only 82.4/55.0%."
Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning,0.477596,"Prior works on action representation learning mainly focus on designing
various architectures to extract the global representations for short video
clips. In contrast, many practical applications such as video alignment have
strong demand for learning dense representations for long videos. In this
paper, we introduce a novel contrastive action representation learning (CARL)
framework to learn frame-wise action representations, especially for long
videos, in a self-supervised manner. Concretely, we introduce a simple yet
efficient video encoder that considers spatio-temporal context to extract
frame-wise representations. Inspired by the recent progress of self-supervised
learning, we present a novel sequence contrastive loss (SCL) applied on two
correlated views obtained through a series of spatio-temporal data
augmentations. SCL optimizes the embedding space by minimizing the
KL-divergence between the sequence similarity of two augmented views and a
prior Gaussian distribution of timestamp distance. Experiments on FineGym,
PennAction and Pouring datasets show that our method outperforms previous
state-of-the-art by a large margin for downstream fine-grained action
classification. Surprisingly, although without training on paired videos, our
approach also shows outstanding performance on video alignment and fine-grained
frame retrieval tasks. Code and models are available at
https://github.com/minghchen/CARL_code."
Learning Appearance-motion Normality for Video Anomaly Detection,0.470494,"Video anomaly detection is a challenging task in the computer vision
community. Most single task-based methods do not consider the independence of
unique spatial and temporal patterns, while two-stream structures lack the
exploration of the correlations. In this paper, we propose spatial-temporal
memories augmented two-stream auto-encoder framework, which learns the
appearance normality and motion normality independently and explores the
correlations via adversarial learning. Specifically, we first design two proxy
tasks to train the two-stream structure to extract appearance and motion
features in isolation. Then, the prototypical features are recorded in the
corresponding spatial and temporal memory pools. Finally, the encoding-decoding
network performs adversarial learning with the discriminator to explore the
correlations between spatial and temporal patterns. Experimental results show
that our framework outperforms the state-of-the-art methods, achieving AUCs of
98.1% and 89.8% on UCSD Ped2 and CUHK Avenue datasets."
Sparse2Dense: Learning to Densify 3D Features for 3D Object Detection,0.555502,"LiDAR-produced point clouds are the major source for most state-of-the-art 3D
object detectors. Yet, small, distant, and incomplete objects with sparse or
few points are often hard to detect. We present Sparse2Dense, a new framework
to efficiently boost 3D detection performance by learning to densify point
clouds in latent space. Specifically, we first train a dense point 3D detector
(DDet) with a dense point cloud as input and design a sparse point 3D detector
(SDet) with a regular point cloud as input. Importantly, we formulate the
lightweight plug-in S2D module and the point cloud reconstruction module in
SDet to densify 3D features and train SDet to produce 3D features, following
the dense 3D features in DDet. So, in inference, SDet can simulate dense 3D
features from regular (sparse) point cloud inputs without requiring dense
inputs. We evaluate our method on the large-scale Waymo Open Dataset and the
Waymo Domain Adaptation Dataset, showing its high performance and efficiency
over the state of the arts."
Semantic-Oriented Unlabeled Priming for Large-Scale Language Models,0.495175,"Due to the high costs associated with finetuning large language models,
various recent works propose to adapt them to specific tasks without any
parameter updates through in-context learning. Unfortunately, for in-context
learning there is currently no way to leverage unlabeled data, which is often
much easier to obtain in large quantities than labeled examples. In this work,
we therefore investigate ways to make use of unlabeled examples to improve the
zero-shot performance of pretrained language models without any finetuning: We
introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies
examples by retrieving semantically similar unlabeled examples, assigning
labels to them in a zero-shot fashion, and then using them for in-context
learning. We also propose bag-of-contexts priming, a new priming strategy that
is more suitable for our setting and enables the usage of more examples than
fit into the context window."
Realistic Blur Synthesis for Learning Image Deblurring,0.490688,"Training learning-based deblurring methods demands a tremendous amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and deblurring models trained on them cannot handle real
blurred images effectively. While real datasets have recently been proposed,
they provide limited diversity of scenes and camera settings, and capturing
real datasets for diverse settings is still challenging. To resolve this, this
paper analyzes various factors that introduce differences between real and
synthetic blurred images. To this end, we present RSBlur, a novel dataset with
real blurred images and the corresponding sharp image sequences to enable a
detailed analysis of the difference between real and synthetic blur. With the
dataset, we reveal the effects of different factors in the blur generation
process. Based on the analysis, we also present a novel blur synthesis pipeline
to synthesize more realistic blur. We show that our synthesis pipeline can
improve the deblurring performance on real blurred images."
DeepInteraction: 3D Object Detection via Modality Interaction,0.487218,"Existing top-performance 3D object detectors typically rely on the
multi-modal fusion strategy. This design is however fundamentally restricted
due to overlooking the modality-specific useful information and finally
hampering the model performance. To address this limitation, in this work we
introduce a novel modality interaction strategy where individual per-modality
representations are learned and maintained throughout for enabling their unique
characteristics to be exploited during object detection. To realize this
proposed strategy, we design a DeepInteraction architecture characterized by a
multi-modal representational interaction encoder and a multi-modal predictive
interaction decoder. Experiments on the large-scale nuScenes dataset show that
our proposed method surpasses all prior arts often by a large margin.
Crucially, our method is ranked at the first position at the highly competitive
nuScenes object detection leaderboard."
Super forecasting the technological singularity risks from artificial intelligence,0.474144,"The article forecasts emerging cyber-risks from the integration of AI in
cybersecurity."
ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering,0.533581,"With the recent advance in large pre-trained language models, researchers
have achieved record performances in NLP tasks that mostly focus on language
pattern matching. The community is experiencing the shift of the challenge from
how to model language to the imitation of complex reasoning abilities like
human beings. In this work, we investigate the application domain of finance
that involves real-world, complex numerical reasoning. We propose a new
large-scale dataset, ConvFinQA, aiming to study the chain of numerical
reasoning in conversational question answering. Our dataset poses great
challenge in modeling long-range, complex numerical reasoning paths in
real-world conversations. We conduct comprehensive experiments and analyses
with both the neural symbolic methods and the prompting-based methods, to
provide insights into the reasoning mechanisms of these two divisions. We
believe our new dataset should serve as a valuable resource to push forward the
exploration of real-world, complex reasoning tasks as the next research focus.
Our dataset and code is publicly available at
https://github.com/czyssrs/ConvFinQA."
CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation,0.52123,"We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based
framework for panoptic segmentation designed around clustering. It rethinks the
existing transformer architectures used in segmentation and detection;
CMT-DeepLab considers the object queries as cluster centers, which fill the
role of grouping the pixels when applied to segmentation. The clustering is
computed with an alternating procedure, by first assigning pixels to the
clusters by their feature affinity, and then updating the cluster centers and
pixel features. Together, these operations comprise the Clustering Mask
Transformer (CMT) layer, which produces cross-attention that is denser and more
consistent with the final segmentation task. CMT-DeepLab improves the
performance over prior art significantly by 4.4% PQ, achieving a new
state-of-the-art of 55.7% PQ on the COCO test-dev set."
Large Language Models and the Reverse Turing Test,0.514337,"Large Language Models (LLMs) have been transformative. They are pre-trained
foundational models that are self-supervised and can be adapted with fine
tuning to a wide range of natural language tasks, each of which previously
would have required a separate network model. This is one step closer to the
extraordinary versatility of human language. GPT-3 and more recently LaMDA can
carry on dialogs with humans on many topics after minimal priming with a few
examples. However, there has been a wide range of reactions and debate on
whether these LLMs understand what they are saying or exhibit signs of
intelligence. This high variance is exhibited in three interviews with LLMs
reaching wildly different conclusions. A new possibility was uncovered that
could explain this divergence. What appears to be intelligence in LLMs may in
fact be a mirror that reflects the intelligence of the interviewer, a
remarkable twist that could be considered a Reverse Turing Test. If so, then by
studying interviews we may be learning more about the intelligence and beliefs
of the interviewer than the intelligence of the LLMs. As LLMs become more
capable they may transform the way we interact with machines and how they
interact with each other. Increasingly, LLMs are being coupled with
sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A
road map for achieving artificial general autonomy is outlined with seven major
improvements inspired by brain systems. LLMs could be used to uncover new
insights into brain function by downloading brain data during natural
behaviors."
Multi-View Document Representation Learning for Open-Domain Dense Retrieval,0.496007,"Dense retrieval has achieved impressive advances in first-stage retrieval
from a large-scale document collection, which is built on bi-encoder
architecture to produce single vector representation of query and document.
However, a document can usually answer multiple potential queries from
different views. So the single vector representation of a document is hard to
match with multi-view queries, and faces a semantic mismatch problem. This
paper proposes a multi-view document representation learning framework, aiming
to produce multi-view embeddings to represent documents and enforce them to
align with different queries. First, we propose a simple yet effective method
of generating multiple embeddings through viewers. Second, to prevent
multi-view embeddings from collapsing to the same one, we further propose a
global-local loss with annealed temperature to encourage the multiple viewers
to better align with different potential queries. Experiments show our method
outperforms recent works and achieves state-of-the-art results."
RuCoLA: Russian Corpus of Linguistic Acceptability,0.456741,"Linguistic acceptability (LA) attracts the attention of the research
community due to its many uses, such as testing the grammatical knowledge of
language models and filtering implausible texts with acceptability classifiers.
However, the application scope of LA in languages other than English is limited
due to the lack of high-quality resources. To this end, we introduce the
Russian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up
under the well-established binary LA approach. RuCoLA consists of $9.8$k
in-domain sentences from linguistic publications and $3.6$k out-of-domain
sentences produced by generative models. The out-of-domain set is created to
facilitate the practical use of acceptability for improving language
generation. Our paper describes the data collection protocol and presents a
fine-grained analysis of acceptability classification experiments with a range
of baseline approaches. In particular, we demonstrate that the most widely used
language models still fall behind humans by a large margin, especially when
detecting morphological and semantic errors. We release RuCoLA, the code of
experiments, and a public leaderboard (rucola-benchmark.com) to assess the
linguistic competence of language models for Russian."
Splicing Detection and Localization In Satellite Imagery Using Conditional GANs,0.494439,"The widespread availability of image editing tools and improvements in image
processing techniques allow image manipulation to be very easy. Oftentimes,
easy-to-use yet sophisticated image manipulation tools yields
distortions/changes imperceptible to the human observer. Distribution of forged
images can have drastic ramifications, especially when coupled with the speed
and vastness of the Internet. Therefore, verifying image integrity poses an
immense and important challenge to the digital forensic community. Satellite
images specifically can be modified in a number of ways, including the
insertion of objects to hide existing scenes and structures. In this paper, we
describe the use of a Conditional Generative Adversarial Network (cGAN) to
identify the presence of such spliced forgeries within satellite images.
Additionally, we identify their locations and shapes. Trained on pristine and
falsified images, our method achieves high success on these detection and
localization objectives."
ALT: um software para anlise de legibilidade de textos em Lngua Portuguesa,0.49958,"In the initial stage of human life, communication, seen as a process of
social interaction, was always the best way to reach consensus between the
parties. Understanding and credibility in this process are essential for the
mutual agreement to be validated. But, how to do it so that this communication
reaches the great mass? This is the main challenge when what is sought is the
dissemination of information and its approval. In this context, this study
presents the ALT software, developed from original readability metrics adapted
to the Portuguese language, available on the web, to reduce communication
difficulties. The development of the software was motivated by the theory of
communicative action of Habermas, which uses a multidisciplinary style to
measure the credibility of the discourse in the communication channels used to
build and maintain a safe and healthy relationship with the public.
  --
  No est\'agio inicial da vida humana a comunica\c{c}\~ao, vista como um
processo de intera\c{c}\~ao social, foi sempre o melhor caminho para o consenso
entre as partes. O entendimento e a credibilidade nesse processo s\~ao
fundamentais para que o acordo m\'utuo seja validado. Mas, como faz\^e-lo de
forma que essa comunica\c{c}\~ao alcance a grande massa? Esse \'e o principal
desafio quando o que se busca \'e a difus\~ao da informa\c{c}\~ao e a sua
aprova\c{c}\~ao. Nesse contexto, este estudo apresenta o software ALT,
desenvolvido a partir de m\'etricas de legibilidade originais adaptadas para a
L\'ingua Portuguesa, dispon\'ivel na web, para reduzir as dificuldades na
comunica\c{c}\~ao. O desenvolvimento do software foi motivado pela teoria do
agir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para
medir a credibilidade do discurso nos canais de comunica\c{c}\~ao utilizados
para construir e manter uma rela\c{c}\~ao segura e saud\'avel com o p\'ublico."
Understanding Translationese in Cross-Lingual Summarization,0.476648,"Given a document in a source language, cross-lingual summarization (CLS) aims
at generating a concise summary in a different target language. Unlike
monolingual summarization (MS), naturally occurring source-language documents
paired with target-language summaries are rare. To collect large-scale CLS
data, existing datasets typically involve translation in their creation.
However, the translated text is distinguished from the text originally written
in that language, i.e., translationese. In this paper, we first confirm that
different approaches of constructing CLS datasets will lead to different
degrees of translationese. Then we systematically investigate how
translationese affects CLS model evaluation and performance when it appears in
source documents or target summaries. In detail, we find that (1) the
translationese in documents or summaries of test sets might lead to the
discrepancy between human judgment and automatic evaluation; (2) the
translationese in training sets would harm model performance in real-world
applications; (3) though machine-translated documents involve translationese,
they are very useful for building CLS systems on low-resource languages under
specific training strategies. Lastly, we give suggestions for future CLS
research including dataset and model developments. We hope that our work could
let researchers notice the phenomenon of translationese in CLS and take it into
account in the future."
Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation,0.541482,"Non-autoregressive translation (NAT) models are typically trained with the
cross-entropy loss, which forces the model outputs to be aligned verbatim with
the target sentence and will highly penalize small shifts in word positions.
Latent alignment models relax the explicit alignment by marginalizing out all
monotonic latent alignments with the CTC loss. However, they cannot handle
non-monotonic alignments, which is non-negligible as there is typically global
word reordering in machine translation. In this work, we explore non-monotonic
latent alignments for NAT. We extend the alignment space to non-monotonic
alignments to allow for the global word reordering and further consider all
alignments that overlap with the target sentence. We non-monotonically match
the alignments to the target sentence and train the latent alignment model to
maximize the F1 score of non-monotonic matching. Extensive experiments on major
WMT benchmarks show that our method substantially improves the translation
performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14
En-De with only one-iteration decoding, closing the gap between
non-autoregressive and autoregressive models."
Learning Optical Flow with Adaptive Graph Reasoning,0.530401,"Estimating per-pixel motion between video frames, known as optical flow, is a
long-standing problem in video understanding and analysis. Most contemporary
optical flow techniques largely focus on addressing the cross-image matching
with feature similarity, with few methods considering how to explicitly reason
over the given scene for achieving a holistic motion understanding. In this
work, taking a fresh perspective, we introduce a novel graph-based approach,
called adaptive graph reasoning for optical flow (AGFlow), to emphasize the
value of scene/context information in optical flow. Our key idea is to decouple
the context reasoning from the matching procedure, and exploit scene
information to effectively assist motion estimation by learning to reason over
the adaptive graph. The proposed AGFlow can effectively exploit the context
information and incorporate it within the matching procedure, producing more
robust and accurate results. On both Sintel clean and final passes, our AGFlow
achieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming
state-of-the-art approaches by 11.2% and 13.6%, respectively."
Memory-Based Model Editing at Scale,0.490758,"Even the largest neural networks make errors, and once-correct predictions
can become invalid as the world changes. Model editors make local updates to
the behavior of base (pre-trained) models to inject updated knowledge or
correct undesirable behaviors. Existing model editors have shown promise, but
also suffer from insufficient expressiveness: they struggle to accurately model
an edit's intended scope (examples affected by the edit), leading to inaccurate
predictions for test inputs loosely related to the edit, and they often fail
altogether after many edits. As a higher-capacity alternative, we propose
Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model
(SERAC), which stores edits in an explicit memory and learns to reason over
them to modulate the base model's predictions as needed. To enable more
rigorous evaluation of model editors, we introduce three challenging language
model editing problems based on question answering, fact-checking, and dialogue
generation. We find that only SERAC achieves high performance on all three
problems, consistently outperforming existing approaches to model editing by a
significant margin. Code, data, and additional project information will be made
available at https://sites.google.com/view/serac-editing."
Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization,0.552302,"The most advanced abstractive dialogue summarizers lack generalization
ability on new domains and the existing researches for domain adaptation in
summarization generally rely on large-scale pre-trainings. To explore the
lightweight fine-tuning methods for domain adaptation of dialogue
summarization, in this paper, we propose an efficient and generalizable
Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized
prefix module to alleviate domain entanglement and adopts discrete prompts to
guide the model to focus on key contents of dialogues and enhance model
generalization. We conduct zero-shot experiments and build domain adaptation
benchmarks on two multi-domain dialogue summarization datasets, TODSum and
QMSum. Adequate experiments and qualitative analysis prove the effectiveness of
our methods."
TransLog: A Unified Transformer-based Framework for Log Anomaly Detection,0.444545,"Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios especially for low-resource domains. However,
previous deep models merely focused on extracting the semantics of log sequence
in the same domain, leading to poor generalization on multi-domain logs.
Therefore, we propose a unified Transformer-based framework for log anomaly
detection (\ourmethod{}), which is comprised of the pretraining and
adapter-based tuning stage. Our model is first pretrained on the source domain
to obtain shared semantic knowledge of log data. Then, we transfer the
pretrained model to the target domain via the adapter-based tuning. The
proposed method is evaluated on three public datasets including one source
domain and two target domains. The experimental results demonstrate that our
simple yet efficient approach, with fewer trainable parameters and lower
training costs in the target domain, achieves state-of-the-art performance on
three benchmarks."
MHMS: Multimodal Hierarchical Multimedia Summarization,0.531967,"Multimedia summarization with multimodal output can play an essential role in
real-world applications, i.e., automatically generating cover images and titles
for news articles or providing introductions to online videos. In this work, we
propose a multimodal hierarchical multimedia summarization (MHMS) framework by
interacting visual and language domains to generate both video and textual
summaries. Our MHMS method contains video and textual segmentation and
summarization module, respectively. It formulates a cross-domain alignment
objective with optimal transport distance which leverages cross-domain
interaction to generate the representative keyframe and textual summary. We
evaluated MHMS on three recent multimodal datasets and demonstrated the
effectiveness of our method in producing high-quality multimodal summaries."
DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech,0.480815,"The majority of current Text-to-Speech (TTS) datasets, which are collections
of individual utterances, contain few conversational aspects. In this paper, we
introduce DailyTalk, a high-quality conversational speech dataset designed for
conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the
open-domain dialogue dataset DailyDialog inheriting its annotated attributes.
On top of our dataset, we extend prior work as our baseline, where a
non-autoregressive TTS is conditioned on historical information in a dialogue.
From the baseline experiment with both general and our novel metrics, we show
that DailyTalk can be used as a general TTS dataset, and more than that, our
baseline can represent contextual information from DailyTalk. The DailyTalk
dataset and baseline code are freely available for academic use with CC-BY-SA
4.0 license."
Pre-training to Match for Unified Low-shot Relation Extraction,0.536421,"Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard."
Accelerating Code Search with Deep Hashing and Code Classification,0.49958,"Code search is to search reusable code snippets from source code corpus based
on natural languages queries. Deep learning-based methods of code search have
shown promising results. However, previous methods focus on retrieval accuracy
but lacked attention to the efficiency of the retrieval process. We propose a
novel method CoSHC to accelerate code search with deep hashing and code
classification, aiming to perform an efficient code search without sacrificing
too much accuracy. To evaluate the effectiveness of CoSHC, we apply our method
to five code search models. Extensive experimental results indicate that
compared with previous code search baselines, CoSHC can save more than 90% of
retrieval time meanwhile preserving at least 99% of retrieval accuracy."
MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,0.468576,"Visual language data such as plots, charts, and infographics are ubiquitous
in the human world. However, state-of-the-art vision-language models do not
perform well on these data. We propose MatCha (Math reasoning and Chart
derendering pretraining) to enhance visual language models' capabilities in
jointly modeling charts/plots and language data. Specifically, we propose
several pretraining tasks that cover plot deconstruction and numerical
reasoning which are the key capabilities in visual language modeling.
  We perform the MatCha pretraining starting from Pix2Struct, a recently
proposed image-to-text visual language model. On standard benchmarks such as
PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as
much as nearly 20%. We also examine how well MatCha pretraining transfers to
domains such as screenshots, textbook diagrams, and document figures and
observe overall improvement, verifying the usefulness of MatCha pretraining on
broader visual language tasks."
CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022,0.473537,"In this paper, we describe our submission to the Simultaneous Speech
Translation at IWSLT 2022. We explore strategies to utilize an offline model in
a simultaneous setting without the need to modify the original model. In our
experiments, we show that our onlinization algorithm is almost on par with the
offline setting while being $3\times$ faster than offline in terms of latency
on the test set. We also show that the onlinized offline model outperforms the
best IWSLT2021 simultaneous system in medium and high latency regimes and is
almost on par in the low latency regime. We make our system publicly available."
Experiments on Generalizability of BERTopic on Multi-Domain Short Text,0.518368,"Topic modeling is widely used for analytically evaluating large collections
of textual data. One of the most popular topic techniques is Latent Dirichlet
Allocation (LDA), which is flexible and adaptive, but not optimal for e.g.
short texts from various domains. We explore how the state-of-the-art BERTopic
algorithm performs on short multi-domain text and find that it generalizes
better than LDA in terms of topic coherence and diversity. We further analyze
the performance of the HDBSCAN clustering algorithm utilized by BERTopic and
find that it classifies a majority of the documents as outliers. This crucial,
yet overseen problem excludes too many documents from further analysis. When we
replace HDBSCAN with k-Means, we achieve similar performance, but without
outliers."
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning,0.496342,"Instruction tuning, a new learning paradigm that fine-tunes pre-trained
language models on tasks specified through instructions, has shown promising
zero-shot performance on various natural language processing tasks. However, it
has yet to be explored for vision and multimodal tasks. In this work, we
introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark
dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq
format covering 10 broad categories. The tasks are derived from 21 existing
open-source datasets and each task is equipped with 5 expert-written
instructions. We take OFA as the base pre-trained model for multimodal
instruction tuning, and to further improve its zero-shot performance, we
explore multiple transfer learning strategies to leverage the large-scale
NATURAL INSTRUCTIONS dataset. Experimental results demonstrate strong zero-shot
performance on various unseen multimodal tasks and the benefit of transfer
learning from a text-only instruction dataset. We also design a new evaluation
metric - Sensitivity, to evaluate how sensitive the model is to the variety of
instructions. Our results indicate that fine-tuning the model on a diverse set
of tasks and instructions leads to a reduced sensitivity to variations in
instructions for each task."
Amodal Panoptic Segmentation,0.544673,"Humans have the remarkable ability to perceive objects as a whole, even when
parts of them are occluded. This ability of amodal perception forms the basis
of our perceptual and cognitive understanding of our world. To enable robots to
reason with this capability, we formulate and propose a novel task that we name
amodal panoptic segmentation. The goal of this task is to simultaneously
predict the pixel-wise semantic segmentation labels of the visible regions of
stuff classes and the instance segmentation labels of both the visible and
occluded regions of thing classes. To facilitate research on this new task, we
extend two established benchmark datasets with pixel-level amodal panoptic
segmentation labels that we make publicly available as KITTI-360-APS and
BDD100K-APS. We present several strong baselines, along with the amodal
panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify
the performance in an interpretable manner. Furthermore, we propose the novel
amodal panoptic segmentation network (APSNet), as a first step towards
addressing this task by explicitly modeling the complex relationships between
the occluders and occludes. Extensive experimental evaluations demonstrate that
APSNet achieves state-of-the-art performance on both benchmarks and more
importantly exemplifies the utility of amodal recognition. The benchmarks are
available at http://amodal-panoptic.cs.uni-freiburg.de."
Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis,0.464344,"Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a
specific aspect in the given sentence. While pre-trained language models such
as BERT have achieved great success, incorporating dynamic semantic changes
into ABSA remains challenging. To this end, in this paper, we propose to
address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method
designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we
first take the Stack-BERT layers as a primary encoder to grasp the overall
semantic of the sentence and then fine-tune it by incorporating a lightweight
Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention
to a small region of the sentences at each step and re-weigh the vitally
important words for better aspect-aware sentiment understanding. Finally,
experimental results on three benchmark datasets demonstrate the effectiveness
and the rationality of our proposed model and provide good interpretable
insights for future semantic modeling."
BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing,0.507285,"Training and evaluating language models increasingly requires the
construction of meta-datasets --diverse collections of curated data with clear
provenance. Natural language prompting has recently lead to improved zero-shot
generalization by transforming existing, supervised datasets into a diversity
of novel pretraining tasks, highlighting the benefits of meta-dataset curation.
While successful in general-domain text, translating these data-centric
approaches to biomedical language modeling remains challenging, as labeled
biomedical datasets are significantly underrepresented in popular data hubs. To
address this challenge, we introduce BigBIO a community library of 126+
biomedical NLP datasets, currently covering 12 task categories and 10+
languages. BigBIO facilitates reproducible meta-dataset curation via
programmatic access to datasets and their metadata, and is compatible with
current platforms for prompt engineering and end-to-end few/zero shot language
model evaluation. We discuss our process for task schema harmonization, data
auditing, contribution guidelines, and outline two illustrative use cases:
zero-shot evaluation of biomedical prompts and large-scale, multi-task
learning. BigBIO is an ongoing community effort and is available at
https://github.com/bigscience-workshop/biomedical"
CrowdMLP: Weakly-Supervised Crowd Counting via Multi-Granularity MLP,0.497148,"Existing state-of-the-art crowd counting algorithms rely excessively on
location-level annotations, which are burdensome to acquire. When only
count-level (weak) supervisory signals are available, it is arduous and
error-prone to regress total counts due to the lack of explicit spatial
constraints. To address this issue, a novel and efficient counter (referred to
as CrowdMLP) is presented, which probes into modelling global dependencies of
embeddings and regressing total counts by devising a multi-granularity MLP
regressor. In specific, a locally-focused pre-trained frontend is cascaded to
extract crude feature maps with intrinsic spatial cues, which prevent the model
from collapsing into trivial outcomes. The crude embeddings, along with raw
crowd scenes, are tokenized at different granularity levels. The
multi-granularity MLP then proceeds to mix tokens at the dimensions of
cardinality, channel, and spatial for mining global information. An effective
proxy task, namely Split-Counting, is also proposed to evade the barrier of
limited samples and the shortage of spatial hints in a self-supervised manner.
Extensive experiments demonstrate that CrowdMLP significantly outperforms
existing weakly-supervised counting algorithms and performs on par with
state-of-the-art location-level supervised approaches."
Deep Learning for Hate Speech Detection: A Comparative Study,0.513164,"Automated hate speech detection is an important tool in combating the spread
of hate speech, particularly in social media. Numerous methods have been
developed for the task, including a recent proliferation of deep-learning based
approaches. A variety of datasets have also been developed, exemplifying
various manifestations of the hate-speech detection problem. We present here a
large-scale empirical comparison of deep and shallow hate-speech detection
methods, mediated through the three most commonly used datasets. Our goal is to
illuminate progress in the area, and identify strengths and weaknesses in the
current state-of-the-art. We particularly focus our analysis on measures of
practical performance, including detection accuracy, computational efficiency,
capability in using pre-trained models, and domain generalization. In doing so
we aim to provide guidance as to the use of hate-speech detection in practice,
quantify the state-of-the-art, and identify future research directions. Code
and dataset are available at
https://github.com/jmjmalik22/Hate-Speech-Detection."
TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models,0.478848,"Language Models (LMs) become outdated as the world changes; they often fail
to perform tasks requiring recent factual information which was absent or
different during training, a phenomenon called temporal misalignment. This is
especially a challenging problem because the research community still lacks a
coherent dataset for assessing the adaptability of LMs to frequently-updated
knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a
lifelong benchmark for ever-evolving LMs that utilizes the difference between
consecutive snapshots of English Wikipedia and English Wikidata for training
and evaluation, respectively. The benchmark hence allows researchers to
periodically track an LM's ability to retain previous knowledge and acquire
updated/new knowledge at each point in time. We also find that training an LM
on the diff data through continual learning methods achieves similar or better
perplexity than on the entire snapshot in our benchmark with 12 times less
computational cost, which verifies that factual knowledge in LMs can be safely
updated with minimal training data via continual learning. The dataset and the
code are available at https://github.com/joeljang/temporalwiki."
InstaFormer: Instance-Aware Image-to-Image Translation with Transformer,0.445781,"We present a novel Transformer-based network architecture for instance-aware
image-to-image translation, dubbed InstaFormer, to effectively integrate
global- and instance-level information. By considering extracted content
features from an image as tokens, our networks discover global consensus of
content features by considering context information through a self-attention
module in Transformers. By augmenting such tokens with an instance-level
feature extracted from the content feature with respect to bounding box
information, our framework is capable of learning an interaction between object
instances and the global image, thus boosting the instance-awareness. We
replace layer normalization (LayerNorm) in standard Transformers with adaptive
instance normalization (AdaIN) to enable a multi-modal translation with style
codes. In addition, to improve the instance-awareness and translation quality
at object regions, we present an instance-level content contrastive loss
defined between input and translated image. We conduct experiments to
demonstrate the effectiveness of our InstaFormer over the latest methods and
provide extensive ablation studies."
Keke AI Competition: Solving puzzle levels in a dynamically changing mechanic space,0.481449,"The Keke AI Competition introduces an artificial agent competition for the
game Baba is You - a Sokoban-like puzzle game where players can create rules
that influence the mechanics of the game. Altering a rule can cause temporary
or permanent effects for the rest of the level that could be part of the
solution space. The nature of these dynamic rules and the deterministic aspect
of the game creates a challenge for AI to adapt to a variety of mechanic
combinations in order to solve a level. This paper describes the framework and
evaluation metrics used to rank submitted agents and baseline results from
sample tree search agents."
Focal Sparse Convolutional Networks for 3D Object Detection,0.485785,"Non-uniformed 3D sparse data, e.g., point clouds or voxels in different
spatial positions, make contribution to the task of 3D object detection in
different ways. Existing basic components in sparse convolutional networks
(Sparse CNNs) process all sparse data, regardless of regular or submanifold
sparse convolution. In this paper, we introduce two new modules to enhance the
capability of Sparse CNNs, both are based on making feature sparsity learnable
with position-wise importance prediction. They are focal sparse convolution
(Focals Conv) and its multi-modal variant of focal sparse convolution with
fusion, or Focals Conv-F for short. The new modules can readily substitute
their plain counterparts in existing Sparse CNNs and be jointly trained in an
end-to-end fashion. For the first time, we show that spatially learnable
sparsity in sparse convolution is essential for sophisticated 3D object
detection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks
validate the effectiveness of our approach. Without bells and whistles, our
results outperform all existing single-model entries on the nuScenes test
benchmark at the paper submission time. Code and models are at
https://github.com/dvlab-research/FocalsConv."
SDS-200: A Swiss German Speech to Standard German Text Corpus,0.526,"We present SDS-200, a corpus of Swiss German dialectal speech with Standard
German text translations, annotated with dialect, age, and gender information
of the speakers. The dataset allows for training speech translation, dialect
recognition, and speech synthesis systems, among others. The data was collected
using a web recording tool that is open to the public. Each participant was
given a text in Standard German and asked to translate it to their Swiss German
dialect before recording it. To increase the corpus quality, recordings were
validated by other participants. The data consists of 200 hours of speech by
around 4000 different speakers and covers a large part of the Swiss-German
dialect landscape. We release SDS-200 alongside a baseline speech translation
model, which achieves a word error rate (WER) of 30.3 and a BLEU score of 53.1
on the SDS-200 test set. Furthermore, we use SDS-200 to fine-tune a pre-trained
XLS-R model, achieving 21.6 WER and 64.0 BLEU."
Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation,0.473141,"Target-guided response generation enables dialogue systems to smoothly
transition a conversation from a dialogue context toward a target sentence.
Such control is useful for designing dialogue systems that direct a
conversation toward specific goals, such as creating non-obtrusive
recommendations or introducing new topics in the conversation. In this paper,
we introduce a new technique for target-guided response generation, which first
finds a bridging path of commonsense knowledge concepts between the source and
the target, and then uses the identified bridging path to generate transition
responses. Additionally, we propose techniques to re-purpose existing dialogue
datasets for target-guided generation. Experiments reveal that the proposed
techniques outperform various baselines on this task. Finally, we observe that
the existing automated metrics for this task correlate poorly with human
judgement ratings. We propose a novel evaluation metric that we demonstrate is
more reliable for target-guided response evaluation. Our work generally enables
dialogue system designers to exercise more control over the conversations that
their systems produce."
A Transformer-Based Siamese Network for Change Detection,0.506076,"This paper presents a transformer-based Siamese network architecture
(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of
co-registered remote sensing images. Different from recent CD frameworks, which
are based on fully convolutional networks (ConvNets), the proposed method
unifies hierarchically structured transformer encoder with Multi-Layer
Perception (MLP) decoder in a Siamese network architecture to efficiently
render multi-scale long-range details required for accurate CD. Experiments on
two CD datasets show that the proposed end-to-end trainable ChangeFormer
architecture achieves better CD performance than previous counterparts. Our
code is available at https://github.com/wgcban/ChangeFormer."
Residual Swin Transformer Channel Attention Network for Image Demosaicing,0.509102,"Image demosaicing is problem of interpolating full- resolution color images
from raw sensor (color filter array) data. During last decade, deep neural
networks have been widely used in image restoration, and in particular, in
demosaicing, attaining significant performance improvement. In recent years,
vision transformers have been designed and successfully used in various
computer vision applications. One of the recent methods of image restoration
based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art
performance with a smaller number of parameters than neural network-based
methods. Inspired by the success of SwinIR, we propose in this paper a novel
Swin Transformer-based network for image demosaicing, called RSTCANet. To
extract image features, RSTCANet stacks several residual Swin Transformer
Channel Attention blocks (RSTCAB), introducing the channel attention for each
two successive ST blocks. Extensive experiments demonstrate that RSTCANet out-
performs state-of-the-art image demosaicing methods, and has a smaller number
of parameters."
The Dark Side of the Language: Pre-trained Transformers in the DarkNet,0.544862,"Pre-trained Transformers are challenging human performances in many NLP
tasks. The massive datasets used for pre-training seem to be the key to their
success on existing tasks. In this paper, we explore how a range of pre-trained
Natural Language Understanding models perform on definitely unseen sentences
provided by classification tasks over a DarkNet corpus. Surprisingly, results
show that syntactic and lexical neural networks perform on par with pre-trained
Transformers even after fine-tuning. Only after what we call extreme domain
adaptation, that is, retraining with the masked language model task on all the
novel corpus, pre-trained Transformers reach their standard high results. This
suggests that huge pre-training corpora may give Transformers unexpected help
since they are exposed to many of the possible sentences."
NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors,0.507365,"Though Neural Radiance Field (NeRF) demonstrates compelling novel view
synthesis results, it is still unintuitive to edit a pre-trained NeRF because
the neural network's parameters and the scene geometry/appearance are often not
explicitly associated. In this paper, we introduce the first framework that
enables users to remove unwanted objects or retouch undesired regions in a 3D
scene represented by a pre-trained NeRF without any category-specific data and
training. The user first draws a free-form mask to specify a region containing
unwanted objects over a rendered view from the pre-trained NeRF. Our framework
first transfers the user-provided mask to other rendered views and estimates
guiding color and depth images within these transferred masked regions. Next,
we formulate an optimization problem that jointly inpaints the image content in
all masked regions across multiple views by updating the NeRF model's
parameters. We demonstrate our framework on diverse scenes and show it obtained
visual plausible and structurally consistent results across multiple views
using shorter time and less user manual efforts."
Device-Cloud Collaborative Recommendation via Meta Controller,0.465388,"On-device machine learning enables the lightweight deployment of
recommendation models in local clients, which reduces the burden of the
cloud-based recommenders and simultaneously incorporates more real-time user
features. Nevertheless, the cloud-based recommendation in the industry is still
very important considering its powerful model capacity and the efficient
candidate generation from the billion-scale item pool. Previous attempts to
integrate the merits of both paradigms mainly resort to a sequential mechanism,
which builds the on-device recommender on top of the cloud-based
recommendation. However, such a design is inflexible when user interests
dramatically change:
  the on-device model is stuck by the limited item cache while the cloud-based
recommendation based on the large item pool do not respond without the new
re-fresh feedback.
  To overcome this issue, we propose a meta controller to dynamically manage
the collaboration between the on-device recommender and the cloud-based
recommender, and introduce a novel efficient sample construction from the
causal perspective to solve the dataset absence issue of meta controller. On
the basis of the counterfactual samples and the extended training, extensive
experiments in the industrial recommendation scenarios show the promise of meta
controller in the device-cloud collaboration."
Adversarial Masking for Self-Supervised Learning,0.490751,"We propose ADIOS, a masked image model (MIM) framework for self-supervised
learning, which simultaneously learns a masking function and an image encoder
using an adversarial objective. The image encoder is trained to minimise the
distance between representations of the original and that of a masked image.
The masking function, conversely, aims at maximising this distance. ADIOS
consistently improves on state-of-the-art self-supervised learning (SSL)
methods on a variety of tasks and datasets -- including classification on
ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and
iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao
et al., 2021) -- while generating semantically meaningful masks. Unlike modern
MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch
tokenisation construction of Vision Transformers, and can be implemented with
convolutional backbones. We further demonstrate that the masks learned by ADIOS
are more effective in improving representation learning of SSL methods than
masking schemes used in popular MIM models. Code is available at
https://github.com/YugeTen/adios."
Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning,0.523062,"Spatiotemporal predictive learning aims to generate future frames by learning
from historical frames. In this paper, we investigate existing methods and
present a general framework of spatiotemporal predictive learning, in which the
spatial encoder and decoder capture intra-frame features and the middle
temporal module catches inter-frame correlations. While the mainstream methods
employ recurrent units to capture long-term temporal dependencies, they suffer
from low computational efficiency due to their unparallelizable architectures.
To parallelize the temporal module, we propose the Temporal Attention Unit
(TAU), which decomposes the temporal attention into intra-frame statical
attention and inter-frame dynamical attention. Moreover, while the mean squared
error loss focuses on intra-frame errors, we introduce a novel differential
divergence regularization to take inter-frame variations into account.
Extensive experiments demonstrate that the proposed method enables the derived
model to achieve competitive performance on various spatiotemporal prediction
benchmarks."
Reinforced Lin-Kernighan-Helsgaun Algorithms for the Traveling Salesman Problems,0.463768,"TSP is a classical NP-hard combinatorial optimization problem with many
practical variants. LKH is one of the state-of-the-art local search algorithms
for the TSP. LKH-3 is a powerful extension of LKH that can solve many TSP
variants. Both LKH and LKH-3 associate a candidate set to each city to improve
the efficiency, and have two different methods, $\alpha$-measure and POPMUSIC,
to decide the candidate sets. In this work, we first propose a Variable
Strategy Reinforced LKH (VSR-LKH) algorithm, which incorporates three
reinforcement learning methods (Q-learning, Sarsa, Monte Carlo) with LKH, for
the TSP. We further propose a new algorithm called VSR-LKH-3 that combines the
variable strategy reinforcement learning method with LKH-3 for typical TSP
variants, including the TSP with time windows (TSPTW) and Colored TSP (CTSP).
The proposed algorithms replace the inflexible traversal operations in LKH and
LKH-3 and let the algorithms learn to make a choice at each search step by
reinforcement learning. Both LKH and LKH-3, with either $\alpha$-measure or
POPMUSIC, can be significantly improved by our methods. Extensive experiments
on 236 widely-used TSP benchmarks with up to 85,900 cities demonstrate the
excellent performance of VSR-LKH. VSR-LKH-3 also significantly outperforms the
state-of-the-art heuristics for TSPTW and CTSP."
Learning Perception-Aware Agile Flight in Cluttered Environments,0.517162,"Recently, neural control policies have outperformed existing model-based
planning-and-control methods for autonomously navigating quadrotors through
cluttered environments in minimum time. However, they are not perception aware,
a crucial requirement in vision-based navigation due to the camera's limited
field of view and the underactuated nature of a quadrotor. We propose a
learning-based system that achieves perception-aware, agile flight in cluttered
environments. Our method combines imitation learning with reinforcement
learning (RL) by leveraging a privileged learning-by-cheating framework. Using
RL, we first train a perception-aware teacher policy with full-state
information to fly in minimum time through cluttered environments. Then, we use
imitation learning to distill its knowledge into a vision-based student policy
that only perceives the environment via a camera. Our approach tightly couples
perception and control, showing a significant advantage in computation speed
(10 times faster) and success rate. We demonstrate the closed-loop control
performance using hardware-in-the-loop simulation."
Probing for the Usage of Grammatical Number,0.454072,"A central quest of probing is to uncover how pre-trained models encode a
linguistic property within their representations. An encoding, however, might
be spurious-i.e., the model might not rely on it when making predictions. In
this paper, we try to find encodings that the model actually uses, introducing
a usage-based probing setup. We first choose a behavioral task which cannot be
solved without using the linguistic property. Then, we attempt to remove the
property by intervening on the model's representations. We contend that, if an
encoding is used by the model, its removal should harm the performance on the
chosen behavioral task. As a case study, we focus on how BERT encodes
grammatical number, and on how it uses this encoding to solve the number
agreement task. Experimentally, we find that BERT relies on a linear encoding
of grammatical number to produce the correct behavioral output. We also find
that BERT uses a separate encoding of grammatical number for nouns and verbs.
Finally, we identify in which layers information about grammatical number is
transferred from a noun to its head verb."
HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator,0.507052,"Video prediction is an important yet challenging problem; burdened with the
tasks of generating future frames and learning environment dynamics. Recently,
autoregressive latent video models have proved to be a powerful video
prediction tool, by separating the video prediction into two sub-problems:
pre-training an image generator model, followed by learning an autoregressive
prediction model in the latent space of the image generator. However,
successfully generating high-fidelity and high-resolution videos has yet to be
seen. In this work, we investigate how to train an autoregressive latent video
prediction model capable of predicting high-fidelity future frames with minimal
modification to existing models, and produce high-resolution (256x256) videos.
Specifically, we scale up prior models by employing a high-fidelity image
generator (VQ-GAN) with a causal transformer model, and introduce additional
techniques of top-k sampling and data augmentation to further improve video
prediction quality. Despite the simplicity, the proposed method achieves
competitive performance to state-of-the-art approaches on standard video
prediction benchmarks with fewer parameters, and enables high-resolution video
prediction on complex and large-scale datasets. Videos are available at
https://sites.google.com/view/harp-videos/home."
A Length-Extrapolatable Transformer,0.457725,"Position modeling plays a critical role in Transformers. In this paper, we
focus on length extrapolation, i.e., training on short texts while evaluating
longer sequences. We define attention resolution as an indicator of
extrapolation. Then we propose two designs to improve the above metric of
Transformers. Specifically, we introduce a relative position embedding to
explicitly maximize attention resolution. Moreover, we use blockwise causal
attention during inference for better resolution. We evaluate different
Transformer variants with language modeling. Experimental results show that our
model achieves strong performance in both interpolation and extrapolation
settings. The code will be available at https://aka.ms/LeX-Transformer."
Universal Domain Adaptive Object Detector,0.55171,"Universal domain adaptive object detection (UniDAOD)is more challenging than
domain adaptive object detection (DAOD) since the label space of the source
domain may not be the same as that of the target and the scale of objects in
the universal scenarios can vary dramatically (i.e, category shift and scale
shift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain
Adaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer
effect during training while maximizing transferability as well as
discriminability in both domains under a variety of scales. Specifically, our
method is implemented by two modules: 1) We facilitate the feature alignment of
common classes and suppress the interference of private classes by designing a
Filter Mechanism module to overcome the negative transfer caused by category
shift. 2) We fill the blank of scale-aware adaptation in object detection by
introducing a new Multi-Label Scale-Aware Adapter to perform individual
alignment between the corresponding scale for two domains. Experiments show
that US-DAF achieves state-of-the-art results on three scenarios (i.e,
Open-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative
improvement on benchmark datasets Clipart1k and Watercolor in particular."
SeSQL: Yet Another Large-scale Session-level Chinese Text-to-SQL Dataset,0.464739,"As the first session-level Chinese dataset, CHASE contains two separate
parts, i.e., 2,003 sessions manually constructed from scratch (CHASE-C), and
3,456 sessions translated from English SParC (CHASE-T). We find the two parts
are highly discrepant and incompatible as training and evaluation data. In this
work, we present SeSQL, yet another large-scale session-level text-to-SQL
dataset in Chinese, consisting of 5,028 sessions all manually constructed from
scratch. In order to guarantee data quality, we adopt an iterative annotation
workflow to facilitate intense and in-time review of previous-round natural
language (NL) questions and SQL queries. Moreover, by completing all
context-dependent NL questions, we obtain 27,012 context-independent
question/SQL pairs, allowing SeSQL to be used as the largest dataset for
single-round multi-DB text-to-SQL parsing. We conduct benchmark session-level
text-to-SQL parsing experiments on SeSQL by employing three competitive
session-level parsers, and present detailed analysis."
Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement,0.450659,"Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes
of permanent blindness worldwide. Designing an automatic grading system with
good generalization ability for DR and DME is vital in clinical practice.
However, prior works either grade DR or DME independently, without considering
internal correlations between them, or grade them jointly by shared feature
representation, yet ignoring potential generalization issues caused by
difficult samples and data bias. Aiming to address these problems, we propose a
framework for joint grading with the dynamic difficulty-aware weighted loss
(DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired
by curriculum learning, DAW learns from simple samples to difficult samples
dynamically via measuring difficulty adaptively. DETACH separates features of
grading tasks to avoid potential emphasis on the bias. With the addition of DAW
and DETACH, the model learns robust disentangled feature representations to
explore internal correlations between DR and DME and achieve better grading
performance. Experiments on three benchmarks show the effectiveness and
robustness of our framework under both the intra-dataset and cross-dataset
tests."
Housekeep: Tidying Virtual Households using Commonsense Reasoning,0.493202,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the
home for embodied AI. In Housekeep, an embodied agent must tidy a house by
rearranging misplaced objects without explicit instructions specifying which
objects need to be rearranged. Instead, the agent must learn from and is
evaluated against human preferences of which objects belong where in a tidy
house. Specifically, we collect a dataset of where humans typically place
objects in tidy and untidy houses constituting 1799 objects, 268 object
categories, 585 placements, and 105 rooms. Next, we propose a modular baseline
approach for Housekeep that integrates planning, exploration, and navigation.
It leverages a fine-tuned large language model (LLM) trained on an internet
text corpus for effective planning. We show that our baseline agent generalizes
to rearranging unseen objects in unknown environments. See our webpage for more
details: https://yashkant.github.io/housekeep/"
Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator,0.457117,"Contextual knowledge is essential for reducing speech recognition errors on
high-valued long-tail words. This paper proposes a novel tree-constrained
pointer generator (TCPGen) component that enables end-to-end ASR models to bias
towards a list of long-tail words obtained using external contextual
information. With only a small overhead in memory use and computation cost,
TCPGen can structure thousands of biasing words efficiently into a symbolic
prefix-tree and creates a neural shortcut between the tree and the final ASR
output to facilitate the recognition of the biasing words. To enhance TCPGen,
we further propose a novel minimum biasing word error (MBWE) loss that directly
optimises biasing word errors during training, along with a biasing-word-driven
language model discounting (BLMD) method during the test. All contextual ASR
systems were evaluated on the public Librispeech audiobook corpus and the data
from the dialogue state tracking challenges (DSTC) with the biasing lists
extracted from the dialogue-system ontology. Consistent word error rate (WER)
reductions were achieved with TCPGen, which were particularly significant on
the biasing words with around 40\% relative reductions in the recognition error
rates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved
more significant WER reductions on the biasing words. TCPGen also achieved
zero-shot learning of words not in the audio training set with large WER
reductions on the out-of-vocabulary words in the biasing list."
Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study),0.496294,"Test Input Prioritizers (TIP) for Deep Neural Networks (DNN) are an important
technique to handle the typically very large test datasets efficiently, saving
computation and labeling costs. This is particularly true for large-scale,
deployed systems, where inputs observed in production are recorded to serve as
potential test or training data for the next versions of the system. Feng et.
al. propose DeepGini, a very fast and simple TIP, and show that it outperforms
more elaborate techniques such as neuron- and surprise coverage. In a
large-scale study (4 case studies, 8 test datasets, 32'200 trained models) we
verify their findings. However, we also find that other comparable or even
simpler baselines from the field of uncertainty quantification, such as the
predicted softmax likelihood or the entropy of the predicted softmax
likelihoods perform equally well as DeepGini."
GBSVM: Granular-ball Support Vector Machine,0.473871,"GBSVM (Granular-ball Support Vector Machine) is a significant attempt to
construct a classifier using the coarse-to-fine granularity of a granular-ball
as input, rather than a single data point. It is the first classifier whose
input contains no points. However, the existing model has some errors, and its
dual model has not been derived. As a result, the current algorithm cannot be
implemented or applied. To address these problems, this paper has fixed the
errors of the original model of the existing GBSVM, and derived its dual model.
Furthermore, a particle swarm optimization algorithm is designed to solve the
dual model. The sequential minimal optimization algorithm is also carefully
designed to solve the dual model. The solution is faster and more stable than
the particle swarm optimization based version. The experimental results on the
UCI benchmark datasets demonstrate that GBSVM has good robustness and
efficiency. All codes have been released in the open source library at
http://www.cquptshuyinxia.com/GBSVM.html or https://github.com/syxiaa/GBSVM."
Controllable Dialogue Simulation with In-Context Learning,0.468054,"Building dialogue systems requires a large corpus of annotated dialogues.
Such datasets are usually created via crowdsourcing, which is expensive and
time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue
simulation method based on large language model in-context learning to automate
dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic}
automatically selects in-context examples for demonstration and prompts GPT-3
to generate new dialogues and annotations in a controllable way. Our method can
rapidly expand a small set of dialogue data with minimum or zero \textit{human
involvement} and \textit{parameter update} and is thus much more cost-efficient
and time-saving than crowdsourcing. Experimental results on the MultiWOZ
dataset demonstrate that training a model on the simulated dialogues leads to
even better performance than using the same amount of human-generated dialogues
under the challenging low-resource settings, with as few as 85 dialogues as a
seed. When enough data is available, our method can still serve as an effective
data augmentation method. Human evaluation results also show that our simulated
dialogues have near-human fluency and annotation accuracy. The code and data
are available at \textbf{\url{https://github.com/Leezekun/dialogic}}."
ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,0.455351,"This work introduces a new multi-task, parameter-efficient language model
(LM) tuning method that learns to transfer knowledge across different tasks via
a mixture of soft prompts-small prefix embedding vectors pre-trained for
different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt
Tuning), obtains source prompts as encodings of large-scale source tasks into a
small number of parameters and trains an attention module to interpolate the
source prompts and a newly initialized target prompt for every instance in the
target task. During training, only the target task prompt and the attention
weights, which are shared between tasks in multi-task training, are updated,
while the original LM and source prompts are intact. ATTEMPT is highly
parameter-efficient (e.g., updates 2,300 times fewer parameters than full
fine-tuning) while achieving high task performance using knowledge from
high-resource tasks. Moreover, it is modular using pre-trained soft prompts,
and can flexibly add or remove source prompts for effective knowledge transfer.
Our experimental results across 21 diverse NLP datasets show that ATTEMPT
significantly outperforms prompt tuning and outperforms or matches fully
fine-tuned or other parameter-efficient tuning approaches that use over ten
times more parameters. Finally, ATTEMPT outperforms previous work in few-shot
learning settings."
Sparse Mixture Once-for-all Adversarial Training for Efficient In-Situ Trade-Off Between Accuracy and Robustness of DNNs,0.550671,"Existing deep neural networks (DNNs) that achieve state-of-the-art (SOTA)
performance on both clean and adversarially-perturbed images rely on either
activation or weight conditioned convolution operations. However, such
conditional learning costs additional multiply-accumulate (MAC) or addition
operations, increasing inference memory and compute costs. To that end, we
present a sparse mixture once for all adversarial training (SMART), that allows
a model to train once and then in-situ trade-off between accuracy and
robustness, that too at a reduced compute and parameter overhead. In
particular, SMART develops two expert paths, for clean and adversarial images,
respectively, that are then conditionally trained via respective dedicated sets
of binary sparsity masks. Extensive evaluations on multiple image
classification datasets across different models show SMART to have up to 2.72x
fewer non-zero parameters costing proportional reduction in compute overhead,
while yielding SOTA accuracy-robustness trade-off. Additionally, we present
insightful observations in designing sparse masks to successfully condition on
both clean and perturbed images."
Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?,0.472582,"What can pre-trained multilingual sequence-to-sequence models like mBART
contribute to translating low-resource languages? We conduct a thorough
empirical experiment in 10 languages to ascertain this, considering five
factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning
data, (3) the amount of pre-training data in the model, (4) the impact of
domain mismatch, and (5) language typology. In addition to yielding several
heuristics, the experiments form a framework for evaluating the data
sensitivities of machine translation systems. While mBART is robust to domain
differences, its translations for unseen and typologically distant languages
remain below 3.0 BLEU. In answer to our title's question, mBART is not a
low-resource panacea; we therefore encourage shifting the emphasis from new
models to new data."
Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires,0.45396,"Automated methods have been widely used to identify and analyze mental health
conditions (e.g., depression) from various sources of information, including
social media. Yet, deployment of such models in real-world healthcare
applications faces challenges including poor out-of-domain generalization and
lack of trust in black box models. In this work, we propose approaches for
depression detection that are constrained to different degrees by the presence
of symptoms described in PHQ9, a questionnaire used by clinicians in the
depression screening process. In dataset-transfer experiments on three social
media datasets, we find that grounding the model in PHQ9's symptoms
substantially improves its ability to generalize to out-of-distribution data
compared to a standard BERT-based approach. Furthermore, this approach can
still perform competitively on in-domain data. These results and our
qualitative analyses suggest that grounding model predictions in
clinically-relevant symptoms can improve generalizability while producing a
model that is easier to inspect."
Provable Benefits of Representational Transfer in Reinforcement Learning,0.512202,"We study the problem of representational transfer in RL, where an agent first
pretrains in a number of source tasks to discover a shared representation,
which is subsequently used to learn a good policy in a \emph{target task}. We
propose a new notion of task relatedness between source and target tasks, and
develop a novel approach for representational transfer under this assumption.
Concretely, we show that given generative access to source tasks, we can
discover a representation, using which subsequent linear RL techniques quickly
converge to a near-optimal policy in the target task.
  The sample complexity is close to knowing the ground truth features in the
target task, and comparable to prior representation learning results in the
source tasks. We complement our positive results with lower bounds without
generative access, and validate our findings with empirical evaluation on rich
observation MDPs that require deep exploration. In our experiments, we observe
a speed up in learning in the target by pre-training, and also validate the
need for generative access in source tasks."
SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues,0.511527,"Dialogue systems are usually categorized into two types, open-domain and
task-oriented. The first one focuses on chatting with users and making them
engage in the conversations, where selecting a proper topic to fit the dialogue
context is essential for a successful dialogue. The other one focuses on a
specific task instead of casual talks, e.g., finding a movie on Friday night,
or playing a song. These two directions have been studied separately due to
their different purposes. However, how smoothly transitioning from social
chatting to task-oriented dialogues is important for triggering business
opportunities, and there is no public data focusing on such scenarios. Hence,
this paper focuses on investigating the conversations starting from open-domain
social chatting and then gradually transitioning to task-oriented purposes, and
releases a large-scale dataset with detailed annotations for encouraging this
research direction. To achieve this goal, this paper proposes a framework to
automatically generate many dialogues without human involvement, in which any
powerful open-domain dialogue generation model can be easily leveraged. The
human evaluation shows that our generated dialogue data has a natural flow at a
reasonable quality, showing that our released data has a great potential of
guiding future research directions and commercial activities. Furthermore, the
released models allow researchers to automatically generate unlimited dialogues
in the target scenarios, which can greatly benefit semi-supervised and
unsupervised approaches."
Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding,0.46992,"Automatic International Classification of Diseases (ICD) coding aims to
assign multiple ICD codes to a medical note with average length of 3,000+
tokens. This task is challenging due to a high-dimensional space of multi-label
assignment (tens of thousands of ICD codes) and the long-tail challenge: only a
few codes (common diseases) are frequently assigned while most codes (rare
diseases) are infrequently assigned. This study addresses the long-tail
challenge by adapting a prompt-based fine-tuning technique with label
semantics, which has been shown to be effective under few-shot setting. To
further enhance the performance in medical domain, we propose a
knowledge-enhanced longformer by injecting three domain-specific knowledge:
hierarchy, synonym, and abbreviation with additional pretraining using
contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of
code assignment, show that our proposed method outperforms previous
state-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P<0.001). To
further test our model on few-shot setting, we created a new rare diseases
coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from
17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method."
Open-Vocabulary Universal Image Segmentation with MaskCLIP,0.473408,"In this paper, we tackle an emerging computer vision task, open-vocabulary
universal image segmentation, that aims to perform semantic/instance/panoptic
segmentation (background semantic labeling + foreground instance segmentation)
for arbitrary categories of text-based descriptions in inference time. We first
build a baseline method by directly adopting pre-trained CLIP models without
finetuning or distillation. We then develop MaskCLIP, a Transformer-based
approach with a MaskCLIP Visual Encoder, which is an encoder-only module that
seamlessly integrates mask tokens with a pre-trained ViT CLIP model for
semantic/instance segmentation and class prediction. MaskCLIP learns to
efficiently and effectively utilize pre-trained partial/dense CLIP features
within the MaskCLIP Visual Encoder that avoids the time-consuming
student-teacher training process. MaskCLIP outperforms previous methods for
semantic/instance/panoptic segmentation on ADE20K and PASCAL datasets. We show
qualitative illustrations for MaskCLIP with online custom categories. Project
website: https://maskclip.github.io."
Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios,0.44993,"Imitation learning (IL) is a simple and powerful way to use high-quality
human driving data, which can be collected at scale, to produce human-like
behavior. However, policies based on imitation learning alone often fail to
sufficiently account for safety and reliability concerns. In this paper, we
show how imitation learning combined with reinforcement learning using simple
rewards can substantially improve the safety and reliability of driving
policies over those learned from imitation alone. In particular, we train a
policy on over 100k miles of urban driving data, and measure its effectiveness
in test scenarios grouped by different levels of collision likelihood. Our
analysis shows that while imitation can perform well in low-difficulty
scenarios that are well-covered by the demonstration data, our proposed
approach significantly improves robustness on the most challenging scenarios
(over 38% reduction in failures). To our knowledge, this is the first
application of a combined imitation and reinforcement learning approach in
autonomous driving that utilizes large amounts of real-world human driving
data."
MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid,0.516775,"Multi-modal entity alignment (MMEA) aims to discover identical entities
across different knowledge graphs (KGs) whose entities are associated with
relevant images. However, current MMEA algorithms rely on KG-level modality
fusion strategies for multi-modal entity representation, which ignores the
variations of modality preferences of different entities, thus compromising
robustness against noise in modalities such as blurry images and relations.
This paper introduces MEAformer, a multi-modal entity alignment transformer
approach for meta modality hybrid, which dynamically predicts the mutual
correlation coefficients among modalities for more fine-grained entity-level
modality fusion and alignment. Experimental results demonstrate that our model
not only achieves SOTA performance in multiple training scenarios, including
supervised, unsupervised, iterative, and low-resource settings, but also has a
limited number of parameters, efficient runtime, and interpretability. Our code
is available at https://github.com/zjukg/MEAformer."
LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval,0.492659,"Retrieval models based on dense representations in semantic space have become
an indispensable branch for first-stage retrieval. These retrievers benefit
from surging advances in representation learning towards compressive global
sequence-level embeddings. However, they are prone to overlook local salient
phrases and entity mentions in texts, which usually play pivot roles in
first-stage retrieval. To mitigate this weakness, we propose to make a dense
retriever align a well-performing lexicon-aware representation model. The
alignment is achieved by weakened knowledge distillations to enlighten the
retriever via two aspects -- 1) a lexicon-augmented contrastive objective to
challenge the dense encoder and 2) a pair-wise rank-consistent regularization
to make dense model's behavior incline to the other. We evaluate our model on
three public benchmarks, which shows that with a comparable lexicon-aware
retriever as the teacher, our proposed dense one can bring consistent and
significant improvements, and even outdo its teacher. In addition, we found our
improvement on the dense retriever is complementary to the standard ranker
distillation, which can further lift state-of-the-art performance."
PanFormer: a Transformer Based Model for Pan-sharpening,0.445169,"Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)
image from a low-resolution (LR) multi-spectral (MS) image and its
corresponding panchromatic (PAN) image acquired by a same satellite. Inspired
by a new fashion in recent deep learning community, we propose a novel
Transformer based model for pan-sharpening. We explore the potential of
Transformer in image feature extraction and fusion. Following the successful
development of vision transformers, we design a two-stream network with the
self-attention to extract the modality-specific features from the PAN and MS
modalities and apply a cross-attention module to merge the spectral and spatial
features. The pan-sharpened image is produced from the enhanced fused features.
Extensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our
Transformer based model achieves impressive results and outperforms many
existing CNN based methods, which shows the great potential of introducing
Transformer to the pan-sharpening task. Codes are available at
https://github.com/zhysora/PanFormer."
Clickbait Spoiling via Question Answering and Passage Retrieval,0.550959,"We introduce and study the task of clickbait spoiling: generating a short
text that satisfies the curiosity induced by a clickbait post. Clickbait links
to a web page and advertises its contents by arousing curiosity instead of
providing an informative summary. Our contributions are approaches to classify
the type of spoiler needed (i.e., a phrase or a passage), and to generate
appropriate spoilers. A large-scale evaluation and error analysis on a new
corpus of 5,000 manually spoiled clickbait posts -- the Webis Clickbait
Spoiling Corpus 2022 -- shows that our spoiler type classifier achieves an
accuracy of 80%, while the question answering model DeBERTa-large outperforms
all others in generating spoilers for both types."
Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs,0.488808,"Recent graph-based models for joint multiple intent detection and slot
filling have obtained promising results through modeling the guidance from the
prediction of intents to the decoding of slot filling. However, existing
methods (1) only model the \textit{unidirectional guidance} from intent to
slot; (2) adopt \textit{homogeneous graphs} to model the interactions between
the slot semantics nodes and intent label nodes, which limit the performance.
In this paper, we propose a novel model termed Co-guiding Net, which implements
a two-stage framework achieving the \textit{mutual guidances} between the two
tasks. In the first stage, the initial estimated labels of both tasks are
produced, and then they are leveraged in the second stage to model the mutual
guidances. Specifically, we propose two \textit{heterogeneous graph attention
networks} working on the proposed two \textit{heterogeneous semantics-label
graphs}, which effectively represent the relations among the semantics nodes
and label nodes. Experiment results show that our model outperforms existing
models by a large margin, obtaining a relative improvement of 19.3\% over the
previous best model on MixATIS dataset in overall accuracy."
Near Perfect GAN Inversion,0.536279,"To edit a real photo using Generative Adversarial Networks (GANs), we need a
GAN inversion algorithm to identify the latent vector that perfectly reproduces
it. Unfortunately, whereas existing inversion algorithms can synthesize images
similar to real photos, they cannot generate the identical clones needed in
most applications. Here, we derive an algorithm that achieves near perfect
reconstructions of photos. Rather than relying on encoder- or
optimization-based methods to find an inverse mapping on a fixed generator
$G(\cdot)$, we derive an approach to locally adjust $G(\cdot)$ to more
optimally represent the photos we wish to synthesize. This is done by locally
tweaking the learned mapping $G(\cdot)$ s.t. $\| {\bf x} - G({\bf z})
\|<\epsilon$, with ${\bf x}$ the photo we wish to reproduce, ${\bf z}$ the
latent vector, $\|\cdot\|$ an appropriate metric, and $\epsilon > 0$ a small
scalar. We show that this approach can not only produce synthetic images that
are indistinguishable from the real photos we wish to replicate, but that these
images are readily editable. We demonstrate the effectiveness of the derived
algorithm on a variety of datasets including human faces, animals, and cars,
and discuss its importance for diversity and inclusion."
Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion,0.553027,"We present a framework for modeling interactional communication in dyadic
conversations: given multimodal inputs of a speaker, we autoregressively output
multiple possibilities of corresponding listener motion. We combine the motion
and speech audio of the speaker using a motion-audio cross attention
transformer. Furthermore, we enable non-deterministic prediction by learning a
discrete latent representation of realistic listener motion with a novel
motion-encoding VQ-VAE. Our method organically captures the multimodal and
non-deterministic nature of nonverbal dyadic interactions. Moreover, it
produces realistic 3D listener facial motion synchronous with the speaker (see
video). We demonstrate that our method outperforms baselines qualitatively and
quantitatively via a rich suite of experiments. To facilitate this line of
research, we introduce a novel and large in-the-wild dataset of dyadic
conversations. Code, data, and videos available at
https://evonneng.github.io/learning2listen/."
Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing,0.555386,"While recent face anti-spoofing methods perform well under the intra-domain
setups, an effective approach needs to account for much larger appearance
variations of images acquired in complex scenes with different sensors for
robust performance. In this paper, we present adaptive vision transformers
(ViT) for robust cross-domain face antispoofing. Specifically, we adopt ViT as
a backbone to exploit its strength to account for long-range dependencies among
pixels. We further introduce the ensemble adapters module and feature-wise
transformation layers in the ViT to adapt to different domains for robust
performance with a few samples. Experiments on several benchmark datasets show
that the proposed models achieve both robust and competitive performance
against the state-of-the-art methods for cross-domain face anti-spoofing using
a few samples."
Stitch it in Time: GAN-Based Facial Editing of Real Videos,0.450771,"The ability of Generative Adversarial Networks to encode rich semantics
within their latent space has been widely adopted for facial image editing.
However, replicating their success with videos has proven challenging. Sets of
high-quality facial videos are lacking, and working with videos introduces a
fundamental barrier to overcome - temporal coherency. We propose that this
barrier is largely artificial. The source video is already temporally coherent,
and deviations from this state arise in part due to careless treatment of
individual components in the editing pipeline. We leverage the natural
alignment of StyleGAN and the tendency of neural networks to learn low
frequency functions, and demonstrate that they provide a strongly consistent
prior. We draw on these insights and propose a framework for semantic editing
of faces in videos, demonstrating significant improvements over the current
state-of-the-art. Our method produces meaningful face manipulations, maintains
a higher degree of temporal consistency, and can be applied to challenging,
high quality, talking head videos which current methods struggle with."
"Video Question Answering: Datasets, Algorithms and Challenges",0.468091,"Video Question Answering (VideoQA) aims to answer natural language questions
according to the given videos. It has earned increasing attention with recent
research trends in joint vision and language understanding. Yet, compared with
ImageQA, VideoQA is largely underexplored and progresses slowly. Although
different algorithms have continually been proposed and shown success on
different VideoQA datasets, we find that there lacks a meaningful survey to
categorize them, which seriously impedes its advancements. This paper thus
provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on
the datasets, algorithms, and unique challenges. We then point out the research
trend of studying beyond factoid QA to inference QA towards the cognition of
video contents, Finally, we conclude some promising directions for future
exploration."
FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition,0.535519,"Conventional convolution neural networks (CNNs) trained on narrow
Field-of-View (FoV) images are the state-of-the-art approaches for object
recognition tasks. Some methods proposed the adaptation of CNNs to ultra-wide
FoV images by learning deformable kernels. However, they are limited by the
Euclidean geometry and their accuracy degrades under strong distortions caused
by fisheye projections. In this work, we demonstrate that learning the shape of
convolution kernels in non-Euclidean spaces is better than existing deformable
kernel methods. In particular, we propose a new approach that learns deformable
kernel parameters (positions) in hyperbolic space. FisheyeHDK is a hybrid CNN
architecture combining hyperbolic and Euclidean convolution layers for
positions and features learning. First, we provide an intuition of hyperbolic
space for wide FoV images. Using synthetic distortion profiles, we demonstrate
the effectiveness of our approach. We select two datasets - Cityscapes and
BDD100K 2020 - of perspective images which we transform to fisheye equivalents
at different scaling factors (analog to focal lengths). Finally, we provide an
experiment on data collected by a real fisheye camera. Validations and
experiments show that our approach improves existing deformable kernel methods
for CNN adaptation on fisheye images."
One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones,0.575011,"We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and
52% relative improvement in unseen success rate over two competitive base
models."
UC-OWOD: Unknown-Classified Open World Object Detection,0.601024,"Open World Object Detection (OWOD) is a challenging computer vision problem
that requires detecting unknown objects and gradually learning the identified
unknown classes. However, it cannot distinguish unknown instances as multiple
unknown classes. In this work, we propose a novel OWOD problem called
Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to
detect unknown instances and classify them into different unknown classes.
Besides, we formulate the problem and devise a two-stage object detector to
solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative
classification head are used to detect known and unknown objects. Then,
similarity-based unknown classification and unknown clustering refinement
modules are constructed to distinguish multiple unknown classes. Moreover, two
novel evaluation protocols are designed to evaluate unknown-class detection.
Abundant experiments and visualizations prove the effectiveness of the proposed
method. Code is available at https://github.com/JohnWuzh/UC-OWOD."
A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation,0.629889,"Early exiting allows instances to exit at different layers according to the
estimation of difficulty. Previous works usually adopt heuristic metrics such
as the entropy of internal outputs to measure instance difficulty, which
suffers from generalization and threshold-tuning. In contrast, learning to
exit, or learning to predict instance difficulty is a more appealing way.
Though some effort has been devoted to employing such ""learn-to-exit"" modules,
it is still unknown whether and how well the instance difficulty can be
learned. As a response, we first conduct experiments on the learnability of
instance difficulty, which demonstrates that modern neural models perform
poorly on predicting instance difficulty. Based on this observation, we propose
a simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces
the learn-to-exit modules with hash functions to assign each token to a fixed
exiting layer. Different from previous methods, HashEE requires no internal
classifiers nor extra parameters, and therefore is more efficient. Experimental
results on classification, regression, and generation tasks demonstrate that
HashEE can achieve higher performance with fewer FLOPs and inference time
compared with previous state-of-the-art early exiting methods."
ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval,0.589126,"Neural retrievers based on pre-trained language models (PLMs), such as
dual-encoders, have achieved promising performance on the task of open-domain
question answering (QA). Their effectiveness can further reach new
state-of-the-arts by incorporating cross-architecture knowledge distillation.
However, most of the existing studies just directly apply conventional
distillation methods. They fail to consider the particular situation where the
teacher and student have different structures. In this paper, we propose a
novel distillation method that significantly advances cross-architecture
distillation for dual-encoders. Our method 1) introduces a self on-the-fly
distillation method that can effectively distill late interaction (i.e.,
ColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation
process to further improve the performance with a cross-encoder teacher.
Extensive experiments are conducted to validate that our proposed solution
outperforms strong baselines and establish a new state-of-the-art on
open-domain QA benchmarks."
4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation,0.638811,"In this work, we present a new paradigm, called 4D-StOP, to tackle the task
of 4D Panoptic LiDAR Segmentation. 4D-StOP first generates spatio-temporal
proposals using voting-based center predictions, where each point in the 4D
volume votes for a corresponding center. These tracklet proposals are further
aggregated using learned geometric features. The tracklet aggregation method
effectively generates a video-level 4D scene representation over the entire
space-time volume. This is in contrast to existing end-to-end trainable
state-of-the-art approaches which use spatio-temporal embeddings that are
represented by Gaussian probability distributions. Our voting-based tracklet
generation method followed by geometric feature-based aggregation generates
significantly improved panoptic LiDAR segmentation quality when compared to
modeling the entire 4D volume using Gaussian probability distributions. 4D-StOP
achieves a new state-of-the-art when applied to the SemanticKITTI test dataset
with a score of 63.9 LSTQ, which is a large (+7%) improvement compared to
current best-performing end-to-end trainable methods. The code and pre-trained
models are available at: https://github.com/LarsKreuzberg/4D-StOP."
Offline RL for Natural Language Generation with Implicit Language Q Learning,0.586294,"Large language models distill broad knowledge from text corpora. However,
they can be inconsistent when it comes to completing user specified tasks. This
issue can be addressed by finetuning such models via supervised learning on
curated datasets, or via reinforcement learning. In this work, we propose a
novel offline RL method, implicit language Q-learning (ILQL), designed for use
on language models, that combines both the flexible utility maximization
framework of RL algorithms with the ability of supervised learning to leverage
previously collected data, as well as its simplicity and stability. Our method
employs a combination of value conservatism alongside an implicit dataset
support constraint in learning value functions, which are then used to guide
language model generations towards maximizing user-specified utility functions.
In addition to empirically validating ILQL, we present a detailed empirical
analysis of situations where offline RL can be useful in natural language
generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can
effectively optimize high variance reward functions based on subjective
judgement, such as whether to label a comment as toxic or not."
Anomaly Detection via Reverse Distillation from One-Class Embedding,0.559367,"Knowledge distillation (KD) achieves promising results on the challenging
problem of unsupervised anomaly detection (AD).The representation discrepancy
of anomalies in the teacher-student (T-S) model provides essential evidence for
AD. However, using similar or identical architectures to build the teacher and
student models in previous studies hinders the diversity of anomalous
representations. To tackle this problem, we propose a novel T-S model
consisting of a teacher encoder and a student decoder and introduce a simple
yet effective ""reverse distillation"" paradigm accordingly. Instead of receiving
raw images directly, the student network takes teacher model's one-class
embedding as input and targets to restore the teacher's multiscale
representations. Inherently, knowledge distillation in this study starts from
abstract, high-level presentations to low-level features. In addition, we
introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S
model. The obtained compact embedding effectively preserves essential
information on normal patterns, but abandons anomaly perturbations. Extensive
experimentation on AD and one-class novelty detection benchmarks shows that our
method surpasses SOTA performance, demonstrating our proposed approach's
effectiveness and generalizability."
Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment,0.614641,"Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting."
Selective Residual M-Net for Real Image Denoising,0.617815,"Image restoration is a low-level vision task which is to restore degraded
images to noise-free images. With the success of deep neural networks, the
convolutional neural networks surpass the traditional restoration methods and
become the mainstream in the computer vision area. To advance the performanceof
denoising algorithms, we propose a blind real image denoising network (SRMNet)
by employing a hierarchical architecture improved from U-Net. Specifically, we
use a selective kernel with residual block on the hierarchical structure called
M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet
has competitive performance results on two synthetic and two real-world noisy
datasets in terms of quantitative metrics and visual quality. The source code
and pretrained model are available at
https://github.com/TentativeGitHub/SRMNet."
Conformance Checking with Uncertainty via SMT (Extended Version),0.569054,"Logs of real-life processes often feature uncertainty pertaining the recorded
timestamps, data values, and/or events. We consider the problem of checking
conformance of uncertain logs against data-aware reference processes.
Specifically, we show how to solve it via SMT encodings, lifting previous work
on data-aware SMT-based conformance checking to this more sophisticated
setting. Our approach is modular, in that it homogeneously accommodates for
different types of uncertainty. Moreover, using appropriate cost functions,
different conformance checking tasks can be addressed. We show the correctness
of our approach and witness feasibility through a proof-of-concept
implementation."
Medical Dataset Classification for Kurdish Short Text over Social Media,0.603448,"The Facebook application is used as a resource for collecting the comments of
this dataset, The dataset consists of 6756 comments to create a Medical Kurdish
Dataset (MKD). The samples are comments of users, which are gathered from
different posts of pages (Medical, News, Economy, Education, and Sport). Six
steps as a preprocessing technique are performed on the raw dataset to clean
and remove noise in the comments by replacing characters. The comments (short
text) are labeled for positive class (medical comment) and negative class
(non-medical comment) as text classification. The percentage ratio of the
negative class is 55% while the positive class is 45%."
Coarse-to-Fine Sparse Sequential Recommendation,0.648534,"Sequential recommendation aims to model dynamic user behavior from historical
interactions. Self-attentive methods have proven effective at capturing
short-term dynamics and long-term preferences. Despite their success, these
approaches still struggle to model sparse data, on which they struggle to learn
high-quality item representations. We propose to model user dynamics from
shopping intents and interacted items simultaneously. The learned intents are
coarse-grained and work as prior knowledge for item recommendation. To this
end, we present a coarse-to-fine self-attention framework, namely CaFe, which
explicitly learns coarse-grained and fine-grained sequential dynamics.
Specifically, CaFe first learns intents from coarse-grained sequences which are
dense and hence provide high-quality user intent representations. Then, CaFe
fuses intent representations into item encoder outputs to obtain improved item
representations. Finally, we infer recommended items based on representations
of items and corresponding intents. Experiments on sparse datasets show that
CaFe outperforms state-of-the-art self-attentive recommenders by 44.03% NDCG@5
on average."
Cross-Spectral Neural Radiance Fields,0.628712,"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations."
In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models,0.617843,"Given the success with in-context learning of large pre-trained language
models, we introduce in-context learning distillation to transfer in-context
few-shot learning ability from large models to smaller models. We propose to
combine in-context learning objectives with language modeling objectives to
distill both the ability to read in-context examples and task knowledge to the
smaller models. We perform in-context learning distillation under two different
few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask
In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask
few-shot learning but also requires more computation than Meta-ICT. Our method
shows consistent improvements for both Meta-ICT and Multitask-ICT on two
benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal
that in-context learning objectives and language modeling objectives are
complementary under the Multitask-ICT paradigm. In-context learning objectives
achieve the best performance when combined with language modeling objectives."
3D Common Corruptions and Data Augmentation,0.610361,"We introduce a set of image transformations that can be used as corruptions
to evaluate the robustness of models as well as data augmentation mechanisms
for training neural networks. The primary distinction of the proposed
transformations is that, unlike existing approaches such as Common Corruptions,
the geometry of the scene is incorporated in the transformations -- thus
leading to corruptions that are more likely to occur in the real world. We also
introduce a set of semantic corruptions (e.g. natural object occlusions). We
show these transformations are `efficient' (can be computed on-the-fly),
`extendable' (can be applied on most image datasets), expose vulnerability of
existing models, and can effectively make models more robust when employed as
`3D data augmentation' mechanisms. The evaluations on several tasks and
datasets suggest incorporating 3D information into benchmarking and training
opens up a promising direction for robustness research."
Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models,0.566874,"Image restoration under adverse weather conditions has been of significant
interest for various computer vision applications. Recent successful methods
rely on the current progress in deep neural network architectural designs
(e.g., with vision transformers). Motivated by the recent progress achieved
with state-of-the-art conditional generative models, we present a novel
patch-based image restoration algorithm based on denoising diffusion
probabilistic models. Our patch-based diffusion modeling approach enables
size-agnostic image restoration by using a guided denoising process with
smoothed noise estimates across overlapping patches during inference. We
empirically evaluate our model on benchmark datasets for image desnowing,
combined deraining and dehazing, and raindrop removal. We demonstrate our
approach to achieve state-of-the-art performances on both weather-specific and
multi-weather image restoration, and experimentally show strong generalization
to real-world test images."
"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",0.563233,"While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations ""detached"" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results."
TRUST XAI: Model-Agnostic Explanations for AI With a Case Study on IIoT Security,0.62827,"Despite AI's significant growth, its ""black box"" nature creates challenges in
generating adequate trust. Thus, it is seldom utilized as a standalone unit in
IoT high-risk applications, such as critical industrial infrastructures,
medical systems, and financial applications, etc. Explainable AI (XAI) has
emerged to help with this problem. However, designing appropriately fast and
accurate XAI is still challenging, especially in numerical applications. Here,
we propose a universal XAI model named Transparency Relying Upon Statistical
Theory (TRUST), which is model-agnostic, high-performing, and suitable for
numerical applications. Simply put, TRUST XAI models the statistical behavior
of the AI's outputs in an AI-based system. Factor analysis is used to transform
the input features into a new set of latent variables. We use mutual
information to rank these variables and pick only the most influential ones on
the AI's outputs and call them ""representatives"" of the classes. Then we use
multi-modal Gaussian distributions to determine the likelihood of any new
sample belonging to each class. We demonstrate the effectiveness of TRUST in a
case study on cybersecurity of the industrial Internet of things (IIoT) using
three different cybersecurity datasets. As IIoT is a prominent application that
deals with numerical data. The results show that TRUST XAI provides
explanations for new random samples with an average success rate of 98%.
Compared with LIME, a popular XAI model, TRUST is shown to be superior in the
context of performance, speed, and the method of explainability. In the end, we
also show how TRUST is explained to the user."
Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks,0.619979,"The wide adoption and application of Masked language models~(MLMs) on
sensitive data (from legal to medical) necessitates a thorough quantitative
investigation into their privacy vulnerabilities -- to what extent do MLMs leak
information about their training data? Prior attempts at measuring leakage of
MLMs via membership inference attacks have been inconclusive, implying the
potential robustness of MLMs to privacy attacks. In this work, we posit that
prior attempts were inconclusive because they based their attack solely on the
MLM's model score. We devise a stronger membership inference attack based on
likelihood ratio hypothesis testing that involves an additional reference MLM
to more accurately quantify the privacy risks of memorization in MLMs. We show
that masked language models are extremely susceptible to likelihood ratio
membership inference attacks: Our empirical results, on models trained on
medical notes, show that our attack improves the AUC of prior membership
inference attacks from 0.66 to an alarmingly high 0.90 level, with a
significant improvement in the low-error region: at 1% false positive rate, our
attack is 51X more powerful than prior work."
MiQA: A Benchmark for Inference on Metaphorical Questions,0.625114,"We propose a benchmark to assess the capability of large language models to
reason with conventional metaphors. Our benchmark combines the previously
isolated topics of metaphor detection and commonsense reasoning into a single
task that requires a model to make inferences by accurately selecting between
the literal and metaphorical register. We examine the performance of
state-of-the-art pre-trained models on binary-choice tasks and find a large
discrepancy between the performance of small and very large models, going from
chance to near-human level. We also analyse the largest model in a generative
setting and find that although human performance is approached, careful
multiple-shot prompting is required."
Repairing Bugs in Python Assignments Using Large Language Models,0.642346,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average."
PP-YOLOE: An evolved version of YOLO,0.62568,"In this report, we present PP-YOLOE, an industrial state-of-the-art object
detector with high performance and friendly deployment. We optimize on the
basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful
backbone and neck equipped with CSPRepResStage, ET-head and dynamic label
assignment algorithm TAL. We provide s/m/l/x models for different practice
scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1
FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed
up) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art
industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference
speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct
extensive experiments to verify the effectiveness of our designs. Source code
and pre-trained models are available at
https://github.com/PaddlePaddle/PaddleDetection."
Multimodal Token Fusion for Vision Transformers,0.589839,"Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images. Our
code is available at https://github.com/yikaiw/TokenFusion."
Perception Prioritized Training of Diffusion Models,0.616106,"Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies."
A Graph Neural Network Approach to Automated Model Building in Cryo-EM Maps,0.579403,"Electron cryo-microscopy (cryo-EM) produces three-dimensional (3D) maps of
the electrostatic potential of biological macromolecules, including proteins.
Along with knowledge about the imaged molecules, cryo-EM maps allow de novo
atomic modelling, which is typically done through a laborious manual process.
Taking inspiration from recent advances in machine learning applications to
protein structure prediction, we propose a graph neural network (GNN) approach
for automated model building of proteins in cryo-EM maps. The GNN acts on a
graph with nodes assigned to individual amino acids and edges representing the
protein chain. Combining information from the voxel-based cryo-EM data, the
amino acid sequence data and prior knowledge about protein geometries, the GNN
refines the geometry of the protein chain and classifies the amino acids for
each of its nodes. Application to 28 test cases shows that our approach
outperforms the state-of-the-art and approximates manual building for cryo-EM
maps with resolutions better than 3.5 \r{A}."
Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,0.663112,"Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning."
ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select,0.56112,"We study the problem of extracting N-ary relation tuples from scientific
articles. This task is challenging because the target knowledge tuples can
reside in multiple parts and modalities of the document. Our proposed method
ReSel decomposes this task into a two-stage procedure that first retrieves the
most relevant paragraph/table and then selects the target entity from the
retrieved component. For the high-level retrieval stage, ReSel designs a simple
and effective feature set, which captures multi-level lexical and semantic
similarities between the query and components. For the low-level selection
stage, ReSel designs a cross-modal entity correlation graph along with a
multi-view architecture, which models both semantic and document-structural
relations between entities. Our experiments on three scientific information
extraction datasets show that ReSel outperforms state-of-the-art baselines
significantly."
PromptBERT: Improving BERT Sentence Embeddings with Prompts,0.624511,"We propose PromptBERT, a novel contrastive learning method for learning
better sentence representation. We firstly analyze the drawback of current
sentence embedding from original BERT and find that it is mainly due to the
static token embedding bias and ineffective BERT layers. Then we propose the
first prompt-based sentence embeddings method and discuss two prompt
representing methods and three prompt searching methods to make BERT achieve
better sentence embeddings. Moreover, we propose a novel unsupervised training
objective by the technology of template denoising, which substantially shortens
the performance gap between the supervised and unsupervised settings. Extensive
experiments show the effectiveness of our method. Compared to SimCSE,
PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and
RoBERTa in the unsupervised setting."
Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense,0.605026,"We develop a novel optimization method for NLPbackdoor inversion. We leverage
a dynamically reducing temperature coefficient in the softmax function to
provide changing loss landscapes to the optimizer such that the process
gradually focuses on the ground truth trigger, which is denoted as a one-hot
value in a convex hull. Our method also features a temperature rollback
mechanism to step away from local optimals, exploiting the observation that
local optimals can be easily deter-mined in NLP trigger inversion (while not in
general optimization). We evaluate the technique on over 1600 models (with
roughly half of them having injected backdoors) on 3 prevailing NLP tasks, with
4 different backdoor attacks and 7 architectures. Our results show that the
technique is able to effectively and efficiently detect and remove backdoors,
outperforming 4 baseline methods."
HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences,0.589843,"In this paper, we tackle the important yet under-investigated problem of
making long-horizon prediction of event sequences. Existing state-of-the-art
models do not perform well at this task due to their autoregressive structure.
We propose HYPRO, a hybridly normalized probabilistic model that naturally fits
this task: its first part is an autoregressive base model that learns to
propose predictions; its second part is an energy function that learns to
reweight the proposals such that more realistic predictions end up with higher
probabilities. We also propose efficient training and inference algorithms for
this model. Experiments on multiple real-world datasets demonstrate that our
proposed HYPRO model can significantly outperform previous models at making
long-horizon predictions of future events. We also conduct a range of ablation
studies to investigate the effectiveness of each component of our proposed
methods."
Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models,0.661245,"Pre-trained vision-language models (e.g., CLIP) have shown promising
zero-shot generalization in many downstream tasks with properly designed text
prompts. Instead of relying on hand-engineered prompts, recent works learn
prompts using the training data from downstream tasks. While effective,
training on domain-specific data reduces a model's generalization capability to
unseen new domains. In this work, we propose test-time prompt tuning (TPT), a
method that can learn adaptive prompts on the fly with a single test sample.
For image classification, TPT optimizes the prompt by minimizing the entropy
with confidence selection so that the model has consistent predictions across
different augmented views of each test sample. In evaluating generalization to
natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP
by 3.6% on average, surpassing previous prompt tuning approaches that require
additional task-specific training data. In evaluating cross-dataset
generalization with unseen categories, TPT performs on par with the
state-of-the-art approaches that use additional training data. Project page:
https://azshue.github.io/TPT."
SALTED: A Framework for SAlient Long-Tail Translation Error Detection,0.568988,"Traditional machine translation (MT) metrics provide an average measure of
translation quality that is insensitive to the long tail of behavioral problems
in MT. Examples include translation of numbers, physical units, dropped content
and hallucinations. These errors, which occur rarely and unpredictably in
Neural Machine Translation (NMT), greatly undermine the reliability of
state-of-the-art MT systems. Consequently, it is important to have visibility
into these problems during model development. Towards this direction, we
introduce SALTED, a specifications-based framework for behavioral testing of MT
models that provides fine-grained views of salient long-tail errors, permitting
trustworthy visibility into previously invisible problems. At the core of our
approach is the development of high-precision detectors that flag errors (or
alternatively, verify output correctness) between a source sentence and a
system output. We demonstrate that such detectors could be used not just to
identify salient long-tail errors in MT systems, but also for higher-recall
filtering of the training data, fixing targeted errors with model fine-tuning
in NMT and generating novel data for metamorphic testing to elicit further bugs
in models."
DocEnTr: An End-to-End Document Image Enhancement Transformer,0.621617,"Document images can be affected by many degradation scenarios, which cause
recognition and processing difficulties. In this age of digitization, it is
important to denoise them for proper usage. To address this challenge, we
present a new encoder-decoder architecture based on vision transformers to
enhance both machine-printed and handwritten document images, in an end-to-end
fashion. The encoder operates directly on the pixel patches with their
positional information without the use of any convolutional layers, while the
decoder reconstructs a clean image from the encoded patches. Conducted
experiments show a superiority of the proposed model compared to the state-of
the-art methods on several DIBCO benchmarks. Code and models will be publicly
available at: \url{https://github.com/dali92002/DocEnTR}."
Biometric Signature Verification Using Recurrent Neural Networks,0.612743,"Architectures based on Recurrent Neural Networks (RNNs) have been
successfully applied to many different tasks such as speech or handwriting
recognition with state-of-the-art results. The main contribution of this work
is to analyse the feasibility of RNNs for on-line signature verification in
real practical scenarios. We have considered a system based on Long Short-Term
Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity
metric from pairs of signatures. For the experimental work, the BiosecurID
database comprised of 400 users and 4 separated acquisition sessions are
considered. Our proposed LSTM RNN system has outperformed the results of recent
published works on the BiosecurID benchmark in figures ranging from 17.76% to
28.00% relative verification performance improvement for skilled forgeries."
S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers,0.656669,"The task of converting a natural language question into an executable SQL
query, known as text-to-SQL, is an important branch of semantic parsing. The
state-of-the-art graph-based encoder has been successfully used in this task
but does not model the question syntax well. In this paper, we propose
S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL
parsers, which effectively leverages the syntactic dependency information of
questions in text-to-SQL to improve the performance. We also employ the
decoupling constraint to induce diverse relational edge embedding, which
further improves the network's performance. Experiments on the Spider and
robustness setting Spider-Syn demonstrate that the proposed approach
outperforms all existing methods when pre-training models are used, resulting
in a performance ranks first on the Spider leaderboard."
TCTrack: Temporal Contexts for Aerial Tracking,0.567683,"Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier."
Automatic Depression Detection: An Emotional Audio-Textual Corpus and a GRU/BiLSTM-based Model,0.627124,"Depression is a global mental health problem, the worst case of which can
lead to suicide. An automatic depression detection system provides great help
in facilitating depression self-assessment and improving diagnostic accuracy.
In this work, we propose a novel depression detection approach utilizing speech
characteristics and linguistic contents from participants' interviews. In
addition, we establish an Emotional Audio-Textual Depression Corpus
(EATD-Corpus) which contains audios and extracted transcripts of responses from
depressed and non-depressed volunteers. To the best of our knowledge,
EATD-Corpus is the first and only public depression dataset that contains audio
and text data in Chinese. Evaluated on two depression datasets, the proposed
method achieves the state-of-the-art performances. The outperforming results
demonstrate the effectiveness and generalization ability of the proposed
method. The source code and EATD-Corpus are available at
https://github.com/speechandlanguageprocessing/ICASSP2022-Depression."
Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models,0.578328,"We explore the idea of compressing the prompts used to condition language
models, and show that compressed prompts can retain a substantive amount of
information about the original prompt. For severely compressed prompts, while
fine-grained information is lost, abstract information and general sentiments
can be retained with surprisingly few parameters, which can be useful in the
context of decode-time algorithms for controllability and toxicity reduction.
We explore contrastive conditioning to steer language model generation towards
desirable text and away from undesirable text, and find that some complex
prompts can be effectively compressed into a single token to guide generation.
We also show that compressed prompts are largely compositional, and can be
constructed such that they can be used to control independent aspects of
generated text."
Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations,0.58123,"We propose an unsupervised method for 3D geometry-aware representation
learning of articulated objects, in which no image-pose pairs or foreground
masks are used for training. Though photorealistic images of articulated
objects can be rendered with explicit pose control through existing 3D neural
representations, these methods require ground truth 3D pose and foreground
masks for training, which are expensive to obtain. We obviate this need by
learning the representations with GAN training. The generator is trained to
produce realistic images of articulated objects from random poses and latent
vectors by adversarial training. To avoid a high computational cost for GAN
training, we propose an efficient neural representation for articulated objects
based on tri-planes and then present a GAN-based framework for its unsupervised
training. Experiments demonstrate the efficiency of our method and show that
GAN-based training enables the learning of controllable 3D representations
without paired supervision."
Active Learning by Feature Mixing,0.57656,"The promise of active learning (AL) is to reduce labelling costs by selecting
the most valuable examples to annotate from a pool of unlabelled data.
Identifying these examples is especially challenging with high-dimensional data
(e.g. images, videos) and in low-data regimes. In this paper, we propose a
novel method for batch AL called ALFA-Mix. We identify unlabelled instances
with sufficiently-distinct features by seeking inconsistencies in predictions
resulting from interventions on their representations. We construct
interpolations between representations of labelled and unlabelled instances
then examine the predicted labels. We show that inconsistencies in these
predictions help discovering features that the model is unable to recognise in
the unlabelled instances. We derive an efficient implementation based on a
closed-form solution to the optimal interpolation causing changes in
predictions. Our method outperforms all recent AL approaches in 30 different
settings on 12 benchmarks of images, videos, and non-visual data. The
improvements are especially significant in low-data regimes and on self-trained
vision transformers, where ALFA-Mix outperforms the state-of-the-art in 59% and
43% of the experiments respectively."
Dynamic Global Memory for Document-level Argument Extraction,0.650797,"Extracting informative arguments of events from news articles is a
challenging problem in information extraction, which requires a global
contextual understanding of each document. While recent work on document-level
extraction has gone beyond single-sentence and increased the cross-sentence
inference capability of end-to-end models, they are still restricted by certain
input sequence length constraints and usually ignore the global context between
events. To tackle this issue, we introduce a new global neural generation-based
framework for document-level event argument extraction by constructing a
document memory store to record the contextual event information and leveraging
it to implicitly and explicitly help with decoding of arguments for later
events. Empirical results show that our framework outperforms prior methods
substantially and it is more robust to adversarially annotated examples with
our constrained decoding design. (Our code and resources are available at
https://github.com/xinyadu/memory_docie for research purpose.)"
"DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",0.647909,"Discriminative learning, restorative learning, and adversarial learning have
proven beneficial for self-supervised learning schemes in computer vision and
medical imaging. Existing efforts, however, omit their synergistic effects on
each other in a ternary setup, which, we envision, can significantly benefit
deep semantic representation learning. To realize this vision, we have
developed DiRA, the first framework that unites discriminative, restorative,
and adversarial learning in a unified manner to collaboratively glean
complementary visual information from unlabeled medical images for fine-grained
semantic representation learning. Our extensive experiments demonstrate that
DiRA (1) encourages collaborative learning among three learning ingredients,
resulting in more generalizable representation across organs, diseases, and
modalities; (2) outperforms fully supervised ImageNet models and increases
robustness in small data regimes, reducing annotation cost across multiple
medical imaging applications; (3) learns fine-grained semantic representation,
facilitating accurate lesion localization with only image-level annotation; and
(4) enhances state-of-the-art restorative approaches, revealing that DiRA is a
general mechanism for united representation learning. All code and pre-trained
models are available at https: //github.com/JLiangLab/DiRA."
Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation,0.565497,"Fashion attribute editing is a task that aims to convert the semantic
attributes of a given fashion image while preserving the irrelevant regions.
Previous works typically employ conditional GANs where the generator explicitly
learns the target attributes and directly execute the conversion. These
approaches, however, are neither scalable nor generic as they operate only with
few limited attributes and a separate generator is required for each dataset or
attribute set. Inspired by the recent advancement of diffusion models, we
explore the classifier-guided diffusion that leverages the off-the-shelf
diffusion model pretrained on general visual semantics such as Imagenet. In
order to achieve a generic editing pipeline, we pose this as multi-attribute
image manipulation task, where the attribute ranges from item category, fabric,
pattern to collar and neckline. We empirically show that conventional methods
fail in our challenging setting, and study efficient adaptation scheme that
involves recently introduced attention-pooling technique to obtain a
multi-attribute classifier guidance. Based on this, we present a mask-free
fashion attribute editing framework that leverages the classifier logits and
the cross-attention map for manipulation. We empirically demonstrate that our
framework achieves convincing sample quality and attribute alignments."
ByT5 model for massively multilingual grapheme-to-phoneme conversion,0.608394,"In this study, we tackle massively multilingual grapheme-to-phoneme
conversion through implementing G2P models based on ByT5. We have curated a G2P
dataset from various sources that covers around 100 languages and trained
large-scale multilingual G2P models based on ByT5. We found that ByT5 operating
on byte-level inputs significantly outperformed the token-based mT5 model in
terms of multilingual G2P. Pairwise comparison with monolingual models in these
languages suggests that multilingual ByT5 models generally lower the phone
error rate by jointly learning from a variety of languages. The pretrained
model can further benefit low resource G2P through zero-shot prediction on
unseen languages or provides pretrained weights for finetuning, which helps the
model converge to a lower phone error rate than randomly initialized weights.
To facilitate future research on multilingual G2P, we make available our code
and pretrained multilingual G2P models at:
https://github.com/lingjzhu/CharsiuG2P."
In-Hand 3D Object Scanning from an RGB Sequence,0.597316,"We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses."
Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs,0.659135,"Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly
challenging problem as traditional subgraph matching methods are not capable to
deal with noise and missing information. To address this problem, it has been
recently introduced a promising approach based on jointly embedding logical
queries and KGs into a low-dimensional space to identify answer entities.
However, existing proposals ignore critical semantic knowledge inherently
available in KGs, such as type information. To leverage type information, we
propose a novel TypE-aware Message Passing (TEMP) model, which enhances the
entity and relation representations in queries, and simultaneously improves
generalization, deductive and inductive reasoning. Remarkably, TEMP is a
plug-and-play model that can be easily incorporated into existing
embedding-based models to improve their performance. Extensive experiments on
three real-world datasets demonstrate TEMP's effectiveness."
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,0.637793,"This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner"
Motron: Multimodal Probabilistic Human Motion Forecasting,0.625971,"Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms."
Penalized Proximal Policy Optimization for Safe Reinforcement Learning,0.628747,"Safe reinforcement learning aims to learn the optimal policy while satisfying
safety constraints, which is essential in real-world applications. However,
current algorithms still struggle for efficient policy updates with hard
constraint satisfaction. In this paper, we propose Penalized Proximal Policy
Optimization (P3O), which solves the cumbersome constrained policy iteration
via a single minimization of an equivalent unconstrained problem. Specifically,
P3O utilizes a simple-yet-effective penalty function to eliminate cost
constraints and removes the trust-region constraint by the clipped surrogate
objective. We theoretically prove the exactness of the proposed method with a
finite penalty factor and provide a worst-case analysis for approximate error
when evaluated on sample trajectories. Moreover, we extend P3O to more
challenging multi-constraint and multi-agent scenarios which are less studied
in previous work. Extensive experiments show that P3O outperforms
state-of-the-art algorithms with respect to both reward improvement and
constraint satisfaction on a set of constrained locomotive tasks."
Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis,0.598595,"Recent literature focuses on utilizing the entity information in the
sentence-level relation extraction (RE), but this risks leaking superficial and
spurious clues of relations. As a result, RE still suffers from unintended
entity bias, i.e., the spurious correlation between entity mentions (names) and
relations. Entity bias can mislead the RE models to extract the relations that
do not exist in the text. To combat this issue, some previous work masks the
entity mentions to prevent the RE models from overfitting entity mentions.
However, this strategy degrades the RE performance because it loses the
semantic information of entities. In this paper, we propose the CORE
(Counterfactual Analysis based Relation Extraction) debiasing method that
guides the RE models to focus on the main effects of textual context without
losing the entity information. We first construct a causal graph for RE, which
models the dependencies between variables in RE models. Then, we propose to
conduct counterfactual analysis on our causal graph to distill and mitigate the
entity bias, that captures the causal effects of specific entity mentions in
each instance. Note that our CORE method is model-agnostic to debias existing
RE systems during inference without changing their training processes.
Extensive experimental results demonstrate that our CORE yields significant
gains on both effectiveness and generalization for RE. The source code is
provided at: https://github.com/vanoracai/CoRE."
COEM: Cross-Modal Embedding for MetaCell Identification,0.64459,"Metacells are disjoint and homogeneous groups of single-cell profiles,
representing discrete and highly granular cell states. Existing metacell
algorithms tend to use only one modality to infer metacells, even though
single-cell multi-omics datasets profile multiple molecular modalities within
the same cell. Here, we present \textbf{C}ross-M\textbf{O}dal
\textbf{E}mbedding for \textbf{M}etaCell Identification (COEM), which utilizes
an embedded space leveraging the information of both scATAC-seq and scRNA-seq
to perform aggregation, balancing the trade-off between fine resolution and
sufficient sequencing coverage. COEM outperforms the state-of-the-art method
SEACells by efficiently identifying accurate and well-separated metacells
across datasets with continuous and discrete cell types. Furthermore, COEM
significantly improves peak-to-gene association analyses, and facilitates
complex gene regulatory inference tasks."
ON-DEMAND-FL: A Dynamic and Efficient Multi-Criteria Federated Learning Client Deployment Scheme,0.653756,"In this paper, we increase the availability and integration of devices in the
learning process to enhance the convergence of federated learning (FL) models.
To address the issue of having all the data in one location, federated
learning, which maintains the ability to learn over decentralized data sets,
combines privacy and technology. Until the model converges, the server combines
the updated weights obtained from each dataset over a number of rounds. The
majority of the literature suggested client selection techniques to accelerate
convergence and boost accuracy. However, none of the existing proposals have
focused on the flexibility to deploy and select clients as needed, wherever and
whenever that may be. Due to the extremely dynamic surroundings, some devices
are actually not available to serve as clients in FL, which affects the
availability of data for learning and the applicability of the existing
solution for client selection. In this paper, we address the aforementioned
limitations by introducing an On-Demand-FL, a client deployment approach for
FL, offering more volume and heterogeneity of data in the learning process. We
make use of the containerization technology such as Docker to build efficient
environments using IoT and mobile devices serving as volunteers. Furthermore,
Kubernetes is used for orchestration. The Genetic algorithm (GA) is used to
solve the multi-objective optimization problem due to its evolutionary
strategy. The performed experiments using the Mobile Data Challenge (MDC)
dataset and the Localfed framework illustrate the relevance of the proposed
approach and the efficiency of the on-the-fly deployment of clients whenever
and wherever needed with less discarded rounds and more available data."
An Ultra-low Power TinyML System for Real-time Visual Processing at Edge,0.609592,"Tiny machine learning (TinyML), executing AI workloads on resource and power
strictly restricted systems, is an important and challenging topic. This brief
firstly presents an extremely tiny backbone to construct high efficiency CNN
models for various visual tasks. Then, a specially designed neural co-processor
(NCP) is interconnected with MCU to build an ultra-low power TinyML system,
which stores all features and weights on chip and completely removes both of
latency and power consumption in off-chip memory access. Furthermore, an
application specific instruction-set is further presented for realizing agile
development and rapid deployment. Extensive experiments demonstrate that the
proposed TinyML system based on our model, NCP and instruction set yields
considerable accuracy and achieves a record ultra-low power of 160mW while
implementing object detection and recognition at 30FPS. The demo video is
available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}."
Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation,0.556403,"The performance of multilingual pretrained models is highly dependent on the
availability of monolingual or parallel text present in a target language.
Thus, the majority of the world's languages cannot benefit from recent progress
in NLP as they have no or limited textual data. To expand possibilities of
using NLP technology in these under-represented languages, we systematically
study strategies that relax the reliance on conventional language resources
through the use of bilingual lexicons, an alternative resource with much better
language coverage. We analyze different strategies to synthesize textual or
labeled data using lexicons, and how this data can be combined with monolingual
or parallel text when available. For 19 under-represented languages across 3
tasks, our methods lead to consistent improvements of up to 5 and 15 points
with and without extra monolingual text respectively. Overall, our study
highlights how NLP methods can be adapted to thousands more languages that are
under-served by current technology"
Learning Non-target Knowledge for Few-shot Semantic Segmentation,0.663444,"Existing studies in few-shot semantic segmentation only focus on mining the
target object information, however, often are hard to tell ambiguous regions,
especially in non-target regions, which include background (BG) and Distracting
Objects (DOs). To alleviate this problem, we propose a novel framework, namely
Non-Target Region Eliminating (NTRE) network, to explicitly mine and eliminate
BG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to
extract the BG region via learning a general BG prototype. To this end, we
design a BG loss to supervise the learning of BGMM only using the known target
object segmentation ground truth. Then, a BG Eliminating Module and a DO
Eliminating Module are proposed to successively filter out the BG and DO
information from the query feature, based on which we can obtain a BG and
DO-free target object segmentation result. Furthermore, we propose a
prototypical contrastive learning algorithm to improve the model ability of
distinguishing the target object from DOs. Extensive experiments on both
PASCAL-5i and COCO-20i datasets show that our approach is effective despite its
simplicity."
Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge,0.605682,"Conversational Causal Emotion Entailment aims to detect causal utterances for
a non-neutral targeted utterance from a conversation. In this work, we build
conversations as graphs to overcome implicit contextual modelling of the
original entailment style. Following the previous work, we further introduce
the emotion information into graphs. Emotion information can markedly promote
the detection of causal utterances whose emotion is the same as the targeted
utterance. However, it is still hard to detect causal utterances with different
emotions, especially neutral ones. The reason is that models are limited in
reasoning causal clues and passing them between utterances. To alleviate this
problem, we introduce social commonsense knowledge (CSK) and propose a
Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two
utterances. As not all CSK is emotionally suitable for utterances, we therefore
propose a sentiment-realized knowledge selecting strategy to filter CSK. To
process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph
networks. Experimental results show that our method outperforms baselines and
infers more causes with different emotions from the targeted utterance."
TransBoost: Improving the Best ImageNet Performance using Deep Transduction,0.632121,"This paper deals with deep transductive learning, and proposes TransBoost as
a procedure for fine-tuning any deep neural model to improve its performance on
any (unlabeled) test set provided at training time. TransBoost is inspired by a
large margin principle and is efficient and simple to use. Our method
significantly improves the ImageNet classification performance on a wide range
of architectures, such as ResNets, MobileNetV3-L, EfficientNetB0, ViT-S, and
ConvNext-T, leading to state-of-the-art transductive performance. Additionally
we show that TransBoost is effective on a wide variety of image classification
datasets. The implementation of TransBoost is provided at:
https://github.com/omerb01/TransBoost ."
minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models,0.61318,"We present minicons, an open source library that provides a standard API for
researchers interested in conducting behavioral and representational analyses
of transformer-based language models (LMs). Specifically, minicons enables
researchers to apply analysis methods at two levels: (1) at the prediction
level -- by providing functions to efficiently extract word/sentence level
probabilities; and (2) at the representational level -- by also facilitating
efficient extraction of word/phrase level vectors from one or more layers. In
this paper, we describe the library and apply it to two motivating case
studies: One focusing on the learning dynamics of the BERT architecture on
relative grammatical judgments, and the other on benchmarking 23 different LMs
on zero-shot abductive reasoning. minicons is available at
https://github.com/kanishkamisra/minicons"
Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology,0.591406,"Tissue phenotyping is a fundamental task in learning objective
characterizations of histopathologic biomarkers within the tumor-immune
microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a
complex computer vision in which: 1) WSIs have enormous image resolutions with
precludes large-scale pixel-level efforts in data curation, and 2) diversity of
morphological phenotypes results in inter- and intra-observer variability in
tissue labeling. To address these limitations, current efforts have proposed
using pretrained image encoders (transfer learning from ImageNet,
self-supervised pretraining) in extracting morphological features from
pathology, but have not been extensively validated. In this work, we conduct a
search for good representations in pathology by training a variety of
self-supervised models with validation on a variety of weakly-supervised and
patch-level tasks. Our key finding is in discovering that Vision Transformers
using DINO-based knowledge distillation are able to learn data-efficient and
interpretable features in histology images wherein the different attention
heads learn distinct morphological phenotypes. We make evaluation code and
pretrained weights publicly-available at:
https://github.com/Richarizardd/Self-Supervised-ViT-Path."
Experimental analysis regarding the influence of iris segmentation on the recognition rate,0.638874,"In this study the authors will look at the detection and segmentation of the
iris and its influence on the overall performance of the iris-biometric tool
chain. The authors will examine whether the segmentation accuracy, based on
conformance with a ground truth, can serve as a predictor for the overall
performance of the iris-biometric tool chain. That is: If the segmentation
accuracy is improved will this always improve the overall performance?
Furthermore, the authors will systematically evaluate the influence of
segmentation parameters, pupillary and limbic boundary and normalisation centre
(based on Daugman's rubbersheet model), on the rest of the iris-biometric tool
chain. The authors will investigate if accurately finding these parameters is
important and how consistency, that is, extracting the same exact region of the
iris during segmenting, influences the overall performance."
REPAIR: REnormalizing Permuted Activations for Interpolation Repair,0.591897,"In this paper we look into the conjecture of Entezari et al. (2021) which
states that if the permutation invariance of neural networks is taken into
account, then there is likely no loss barrier to the linear interpolation
between SGD solutions. First, we observe that neuron alignment methods alone
are insufficient to establish low-barrier linear connectivity between SGD
solutions due to a phenomenon we call variance collapse: interpolated deep
networks suffer a collapse in the variance of their activations, causing poor
performance. Next, we propose REPAIR (REnormalizing Permuted Activations for
Interpolation Repair) which mitigates variance collapse by rescaling the
preactivations of such interpolated networks. We explore the interaction
between our method and the choice of normalization layer, network width, and
depth, and demonstrate that using REPAIR on top of neuron alignment methods
leads to 60%-100% relative barrier reduction across a wide variety of
architecture families and tasks. In particular, we report a 74% barrier
reduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on
CIFAR10."
Intent Contrastive Learning for Sequential Recommendation,0.591449,"Users' interactions with items are driven by various intents (e.g., preparing
for holiday gifts, shopping for fishing equipment, etc.).However, users'
underlying intents are often unobserved/latent, making it challenging to
leverage such latent intents forSequentialrecommendation(SR). To investigate
the benefits of latent intents and leverage them effectively for
recommendation, we proposeIntentContrastiveLearning(ICL), a general learning
paradigm that leverages a latent intent variable into SR. The core idea is to
learn users' intent distribution functions from unlabeled user behavior
sequences and optimize SR models with contrastive self-supervised learning
(SSL) by considering the learned intents to improve recommendation.
Specifically, we introduce a latent variable to represent users' intents and
learn the distribution function of the latent variable via clustering. We
propose to leverage the learned intents into SR models via contrastive SSL,
which maximizes the agreement between a view of sequence and its corresponding
intent. The training is alternated between intent representation learning and
the SR model optimization steps within the generalized expectation-maximization
(EM) framework. Fusing user intent information into SR also improves model
robustness. Experiments conducted on four real-world datasets demonstrate the
superiority of the proposed learning paradigm, which improves performance, and
robustness against data sparsity and noisy interaction issues."
Unified Chinese License Plate Detection and Recognition with High Efficiency,0.638564,"Recently, deep learning-based methods have reached an excellent performance
on License Plate (LP) detection and recognition tasks. However, it is still
challenging to build a robust model for Chinese LPs since there are not enough
large and representative datasets. In this work, we propose a new dataset named
Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP
images as a supplement to the existing public benchmarks. The images are mainly
captured with electronic monitoring systems with detailed annotations. To our
knowledge, CRPD is the largest public multi-objective Chinese LP dataset with
annotations of vertices. With CRPD, a unified detection and recognition network
with high efficiency is presented as the baseline. The network is end-to-end
trainable with totally real-time inference efficiency (30 fps with 640p). The
experiments on several public benchmarks demonstrate that our method has
reached competitive performance. The code and dataset will be publicly
available at https://github.com/yxgong0/CRPD."
Engineering Monosemanticity in Toy Models,0.567355,"In some neural networks, individual neurons correspond to natural
``features'' in the input. Such \emph{monosemantic} neurons are of great help
in interpretability studies, as they can be cleanly understood. In this work we
report preliminary attempts to engineer monosemanticity in toy models. We find
that models can be made more monosemantic without increasing the loss by just
changing which local minimum the training process finds. More monosemantic loss
minima have moderate negative biases, and we are able to use this fact to
engineer highly monosemantic models. We are able to mechanistically interpret
these models, including the residual polysemantic neurons, and uncover a simple
yet surprising algorithm. Finally, we find that providing models with more
neurons per layer makes the models more monosemantic, albeit at increased
computational cost. These findings point to a number of new questions and
avenues for engineering monosemanticity, which we intend to study these in
future work."
BotSIM: An End-to-End Bot Simulation Toolkit for Commercial Task-Oriented Dialog Systems,0.632121,"We introduce BotSIM, a modular, open-source Bot SIMulation environment with
dialog generation, user simulation and conversation analytics capabilities.
BotSIM aims to serve as a one-stop solution for large-scale data-efficient
end-to-end evaluation, diagnosis and remediation of commercial task-oriented
dialog (TOD) systems to significantly accelerate commercial bot development and
evaluation, reduce cost and time-to-market. BotSIM adopts a layered design
comprising the infrastructure layer, the adaptor layer and the application
layer. The infrastructure layer hosts key models and components to support
BotSIM's major functionalities via a streamlined
""generation-simulation-remediation"" pipeline. The adaptor layer is used to
extend BotSIM to accommodate new bot platforms. The application layer provides
a suite of command line tools and a Web App to significantly lower the entry
barrier for BotSIM users such as bot admins or practitioners. In this report,
we focus on the technical designs of various system components. A detailed case
study using Einstein BotBuilder is also presented to show how to apply BotSIM
pipeline for bot evaluation and remediation. The detailed system descriptions
can be found in our system demo paper. The toolkit is available at:
https://github.com/salesforce/BotSIM ."
Label-enhanced Prototypical Network with Contrastive Learning for Multi-label Few-shot Aspect Category Detection,0.575907,"Multi-label aspect category detection allows a given review sentence to
contain multiple aspect categories, which is shown to be more practical in
sentiment analysis and attracting increasing attention. As annotating large
amounts of data is time-consuming and labor-intensive, data scarcity occurs
frequently in real-world scenarios, which motivates multi-label few-shot aspect
category detection. However, research on this problem is still in infancy and
few methods are available. In this paper, we propose a novel label-enhanced
prototypical network (LPN) for multi-label few-shot aspect category detection.
The highlights of LPN can be summarized as follows. First, it leverages label
description as auxiliary knowledge to learn more discriminative prototypes,
which can retain aspect-relevant information while eliminating the harmful
effect caused by irrelevant aspects. Second, it integrates with contrastive
learning, which encourages that the sentences with the same aspect label are
pulled together in embedding space while simultaneously pushing apart the
sentences with different aspect labels. In addition, it introduces an adaptive
multi-label inference module to predict the aspect count in the sentence, which
is simple yet effective. Extensive experimental results on three datasets
demonstrate that our proposed model LPN can consistently achieve
state-of-the-art performance."
Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement,0.63148,"Low-Light Image Enhancement is a computer vision task which intensifies the
dark images to appropriate brightness. It can also be seen as an ill-posed
problem in image restoration domain. With the success of deep neural networks,
the convolutional neural networks surpass the traditional algorithm-based
methods and become the mainstream in the computer vision area. To advance the
performance of enhancement algorithms, we propose an image enhancement network
(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use
a half wavelet attention block on M-Net+ to enrich the features from wavelet
domain. Furthermore, our HWMNet has competitive performance results on two
image enhancement datasets in terms of quantitative metrics and visual quality.
The source code and pretrained model are available at
https://github.com/FanChiMao/HWMNet."
DSI++: Updating Transformer Memory with New Documents,0.638969,"Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence."
Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation,0.606611,"The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks."
Communication Beyond Transmitting Bits: Semantics-Guided Source and Channel Coding,0.557329,"Classical communication paradigms focus on accurately transmitting bits over
a noisy channel, and Shannon theory provides a fundamental theoretical limit on
the rate of reliable communications. In this approach, bits are treated
equally, and the communication system is oblivious to what meaning these bits
convey or how they would be used. Future communications towards intelligence
and conciseness will predictably play a dominant role, and the proliferation of
connected intelligent agents requires a radical rethinking of coded
transmission paradigm to support the new communication morphology on the
horizon. The recent concept of ""semantic communications"" offers a promising
research direction. Injecting semantic guidance into the coded transmission
design to achieve semantics-aware communications shows great potential for
further breakthrough in effectiveness and reliability. This article sheds light
on semantics-guided source and channel coding as a transmission paradigm of
semantic communications, which exploits both data semantics diversity and
wireless channel diversity together to boost the whole system performance. We
present the general system architecture and key techniques, and indicate some
open issues on this topic."
Data Contamination: From Memorization to Exploitation,0.649438,"Pretrained language models are typically trained on massive web-based
datasets, which are often ""contaminated"" with downstream test sets. It is not
clear to what extent models exploit the contaminated data for downstream tasks.
We present a principled method to study this question. We pretrain BERT models
on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune
them on the relevant task. Comparing performance between samples seen and
unseen during pretraining enables us to define and quantify levels of
memorization and exploitation. Experiments with two models and three downstream
tasks show that exploitation exists in some cases, but in others the models
memorize the contaminated data, but do not exploit it. We show that these two
measures are affected by different factors such as the number of duplications
of the contaminated data and the model size. Our results highlight the
importance of analyzing massive web-scale datasets to verify that progress in
NLP is obtained by better language understanding and not better data
exploitation."
Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training,0.578609,"Keyphrase generation is the task of automatically predicting keyphrases given
a piece of long text. Despite its recent flourishing, keyphrase generation on
non-English languages haven't been vastly investigated. In this paper, we call
attention to a new setting named multilingual keyphrase generation and we
contribute two new datasets, EcommerceMKP and AcademicMKP, covering six
languages. Technically, we propose a retrieval-augmented method for
multilingual keyphrase generation to mitigate the data shortage problem in
non-English languages. The retrieval-augmented model leverages keyphrase
annotations in English datasets to facilitate generating keyphrases in
low-resource languages. Given a non-English passage, a cross-lingual dense
passage retrieval module finds relevant English passages. Then the associated
English keyphrases serve as external knowledge for keyphrase generation in the
current language. Moreover, we develop a retriever-generator iterative training
algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual
passage retriever. Comprehensive experiments and ablations show that the
proposed approach outperforms all baselines."
FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation,0.616375,"The speaker-follower models have proven to be effective in
vision-and-language navigation, where a speaker model is used to synthesize new
instructions to augment the training data for a follower navigation model.
However, in many of the previous methods, the generated instructions are not
directly trained to optimize the performance of the follower. In this paper, we
present \textsc{foam}, a \textsc{Fo}llower-\textsc{a}ware speaker
\textsc{M}odel that is constantly updated given the follower feedback, so that
the generated instructions can be more suitable to the current learning state
of the follower. Specifically, we optimize the speaker using a bi-level
optimization framework and obtain its training signals by evaluating the
follower on labeled data. Experimental results on the Room-to-Room and
Room-across-Room datasets demonstrate that our methods can outperform strong
baseline models across settings. Analyses also reveal that our generated
instructions are of higher quality than the baselines."
Gendered Mental Health Stigma in Masked Language Models,0.659273,"Mental health stigma prevents many individuals from receiving the appropriate
care, and social psychology studies have shown that mental health tends to be
overlooked in men. In this work, we investigate gendered mental health stigma
in masked language models. In doing so, we operationalize mental health stigma
by developing a framework grounded in psychology research: we use clinical
psychology literature to curate prompts, then evaluate the models' propensity
to generate gendered words. We find that masked language models capture
societal stigma about gender in mental health: models are consistently more
likely to predict female subjects than male in sentences about having a mental
health condition (32% vs. 19%), and this disparity is exacerbated for sentences
that indicate treatment-seeking behavior. Furthermore, we find that different
models capture dimensions of stigma differently for men and women, associating
stereotypes like anger, blame, and pity more with women with mental health
conditions than with men. In showing the complex nuances of models' gendered
mental health stigma, we demonstrate that context and overlapping dimensions of
identity are important considerations when assessing computational models'
social biases."
Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets,0.595504,"Natural language processing models often exploit spurious correlations
between task-independent features and labels in datasets to perform well only
within the distributions they are trained on, while not generalising to
different task distributions. We propose to tackle this problem by generating a
debiased version of a dataset, which can then be used to train a debiased,
off-the-shelf model, by simply replacing its training data. Our approach
consists of 1) a method for training data generators to generate high-quality,
label-consistent data samples; and 2) a filtering mechanism for removing data
points that contribute to spurious correlations, measured in terms of
z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and
we evaluate on a large suite of debiased, out-of-distribution, and adversarial
test sets. Results show that models trained on our debiased datasets generalise
better than those trained on the original datasets in all settings. On the
majority of the datasets, our method outperforms or performs comparably to
previous state-of-the-art debiasing strategies, and when combined with an
orthogonal technique, product-of-experts, it improves further and outperforms
previous best results of SNLI-hard and MNLI-hard."
Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction,0.630297,"Recently, neural implicit surfaces learning by volume rendering has become
popular for multi-view reconstruction. However, one key challenge remains:
existing approaches lack explicit multi-view geometry constraints, hence
usually fail to generate geometry consistent surface reconstruction. To address
this challenge, we propose geometry-consistent neural implicit surfaces
learning for multi-view reconstruction. We theoretically analyze that there
exists a gap between the volume rendering integral and point-based signed
distance function (SDF) modeling. To bridge this gap, we directly locate the
zero-level set of SDF networks and explicitly perform multi-view geometry
optimization by leveraging the sparse geometry from structure from motion (SFM)
and photometric consistency in multi-view stereo. This makes our SDF
optimization unbiased and allows the multi-view geometry constraints to focus
on the true surface optimization. Extensive experiments show that our proposed
method achieves high-quality surface reconstruction in both complex thin
structures and large smooth regions, thus outperforming the state-of-the-arts
by a large margin."
Towards a Unified Multi-Dimensional Evaluator for Text Generation,0.637384,"Multi-dimensional evaluation is the dominant paradigm for human evaluation in
Natural Language Generation (NLG), i.e., evaluating the generated text from
multiple explainable dimensions, such as coherence and fluency. However,
automatic evaluation in NLG is still dominated by similarity-based metrics, and
we lack a reliable framework for a more comprehensive evaluation of advanced
models. In this paper, we propose a unified multi-dimensional evaluator UniEval
for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,
and by guiding the model with different questions, we can use one evaluator to
evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean
QA format, we are able to introduce an intermediate learning phase that enables
UniEval to incorporate external knowledge from multiple related tasks and gain
further improvement. Experiments on three typical NLG tasks show that UniEval
correlates substantially better with human judgments than existing metrics.
Specifically, compared to the top-performing unified evaluators, UniEval
achieves a 23% higher correlation on text summarization, and over 43% on
dialogue response generation. Also, UniEval demonstrates a strong zero-shot
learning ability for unseen evaluation dimensions and tasks. Source code, data
and all pre-trained evaluators are available on our GitHub repository
(https://github.com/maszhongming/UniEval)."
Red Teaming Language Models with Language Models,0.61459,"Language Models (LMs) often cannot be deployed because of their potential to
harm users in hard-to-predict ways. Prior work identifies harmful behaviors
before deployment by using human annotators to hand-write test cases. However,
human annotation is expensive, limiting the number and diversity of test cases.
In this work, we automatically find cases where a target LM behaves in a
harmful way, by generating test cases (""red teaming"") using another LM. We
evaluate the target LM's replies to generated test questions using a classifier
trained to detect offensive content, uncovering tens of thousands of offensive
replies in a 280B parameter LM chatbot. We explore several methods, from
zero-shot generation to reinforcement learning, for generating test cases with
varying levels of diversity and difficulty. Furthermore, we use prompt
engineering to control LM-generated test cases to uncover a variety of other
harms, automatically finding groups of people that the chatbot discusses in
offensive ways, personal and hospital phone numbers generated as the chatbot's
own contact info, leakage of private training data in generated text, and harms
that occur over the course of a conversation. Overall, LM-based red teaming is
one promising tool (among many needed) for finding and fixing diverse,
undesirable LM behaviors before impacting users."
A Fast Post-Training Pruning Framework for Transformers,0.579286,"Pruning is an effective way to reduce the huge inference cost of Transformer
models. However, prior work on pruning Transformers requires retraining the
models. This can add high training cost and high complexity to model
deployment, making it difficult to use in many practical situations. To address
this, we propose a fast post-training pruning framework for Transformers that
does not require any retraining. Given a resource constraint and a sample
dataset, our framework automatically prunes the Transformer model using
structured sparsity methods. To retain high accuracy without retraining, we
introduce three novel techniques: (i) a lightweight mask search algorithm that
finds which heads and filters to prune based on the Fisher information; (ii)
mask rearrangement that complements the search algorithm; and (iii) mask tuning
that reconstructs the output activations for each layer. We apply our method to
BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD
benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x
speedup in inference latency, while maintaining < 1% loss in accuracy.
Importantly, our framework prunes Transformers in less than 3 minutes on a
single GPU, which is over two orders of magnitude faster than existing pruning
approaches that retrain the models."
BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach,0.576162,"Bilevel optimization (BO) is useful for solving a variety of important
machine learning problems including but not limited to hyperparameter
optimization, meta-learning, continual learning, and reinforcement learning.
Conventional BO methods need to differentiate through the low-level
optimization process with implicit differentiation, which requires expensive
calculations related to the Hessian matrix. There has been a recent quest for
first-order methods for BO, but the methods proposed to date tend to be
complicated and impractical for large-scale deep learning applications. In this
work, we propose a simple first-order BO algorithm that depends only on
first-order gradient information, requires no implicit differentiation, and is
practical and efficient for large-scale non-convex functions in deep learning.
We provide non-asymptotic convergence analysis of the proposed method to
stationary points for non-convex objectives and present empirical results that
show its superior practical performance."
Knowledge Distillation with the Reused Teacher Classifier,0.612966,"Knowledge distillation aims to compress a powerful yet cumbersome teacher
model into a lightweight student model without much sacrifice of performance.
For this purpose, various approaches have been proposed over the past few
years, generally with elaborately designed knowledge representations, which in
turn increase the difficulty of model development and interpretation. In
contrast, we empirically show that a simple knowledge distillation technique is
enough to significantly narrow down the teacher-student performance gap. We
directly reuse the discriminative classifier from the pre-trained teacher model
for student inference and train a student encoder through feature alignment
with a single $\ell_2$ loss. In this way, the student model is able to achieve
exactly the same performance as the teacher model provided that their extracted
features are perfectly aligned. An additional projector is developed to help
the student encoder match with the teacher classifier, which renders our
technique applicable to various teacher and student architectures. Extensive
experiments demonstrate that our technique achieves state-of-the-art results at
the modest cost of compression ratio due to the added projector."
Self-conditioned Embedding Diffusion for Text Generation,0.643317,"Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion."
Text-Only Training for Image Captioning using Noise-Injected CLIP,0.587654,"We consider the task of image-captioning using only the CLIP model and
additional text data at training time, and no additional captioned images. Our
approach relies on the fact that CLIP is trained to make visual and textual
embeddings similar. Therefore, we only need to learn how to translate CLIP
textual embeddings back into text, and we can learn how to do this by learning
a decoder for the frozen CLIP text encoder using only text. We argue that this
intuition is ""almost correct"" because of a gap between the embedding spaces,
and propose to rectify this via noise injection during training. We demonstrate
the effectiveness of our approach by showing SOTA zero-shot image captioning
across four benchmarks, including style transfer. Code, data, and models are
available on GitHub."
Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation,0.60324,"Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime."
Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning,0.640243,"The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages."
SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training,0.59343,"The conventional success of textual classification relies on annotated data,
and the new paradigm of pre-trained language models (PLMs) still requires a few
labeled data for downstream tasks. However, in real-world applications, label
noise inevitably exists in training data, damaging the effectiveness,
robustness, and generalization of the models constructed on such data.
Recently, remarkable achievements have been made to mitigate this dilemma in
visual data, while only a few explore textual data. To fill this gap, we
present SelfMix, a simple yet effective method, to handle label noise in text
classification tasks. SelfMix uses the Gaussian Mixture Model to separate
samples and leverages semi-supervised learning. Unlike previous works requiring
multiple models, our method utilizes the dropout mechanism on a single model to
reduce the confirmation bias in self-training and introduces a textual-level
mixup training strategy. Experimental results on three text classification
benchmarks with different types of text show that the performance of our
proposed method outperforms these strong baselines designed for both textual
and visual data under different noise ratios and noise types. Our code is
available at https://github.com/noise-learning/SelfMix."
Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization,0.575584,"Neural abstractive summarization models are prone to generate summaries which
are factually inconsistent with their source documents. Previous work has
introduced the task of recognizing such factual inconsistency as a downstream
application of natural language inference (NLI). However, state-of-the-art NLI
models perform poorly in this context due to their inability to generalize to
the target task. In this work, we show that NLI models can be effective for
this task when the training data is augmented with high-quality task-oriented
examples. We introduce Falsesum, a data generation pipeline leveraging a
controllable text generation model to perturb human-annotated summaries,
introducing varying types of factual inconsistencies. Unlike previously
introduced document-level NLI datasets, our generated dataset contains examples
that are diverse and inconsistent yet plausible. We show that models trained on
a Falsesum-augmented NLI dataset improve the state-of-the-art performance
across four benchmarks for detecting factual inconsistency in summarization.
  The code to obtain the dataset is available online at
https://github.com/joshbambrick/Falsesum"
Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation,0.617175,"Although the problem of hallucinations in neural machine translation (NMT)
has received some attention, research on this highly pathological phenomenon
lacks solid ground. Previous work has been limited in several ways: it often
resorts to artificial settings where the problem is amplified, it disregards
some (common) types of hallucinations, and it does not validate adequacy of
detection heuristics. In this paper, we set foundations for the study of NMT
hallucinations. First, we work in a natural setting, i.e., in-domain data
without artificial noise neither in training nor in inference. Next, we
annotate a dataset of over 3.4k sentences indicating different kinds of
critical errors and hallucinations. Then, we turn to detection methods and both
revisit methods used previously and propose using glass-box uncertainty-based
detectors. Overall, we show that for preventive settings, (i) previously used
methods are largely inadequate, (ii) sequence log-probability works best and
performs on par with reference-based methods. Finally, we propose
DeHallucinator, a simple method for alleviating hallucinations at test time
that significantly reduces the hallucinatory rate. To ease future research, we
release our annotated dataset for WMT18 German-English data, along with the
model, training data, and code."
Generalizing to New Physical Systems via Context-Informed Dynamics Model,0.609179,"Data-driven approaches to modeling physical systems fail to generalize to
unseen systems that share the same general dynamics with the learning domain,
but correspond to different physical contexts. We propose a new framework for
this key problem, context-informed dynamics adaptation (CoDA), which takes into
account the distributional shift across systems for fast and efficient
adaptation to new dynamics. CoDA leverages multiple environments, each
associated to a different dynamic, and learns to condition the dynamics model
on contextual parameters, specific to each environment. The conditioning is
performed via a hypernetwork, learned jointly with a context vector from
observed data. The proposed formulation constrains the search hypothesis space
to foster fast adaptation and better generalization across environments. We
theoretically motivate our approach and show state-of-the-art generalization
results on a set of nonlinear dynamics, representative of a variety of
application domains. We also show, on these systems, that new system parameters
can be inferred from context vectors with minimal supervision. Code is
available at https://github.com/yuan-yin/CoDA ."
RangeSeg: Range-Aware Real Time Segmentation of 3D LiDAR Point Clouds,0.557557,"Semantic outdoor scene understanding based on 3D LiDAR point clouds is a
challenging task for autonomous driving due to the sparse and irregular data
structure. This paper takes advantages of the uneven range distribution of
different LiDAR laser beams to propose a range aware instance segmentation
network, RangeSeg. RangeSeg uses a shared encoder backbone with two range
dependent decoders. A heavy decoder only computes top of a range image where
the far and small objects locate to improve small object detection accuracy,
and a light decoder computes whole range image for low computational cost. The
results are further clustered by the DBSCAN method with a resolution weighted
distance function to get instance-level segmentation results. Experiments on
the KITTI dataset show that RangeSeg outperforms the state-of-the-art semantic
segmentation methods with enormous speedup and improves the instance-level
segmentation performance on small and far objects. The whole RangeSeg pipeline
meets the real time requirement on NVIDIA\textsuperscript{\textregistered}
JETSON AGX Xavier with 19 frames per second in average."
A Secure Clustering Protocol with Fuzzy Trust Evaluation and Outlier Detection for Industrial Wireless Sensor Networks,0.577775,"Security is one of the major concerns in Industrial Wireless Sensor Networks
(IWSNs). To assure the security in clustered IWSNs, this paper presents a
secure clustering protocol with fuzzy trust evaluation and outlier detection
(SCFTO). Firstly, to deal with the transmission uncertainty in an open wireless
medium, an interval type-2 fuzzy logic controller is adopted to estimate the
trusts. And then a density based outlier detection mechanism is introduced to
acquire an adaptive trust threshold used to isolate the malicious nodes from
being cluster heads. Finally, a fuzzy based cluster heads election method is
proposed to achieve a balance between energy saving and security assurance, so
that a normal sensor node with more residual energy or less confidence on other
nodes has higher probability to be the cluster head. Extensive experiments
verify that our secure clustering protocol can effectively defend the network
against attacks from internal malicious or compromised nodes."
Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation,0.606661,"We present a simple yet effective self-supervised framework for audio-visual
representation learning, to localize the sound source in videos. To understand
what enables to learn useful representations, we systematically investigate the
effects of data augmentations, and reveal that (1) composition of data
augmentations plays a critical role, i.e. explicitly encouraging the
audio-visual representations to be invariant to various transformations~({\em
transformation invariance}); (2) enforcing geometric consistency substantially
improves the quality of learned representations, i.e. the detected sound source
should follow the same transformation applied on input video frames~({\em
transformation equivariance}). Extensive experiments demonstrate that our model
significantly outperforms previous methods on two sound localization
benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally, we also
evaluate audio retrieval and cross-modal retrieval tasks. In both cases, our
self-supervised models demonstrate superior retrieval performances, even
competitive with the supervised approach in audio retrieval. This reveals the
proposed framework learns strong multi-modal representations that are
beneficial to sound localisation and generalization to further applications.
\textit{All codes will be available}."
On the Effectiveness of Parameter-Efficient Fine-Tuning,0.617252,"Fine-tuning pre-trained models has been ubiquitously proven to be effective
in a wide range of NLP tasks. However, fine-tuning the whole model is parameter
inefficient as it always yields an entirely new model for each task. Currently,
many research works propose to only fine-tune a small portion of the parameters
while keeping most of the parameters shared across different tasks. These
methods achieve surprisingly good performance and are shown to be more stable
than their corresponding fully fine-tuned counterparts. However, such kind of
methods is still not well understood. Some natural questions arise: How does
the parameter sparsity lead to promising performance? Why is the model more
stable than the fully fine-tuned models? How to choose the tunable parameters?
In this paper, we first categorize the existing methods into random approaches,
rule-based approaches, and projection-based approaches based on how they choose
which parameters to tune. Then, we show that all of the methods are actually
sparse fine-tuned models and conduct a novel theoretical analysis of them. We
indicate that the sparsity is actually imposing a regularization on the
original model by controlling the upper bound of the stability. Such stability
leads to better generalization capability which has been empirically observed
in a lot of recent research works. Despite the effectiveness of sparsity
grounded by our theory, it still remains an open problem of how to choose the
tunable parameters. To better choose the tunable parameters, we propose a novel
Second-order Approximation Method (SAM) which approximates the original problem
with an analytically solvable optimization function. The tunable parameters are
determined by directly optimizing the approximation function. The experimental
results show that our proposed SAM model outperforms many strong baseline
models and it also verifies our theoretical analysis."
Speech Emotion Recognition using Self-Supervised Features,0.57869,"Self-supervised pre-trained features have consistently delivered state-of-art
results in the field of natural language processing (NLP); however, their
merits in the field of speech emotion recognition (SER) still need further
investigation. In this paper we introduce a modular End-to- End (E2E) SER
system based on an Upstream + Downstream architecture paradigm, which allows
easy use/integration of a large variety of self-supervised features. Several
SER experiments for predicting categorical emotion classes from the IEMOCAP
dataset are performed. These experiments investigate interactions among
fine-tuning of self-supervised feature models, aggregation of frame-level
features into utterance-level features and back-end classification networks.
The proposed monomodal speechonly based system not only achieves SOTA results,
but also brings light to the possibility of powerful and well finetuned
self-supervised acoustic features that reach results similar to the results
achieved by SOTA multimodal systems using both Speech and Text modalities."
Polarimetric Multi-View Inverse Rendering,0.592084,"A polarization camera has great potential for 3D reconstruction since the
angle of polarization (AoP) and the degree of polarization (DoP) of reflected
light are related to an object's surface normal. In this paper, we propose a
novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering
(Polarimetric MVIR) that effectively exploits geometric, photometric, and
polarimetric cues extracted from input multi-view color-polarization images. We
first estimate camera poses and an initial 3D model by geometric reconstruction
with a standard structure-from-motion and multi-view stereo pipeline. We then
refine the initial model by optimizing photometric rendering errors and
polarimetric errors using multi-view RGB, AoP, and DoP images, where we propose
a novel polarimetric cost function that enables an effective constraint on the
estimated surface normal of each vertex, while considering four possible
ambiguous azimuth angles revealed from the AoP measurement. The weight for the
polarimetric cost is effectively determined based on the DoP measurement, which
is regarded as the reliability of polarimetric information. Experimental
results using both synthetic and real data demonstrate that our Polarimetric
MVIR can reconstruct a detailed 3D shape without assuming a specific surface
material and lighting condition."
HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction,0.59356,"We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities."
Uncertainty-guided Source-free Domain Adaptation,0.605125,"Source-free domain adaptation (SFDA) aims to adapt a classifier to an
unlabelled target data set by only using a pre-trained source model. However,
the absence of the source data and the domain shift makes the predictions on
the target data unreliable. We propose quantifying the uncertainty in the
source model predictions and utilizing it to guide the target adaptation. For
this, we construct a probabilistic source model by incorporating priors on the
network parameters inducing a distribution over the model predictions.
Uncertainties are estimated by employing a Laplace approximation and
incorporated to identify target data points that do not lie in the source
manifold and to down-weight them when maximizing the mutual information on the
target data. Unlike recent works, our probabilistic treatment is
computationally lightweight, decouples source training and target adaptation,
and requires no specialized source training or changes of the model
architecture. We show the advantages of uncertainty-guided SFDA over
traditional SFDA in the closed-set and open-set settings and provide empirical
evidence that our approach is more robust to strong domain shifts even without
tuning."
Underspecification in Scene Description-to-Depiction Tasks,0.66495,"Questions regarding implicitness, ambiguity and underspecification are
crucial for understanding the task validity and ethical concerns of multimodal
image+text systems, yet have received little attention to date. This position
paper maps out a conceptual framework to address this gap, focusing on systems
which generate images depicting scenes from scene descriptions. In doing so, we
account for how texts and images convey meaning differently. We outline a set
of core challenges concerning textual and visual ambiguity, as well as risks
that may be amplified by ambiguous and underspecified elements. We propose and
discuss strategies for addressing these challenges, including generating
visually ambiguous images, and generating a set of diverse images."
SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems,0.611943,"We design deep neural networks (DNNs) and corresponding networks' splittings
to distribute DNNs' workload to camera sensors and a centralized aggregator on
head mounted devices to meet system performance targets in inference accuracy
and latency under the given hardware resource constraints. To achieve an
optimal balance among computation, communication, and performance, a
split-aware neural architecture search framework, SplitNets, is introduced to
conduct model designing, splitting, and communication reduction simultaneously.
We further extend the framework to multi-view systems for learning to fuse
inputs from multiple camera sensors with optimal performance and systemic
efficiency. We validate SplitNets for single-view system on ImageNet as well as
multi-view system on 3D classification, and show that the SplitNets framework
achieves state-of-the-art (SOTA) performance and system latency compared with
existing approaches."
DG-STGCN: Dynamic Spatial-Temporal Modeling for Skeleton-based Action Recognition,0.63062,"Graph convolution networks (GCN) have been widely used in skeleton-based
action recognition. We note that existing GCN-based approaches primarily rely
on prescribed graphical structures (ie., a manually defined topology of
skeleton joints), which limits their flexibility to capture complicated
correlations between joints. To move beyond this limitation, we propose a new
framework for skeleton-based action recognition, namely Dynamic Group
Spatio-Temporal GCN (DG-STGCN). It consists of two modules, DG-GCN and DG-TCN,
respectively, for spatial and temporal modeling. In particular, DG-GCN uses
learned affinity matrices to capture dynamic graphical structures instead of
relying on a prescribed one, while DG-TCN performs group-wise temporal
convolutions with varying receptive fields and incorporates a dynamic
joint-skeleton fusion module for adaptive multi-level temporal modeling. On a
wide range of benchmarks, including NTURGB+D, Kinetics-Skeleton, BABEL, and
Toyota SmartHome, DG-STGCN consistently outperforms state-of-the-art methods,
often by a notable margin."
Perceptual Quality Assessment of Omnidirectional Images,0.560729,"Omnidirectional images and videos can provide immersive experience of
real-world scenes in Virtual Reality (VR) environment. We present a perceptual
omnidirectional image quality assessment (IQA) study in this paper since it is
extremely important to provide a good quality of experience under the VR
environment. We first establish an omnidirectional IQA (OIQA) database, which
includes 16 source images and 320 distorted images degraded by 4 commonly
encountered distortion types, namely JPEG compression, JPEG2000 compression,
Gaussian blur and Gaussian noise. Then a subjective quality evaluation study is
conducted on the OIQA database in the VR environment. Considering that humans
can only see a part of the scene at one movement in the VR environment, visual
attention becomes extremely important. Thus we also track head and eye movement
data during the quality rating experiments. The original and distorted
omnidirectional images, subjective quality ratings, and the head and eye
movement data together constitute the OIQA database. State-of-the-art
full-reference (FR) IQA measures are tested on the OIQA database, and some new
observations different from traditional IQA are made."
MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment,0.590395,"The acoustic environment can degrade speech quality during communication
(e.g., video call, remote presentation, outside voice recording), and its
impact is often unknown. Objective metrics for speech quality have proven
challenging to develop given the multi-dimensionality of factors that affect
speech quality and the difficulty of collecting labeled data. Hypothesizing the
impact of acoustics on speech quality, this paper presents MOSRA: a
non-intrusive multi-dimensional speech quality metric that can predict room
acoustics parameters (SNR, STI, T60, DRR, and C50) alongside the overall mean
opinion score (MOS) for speech quality. By explicitly optimizing the model to
learn these room acoustics parameters, we can extract more informative features
and improve the generalization for the MOS task when the training data is
limited. Furthermore, we also show that this joint training method enhances the
blind estimation of room acoustics, improving the performance of current
state-of-the-art models. An additional side-effect of this joint prediction is
the improvement in the explainability of the predictions, which is a valuable
feature for many applications."
End-to-end Generative Pretraining for Multimodal Video Captioning,0.646721,"Recent video and language pretraining frameworks lack the ability to generate
sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new
pretraining framework for learning from unlabelled videos which can be
effectively used for generative tasks such as multimodal video captioning.
Unlike recent video-language pretraining frameworks, our framework trains both
a multimodal video encoder and a sentence decoder jointly. To overcome the lack
of captions in unlabelled videos, we leverage the future utterance as an
additional text source and propose a bidirectional generation objective -- we
generate future utterances given the present mulitmodal context, and also the
present utterance given future observations. With this objective, we train an
encoder-decoder model end-to-end to generate a caption from raw pixels and
transcribed speech directly. Our model achieves state-of-the-art performance
for multimodal video captioning on four standard benchmarks, as well as for
other video understanding tasks such as VideoQA, video retrieval and action
classification."
On the Identifiability of Nonlinear ICA: Sparsity and Beyond,0.59953,"Nonlinear independent component analysis (ICA) aims to recover the underlying
independent latent sources from their observable nonlinear mixtures. How to
make the nonlinear ICA model identifiable up to certain trivial indeterminacies
is a long-standing problem in unsupervised learning. Recent breakthroughs
reformulate the standard independence assumption of sources as conditional
independence given some auxiliary variables (e.g., class labels and/or
domain/time indexes) as weak supervision or inductive bias. However, nonlinear
ICA with unconditional priors cannot benefit from such developments. We explore
an alternative path and consider only assumptions on the mixing process, such
as Structural Sparsity. We show that under specific instantiations of such
constraints, the independent latent sources can be identified from their
nonlinear mixtures up to a permutation and a component-wise transformation,
thus achieving nontrivial identifiability of nonlinear ICA without auxiliary
variables. We provide estimation methods and validate the theoretical results
experimentally. The results on image data suggest that our conditions may hold
in a number of practical data generating processes."
Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction,0.60866,"This paper presents a high-quality human motion prediction method that
accurately predicts future human poses given observed ones. Our method is based
on the observation that a good initial guess of the future poses is very
helpful in improving the forecasting accuracy. This motivates us to propose a
novel two-stage prediction framework, including an init-prediction network that
just computes the good guess and then a formal-prediction network that predicts
the target future poses based on the guess. More importantly, we extend this
idea further and design a multi-stage prediction framework where each stage
predicts initial guess for the next stage, which brings more performance gain.
To fulfill the prediction task at each stage, we propose a network comprising
Spatial Dense Graph Convolutional Networks (S-DGCN) and Temporal Dense Graph
Convolutional Networks (T-DGCN). Alternatively executing the two networks helps
extract spatiotemporal features over the global receptive field of the whole
pose sequence. All the above design choices cooperating together make our
method outperform previous approaches by large margins: 6%-7% on Human3.6M,
5%-10% on CMU-MoCap, and 13%-16% on 3DPW."
Neural-Symbolic Entangled Framework for Complex Query Answering,0.64732,"Answering complex queries over knowledge graphs (KG) is an important yet
challenging task because of the KG incompleteness issue and cascading errors
during reasoning. Recent query embedding (QE) approaches to embed the entities
and relations in a KG and the first-order logic (FOL) queries into a low
dimensional space, answering queries by dense similarity search. However,
previous works mainly concentrate on the target answers, ignoring intermediate
entities' usefulness, which is essential for relieving the cascading error
problem in logical query answering. In addition, these methods are usually
designed with their own geometric or distributional embeddings to handle
logical operators like union, intersection, and negation, with the sacrifice of
the accuracy of the basic operator - projection, and they could not absorb
other embedding methods to their models. In this work, we propose a Neural and
Symbolic Entangled framework (ENeSy) for complex query answering, which enables
the neural and symbolic reasoning to enhance each other to alleviate the
cascading error and KG incompleteness. The projection operator in ENeSy could
be any embedding method with the capability of link prediction, and the other
FOL operators are handled without parameters. With both neural and symbolic
reasoning results contained, ENeSy answers queries in ensembles. ENeSy achieves
the SOTA performance on several benchmarks, especially in the setting of the
training model only with the link prediction task."
FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations,0.576612,"Recent advances in generative adversarial networks have shown that it is
possible to generate high-resolution and hyperrealistic images. However, the
images produced by GANs are only as fair and representative as the datasets on
which they are trained. In this paper, we propose a method for directly
modifying a pre-trained StyleGAN2 model that can be used to generate a balanced
set of images with respect to one (e.g., eyeglasses) or more attributes (e.g.,
gender and eyeglasses). Our method takes advantage of the style space of the
StyleGAN2 model to perform disentangled control of the target attributes to be
debiased. Our method does not require training additional models and directly
debiases the GAN model, paving the way for its use in various downstream
applications. Our experiments show that our method successfully debiases the
GAN model within a few minutes without compromising the quality of the
generated images. To promote fair generative models, we share the code and
debiased models at http://catlab-team.github.io/fairstyle."
VCSE: Time-Domain Visual-Contextual Speaker Extraction Network,0.561652,"Speaker extraction seeks to extract the target speech in a multi-talker
scenario given an auxiliary reference. Such reference can be auditory, i.e., a
pre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic
sequence. References in different modalities provide distinct and complementary
information that could be fused to form top-down attention on the target
speaker. Previous studies have introduced visual and contextual modalities in a
single model. In this paper, we propose a two-stage time-domain
visual-contextual speaker extraction network named VCSE, which incorporates
visual and self-enrolled contextual cues stage by stage to take full advantage
of every modality. In the first stage, we pre-extract a target speech with
visual cues and estimate the underlying phonetic sequence. In the second stage,
we refine the pre-extracted target speech with the self-enrolled contextual
cues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3)
database demonstrate that our proposed VCSE network consistently outperforms
other state-of-the-art baselines."
N-Best Hypotheses Reranking for Text-To-SQL Systems,0.629021,"Text-to-SQL task maps natural language utterances to structured queries that
can be issued to a database. State-of-the-art (SOTA) systems rely on finetuning
large, pre-trained language models in conjunction with constrained decoding
applying a SQL parser. On the well established Spider dataset, we begin with
Oracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's
10-best list, yields a $7.7\%$ absolute improvement in both exact match (EM)
and execution (EX) accuracy, showing significant potential improvements with
reranking. Identifying coherence and correctness as reranking approaches, we
design a model generating a query plan and propose a heuristic schema linking
algorithm. Combining both approaches, with T5-Large, we obtain a consistent
$1\% $ improvement in EM accuracy, and a $~2.5\%$ improvement in EX,
establishing a new SOTA for this task. Our comprehensive error studies on DEV
data show the underlying difficulty in making progress on this task."
Causal Machine Learning for Healthcare and Precision Medicine,0.579389,"Causal machine learning (CML) has experienced increasing popularity in
healthcare. Beyond the inherent capabilities of adding domain knowledge into
learning systems, CML provides a complete toolset for investigating how a
system would react to an intervention (e.g.\ outcome given a treatment).
Quantifying effects of interventions allows actionable decisions to be made
whilst maintaining robustness in the presence of confounders. Here, we explore
how causal inference can be incorporated into different aspects of clinical
decision support (CDS) systems by using recent advances in machine learning.
Throughout this paper, we use Alzheimer's disease (AD) to create examples for
illustrating how CML can be advantageous in clinical scenarios. Furthermore, we
discuss important challenges present in healthcare applications such as
processing high-dimensional and unstructured data, generalisation to
out-of-distribution samples, and temporal relationships, that despite the great
effort from the research community remain to be solved. Finally, we review
lines of research within causal representation learning, causal discovery and
causal reasoning which offer the potential towards addressing the
aforementioned challenges."
Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning,0.585728,"Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice."
Semantic Probabilistic Layers for Neuro-Symbolic Learning,0.611664,"We design a predictive layer for structured-output prediction (SOP) that can
be plugged into any neural network guaranteeing its predictions are consistent
with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer
(SPL) can model intricate correlations, and hard constraints, over a structured
output space all while being amenable to end-to-end learning via maximum
likelihood. SPLs combine exact probabilistic inference with logical reasoning
in a clean and modular way, learning complex distributions and restricting
their support to solutions of the constraint. As such, they can faithfully, and
efficiently, model complex SOP tasks beyond the reach of alternative
neuro-symbolic approaches. We empirically demonstrate that SPLs outperform
these competitors in terms of accuracy on challenging SOP tasks including
hierarchical multi-label classification, pathfinding and preference learning,
while retaining perfect constraint satisfaction."
Forming Effective Human-AI Teams: Building Machine Learning Models that Complement the Capabilities of Multiple Experts,0.570898,"Machine learning (ML) models are increasingly being used in application
domains that often involve working together with human experts. In this
context, it can be advantageous to defer certain instances to a single human
expert when they are difficult to predict for the ML model. While previous work
has focused on scenarios with one distinct human expert, in many real-world
situations several human experts with varying capabilities may be available. In
this work, we propose an approach that trains a classification model to
complement the capabilities of multiple human experts. By jointly training the
classifier together with an allocation system, the classifier learns to
accurately predict those instances that are difficult for the human experts,
while the allocation system learns to pass each instance to the most suitable
team member -- either the classifier or one of the human experts. We evaluate
our proposed approach in multiple experiments on public datasets with
""synthetic"" experts and a real-world medical dataset annotated by multiple
radiologists. Our approach outperforms prior work and is more accurate than the
best human expert or a classifier. Furthermore, it is flexibly adaptable to
teams of varying sizes and different levels of expert diversity."
"CounterGeDi: A controllable approach to generate polite, detoxified and emotional counterspeech",0.662506,"Recently, many studies have tried to create generation models to assist
counter speakers by providing counterspeech suggestions for combating the
explosive proliferation of online hate. However, since these suggestions are
from a vanilla generation model, they might not include the appropriate
properties required to counter a particular hate speech instance. In this
paper, we propose CounterGeDi - an ensemble of generative discriminators (GeDi)
to guide the generation of a DialoGPT model toward more polite, detoxified, and
emotionally laden counterspeech. We generate counterspeech using three datasets
and observe significant improvement across different attribute scores. The
politeness and detoxification scores increased by around 15% and 6%
respectively, while the emotion in the counterspeech increased by at least 10%
across all the datasets. We also experiment with triple-attribute control and
observe significant improvement over single attribute results when combining
complementing attributes, e.g., politeness, joyfulness and detoxification. In
all these experiments, the relevancy of the generated text does not deteriorate
due to the application of these controls"
Legal Prompting: Teaching a Language Model to Think Like a Lawyer,0.561785,"Large language models that are capable of zero or few-shot prompting
approaches have given rise to the new research area of prompt engineering.
Recent advances showed that for example Chain-of-Thought (CoT) prompts can
improve arithmetic or common sense tasks significantly. We explore how such
approaches fare with legal reasoning tasks and take the COLIEE entailment task
based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning
approaches. Our findings show that while CoT prompting and fine-tuning with
explanations approaches show improvements, the best results are produced by
prompts that are derived from specific legal reasoning techniques such as IRAC
(Issue, Rule, Application, Conclusion). Based on our experiments we improve the
2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best
system of 0.6789 accuracy with an accuracy of 0.7431."
Mining Logical Event Schemas From Pre-Trained Language Models,0.658077,"We present NESL (the Neuro-Episodic Schema Learner), an event schema learning
system that combines large language models, FrameNet parsing, a powerful
logical representation of language, and a set of simple behavioral schemas
meant to bootstrap the learning process. In lieu of a pre-made corpus of
stories, our dataset is a continuous feed of ""situation samples"" from a
pre-trained language model, which are then parsed into FrameNet frames, mapped
into simple behavioral schemas, and combined and generalized into complex,
hierarchical schemas for a variety of everyday scenarios. We show that careful
sampling from the language model can help emphasize stereotypical properties of
situations and de-emphasize irrelevant details, and that the resulting schemas
specify situations more comprehensively than those learned by other systems."
SC^2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration,0.600392,"In this paper, we present a second order spatial compatibility (SC^2) measure
based method for efficient and robust point cloud registration (PCR), called
SC^2-PCR. Firstly, we propose a second order spatial compatibility (SC^2)
measure to compute the similarity between correspondences. It considers the
global compatibility instead of local consistency, allowing for more
distinctive clustering between inliers and outliers at early stage. Based on
this measure, our registration pipeline employs a global spectral technique to
find some reliable seeds from the initial correspondences. Then we design a
two-stage strategy to expand each seed to a consensus set based on the SC^2
measure matrix. Finally, we feed each consensus set to a weighted SVD algorithm
to generate a candidate rigid transformation and select the best model as the
final result. Our method can guarantee to find a certain number of outlier-free
consensus sets using fewer samplings, making the model estimation more
efficient and robust. In addition, the proposed SC^2 measure is general and can
be easily plugged into deep learning based frameworks. Extensive experiments
are carried out to investigate the performance of our method. Code will be
available at \url{https://github.com/ZhiChen902/SC2-PCR}."
What Does DALL-E 2 Know About Radiology?,0.626874,"Generative models such as DALL-E 2 could represent a promising future tool
for image generation, augmentation, and manipulation for artificial
intelligence research in radiology provided that these models have sufficient
medical domain knowledge. Here we show that DALL-E 2 has learned relevant
representations of X-ray images with promising capabilities in terms of
zero-shot text-to-image generation of new images, continuation of an image
beyond its original boundaries, or removal of elements, while pathology
generation or CT, MRI, and ultrasound images are still limited. The use of
generative models for augmenting and generating radiological data thus seems
feasible, even if further fine-tuning and adaptation of these models to the
respective domain is required beforehand."
Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics,0.611798,"Modern machine learning research relies on relatively few carefully curated
datasets. Even in these datasets, and typically in `untidy' or raw data,
practitioners are faced with significant issues of data quality and diversity
which can be prohibitively labor intensive to address. Existing methods for
dealing with these challenges tend to make strong assumptions about the
particular issues at play, and often require a priori knowledge or metadata
such as domain labels. Our work is orthogonal to these methods: we instead
focus on providing a unified and efficient framework for Metadata Archaeology
-- uncovering and inferring metadata of examples in a dataset. We curate
different subsets of data that might exist in a dataset (e.g. mislabeled,
atypical, or out-of-distribution examples) using simple transformations, and
leverage differences in learning dynamics between these probe suites to infer
metadata of interest. Our method is on par with far more sophisticated
mitigation methods across different tasks: identifying and correcting
mislabeled examples, classifying minority-group samples, prioritizing points
relevant for training and enabling scalable human auditing of relevant
examples."
Pretraining is All You Need for Image-to-Image Translation,0.592527,"We propose to use pretraining to boost general image-to-image translation.
Prior image-to-image translation methods usually need dedicated architectural
design and train individual translation models from scratch, struggling for
high-quality generation of complex scenes, especially when paired training data
are not abundant. In this paper, we regard each image-to-image translation
problem as a downstream task and introduce a simple and generic framework that
adapts a pretrained diffusion model to accommodate various kinds of
image-to-image translation. We also propose adversarial training to enhance the
texture synthesis in the diffusion model training, in conjunction with
normalized guidance sampling to improve the generation quality. We present
extensive empirical comparison across various tasks on challenging benchmarks
such as ADE20K, COCO-Stuff, and DIODE, showing the proposed pretraining-based
image-to-image translation (PITI) is capable of synthesizing images of
unprecedented realism and faithfulness."
SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization,0.577852,"Matching-based methods, especially those based on space-time memory, are
significantly ahead of other solutions in semi-supervised video object
segmentation (VOS). However, continuously growing and redundant template
features lead to an inefficient inference. To alleviate this, we propose a
novel Sequential Weighted Expectation-Maximization (SWEM) network to greatly
reduce the redundancy of memory features. Different from the previous methods
which only detect feature redundancy between frames, SWEM merges both
intra-frame and inter-frame similar features by leveraging the sequential
weighted EM algorithm. Further, adaptive weights for frame features endow SWEM
with the flexibility to represent hard samples, improving the discrimination of
templates. Besides, the proposed method maintains a fixed number of template
features in memory, which ensures the stable inference complexity of the VOS
system. Extensive experiments on commonly used DAVIS and YouTube-VOS datasets
verify the high efficiency (36 FPS) and high performance (84.3\%
$\mathcal{J}\&\mathcal{F}$ on DAVIS 2017 validation dataset) of SWEM. Code is
available at: https://github.com/lmm077/SWEM."
Excavating RoI Attention for Underwater Object Detection,0.624596,"Self-attention is one of the most successful designs in deep learning, which
calculates the similarity of different tokens and reconstructs the feature
based on the attention matrix. Originally designed for NLP, self-attention is
also popular in computer vision, and can be categorized into pixel-level
attention and patch-level attention. In object detection, RoI features can be
seen as patches from base feature maps. This paper aims to apply the attention
module to RoI features to improve performance. Instead of employing an original
self-attention module, we choose the external attention module, a modified
self-attention with reduced parameters. With the proposed double head structure
and the Positional Encoding module, our method can achieve promising
performance in object detection. The comprehensive experiments show that it
achieves promising performance, especially in the underwater object detection
dataset. The code will be avaiable in:
https://github.com/zsyasd/Excavating-RoI-Attention-for-Underwater-Object-Detection"
SimpleTrack: Rethinking and Improving the JDE Approach for Multi-Object Tracking,0.567732,"Joint detection and embedding (JDE) based methods usually estimate bounding
boxes and embedding features of objects with a single network in Multi-Object
Tracking (MOT). In the tracking stage, JDE-based methods fuse the target motion
information and appearance information by applying the same rule, which could
fail when the target is briefly lost or blocked. To overcome this problem, we
propose a new association matrix, the Embedding and Giou matrix, which combines
embedding cosine distance and Giou distance of objects. To further improve the
performance of data association, we develop a simple, effective tracker named
SimpleTrack, which designs a bottom-up fusion method for Re-identity and
proposes a new tracking strategy based on our EG matrix. The experimental
results indicate that SimpleTrack has powerful data association capability,
e.g., 61.6 HOTA and 76.3 IDF1 on MOT17. In addition, we apply the EG matrix to
5 different state-of-the-art JDE-based methods and achieve significant
improvements in IDF1, HOTA and IDsw metrics, and increase the tracking speed of
these methods by about 20%."
HLDC: Hindi Legal Documents Corpus,0.57676,"Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: https://github.com/Exploration-Lab/HLDC"
Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training,0.596046,"The paper presents a simple and effective learning-based method for computing
a discriminative 3D point cloud descriptor for place recognition purposes.
Recent state-of-the-art methods have relatively complex architectures such as
multi-scale oyramid of point Transformers combined with a pyramid of feature
aggregation modules. Our method uses a simple and efficient 3D convolutional
feature extraction, based on a sparse voxelized representation, enhanced with
channel attention blocks. We employ recent advances in image retrieval and
propose a modified version of a loss function based on a differentiable average
precision approximation. Such loss function requires training with very large
batches for the best results. This is enabled by using multistaged
backpropagation. Experimental evaluation on the popular benchmarks proves the
effectiveness of our approach, with a consistent improvement over the state of
the art"
Speech Synthesis with Mixed Emotions,0.5801,"Emotional speech synthesis aims to synthesize human voices with various
emotional effects. The current studies are mostly focused on imitating an
averaged style belonging to a specific emotion type. In this paper, we seek to
generate speech with a mixture of emotions at run-time. We propose a novel
formulation that measures the relative difference between the speech samples of
different emotions. We then incorporate our formulation into a
sequence-to-sequence emotional text-to-speech framework. During the training,
the framework does not only explicitly characterize emotion styles, but also
explores the ordinal nature of emotions by quantifying the differences with
other emotions. At run-time, we control the model to produce the desired
emotion mixture by manually defining an emotion attribute vector. The objective
and subjective evaluations have validated the effectiveness of the proposed
framework. To our best knowledge, this research is the first study on
modelling, synthesizing, and evaluating mixed emotions in speech."
Accelerating DETR Convergence via Semantic-Aligned Matching,0.614704,"The recently developed DEtection TRansformer (DETR) establishes a new object
detection paradigm by eliminating a series of hand-crafted components. However,
DETR suffers from extremely slow convergence, which increases the training cost
significantly. We observe that the slow convergence is largely attributed to
the complication in matching object queries with target features in different
feature embedding spaces. This paper presents SAM-DETR, a
Semantic-Aligned-Matching DETR that greatly accelerates DETR's convergence
without sacrificing its accuracy. SAM-DETR addresses the convergence issue from
two perspectives. First, it projects object queries into the same embedding
space as encoded image features, where the matching can be accomplished
efficiently with aligned semantics. Second, it explicitly searches salient
points with the most discriminative features for semantic-aligned matching,
which further speeds up the convergence and boosts detection accuracy as well.
Being like a plug and play, SAM-DETR complements existing convergence solutions
well yet only introduces slight computational overhead. Extensive experiments
show that the proposed SAM-DETR achieves superior convergence as well as
competitive detection accuracy. The implementation codes are available at
https://github.com/ZhangGongjie/SAM-DETR."
Diverse Plausible 360-Degree Image Outpainting for Efficient 3DCG Background Creation,0.596141,"We address the problem of generating a 360-degree image from a single image
with a narrow field of view by estimating its surroundings. Previous methods
suffered from overfitting to the training resolution and deterministic
generation. This paper proposes a completion method using a transformer for
scene modeling and novel methods to improve the properties of a 360-degree
image on the output image. Specifically, we use CompletionNets with a
transformer to perform diverse completions and AdjustmentNet to match color,
stitching, and resolution with an input image, enabling inference at any
resolution. To improve the properties of a 360-degree image on an output image,
we also propose WS-perceptual loss and circular inference. Thorough experiments
show that our method outperforms state-of-the-art (SOTA) methods both
qualitatively and quantitatively. For example, compared to SOTA methods, our
method completes images 16 times larger in resolution and achieves 1.7 times
lower Frechet inception distance (FID). Furthermore, we propose a pipeline that
uses the completion results for lighting and background of 3DCG scenes. Our
plausible background completion enables perceptually natural results in the
application of inserting virtual objects with specular surfaces."
Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training,0.658667,"Training deep neural networks (DNNs) is becoming increasingly more resource-
and energy-intensive every year. Unfortunately, existing works primarily focus
on optimizing DNN training for faster completion, often without considering the
impact on energy efficiency.
  In this paper, we observe that common practices to improve training
performance can often lead to inefficient energy usage. More importantly, we
demonstrate that there is a tradeoff between energy consumption and performance
optimization. To this end, we propose Zeus, an optimization framework to
navigate this tradeoff by automatically finding optimal job- and GPU-level
configurations for recurring DNN training jobs. Zeus uses an online
exploration-exploitation approach in conjunction with just-in-time energy
profiling, averting the need for expensive offline measurements, while adapting
to data drifts over time. Our evaluation shows that Zeus can improve the energy
efficiency of DNN training by 15.3%-75.8% for diverse workloads."
Designing Network Design Strategies Through Gradient Path Analysis,0.637453,"Designing a high-efficiency and high-quality expressive network architecture
has always been the most important research topic in the field of deep
learning. Most of today's network design strategies focus on how to integrate
features extracted from different layers, and how to design computing units to
effectively extract these features, thereby enhancing the expressiveness of the
network. This paper proposes a new network design strategy, i.e., to design the
network architecture based on gradient path analysis. On the whole, most of
today's mainstream network design strategies are based on feed forward path,
that is, the network architecture is designed based on the data path. In this
paper, we hope to enhance the expressive ability of the trained model by
improving the network learning ability. Due to the mechanism driving the
network parameter learning is the backward propagation algorithm, we design
network design strategies based on back propagation path. We propose the
gradient path design strategies for the layer-level, the stage-level, and the
network-level, and the design strategies are proved to be superior and feasible
from theoretical analysis and experiments."
Mildly Conservative Q-Learning for Offline Reinforcement Learning,0.595894,"Offline reinforcement learning (RL) defines the task of learning from a
static logged dataset without continually interacting with the environment. The
distribution shift between the learned policy and the behavior policy makes it
necessary for the value function to stay conservative such that
out-of-distribution (OOD) actions will not be severely overestimated. However,
existing approaches, penalizing the unseen actions or regularizing with the
behavior policy, are too pessimistic, which suppresses the generalization of
the value function and hinders the performance improvement. This paper explores
mild but enough conservatism for offline learning while not harming
generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD
actions are actively trained by assigning them proper pseudo Q values. We
theoretically show that MCQ induces a policy that behaves at least as well as
the behavior policy and no erroneous overestimation will occur for OOD actions.
Experimental results on the D4RL benchmarks demonstrate that MCQ achieves
remarkable performance compared with prior work. Furthermore, MCQ shows
superior generalization ability when transferring from offline to online, and
significantly outperforms baselines. Our code is publicly available at
https://github.com/dmksjfl/MCQ."
SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels,0.611011,"Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC."
SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds,0.598635,"With the recent availability and affordability of commercial depth sensors
and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets
have been publicized to facilitate research in 3D computer vision. However,
existing datasets either cover relatively small areas or have limited semantic
annotations. Fine-grained understanding of urban-scale 3D scenes is still in
its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV
photogrammetry point cloud dataset consisting of nearly three billion points
collected from three UK cities, covering 7.6 km^2. Each point in the dataset
has been labelled with fine-grained semantic annotations, resulting in a
dataset that is three times the size of the previous existing largest
photogrammetric point cloud dataset. In addition to the more commonly
encountered categories such as road and vegetation, urban-level categories
including rail, bridge, and river are also included in our dataset. Based on
this dataset, we further build a benchmark to evaluate the performance of
state-of-the-art segmentation algorithms. In particular, we provide a
comprehensive analysis and identify several key challenges limiting urban-scale
point cloud understanding. The dataset is available at
http://point-cloud-analysis.cs.ox.ac.uk."
"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",0.661352,"Human language offers a powerful window into our thoughts -- we tell stories,
give explanations, and express our beliefs and goals through words. Abundant
evidence also suggests that language plays a developmental role in structuring
our learning. Here, we ask: how much of human-like thinking can be captured by
learning statistical patterns in language alone? We first contribute a new
challenge benchmark for comparing humans and distributional large language
models (LLMs). Our benchmark contains two problem-solving domains (planning and
explanation generation) and is designed to require generalization to new,
out-of-distribution problems expressed in language. We find that humans are far
more robust than LLMs on this benchmark. Next, we propose a hybrid
Parse-and-Solve model, which augments distributional LLMs with a structured
symbolic reasoning module. We find that this model shows more robust adaptation
to out-of-distribution planning problems, demonstrating the promise of hybrid
AI models for more human-like reasoning."
Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control,0.602108,"Abstractive summarization systems leveraging pre-training language models
have achieved superior results on benchmark datasets. However, such models have
been shown to be more prone to hallucinate facts that are unfaithful to the
input context. In this paper, we propose a method to remedy entity-level
extrinsic hallucinations with Entity Coverage Control (ECC). We first compute
entity coverage precision and prepend the corresponding control code for each
training example, which implicitly guides the model to recognize faithfulness
contents in the training phase. We further extend our method via intermediate
fine-tuning on large but noisy data extracted from Wikipedia to unlock
zero-shot summarization. We show that the proposed method leads to more
faithful and salient abstractive summarization in supervised fine-tuning and
zero-shot settings according to our experimental results on three benchmark
datasets XSum, Pubmed, and SAMSum of very different domains and styles."
Single Stage Virtual Try-on via Deformable Attention Flows,0.561338,"Virtual try-on aims to generate a photo-realistic fitting result given an
in-shop garment and a reference person image. Existing methods usually build up
multi-stage frameworks to deal with clothes warping and body blending
respectively, or rely heavily on intermediate parser-based labels which may be
noisy or even inaccurate. To solve the above challenges, we propose a
single-stage try-on framework by developing a novel Deformable Attention Flow
(DAFlow), which applies the deformable attention scheme to multi-flow
estimation. With pose keypoints as the guidance only, the self- and
cross-deformable attention flows are estimated for the reference person and the
garment images, respectively. By sampling multiple flow fields, the
feature-level and pixel-level information from different semantic areas are
simultaneously extracted and merged through the attention mechanism. It enables
clothes warping and body synthesizing at the same time which leads to
photo-realistic results in an end-to-end manner. Extensive experiments on two
try-on datasets demonstrate that our proposed method achieves state-of-the-art
performance both qualitatively and quantitatively. Furthermore, additional
experiments on the other two image editing tasks illustrate the versatility of
our method for multi-view synthesis and image animation."
The Arabic Ontology -- An Arabic Wordnet with Ontologically Clean Content,0.63486,"We present a formal Arabic wordnet built on the basis of a carefully designed
ontology hereby referred to as the Arabic Ontology. The ontology provides a
formal representation of the concepts that the Arabic terms convey, and its
content was built with ontological analysis in mind, and benchmarked to
scientific advances and rigorous knowledge sources as much as this is possible,
rather than to only speakers' beliefs as lexicons typically are. A
comprehensive evaluation was conducted thereby demonstrating that the current
version of the top-levels of the ontology can top the majority of the Arabic
meanings. The ontology consists currently of about 1,300 well-investigated
concepts in addition to 11,000 concepts that are partially validated. The
ontology is accessible and searchable through a lexicographic search engine
(https://ontology.birzeit.edu) that also includes about 150 Arabic-multilingual
lexicons, and which are being mapped and enriched using the ontology. The
ontology is fully mapped with Princeton WordNet, Wikidata, and other resources."
GIAOTracker: A comprehensive framework for MCMOT with global information and optimizing strategies in VisDrone 2021,0.606136,"In recent years, algorithms for multiple object tracking tasks have benefited
from great progresses in deep models and video quality. However, in challenging
scenarios like drone videos, they still suffer from problems, such as small
objects, camera movements and view changes. In this paper, we propose a new
multiple object tracker, which employs Global Information And some Optimizing
strategies, named GIAOTracker. It consists of three stages, i.e., online
tracking, global link and post-processing. Given detections in every frame, the
first stage generates reliable tracklets using information of camera motion,
object motion and object appearance. Then they are associated into trajectories
by exploiting global clues and refined through four post-processing methods.
With the effectiveness of the three stages, GIAOTracker achieves
state-of-the-art performance on the VisDrone MOT dataset and wins the 3rd place
in the VisDrone2021 MOT Challenge."
Dual-Branched Spatio-temporal Fusion Network for Multi-horizon Tropical Cyclone Track Forecast,0.575225,"Tropical cyclone (TC) is an extreme tropical weather system and its
trajectory can be described by a variety of spatio-temporal data. Effective
mining of these data is the key to accurate TCs track forecasting. However,
existing methods face the problem that the model complexity is too high or it
is difficult to efficiently extract features from multi-modal data. In this
paper, we propose the Dual-Branched spatio-temporal Fusion Network (DBF-Net) --
a novel multi-horizon tropical cyclone track forecasting model which fuses the
multi-modal features efficiently. DBF-Net contains a TC features branch that
extracts temporal features from 1D inherent features of TCs and a pressure
field branch that extracts spatio-temporal features from reanalysis 2D pressure
field. Through the encoder-decoder-based architecture and efficient feature
fusion, DBF-Net can fully mine the information of the two types of data, and
achieve good TCs track prediction results. Extensive experiments on historical
TCs track data in the Northwest Pacific show that our DBF-Net achieves
significant improvement compared with existing statistical and deep learning
TCs track forecast methods."
"Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",0.617061,"The propensity of abstractive summarization models to make factual errors has
been studied extensively, including design of metrics to detect factual errors
and annotation of errors in current systems' outputs. However, the
ever-evolving nature of summarization systems, metrics, and annotated
benchmarks makes factuality evaluation a moving target, and drawing clear
comparisons among metrics has become increasingly difficult. In this work, we
aggregate factuality error annotations from nine existing datasets and stratify
them according to the underlying summarization model. We compare performance of
state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on
this stratified benchmark and show that their performance varies significantly
across different types of summarization models. Critically, our analysis shows
that much of the recent improvement in the factuality detection space has been
on summaries from older (pre-Transformer) models instead of more relevant
recent summarization models. We further perform a finer-grained analysis per
error-type and find similar performance variance across error types for
different factuality metrics. Our results show that no one metric is superior
in all settings or for all error types, and we provide recommendations for best
practices given these insights."
Relationformer: A Unified Framework for Image-to-Graph Generation,0.57565,"A comprehensive representation of an image requires understanding objects and
their mutual relationship, especially in image-to-graph generation, e.g., road
network extraction, blood-vessel network extraction, or scene graph generation.
Traditionally, image-to-graph generation is addressed with a two-stage approach
consisting of object detection followed by a separate relation prediction,
which prevents simultaneous object-relation interaction. This work proposes a
unified one-stage transformer-based framework, namely Relationformer, that
jointly predicts objects and their relations. We leverage direct set-based
object prediction and incorporate the interaction among the objects to learn an
object-relation representation jointly. In addition to existing [obj]-tokens,
we propose a novel learnable token, namely [rln]-token. Together with
[obj]-tokens, [rln]-token exploits local and global semantic reasoning in an
image through a series of mutual associations. In combination with the
pair-wise [obj]-token, the [rln]-token contributes to a computationally
efficient relation prediction. We achieve state-of-the-art performance on
multiple, diverse and multi-domain datasets that demonstrate our approach's
effectiveness and generalizability."
Prior-Guided One-shot Neural Architecture Search,0.607863,"Neural architecture search methods seek optimal candidates with efficient
weight-sharing supernet training. However, recent studies indicate poor ranking
consistency about the performance between stand-alone architectures and
shared-weight networks. In this paper, we present Prior-Guided One-shot NAS
(PGONAS) to strengthen the ranking correlation of supernets. Specifically, we
first explore the effect of activation functions and propose a balanced
sampling strategy based on the Sandwich Rule to alleviate weight coupling in
the supernet. Then, FLOPs and Zen-Score are adopted to guide the training of
supernet with ranking correlation loss. Our PGONAS ranks 3rd place in the
supernet Track Track of CVPR2022 Second lightweight NAS challenge. Code is
available in
https://github.com/pprp/CVPR2022-NAS?competition-Track1-3th-solution."
A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction,0.630915,"Most previous studies aim at extracting events from a single sentence, while
document-level event extraction still remains under-explored. In this paper, we
focus on extracting event arguments from an entire document, which mainly faces
two critical problems: a) the long-distance dependency between trigger and
arguments over sentences; b) the distracting context towards an event in the
document. To address these issues, we propose a Two-Stream Abstract meaning
Representation enhanced extraction model (TSAR). TSAR encodes the document from
different perspectives by a two-stream encoding module, to utilize local and
global information and lower the impact of distracting context. Besides, TSAR
introduces an AMR-guided interaction module to capture both intra-sentential
and inter-sentential features, based on the locally and globally constructed
AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the
boundary information for text spans explicitly. Extensive experiments
illustrate that TSAR outperforms previous state-of-the-art by a large margin,
with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents
datasets respectively, showing the superiority in the cross-sentence arguments
extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR."
Multitrack Music Transformer,0.604469,"Existing approaches for generating multitrack music with transformer models
have been limited in terms of the number of instruments, the length of the
music segments and slow inference. This is partly due to the memory
requirements of the lengthy input sequences necessitated by existing
representations. In this work, we propose a new multitrack music representation
that allows a diverse set of instruments while keeping a short sequence length.
Our proposed Multitrack Music Transformer (MMT) achieves comparable performance
with state-of-the-art systems, landing in between two recently proposed models
in a subjective listening test, while achieving substantial speedups and memory
reductions over both, making the method attractive for real time improvisation
or near real time creative applications. Further, we propose a new measure for
analyzing musical self-attention and show that the trained model attends more
to notes that form a consonant interval with the current note and to notes that
are 4N beats away from the current step."
FPIC: A Novel Semantic Dataset for Optical PCB Assurance,0.636013,"Outsourced printed circuit board (PCB) fabrication necessitates increased
hardware assurance capabilities. Several assurance techniques based on
automated optical inspection (AOI) have been proposed that leverage PCB images
acquired using digital cameras. We review state-of-the-art AOI techniques and
observe a strong, rapid trend toward machine learning (ML) solutions. These
require significant amounts of labeled ground truth data, which is lacking in
the publicly available PCB data space. We contribute the FICS PCB Image
Collection (FPIC) dataset to address this need. Additionally, we outline new
hardware security methodologies enabled by our data set."
Visual Abductive Reasoning,0.57481,"Abductive reasoning seeks the likeliest possible explanation for partial
observations. Although abduction is frequently employed in human daily
reasoning, it is rarely explored in computer vision literature. In this paper,
we propose a new task and dataset, Visual Abductive Reasoning (VAR), for
examining abductive reasoning ability of machine intelligence in everyday
visual situations. Given an incomplete set of visual events, AI systems are
required to not only describe what is observed, but also infer the hypothesis
that can best explain the visual premise. Based on our large-scale VAR dataset,
we devise a strong baseline model, Reasoner (causal-and-cascaded reasoning
Transformer). First, to capture the causal structure of the observations, a
contextualized directional position embedding strategy is adopted in the
encoder, that yields discriminative representations for the premise and
hypothesis. Then, multiple decoders are cascaded to generate and progressively
refine the premise and hypothesis sentences. The prediction scores of the
sentences are used to guide cross-sentence information flow in the cascaded
reasoning procedure. Our VAR benchmarking results show that Reasoner surpasses
many famous video-language models, while still being far behind human
performance. This work is expected to foster future efforts in the
reasoning-beyond-observation paradigm."
EOD: The IEEE GRSS Earth Observation Database,0.628268,"In the era of deep learning, annotated datasets have become a crucial asset
to the remote sensing community. In the last decade, a plethora of different
datasets was published, each designed for a specific data type and with a
specific task or application in mind. In the jungle of remote sensing datasets,
it can be hard to keep track of what is available already. With this paper, we
introduce EOD - the IEEE GRSS Earth Observation Database (EOD) - an interactive
online platform for cataloguing different types of datasets leveraging remote
sensing imagery."
Questions Are All You Need to Train a Dense Passage Retriever,0.706879,"We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses."
Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding,0.731829,"Contrastive learning has become a new paradigm for unsupervised sentence
embeddings. Previous studies focus on instance-wise contrastive learning,
attempting to construct positive pairs with textual data augmentation. In this
paper, we propose a novel Contrastive learning method with Prompt-derived
Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts,
we construct virtual semantic prototypes to each instance, and derive negative
prototypes by using the negative form of the prompts. Using a prototypical
contrastive loss, we enforce the anchor sentence embedding to be close to its
corresponding semantic prototypes, and far apart from the negative prototypes
as well as the prototypes of other sentences. Extensive experimental results on
semantic textual similarity, transfer, and clustering tasks demonstrate the
effectiveness of our proposed model compared to strong baselines. Code is
available at https://github.com/lemon0830/promptCSE."
Cross-modal Contrastive Learning for Speech Translation,0.669684,"How can we learn unified representations for spoken utterances and their
written text? Learning similar representations for semantically similar speech
and text is important for speech translation. To this end, we propose ConST, a
cross-modal contrastive learning method for end-to-end speech-to-text
translation. We evaluate ConST and a variety of previous baselines on a popular
benchmark MuST-C. Experiments show that the proposed ConST consistently
outperforms the previous methods on, and achieves an average BLEU of 29.4. The
analysis further verifies that ConST indeed closes the representation gap of
different modalities -- its learned representation improves the accuracy of
cross-modal speech-text retrieval from 4% to 88%. Code and models are available
at https://github.com/ReneeYe/ConST."
WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation,0.683889,"Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN"
From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer,0.671794,"Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC."
CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,0.764101,"Constructing benchmarks that test the abilities of modern natural language
understanding models is difficult - pre-trained language models exploit
artifacts in benchmarks to achieve human parity, but still fail on adversarial
examples and make errors that demonstrate a lack of common sense. In this work,
we propose gamification as a framework for data construction. The goal of
players in the game is to compose questions that mislead a rival AI while using
specific phrases for extra points. The game environment leads to enhanced user
engagement and simultaneously gives the game designer control over the
collected data, allowing us to collect high-quality data at scale. Using our
method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and
demonstrate its difficulty for models that are orders-of-magnitude larger than
the AI used in the game itself. Our best baseline, the T5-based Unicorn with
11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3
(52.9%) in a few-shot inference setup. Both score well below human performance
which is at 94.1%."
Towards a Cleaner Document-Oriented Multilingual Crawled Corpus,0.69908,"The need for raw large raw corpora has dramatically increased in recent years
with the introduction of transfer learning and semi-supervised learning methods
to Natural Language Processing. And while there have been some recent attempts
to manually curate the amount of data necessary to train large language models,
the main way to obtain this data is still through automatic web crawling. In
this paper we take the existing multilingual web corpus OSCAR and its pipeline
Ungoliant that extracts and classifies data from Common Crawl at the line
level, and propose a set of improvements and automatic annotations in order to
produce a new document-oriented version of OSCAR that could prove more suitable
to pre-train large generative language models as well as hopefully other
applications in Natural Language Processing and Digital Humanities."
How to Fine-Tune Vision Models with SGD,0.699011,"SGD and AdamW are the two most used optimizers for fine-tuning large neural
networks in computer vision. When the two methods perform the same, SGD is
preferable because it uses less memory (12 bytes/parameter with momentum and 8
bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite
of downstream tasks, especially those with distribution shifts, we find that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first ""embedding""
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: freezing the
embedding layer (less than 1% of the parameters) leads to SGD with or without
momentum performing slightly better than AdamW while using less memory (e.g.,
on ViT-L, SGD uses 33% less GPU memory). Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet."
DiGamma: Domain-aware Genetic Algorithm for HW-Mapping Co-optimization for DNN Accelerators,0.73134,"The design of DNN accelerators includes two key parts: HW resource
configuration and mapping strategy. Intensive research has been conducted to
optimize each of them independently. Unfortunately, optimizing for both
together is extremely challenging due to the extremely large cross-coupled
search space. To address this, in this paper, we propose a HW-Mapping
co-optimization framework, an efficient encoding of the immense design space
constructed by HW and Mapping, and a domain-aware genetic algorithm, named
DiGamma, with specialized operators for improving search efficiency. We
evaluate DiGamma with seven popular DNNs models with different properties. Our
evaluations show DiGamma can achieve (geomean) 3.0x and 10.0x speedup,
comparing to the best-performing baseline optimization algorithms, in edge and
cloud settings."
M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction,0.774122,"Predicting future motions of road participants is an important task for
driving autonomously in urban scenes. Existing models excel at predicting
marginal trajectories for single agents, yet it remains an open question to
jointly predict scene compliant trajectories over multiple agents. The
challenge is due to exponentially increasing prediction space as a function of
the number of agents. In this work, we exploit the underlying relations between
interacting agents and decouple the joint prediction problem into marginal
prediction problems. Our proposed approach M2I first classifies interacting
agents as pairs of influencers and reactors, and then leverages a marginal
prediction model and a conditional prediction model to predict trajectories for
the influencers and reactors, respectively. The predictions from interacting
agents are combined and selected according to their joint likelihoods.
Experiments show that our simple but effective approach achieves
state-of-the-art performance on the Waymo Open Motion Dataset interactive
prediction benchmark."
Learning Progressive Modality-shared Transformers for Effective Visible-Infrared Person Re-identification,0.708943,"Visible-Infrared Person Re-Identification (VI-ReID) is a challenging
retrieval task under complex modality changes. Existing methods usually focus
on extracting discriminative visual features while ignoring the reliability and
commonality of visual features between different modalities. In this paper, we
propose a novel deep learning framework named Progressive Modality-shared
Transformer (PMT) for effective VI-ReID. To reduce the negative effect of
modality gaps, we first take the gray-scale images as an auxiliary modality and
propose a progressive learning strategy. Then, we propose a Modality-Shared
Enhancement Loss (MSEL) to guide the model to explore more reliable identity
information from modality-shared features. Finally, to cope with the problem of
large intra-class differences and small inter-class differences, we propose a
Discriminative Center Loss (DCL) combined with the MSEL to further improve the
discrimination of reliable features. Extensive experiments on SYSU-MM01 and
RegDB datasets show that our proposed framework performs better than most
state-of-the-art methods. For model reproduction, we release the source code at
https://github.com/hulu88/PMT."
Event Transformer. A sparse-aware solution for efficient event data processing,0.696371,"Event cameras are sensors of great interest for many applications that run in
low-resource and challenging environments. They log sparse illumination changes
with high temporal resolution and high dynamic range, while they present
minimal power consumption. However, top-performing methods often ignore
specific event-data properties, leading to the development of generic but
computationally expensive algorithms. Efforts toward efficient solutions
usually do not achieve top-accuracy results for complex tasks. This work
proposes a novel framework, Event Transformer (EvT), that effectively takes
advantage of event-data properties to be highly efficient and accurate. We
introduce a new patch-based event representation and a compact transformer-like
architecture to process it. EvT is evaluated on different event-based
benchmarks for action and gesture recognition. Evaluation results show better
or comparable accuracy to the state-of-the-art while requiring significantly
less computation resources, which makes EvT able to work with minimal latency
both on GPU and CPU."
"Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus",0.771684,"The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the
performance of general artificial intelligence algorithms. The ARC's focus on
broad generalization and few-shot learning has made it difficult to solve using
pure machine learning. A more promising approach has been to perform program
synthesis within an appropriately designed Domain Specific Language (DSL).
However, these too have seen limited success. We propose Abstract Reasoning
with Graph Abstractions (ARGA), a new object-centric framework that first
represents images using graphs and then performs a search for a correct program
in a DSL that is based on the abstracted graph space. The complexity of this
combinatorial search is tamed through the use of constraint acquisition, state
hashing, and Tabu search. An extensive set of experiments demonstrates the
promise of ARGA in tackling some of the complicated object-centric tasks of the
ARC rather efficiently, producing programs that are correct and easy to
understand."
RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs,0.762052,"Blind face restoration is to recover a high-quality face image from unknown
degradations. As face image contains abundant contextual information, we
propose a method, RestoreFormer, which explores fully-spatial attentions to
model contextual information and surpasses existing works that use local
operators. RestoreFormer has several benefits compared to prior arts. First,
unlike the conventional multi-head self-attention in previous Vision
Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention
layer to learn fully-spatial interactions between corrupted queries and
high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are
sampled from a reconstruction-oriented high-quality dictionary, whose elements
are rich in high-quality facial features specifically aimed for face
reconstruction, leading to superior restoration results. Third, RestoreFormer
outperforms advanced state-of-the-art methods on one synthetic dataset and
three real-world datasets, as well as produces images with better visual
quality."
Training Language Models with Memory Augmentation,0.757998,"Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs."
AdaPrompt: Adaptive Model Training for Prompt-based NLP,0.758025,"Prompt-based learning, with its capability to tackle zero-shot and few-shot
NLP tasks, has gained much attention in community. The main idea is to bridge
the gap between NLP downstream tasks and language modeling (LM), by mapping
these tasks into natural language prompts, which are then filled by pre-trained
language models (PLMs). However, for prompt learning, there are still two
salient gaps between NLP tasks and pretraining. First, prompt information is
not necessarily sufficiently present during LM pretraining. Second,
task-specific data are not necessarily well represented during pretraining. We
address these two issues by proposing AdaPrompt, adaptively retrieving external
data for continual pretraining of PLMs by making use of both task and prompt
characteristics. In addition, we make use of knowledge in Natural Language
Inference models for deriving adaptive verbalizers. Experimental results on
five NLP benchmarks show that AdaPrompt can improve over standard PLMs in
few-shot settings. In addition, in zero-shot settings, our method outperforms
standard prompt-based methods by up to 26.35\% relative error reduction."
Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation,0.739853,"Transformers have revolutionized vision and natural language processing with
their ability to scale with large datasets. But in robotic manipulation, data
is both limited and expensive. Can manipulation still benefit from Transformers
with the right problem formulation? We investigate this question with PerAct, a
language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.
PerAct encodes language goals and RGB-D voxel observations with a Perceiver
Transformer, and outputs discretized actions by ``detecting the next best voxel
action''. Unlike frameworks that operate on 2D images, the voxelized 3D
observation and action space provides a strong structural prior for efficiently
learning 6-DoF actions. With this formulation, we train a single multi-task
Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks
(with 18 variations) from just a few demonstrations per task. Our results show
that PerAct significantly outperforms unstructured image-to-action agents and
3D ConvNet baselines for a wide range of tabletop tasks."
PLM-ICD: Automatic ICD Coding with Pretrained Language Models,0.718648,"Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD"
Variable Bitrate Neural Fields,0.68203,"Neural approximations of scalar and vector fields, such as signed distance
functions and radiance fields, have emerged as accurate, high-quality
representations. State-of-the-art results are obtained by conditioning a neural
approximation with a lookup from trainable feature grids that take on part of
the learning task and allow for smaller, more efficient neural networks.
Unfortunately, these feature grids usually come at the cost of significantly
increased memory consumption compared to stand-alone neural network models. We
present a dictionary method for compressing such feature grids, reducing their
memory consumption by up to 100x and permitting a multiresolution
representation which can be useful for out-of-core streaming. We formulate the
dictionary optimization as a vector-quantized auto-decoder problem which lets
us learn end-to-end discrete neural representations in a space where no direct
supervision is available and with dynamic topology and structure. Our source
code will be available at https://github.com/nv-tlabs/vqad."
Comparative layer-wise analysis of self-supervised speech models,0.678831,"Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive successes on downstream tasks, we still have a
limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models."
CINO: A Chinese Minority Pre-trained Language Model,0.7092,"Multilingual pre-trained language models have shown impressive performance on
cross-lingual tasks. It greatly facilitates the applications of natural
language processing on low-resource languages. However, there are still some
languages that the current multilingual models do not perform well on. In this
paper, we propose CINO (Chinese Minority Pre-trained Language Model), a
multilingual pre-trained language model for Chinese minority languages. It
covers Standard Chinese, Yue Chinese, and six other ethnic minority languages.
To evaluate the cross-lingual ability of the multilingual model on ethnic
minority languages, we collect documents from Wikipedia and news websites, and
construct two text classification datasets, WCM (Wiki-Chinese-Minority) and
CMNews (Chinese-Minority-News). We show that CINO notably outperforms the
baselines on various classification tasks. The CINO model and the datasets are
publicly available at http://cino.hfl-rc.com."
SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,0.69777,"We present SpeechMatrix, a large-scale multilingual corpus of
speech-to-speech translations mined from real speech of European Parliament
recordings. It contains speech alignments in 136 language pairs with a total of
418 thousand hours of speech. To evaluate the quality of this parallel speech,
we train bilingual speech-to-speech translation models on mined data only and
establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test
sets. Enabled by the multilinguality of SpeechMatrix, we also explore
multilingual speech-to-speech translation, a topic which was addressed by few
other works. We also demonstrate that model pre-training and sparse scaling
using Mixture-of-Experts bring large gains to translation performance. The
mined data and models are freely available."
IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images,0.767944,"We propose a neural inverse rendering pipeline called IRON that operates on
photometric images and outputs high-quality 3D content in the format of
triangle meshes and material textures readily deployable in existing graphics
pipelines. Our method adopts neural representations for geometry as signed
distance fields (SDFs) and materials during optimization to enjoy their
flexibility and compactness, and features a hybrid optimization scheme for
neural SDFs: first, optimize using a volumetric radiance field approach to
recover correct topology, then optimize further using edgeaware physics-based
surface rendering for geometry refinement and disentanglement of materials and
lighting. In the second stage, we also draw inspiration from mesh-based
differentiable rendering, and design a novel edge sampling algorithm for neural
SDFs to further improve performance. We show that our IRON achieves
significantly better inverse rendering quality compared to prior works. Our
project page is here: https://kai-46.github.io/IRON-website/"
Generative Modelling With Inverse Heat Dissipation,0.713086,"While diffusion models have shown great success in image generation, their
noise-inverting generative process does not explicitly consider the structure
of images, such as their inherent multi-scale nature. Inspired by diffusion
models and the empirical success of coarse-to-fine modelling, we propose a new
diffusion-like model that generates images through stochastically reversing the
heat equation, a PDE that locally erases fine-scale information when run over
the 2D plane of the image. We interpret the solution of the forward heat
equation with constant additive noise as a variational approximation in the
diffusion latent variable model. Our new model shows emergent qualitative
properties not seen in standard diffusion models, such as disentanglement of
overall colour and shape in images. Spectral analysis on natural images
highlights connections to diffusion models and reveals an implicit
coarse-to-fine inductive bias in them."
Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting,0.694871,"In this paper, we propose a novel end-to-end user-defined keyword spotting
method that utilizes linguistically corresponding patterns between speech and
text sequences. Unlike previous approaches requiring speech keyword enrollment,
our method compares input queries with an enrolled text keyword sequence. To
place the audio and text representations within a common latent space, we adopt
an attention-based cross-modal matching approach that is trained in an
end-to-end manner with monotonic matching loss and keyword classification loss.
We also utilize a de-noising loss for the acoustic embedding network to improve
robustness in noisy environments. Additionally, we introduce the LibriPhrase
dataset, a new short-phrase dataset based on LibriSpeech for efficiently
training keyword spotting models. Our proposed method achieves competitive
results on various evaluation sets compared to other single-modal and
cross-modal baselines."
Meta Spatio-Temporal Debiasing for Video Scene Graph Generation,0.694763,"Video scene graph generation (VidSGG) aims to parse the video content into
scene graphs, which involves modeling the spatio-temporal contextual
information in the video. However, due to the long-tailed training data in
datasets, the generalization performance of existing VidSGG models can be
affected by the spatio-temporal conditional bias problem. In this work, from
the perspective of meta-learning, we propose a novel Meta Video Scene Graph
Generation (MVSGG) framework to address such a bias problem. Specifically, to
handle various types of spatio-temporal conditional biases, our framework first
constructs a support set and a group of query sets from the training data,
where the data distribution of each query set is different from that of the
support set w.r.t. a type of conditional bias. Then, by performing a novel meta
training and testing process to optimize the model to obtain good testing
performance on these query sets after training on the support set, our
framework can effectively guide the model to learn to well generalize against
biases. Extensive experiments demonstrate the efficacy of our proposed
framework."
MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations,0.68078,"Emotion Recognition in Conversations (ERC) has considerable prospects for
developing empathetic machines. For multimodal ERC, it is vital to understand
context and fuse modality information in conversations. Recent graph-based
fusion methods generally aggregate multimodal information by exploring unimodal
and cross-modal interactions in a graph. However, they accumulate redundant
information at each layer, limiting the context understanding between
modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network
(MM-DFN) to recognize emotions by fully understanding multimodal conversational
context. Specifically, we design a new graph-based dynamic fusion module to
fuse multimodal contextual features in a conversation. The module reduces
redundancy and enhances complementarity between modalities by capturing the
dynamics of contextual information in different semantic spaces. Extensive
experiments on two public benchmark datasets demonstrate the effectiveness and
superiority of MM-DFN."
Bangla hate speech detection on social media using attention-based recurrent neural network,0.75001,"Hate speech has spread more rapidly through the daily use of technology and,
most notably, by sharing your opinions or feelings on social media in a
negative aspect. Although numerous works have been carried out in detecting
hate speeches in English, German, and other languages, very few works have been
carried out in the context of the Bengali language. In contrast, millions of
people communicate on social media in Bengali. The few existing works that have
been carried out need improvements in both accuracy and interpretability. This
article proposed encoder decoder based machine learning model, a popular tool
in NLP, to classify user's Bengali comments on Facebook pages. A dataset of
7,425 Bengali comments, consisting of seven distinct categories of hate
speeches, was used to train and evaluate our model. For extracting and encoding
local features from the comments, 1D convolutional layers were used. Finally,
the attention mechanism, LSTM, and GRU based decoders have been used for
predicting hate speech categories. Among the three encoder decoder algorithms,
the attention-based decoder obtained the best accuracy (77%)."
Delving into Out-of-Distribution Detection with Vision-Language Representations,0.693359,"Recognizing out-of-distribution (OOD) samples is critical for machine
learning systems deployed in the open world. The vast majority of OOD detection
methods are driven by a single modality (e.g., either vision or language),
leaving the rich information in multi-modal representations untapped. Inspired
by the recent success of vision-language pre-training, this paper enriches the
landscape of OOD detection from a single-modal to a multi-modal regime.
Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective
zero-shot OOD detection method based on aligning visual features with textual
concepts. We contribute in-depth analysis and theoretical insights to
understand the effectiveness of MCM. Extensive experiments demonstrate that MCM
achieves superior performance on a wide variety of real-world tasks. MCM with
vision-language features outperforms a common baseline with pure visual
features on a hard OOD task with semantically similar classes by 13.1% (AUROC).
Code is available at https://github.com/deeplearning-wisc/MCM."
Error Compensation Framework for Flow-Guided Video Inpainting,0.742475,"The key to video inpainting is to use correlation information from as many
reference frames as possible. Existing flow-based propagation methods split the
video synthesis process into multiple steps: flow completion -> pixel
propagation -> synthesis. However, there is a significant drawback that the
errors in each step continue to accumulate and amplify in the next step. To
this end, we propose an Error Compensation Framework for Flow-guided Video
Inpainting (ECFVI), which takes advantage of the flow-based method and offsets
its weaknesses. We address the weakness with the newly designed flow completion
module and the error compensation network that exploits the error guidance map.
Our approach greatly improves the temporal consistency and the visual quality
of the completed videos. Experimental results show the superior performance of
our proposed method with the speed up of x6, compared to the state-of-the-art
methods. In addition, we present a new benchmark dataset for evaluation by
supplementing the weaknesses of existing test datasets."
Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small,0.76108,"Research in mechanistic interpretability seeks to explain behaviors of
machine learning models in terms of their internal components. However, most
previous work either focuses on simple behaviors in small models, or describes
complicated behaviors in larger models with broad strokes. In this work, we
bridge this gap by presenting an explanation for how GPT-2 small performs a
natural language task called indirect object identification (IOI). Our
explanation encompasses 26 attention heads grouped into 7 main classes, which
we discovered using a combination of interpretability approaches relying on
causal interventions. To our knowledge, this investigation is the largest
end-to-end attempt at reverse-engineering a natural behavior ""in the wild"" in a
language model. We evaluate the reliability of our explanation using three
quantitative criteria--faithfulness, completeness and minimality. Though these
criteria support our explanation, they also point to remaining gaps in our
understanding. Our work provides evidence that a mechanistic understanding of
large ML models is feasible, opening opportunities to scale our understanding
to both larger models and more complex tasks."
Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss,0.734401,"In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings."
AutoAttention: Automatic Field Pair Selection for Attention in User Behavior Modeling,0.698806,"In Click-through rate (CTR) prediction models, a user's interest is usually
represented as a fixed-length vector based on her history behaviors. Recently,
several methods are proposed to learn an attentive weight for each user
behavior and conduct weighted sum pooling. However, these methods only manually
select several fields from the target item side as the query to interact with
the behaviors, neglecting the other target item fields, as well as user and
context fields. Directly including all these fields in the attention may
introduce noise and deteriorate the performance. In this paper, we propose a
novel model named AutoAttention, which includes all item/user/context side
fields as the query, and assigns a learnable weight for each field pair between
behavior fields and query fields. Pruning on these field pairs via these
learnable weights lead to automatic field pair selection, so as to identify and
remove noisy field pairs. Though including more fields, the computation cost of
AutoAttention is still low due to using a simple attention function and field
pair selection. Extensive experiments on the public dataset and Tencent's
production dataset demonstrate the effectiveness of the proposed approach."
Human-to-Robot Imitation in the Wild,0.712292,"We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io"
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning,0.686992,"Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt."
Crosslingual Generalization through Multitask Finetuning,0.772832,"Multitask prompted finetuning (MTF) has been shown to help large language
models generalize to new tasks in a zero-shot setting, but so far explorations
of MTF have focused on English data and models. We apply MTF to the pretrained
multilingual BLOOM and mT5 model families to produce finetuned variants called
BLOOMZ and mT0. We find finetuning large multilingual language models on
English tasks with English prompts allows for task generalization to
non-English languages that appear only in the pretraining corpus. Finetuning on
multilingual tasks with English prompts further improves performance on English
and non-English tasks leading to various state-of-the-art zero-shot results. We
also investigate finetuning on multilingual tasks with prompts that have been
machine-translated from English to match the language of each dataset. We find
training on these machine-translated prompts leads to better performance on
human-written prompts in the respective languages. Surprisingly, we find models
are capable of zero-shot generalization to tasks in languages they have never
intentionally seen. We conjecture that the models are learning higher-level
capabilities that are both task- and language-agnostic. In addition, we
introduce xP3, a composite of supervised datasets in 46 languages with English
and machine-translated prompts. Our code, datasets and models are freely
available at https://github.com/bigscience-workshop/xmtf."
Semantic-aligned Fusion Transformer for One-shot Object Detection,0.692228,"One-shot object detection aims at detecting novel objects according to merely
one given instance. With extreme data scarcity, current approaches explore
various feature fusions to obtain directly transferable meta-knowledge. Yet,
their performances are often unsatisfactory. In this paper, we attribute this
to inappropriate correlation methods that misalign query-support semantics by
overlooking spatial structures and scale variances. Upon analysis, we leverage
the attention mechanism and propose a simple but effective architecture named
Semantic-aligned Fusion Transformer (SaFT) to resolve these issues.
Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale
semantic enhancement and a horizontal fusion module (HFM) for cross-sample
feature fusion. Together, they broaden the vision for each feature point from
the support to a whole augmented feature pyramid from the query, facilitating
semantic-aligned associations. Extensive experiments on multiple benchmarks
demonstrate the superiority of our framework. Without fine-tuning on novel
classes, it brings significant performance gains to one-stage baselines,
lifting state-of-the-art results to a higher level."
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing,0.701912,"We present PHORHUM, a novel, end-to-end trainable, deep neural network
methodology for photorealistic 3D human reconstruction given just a monocular
RGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the
first time, the unshaded surface color together with the scene illumination.
Observing that 3D supervision alone is not sufficient for high fidelity color
reconstruction, we introduce patch-based rendering losses that enable reliable
color reconstruction on visible parts of the human, and detailed and plausible
color estimation for the non-visible parts. Moreover, our method specifically
addresses methodological and practical limitations of prior work in terms of
representing geometry, albedo, and illumination effects, in an end-to-end model
where factors can be effectively disentangled. In extensive experiments, we
demonstrate the versatility and robustness of our approach. Our
state-of-the-art results validate the method qualitatively and for different
metrics, for both geometric and color reconstruction."
Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation,0.687172,"In this paper we present Mask DINO, a unified object detection and
segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising
Anchor Boxes) by adding a mask prediction branch which supports all image
segmentation tasks (instance, panoptic, and semantic). It makes use of the
query embeddings from DINO to dot-product a high-resolution pixel embedding map
to predict a set of binary masks. Some key components in DINO are extended for
segmentation through a shared architecture and training process. Mask DINO is
simple, efficient, and scalable, and it can benefit from joint large-scale
detection and segmentation datasets. Our experiments show that Mask DINO
significantly outperforms all existing specialized segmentation methods, both
on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably,
Mask DINO establishes the best results to date on instance segmentation (54.5
AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation
(60.8 mIoU on ADE20K) among models under one billion parameters. Code is
available at \url{https://github.com/IDEACVR/MaskDINO}."
Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer,0.750273,"Learning Bird's Eye View (BEV) representation from surrounding-view cameras
is of great importance for autonomous driving. In this work, we propose a
Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation
learning mechanism. GKT leverages the geometric priors to guide the transformer
to focus on discriminative regions and unfolds kernel features to generate BEV
representation. For fast inference, we further introduce a look-up table (LUT)
indexing method to get rid of the camera's calibrated parameters at runtime.
GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust
to the camera deviation and the predefined BEV height. And GKT achieves the
state-of-the-art real-time segmentation results, i.e., 38.0 mIoU
(100m$\times$100m perception range at a 0.5m resolution) on the nuScenes val
set. Given the efficiency, effectiveness, and robustness, GKT has great
practical values in autopilot scenarios, especially for real-time running
systems. Code and models will be available at
\url{https://github.com/hustvl/GKT}."
Fine-Grained Scene Graph Generation with Data Transfer,0.752083,"Scene graph generation (SGG) is designed to extract (subject, predicate,
object) triplets in images. Recent works have made a steady progress on SGG,
and provide useful tools for high-level vision and language understanding.
However, due to the data distribution problems including long-tail distribution
and semantic ambiguity, the predictions of current SGG models tend to collapse
to several frequent but uninformative predicates (e.g., on, at), which limits
practical application of these models in downstream tasks. To deal with the
problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a plug-and-play fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the enhanced dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The code and data
are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch."
SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations,0.721929,"Accurate mapping of large-scale environments is an essential building block
of most outdoor autonomous systems. Challenges of traditional mapping methods
include the balance between memory consumption and mapping accuracy. This paper
addresses the problem of achieving large-scale 3D reconstruction using implicit
representations built from 3D LiDAR measurements. We learn and store implicit
features through an octree-based, hierarchical structure, which is sparse and
extensible. The implicit features can be turned into signed distance values
through a shallow neural network. We leverage binary cross entropy loss to
optimize the local features with the 3D measurements as supervision. Based on
our implicit representation, we design an incremental mapping system with
regularization to tackle the issue of forgetting in continual learning. Our
experiments show that our 3D reconstructions are more accurate, complete, and
memory-efficient than current state-of-the-art 3D mapping methods."
AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,0.766493,"Knowledge distillation (KD) methods compress large models into smaller
students with manually-designed student architectures given pre-specified
computational cost. This requires several trials to find a viable student, and
further repeating the process for each student or computational budget change.
We use Neural Architecture Search (NAS) to automatically distill several
compressed students with variable cost from a large model. Current works train
a single SuperLM consisting of millions of subnetworks with weight-sharing,
resulting in interference between subnetworks of different sizes. Our framework
AutoDistil addresses above challenges with the following steps: (a)
Incorporates inductive bias and heuristics to partition Transformer search
space into K compact sub-spaces (K=3 for typical student sizes of base, small
and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic
objective (e.g., self-attention distillation) with weight-sharing of students;
(c) Lightweight search for the optimal student without re-training. Fully
task-agnostic training and search allow students to be reused for fine-tuning
on any downstream task. Experiments on GLUE benchmark against state-of-the-art
KD and NAS methods demonstrate AutoDistil to outperform leading compression
techniques with upto 2.7x reduction in computational cost and negligible loss
in task performance."
A Generalist Framework for Panoptic Segmentation of Images and Videos,0.758526,"Panoptic segmentation assigns semantic and instance ID labels to every pixel
of an image. As permutations of instance IDs are also valid solutions, the task
requires learning of high-dimensional one-to-many mapping. As a result,
state-of-the-art approaches use customized architectures and task-specific loss
functions. We formulate panoptic segmentation as a discrete data generation
problem, without relying on inductive bias of the task. A diffusion model is
proposed to model panoptic masks, with a simple architecture and generic loss
function. By simply adding past predictions as a conditioning signal, our
method is capable of modeling video (in a streaming setting) and thereby learns
to track object instances automatically. With extensive experiments, we
demonstrate that our simple approach can perform competitively to
state-of-the-art specialist methods in similar settings."
MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing,0.728664,"Snow removal causes challenges due to its characteristic of complex
degradations. To this end, targeted treatment of multi-scale snow degradations
is critical for the network to learn effective snow removal. In order to handle
the diverse scenes, we propose a multi-scale projection transformer
(MSP-Former), which understands and covers a variety of snow degradation
features in a multi-path manner, and integrates comprehensive scene context
information for clean reconstruction via self-attention operation. For the
local details of various snow degradations, the local capture module is
introduced in parallel to assist in the rebuilding of a clean image. Such
design achieves the SOTA performance on three desnowing benchmark datasets
while costing the low parameters and computational complexity, providing a
guarantee of practicality."
iCaps: Iterative Category-level Object Pose and Shape Estimation,0.697521,"This paper proposes a category-level 6D object pose and shape estimation
approach iCaps, which allows tracking 6D poses of unseen objects in a category
and estimating their 3D shapes. We develop a category-level auto-encoder
network using depth images as input, where feature embeddings from the
auto-encoder encode poses of objects in a category. The auto-encoder can be
used in a particle filter framework to estimate and track 6D poses of objects
in a category. By exploiting an implicit shape representation based on signed
distance functions, we build a LatentNet to estimate a latent representation of
the 3D shape given the estimated pose of an object. Then the estimated pose and
shape can be used to update each other in an iterative way. Our category-level
6D object pose and shape estimation pipeline only requires 2D detection and
segmentation for initialization. We evaluate our approach on a publicly
available dataset and demonstrate its effectiveness. In particular, our method
achieves comparably high accuracy on shape estimation."
LISA: Learning Implicit Shape and Appearance of Hands,0.734463,"This paper proposes a do-it-all neural model of human hands, named LISA. The
model can capture accurate hand shape and appearance, generalize to arbitrary
hand subjects, provide dense surface correspondences, be reconstructed from
images in the wild and easily animated. We train LISA by minimizing the shape
and appearance losses on a large set of multi-view RGB image sequences
annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand
local coordinate, our model predicts the color and the signed distance with
respect to each hand bone independently, and then combines the per-bone
predictions using predicted skinning weights. The shape, color and pose
representations are disentangled by design, allowing to estimate or animate
only selected parameters. We experimentally demonstrate that LISA can
accurately reconstruct a dynamic hand from monocular or multi-view sequences,
achieving a noticeably higher quality of reconstructed hand shapes compared to
baseline approaches. Project page:
https://www.iri.upc.edu/people/ecorona/lisa/."
DeiT III: Revenge of the ViT,0.687369,"A Vision Transformer (ViT) is a simple neural architecture amenable to serve
several computer vision tasks. It has limited built-in architectural priors, in
contrast to more recent architectures that incorporate priors either about the
input data or of specific tasks. Recent works show that ViTs benefit from
self-supervised pre-training, in particular BerT-like pre-training like BeiT.
In this paper, we revisit the supervised training of ViTs. Our procedure builds
upon and simplifies a recipe introduced for training ResNet-50. It includes a
new simple data-augmentation procedure with only 3 augmentations, closer to the
practice in self-supervised learning. Our evaluations on Image classification
(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning
and semantic segmentation show that our procedure outperforms by a large margin
previous fully supervised training recipes for ViT. It also reveals that the
performance of our ViT trained with supervision is comparable to that of more
recent architectures. Our results could serve as better baselines for recent
self-supervised approaches demonstrated on ViT."
Large Language Models Can Self-Improve,0.751607,"Large Language Models (LLMs) have achieved excellent performances in various
tasks. However, fine-tuning an LLM requires extensive supervision. Human, on
the other hand, may improve their reasoning abilities by self-thinking without
external inputs. In this work, we demonstrate that an LLM is also capable of
self-improving with only unlabeled datasets. We use a pre-trained LLM to
generate ""high-confidence"" rationale-augmented answers for unlabeled questions
using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM
using those self-generated solutions as target outputs. We show that our
approach improves the general reasoning ability of a 540B-parameter LLM
(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and
63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,
without any ground truth label. We conduct ablation studies and show that
fine-tuning on reasoning is critical for self-improvement."
"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",0.741303,"The past decade has witnessed dramatic gains in natural language processing
and an unprecedented scaling of large language models. These developments have
been accelerated by the advent of few-shot techniques such as chain of thought
(CoT) prompting. Specifically, CoT pushes the performance of large language
models in a few-shot setup by augmenting the prompts with intermediate steps.
Despite impressive results across various tasks, the reasons behind their
success have not been explored. This work uses counterfactual prompting to
develop a deeper understanding of CoT-based few-shot prompting mechanisms in
large language models. We first systematically identify and define the key
components of a prompt: symbols, patterns, and text. Then, we devise and
conduct an exhaustive set of experiments across four different tasks, by
querying the model with counterfactual prompts where only one of these
components is altered. Our experiments across three models (PaLM, GPT-3, and
CODEX) reveal several surprising findings and brings into question the
conventional wisdom around few-shot prompting. First, the presence of factual
patterns in a prompt is practically immaterial to the success of CoT. Second,
our results conclude that the primary role of intermediate steps may not be to
facilitate learning how to solve a task. The intermediate steps are rather a
beacon for the model to realize what symbols to replicate in the output to form
a factual answer. Further, text imbues patterns with commonsense knowledge and
meaning. Our empirical and qualitative analysis reveals that a symbiotic
relationship between text and patterns explains the success of few-shot
prompting: text helps extract commonsense from the question to help patterns,
and patterns enforce task understanding and direct text generation."
LEBP -- Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents,0.666676,"People always desire an embodied agent that can perform a task by
understanding language instruction. Moreover, they also want to monitor and
expect agents to understand commands the way they expected. But, how to build
such an embodied agent is still unclear. Recently, people can explore this
problem with the Vision-and-Language Interaction benchmark ALFRED, which
requires an agent to perform complicated daily household tasks following
natural language instructions in unseen scenes. In this paper, we propose LEBP
-- Language Expectation and Binding Policy Module to tackle the ALFRED. The
LEBP contains a two-stream process: 1) It first conducts a language expectation
module to generate an expectation describing how to perform tasks by
understanding the language instruction. The expectation consists of a sequence
of sub-steps for the task (e.g., Pick an apple). The expectation allows people
to access and check the understanding results of instructions before the agent
takes actual actions, in case the task might go wrong. 2) Then, it uses the
binding policy module to bind sub-steps in expectation to actual actions to
specific scenarios. Actual actions include navigation and object manipulation.
Experimental results suggest our approach achieves comparable performance to
currently published SOTA methods and can avoid large decay from seen scenarios
to unseen scenarios."
Is Conditional Generative Modeling all you need for Decision-Making?,0.76689,"Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making."
Latent Image Animator: Learning to Animate Images via Latent Space Navigation,0.70954,"Due to the remarkable progress of deep generative models, animating images
has become increasingly efficient, whereas associated results have become
increasingly realistic. Current animation-approaches commonly exploit structure
representation extracted from driving videos. Such structure representation is
instrumental in transferring motion from driving videos to still images.
However, such approaches fail in case the source image and driving video
encompass large appearance variation. Moreover, the extraction of structure
information requires additional modules that endow the animation-model with
increased complexity. Deviating from such models, we here introduce the Latent
Image Animator (LIA), a self-supervised autoencoder that evades need for
structure representation. LIA is streamlined to animate images by linear
navigation in the latent space. Specifically, motion in generated video is
constructed by linear displacement of codes in the latent space. Towards this,
we learn a set of orthogonal motion directions simultaneously, and use their
linear combination, in order to represent any displacement in the latent space.
Extensive quantitative and qualitative analysis suggests that our model
systematically and significantly outperforms state-of-art methods on VoxCeleb,
Taichi and TED-talk datasets w.r.t. generated quality."
Diffusion Posterior Sampling for General Noisy Inverse Problems,0.695162,"Diffusion models have been recently studied as powerful generative inverse
problem solvers, owing to their high quality reconstructions and the ease of
combining existing iterative solvers. However, most works focus on solving
simple linear inverse problems in noiseless settings, which significantly
under-represents the complexity of real-world problems. In this work, we extend
diffusion solvers to efficiently handle general noisy (non)linear inverse
problems via approximation of the posterior sampling. Interestingly, the
resulting posterior sampling scheme is a blended version of diffusion sampling
with the manifold constrained gradient without a strict measurement consistency
projection step, yielding a more desirable generative path in noisy settings
compared to the previous studies. Our method demonstrates that diffusion models
can incorporate various measurement noise statistics such as Gaussian and
Poisson, and also efficiently handle noisy nonlinear inverse problems such as
Fourier phase retrieval and non-uniform deblurring. Code available at
https://github.com/DPS2022/diffusion-posterior-sampling"
Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET,0.718922,"Neural metrics have achieved impressive correlation with human judgements in
the evaluation of machine translation systems, but before we can safely
optimise towards such metrics, we should be aware of (and ideally eliminate)
biases toward bad translations that receive high scores. Our experiments show
that sample-based Minimum Bayes Risk decoding can be used to explore and
quantify such weaknesses. When applying this strategy to COMET for en-de and
de-en, we find that COMET models are not sensitive enough to discrepancies in
numbers and named entities. We further show that these biases are hard to fully
remove by simply training on additional synthetic data and release our code and
data for facilitating further experiments."
GreaseLM: Graph REASoning Enhanced Language Models for Question Answering,0.684991,"Answering complex questions about textual narratives requires reasoning over
both stated context and the world knowledge that underlies it. However,
pretrained language models (LM), the foundation of most modern QA systems, do
not robustly represent latent relationships between concepts, which is
necessary for reasoning. While knowledge graphs (KG) are often used to augment
LMs with structured representations of world knowledge, it remains an open
question how to effectively fuse and reason over the KG representations and the
language context, which provides situational constraints and nuances. In this
work, we propose GreaseLM, a new model that fuses encoded representations from
pretrained LMs and graph neural networks over multiple layers of modality
interaction operations. Information from both modalities propagates to the
other, allowing language context representations to be grounded by structured
world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in
the context to inform the graph representations of knowledge. Our results on
three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA)
and medical question answering (i.e., MedQA-USMLE) domains demonstrate that
GreaseLM can more reliably answer questions that require reasoning over both
situational constraints and structured knowledge, even outperforming models 8x
larger."
ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information,0.698806,"ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10
billion web pages affiliated with rich information. Its design was influenced
by the need for a high quality, large scale web corpus to support a range of
academic and industry research, for example, in information systems,
retrieval-augmented AI systems, and model pretraining. Compared with earlier
ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of
higher-quality, and aligned with the document distributions in commercial web
search. Besides raw HTML, ClueWeb22 includes rich information about the web
pages provided by industry-standard document understanding systems, including
the visual representation of pages rendered by a web browser, parsed HTML
structure information from a neural network parser, and pre-processed cleaned
document text to lower the barrier to entry. Many of these signals have been
widely used in industry but are available to the research community for the
first time at this scale."
Uncertainty Inspired Underwater Image Enhancement,0.728507,"A main challenge faced in the deep learning-based Underwater Image
Enhancement (UIE) is that the ground truth high-quality image is unavailable.
Most of the existing methods first generate approximate reference maps and then
train an enhancement network with certainty. This kind of method fails to
handle the ambiguity of the reference map. In this paper, we resolve UIE into
distribution estimation and consensus process. We present a novel probabilistic
network to learn the enhancement distribution of degraded underwater images.
Specifically, we combine conditional variational autoencoder with adaptive
instance normalization to construct the enhancement distribution. After that,
we adopt a consensus process to predict a deterministic result based on a set
of samples from the distribution. By learning the enhancement distribution, our
method can cope with the bias introduced in the reference map labeling to some
extent. Additionally, the consensus process is useful to capture a robust and
stable result. We examined the proposed method on two widely used real-world
underwater image enhancement datasets. Experimental results demonstrate that
our approach enables sampling possible enhancement predictions. Meanwhile, the
consensus estimate yields competitive performance compared with
state-of-the-art UIE methods. Code available at
https://github.com/zhenqifu/PUIE-Net."
Online Decision Transformer,0.770206,"Recent work has shown that offline reinforcement learning (RL) can be
formulated as a sequence modeling problem (Chen et al., 2021; Janner et al.,
2021) and solved via approaches similar to large-scale language modeling.
However, any practical instantiation of RL also involves an online component,
where policies pretrained on passive offline datasets are finetuned via
taskspecific interactions with the environment. We propose Online Decision
Transformers (ODT), an RL algorithm based on sequence modeling that blends
offline pretraining with online finetuning in a unified framework. Our
framework uses sequence-level entropy regularizers in conjunction with
autoregressive modeling objectives for sample-efficient exploration and
finetuning. Empirically, we show that ODT is competitive with the
state-of-the-art in absolute performance on the D4RL benchmark but shows much
more significant gains during the finetuning procedure."
Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer,0.66673,"Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks."
A decomposition of book structure through ousiometric fluctuations in cumulative word-time,0.698806,"While quantitative methods have been used to examine changes in word usage in
books, studies have focused on overall trends, such as the shapes of
narratives, which are independent of book length. We instead look at how words
change over the course of a book as a function of the number of words, rather
than the fraction of the book, completed at any given point; we define this
measure as ""cumulative word-time"". Using ousiometrics, a reinterpretation of
the valence-arousal-dominance framework of meaning obtained from semantic
differentials, we convert text into time series of power and danger scores in
cumulative word-time. Each time series is then decomposed using empirical mode
decomposition into a sum of constituent oscillatory modes and a non-oscillatory
trend. By comparing the decomposition of the original power and danger time
series with those derived from shuffled text, we find that shorter books
exhibit only a general trend, while longer books have fluctuations in addition
to the general trend. These fluctuations typically have a period of a few
thousand words regardless of the book length or library classification code,
but vary depending on the content and structure of the book. Our findings
suggest that, in the ousiometric sense, longer books are not expanded versions
of shorter books, but are more similar in structure to a concatenation of
shorter texts. Further, they are consistent with editorial practices that
require longer texts to be broken down into sections, such as chapters. Our
method also provides a data-driven denoising approach that works for texts of
various lengths, in contrast to the more traditional approach of using large
window sizes that may inadvertently smooth out relevant information, especially
for shorter texts. These results open up avenues for future work in
computational literary analysis, particularly the measurement of a basic unit
of narrative."
"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",0.684501,"Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for ""general continual learning"". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings."
Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,0.704122,"End-to-end speech-to-speech translation (S2ST) without relying on
intermediate text representations is a rapidly emerging frontier of research.
Recent works have demonstrated that the performance of such direct S2ST systems
is approaching that of conventional cascade S2ST when trained on comparable
datasets. However, in practice, the performance of direct S2ST is bounded by
the availability of paired S2ST training data. In this work, we explore
multiple approaches for leveraging much more widely available unsupervised and
weakly-supervised speech and text data to improve the performance of direct
S2ST based on Translatotron 2. With our most effective approaches, the average
translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is
improved by +13.6 BLEU (or +113% relatively), as compared to the previous
state-of-the-art trained without additional data. The improvements on
low-resource language are even more significant (+398% relatively on average).
Our comparative studies suggest future research directions for S2ST and speech
representation learning."
When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,0.709601,"AI systems are becoming increasingly intertwined with human life. In order to
effectively collaborate with humans and ensure safety, AI systems need to be
able to understand, interpret and predict human moral judgments and decisions.
Human moral judgments are often guided by rules, but not always. A central
challenge for AI safety is capturing the flexibility of the human moral mind --
the ability to determine when a rule should be broken, especially in novel or
unusual situations. In this paper, we present a novel challenge set consisting
of rule-breaking question answering (RBQA) of cases that involve potentially
permissible rule-breaking -- inspired by recent moral psychology studies. Using
a state-of-the-art large language model (LLM) as a basis, we propose a novel
moral chain of thought (MORALCOT) prompting strategy that combines the
strengths of LLMs with theories of moral reasoning developed in cognitive
science to predict human moral judgments. MORALCOT outperforms seven existing
LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to
capture the flexibility of the human moral mind. We also conduct a detailed
error analysis to suggest directions for future work to improve AI safety using
RBQA. Our data is open-sourced at
https://huggingface.co/datasets/feradauto/MoralExceptQA and code at
https://github.com/feradauto/MoralCoT"
Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data,0.755447,"Retrieval-based methods have been shown to be effective in NLP tasks via
introducing external knowledge. However, the indexing and retrieving of
large-scale corpora bring considerable computational cost. Surprisingly, we
found that REtrieving from the traINing datA (REINA) only can lead to
significant gains on multiple NLG and NLU tasks. We retrieve the labeled
training instances most similar to the input text and then concatenate them
with the input to feed into the model to generate the output. Experimental
results show that this simple method can achieve significantly better
performance on a variety of NLU and NLG tasks, including summarization, machine
translation, language modeling, and question answering tasks. For instance, our
proposed method achieved state-of-the-art results on XSum, BigPatent, and
CommonsenseQA. Our code is released, https://github.com/microsoft/REINA ."
ProQA: Structural Prompt-based Pre-training for Unified Question Answering,0.734463,"Question Answering (QA) is a longstanding challenge in natural language
processing. Existing QA works mostly focus on specific question types,
knowledge domains, or reasoning skills. The specialty in QA research hinders
systems from modeling commonalities between tasks and generalization for wider
applications. To address this issue, we present ProQA, a unified QA paradigm
that solves various tasks through a single model. ProQA takes a unified
structural prompt as the bridge and improves the QA-centric ability by
structural prompt-based pre-training. Through a structurally designed
prompt-based input schema, ProQA concurrently models the knowledge
generalization for all QA tasks while keeping the knowledge customization for
every specific QA task. Furthermore, ProQA is pre-trained with structural
prompt-formatted large-scale synthesized corpus, which empowers the model with
the commonly-required QA ability. Experimental results on 11 QA benchmarks
demonstrate that ProQA consistently boosts performance on both full data
fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,
ProQA exhibits strong ability in both continual learning and transfer learning
by taking the advantages of the structural prompt."
Large Language Models are few(1)-shot Table Reasoners,0.678523,"Recent literature has shown that large language models (LLMs) are generally
excellent few-shot reasoners to solve text reasoning tasks. However, the
capability of LLMs on table reasoning tasks is yet to be explored. In this
paper, we aim at understanding how well LLMs can perform table-related tasks
with few-shot in-context learning. Specifically, we evaluated LLMs on popular
table QA and fact verification datasets like WikiTableQuestion, FetaQA,
TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning
over table structures, though these models are not pre-trained on any table
corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very
strong performance with only a 1-shot demonstration, even on par with some SoTA
models. We show that LLMs are even more competent at generating comprehensive
long-form answers on FetaQA than tuned T5-large. We further manually studied
the reasoning chains elicited from LLMs and found that these reasoning chains
are highly consistent with the underlying semantic form. We believe that LLMs
can serve as a simple yet generic baseline for future research. The code and
data are released in https://github.com/wenhuchen/TableCoT."
Toward Efficient Task Planning for Dual-Arm Tabletop Object Rearrangement,0.716951,"We investigate the problem of coordinating two robot arms to solve
non-monotone tabletop multi-object rearrangement tasks. In a non-monotone
rearrangement task, complex object-object dependencies exist that require
moving some objects multiple times to solve an instance. In working with two
arms in a large workspace, some objects must be handed off between the robots,
which further complicates the planning process. For the challenging dual-arm
tabletop rearrangement problem, we develop effective task planning algorithms
for scheduling the pick-n-place sequence that can be properly distributed
between the two arms. We show that, even without using a sophisticated motion
planner, our method achieves significant time savings in comparison to greedy
approaches and naive parallelization of single-robot plans."
ST-MoE: Designing Stable and Transferable Sparse Expert Models,0.699852,"Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3)."
MTEB: Massive Text Embedding Benchmark,0.761872,"Text embeddings are commonly evaluated on a small set of datasets from a
single task not covering their possible applications to other tasks. It is
unclear whether state-of-the-art embeddings on semantic textual similarity
(STS) can be equally well applied to other tasks like clustering or reranking.
This makes progress in the field difficult to track, as various models are
constantly being proposed without proper evaluation. To solve this problem, we
introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding
tasks covering a total of 58 datasets and 112 languages. Through the
benchmarking of 33 models on MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We find that no particular text embedding
method dominates across all tasks. This suggests that the field has yet to
converge on a universal text embedding method and scale it up sufficiently to
provide state-of-the-art results on all embedding tasks. MTEB comes with
open-source code and a public leaderboard at
https://github.com/embeddings-benchmark/mteb."
Masked Visual Pre-training for Motor Control,0.67767,"This paper shows that self-supervised visual pre-training from real-world
images is effective for learning motor control tasks from pixels. We first
train the visual representations by masked modeling of natural images. We then
freeze the visual encoder and train neural network controllers on top with
reinforcement learning. We do not perform any task-specific fine-tuning of the
encoder; the same visual representations are used for all motor control tasks.
To the best of our knowledge, this is the first self-supervised model to
exploit real-world images at scale for motor control. To accelerate progress in
learning from pixels, we contribute a benchmark suite of hand-designed tasks
varying in movements, scenes, and robots. Without relying on labels,
state-estimation, or expert demonstrations, we consistently outperform
supervised encoders by up to 80% absolute success rate, sometimes even matching
the oracle state performance. We also find that in-the-wild images, e.g., from
YouTube or Egocentric videos, lead to better visual representations for various
manipulation tasks than ImageNet images."
Part-guided Relational Transformers for Fine-grained Visual Recognition,0.732865,"Fine-grained visual recognition is to classify objects with visually similar
appearances into subcategories, which has made great progress with the
development of deep CNNs. However, handling subtle differences between
different subcategories still remains a challenge. In this paper, we propose to
solve this issue in one unified framework from two aspects, i.e., constructing
feature-level interrelationships, and capturing part-level discriminative
features. This framework, namely PArt-guided Relational Transformers (PART), is
proposed to learn the discriminative part features with an automatic part
discovery module, and to explore the intrinsic correlations with a feature
transformation module by adapting the Transformer models from the field of
natural language processing. The part discovery module efficiently discovers
the discriminative regions which are highly-corresponded to the gradient
descent procedure. Then the second feature transformation module builds
correlations within the global embedding and multiple part embedding, enhancing
spatial interactions among semantic pixels. Moreover, our proposed approach
does not rely on additional part branches in the inference time and reaches
state-of-the-art performance on 3 widely-used fine-grained object recognition
benchmarks. Experimental results and explainable visualizations demonstrate the
effectiveness of our proposed approach. The code can be found at
https://github.com/iCVTEAM/PART."
Structured Pruning Learns Compact and Accurate Models,0.768441,"The growing size of neural language models has led to increased attention in
model compression. The two predominant approaches are pruning, which gradually
removes weights from a pre-trained model, and distillation, which trains a
smaller compact model to match a larger one. Pruning methods can significantly
reduce the model size but hardly achieve large speedups as distillation.
However, distillation methods require large amounts of unlabeled data and are
expensive to train. In this work, we propose a task-specific structured pruning
method CoFi (Coarse- and Fine-grained Pruning), which delivers highly
parallelizable subnetworks and matches the distillation methods in both
accuracy and latency, without resorting to any unlabeled data. Our key insight
is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads
and hidden units) modules, which controls the pruning decision of each
parameter with masks of different granularity. We also devise a layerwise
distillation strategy to transfer knowledge from unpruned to pruned models
during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi
yields models with over 10x speedups with a small accuracy drop, showing its
effectiveness and efficiency compared to previous pruning and distillation
approaches."
Modeling Image Composition for Complex Scene Generation,0.684008,"We present a method that achieves state-of-the-art results on challenging
(few-shot) layout-to-image generation tasks by accurately modeling textures,
structures and relationships contained in a complex scene. After compressing
RGB images into patch tokens, we propose the Transformer with Focal Attention
(TwFA) for exploring dependencies of object-to-object, object-to-patch and
patch-to-patch. Compared to existing CNN-based and Transformer-based generation
models that entangled modeling on pixel-level&patch-level and
object-level&patch-level respectively, the proposed focal attention predicts
the current patch token by only focusing on its highly-related tokens that
specified by the spatial layout, thereby achieving disambiguation during
training. Furthermore, the proposed TwFA largely increases the data efficiency
during training, therefore we propose the first few-shot complex scene
generation strategy based on the well-trained TwFA. Comprehensive experiments
show the superiority of our method, which significantly increases both
quantitative metrics and qualitative visual realism with respect to
state-of-the-art CNN-based and transformer-based methods. Code is available at
https://github.com/JohnDreamer/TwFA."
InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds,0.758088,"In this paper, we take a significant step towards real-world applicability of
monocular neural avatar reconstruction by contributing InstantAvatar, a system
that can reconstruct human avatars from a monocular video within seconds, and
these avatars can be animated and rendered at an interactive rate. To achieve
this efficiency we propose a carefully designed and engineered system, that
leverages emerging acceleration structures for neural fields, in combination
with an efficient empty space-skipping strategy for dynamic scenes. We also
contribute an efficient implementation that we will make available for research
purposes. Compared to existing methods, InstantAvatar converges 130x faster and
can be trained in minutes instead of hours. It achieves comparable or even
better reconstruction quality and novel pose synthesis results. When given the
same time budget, our method significantly outperforms SoTA methods.
InstantAvatar can yield acceptable visual quality in as little as 10 seconds
training time."
STaR: Bootstrapping Reasoning With Reasoning,0.714529,"Generating step-by-step ""chain-of-thought"" rationales improves language model
performance on complex reasoning tasks like mathematics or commonsense
question-answering. However, inducing language model rationale generation
currently requires either constructing massive rationale datasets or
sacrificing accuracy by using only few-shot inference. We propose a technique
to iteratively leverage a small number of rationale examples and a large
dataset without rationales, to bootstrap the ability to perform successively
more complex reasoning. This technique, the ""Self-Taught Reasoner"" (STaR),
relies on a simple loop: generate rationales to answer many questions, prompted
with a few rationale examples; if the generated answers are wrong, try again to
generate a rationale given the correct answer; fine-tune on all the rationales
that ultimately yielded correct answers; repeat. We show that STaR
significantly improves performance on multiple datasets compared to a model
fine-tuned to directly predict final answers, and performs comparably to
fine-tuning a 30$\times$ larger state-of-the-art language model on
CommensenseQA. Thus, STaR lets a model improve itself by learning from its own
generated reasoning."
Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas,0.694283,"Vision and language navigation (VLN) is a challenging visually-grounded
language understanding task. Given a natural language navigation instruction, a
visual agent interacts with a graph-based environment equipped with panorama
images and tries to follow the described route. Most prior work has been
conducted in indoor scenarios where best results were obtained for navigation
on routes that are similar to the training routes, with sharp drops in
performance when testing on unseen environments. We focus on VLN in outdoor
scenarios and find that in contrast to indoor VLN, most of the gain in outdoor
VLN on unseen data is due to features like junction type embedding or heading
delta that are specific to the respective environment graph, while image
information plays a very minor role in generalizing VLN to unseen outdoor
areas. These findings show a bias to specifics of graph representations of
urban environments, demanding that VLN tasks grow in scale and diversity of
geographical environments."
Investigating Reasons for Disagreement in Natural Language Inference,0.722576,"We investigate how disagreement in natural language inference (NLI)
annotation arises. We developed a taxonomy of disagreement sources with 10
categories spanning 3 high-level classes. We found that some disagreements are
due to uncertainty in the sentence meaning, others to annotator biases and task
artifacts, leading to different interpretations of the label distribution. We
explore two modeling approaches for detecting items with potential
disagreement: a 4-way classification with a ""Complicated"" label in addition to
the three standard NLI labels, and a multilabel classification approach. We
found that the multilabel classification is more expressive and gives better
recall of the possible interpretations in the data."
Learning to generate line drawings that convey geometry and semantics,0.765774,"This paper presents an unpaired method for creating line drawings from
photographs. Current methods often rely on high quality paired datasets to
generate line drawings. However, these datasets often have limitations due to
the subjects of the drawings belonging to a specific domain, or in the amount
of data collected. Although recent work in unsupervised image-to-image
translation has shown much progress, the latest methods still struggle to
generate compelling line drawings. We observe that line drawings are encodings
of scene information and seek to convey 3D shape and semantic meaning. We build
these observations into a set of objectives and train an image translation to
map photographs into line drawings. We introduce a geometry loss which predicts
depth information from the image features of a line drawing, and a semantic
loss which matches the CLIP features of a line drawing with its corresponding
photograph. Our approach outperforms state-of-the-art unpaired image
translation and line drawing generation methods on creating line drawings from
arbitrary photographs. For code and demo visit our webpage
carolineec.github.io/informative_drawings"
On Explaining Multimodal Hateful Meme Detection Models,0.706767,"Hateful meme detection is a new multimodal task that has gained significant
traction in academic and industry research communities. Recently, researchers
have applied pre-trained visual-linguistic models to perform the multimodal
classification task, and some of these solutions have yielded promising
results. However, what these visual-linguistic models learn for the hateful
meme classification task remains unclear. For instance, it is unclear if these
models are able to capture the derogatory or slurs references in multimodality
(i.e., image and text) of the hateful memes. To fill this research gap, this
paper propose three research questions to improve our understanding of these
visual-linguistic models performing the hateful meme classification task. We
found that the image modality contributes more to the hateful meme
classification task, and the visual-linguistic models are able to perform
visual-text slurs grounding to a certain extent. Our error analysis also shows
that the visual-linguistic models have acquired biases, which resulted in
false-positive predictions."
Capturing Failures of Large Language Models via Human Cognitive Biases,0.72955,"Large language models generate complex, open-ended outputs: instead of
outputting a class label they write summaries, generate dialogue, or produce
working code. In order to asses the reliability of these open-ended generation
systems, we aim to identify qualitative categories of erroneous behavior,
beyond identifying individual errors. To hypothesize and test for such
qualitative errors, we draw inspiration from human cognitive biases --
systematic patterns of deviation from rational judgement. Specifically, we use
cognitive biases as motivation to (i) generate hypotheses for problems that
models may have, and (ii) develop experiments that elicit these problems. Using
code generation as a case study, we find that OpenAI's Codex errs predictably
based on how the input prompt is framed, adjusts outputs towards anchors, and
is biased towards outputs that mimic frequent training examples. We then use
our framework to elicit high-impact errors such as incorrectly deleting files.
Our results indicate that experimental methodology from cognitive science can
help characterize how machine learning systems behave."
ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries,0.67077,"Perception and prediction are two separate modules in the existing autonomous
driving systems. They interact with each other via hand-picked features such as
agent bounding boxes and trajectories. Due to this separation, prediction, as a
downstream module, only receives limited information from the perception
module. To make matters worse, errors from the perception modules can propagate
and accumulate, adversely affecting the prediction results. In this work, we
propose ViP3D, a query-based visual trajectory prediction pipeline that
exploits rich information from raw videos to directly predict future
trajectories of agents in a scene. ViP3D employs sparse agent queries to
detect, track, and predict throughout the pipeline, making it the first fully
differentiable vision-based trajectory prediction approach. Instead of using
historical feature maps and trajectories, useful information from previous
timestamps is encoded in agent queries, which makes ViP3D a concise streaming
prediction method. Furthermore, extensive experimental results on the nuScenes
dataset show the strong vision-based prediction performance of ViP3D over
traditional pipelines and previous end-to-end models."
"RARR: Researching and Revising What Language Models Say, Using Language Models",0.674691,"Language models (LMs) now excel at many tasks such as few-shot learning,
question answering, reasoning, and dialog. However, they sometimes generate
unsupported or misleading content. A user cannot easily determine whether their
outputs are trustworthy or not, because most LMs do not have any built-in
mechanism for attribution to external evidence. To enable attribution while
still preserving all the powerful advantages of recent generation models, we
propose RARR (Retrofit Attribution using Research and Revision), a system that
1) automatically finds attribution for the output of any text generation model
and 2) post-edits the output to fix unsupported content while preserving the
original output as much as possible. When applied to the output of several
state-of-the-art LMs on a diverse set of generation tasks, we find that RARR
significantly improves attribution while otherwise preserving the original
input to a much greater degree than previously explored edit models.
Furthermore, the implementation of RARR requires only a handful of training
examples, a large language model, and standard web search."
A Rationale-Centric Framework for Human-in-the-loop Machine Learning,0.760577,"We present a novel rationale-centric framework with human-in-the-loop --
Rationales-centric Double-robustness Learning (RDL) -- to boost model
out-of-distribution performance in few-shot learning scenarios. By using static
semi-factual generation and dynamic human-intervened correction, RDL exploits
rationales (i.e. phrases that cause the prediction), human interventions and
semi-factual augmentations to decouple spurious associations and bias models
towards generally applicable underlying distributions, which enables fast and
accurate generalisation. Experimental results show that RDL leads to
significant prediction benefits on both in-distribution and out-of-distribution
tests compared to many state-of-the-art benchmarks -- especially for few-shot
learning scenarios. We also perform extensive ablation studies to support
in-depth analyses of each component in our framework."
Deepfake Video Detection with Spatiotemporal Dropout Transformer,0.719901,"While the abuse of deepfake technology has caused serious concerns recently,
how to detect deepfake videos is still a challenge due to the high
photo-realistic synthesis of each frame. Existing image-level approaches often
focus on single frame and ignore the spatiotemporal cues hidden in deepfake
videos, resulting in poor generalization and robustness. The key of a
video-level detector is to fully exploit the spatiotemporal inconsistency
distributed in local facial regions across different frames in deepfake videos.
Inspired by that, this paper proposes a simple yet effective patch-level
approach to facilitate deepfake video detection via spatiotemporal dropout
transformer. The approach reorganizes each input video into bag of patches that
is then fed into a vision transformer to achieve robust representation.
Specifically, a spatiotemporal dropout operation is proposed to fully explore
patch-level spatiotemporal cues and serve as effective data augmentation to
further enhance model's robustness and generalization ability. The operation is
flexible and can be easily plugged into existing vision transformers. Extensive
experiments demonstrate the effectiveness of our approach against 25
state-of-the-arts with impressive robustness, generalizability, and
representation ability."
Learning to Execute Actions or Ask Clarification Questions,0.732715,"Collaborative tasks are ubiquitous activities where a form of communication
is required in order to reach a joint goal. Collaborative building is one of
such tasks. We wish to develop an intelligent builder agent in a simulated
building environment (Minecraft) that can build whatever users wish to build by
just talking to the agent. In order to achieve this goal, such agents need to
be able to take the initiative by asking clarification questions when further
information is needed. Existing works on Minecraft Corpus Dataset only learn to
execute instructions neglecting the importance of asking for clarifications. In
this paper, we extend the Minecraft Corpus Dataset by annotating all builder
utterances into eight types, including clarification questions, and propose a
new builder agent model capable of determining when to ask or execute
instructions. Experimental results show that our model achieves
state-of-the-art performance on the collaborative building task with a
substantial improvement. We also define two new tasks, the learning to ask task
and the joint learning task. The latter consists of solving both collaborating
building and learning to ask tasks jointly."
Earnings-22: A Practical Benchmark for Accents in the Wild,0.708016,"Modern automatic speech recognition (ASR) systems have achieved superhuman
Word Error Rate (WER) on many common corpora despite lacking adequate
performance on speech in the wild. Beyond that, there is a lack of real-world,
accented corpora to properly benchmark academic and commercial models. To
ensure this type of speech is represented in ASR benchmarking, we present
Earnings-22, a 125 file, 119 hour corpus of English-language earnings calls
gathered from global companies. We run a comparison across 4 commercial models
showing the variation in performance when taking country of origin into
consideration. Looking at hypothesis transcriptions, we explore errors common
to all ASR systems tested. By examining Individual Word Error Rate (IWER), we
find that key speech features impact model performance more for certain accents
than others. Earnings-22 provides a free-to-use benchmark of real-world,
accented audio to bridge academic and industrial research."
Parallel Spatio-Temporal Attention-Based TCN for Multivariate Time Series Prediction,0.743096,"As industrial systems become more complex and monitoring sensors for
everything from surveillance to our health become more ubiquitous, multivariate
time series prediction is taking an important place in the smooth-running of
our society. A recurrent neural network with attention to help extend the
prediction windows is the current-state-of-the-art for this task. However, we
argue that their vanishing gradients, short memories, and serial architecture
make RNNs fundamentally unsuited to long-horizon forecasting with complex data.
Temporal convolutional networks (TCNs) do not suffer from gradient problems and
they support parallel calculations, making them a more appropriate choice.
Additionally, they have longer memories than RNNs, albeit with some instability
and efficiency problems. Hence, we propose a framework, called PSTA-TCN, that
combines a parallel spatio-temporal attention mechanism to extract dynamic
internal correlations with stacked TCN backbones to extract features from
different window sizes. The framework makes full use parallel calculations to
dramatically reduce training times, while substantially increasing accuracy
with stable prediction windows up to 13 times longer than the status quo."
Character-Aware Models Improve Visual Text Rendering,0.702044,"Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify this
effect, we conduct a series of experiments comparing character-aware vs.
character-blind text encoders. In the text-only domain, we find that
character-aware models provide large gains on a novel spelling task
(WikiSpell). Applying our learnings to the visual domain, we train a suite of
image generation models, and show that character-aware variants outperform
their character-blind counterparts across a range of novel text rendering tasks
(our DrawText benchmark). Our models set a much higher state-of-the-art on
visual spelling, with 30+ point accuracy gains over competitors on rare words,
despite training on far fewer examples."
MAViL: Masked Audio-Video Learners,0.759932,"We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks."
How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning,0.749536,"To avoid collapse in self-supervised learning (SSL), a contrastive loss is
widely used but often requires a large number of negative samples. Without
negative samples yet achieving competitive performance, a recent work has
attracted significant attention for providing a minimalist simple Siamese
(SimSiam) method to avoid collapse. However, the reason for how it avoids
collapse without negative samples remains not fully clear and our investigation
starts by revisiting the explanatory claims in the original SimSiam. After
refuting their claims, we introduce vector decomposition for analyzing the
collapse based on the gradient analysis of the $l_2$-normalized representation
vector. This yields a unified perspective on how negative samples and SimSiam
alleviate collapse. Such a unified perspective comes timely for understanding
the recent progress in SSL."
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers,0.702804,"We propose an efficient design of Transformer-based models for multivariate
time series forecasting and self-supervised representation learning. It is
based on two key components: (i) segmentation of time series into
subseries-level patches which are served as input tokens to Transformer; (ii)
channel-independence where each channel contains a single univariate time
series that shares the same embedding and Transformer weights across all the
series. Patching design naturally has three-fold benefit: local semantic
information is retained in the embedding; computation and memory usage of the
attention maps are quadratically reduced given the same look-back window; and
the model can attend longer history. Our channel-independent patch time series
Transformer (PatchTST) can improve the long-term forecasting accuracy
significantly when compared with that of SOTA Transformer-based models. We also
apply our model to self-supervised pre-training tasks and attain excellent
fine-tuning performance, which outperforms supervised training on large
datasets. Transferring of masked pre-trained representation on one dataset to
others also produces SOTA forecasting accuracy. Code is available at:
https://github.com/yuqinie98/PatchTST."
Contrastive Decoding: Open-ended Text Generation as Optimization,0.762185,"Given a language model (LM), maximum probability is a poor decoding objective
for open-ended generation, because it produces short and repetitive text. On
the other hand, sampling can often produce incoherent text that drifts from the
original topics. We propose contrastive decoding (CD), a reliable decoding
approach that optimizes a contrastive objective subject to a plausibility
constraint. The contrastive objective returns the difference between the
likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM
(called the amateur, e.g. OPT-125M), and the constraint ensures that the
outputs are plausible. CD is inspired by the fact that the failures of larger
LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and
that this difference signals which texts should be preferred. CD requires zero
additional training, and produces higher quality text than decoding from the
larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and
significantly outperforms four strong decoding algorithms (e.g., nucleus,
top-k) in automatic and human evaluations across wikipedia, news and story
domains."
RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states,0.726354,"Multi-agent deep reinforcement learning makes optimal decisions dependent on
system states observed by agents, but any uncertainty on the observations may
mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement
learning (MFAC) is well-known in the multi-agent field since it can effectively
handle a scalability problem. However, it is sensitive to state perturbations
that can significantly degrade the team rewards. This work proposes a Robust
Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two
innovations: 1) a new objective function of training actors, composed of a
\emph{policy gradient function} that is related to the expected cumulative
discount reward on sampled clean states and an \emph{action loss function} that
represents the difference between actions taken on clean and adversarial
states; and 2) a repetitive regularization of the action loss, ensuring the
trained actors to obtain excellent performance. Furthermore, this work proposes
a game model named a State-Adversarial Stochastic Game (SASG). Despite the Nash
equilibrium of SASG may not exist, adversarial perturbations to states in the
RoMFAC are proven to be defensible based on SASG. Experimental results show
that RoMFAC is robust against adversarial perturbations while maintaining its
competitive performance in environments without perturbations."
InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering,0.682053,"We present an information-theoretic regularization technique for few-shot
novel view synthesis based on neural implicit representation. The proposed
approach minimizes potential reconstruction inconsistency that happens due to
insufficient viewpoints by imposing the entropy constraint of the density in
each ray. In addition, to alleviate the potential degenerate issue when all
training images are acquired from almost redundant viewpoints, we further
incorporate the spatially smoothness constraint into the estimated images by
restricting information gains from a pair of rays with slightly different
viewpoints. The main idea of our algorithm is to make reconstructed scenes
compact along individual rays and consistent across rays in the neighborhood.
The proposed regularizers can be plugged into most of existing neural volume
rendering techniques based on NeRF in a straightforward way. Despite its
simplicity, we achieve consistently improved performance compared to existing
neural view synthesis methods by large margins on multiple standard benchmarks."
Bringing Old Films Back to Life,0.677787,"We present a learning-based framework, recurrent transformer network (RTN),
to restore heavily degraded old films. Instead of performing frame-wise
restoration, our method is based on the hidden knowledge learned from adjacent
frames that contain abundant information about the occlusion, which is
beneficial to restore challenging artifacts of each frame while ensuring
temporal coherency. Moreover, contrasting the representation of the current
frame and the hidden knowledge makes it possible to infer the scratch position
in an unsupervised manner, and such defect localization generalizes well to
real-world degradations. To better resolve mixed degradation and compensate for
the flow estimation error during frame alignment, we propose to leverage more
expressive transformer blocks for spatial restoration. Experiments on both
synthetic dataset and real-world old films demonstrate the significant
superiority of the proposed RTN over existing solutions. In addition, the same
framework can effectively propagate the color from keyframes to the whole
video, ultimately yielding compelling restored films. The implementation and
model will be released at
https://github.com/raywzy/Bringing-Old-Films-Back-to-Life."
When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,0.759777,"Despite their impressive performance on diverse tasks, large language models
(LMs) still struggle with tasks requiring rich world knowledge, implying the
limitations of relying solely on their parameters to encode a wealth of world
knowledge. This paper aims to understand LMs' strengths and limitations in
memorizing factual knowledge, by conducting large-scale knowledge probing
experiments of 10 models and 4 augmentation methods on PopQA, our new
open-domain QA dataset with 14k questions. We find that LMs struggle with less
popular factual knowledge, and that scaling fails to appreciably improve
memorization of factual knowledge in the long tail. We then show that
retrieval-augmented LMs largely outperform orders of magnitude larger LMs,
while unassisted LMs remain competitive in questions about high-popularity
entities. Based on those findings, we devise a simple, yet effective, method
for powerful and efficient retrieval-augmented LMs, which retrieves
non-parametric memories only when necessary. Experimental results show that
this significantly improves models' performance while reducing the inference
costs."
CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism,0.685297,"While neural representations for static 3D shapes are widely studied,
representations for deformable surfaces are limited to be template-dependent or
lack efficiency. We introduce Canonical Deformation Coordinate Space (CaDeX), a
unified representation of both shape and nonrigid motion. Our key insight is
the factorization of the deformation between frames by continuous bijective
canonical maps (homeomorphisms) and their inverses that go through a learned
canonical shape. Our novel deformation representation and its implementation
are simple, efficient, and guarantee cycle consistency, topology preservation,
and, if needed, volume conservation. Our modelling of the learned canonical
shapes provides a flexible and stable space for shape prior learning. We
demonstrate state-of-the-art performance in modelling a wide range of
deformable geometries: human bodies, animal bodies, and articulated objects."
Occlusion-Aware Cost Constructor for Light Field Depth Estimation,0.669599,"Matching cost construction is a key step in light field (LF) depth
estimation, but was rarely studied in the deep learning era. Recent deep
learning-based LF depth estimation methods construct matching cost by
sequentially shifting each sub-aperture image (SAI) with a series of predefined
offsets, which is complex and time-consuming. In this paper, we propose a
simple and fast cost constructor to construct matching cost for LF depth
estimation. Our cost constructor is composed by a series of convolutions with
specifically designed dilation rates. By applying our cost constructor to SAI
arrays, pixels under predefined disparities can be integrated and matching cost
can be constructed without using any shifting operation. More importantly, the
proposed cost constructor is occlusion-aware and can handle occlusions by
dynamically modulating pixels from different views. Based on the proposed cost
constructor, we develop a deep network for LF depth estimation. Our network
ranks first on the commonly used 4D LF benchmark in terms of the mean square
error (MSE), and achieves a faster running time than other state-of-the-art
methods."
AutoSNN: Towards Energy-Efficient Spiking Neural Networks,0.673412,"Spiking neural networks (SNNs) that mimic information transmission in the
brain can energy-efficiently process spatio-temporal information through
discrete and sparse spikes, thereby receiving considerable attention. To
improve accuracy and energy efficiency of SNNs, most previous studies have
focused solely on training methods, and the effect of architecture has rarely
been studied. We investigate the design choices used in the previous studies in
terms of the accuracy and number of spikes and figure out that they are not
best-suited for SNNs. To further improve the accuracy and reduce the spikes
generated by SNNs, we propose a spike-aware neural architecture search
framework called AutoSNN. We define a search space consisting of architectures
without undesirable design choices. To enable the spike-aware architecture
search, we introduce a fitness that considers both the accuracy and number of
spikes. AutoSNN successfully searches for SNN architectures that outperform
hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate
the effectiveness of AutoSNN on various datasets including neuromorphic
datasets."
GPT Takes the Bar Exam,0.713121,"Nearly all jurisdictions in the United States require a professional license
exam, commonly referred to as ""the Bar Exam,"" as a precondition for law
practice. To even sit for the exam, most jurisdictions require that an
applicant completes at least seven years of post-secondary education, including
three years at an accredited law school. In addition, most test-takers also
undergo weeks to months of further, exam-specific preparation. Despite this
significant investment of time and capital, approximately one in five
test-takers still score under the rate required to pass the exam on their first
try. In the face of a complex task that requires such depth of knowledge, what,
then, should we expect of the state of the art in ""AI?"" In this research, we
document our experimental evaluation of the performance of OpenAI's
`text-davinci-003` model, often-referred to as GPT-3.5, on the multistate
multiple choice (MBE) section of the exam. While we find no benefit in
fine-tuning over GPT-3.5's zero-shot performance at the scale of our training
data, we do find that hyperparameter optimization and prompt engineering
positively impacted GPT-3.5's zero-shot performance. For best prompt and
parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete
NCBE MBE practice exam, significantly in excess of the 25% baseline guessing
rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's
ranking of responses is also highly-correlated with correctness; its top two
and top three choices are correct 71% and 88% of the time, respectively,
indicating very strong non-entailment performance. While our ability to
interpret these results is limited by nascent scientific understanding of LLMs
and the proprietary nature of GPT, we believe that these results strongly
suggest that an LLM will pass the MBE component of the Bar Exam in the near
future."
A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction,0.74012,"The recent advances of deep learning have dramatically changed how machine
learning, especially in the domain of natural language processing, can be
applied to legal domain. However, this shift to the data-driven approaches
calls for larger and more diverse datasets, which are nevertheless still small
in number, especially in non-English languages. Here we present the first
large-scale benchmark of Korean legal AI datasets, LBOX OPEN, that consists of
one legal corpus, two classification tasks, two legal judgement prediction
(LJP) tasks, and one summarization task. The legal corpus consists of 147k
Korean precedents (259M tokens), of which 63k are sentenced in last 4 years and
96k are from the first and the second level courts in which factual issues are
reviewed. The two classification tasks are case names (11.3k) and statutes
(2.8k) prediction from the factual description of individual cases. The LJP
tasks consist of (1) 10.5k criminal examples where the model is asked to
predict fine amount, imprisonment with labor, and imprisonment without labor
ranges for the given facts, and (2) 4.7k civil examples where the inputs are
facts and claim for relief and outputs are the degrees of claim acceptance. The
summarization task consists of the Supreme Court precedents and the
corresponding summaries (20k). We also release realistic variants of the
datasets by extending the domain (1) to infrequent case categories in case name
(31k examples) and statute (17.7k) classification tasks, and (2) to long input
sequences in the summarization task (51k). Finally, we release LCUBE, the first
Korean legal language model trained on the legal corpus from this study. Given
the uniqueness of the Law of South Korea and the diversity of the legal tasks
covered in this work, we believe that LBOX OPEN contributes to the
multilinguality of global legal research. LBOX OPEN and LCUBE will be publicly
available."
Unified Speech-Text Pre-training for Speech Translation and Recognition,0.735517,"We describe a method to jointly pre-train speech and text in an
encoder-decoder modeling framework for speech translation and recognition. The
proposed method incorporates four self-supervised and supervised subtasks for
cross modality learning. A self-supervised speech subtask leverages unlabelled
speech data, and a (self-)supervised text to text subtask makes use of abundant
text training data. Two auxiliary supervised speech tasks are included to unify
speech and text modeling space. Our contribution lies in integrating linguistic
information from the text corpus into the speech pre-training. Detailed
analysis reveals learning interference among subtasks. Two pre-training
configurations for speech translation and recognition, respectively, are
presented to alleviate subtask interference. Our experiments show the proposed
method can effectively fuse speech and text information into one model. It
achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the
MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the
Librispeech speech recognition task."
Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation,0.679228,"Despite recent advances in detecting fake news generated by neural models,
their results are not readily applicable to effective detection of
human-written disinformation. What limits the successful transfer between them
is the sizable gap between machine-generated fake news and human-authored ones,
including the notable differences in terms of style and underlying intent. With
this in mind, we propose a novel framework for generating training examples
that are informed by the known styles and strategies of human-authored
propaganda. Specifically, we perform self-critical sequence training guided by
natural language inference to ensure the validity of the generated articles,
while also incorporating propaganda techniques, such as appeal to authority and
loaded language. In particular, we create a new training dataset, PropaNews,
with 2,256 examples, which we release for future use. Our experimental results
show that fake news detectors trained on PropaNews are better at detecting
human-written disinformation by 3.62 - 7.69% F1 score on two public datasets."
CLRNet: Cross Layer Refinement Network for Lane Detection,0.717816,"Lane is critical in the vision navigation system of the intelligent vehicle.
Naturally, lane is a traffic sign with high-level semantics, whereas it owns
the specific local pattern which needs detailed low-level features to localize
accurately. Using different feature levels is of great importance for accurate
lane detection, but it is still under-explored. In this work, we present Cross
Layer Refinement Network (CLRNet) aiming at fully utilizing both high-level and
low-level features in lane detection. In particular, it first detects lanes
with high-level semantic features then performs refinement based on low-level
features. In this way, we can exploit more contextual information to detect
lanes while leveraging local detailed lane features to improve localization
accuracy. We present ROIGather to gather global context, which further enhances
the feature representation of lanes. In addition to our novel network design,
we introduce Line IoU loss which regresses the lane line as a whole unit to
improve the localization accuracy. Experiments demonstrate that the proposed
method greatly outperforms the state-of-the-art lane detection approaches."
Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction,0.731751,"In this paper, we propose an effective yet efficient model PAIE for both
sentence-level and document-level Event Argument Extraction (EAE), which also
generalizes well when there is a lack of training data. On the one hand, PAIE
utilizes prompt tuning for extractive objectives to take the best advantages of
Pre-trained Language Models (PLMs). It introduces two span selectors based on
the prompt to select start/end tokens among input texts for each role. On the
other hand, it captures argument interactions via multi-role prompts and
conducts joint optimization with optimal span assignments via a bipartite
matching loss. Also, with a flexible prompt design, PAIE can extract multiple
arguments with the same role instead of conventional heuristic threshold
tuning. We have conducted extensive experiments on three benchmarks, including
both sentence- and document-level EAE. The results present promising
improvements from PAIE (3.5\% and 2.3\% F1 gains in average on three
benchmarks, for PAIE-base and PAIE-large respectively). Further analysis
demonstrates the efficiency, generalization to few-shot settings, and
effectiveness of different extractive prompt tuning strategies. Our code is
available at https://github.com/mayubo2333/PAIE."
Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances,0.74686,"In this paper, we consider mimicking fictional characters as a promising
direction for building engaging conversation models. To this end, we present a
new practical task where only a few utterances of each fictional character are
available to generate responses mimicking them. Furthermore, we propose a new
method named Pseudo Dialog Prompting (PDP) that generates responses by
leveraging the power of large-scale language models with prompts containing the
target character's utterances. To better reflect the style of the character,
PDP builds the prompts in the form of dialog that includes the character's
utterances as dialog history. Since only utterances of the characters are
available in the proposed task, PDP matches each utterance with an appropriate
pseudo-context from a predefined set of context candidates using a retrieval
model. Through human and automatic evaluation, we show that PDP generates
responses that better reflect the style of fictional characters than baseline
methods."
Adversarial Texture for Fooling Person Detectors in the Physical World,0.749783,"Nowadays, cameras equipped with AI systems can capture and analyze images to
detect people automatically. However, the AI system can make mistakes when
receiving deliberately designed patterns in the real world, i.e., physical
adversarial examples. Prior works have shown that it is possible to print
adversarial patches on clothes to evade DNN-based person detectors. However,
these adversarial examples could have catastrophic drops in the attack success
rate when the viewing angle (i.e., the camera's angle towards the object)
changes. To perform a multi-angle attack, we propose Adversarial Texture
(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people
wearing such clothes can hide from person detectors from different viewing
angles. We propose a generative method, named Toroidal-Cropping-based
Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive
structures. We printed several pieces of cloth with AdvTexure and then made
T-shirts, skirts, and dresses in the physical world. Experiments showed that
these clothes could fool person detectors in the physical world."
Exploring Lottery Ticket Hypothesis in Spiking Neural Networks,0.752767,"Spiking Neural Networks (SNNs) have recently emerged as a new generation of
low-power deep neural networks, which is suitable to be implemented on
low-power mobile/edge devices. As such devices have limited memory storage,
neural pruning on SNNs has been widely explored in recent years. Most existing
SNN pruning works focus on shallow SNNs (2~6 layers), however, deeper SNNs (>16
layers) are proposed by state-of-the-art SNN works, which is difficult to be
compatible with the current SNN pruning work. To scale up a pruning technique
towards deep SNNs, we investigate Lottery Ticket Hypothesis (LTH) which states
that dense networks contain smaller subnetworks (i.e., winning tickets) that
achieve comparable performance to the dense networks. Our studies on LTH reveal
that the winning tickets consistently exist in deep SNNs across various
datasets and architectures, providing up to 97% sparsity without huge
performance degradation. However, the iterative searching process of LTH brings
a huge training computational cost when combined with the multiple timesteps of
SNNs. To alleviate such heavy searching cost, we propose Early-Time (ET) ticket
where we find the important weight connectivity from a smaller number of
timesteps. The proposed ET ticket can be seamlessly combined with a common
pruning techniques for finding winning tickets, such as Iterative Magnitude
Pruning (IMP) and Early-Bird (EB) tickets. Our experiment results show that the
proposed ET ticket reduces search time by up to 38% compared to IMP or EB
methods. Code is available at Github."
Treeformer: Dense Gradient Trees for Efficient Attention Computation,0.675348,"Standard inference and training with transformer based architectures scale
quadratically with input sequence length. This is prohibitively large for a
variety of applications especially in web-page translation, query-answering
etc. Consequently, several approaches have been developed recently to speedup
attention computation by enforcing different attention structures such as
sparsity, low-rank, approximating attention using kernels. In this work, we
view attention computation as that of nearest neighbor retrieval, and use
decision tree based hierarchical navigation to reduce the retrieval cost per
query token from linear in sequence length to nearly logarithmic. Based on such
hierarchical navigation, we design Treeformer which can use one of two
efficient attention layers -- TF-Attention and TC-Attention. TF-Attention
computes the attention in a fine-grained style, while TC-Attention is a coarse
attention layer which also ensures that the gradients are ""dense"". To optimize
such challenging discrete layers, we propose a two-level bootstrapped training
method. Using extensive experiments on standard NLP benchmarks, especially for
long-sequences, we demonstrate that our Treeformer architecture can be almost
as accurate as baseline Transformer while using 30x lesser FLOPs in the
attention layer. Compared to Linformer, the accuracy can be as much as 12%
higher while using similar FLOPs in the attention layer."
Flow Matching for Generative Modeling,0.767595,"We introduce a new paradigm for generative modeling built on Continuous
Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale.
Specifically, we present the notion of Flow Matching (FM), a simulation-free
approach for training CNFs based on regressing vector fields of fixed
conditional probability paths. Flow Matching is compatible with a general
family of Gaussian probability paths for transforming between noise and data
samples -- which subsumes existing diffusion paths as specific instances.
Interestingly, we find that employing FM with diffusion paths results in a more
robust and stable alternative for training diffusion models. Furthermore, Flow
Matching opens the door to training CNFs with other, non-diffusion probability
paths. An instance of particular interest is using Optimal Transport (OT)
displacement interpolation to define the conditional probability paths. These
paths are more efficient than diffusion paths, provide faster training and
sampling, and result in better generalization. Training CNFs using Flow
Matching on ImageNet leads to consistently better performance than alternative
diffusion-based methods in terms of both likelihood and sample quality, and
allows fast and reliable sample generation using off-the-shelf numerical ODE
solvers."
The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing,0.729673,"Machine learning is transforming the video editing industry. Recent advances
in computer vision have leveled-up video editing tasks such as intelligent
reframing, rotoscoping, color grading, or applying digital makeups. However,
most of the solutions have focused on video manipulation and VFX. This work
introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster
research in AI-assisted video editing. Our benchmark suite focuses on video
editing tasks, beyond visual effects, such as automatic footage organization
and assisted video assembling. To enable research on these fronts, we annotate
more than 1.5M tags, with relevant concepts to cinematography, from 196176
shots sampled from movie scenes. We establish competitive baseline methods and
detailed analyses for each of the tasks. We hope our work sparks innovative
research towards underexplored areas of AI-assisted video editing."
Scalable Diffusion Models with Transformers,0.704514,"We explore a new class of diffusion models based on the transformer
architecture. We train latent diffusion models of images, replacing the
commonly-used U-Net backbone with a transformer that operates on latent
patches. We analyze the scalability of our Diffusion Transformers (DiTs)
through the lens of forward pass complexity as measured by Gflops. We find that
DiTs with higher Gflops -- through increased transformer depth/width or
increased number of input tokens -- consistently have lower FID. In addition to
possessing good scalability properties, our largest DiT-XL/2 models outperform
all prior diffusion models on the class-conditional ImageNet 512x512 and
256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter."
Exploiting Temporal Relations on Radar Perception for Autonomous Driving,0.734843,"We consider the object recognition problem in autonomous driving using
automotive radar sensors. Comparing to Lidar sensors, radar is cost-effective
and robust in all-weather conditions for perception in autonomous driving.
However, radar signals suffer from low angular resolution and precision in
recognizing surrounding objects. To enhance the capacity of automotive radar,
in this work, we exploit the temporal information from successive ego-centric
bird-eye-view radar image frames for radar object recognition. We leverage the
consistency of an object's existence and attributes (size, orientation, etc.),
and propose a temporal relational layer to explicitly model the relations
between objects within successive radar images. In both object detection and
multiple object tracking, we show the superiority of our method compared to
several baseline approaches."
How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?,0.689497,"Text-to-image generative models have achieved unprecedented success in
generating high-quality images based on natural language descriptions. However,
it is shown that these models tend to favor specific social groups when
prompted with neutral text descriptions (e.g., 'a photo of a lawyer').
Following Zhao et al. (2021), we study the effect on the diversity of the
generated images when adding ethical intervention that supports equitable
judgment (e.g., 'if all individuals can be a lawyer irrespective of their
gender') in the input prompts. To this end, we introduce an Ethical NaTural
Language Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset
to evaluate the change in image generations conditional on ethical
interventions across three social axes -- gender, skin color, and culture.
Through ENTIGEN framework, we find that the generations from minDALL.E,
DALL.E-mini and Stable Diffusion cover diverse social groups while preserving
the image quality. Preliminary studies indicate that a large change in the
model predictions is triggered by certain phrases such as 'irrespective of
gender' in the context of gender bias in the ethical interventions. We release
code and annotated data at https://github.com/Hritikbansal/entigen_emnlp."
VL-BEiT: Generative Vision-Language Pretraining,0.676427,"We introduce a vision-language foundation model called VL-BEiT, which is a
bidirectional multimodal Transformer learned by generative pretraining. Our
minimalist solution conducts masked prediction on both monomodal and multimodal
data with a shared Transformer. Specifically, we perform masked vision-language
modeling on image-text pairs, masked language modeling on texts, and masked
image modeling on images. VL-BEiT is learned from scratch with one unified
pretraining task, one shared backbone, and one-stage training. Our method is
conceptually simple and empirically effective. Experimental results show that
VL-BEiT obtains strong results on various vision-language benchmarks, such as
visual question answering, visual reasoning, and image-text retrieval.
Moreover, our method learns transferable visual features, achieving competitive
performance on image classification, and semantic segmentation."
Learning Neuro-Symbolic Skills for Bilevel Planning,0.734172,"Decision-making is challenging in robotics environments with continuous
object-centric states, continuous actions, long horizons, and sparse feedback.
Hierarchical approaches, such as task and motion planning (TAMP), address these
challenges by decomposing decision-making into two or more levels of
abstraction. In a setting where demonstrations and symbolic predicates are
given, prior work has shown how to learn symbolic operators and neural samplers
for TAMP with manually designed parameterized policies. Our main contribution
is a method for learning parameterized polices in combination with operators
and samplers. These components are packaged into modular neuro-symbolic skills
and sequenced together with search-then-sample TAMP to solve new tasks. In
experiments in four robotics domains, we show that our approach -- bilevel
planning with neuro-symbolic skills -- can solve a wide range of tasks with
varying initial states, goals, and objects, outperforming six baselines and
ablations. Video: https://youtu.be/PbFZP8rPuGg Code:
https://tinyurl.com/skill-learning"
u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality,0.725644,"While audio-visual speech models can yield superior performance and
robustness compared to audio-only models, their development and adoption are
hindered by the lack of labeled and unlabeled audio-visual data and the cost to
deploy one model per modality. In this paper, we present u-HuBERT, a
self-supervised pre-training framework that can leverage both multimodal and
unimodal speech with a unified masked cluster prediction objective. By
utilizing modality dropout during pre-training, we demonstrate that a single
fine-tuned model can achieve performance on par or better than the
state-of-the-art modality-specific models. Moreover, our model fine-tuned only
on audio can perform well with audio-visual and visual speech input, achieving
zero-shot modality generalization for multiple speech processing tasks. In
particular, our single model yields 1.2%/1.4%/27.2% speech recognition word
error rate on LRS3 with audio-visual/audio/visual input. Codes and models are
available at https://github.com/facebookresearch/av_hubert"
Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion,0.754668,"An attached arm can significantly increase the applicability of legged robots
to several mobile manipulation tasks that are not possible for the wheeled or
tracked counterparts. The standard hierarchical control pipeline for such
legged manipulators is to decouple the controller into that of manipulation and
locomotion. However, this is ineffective. It requires immense engineering to
support coordination between the arm and legs, and error can propagate across
modules causing non-smooth unnatural motions. It is also biological implausible
given evidence for strong motor synergies across limbs. In this work, we
propose to learn a unified policy for whole-body control of a legged
manipulator using reinforcement learning. We propose Regularized Online
Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage
Mixing exploiting the causal dependency in the action space to overcome local
minima during training the whole-body system. We also present a simple design
for a low-cost legged manipulator, and find that our unified policy can
demonstrate dynamic and agile behaviors across several task setups. Videos are
at https://maniploco.github.io"
XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model,0.721662,"We present XMem, a video object segmentation architecture for long videos
with unified feature memory stores inspired by the Atkinson-Shiffrin memory
model. Prior work on video object segmentation typically only uses one type of
feature memory. For videos longer than a minute, a single feature memory model
tightly links memory consumption and accuracy. In contrast, following the
Atkinson-Shiffrin model, we develop an architecture that incorporates multiple
independent yet deeply-connected feature memory stores: a rapidly updated
sensory memory, a high-resolution working memory, and a compact thus sustained
long-term memory. Crucially, we develop a memory potentiation algorithm that
routinely consolidates actively used working memory elements into the long-term
memory, which avoids memory explosion and minimizes performance decay for
long-term prediction. Combined with a new memory reading mechanism, XMem
greatly exceeds state-of-the-art performance on long-video datasets while being
on par with state-of-the-art methods (that do not work on long videos) on
short-video datasets. Code is available at https://hkchengrex.github.io/XMem"
NEWTS: A Corpus for News Topic-Focused Summarization,0.689056,"Text summarization models are approaching human levels of fidelity. Existing
benchmarking corpora provide concordant pairs of full and abridged versions of
Web, news or, professional content. To date, all summarization datasets operate
under a one-size-fits-all paradigm that may not reflect the full range of
organic summarization needs. Several recently proposed models (e.g., plug and
play language models) have the capacity to condition the generated summaries on
a desired range of themes. These capacities remain largely unused and
unevaluated as there is no dedicated dataset that would support the task of
topic-focused summarization.
  This paper introduces the first topical summarization corpus NEWTS, based on
the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing.
Each source article is paired with two reference summaries, each focusing on a
different theme of the source document. We evaluate a representative range of
existing techniques and analyze the effectiveness of different prompting
methods."
Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion,0.733405,"Human behavior has the nature of indeterminacy, which requires the pedestrian
trajectory prediction system to model the multi-modality of future motion
states. Unlike existing stochastic trajectory prediction methods which usually
use a latent variable to represent multi-modality, we explicitly simulate the
process of human motion variation from indeterminate to determinate. In this
paper, we present a new framework to formulate the trajectory prediction task
as a reverse process of motion indeterminacy diffusion (MID), in which we
progressively discard indeterminacy from all the walkable areas until reaching
the desired trajectory. This process is learned with a parameterized Markov
chain conditioned by the observed trajectories. We can adjust the length of the
chain to control the degree of indeterminacy and balance the diversity and
determinacy of the predictions. Specifically, we encode the history behavior
information and the social interactions as a state embedding and devise a
Transformer-based diffusion model to capture the temporal dependencies of
trajectories. Extensive experiments on the human trajectory prediction
benchmarks including the Stanford Drone and ETH/UCY datasets demonstrate the
superiority of our method. Code is available at
https://github.com/gutianpei/MID."
Diverse Weight Averaging for Out-of-Distribution Generalization,0.769323,"Standard neural networks struggle to generalize under distribution shifts in
computer vision. Fortunately, combining multiple networks can consistently
improve out-of-distribution generalization. In particular, weight averaging
(WA) strategies were shown to perform best on the competitive DomainBed
benchmark; they directly average the weights of multiple networks despite their
nonlinearities. In this paper, we propose Diverse Weight Averaging (DiWA), a
new WA strategy whose main motivation is to increase the functional diversity
across averaged models. To this end, DiWA averages weights obtained from
several independent training runs: indeed, models obtained from different runs
are more diverse than those collected along a single run thanks to differences
in hyperparameters and training procedures. We motivate the need for diversity
by a new bias-variance-covariance-locality decomposition of the expected error,
exploiting similarities between WA and standard functional ensembling.
Moreover, this decomposition highlights that WA succeeds when the variance term
dominates, which we show occurs when the marginal distribution changes at test
time. Experimentally, DiWA consistently improves the state of the art on
DomainBed without inference overhead."
LAS-AT: Adversarial Training with Learnable Attack Strategy,0.748254,"Adversarial training (AT) is always formulated as a minimax problem, of which
the performance depends on the inner optimization that involves the generation
of adversarial examples (AEs). Most previous methods adopt Projected Gradient
Decent (PGD) with manually specifying attack parameters for AE generation. A
combination of the attack parameters can be referred to as an attack strategy.
Several works have revealed that using a fixed attack strategy to generate AEs
during the whole training phase limits the model robustness and propose to
exploit different attack strategies at different training stages to improve
robustness. But those multi-stage hand-crafted attack strategies need much
domain expertise, and the robustness improvement is limited. In this paper, we
propose a novel framework for adversarial training by introducing the concept
of ""learnable attack strategy"", dubbed LAS-AT, which learns to automatically
produce attack strategies to improve the model robustness. Our framework is
composed of a target network that uses AEs for training to improve robustness
and a strategy network that produces attack strategies to control the AE
generation. Experimental evaluations on three benchmark databases demonstrate
the superiority of the proposed method. The code is released at
https://github.com/jiaxiaojunQAQ/LAS-AT."
ArCovidVac: Analyzing Arabic Tweets About COVID-19 Vaccination,0.684463,"The emergence of the COVID-19 pandemic and the first global infodemic have
changed our lives in many different ways. We relied on social media to get the
latest information about the COVID-19 pandemic and at the same time to
disseminate information. The content in social media consisted not only health
related advises, plans, and informative news from policy makers, but also
contains conspiracies and rumors. It became important to identify such
information as soon as they are posted to make actionable decisions (e.g.,
debunking rumors, or taking certain measures for traveling). To address this
challenge, we develop and publicly release the first largest manually annotated
Arabic tweet dataset, ArCovidVac, for the COVID-19 vaccination campaign,
covering many countries in the Arab region. The dataset is enriched with
different layers of annotation, including, (i) Informativeness (more vs. less
importance of the tweets); (ii) fine-grained tweet content types (e.g., advice,
rumors, restriction, authenticate news/information); and (iii) stance towards
vaccination (pro-vaccination, neutral, anti-vaccination). Further, we performed
in-depth analysis of the data, exploring the popularity of different vaccines,
trending hashtags, topics and presence of offensiveness in the tweets. We
studied the data for individual types of tweets and temporal changes in stance
towards vaccine. We benchmarked the ArCovidVac dataset using transformer
architectures for informativeness, content types, and stance detection."
Monocular Dynamic View Synthesis: A Reality Check,0.696957,"We study the recent progress on dynamic view synthesis (DVS) from monocular
video. Though existing approaches have demonstrated impressive results, we show
a discrepancy between the practical capture process and the existing
experimental protocols, which effectively leaks in multi-view signals during
training. We define effective multi-view factors (EMFs) to quantify the amount
of multi-view signal present in the input capture sequence based on the
relative camera-scene motion. We introduce two new metrics: co-visibility
masked image metrics and correspondence accuracy, which overcome the issue in
existing protocols. We also propose a new iPhone dataset that includes more
diverse real-life deformation sequences. Using our proposed experimental
protocol, we show that the state-of-the-art approaches observe a 1-2 dB drop in
masked PSNR in the absence of multi-view cues and 4-5 dB drop when modeling
complex motion. Code and data can be found at https://hangg7.com/dycheck."
An Empirical Study on Explanations in Out-of-Domain Settings,0.681423,"Recent work in Natural Language Processing has focused on developing
approaches that extract faithful explanations, either via identifying the most
important tokens in the input (i.e. post-hoc explanations) or by designing
inherently faithful models that first select the most important tokens and then
use them to predict the correct label (i.e. select-then-predict models).
Currently, these approaches are largely evaluated on in-domain settings. Yet,
little is known about how post-hoc explanations and inherently faithful models
perform in out-of-domain settings. In this paper, we conduct an extensive
empirical study that examines: (1) the out-of-domain faithfulness of post-hoc
explanations, generated by five feature attribution methods; and (2) the
out-of-domain performance of two inherently faithful models over six datasets.
Contrary to our expectations, results show that in many cases out-of-domain
post-hoc explanation faithfulness measured by sufficiency and comprehensiveness
is higher compared to in-domain. We find this misleading and suggest using a
random baseline as a yardstick for evaluating post-hoc explanation
faithfulness. Our findings also show that select-then predict models
demonstrate comparable predictive performance in out-of-domain settings to
full-text trained models."
FOSTER: Feature Boosting and Compression for Class-Incremental Learning,0.695851,"The ability to learn new concepts continually is necessary in this
ever-changing world. However, deep neural networks suffer from catastrophic
forgetting when learning new categories. Many works have been proposed to
alleviate this phenomenon, whereas most of them either fall into the
stability-plasticity dilemma or take too much computation or storage overhead.
Inspired by the gradient boosting algorithm to gradually fit the residuals
between the target model and the previous ensemble model, we propose a novel
two-stage learning paradigm FOSTER, empowering the model to learn new
categories adaptively. Specifically, we first dynamically expand new modules to
fit the residuals between the target and the output of the original model.
Next, we remove redundant parameters and feature dimensions through an
effective distillation strategy to maintain the single backbone model. We
validate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different
settings. Experimental results show that our method achieves state-of-the-art
performance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER."
Multi-modal Alignment using Representation Codebook,0.71753,"Aligning signals from different modalities is an important step in
vision-language representation learning as it affects the performance of later
stages such as cross-modality fusion. Since image and text typically reside in
different regions of the feature space, directly aligning them at instance
level is challenging especially when features are still evolving during
training. In this paper, we propose to align at a higher and more stable level
using cluster representation. Specifically, we treat image and text as two
""views"" of the same entity, and encode them into a joint vision-language coding
space spanned by a dictionary of cluster centers (codebook). We contrast
positive and negative samples via their cluster assignments while
simultaneously optimizing the cluster centers. To further smooth out the
learning process, we adopt a teacher-student distillation paradigm, where the
momentum teacher of one view guides the student learning of the other. We
evaluated our approach on common vision language benchmarks and obtain new SoTA
on zero-shot cross modality retrieval while being competitive on various other
transfer tasks."
Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model,0.771528,"Most existing Image Restoration (IR) models are task-specific, which can not
be generalized to different degradation operators. In this work, we propose the
Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for
arbitrary linear IR problems, including but not limited to image
super-resolution, colorization, inpainting, compressed sensing, and deblurring.
DDNM only needs a pre-trained off-the-shelf diffusion model as the generative
prior, without any extra training or network modifications. By refining only
the null-space contents during the reverse diffusion process, we can yield
diverse results satisfying both data consistency and realness. We further
propose an enhanced and robust version, dubbed DDNM+, to support noisy
restoration and improve restoration quality for hard tasks. Our experiments on
several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot
IR methods. We also demonstrate that DDNM+ can solve complex real-world
applications, e.g., old photo restoration."
BERN2: an advanced neural biomedical named entity recognition and normalization tool,0.70606,"In biomedical natural language processing, named entity recognition (NER) and
named entity normalization (NEN) are key tasks that enable the automatic
extraction of biomedical entities (e.g. diseases and drugs) from the
ever-growing biomedical literature. In this article, we present BERN2 (Advanced
Biomedical Entity Recognition and Normalization), a tool that improves the
previous neural network-based NER tool by employing a multi-task NER model and
neural network-based NEN models to achieve much faster and more accurate
inference. We hope that our tool can help annotate large-scale biomedical texts
for various tasks such as biomedical knowledge graph construction."
Emotion-Controllable Generalized Talking Face Generation,0.743179,"Despite the significant progress in recent years, very few of the AI-based
talking face generation methods attempt to render natural emotions. Moreover,
the scope of the methods is majorly limited to the characteristics of the
training dataset, hence they fail to generalize to arbitrary unseen faces. In
this paper, we propose a one-shot facial geometry-aware emotional talking face
generation method that can generalize to arbitrary faces. We propose a graph
convolutional neural network that uses speech content feature, along with an
independent emotion input to generate emotion and speech-induced motion on
facial geometry-aware landmark representation. This representation is further
used in our optical flow-guided texture generation network for producing the
texture. We propose a two-branch texture generation network, with motion and
texture branches designed to consider the motion and texture content
independently. Compared to the previous emotion talking face methods, our
method can adapt to arbitrary faces captured in-the-wild by fine-tuning with
only a single image of the target identity in neutral emotion."
D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat,0.668458,"In a depression-diagnosis-directed clinical session, doctors initiate a
conversation with ample emotional support that guides the patients to expose
their symptoms based on clinical diagnosis criteria. Such a dialogue system is
distinguished from existing single-purpose human-machine dialog systems, as it
combines task-oriented and chit-chats with uniqueness in dialogue topics and
procedures. However, due to the social stigma associated with mental illness,
the dialogue data related to depression consultation and diagnosis are rarely
disclosed. Based on clinical depression diagnostic criteria ICD-11 and DSM-5,
we designed a 3-phase procedure to construct D$^4$: a Chinese Dialogue Dataset
for Depression-Diagnosis-Oriented Chat, which simulates the dialogue between
doctors and patients during the diagnosis of depression, including diagnosis
results and symptom summary given by professional psychiatrists for each
conversation. Upon the newly-constructed dataset, four tasks mirroring the
depression diagnosis process are established: response generation, topic
prediction, dialog summary, and severity classification of depressive episode
and suicide risk. Multi-scale evaluation results demonstrate that a more
empathy-driven and diagnostic-accurate consultation dialogue system trained on
our dataset can be achieved compared to rule-based bots."
ExAID: A Multimodal Explanation Framework for Computer-Aided Diagnosis of Skin Lesions,0.688027,"One principal impediment in the successful deployment of AI-based
Computer-Aided Diagnosis (CAD) systems in clinical workflows is their lack of
transparent decision making. Although commonly used eXplainable AI methods
provide some insight into opaque algorithms, such explanations are usually
convoluted and not readily comprehensible except by highly trained experts. The
explanation of decisions regarding the malignancy of skin lesions from
dermoscopic images demands particular clarity, as the underlying medical
problem definition is itself ambiguous. This work presents ExAID (Explainable
AI for Dermatology), a novel framework for biomedical image analysis, providing
multi-modal concept-based explanations consisting of easy-to-understand textual
explanations supplemented by visual maps justifying the predictions. ExAID
relies on Concept Activation Vectors to map human concepts to those learnt by
arbitrary Deep Learning models in latent space, and Concept Localization Maps
to highlight concepts in the input space. This identification of relevant
concepts is then used to construct fine-grained textual explanations
supplemented by concept-wise location information to provide comprehensive and
coherent multi-modal explanations. All information is comprehensively presented
in a diagnostic interface for use in clinical routines. An educational mode
provides dataset-level explanation statistics and tools for data and model
exploration to aid medical research and education. Through rigorous
quantitative and qualitative evaluation of ExAID, we show the utility of
multi-modal explanations for CAD-assisted scenarios even in case of wrong
predictions. We believe that ExAID will provide dermatologists an effective
screening tool that they both understand and trust. Moreover, it will be the
basis for similar applications in other biomedical imaging fields."
Modern Views of Machine Learning for Precision Psychiatry,0.742029,"In light of the NIMH's Research Domain Criteria (RDoC), the advent of
functional neuroimaging, novel technologies and methods provide new
opportunities to develop precise and personalized prognosis and diagnosis of
mental disorders. Machine learning (ML) and artificial intelligence (AI)
technologies are playing an increasingly critical role in the new era of
precision psychiatry. Combining ML/AI with neuromodulation technologies can
potentially provide explainable solutions in clinical practice and effective
therapeutic treatment. Advanced wearable and mobile technologies also call for
the new role of ML/AI for digital phenotyping in mobile mental health. In this
review, we provide a comprehensive review of the ML methodologies and
applications by combining neuroimaging, neuromodulation, and advanced mobile
technologies in psychiatry practice. Additionally, we review the role of ML in
molecular phenotyping and cross-species biomarker identification in precision
psychiatry. We further discuss explainable AI (XAI) and causality testing in a
closed-human-in-the-loop manner, and highlight the ML potential in multimedia
information extraction and multimodal data fusion. Finally, we discuss
conceptual and practical challenges in precision psychiatry and highlight ML
opportunities in future research."
JIFF: Jointly-aligned Implicit Face Function for High Quality Single View Clothed Human Reconstruction,0.741651,"This paper addresses the problem of single view 3D human reconstruction.
Recent implicit function based methods have shown impressive results, but they
fail to recover fine face details in their reconstructions. This largely
degrades user experience in applications like 3D telepresence. In this paper,
we focus on improving the quality of face in the reconstruction and propose a
novel Jointly-aligned Implicit Face Function (JIFF) that combines the merits of
the implicit function based approach and model based approach. We employ a 3D
morphable face model as our shape prior and compute space-aligned 3D features
that capture detailed face geometry information. Such space-aligned 3D features
are combined with pixel-aligned 2D features to jointly predict an implicit face
function for high quality face reconstruction. We further extend our pipeline
and introduce a coarse-to-fine architecture to predict high quality texture for
our detailed face model. Extensive evaluations have been carried out on public
datasets and our proposed JIFF has demonstrates superior performance (both
quantitatively and qualitatively) over existing state-of-the-arts."
Eco2AI: carbon emissions tracking of machine learning models as the first step towards sustainable AI,0.726407,"The size and complexity of deep neural networks continue to grow
exponentially, significantly increasing energy consumption for training and
inference by these models. We introduce an open-source package eco2AI to help
data scientists and researchers to track energy consumption and equivalent CO2
emissions of their models in a straightforward way. In eco2AI we put emphasis
on accuracy of energy consumption tracking and correct regional CO2 emissions
accounting. We encourage research community to search for new optimal
Artificial Intelligence (AI) architectures with a lower computational cost. The
motivation also comes from the concept of AI-based green house gases
sequestrating cycle with both Sustainable AI and Green AI pathways."
Rating the Crisis of Online Public Opinion Using a Multi-Level Index System,0.753431,"Online public opinion usually spreads rapidly and widely, thus a small
incident probably evolves into a large social crisis in a very short time, and
results in a heavy loss in credit or economic aspects. We propose a method to
rate the crisis of online public opinion based on a multi-level index system to
evaluate the impact of events objectively. Firstly, the dissemination mechanism
of online public opinion is explained from the perspective of information
ecology. According to the mechanism, some evaluation indexes are selected
through correlation analysis and principal component analysis. Then, a
classification model of text emotion is created via the training by deep
learning to achieve the accurate quantification of the emotional indexes in the
index system. Finally, based on the multi-level evaluation index system and
grey correlation analysis, we propose a method to rate the crisis of online
public opinion. The experiment with the real-time incident show that this
method can objectively evaluate the emotional tendency of Internet users and
rate the crisis in different dissemination stages of online public opinion. It
is helpful to realizing the crisis warning of online public opinion and timely
blocking the further spread of the crisis."
Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction,0.694898,"Emotion cause pair extraction (ECPE), as one of the derived subtasks of
emotion cause analysis (ECA), shares rich inter-related features with emotion
extraction (EE) and cause extraction (CE). Therefore EE and CE are frequently
utilized as auxiliary tasks for better feature learning, modeled via multi-task
learning (MTL) framework by prior works to achieve state-of-the-art (SoTA) ECPE
results. However, existing MTL-based methods either fail to simultaneously
model the specific features and the interactive feature in between, or suffer
from the inconsistency of label prediction. In this work, we consider
addressing the above challenges for improving ECPE by performing two alignment
mechanisms with a novel A^2Net model. We first propose a feature-task alignment
to explicitly model the specific emotion-&cause-specific features and the
shared interactive feature. Besides, an inter-task alignment is implemented, in
which the label distance between the ECPE and the combinations of EE&CE are
learned to be narrowed for better label consistency. Evaluations of benchmarks
show that our methods outperform current best-performing systems on all ECA
subtasks. Further analysis proves the importance of our proposed alignment
mechanisms for the task."
Bridging the Gap to Real-World Object-Centric Learning,0.67604,"Humans naturally decompose their environment into entities at the appropriate
level of abstraction to act in the world. Allowing machine learning algorithms
to derive this decomposition in an unsupervised way has become an important
line of research. However, current methods are restricted to simulated data or
require additional information in the form of motion or depth in order to
successfully discover objects. In this work, we overcome this limitation by
showing that reconstructing features from models trained in a self-supervised
manner is a sufficient training signal for object-centric representations to
arise in a fully unsupervised way. Our approach, DINOSAUR, significantly
out-performs existing image-based object-centric learning models on simulated
data and is the first unsupervised object-centric model that scales to
real-world datasets such as COCO and PASCAL VOC. DINOSAUR is conceptually
simple and shows competitive performance compared to more involved pipelines
from the computer vision literature."
TSA-Net: Tube Self-Attention Network for Action Quality Assessment,0.722532,"In recent years, assessing action quality from videos has attracted growing
attention in computer vision community and human computer interaction. Most
existing approaches usually tackle this problem by directly migrating the model
from action recognition tasks, which ignores the intrinsic differences within
the feature map such as foreground and background information. To address this
issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality
assessment (AQA). Specifically, we introduce a single object tracker into AQA
and propose the Tube Self-Attention Module (TSA), which can efficiently
generate rich spatio-temporal contextual information by adopting sparse feature
interactions. The TSA module is embedded in existing video networks to form
TSA-Net. Overall, our TSA-Net is with the following merits: 1) High
computational efficiency, 2) High flexibility, and 3) The state-of-the art
performance. Extensive experiments are conducted on popular action quality
assessment datasets including AQA-7 and MTL-AQA. Besides, a dataset named Fall
Recognition in Figure Skating (FR-FS) is proposed to explore the basic action
assessment in the figure skating scene."
Are Large Pre-Trained Language Models Leaking Your Personal Information?,0.721059,"Are Large Pre-Trained Language Models Leaking Your Personal Information? In
this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to
leaking personal information. Specifically, we query PLMs for email addresses
with contexts of the email address or prompts containing the owner's name. We
find that PLMs do leak personal information due to memorization. However, since
the models are weak at association, the risk of specific personal information
being extracted by attackers is low. We hope this work could help the community
to better understand the privacy risk of PLMs and bring new insights to make
PLMs safe."
BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning,0.728095,"Deep neural networks are vulnerable to Trojan attacks. Existing attacks use
visible patterns (e.g., a patch or image transformations) as triggers, which
are vulnerable to human inspection. In this paper, we propose stealthy and
efficient Trojan attacks, BppAttack. Based on existing biology literature on
human visual systems, we propose to use image quantization and dithering as the
Trojan trigger, making imperceptible changes. It is a stealthy and efficient
attack without training auxiliary models. Due to the small changes made to
images, it is hard to inject such triggers during training. To alleviate this
problem, we propose a contrastive learning based approach that leverages
adversarial attacks to generate negative sample pairs so that the learned
trigger is precise and accurate. The proposed method achieves high attack
success rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and
CelebA. It also effectively bypasses existing Trojan defenses and human
inspection. Our code can be found in
https://github.com/RU-System-Software-and-Security/BppAttack."
The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues,0.697832,"How can we test whether state-of-the-art generative models, such as Blender
and GPT-3, are good AI teachers, capable of replying to a student in an
educational dialogue? Designing an AI teacher test is challenging: although
evaluation methods are much-needed, there is no off-the-shelf solution to
measuring pedagogical ability. This paper reports on a first attempt at an AI
teacher test. We built a solution around the insight that you can run
conversational agents in parallel to human teachers in real-world dialogues,
simulate how different agents would respond to a student, and compare these
counterpart responses in terms of three abilities: speak like a teacher,
understand a student, help a student. Our method builds on the reliability of
comparative judgments in education and uses a probabilistic model and Bayesian
sampling to infer estimates of pedagogical ability. We find that, even though
conversational agents (Blender in particular) perform well on conversational
uptake, they are quantifiably worse than real teachers on several pedagogical
dimensions, especially with regard to helpfulness (Blender: {\Delta} ability =
-0.75; GPT-3: {\Delta} ability = -0.93)."
Table-To-Text generation and pre-training with TabT5,0.688171,"Encoder-only transformer models have been successfully applied to different
table understanding tasks, as in TAPAS (Herzig et al., 2020). A major
limitation of these architectures is that they are constrained to
classification-like tasks such as cell selection or entailment detection. We
present TABT5, an encoder-decoder model that generates natural language text
based on tables and textual inputs. TABT5 overcomes the encoder-only limitation
by incorporating a decoder component and leverages the input structure with
table specific embeddings and pre-training. TABT5 achieves new state-of-the-art
results on several domains, including spreadsheet formula prediction with a 15%
increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and
data-to-text generation with a 2.5% increase in BLEU."
Neural Texture Extraction and Distribution for Controllable Person Image Synthesis,0.725299,"We deal with the controllable person image synthesis task which aims to
re-render a human from a reference image with explicit control over body pose
and appearance. Observing that person images are highly structured, we propose
to generate desired images by extracting and distributing semantic entities of
reference images. To achieve this goal, a neural texture extraction and
distribution operation based on double attention is described. This operation
first extracts semantic neural textures from reference feature maps. Then, it
distributes the extracted neural textures according to the spatial
distributions learned from target poses. Our model is trained to predict human
images in arbitrary poses, which encourages it to extract disentangled and
expressive neural textures representing the appearance of different semantic
entities. The disentangled representation further enables explicit appearance
control. Neural textures of different reference images can be fused to control
the appearance of the interested areas. Experimental comparisons show the
superiority of the proposed model. Code is available at
https://github.com/RenYurui/Neural-Texture-Extraction-Distribution."
PYSKL: Towards Good Practices for Skeleton Action Recognition,0.742361,"We present PYSKL: an open-source toolbox for skeleton-based action
recognition based on PyTorch. The toolbox supports a wide variety of skeleton
action recognition algorithms, including approaches based on GCN and CNN. In
contrast to existing open-source skeleton action recognition projects that
include only one or two algorithms, PYSKL implements six different algorithms
under a unified framework with both the latest and original good practices to
ease the comparison of efficacy and efficiency. We also provide an original
GCN-based skeleton action recognition model named ST-GCN++, which achieves
competitive recognition performance without any complicated attention schemes,
serving as a strong baseline. Meanwhile, PYSKL supports the training and
testing of nine skeleton-based action recognition benchmarks and achieves
state-of-the-art recognition performance on eight of them. To facilitate future
research on skeleton action recognition, we also provide a large number of
trained models and detailed benchmark results to give some insights. PYSKL is
released at https://github.com/kennymckormick/pyskl and is actively maintained.
We will update this report when we add new features or benchmarks. The current
version corresponds to PYSKL v0.2."
Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles,0.791707,"Video Anomaly Detection (VAD) is an important topic in computer vision.
Motivated by the recent advances in self-supervised learning, this paper
addresses VAD by solving an intuitive yet challenging pretext task, i.e.,
spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained
classification problem. Our method exhibits several advantages over existing
works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial
and temporal dimensions, responsible for capturing highly discriminative
appearance and motion features, respectively; 2) full permutations are used to
provide abundant jigsaw puzzles covering various difficulty levels, allowing
the network to distinguish subtle spatio-temporal differences between normal
and abnormal events; and 3) the pretext task is tackled in an end-to-end manner
without relying on any pre-trained models. Our method outperforms
state-of-the-art counterparts on three public benchmarks. Especially on
ShanghaiTech Campus, the result is superior to reconstruction and
prediction-based methods by a large margin."
Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,0.864665,"This paper presents exploratory work on whether and to what extent biases
against queer and trans people are encoded in large language models (LLMs) such
as BERT. We also propose a method for reducing these biases in downstream
tasks: finetuning the models on data written by and/or about queer people. To
measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,
modeled after other bias-detection benchmarks but addressing homophobic and
transphobic biases. We found that BERT shows significant homophobic bias, but
this bias can be mostly mitigated by finetuning BERT on a natural language
corpus written by members of the LGBTQ+ community."
How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,0.854876,"Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection."
"GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction",0.863321,"We present GO-Surf, a direct feature grid optimization method for accurate
and fast surface reconstruction from RGB-D sequences. We model the underlying
scene with a learned hierarchical feature voxel grid that encapsulates
multi-level geometric and appearance local information. Feature vectors are
directly optimized such that after being tri-linearly interpolated, decoded by
two shallow MLPs into signed distance and radiance values, and rendered via
surface volume rendering, the discrepancy between synthesized and observed
RGB/depth values is minimized. Our supervision signals -- RGB, depth and
approximate SDF -- can be obtained directly from input images without any need
for fusion or post-processing. We formulate a novel SDF gradient regularization
term that encourages surface smoothness and hole filling while maintaining high
frequency details. GO-Surf can optimize sequences of $1$-$2$K frames in
$15$-$45$ minutes, a speedup of $\times60$ over NeuralRGB-D, the most related
approach based on an MLP representation, while maintaining on par performance
on standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/"
The Stack: 3 TB of permissively licensed source code,0.877409,"Large Language Models (LLMs) play an ever-increasing role in the field of
Artificial Intelligence (AI)--not only for natural language processing but also
for code understanding and generation. To stimulate open and responsible
research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting
of permissively licensed source code in 30 programming languages. We describe
how we collect the full dataset, construct a permissively licensed subset,
present a data governance plan, discuss limitations, and show promising results
on text2code benchmarks by training 350M-parameter decoders on different Python
subsets. We find that (1) near-deduplicating the data significantly boosts
performance across all experiments, and (2) it is possible to match previously
reported HumanEval and MBPP performance using only permissively licensed data.
We make the dataset available at https://hf.co/BigCode, provide a tool called
""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers
to search The Stack for copies of their code, and provide a process for code to
be removed from the dataset by following the instructions at
https://www.bigcode-project.org/docs/about/the-stack/."
Rationale-Augmented Ensembles in Language Models,0.868289,"Recent research has shown that rationales, or step-by-step chains of thought,
can be used to improve performance in multi-step reasoning tasks. We reconsider
rationale-augmented prompting for few-shot in-context learning, where (input ->
output) prompts are expanded to (input, rationale -> output) prompts. For
rationale-augmented prompting we demonstrate how existing approaches, which
rely on manual prompt engineering, are subject to sub-optimal rationales that
may harm performance. To mitigate this brittleness, we propose a unified
framework of rationale-augmented ensembles, where we identify rationale
sampling in the output space as the key component to robustly improve
performance. This framework is general and can easily be extended to common
natural language processing tasks, even those that do not traditionally
leverage intermediate steps, such as question answering, word sense
disambiguation, and sentiment analysis. We demonstrate that rationale-augmented
ensembles achieve more accurate and interpretable results than existing
prompting approaches--including standard prompting without rationales and
rationale-based chain-of-thought prompting--while simultaneously improving
interpretability of model predictions through the associated rationales."
CSL: A Large-scale Chinese Scientific Literature Dataset,0.851738,"Scientific literature serves as a high-quality corpus, supporting a lot of
Natural Language Processing (NLP) research. However, existing datasets are
centered around the English language, which restricts the development of
Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese
Scientific Literature dataset, which contains the titles, abstracts, keywords
and academic fields of 396k papers. To our knowledge, CSL is the first
scientific document dataset in Chinese. The CSL can serve as a Chinese corpus.
Also, this semi-structured data is a natural annotation that can constitute
many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the
performance of models across scientific domain tasks, i.e., summarization,
keyword generation and text classification. We analyze the behavior of existing
text-to-text models on the evaluation tasks and reveal the challenges for
Chinese scientific NLP tasks, which provides a valuable reference for future
research. Data and code are available at https://github.com/ydli-ai/CSL"
ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations,0.864665,"Precise representations of 3D faces are beneficial to various computer vision
and graphics applications. Due to the data discretization and model linearity,
however, it remains challenging to capture accurate identity and expression
clues in current studies. This paper presents a novel 3D morphable face model,
namely ImFace, to learn a nonlinear and continuous space with implicit neural
representations. It builds two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
and designs an improved learning strategy to extend embeddings of expressions
to allow more diverse changes. We further introduce a Neural Blend-Field to
learn sophisticated details by adaptively blending a series of local fields. In
addition to ImFace, an effective preprocessing pipeline is proposed to address
the issue of watertight input requirement in implicit representations, enabling
them to work with common facial surfaces for the first time. Extensive
experiments are performed to demonstrate the superiority of ImFace."
Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation,0.864665,"3D Human body pose and shape estimation within a temporal sequence can be
quite critical for understanding human behavior. Despite the significant
progress in human pose estimation in the recent years, which are often based on
single images or videos, human motion estimation on live stream videos is still
a rarely-touched area considering its special requirements for real-time output
and temporal consistency. To address this problem, we present a temporally
embedded 3D human body pose and shape estimation (TePose) method to improve the
accuracy and temporal consistency of pose estimation in live stream videos.
TePose uses previous predictions as a bridge to feedback the error for better
estimation in the current frame and to learn the correspondence between data
frames and predictions in the history. A multi-scale spatio-temporal graph
convolutional network is presented as the motion discriminator for adversarial
training using datasets without any 3D labeling. We propose a sequential data
loading strategy to meet the special start-to-end data processing requirement
of live stream. We demonstrate the importance of each proposed module with
extensive experiments. The results show the effectiveness of TePose on
widely-used human pose benchmarks with state-of-the-art performance."
CORE: Simple and Effective Session-based Recommendation within Consistent Representation Space,0.786109,"Session-based Recommendation (SBR) refers to the task of predicting the next
item based on short-term user behaviors within an anonymous session. However,
session embedding learned by a non-linear encoder is usually not in the same
representation space as item embeddings, resulting in the inconsistent
prediction issue while recommending items. To address this issue, we propose a
simple and effective framework named CORE, which can unify the representation
space for both the encoding and decoding processes. Firstly, we design a
representation-consistent encoder that takes the linear combination of input
item embeddings as session embedding, guaranteeing that sessions and items are
in the same representation space. Besides, we propose a robust distance
measuring method to prevent overfitting of embeddings in the consistent
representation space. Extensive experiments conducted on five public real-world
datasets demonstrate the effectiveness and efficiency of the proposed method.
The code is available at: https://github.com/RUCAIBox/CORE."
Faithful Reasoning Using Large Language Models,0.86867,"Although contemporary large language models (LMs) demonstrate impressive
question-answering capabilities, their answers are typically the product of a
single call to the model. This entails an unwelcome degree of opacity and
compromises performance, especially on problems that are inherently multi-step.
To address these limitations, we show how LMs can be made to perform faithful
multi-step reasoning via a process whose causal structure mirrors the
underlying logical structure of the problem. Our approach works by chaining
together reasoning steps, where each step results from calls to two fine-tuned
LMs, one for selection and one for inference, to produce a valid reasoning
trace. Our method carries out a beam search through the space of reasoning
traces to improve reasoning quality. We demonstrate the effectiveness of our
model on multi-step logical deduction and scientific question-answering,
showing that it outperforms baselines on final answer accuracy, and generates
humanly interpretable reasoning traces whose validity can be checked by the
user."
TAFNet: A Three-Stream Adaptive Fusion Network for RGB-T Crowd Counting,0.843613,"In this paper, we propose a three-stream adaptive fusion network named
TAFNet, which uses paired RGB and thermal images for crowd counting.
Specifically, TAFNet is divided into one main stream and two auxiliary streams.
We combine a pair of RGB and thermal images to constitute the input of main
stream. Two auxiliary streams respectively exploit RGB image and thermal image
to extract modality-specific features. Besides, we propose an Information
Improvement Module (IIM) to fuse the modality-specific features into the main
stream adaptively. Experiment results on RGBT-CC dataset show that our method
achieves more than 20% improvement on mean average error and root mean squared
error compared with state-of-the-art method. The source code will be publicly
available at https://github.com/TANGHAIHAN/TAFNet."
Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors,0.848092,"The main question this work aims at answering is: ""can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?"".
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is publicly available
for research purposes."
Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video,0.864665,"We present HandAvatar, a novel representation for hand animation and
rendering, which can generate smoothly compositional geometry and
self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as
a high-resolution mesh topology to fit personalized hand shapes. Sequentially,
we decompose hand geometry into per-bone rigid parts, and then re-compose
paired geometry encodings to derive an across-part consistent occupancy field.
As for texture modeling, we propose a self-occlusion-aware shading field
(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record
albedo information under a wide variety of hand poses. Moreover, directed soft
occupancy is designed to describe the ray-to-surface relation, which is
leveraged to generate an illumination field for the disentanglement of
pose-independent albedo and pose-dependent illumination. Trained from monocular
video data, our HandAvatar can perform free-pose hand animation and rendering
while at the same time achieving superior appearance fidelity. We also
demonstrate that HandAvatar provides a route for hand appearance editing.
Project website: https://seanchenxy.github.io/HandAvatarWeb."
Simple Open-Vocabulary Object Detection with Vision Transformers,0.824939,"Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub."
TripleRE: Knowledge Graph Embeddings via Tripled Relation Vectors,0.864665,"Translation-based knowledge graph embedding has been one of the most
important branches for knowledge representation learning since TransE came out.
Although many translation-based approaches have achieved some progress in
recent years, the performance was still unsatisfactory. This paper proposes a
novel knowledge graph embedding method named TripleRE with two versions. The
first version of TripleRE creatively divide the relationship vector into three
parts. The second version takes advantage of the concept of residual and
achieves better performance. In addition, attempts on using NodePiece to encode
entities achieved promising results in reducing the parametric size, and solved
the problems of scalability. Experiments show that our approach achieved
state-of-the-art performance on the large-scale knowledge graph dataset, and
competitive performance on other datasets."
FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,0.842614,"Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the
architecture used for FiD was chosen by making minimal modifications to a
standard T5 model, which our analysis shows to be highly suboptimal for a
retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to
the encoder, while the majority of inference time results from memory bandwidth
constraints in the decoder. We propose two simple changes to the FiD
architecture to alleviate memory bandwidth constraints, and speed up inference
by 7x. This allows us to use a much larger decoder at modest cost. We denote
FiD with the above modifications as FiDO, and show that it strongly improves
performance over existing FiD models for a wide range of inference budgets. For
example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves
better performance than FiD-Large."
PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch,0.864665,"Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and ""zeros out"" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types."
Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks,0.864665,"Representation learning on networks aims to derive a meaningful vector
representation for each node, thereby facilitating downstream tasks such as
link prediction, node classification, and node clustering. In heterogeneous
text-rich networks, this task is more challenging due to (1) presence or
absence of text: Some nodes are associated with rich textual information, while
others are not; (2) diversity of types: Nodes and edges of multiple types form
a heterogeneous network structure. As pretrained language models (PLMs) have
demonstrated their effectiveness in obtaining widely generalizable text
representations, a substantial amount of effort has been made to incorporate
PLMs into representation learning on text-rich networks. However, few of them
can jointly consider heterogeneous structure (network) information as well as
rich textual semantic information of each node effectively. In this paper, we
propose Heterformer, a Heterogeneous Network-Empowered Transformer that
performs contextualized text encoding and heterogeneous structure encoding in a
unified model. Specifically, we inject heterogeneous structure information into
each Transformer layer when encoding node texts. Meanwhile, Heterformer is
capable of characterizing node/edge type heterogeneity and encoding nodes with
or without texts. We conduct comprehensive experiments on three tasks (i.e.,
link prediction, node classification, and node clustering) on three large-scale
datasets from different domains, where Heterformer outperforms competitive
baselines significantly and consistently."
LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding,0.78243,"Structured document understanding has attracted considerable attention and
made significant progress recently, owing to its crucial role in intelligent
document processing. However, most existing related models can only deal with
the document data of specific language(s) (typically English) included in the
pre-training collection, which is extremely limited. To address this issue, we
propose a simple yet effective Language-independent Layout Transformer (LiLT)
for structured document understanding. LiLT can be pre-trained on the
structured documents of a single language and then directly fine-tuned on other
languages with the corresponding off-the-shelf monolingual/multilingual
pre-trained textual models. Experimental results on eight languages have shown
that LiLT can achieve competitive or even superior performance on diverse
widely-used downstream benchmarks, which enables language-independent benefit
from the pre-training of document layout structure. Code and model are publicly
available at https://github.com/jpWang/LiLT."
Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset,0.846832,"Speech is inherently continuous, where discrete words, phonemes and other
units are not clearly segmented, and so speech recognition has been an active
research problem for decades. In this work we have fine-tuned wav2vec 2.0 to
recognize and transcribe Bengali speech -- training it on the Bengali Common
Voice Speech Dataset. After training for 71 epochs, on a training set
consisting of 36919 mp3 files, we achieved a training loss of 0.3172 and WER of
0.2524 on a validation set of size 7,747. Using a 5-gram language model, the
Levenshtein Distance was 2.6446 on a test set of size 7,747. Then the training
set and validation set were combined, shuffled and split into 85-15 ratio.
Training for 7 more epochs on this combined dataset yielded an improved
Levenshtein Distance of 2.60753 on the test set. Our model was the best
performing one, achieving a Levenshtein Distance of 6.234 on a hidden dataset,
which was 1.1049 units lower than other competing submissions."
AUTOLEX: An Automatic Framework for Linguistic Exploration,0.798104,"Each language has its own complex systems of word, phrase, and sentence
construction, the guiding principles of which are often summarized in grammar
descriptions for the consumption of linguists or language learners. However,
manual creation of such descriptions is a fraught process, as creating
descriptions which describe the language in ""its own terms"" without bias or
error requires both a deep understanding of the language at hand and
linguistics as a whole. We propose an automatic framework AutoLEX that aims to
ease linguists' discovery and extraction of concise descriptions of linguistic
phenomena. Specifically, we apply this framework to extract descriptions for
three phenomena: morphological agreement, case marking, and word order, across
several languages. We evaluate the descriptions with the help of language
experts and propose a method for automated evaluation when human evaluation is
infeasible."
OTExtSum: Extractive Text Summarisation with Optimal Transport,0.864665,"Extractive text summarisation aims to select salient sentences from a
document to form a short yet informative summary. While learning-based methods
have achieved promising results, they have several limitations, such as
dependence on expensive training and lack of interpretability. Therefore, in
this paper, we propose a novel non-learning-based method by for the first time
formulating text summarisation as an Optimal Transport (OT) problem, namely
Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction
is conceptualised as obtaining an optimal summary that minimises the
transportation cost to a given document regarding their semantic distributions.
Such a cost is defined by the Wasserstein distance and used to measure the
summary's semantic coverage of the original document. Comprehensive experiments
on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and
CNN/DM demonstrate that our proposed method outperforms the state-of-the-art
non-learning-based methods and several recent learning-based methods in terms
of the ROUGE metric."
BOREx: Bayesian-Optimization--Based Refinement of Saliency Map for Image- and Video-Classification Models,0.864665,"Explaining a classification result produced by an image- and
video-classification model is one of the important but challenging issues in
computer vision. Many methods have been proposed for producing heat-map--based
explanations for this purpose, including ones based on the white-box approach
that uses the internal information of a model (e.g., LRP, Grad-CAM, and
Grad-CAM++) and ones based on the black-box approach that does not use any
internal information (e.g., LIME, SHAP, and RISE). We propose a new black-box
method BOREx (Bayesian Optimization for Refinement of visual model Explanation)
to refine a heat map produced by any method. Our observation is that a
heat-map--based explanation can be seen as a prior for an explanation method
based on Bayesian optimization. Based on this observation, BOREx conducts
Gaussian process regression (GPR) to estimate the saliency of each pixel in a
given image starting from the one produced by another explanation method. Our
experiments statistically demonstrate that the refinement by BOREx improves
low-quality heat maps for image- and video-classification results."
On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation,0.787022,"In recent years, pre-trained models have become dominant in most natural
language processing (NLP) tasks. However, in the area of Automated Essay
Scoring (AES), pre-trained models such as BERT have not been properly used to
outperform other deep learning models such as LSTM. In this paper, we introduce
a novel multi-scale essay representation for BERT that can be jointly learned.
We also employ multiple losses and transfer learning from out-of-domain essays
to further improve the performance. Experiment results show that our approach
derives much benefit from joint learning of multi-scale essay representation
and obtains almost the state-of-the-art result among all deep learning models
in the ASAP task. Our multi-scale essay representation also generalizes well to
CommonLit Readability Prize data set, which suggests that the novel text
representation proposed in this paper may be a new and effective choice for
long-text tasks."
PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models,0.864665,"Generalizable 3D part segmentation is important but challenging in vision and
robotics. Training deep models via conventional supervised methods requires
large-scale 3D datasets with fine-grained part annotations, which are costly to
collect. This paper explores an alternative way for low-shot part segmentation
of 3D point clouds by leveraging a pretrained image-language model, GLIP, which
achieves superior performance on open-vocabulary 2D detection. We transfer the
rich knowledge from 2D to 3D through GLIP-based part detection on point cloud
rendering and a novel 2D-to-3D label lifting algorithm. We also utilize
multi-view 3D priors and few-shot prompt tuning to boost performance
significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets
shows that our method enables excellent zero-shot 3D part segmentation. Our
few-shot version not only outperforms existing few-shot approaches by a large
margin but also achieves highly competitive results compared to the fully
supervised counterpart. Furthermore, we demonstrate that our method can be
directly applied to iPhone-scanned point clouds without significant domain
gaps."
"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink",0.862472,"Machine Learning (ML) workloads have rapidly grown in importance, but raised
concerns about their carbon footprint. Four best practices can reduce ML
training energy by up to 100x and CO2 emissions up to 1000x. By following best
practices, overall ML energy use (across research, development, and production)
held steady at <15% of Google's total energy use for the past three years. If
the whole ML field were to adopt best practices, total carbon emissions from
training would reduce. Hence, we recommend that ML papers include emissions
explicitly to foster competition on more than just model quality. Estimates of
emissions in papers that omitted them have been off 100x-100,000x, so
publishing emissions has the added benefit of ensuring accurate accounting.
Given the importance of climate change, we must get the numbers right to make
certain that we work on its biggest challenges."
TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers,0.864665,"Mixup is a commonly adopted data augmentation technique for image
classification. Recent advances in mixup methods primarily focus on mixing
based on saliency. However, many saliency detectors require intense computation
and are especially burdensome for parameter-heavy transformer models. To this
end, we propose TokenMixup, an efficient attention-guided token-level data
augmentation method that aims to maximize the saliency of a mixed set of
tokens. TokenMixup provides x15 faster saliency-aware data augmentation
compared to gradient-based methods. Moreover, we introduce a variant of
TokenMixup which mixes tokens within a single instance, thereby enabling
multi-scale feature augmentation. Experiments show that our methods
significantly improve the baseline models' performance on CIFAR and
ImageNet-1K, while being more efficient than previous methods. We also reach
state-of-the-art performance on CIFAR-100 among from-scratch transformer
models. Code is available at https://github.com/mlvlab/TokenMixup."
"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",0.841303,"Pretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations."
CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,0.864665,"Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities."
REGTR: End-to-end Point Cloud Correspondences with Transformers,0.79298,"Despite recent success in incorporating learning into point cloud
registration, many works focus on learning feature descriptors and continue to
rely on nearest-neighbor feature matching and outlier filtering through RANSAC
to obtain the final set of correspondences for pose estimation. In this work,
we conjecture that attention mechanisms can replace the role of explicit
feature matching and RANSAC, and thus propose an end-to-end framework to
directly predict the final set of correspondences. We use a network
architecture consisting primarily of transformer layers containing self and
cross attentions, and train it to predict the probability each point lies in
the overlapping region and its corresponding position in the other point cloud.
The required rigid transformation can then be estimated directly from the
predicted correspondences without further post-processing. Despite its
simplicity, our approach achieves state-of-the-art performance on 3DMatch and
ModelNet benchmarks. Our source code can be found at
https://github.com/yewzijian/RegTR ."
HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,0.827891,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex label hierarchy. Recently, the
pretrained language models (PLM)have been widely adopted in HTC through a
fine-tuning paradigm. However, in this paradigm, there exists a huge gap
between the classification tasks with sophisticated label hierarchy and the
masked language model (MLM) pretraining tasks of PLMs and thus the potentials
of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose
HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label
MLM perspective. Specifically, we construct a dynamic virtual template and
label words that take the form of soft prompts to fuse the label hierarchy
knowledge and introduce a zero-bounded multi-label cross entropy loss to
harmonize the objectives of HTC and MLM. Extensive experiments show HPT
achieves state-of-the-art performances on 3 popular HTC datasets and is adept
at handling the imbalance and low resource situations. Our code is available at
https://github.com/wzh9969/HPT."
RelTR: Relation Transformer for Scene Graph Generation,0.821438,"Different objects in the same scene are more or less related to each other,
but only a limited number of these relationships are noteworthy. Inspired by
DETR, which excels in object detection, we view scene graph generation as a set
prediction problem and propose an end-to-end scene graph generation model RelTR
which has an encoder-decoder architecture. The encoder reasons about the visual
feature context while the decoder infers a fixed-size set of triplets
subject-predicate-object using different types of attention mechanisms with
coupled subject and object queries. We design a set prediction loss performing
the matching between the ground truth and predicted triplets for the end-to-end
training. In contrast to most existing scene graph generation methods, RelTR is
a one-stage method that predicts a set of relationships directly only using
visual appearance without combining entities and labeling all possible
predicates. Extensive experiments on the Visual Genome and Open Images V6
datasets demonstrate the superior performance and fast inference of our model."
EfficientNeRF: Efficient Neural Radiance Fields,0.864665,"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications."
KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,0.864665,"Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets. Our implementation and pretrained checkpoints are released
at https://github.com/chijames/KERPLE.git."
DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,0.793902,"Language-guided Embodied AI benchmarks requiring an agent to navigate an
environment and manipulate objects typically allow one-way communication: the
human user gives a natural language command to the agent, and the agent can
only follow the command passively. We present DialFRED, a dialogue-enabled
embodied instruction following benchmark based on the ALFRED benchmark.
DialFRED allows an agent to actively ask questions to the human user; the
additional information in the user's response is used by the agent to better
complete its task. We release a human-annotated dataset with 53K task-relevant
questions and answers and an oracle to answer questions. To solve DialFRED, we
propose a questioner-performer framework wherein the questioner is pre-trained
with the human-annotated data and fine-tuned with reinforcement learning. We
make DialFRED publicly available and encourage researchers to propose and
evaluate their solutions to building dialog-enabled embodied agents."
Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic Process,0.864665,"We introduce FiLex, a self-reinforcing stochastic process which models finite
lexicons in emergent language experiments. The central property of FiLex is
that it is a self-reinforcing process, parallel to the intuition that the more
a word is used in a language, the more its use will continue. As a theoretical
model, FiLex serves as a way to both explain and predict the behavior of the
emergent language system. We empirically test FiLex's ability to capture the
relationship between the emergent language's hyperparameters and the lexicon's
Shannon entropy."
SafeText: A Benchmark for Exploring Physical Safety in Language Models,0.864665,"Understanding what constitutes safe text is an important issue in natural
language processing and can often prevent the deployment of models deemed
harmful and unsafe. One such type of safety that has been scarcely studied is
commonsense physical safety, i.e. text that is not explicitly violent and
requires additional commonsense knowledge to comprehend that it leads to
physical harm. We create the first benchmark dataset, SafeText, comprising
real-life scenarios with paired safe and physically unsafe pieces of advice. We
utilize SafeText to empirically study commonsense physical safety across
various models designed for text generation and commonsense reasoning tasks. We
find that state-of-the-art large language models are susceptible to the
generation of unsafe text and have difficulty rejecting unsafe advice. As a
result, we argue for further studies of safety and the assessment of
commonsense physical safety in models before release."
Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control,0.864665,"Pretrained language models have demonstrated extraordinary capabilities in
language generation. However, real-world tasks often require controlling the
distribution of generated text in order to mitigate bias, promote fairness, and
achieve personalization. Existing techniques for controlling the distribution
of generated text only work with quantified distributions, which require
pre-defined categories, proportions of the distribution, or an existing corpus
following the desired distributions. However, many important distributions,
such as personal preferences, are unquantified. In this work, we tackle the
problem of generating text following arbitrary distributions (quantified and
unquantified) by proposing Nano, a few-shot human-in-the-loop training
algorithm that continuously learns from human feedback. Nano achieves
state-of-the-art results on single topic/attribute as well as quantified
distribution control compared to previous works. We also show that Nano is able
to learn unquantified distributions, achieves personalization, and captures
differences between different individuals' personal preferences with high
sample efficiency."
The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts,0.781799,"Classroom discourse is a core medium of instruction - analyzing it can
provide a window into teaching and learning as well as driving the development
of new tools for improving instruction. We introduce the largest dataset of
mathematics classroom transcripts available to researchers, and demonstrate how
this data can help improve instruction. The dataset consists of 1,660 45-60
minute long 4th and 5th grade elementary mathematics observations collected by
the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The
anonymized transcripts represent data from 317 teachers across 4 school
districts that serve largely historically marginalized students. The
transcripts come with rich metadata, including turn-level annotations for
dialogic discourse moves, classroom observation scores, demographic
information, survey responses and student test scores. We demonstrate that our
natural language processing model, trained on our turn-level annotations, can
learn to identify dialogic discourse moves and these moves are correlated with
better classroom observation scores and learning outcomes. This dataset opens
up several possibilities for researchers, educators and policymakers to learn
about and improve K-12 instruction. The dataset can be found at
https://github.com/ddemszky/classroom-transcript-analysis."
XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence,0.864665,"Recent advances in machine learning have significantly improved the
understanding of source code data and achieved good performance on a number of
downstream tasks. Open source repositories like GitHub enable this process with
rich unlabeled code data. However, the lack of high quality labeled data has
largely hindered the progress of several code related tasks, such as program
translation, summarization, synthesis, and code search. This paper introduces
XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for
cross-lingual code intelligence. Our dataset contains fine-grained parallel
data from 8 languages (7 commonly used programming languages and English), and
supports 10 cross-lingual code tasks. To the best of our knowledge, it is the
largest parallel dataset for source code both in terms of size and the number
of languages. We also provide the performance of several state-of-the-art
baseline models for each task. We believe this new dataset can be a valuable
asset for the research community and facilitate the development and validation
of new methods for cross-lingual code intelligence."
V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer,0.827607,"In this paper, we investigate the application of Vehicle-to-Everything (V2X)
communication to improve the perception performance of autonomous vehicles. We
present a robust cooperative perception framework with V2X communication using
a novel vision Transformer. Specifically, we build a holistic attention model,
namely V2X-ViT, to effectively fuse information across on-road agents (i.e.,
vehicles and infrastructure). V2X-ViT consists of alternating layers of
heterogeneous multi-agent self-attention and multi-scale window self-attention,
which captures inter-agent interaction and per-agent spatial relationships.
These key modules are designed in a unified Transformer architecture to handle
common V2X challenges, including asynchronous information sharing, pose errors,
and heterogeneity of V2X components. To validate our approach, we create a
large-scale V2X perception dataset using CARLA and OpenCDA. Extensive
experimental results demonstrate that V2X-ViT sets new state-of-the-art
performance for 3D object detection and achieves robust performance even under
harsh, noisy environments. The code is available at
https://github.com/DerrickXuNu/v2x-vit."
Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models,0.817992,"Motivations for methods in explainable artificial intelligence (XAI) often
include detecting, quantifying and mitigating bias, and contributing to making
machine learning models fairer. However, exactly how an XAI method can help in
combating biases is often left unspecified. In this paper, we briefly review
trends in explainability and fairness in NLP research, identify the current
practices in which explainability methods are applied to detect and mitigate
bias, and investigate the barriers preventing XAI methods from being used more
widely in tackling fairness issues."
FEAT: Face Editing with Attention,0.796232,"Employing the latent space of pretrained generators has recently been shown
to be an effective means for GAN-based face manipulation. The success of this
approach heavily relies on the innate disentanglement of the latent space axes
of the generator. However, face manipulation often intends to affect local
regions only, while common generators do not tend to have the necessary spatial
disentanglement. In this paper, we build on the StyleGAN generator, and present
a method that explicitly encourages face manipulation to focus on the intended
regions by incorporating learned attention maps. During the generation of the
edited image, the attention map serves as a mask that guides a blending between
the original features and the modified ones. The guidance for the latent space
edits is achieved by employing CLIP, which has recently been shown to be
effective for text-driven edits. We perform extensive experiments and show that
our method can perform disentangled and controllable face manipulations based
on text descriptions by attending to the relevant regions only. Both
qualitative and quantitative experimental results demonstrate the superiority
of our method for facial region editing over alternative methods."
"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",0.782825,"The formalization of existing mathematical proofs is a notoriously difficult
process. Despite decades of research on automation and proof assistants,
writing formal proofs remains arduous and only accessible to a few experts.
While previous studies to automate formalization focused on powerful search
algorithms, no attempts were made to take advantage of available informal
proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method
that maps informal proofs to formal proof sketches, and uses the sketches to
guide an automated prover by directing its search to easier sub-problems. We
investigate two relevant setups where informal proofs are either written by
humans or generated by a language model. Our experiments and ablation studies
show that large language models are able to produce well-structured formal
sketches that follow the same reasoning steps as the informal proofs. Guiding
an automated prover with these sketches enhances its performance from 20.9% to
39.3% on a collection of mathematical competition problems."
MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model,0.869449,"Human motion modeling is important for many modern graphics applications,
which typically require professional skills. In order to remove the skill
barriers for laymen, recent motion generation methods can directly generate
human motions conditioned on natural languages. However, it remains challenging
to achieve diverse and fine-grained motion generation with various text inputs.
To address this problem, we propose MotionDiffuse, the first diffusion
model-based text-driven motion generation framework, which demonstrates several
desired properties over existing methods. 1) Probabilistic Mapping. Instead of
a deterministic language-motion mapping, MotionDiffuse generates motions
through a series of denoising steps in which variations are injected. 2)
Realistic Synthesis. MotionDiffuse excels at modeling complicated data
distribution and generating vivid motion sequences. 3) Multi-Level
Manipulation. MotionDiffuse responds to fine-grained instructions on body
parts, and arbitrary-length motion synthesis with time-varied text prompts. Our
experiments show MotionDiffuse outperforms existing SoTA methods by convincing
margins on text-driven motion generation and action-conditioned motion
generation. A qualitative analysis further demonstrates MotionDiffuse's
controllability for comprehensive motion generation. Homepage:
https://mingyuan-zhang.github.io/projects/MotionDiffuse.html"
BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment,0.778042,"This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge."
"Pruned RNN-T for fast, memory-efficient ASR training",0.856165,"The RNN-Transducer (RNN-T) framework for speech recognition has been growing
in popularity, particularly for deployed real-time ASR systems, because it
combines high accuracy with naturally streaming recognition. One of the
drawbacks of RNN-T is that its loss function is relatively slow to compute, and
can use a lot of memory. Excessive GPU memory usage can make it impractical to
use RNN-T loss in cases where the vocabulary size is large: for example, for
Chinese character-based ASR. We introduce a method for faster and more
memory-efficient RNN-T loss computation. We first obtain pruning bounds for the
RNN-T recursion using a simple joiner network that is linear in the encoder and
decoder embeddings; we can evaluate this without using much memory. We then use
those pruning bounds to evaluate the full, non-linear joiner network."
Neural Neighbor Style Transfer,0.844796,"We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers
state-of-the-art quality, generalization, and competitive efficiency for
artistic style transfer. Our approach is based on explicitly replacing neural
features extracted from the content input (to be stylized) with those from a
style exemplar, then synthesizing the final output based on these rearranged
features. While the spirit of our approach is similar to prior work, we show
that our design decisions dramatically improve the final visual quality."
TEMPERA: Test-Time Prompting via Reinforcement Learning,0.83425,"Careful prompt design is critical to the use of large language models in
zero-shot or few-shot learning. As a consequence, there is a growing interest
in automated methods to design optimal prompts. In this work, we propose
Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to
prior prompt generation methods, TEMPERA can efficiently leverage prior
knowledge, is adaptive to different queries and provides an interpretable
prompt for every query. To achieve this, we design a novel action space that
allows flexible editing of the initial prompts covering a wide set of
commonly-used components like instructions, few-shot exemplars, and
verbalizers. The proposed method achieves significant gains compared with
recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a
variety of tasks including sentiment analysis, topic classification, natural
language inference, and reading comprehension. Our method achieves 5.33x on
average improvement in sample efficiency when compared to the traditional
fine-tuning methods."
Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds,0.830527,"Despite impressive successes, deep reinforcement learning (RL) systems still
fall short of human performance on generalization to new tasks and environments
that differ from their training. As a benchmark tailored for studying RL
generalization, we introduce Avalon, a set of tasks in which embodied agents in
highly diverse procedural 3D worlds must survive by navigating terrain, hunting
or gathering food, and avoiding hazards. Avalon is unique among existing RL
benchmarks in that the reward function, world dynamics, and action space are
the same for every task, with tasks differentiated solely by altering the
environment; its 20 tasks, ranging in complexity from eat and throw to hunt and
navigate, each create worlds in which the agent must perform specific skills in
order to survive. This setup enables investigations of generalization within
tasks, between tasks, and to compositional tasks that require combining skills
learned from previous tasks. Avalon includes a highly efficient simulator, a
library of baselines, and a benchmark with scoring metrics evaluated against
hundreds of hours of human performance, all of which are open-source and
publicly available. We find that standard RL baselines make progress on most
tasks but are still far from human performance, suggesting Avalon is
challenging enough to advance the quest for generalizable RL."
EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain,0.868408,"Existing summarization datasets come with two main drawbacks: (1) They tend
to focus on overly exposed domains, such as news articles or wiki-like texts,
and (2) are primarily monolingual, with few multilingual datasets. In this
work, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated
document summaries of legal acts from the European Union law platform
(EUR-Lex). Documents and their respective summaries exist as cross-lingual
paragraph-aligned data in several of the 24 official European languages,
enabling access to various cross-lingual and lower-resourced summarization
setups. We obtain up to 1,500 document/summary pairs per language, including a
subset of 375 cross-lingually aligned legal acts with texts available in all 24
languages. In this work, the data acquisition process is detailed and key
characteristics of the resource are compared to existing summarization
resources. In particular, we illustrate challenging sub-problems and open
questions on the dataset that could help the facilitation of future research in
the direction of domain-specific cross-lingual summarization. Limited by the
extreme length and language diversity of samples, we further conduct
experiments with suitable extractive monolingual and cross-lingual baselines
for future work. Code for the extraction as well as access to our data and
baselines is available online at: https://github.com/achouhan93/eur-lex-sum."
3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows,0.880469,"Text-to-image AI are capable of generating novel images for inspiration, but
their applications for 3D design workflows and how designers can build 3D
models using AI-provided inspiration have not yet been explored. To investigate
this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a
plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users
to construct text and image prompts based on what they are modeling. In a study
with 13 designers, we found that designers saw great potential in 3DALL-E
within their workflows and could use text-to-image AI to produce reference
images, prevent design fixation, and inspire design considerations. We
elaborate on prompting patterns observed across 3D modeling tasks and provide
measures of prompt complexity observed across participants. From our findings,
we discuss how 3DALL-E can merge with existing generative design workflows and
propose prompt bibliographies as a form of human-AI design history."
EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing,0.864665,"The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP)."
IMPaSh: A Novel Domain-shift Resistant Representation for Colorectal Cancer Tissue Classification,0.864665,"The appearance of histopathology images depends on tissue type, staining and
digitization procedure. These vary from source to source and are the potential
causes for domain-shift problems. Owing to this problem, despite the great
success of deep learning models in computational pathology, a model trained on
a specific domain may still perform sub-optimally when we apply them to another
domain. To overcome this, we propose a new augmentation called PatchShuffling
and a novel self-supervised contrastive learning framework named IMPaSh for
pre-training deep learning models. Using these, we obtained a ResNet50 encoder
that can extract image representation resistant to domain-shift. We compared
our derived representation against those acquired based on other
domain-generalization techniques by using them for the cross-domain
classification of colorectal tissue images. We show that the proposed method
outperforms other traditional histology domain-adaptation and state-of-the-art
self-supervised learning methods. Code is available at:
https://github.com/trinhvg/IMPash ."
Lipschitz-constrained Unsupervised Skill Discovery,0.864665,"We study the problem of unsupervised skill discovery, whose goal is to learn
a set of diverse and useful skills with no external reward. There have been a
number of skill discovery methods based on maximizing the mutual information
(MI) between skills and states. However, we point out that their MI objectives
usually prefer static skills to dynamic ones, which may hinder the application
for downstream tasks. To address this issue, we propose Lipschitz-constrained
Skill Discovery (LSD), which encourages the agent to discover more diverse,
dynamic, and far-reaching skills. Another benefit of LSD is that its learned
representation function can be utilized for solving goal-following downstream
tasks even in a zero-shot manner - i.e., without further training or complex
planning. Through experiments on various MuJoCo robotic locomotion and
manipulation environments, we demonstrate that LSD outperforms previous
approaches in terms of skill diversity, state space coverage, and performance
on seven downstream tasks including the challenging task of following multiple
goals on Humanoid. Our code and videos are available at
https://shpark.me/projects/lsd/."
PercentMatch: Percentile-based Dynamic Thresholding for Multi-Label Semi-Supervised Classification,0.864665,"While much of recent study in semi-supervised learning (SSL) has achieved
strong performance on single-label classification problems, an equally
important yet underexplored problem is how to leverage the advantage of
unlabeled data in multi-label classification tasks. To extend the success of
SSL to multi-label classification, we first analyze with illustrative examples
to get some intuition about the extra challenges exist in multi-label
classification. Based on the analysis, we then propose PercentMatch, a
percentile-based threshold adjusting scheme, to dynamically alter the score
thresholds of positive and negative pseudo-labels for each class during the
training, as well as dynamic unlabeled loss weights that further reduces noise
from early-stage unlabeled predictions. Without loss of simplicity, we achieve
strong performance on Pascal VOC2007 and MS-COCO datasets when compared to
recent SSL methods."
Automatic Semantic Modeling for Structural Data Source with the Prior Knowledge from Knowledge Base,0.864665,"A critical step in sharing semantic content online is to map the structural
data source to a public domain ontology. This problem is denoted as the
Relational-To-Ontology Mapping Problem (Rel2Onto). A huge effort and expertise
are required for manually modeling the semantics of data. Therefore, an
automatic approach for learning the semantics of a data source is desirable.
Most of the existing work studies the semantic annotation of source attributes.
However, although critical, the research for automatically inferring the
relationships between attributes is very limited. In this paper, we propose a
novel method for semantically annotating structured data sources using machine
learning, graph matching and modified frequent subgraph mining to amend the
candidate model. In our work, Knowledge graph is used as prior knowledge. Our
evaluation shows that our approach outperforms two state-of-the-art solutions
in tricky cases where only a few semantic models are known."
TVLT: Textless Vision-Language Transformer,0.831402,"In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT"
Learning to Act with Affordance-Aware Multimodal Neural SLAM,0.864665,"Recent years have witnessed an emerging paradigm shift toward embodied
artificial intelligence, in which an agent must learn to solve challenging
tasks by interacting with its environment. There are several challenges in
solving embodied multimodal tasks, including long-horizon planning,
vision-and-language grounding, and efficient exploration. We focus on a
critical bottleneck, namely the performance of planning and navigation. To
tackle this challenge, we propose a Neural SLAM approach that, for the first
time, utilizes several modalities for exploration, predicts an affordance-aware
semantic map, and plans over it at the same time. This significantly improves
exploration efficiency, leads to robust long-horizon planning, and enables
effective vision-and-language grounding. With the proposed Affordance-aware
Multimodal Neural SLAM (AMSLAM) approach, we obtain more than 40% improvement
over prior published work on the ALFRED benchmark and set a new
state-of-the-art generalization performance at a success rate of 23.48% on the
test unseen scenes."
Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion,0.851609,"Digital art synthesis is receiving increasing attention in the multimedia
community because of engaging the public with art effectively. Current digital
art synthesis methods usually use single-modality inputs as guidance, thereby
limiting the expressiveness of the model and the diversity of generated
results. To solve this problem, we propose the multimodal guided artwork
diffusion (MGAD) model, which is a diffusion-based digital artwork generation
approach that utilizes multimodal prompts as guidance to control the
classifier-free diffusion model. Additionally, the contrastive language-image
pretraining (CLIP) model is used to unify text and image modalities. Extensive
experimental results on the quality and quantity of the generated digital art
paintings confirm the effectiveness of the combination of the diffusion model
and multimodal guidance. Code is available at
https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion."
DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,0.837716,"Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks."
Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,0.83482,"Prompting-based large language models (LLMs) are surprisingly powerful at
generating natural language reasoning steps or Chains-of-Thoughts (CoT) for
multi-step question answering (QA). They struggle, however, when the necessary
knowledge is either unavailable to the LLM or not up-to-date within its
parameters. While using the question to retrieve relevant text from an external
knowledge source helps LLMs, we observe that this one-step retrieve-and-read
approach is insufficient for multi-step QA. Here, \textit{what to retrieve}
depends on \textit{what has already been derived}, which in turn may depend on
\textit{what was previously retrieved}. To address this, we propose IRCoT, a
new approach for multi-step QA that interleaves retrieval with steps
(sentences) in a CoT, guiding the retrieval with CoT and in turn using
retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves
retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four
datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar
substantial gains in out-of-distribution (OOD) settings as well as with much
smaller models such as Flan-T5-large without additional training. IRCoT reduces
model hallucination, resulting in factually more accurate CoT reasoning. Code,
data, and prompts are available at \url{https://github.com/stonybrooknlp/ircot}"
Conformal Risk Control,0.887906,"We extend conformal prediction to control the expected value of any monotone
loss function. The algorithm generalizes split conformal prediction together
with its coverage guarantee. Like conformal prediction, the conformal risk
control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also
introduce extensions of the idea to distribution shift, quantile risk control,
multiple and adversarial risk control, and expectations of U-statistics. Worked
examples from computer vision and natural language processing demonstrate the
usage of our algorithm to bound the false negative rate, graph distance, and
token-level F1-score."
Teaching language models to support answers with verified quotes,0.864411,"Recent large language models often answer factual questions correctly. But
users can't trust any given claim a model makes without fact-checking, because
language models can hallucinate convincing nonsense. In this work we use
reinforcement learning from human preferences (RLHP) to train ""open-book"" QA
models that generate answers whilst also citing specific evidence for their
claims, which aids in the appraisal of correctness. Supporting evidence is
drawn from multiple documents found via a search engine, or from a single
user-provided document. Our 280 billion parameter model, GopherCite, is able to
produce answers with high quality supporting evidence and abstain from
answering when unsure. We measure the performance of GopherCite by conducting
human evaluation of answers to questions in a subset of the NaturalQuestions
and ELI5 datasets. The model's response is found to be high-quality 80\% of the
time on this Natural Questions subset, and 67\% of the time on the ELI5 subset.
Abstaining from the third of questions for which it is most unsure improves
performance to 90\% and 80\% respectively, approaching human baselines.
However, analysis on the adversarial TruthfulQA dataset shows why citation is
only one part of an overall strategy for safety and trustworthiness: not all
claims supported by evidence are true."
ezCoref: Towards Unifying Annotation Guidelines for Coreference Resolution,0.864665,"Large-scale, high-quality corpora are critical for advancing research in
coreference resolution. However, existing datasets vary in their definition of
coreferences and have been collected via complex and lengthy guidelines that
are curated for linguistic experts. These concerns have sparked a growing
interest among researchers to curate a unified set of guidelines suitable for
annotators with various backgrounds. In this work, we develop a
crowdsourcing-friendly coreference annotation methodology, ezCoref, consisting
of an annotation tool and an interactive tutorial. We use ezCoref to
re-annotate 240 passages from seven existing English coreference datasets
(spanning fiction, news, and multiple other domains) while teaching annotators
only cases that are treated similarly across these datasets. Surprisingly, we
find that reasonable quality annotations were already achievable (>90%
agreement between the crowd and expert annotations) even without extensive
training. On carefully analyzing the remaining disagreements, we identify the
presence of linguistic cases that our annotators unanimously agree upon but
lack unified treatments (e.g., generic pronouns, appositives) in existing
datasets. We propose the research community should revisit these phenomena when
curating future unified annotation guidelines."
PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions,0.832064,"Cross-entropy loss and focal loss are the most common choices when training
deep neural networks for classification problems. Generally speaking, however,
a good loss function can take on much more flexible forms, and should be
tailored for different tasks and datasets. Motivated by how functions can be
approximated via Taylor expansion, we propose a simple framework, named
PolyLoss, to view and design loss functions as a linear combination of
polynomial functions. Our PolyLoss allows the importance of different
polynomial bases to be easily adjusted depending on the targeting tasks and
datasets, while naturally subsuming the aforementioned cross-entropy loss and
focal loss as special cases. Extensive experimental results show that the
optimal choice within the PolyLoss is indeed dependent on the task and dataset.
Simply by introducing one extra hyperparameter and adding one line of code, our
Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D
image classification, instance segmentation, object detection, and 3D object
detection tasks, sometimes by a large margin."
RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization,0.817124,"6-DoF object pose estimation from a monocular image is challenging, and a
post-refinement procedure is generally needed for high-precision estimation. In
this paper, we propose a framework based on a recurrent neural network (RNN)
for object pose refinement, which is robust to erroneous initial poses and
occlusions. During the recurrent iterations, object pose refinement is
formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
enabling end-to-end training. The correspondence field estimation and pose
refinement are conducted alternatively in each iteration to recover the object
poses. Furthermore, to improve the robustness to occlusion, we introduce a
consistency-check mechanism based on the learned descriptors of the 3D model
and observed 2D images, which downweights the unreliable correspondences during
pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance."
InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges,0.884418,"In this report, we present our champion solutions to five tracks at Ego4D
challenge. We leverage our developed InternVideo, a video foundation model, for
five Ego4D tasks, including Moment Queries, Natural Language Queries, Future
Hand Prediction, State Change Object Detection, and Short-term Object
Interaction Anticipation. InternVideo-Ego4D is an effective paradigm to adapt
the strong foundation model to the downstream ego-centric video understanding
tasks with simple head designs. In these five tasks, the performance of
InternVideo-Ego4D comprehensively surpasses the baseline methods and the
champions of CVPR2022, demonstrating the powerful representation ability of
InternVideo as a video foundation model. Our code will be released at
https://github.com/OpenGVLab/ego4d-eccv2022-solutions"
"BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking",0.875486,"Estimating human motion from video is an active research area due to its many
potential applications. Most state-of-the-art methods predict human shape and
posture estimates for individual images and do not leverage the temporal
information available in video. Many ""in the wild"" sequences of human motion
are captured by a moving camera, which adds the complication of conflated
camera and human motion to the estimation. We therefore present BodySLAM, a
monocular SLAM system that jointly estimates the position, shape, and posture
of human bodies, as well as the camera trajectory. We also introduce a novel
human motion model to constrain sequential body postures and observe the scale
of the scene. Through a series of experiments on video sequences of human
motion captured by a moving monocular camera, we demonstrate that BodySLAM
improves estimates of all human body parameters and camera poses when compared
to estimating these separately."
Thin-Plate Spline Motion Model for Image Animation,0.840597,"Image animation brings life to the static object in the source image
according to the driving video. Recent works attempt to perform motion transfer
on arbitrary objects through unsupervised methods without using a priori
knowledge. However, it remains a significant challenge for current unsupervised
methods when there is a large pose gap between the objects in the source and
driving images. In this paper, a new end-to-end unsupervised motion transfer
framework is proposed to overcome such issue. Firstly, we propose thin-plate
spline motion estimation to produce a more flexible optical flow, which warps
the feature maps of the source image to the feature domain of the driving
image. Secondly, in order to restore the missing regions more realistically, we
leverage multi-resolution occlusion masks to achieve more effective feature
fusion. Finally, additional auxiliary loss functions are designed to ensure
that there is a clear division of labor in the network modules, encouraging the
network to generate high-quality images. Our method can animate a variety of
objects, including talking faces, human bodies, and pixel animations.
Experiments demonstrate that our method performs better on most benchmarks than
the state of the art with visible improvements in pose-related metrics."
Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,0.861396,"Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys."
Can language models learn from explanations in context?,0.781143,"Language Models (LMs) can perform new tasks by adapting to a few in-context
examples. For humans, explanations that connect examples to task principles can
improve learning. We therefore investigate whether explanations of few-shot
examples can help LMs. We annotate questions from 40 challenging tasks with
answer explanations, and various matched control explanations. We evaluate how
different types of explanations, instructions, and controls affect zero- and
few-shot performance. We analyze these results using statistical multilevel
modeling techniques that account for the nested dependencies among conditions,
tasks, prompts, and models. We find that explanations can improve performance
-- even without tuning. Furthermore, explanations hand-tuned for performance on
a small validation set offer substantially larger benefits, and building a
prompt by selecting examples and explanations together substantially improves
performance over selecting examples alone. Finally, even untuned explanations
outperform carefully matched controls, suggesting that the benefits are due to
the link between an example and its explanation, rather than lower-level
features. However, only large models benefit. In summary, explanations can
support the in-context learning of large LMs on challenging tasks."
Czech Grammar Error Correction with a Large and Diverse Corpus,0.838224,"We introduce a large and diverse Czech corpus annotated for grammatical error
correction (GEC) with the aim to contribute to the still scarce data resources
in this domain for languages other than English. The Grammar Error Correction
Corpus for Czech (GECCC) offers a variety of four domains, covering error
distributions ranging from high error density essays written by non-native
speakers, to website texts, where errors are expected to be much less common.
We compare several Czech GEC systems, including several Transformer-based ones,
setting a strong baseline to future research. Finally, we meta-evaluate common
GEC metrics against human judgements on our data. We make the new Czech GEC
corpus publicly available under the CC BY-SA 4.0 license at
http://hdl.handle.net/11234/1-4639 ."
LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension,0.864665,"The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension."
SCAMPS: Synthetics for Camera Measurement of Physiological Signals,0.818984,"The use of cameras and computational algorithms for noninvasive, low-cost and
scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs
is very attractive. However, diverse data representing a range of environments,
body motions, illumination conditions and physiological states is laborious,
time consuming and expensive to obtain. Synthetic data have proven a valuable
tool in several areas of machine learning, yet are not widely available for
camera measurement of physiological states. Synthetic data offer ""perfect""
labels (e.g., without noise and with precise synchronization), labels that may
not be possible to obtain otherwise (e.g., precise pixel level segmentation
maps) and provide a high degree of control over variation and diversity in the
dataset. We present SCAMPS, a dataset of synthetics containing 2,800 videos
(1.68M frames) with aligned cardiac and respiratory signals and facial action
intensities. The RGB frames are provided alongside segmentation maps. We
provide precise descriptive statistics about the underlying waveforms,
including inter-beat interval, heart rate variability, and pulse arrival time.
Finally, we present baseline results training on these synthetic data and
testing on real-world datasets to illustrate generalizability."
"When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues",0.875936,"Indirect speech such as sarcasm achieves a constellation of discourse goals
in human communication. While the indirectness of figurative language warrants
speakers to achieve certain pragmatic goals, it is challenging for AI agents to
comprehend such idiosyncrasies of human communication. Though sarcasm
identification has been a well-explored topic in dialogue analysis, for
conversational systems to truly grasp a conversation's innate meaning and
generate appropriate responses, simply detecting sarcasm is not enough; it is
vital to explain its underlying sarcastic connotation to capture its true
essence. In this work, we study the discourse structure of sarcastic
conversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).
Set in a multimodal and code-mixed setting, the task aims to generate natural
language explanations of satirical conversations. To this end, we curate WITS,
a new dataset to support our task. We propose MAF (Modality Aware Fusion), a
multimodal context-aware attention and global information fusion module to
capture multimodality and use it to benchmark WITS. The proposed attention
module surpasses the traditional multimodal fusion baselines and reports the
best performance on almost all metrics. Lastly, we carry out detailed analyses
both quantitatively and qualitatively."
GRiT: A Generative Region-to-text Transformer for Object Understanding,0.781912,"This paper presents a Generative RegIon-to-Text transformer, GRiT, for object
understanding. The spirit of GRiT is to formulate object understanding as
<region, text> pairs, where region locates objects and text describes objects.
For example, the text in object detection denotes class names while that in
dense captioning refers to descriptive sentences. Specifically, GRiT consists
of a visual encoder to extract image features, a foreground object extractor to
localize objects, and a text decoder to generate open-set object descriptions.
With the same model architecture, GRiT can understand objects via not only
simple nouns, but also rich descriptive sentences including object attributes
or actions. Experimentally, we apply GRiT to object detection and dense
captioning tasks. GRiT achieves 60.4 AP on COCO 2017 test-dev for object
detection and 15.5 mAP on Visual Genome for dense captioning. Code is available
at https://github.com/JialianW/GRiT"
When Is Partially Observable Reinforcement Learning Not Scary?,0.824867,"Applications of Reinforcement Learning (RL), in which agents learn to make a
sequence of decisions despite lacking complete information about the latent
states of the controlled system, that is, they act under partial observability
of the states, are ubiquitous. Partially observable RL can be notoriously
difficult -- well-known information-theoretic results show that learning
partially observable Markov decision processes (POMDPs) requires an exponential
number of samples in the worst case. Yet, this does not rule out the existence
of large subclasses of POMDPs over which learning is tractable.
  In this paper we identify such a subclass, which we call weakly revealing
POMDPs. This family rules out the pathological instances of POMDPs where
observations are uninformative to a degree that makes learning hard. We prove
that for weakly revealing POMDPs, a simple algorithm combining optimism and
Maximum Likelihood Estimation (MLE) is sufficient to guarantee polynomial
sample complexity. To the best of our knowledge, this is the first provably
sample-efficient result for learning from interactions in overcomplete POMDPs,
where the number of latent states can be larger than the number of
observations."
The Importance of Credo in Multiagent Learning,0.781109,"We propose a model for multi-objective optimization, a credo, for agents in a
system that are configured into multiple groups (i.e., teams). Our model of
credo regulates how agents optimize their behavior for the groups they belong
to. We evaluate credo in the context of challenging social dilemmas with
reinforcement learning agents. Our results indicate that the interests of
teammates, or the entire system, are not required to be fully aligned for
achieving globally beneficial outcomes. We identify two scenarios without full
common interest that achieve high equality and significantly higher mean
population rewards compared to when the interests of all agents are aligned."
DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models,0.866561,"We study the way DALLE-2 maps symbols (words) in the prompt to their
references (entities or properties of entities in the generated image). We show
that in stark contrast to the way human process language, DALLE-2 does not
follow the constraint that each word has a single role in the interpretation,
and sometimes re-use the same symbol for different purposes. We collect a set
of stimuli that reflect the phenomenon: we show that DALLE-2 depicts both
senses of nouns with multiple senses at once; and that a given word can modify
the properties of two distinct entities in the image, or can be depicted as one
object and also modify the properties of another object, creating a semantic
leakage of properties between entities. Taken together, our study highlights
the differences between DALLE-2 and human language processing and opens an
avenue for future study on the inductive biases of text-to-image models."
DDNeRF: Depth Distribution Neural Radiance Fields,0.864665,"In recent years, the field of implicit neural representation has progressed
significantly. Models such as neural radiance fields (NeRF), which uses
relatively small neural networks, can represent high-quality scenes and achieve
state-of-the-art results for novel view synthesis. Training these types of
networks, however, is still computationally very expensive. We present depth
distribution neural radiance field (DDNeRF), a new method that significantly
increases sampling efficiency along rays during training while achieving
superior results for a given sampling budget. DDNeRF achieves this by learning
a more accurate representation of the density distribution along rays. More
specifically, we train a coarse model to predict the internal distribution of
the transparency of an input volume in addition to the volume's total density.
This finer distribution then guides the sampling procedure of the fine model.
This method allows us to use fewer samples during training while reducing
computational resources."
AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks,0.864665,"Transformer-based pre-trained models with millions of parameters require
large storage. Recent approaches tackle this shortcoming by training adapters,
but these approaches still require a relatively large number of parameters. In
this study, AdapterBias, a surprisingly simple yet effective adapter
architecture, is proposed. AdapterBias adds a token-dependent shift to the
hidden output of transformer layers to adapt to downstream tasks with only a
vector and a linear layer. Extensive experiments are conducted to demonstrate
the effectiveness of AdapterBias. The experiments show that our proposed method
can dramatically reduce the trainable parameters compared to the previous works
with a minimal decrease in task performances compared with fine-tuned
pre-trained models. We further find that AdapterBias automatically learns to
assign more significant representation shifts to the tokens related to the task
in consideration."
QuadTree Attention for Vision Transformers,0.809951,"Transformers have been successful in many vision tasks, thanks to their
capability of capturing long-range dependency. However, their quadratic
computational complexity poses a major obstacle for applying them to vision
tasks requiring dense predictions, such as object detection, feature matching,
stereo, etc. We introduce QuadTree Attention, which reduces the computational
complexity from quadratic to linear. Our quadtree transformer builds token
pyramids and computes attention in a coarse-to-fine manner. At each level, the
top K patches with the highest attention scores are selected, such that at the
next level, attention is only evaluated within the relevant regions
corresponding to these top K patches. We demonstrate that quadtree attention
achieves state-of-the-art performance in various vision tasks, e.g. with 4.0%
improvement in feature matching on ScanNet, about 50% flops reduction in stereo
matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,
1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on
semantic segmentation over previous state-of-the-art transformers. The codes
are available at https://github.com/Tangshitao/QuadtreeAttention."
HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crisis Response,0.811124,"Timely and effective response to humanitarian crises requires quick and
accurate analysis of large amounts of text data - a process that can highly
benefit from expert-assisted NLP systems trained on validated and annotated
data in the humanitarian response domain. To enable creation of such NLP
systems, we introduce and release HumSet, a novel and rich multilingual dataset
of humanitarian response documents annotated by experts in the humanitarian
response community. The dataset provides documents in three languages (English,
French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021
across the globe. For each document, HUMSET provides selected snippets
(entries) as well as assigned classes to each entry annotated using common
humanitarian information analysis frameworks. HUMSET also provides novel and
challenging entry extraction and multi-label entry classification tasks. In
this paper, we take a first step towards approaching these tasks and conduct a
set of experiments on Pre-trained Language Models (PLM) to establish strong
baselines for future research in this domain. The dataset is available at
https://blog.thedeep.io/humset/."
PoissonMat: Remodeling Matrix Factorization using Poisson Distribution and Solving the Cold Start Problem without Input Data,0.864665,"Matrix Factorization is one of the most successful recommender system
techniques over the past decade. However, the classic probabilistic theory
framework for matrix factorization is modeled using normal distributions. To
find better probabilistic models, algorithms such as RankMat, ZeroMat and
DotMat have been invented in recent years. In this paper, we model the user
rating behavior in recommender system as a Poisson process, and design an
algorithm that relies on no input data to solve the recommendation problem and
the cold start issue at the same time. We prove the superiority of our
algorithm in comparison with matrix factorization, random placement, Zipf
placement, ZeroMat, DotMat, etc."
FS6D: Few-Shot 6D Pose Estimation of Novel Objects,0.817971,"6D object pose estimation networks are limited in their capability to scale
to large numbers of object instances due to the close-set assumption and their
reliance on high-fidelity object CAD models. In this work, we study a new open
set problem; the few-shot 6D object poses estimation: estimating the 6D pose of
an unknown object by a few support views without extra training. To tackle the
problem, we point out the importance of fully exploring the appearance and
geometric relationship between the given support views and query scene patches
and propose a dense prototypes matching framework by extracting and matching
dense RGBD prototypes with transformers. Moreover, we show that the priors from
diverse appearances and shapes are crucial to the generalization capability
under the problem setting and thus propose a large-scale RGBD photorealistic
dataset (ShapeNet6D) for network pre-training. A simple and effective online
texture blending approach is also introduced to eliminate the domain gap from
the synthesis dataset, which enriches appearance diversity at a low cost.
Finally, we discuss possible solutions to this problem and establish benchmarks
on popular datasets to facilitate future research. The project page is at
\url{https://fs6d.github.io/}."
Effidit: Your AI Writing Assistant,0.864665,"In this technical report, we introduce Effidit (Efficient and Intelligent
Editing), a digital writing assistant that facilitates users to write
higher-quality text more efficiently by using artificial intelligence (AI)
technologies. Previous writing assistants typically provide the function of
error checking (to detect and correct spelling and grammatical errors) and
limited text-rewriting functionality. With the emergence of large-scale neural
language models, some systems support automatically completing a sentence or a
paragraph. In Effidit, we significantly expand the capacities of a writing
assistant by providing functions in five categories: text completion, error
checking, text polishing, keywords to sentences (K2S), and cloud input methods
(cloud IME). In the text completion category, Effidit supports generation-based
sentence completion, retrieval-based sentence completion, and phrase
completion. In contrast, many other writing assistants so far only provide one
or two of the three functions. For text polishing, we have three functions:
(context-aware) phrase polishing, sentence paraphrasing, and sentence
expansion, whereas many other writing assistants often support one or two
functions in this category. The main contents of this report include major
modules of Effidit, methods for implementing these modules, and evaluation
results of some key methods."
EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start,0.795868,"We present EdiT5 - a novel semi-autoregressive text-editing model designed to
combine the strengths of non-autoregressive text-editing and autoregressive
decoding. EdiT5 is faster during inference than conventional
sequence-to-sequence (seq2seq) models, while being capable of modelling
flexible input-output transformations.
  This is achieved by decomposing the generation process into three sub-tasks:
(1) tagging to decide on the subset of input tokens to be preserved in the
output, (2) re-ordering to define their order in the output text, and (3)
insertion to infill the missing tokens that are not present in the input. The
tagging and re-ordering steps, which are responsible for generating the largest
portion of the output, are non-autoregressive, while the insertion step uses an
autoregressive decoder.
  Depending on the task, EdiT5 on average requires significantly fewer
autoregressive steps, demonstrating speedups of up to 25x when compared to
seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5
checkpoint yielding comparable performance to T5 in high-resource settings when
evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,
and Decontextualization while clearly outperforming T5 in low-resource
settings."
GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from Diffusion Models,0.864665,"We propose AudioStyleGAN (ASGAN), a new generative adversarial network (GAN)
for unconditional speech synthesis. As in the StyleGAN family of image
synthesis models, ASGAN maps sampled noise to a disentangled latent vector
which is then mapped to a sequence of audio features so that signal aliasing is
suppressed at every layer. To successfully train ASGAN, we introduce a number
of new techniques, including a modification to adaptive discriminator
augmentation to probabilistically skip discriminator updates. ASGAN achieves
state-of-the-art results in unconditional speech synthesis on the Google Speech
Commands dataset. It is also substantially faster than the top-performing
diffusion models. Through a design that encourages disentanglement, ASGAN is
able to perform voice conversion and speech editing without being explicitly
trained to do so. ASGAN demonstrates that GANs are still highly competitive
with diffusion models. Code, models, samples:
https://github.com/RF5/simple-asgan/."
Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation,0.808628,"Sparsely annotated semantic segmentation (SASS) aims to train a segmentation
network with coarse-grained (i.e., point-, scribble-, and block-wise)
supervisions, where only a small proportion of pixels are labeled in each
image. In this paper, we propose a novel tree energy loss for SASS by providing
semantic guidance for unlabeled pixels. The tree energy loss represents images
as minimum spanning trees to model both low-level and high-level pair-wise
affinities. By sequentially applying these affinities to the network
prediction, soft pseudo labels for unlabeled pixels are generated in a
coarse-to-fine manner, achieving dynamic online self-training. The tree energy
loss is effective and easy to be incorporated into existing frameworks by
combining it with a traditional segmentation loss. Compared with previous SASS
methods, our method requires no multistage training strategies, alternating
optimization procedures, additional supervised data, or time-consuming
post-processing while outperforming them in all SASS settings. Code is
available at https://github.com/megvii-research/TreeEnergyLoss."
Geometry Interaction Knowledge Graph Embeddings,0.823379,"Knowledge graph (KG) embeddings have shown great power in learning
representations of entities and relations for link prediction tasks. Previous
work usually embeds KGs into a single geometric space such as Euclidean space
(zero curved), hyperbolic space (negatively curved) or hyperspherical space
(positively curved) to maintain their specific geometric structures (e.g.,
chain, hierarchy and ring structures). However, the topological structure of
KGs appears to be complicated, since it may contain multiple types of geometric
structures simultaneously. Therefore, embedding KGs in a single space, no
matter the Euclidean space, hyperbolic space or hyperspheric space, cannot
capture the complex structures of KGs accurately. To overcome this challenge,
we propose Geometry Interaction knowledge graph Embeddings (GIE), which learns
spatial structures interactively between the Euclidean, hyperbolic and
hyperspherical spaces. Theoretically, our proposed GIE can capture a richer set
of relational information, model key inference patterns, and enable expressive
semantic matching across entities. Experimental results on three
well-established knowledge graph completion benchmarks show that our GIE
achieves the state-of-the-art performance with fewer parameters."
Pure Transformers are Powerful Graph Learners,0.864665,"We show that standard Transformers without graph-specific modifications can
lead to promising results in graph learning both in theory and practice. Given
a graph, we simply treat all nodes and edges as independent tokens, augment
them with token embeddings, and feed them to a Transformer. With an appropriate
choice of token embeddings, we prove that this approach is theoretically at
least as expressive as an invariant graph network (2-IGN) composed of
equivariant linear layers, which is already more expressive than all
message-passing Graph Neural Networks (GNN). When trained on a large-scale
graph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer
(TokenGT) achieves significantly better results compared to GNN baselines and
competitive results compared to Transformer variants with sophisticated
graph-specific inductive bias. Our implementation is available at
https://github.com/jw9730/tokengt."
Robust Disentangled Variational Speech Representation Learning for Zero-shot Voice Conversion,0.886708,"Traditional studies on voice conversion (VC) have made progress with parallel
training data and known speakers. Good voice conversion quality is obtained by
exploring better alignment modules or expressive mapping functions. In this
study, we investigate zero-shot VC from a novel perspective of self-supervised
disentangled speech representation learning. Specifically, we achieve the
disentanglement by balancing the information flow between global speaker
representation and time-varying content representation in a sequential
variational autoencoder (VAE). A zero-shot voice conversion is performed by
feeding an arbitrary speaker embedding and content embeddings to the VAE
decoder. Besides that, an on-the-fly data augmentation training strategy is
applied to make the learned representation noise invariant. On TIMIT and VCTK
datasets, we achieve state-of-the-art performance on both objective evaluation,
i.e., speaker verification (SV) on speaker embedding and content embedding, and
subjective evaluation, i.e., voice naturalness and similarity, and remains to
be robust even with noisy source/target utterances."
Protecting Celebrities from DeepFake with Identity Consistency Transformer,0.820461,"In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities. Code will be released at
\url{https://github.com/LightDXY/ICT_DeepFake}"
TALM: Tool Augmented Language Models,0.840469,"Transformer based language models (LMs) demonstrate increasing performance
with scale across a wide variety of tasks. Scale alone however cannot enable
models to solve tasks that require access to ephemeral, changing, or private
data that was unavailable at training time. Many useful tasks may also benefit
from LMs being able to access APIs that read or modify state. In this work, we
present Tool Augmented Language Models (TALM), combining a text-only approach
to augment language models with non-differentiable tools, and an iterative
""self-play"" technique to bootstrap performance starting from few tool
demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA
task and a reasoning oriented math task with simple tools. At a given model
scale, TALM significantly outperforms non-augmented LMs. We further demonstrate
that TALM successfully performs out-of-distribution inferences on both QA and
math tasks, where non-augmented LMs fail. Our results suggest that Tool
Augmented Language Models are a promising direction to enrich LMs'
capabilities, with less dependence on scale."
WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution,0.864665,"Exploiting image patches instead of whole images have proved to be a powerful
approach to tackle various problems in image processing. Recently, Wasserstein
patch priors (WPP), which are based on the comparison of the patch
distributions of the unknown image and a reference image, were successfully
used as data-driven regularizers in the variational formulation of
superresolution. However, for each input image, this approach requires the
solution of a non-convex minimization problem which is computationally costly.
In this paper, we propose to learn two kind of neural networks in an
unsupervised way based on WPP loss functions. First, we show how convolutional
neural networks (CNNs) can be incorporated. Once the network, called WPPNet, is
learned, it can be very efficiently applied to any input image. Second, we
incorporate conditional normalizing flows to provide a tool for uncertainty
quantification. Numerical examples demonstrate the very good performance of
WPPNets for superresolution in various image classes even if the forward
operator is known only approximately."
DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta,0.864665,"Learning to generate new images for a novel category based on only a few
images, named as few-shot image generation, has attracted increasing research
interest. Several state-of-the-art works have yielded impressive results, but
the diversity is still limited. In this work, we propose a novel Delta
Generative Adversarial Network (DeltaGAN), which consists of a reconstruction
subnetwork and a generation subnetwork. The reconstruction subnetwork captures
intra-category transformation, i.e., delta, between same-category pairs. The
generation subnetwork generates sample-specific delta for an input image, which
is combined with this input image to generate a new image within the same
category. Besides, an adversarial delta matching loss is designed to link the
above two subnetworks together. Extensive experiments on six benchmark datasets
demonstrate the effectiveness of our proposed method. Our code is available at
https://github.com/bcmi/DeltaGAN-Few-Shot-Image-Generation."
TEMOS: Generating diverse human motions from textual descriptions,0.877321,"We address the problem of generating diverse 3D human motions from textual
descriptions. This challenging task requires joint modeling of both modalities:
understanding and extracting useful human-centric information from the text,
and then generating plausible and realistic sequences of human poses. In
contrast to most previous work which focuses on generating a single,
deterministic, motion from a textual description, we design a variational
approach that can produce multiple diverse human motions. We propose TEMOS, a
text-conditioned generative model leveraging variational autoencoder (VAE)
training with human motion data, in combination with a text encoder that
produces distribution parameters compatible with the VAE latent space. We show
the TEMOS framework can produce both skeleton-based animations as in prior
work, as well more expressive SMPL body motions. We evaluate our approach on
the KIT Motion-Language benchmark and, despite being relatively
straightforward, demonstrate significant improvements over the state of the
art. Code and models are available on our webpage."
ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection,0.864665,"Existing approaches for unsupervised point cloud pre-training are constrained
to either scene-level or point/voxel-level instance discrimination. Scene-level
methods tend to lose local details that are crucial for recognizing the road
objects, while point/voxel-level methods inherently suffer from limited
receptive field that is incapable of perceiving large objects or context
environments. Considering region-level representations are more suitable for 3D
object detection, we devise a new unsupervised point cloud pre-training
framework, called ProposalContrast, that learns robust 3D representations by
contrasting region proposals. Specifically, with an exhaustive set of region
proposals sampled from each point cloud, geometric point relations within each
proposal are modeled for creating expressive proposal representations. To
better accommodate 3D detection properties, ProposalContrast optimizes with
both inter-cluster and inter-proposal separation, i.e., sharpening the
discriminativeness of proposal representations across semantic classes and
object instances. The generalizability and transferability of ProposalContrast
are verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars
and PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE)."
Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again,0.823288,"The strong few-shot in-context learning capability of large pre-trained
language models (PLMs) such as GPT-3 is highly appealing for application
domains such as biomedicine, which feature high and diverse demands of language
technologies but also high data annotation costs. In this paper, we present the
first systematic and comprehensive study to compare the few-shot performance of
GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on
two highly representative biomedical information extraction tasks, named entity
recognition and relation extraction. We follow the true few-shot setting to
avoid overestimating models' few-shot performance by model selection over a
large validation set. We also optimize GPT-3's performance with known
techniques such as contextual calibration and dynamic in-context example
retrieval. However, our results show that GPT-3 still significantly
underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3
in-context learning also yields smaller gains in accuracy when more training
data becomes available. Our in-depth analyses further reveal issues of the
in-context learning setting that may be detrimental to information extraction
tasks in general. Given the high cost of experimenting with GPT-3, we hope our
study provides guidance for biomedical researchers and practitioners towards
more promising directions such as fine-tuning small PLMs."
QAScore -- An Unsupervised Unreferenced Metric for the Question Generation Evaluation,0.864665,"Question Generation (QG) aims to automate the task of composing questions for
a passage with a set of chosen answers found within the passage. In recent
years, the introduction of neural generation models has resulted in substantial
improvements of automatically generated questions in terms of quality,
especially compared to traditional approaches that employ manually crafted
heuristics. However, the metrics commonly applied in QG evaluations have been
criticized for their low agreement with human judgement. We therefore propose a
new reference-free evaluation metric that has the potential to provide a better
mechanism for evaluating QG systems, called QAScore. Instead of fine-tuning a
language model to maximize its correlation with human judgements, QAScore
evaluates a question by computing the cross entropy according to the
probability that the language model can correctly generate the masked words in
the answer to that question. Furthermore, we conduct a new crowd-sourcing human
evaluation experiment for the QG evaluation to investigate how QAScore and
other metrics can correlate with human judgements. Experiments show that
QAScore obtains a stronger correlation with the results of our proposed human
evaluation method compared to existing traditional word-overlap-based metrics
such as BLEU and ROUGE, as well as the existing pretrained-model-based metric
BERTScore."
The AI Index 2022 Annual Report,0.866615,"Welcome to the fifth edition of the AI Index Report! The latest edition
includes data from a broad set of academic, private, and nonprofit
organizations as well as more self-collected data and original analysis than
any previous editions, including an expanded technical performance chapter, a
new survey of robotics researchers around the world, data on global AI
legislation records in 25 countries, and a new chapter with an in-depth
analysis of technical AI ethics metrics.
  The AI Index Report tracks, collates, distills, and visualizes data related
to artificial intelligence. Its mission is to provide unbiased, rigorously
vetted, and globally sourced data for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The report aims to be the world's
most credible and authoritative source for data and insights about AI."
Towards an Accountable and Reproducible Federated Learning: A FactSheets Approach,0.846645,"Federated Learning (FL) is a novel paradigm for the shared training of models
based on decentralized and private data. With respect to ethical guidelines, FL
is promising regarding privacy, but needs to excel vis-\`a-vis transparency and
trustworthiness. In particular, FL has to address the accountability of the
parties involved and their adherence to rules, law and principles. We introduce
AF^2 Framework, where we instrument FL with accountability by fusing verifiable
claims with tamper-evident facts, into reproducible arguments. We build on AI
FactSheets for instilling transparency and trustworthiness into the AI
lifecycle and expand it to incorporate dynamic and nested facts, as well as
complex model compositions in FL. Based on our approach, an auditor can
validate, reproduce and certify a FL process. This can be directly applied in
practice to address the challenges of AI engineering and ethics."
MAGVIT: Masked Generative Video Transformer,0.794887,"We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle
various video synthesis tasks with a single model. We introduce a 3D tokenizer
to quantize a video into spatial-temporal visual tokens and propose an
embedding method for masked video token modeling to facilitate multi-task
learning. We conduct extensive experiments to demonstrate the quality,
efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT
performs favorably against state-of-the-art approaches and establishes the
best-published FVD on three video generation benchmarks, including the
challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference
time by two orders of magnitude against diffusion models and by 60x against
autoregressive models. (iii) A single MAGVIT model supports ten diverse
generation tasks and generalizes across videos from different visual domains.
The source code and trained models will be released to the public at
https://magvit.cs.cmu.edu."
Towards Grand Unification of Object Tracking,0.864665,"We present a unified method, termed Unicorn, that can simultaneously solve
four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the
same model parameters. Due to the fragmented definitions of the object tracking
problem itself, most existing trackers are developed to address a single or
part of tasks and overspecialize on the characteristics of specific tasks. By
contrast, Unicorn provides a unified solution, adopting the same input,
backbone, embedding, and head across all tracking tasks. For the first time, we
accomplish the great unification of the tracking network architecture and
learning paradigm. Unicorn performs on-par or better than its task-specific
counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,
BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will
serve as a solid step towards the general vision model. Code is available at
https://github.com/MasterBin-IIAU/Unicorn."
Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation,0.864665,"Monocular 3D human pose estimation technologies have the potential to greatly
increase the availability of human movement data. The best-performing models
for single-image 2D-3D lifting use graph convolutional networks (GCNs) that
typically require some manual input to define the relationships between
different body joints. We propose a novel transformer-based approach that uses
the more generalised self-attention mechanism to learn these relationships
within a sequence of tokens representing joints. We find that the use of
intermediate supervision, as well as residual connections between the stacked
encoders benefits performance. We also suggest that using error prediction as
part of a multi-task learning framework improves performance by allowing the
network to compensate for its confidence level. We perform extensive ablation
studies to show that each of our contributions increases performance.
Furthermore, we show that our approach outperforms the recent state of the art
for single-frame 3D human pose estimation by a large margin. Our code and
trained models are made publicly available on Github."
CATs are Fuzzy PETs: A Corpus and Analysis of Potentially Euphemistic Terms,0.85792,"Euphemisms have not received much attention in natural language processing,
despite being an important element of polite and figurative language.
Euphemisms prove to be a difficult topic, not only because they are subject to
language change, but also because humans may not agree on what is a euphemism
and what is not. Nevertheless, the first step to tackling the issue is to
collect and analyze examples of euphemisms. We present a corpus of potentially
euphemistic terms (PETs) along with example texts from the GloWbE corpus.
Additionally, we present a subcorpus of texts where these PETs are not being
used euphemistically, which may be useful for future applications. We also
discuss the results of multiple analyses run on the corpus. Firstly, we find
that sentiment analysis on the euphemistic texts supports that PETs generally
decrease negative and offensive sentiment. Secondly, we observe cases of
disagreement in an annotation task, where humans are asked to label PETs as
euphemistic or not in a subset of our corpus text examples. We attribute the
disagreement to a variety of potential reasons, including if the PET was a
commonly accepted term (CAT)."
DrapeNet: Garment Generation and Self-Supervised Draping,0.864665,"Recent approaches to drape garments quickly over arbitrary human bodies
leverage self-supervision to eliminate the need for large training sets.
However, they are designed to train one network per clothing item, which
severely limits their generalization abilities. In our work, we rely on
self-supervision to train a single network to drape multiple garments. This is
achieved by predicting a 3D deformation field conditioned on the latent codes
of a generative network, which models garments as unsigned distance fields. Our
pipeline can generate and drape previously unseen garments of any topology,
whose shape can be edited by manipulating their latent codes. Being fully
differentiable, our formulation makes it possible to recover accurate 3D models
of garments from partial observations -- images or 3D scans -- via gradient
descent. Our code is publicly available at
https://github.com/liren2515/DrapeNet ."
Confident Adaptive Language Modeling,0.861375,"Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance."
Peripheral Vision Transformer,0.864665,"Human vision possesses a special type of visual processing systems called
peripheral vision. Partitioning the entire visual field into multiple contour
regions based on the distance to the center of our gaze, the peripheral vision
provides us the ability to perceive various visual features at different
regions. In this work, we take a biologically inspired approach and explore to
model peripheral vision in deep neural networks for visual recognition. We
propose to incorporate peripheral position encoding to the multi-head
self-attention layers to let the network learn to partition the visual field
into diverse peripheral regions given training data. We evaluate the proposed
network, dubbed PerViT, on ImageNet-1K and systematically investigate the inner
workings of the model for machine perception, showing that the network learns
to perceive visual data similarly to the way that human vision does. The
performance improvements in image classification over the baselines across
different model sizes demonstrate the efficacy of the proposed method."
Geoclidean: Few-Shot Generalization in Euclidean Geometry,0.864665,"Euclidean geometry is among the earliest forms of mathematical thinking.
While the geometric primitives underlying its constructions, such as perfect
lines and circles, do not often occur in the natural world, humans rarely
struggle to perceive and reason with them. Will computer vision models trained
on natural images show the same sensitivity to Euclidean geometry? Here we
explore these questions by studying few-shot generalization in the universe of
Euclidean geometry constructions. We introduce Geoclidean, a domain-specific
language for Euclidean geometry, and use it to generate two datasets of
geometric concept learning tasks for benchmarking generalization judgements of
humans and machines. We find that humans are indeed sensitive to Euclidean
geometry and generalize strongly from a few visual examples of a geometric
concept. In contrast, low-level and high-level visual features from standard
computer vision models pretrained on natural images do not support correct
generalization. Thus Geoclidean represents a novel few-shot generalization
benchmark for geometric concept learning, where the performance of humans and
of AI models diverge. The Geoclidean framework and dataset are publicly
available for download."
SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis,0.876241,"We propose MINT, a new Multilingual INTimacy analysis dataset covering 13,372
tweets in 10 languages including English, French, Spanish, Italian, Portuguese,
Korean, Dutch, Chinese, Hindi, and Arabic. We benchmarked a list of popular
multilingual pre-trained language models. The dataset is released along with
the SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis
(https://sites.google.com/umich.edu/semeval-2023-tweet-intimacy)."
Localizing Visual Sounds the Easy Way,0.778599,"Unsupervised audio-visual source localization aims at localizing visible
sound sources in a video without relying on ground-truth localization for
training. Previous works often seek high audio-visual similarities for likely
positive (sounding) regions and low similarities for likely negative regions.
However, accurately distinguishing between sounding and non-sounding regions is
challenging without manual annotations. In this work, we propose a simple yet
effective approach for Easy Visual Sound Localization, namely EZ-VSL, without
relying on the construction of positive and/or negative regions during
training. Instead, we align audio and visual spaces by seeking audio-visual
representations that are aligned in, at least, one location of the associated
image, while not matching other images, at any location. We also introduce a
novel object guided localization scheme at inference time for improved
precision. Our simple and effective framework achieves state-of-the-art
performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In
particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to
83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is
available at https://github.com/stoneMo/EZ-VSL."
BlobGAN: Spatially Disentangled Scene Representations,0.864665,"We propose an unsupervised, mid-level representation for a generative model
of scenes. The representation is mid-level in that it is neither per-pixel nor
per-image; rather, scenes are modeled as a collection of spatial, depth-ordered
""blobs"" of features. Blobs are differentiably placed onto a feature grid that
is decoded into an image by a generative adversarial network. Due to the
spatial uniformity of blobs and the locality inherent to convolution, our
network learns to associate different blobs with different entities in a scene
and to arrange these blobs to capture scene layout. We demonstrate this
emergent behavior by showing that, despite training without any supervision,
our method enables applications such as easy manipulation of objects within a
scene (e.g., moving, removing, and restyling furniture), creation of feasible
scenes given constraints (e.g., plausible rooms with drawers at a particular
location), and parsing of real-world images into constituent parts. On a
challenging multi-category dataset of indoor scenes, BlobGAN outperforms
StyleGAN2 in image quality as measured by FID. See our project page for video
results and interactive demo: https://www.dave.ml/blobgan"
Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech,0.8427,"We introduce a generic, language-independent method to collect a large
percentage of offensive and hate tweets regardless of their topics or genres.
We harness the extralinguistic information embedded in the emojis to collect a
large number of offensive tweets. We apply the proposed method on Arabic tweets
and compare it with English tweets - analysing key cultural differences. We
observed a constant usage of these emojis to represent offensiveness throughout
different timespans on Twitter. We manually annotate and publicly release the
largest Arabic dataset for offensive, fine-grained hate speech, vulgar and
violence content. Furthermore, we benchmark the dataset for detecting
offensiveness and hate speech using different transformer architectures and
perform in-depth linguistic analysis. We evaluate our models on external
datasets - a Twitter dataset collected using a completely different method, and
a multi-platform dataset containing comments from Twitter, YouTube and
Facebook, for assessing generalization capability. Competitive results on these
datasets suggest that the data collected using our method captures universal
characteristics of offensive language. Our findings also highlight the common
words used in offensive communications, common targets for hate speech,
specific patterns in violence tweets; and pinpoint common classification errors
that can be attributed to limitations of NLP models. We observe that even
state-of-the-art transformer models may fail to take into account culture,
background and context or understand nuances present in real-world data such as
sarcasm."
Red-Teaming the Stable Diffusion Safety Filter,0.833032,"Stable Diffusion is a recent open-source image generation model comparable to
proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with
a safety filter that aims to prevent generating explicit images. Unfortunately,
the filter is obfuscated and poorly documented. This makes it hard for users to
prevent misuse in their applications, and to understand the filter's
limitations and improve it. We first show that it is easy to generate
disturbing content that bypasses the safety filter. We then reverse-engineer
the filter and find that while it aims to prevent sexual content, it ignores
violence, gore, and other similarly disturbing content. Based on our analysis,
we argue safety measures in future model releases should strive to be fully
open and properly documented to stimulate security contributions from the
community."
NerfAcc: A General NeRF Acceleration Toolbox,0.823652,"We propose NerfAcc, a toolbox for efficient volumetric rendering of radiance
fields. We build on the techniques proposed in Instant-NGP, and extend these
techniques to not only support bounded static scenes, but also for dynamic
scenes and unbounded scenes. NerfAcc comes with a user-friendly Python API, and
is ready for plug-and-play acceleration of most NeRFs. Various examples are
provided to show how to use this toolbox. Code can be found here:
https://github.com/KAIR-BAIR/nerfacc. Note this write-up matches with NerfAcc
v0.3.5. For the latest features in NerfAcc, please check out our more recent
write-up at arXiv:2305.04966"
IGLU 2022: Interactive Grounded Language Understanding in a Collaborative Environment at NeurIPS 2022,0.807633,"Human intelligence has the remarkable ability to adapt to new tasks and
environments quickly. Starting from a very young age, humans acquire new skills
and learn how to solve new tasks either by imitating the behavior of others or
by following provided natural language instructions. To facilitate research in
this direction, we propose IGLU: Interactive Grounded Language Understanding in
a Collaborative Environment. The primary goal of the competition is to approach
the problem of how to develop interactive embodied agents that learn to solve a
task while provided with grounded natural language instructions in a
collaborative environment. Understanding the complexity of the challenge, we
split it into sub-tasks to make it feasible for participants.
  This research challenge is naturally related, but not limited, to two fields
of study that are highly relevant to the NeurIPS community: Natural Language
Understanding and Generation (NLU/G) and Reinforcement Learning (RL).
Therefore, the suggested challenge can bring two communities together to
approach one of the crucial challenges in AI. Another critical aspect of the
challenge is the dedication to perform a human-in-the-loop evaluation as a
final evaluation for the agents developed by contestants."
LawngNLI: A Long-Premise Benchmark for In-Domain Generalization from Short to Long Contexts and for Implication-Based Retrieval,0.864665,"Natural language inference has trended toward studying contexts beyond the
sentence level. An important application area is law: past cases often do not
foretell how they apply to new situations and implications must be inferred.
This paper introduces LawngNLI, constructed from U.S. legal opinions with
automatic labels with high human-validated accuracy. Premises are long and
multigranular. Experiments show two use cases. First, LawngNLI can benchmark
for in-domain generalization from short to long contexts. It has remained
unclear if large-scale long-premise NLI datasets actually need to be
constructed: near-top performance on long premises could be achievable by
fine-tuning using short premises. Without multigranularity, benchmarks cannot
distinguish lack of fine-tuning on long premises versus domain shift between
short and long datasets. In contrast, our long and short premises share the
same examples and domain. Models fine-tuned using several past NLI datasets
and/or our short premises fall short of top performance on our long premises.
So for at least certain domains (such as ours), large-scale long-premise
datasets are needed. Second, LawngNLI can benchmark for implication-based
retrieval. Queries are entailed or contradicted by target documents, allowing
users to move between arguments and evidence. Leading retrieval models perform
reasonably zero shot on a LawngNLI-derived retrieval task. We compare different
systems for re-ranking, including lexical overlap and cross-encoders fine-tuned
using a modified LawngNLI or past NLI datasets. LawngNLI can train and test
systems for implication-based case retrieval and argumentation."
AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning,0.811671,"Standard fine-tuning of large pre-trained language models (PLMs) for
downstream tasks requires updating hundreds of millions to billions of
parameters, and storing a large copy of the PLM weights for every task
resulting in increased cost for storing, sharing and serving the models. To
address this, parameter-efficient fine-tuning (PEFT) techniques were introduced
where small trainable components are injected in the PLM and updated during
fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of
adaptation modules -- given the underlying PEFT method of choice -- introduced
in each Transformer layer while keeping most of the PLM weights frozen. For
instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture
of low rank decomposition matrices like LoRA to improve downstream task
performance over the corresponding PEFT methods for fully supervised and
few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the
same computational cost and the number of tunable parameters as the underlying
PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix
outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for
both NLU and NLG tasks."
Face segmentation: A comparison between visible and thermal images,0.82689,"Face segmentation is a first step for face biometric systems. In this paper
we present a face segmentation algorithm for thermographic images. This
algorithm is compared with the classic Viola and Jones algorithm used for
visible images. Experimental results reveal that, when segmenting a
multispectral (visible and thermal) face database, the proposed algorithm is
more than 10 times faster, while the accuracy of face segmentation in thermal
images is higher than in case of Viola-Jones"
Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition,0.884587,"Capturing the dependencies between joints is critical in skeleton-based
action recognition task. Transformer shows great potential to model the
correlation of important joints. However, the existing Transformer-based
methods cannot capture the correlation of different joints between frames,
which the correlation is very useful since different body parts (such as the
arms and legs in ""long jump"") between adjacent frames move together. Focus on
this problem, A novel spatio-temporal tuples Transformer (STTFormer) method is
proposed. The skeleton sequence is divided into several parts, and several
consecutive frames contained in each part are encoded. And then a
spatio-temporal tuples self-attention module is proposed to capture the
relationship of different joints in consecutive frames. In addition, a feature
aggregation module is introduced between non-adjacent frames to enhance the
ability to distinguish similar actions. Compared with the state-of-the-art
methods, our method achieves better performance on two large-scale datasets."
Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields,0.864665,"Image translation and manipulation have gain increasing attention along with
the rapid development of deep generative models. Although existing approaches
have brought impressive results, they mainly operated in 2D space. In light of
recent advances in NeRF-based 3D-aware generative models, we introduce a new
task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene
modelled by NeRF, conditioned on one single-view semantic mask as input. To
kick-off this novel task, we propose the Sem2NeRF framework. In particular,
Sem2NeRF addresses the highly challenging task by encoding the semantic mask
into the latent code that controls the 3D scene representation of a pre-trained
decoder. To further improve the accuracy of the mapping, we integrate a new
region-aware learning strategy into the design of both the encoder and the
decoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that
it outperforms several strong baselines on two benchmark datasets. Code and
video are available at https://donydchen.github.io/sem2nerf/"
A Mask Attention Interaction and Scale Enhancement Network for SAR Ship Instance Segmentation,0.871031,"Most of existing synthetic aperture radar (SAR) ship in-stance segmentation
models do not achieve mask interac-tion or offer limited interaction
performance. Besides, their multi-scale ship instance segmentation performance
is moderate especially for small ships. To solve these problems, we propose a
mask attention interaction and scale enhancement network (MAI-SE-Net) for SAR
ship instance segmentation. MAI uses an atrous spatial pyra-mid pooling (ASPP)
to gain multi-resolution feature re-sponses, a non-local block (NLB) to model
long-range spa-tial dependencies, and a concatenation shuffle attention block
(CSAB) to improve interaction benefits. SE uses a content-aware reassembly of
features block (CARAFEB) to generate an extra pyramid bottom-level to boost
small ship performance, a feature balance operation (FBO) to improve scale
feature description, and a global context block (GCB) to refine features.
Experimental results on two public SSDD and HRSID datasets reveal that
MAI-SE-Net outperforms the other nine competitive models, better than the
suboptimal model by 4.7% detec-tion AP and 3.4% segmentation AP on SSDD and by
3.0% detection AP and 2.4% segmentation AP on HRSID."
"AugShuffleNet: Communicate More, Compute Less",0.864665,"As a remarkable compact model, ShuffleNetV2 offers a good example to design
efficient ConvNets but its limit is rarely noticed. In this paper, we rethink
the design pattern of ShuffleNetV2 and find that the channel-wise redundancy
problem still constrains the efficiency improvement of Shuffle block in the
wider ShuffleNetV2. To resolve this issue, we propose another augmented variant
of shuffle block in the form of bottleneck-like structure and more implicit
short connections. To verify the effectiveness of this building block, we
further build a more powerful and efficient model family, termed as
AugShuffleNets. Evaluated on the CIFAR-10 and CIFAR-100 datasets, AugShuffleNet
consistently outperforms ShuffleNetV2 in terms of accuracy with less
computational cost and fewer parameter count."
The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,0.819827,"Conversational agents have come increasingly closer to human competence in
open-domain dialogue settings; however, such models can reflect insensitive,
hurtful, or entirely incoherent viewpoints that erode a user's trust in the
moral integrity of the system. Moral deviations are difficult to mitigate
because moral judgments are not universal, and there may be multiple competing
judgments that apply to a situation simultaneously. In this work, we introduce
a new resource, not to authoritatively resolve moral ambiguities, but instead
to facilitate systematic understanding of the intuitions, values and moral
judgments reflected in the utterances of dialogue systems. The Moral Integrity
Corpus, MIC, is such a resource, which captures the moral assumptions of 38k
prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects
a particular moral conviction that can explain why a chatbot's reply may appear
acceptable or problematic. We further organize RoTs with a set of 9 moral and
social attributes and benchmark performance for attribute classification. Most
importantly, we show that current neural language models can automatically
generate new RoTs that reasonably describe previously unseen interactions, but
they still struggle with certain scenarios. Our findings suggest that MIC will
be a useful resource for understanding and language models' implicit moral
assumptions and flexibly benchmarking the integrity of conversational agents.
To download the data, see https://github.com/GT-SALT/mic"
Generating Sequences by Learning to Self-Correct,0.821005,"Sequence generation applications require satisfying semantic constraints,
such as ensuring that programs are correct, using certain keywords, or avoiding
undesirable content. Language models, whether fine-tuned or prompted with
few-shot demonstrations, frequently violate these constraints, and lack a
mechanism to iteratively revise their outputs. Moreover, some powerful language
models are of extreme scale or inaccessible, making it inefficient, if not
infeasible, to update their parameters for task-specific adaptation. We present
Self-Correction, an approach that decouples an imperfect base generator (an
off-the-shelf language model or supervised sequence-to-sequence model) from a
separate corrector that learns to iteratively correct imperfect generations. To
train the corrector, we propose an online training procedure that can use
either scalar or natural language feedback on intermediate imperfect
generations. We show that Self-Correction improves upon the base generator in
three diverse generation tasks - mathematical program synthesis,
lexically-constrained generation, and toxicity control - even when the
corrector is much smaller than the base generator."
Sequence-to-Sequence Knowledge Graph Completion and Question Answering,0.800494,"Knowledge graph embedding (KGE) models represent each entity and relation of
a knowledge graph (KG) with low-dimensional embedding vectors. These methods
have recently been applied to KG link prediction and question answering over
incomplete KGs (KGQA). KGEs typically create an embedding for each entity in
the graph, which results in large model sizes on real-world graphs with
millions of entities. For downstream tasks these atomic entity representations
often need to be integrated into a multi stage pipeline, limiting their
utility. We show that an off-the-shelf encoder-decoder Transformer model can
serve as a scalable and versatile KGE model obtaining state-of-the-art results
for KG link prediction and incomplete KG question answering. We achieve this by
posing KG link prediction as a sequence-to-sequence task and exchange the
triple scoring approach taken by prior KGE methods with autoregressive
decoding. Such a simple but powerful method reduces the model size up to 98%
compared to conventional KGE models while keeping inference time tractable.
After finetuning this model on the task of KGQA over incomplete KGs, our
approach outperforms baselines on multiple large-scale datasets without
extensive hyperparameter tuning."
DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models,0.887886,"Traditionally, monocular 3D human pose estimation employs a machine learning
model to predict the most likely 3D pose for a given input image. However, a
single image can be highly ambiguous and induces multiple plausible solutions
for the 2D-3D lifting step which results in overly confident 3D pose
predictors. To this end, we propose \emph{DiffPose}, a conditional diffusion
model, that predicts multiple hypotheses for a given input image. In comparison
to similar approaches, our diffusion model is straightforward and avoids
intensive hyperparameter tuning, complex network structures, mode collapse, and
unstable training. Moreover, we tackle a problem of the common two-step
approach that first estimates a distribution of 2D joint locations via
joint-wise heatmaps and consecutively approximates them based on first- or
second-moment statistics. Since such a simplification of the heatmaps removes
valid information about possibly correct, though labeled unlikely, joint
locations, we propose to represent the heatmaps as a set of 2D joint candidate
samples. To extract information about the original distribution from these
samples we introduce our \emph{embedding transformer} that conditions the
diffusion model. Experimentally, we show that DiffPose slightly improves upon
the state of the art for multi-hypothesis pose estimation for simple poses and
outperforms it by a large margin for highly ambiguous poses."
LightX3ECG: A Lightweight and eXplainable Deep Learning System for 3-lead Electrocardiogram Classification,0.864665,"Cardiovascular diseases (CVDs) are a group of heart and blood vessel
disorders that is one of the most serious dangers to human health, and the
number of such patients is still growing. Early and accurate detection plays a
key role in successful treatment and intervention. Electrocardiogram (ECG) is
the gold standard for identifying a variety of cardiovascular abnormalities. In
clinical practices and most of the current research, standard 12-lead ECG is
mainly used. However, using a lower number of leads can make ECG more prevalent
as it can be conveniently recorded by portable or wearable devices. In this
research, we develop a novel deep learning system to accurately identify
multiple cardiovascular abnormalities by using only three ECG leads."
CXTrack: Improving 3D Point Cloud Tracking with Contextual Information,0.864665,"3D single object tracking plays an essential role in many applications, such
as autonomous driving. It remains a challenging problem due to the large
appearance variation and the sparsity of points caused by occlusion and limited
sensor capabilities. Therefore, contextual information across two consecutive
frames is crucial for effective object tracking. However, points containing
such useful information are often overlooked and cropped out in existing
methods, leading to insufficient use of important contextual knowledge. To
address this issue, we propose CXTrack, a novel transformer-based network for
3D object tracking, which exploits ConteXtual information to improve the
tracking results. Specifically, we design a target-centric transformer network
that directly takes point features from two consecutive frames and the previous
bounding box as input to explore contextual information and implicitly
propagate target cues. To achieve accurate localization for objects of all
sizes, we propose a transformer-based localization head with a novel center
embedding module to distinguish the target from distractors. Extensive
experiments on three large-scale datasets, KITTI, nuScenes and Waymo Open
Dataset, show that CXTrack achieves state-of-the-art tracking performance while
running at 34 FPS."
DFNet: Enhance Absolute Pose Regression with Direct Feature Matching,0.853611,"We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods."
Language Models are Multilingual Chain-of-Thought Reasoners,0.822166,"We evaluate the reasoning abilities of large language models in multilingual
settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by
manually translating 250 grade-school math problems from the GSM8K dataset
(Cobbe et al., 2021) into ten typologically diverse languages. We find that the
ability to solve MGSM problems via chain-of-thought prompting emerges with
increasing model scale, and that models have strikingly strong multilingual
reasoning abilities, even in underrepresented languages such as Bengali and
Swahili. Finally, we show that the multilingual reasoning abilities of language
models extend to other tasks such as commonsense reasoning and word-in-context
semantic judgment. The MGSM benchmark is publicly available at
https://github.com/google-research/url-nlp."
MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis,0.864665,"Recently, self-supervised pre-training has advanced Vision Transformers on
various tasks w.r.t. different data modalities, e.g., image and 3D point cloud
data. In this paper, we explore this learning paradigm for 3D mesh data
analysis based on Transformers. Since applying Transformer architectures to new
modalities is usually non-trivial, we first adapt Vision Transformer to 3D mesh
data processing, i.e., Mesh Transformer. In specific, we divide a mesh into
several non-overlapping local patches with each containing the same number of
faces and use the 3D position of each patch's center point to form positional
embeddings. Inspired by MAE, we explore how pre-training on 3D mesh data with
the Transformer-based structure benefits downstream 3D mesh analysis tasks. We
first randomly mask some patches of the mesh and feed the corrupted mesh into
Mesh Transformers. Then, through reconstructing the information of masked
patches, the network is capable of learning discriminative representations for
mesh data. Therefore, we name our method MeshMAE, which can yield
state-of-the-art or comparable performance on mesh analysis tasks, i.e.,
classification and segmentation. In addition, we also conduct comprehensive
ablation studies to show the effectiveness of key designs in our method."
Diffusion-LM Improves Controllable Text Generation,0.834352,"Controlling the behavior of language models (LMs) without re-training is a
major open problem in natural language generation. While recent works have
demonstrated successes on controlling simple sentence attributes (e.g.,
sentiment), there has been little progress on complex, fine-grained controls
(e.g., syntactic structure). To address this challenge, we develop a new
non-autoregressive language model based on continuous diffusions that we call
Diffusion-LM. Building upon the recent successes of diffusion models in
continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian
vectors into word vectors, yielding a sequence of intermediate latent
variables. The continuous, hierarchical nature of these intermediate variables
enables a simple gradient-based algorithm to perform complex, controllable
generation tasks. We demonstrate successful control of Diffusion-LM for six
challenging fine-grained control tasks, significantly outperforming prior work."
Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method,0.82186,"As the quality of optical sensors improves, there is a need for processing
large-scale images. In particular, the ability of devices to capture ultra-high
definition (UHD) images and video places new demands on the image processing
pipeline. In this paper, we consider the task of low-light image enhancement
(LLIE) and introduce a large-scale database consisting of images at 4K and 8K
resolution. We conduct systematic benchmarking studies and provide a comparison
of current LLIE algorithms. As a second contribution, we introduce LLFormer, a
transformer-based low-light enhancement method. The core components of LLFormer
are the axis-based multi-head self-attention and cross-layer attention fusion
block, which significantly reduces the linear complexity. Extensive experiments
on the new dataset and existing public datasets show that LLFormer outperforms
state-of-the-art methods. We also show that employing existing LLIE methods
trained on our benchmark as a pre-processing step significantly improves the
performance of downstream tasks, e.g., face detection in low-light conditions.
The source code and pre-trained models are available at
https://github.com/TaoWangzj/LLFormer."
UIGR: Unified Interactive Garment Retrieval,0.864665,"Interactive garment retrieval (IGR) aims to retrieve a target garment image
based on a reference garment image along with user feedback on what to change
on the reference garment. Two IGR tasks have been studied extensively:
text-guided garment retrieval (TGR) and visually compatible garment retrieval
(VCR). The user feedback for the former indicates what semantic attributes to
change with the garment category preserved, while the category is the only
thing to be changed explicitly for the latter, with an implicit requirement on
style preservation. Despite the similarity between these two tasks and the
practical need for an efficient system tackling both, they have never been
unified and modeled jointly. In this paper, we propose a Unified Interactive
Garment Retrieval (UIGR) framework to unify TGR and VCR. To this end, we first
contribute a large-scale benchmark suited for both problems. We further propose
a strong baseline architecture to integrate TGR and VCR in one model. Extensive
experiments suggest that unifying two tasks in one framework is not only more
efficient by requiring a single model only, it also leads to better
performance. Code and datasets are available at
https://github.com/BrandonHanx/CompFashion."
EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model,0.782558,"Although significant progress has been made to audio-driven talking face
generation, existing methods either neglect facial emotion or cannot be applied
to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model
(EAMM) to generate one-shot emotional talking faces by involving an emotion
source video. Specifically, we first propose an Audio2Facial-Dynamics module,
which renders talking faces from audio-driven unsupervised zero- and
first-order key-points motion. Then through exploring the motion model's
properties, we further propose an Implicit Emotion Displacement Learner to
represent emotion-related facial dynamics as linearly additive displacements to
the previously acquired motion representations. Comprehensive experiments
demonstrate that by incorporating the results from both modules, our method can
generate satisfactory talking face results on arbitrary subjects with realistic
emotion patterns."
BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion,0.792763,"Dense 3D reconstruction from a stream of depth images is the key to many
mixed reality and robotic applications. Although methods based on Truncated
Signed Distance Function (TSDF) Fusion have advanced the field over the years,
the TSDF volume representation is confronted with striking a balance between
the robustness to noisy measurements and maintaining the level of detail. We
present Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent
advances in neural implicit representations and neural rendering for dense 3D
reconstruction. In order to incrementally integrate new depth maps into a
global neural implicit representation, we propose a novel bi-level fusion
strategy that considers both efficiency and reconstruction quality by design.
We evaluate the proposed method on multiple datasets quantitatively and
qualitatively, demonstrating a significant improvement over existing methods."
Speech Emotion Recognition with Co-Attention based Multi-level Acoustic Information,0.875392,"Speech Emotion Recognition (SER) aims to help the machine to understand
human's subjective emotion from only audio information. However, extracting and
utilizing comprehensive in-depth audio information is still a challenging task.
In this paper, we propose an end-to-end speech emotion recognition system using
multi-level acoustic information with a newly designed co-attention module. We
firstly extract multi-level acoustic information, including MFCC, spectrogram,
and the embedded high-level acoustic information with CNN, BiLSTM and wav2vec2,
respectively. Then these extracted features are treated as multimodal inputs
and fused by the proposed co-attention mechanism. Experiments are carried on
the IEMOCAP dataset, and our model achieves competitive performance with two
different speaker-independent cross-validation strategies. Our code is
available on GitHub."
ProgPrompt: Generating Situated Robot Task Plans using Large Language Models,0.864665,"Task planning can require defining myriad domain knowledge about the world in
which a robot needs to act. To ameliorate that effort, large language models
(LLMs) can be used to score potential next actions during task planning, and
even generate action sequences directly, given an instruction in natural
language with no additional domain information. However, such methods either
require enumerating all possible next steps for scoring, or generate free-form
text that may contain actions not possible on a given robot in its current
context. We present a programmatic LLM prompt structure that enables plan
generation functional across situated environments, robot capabilities, and
tasks. Our key insight is to prompt the LLM with program-like specifications of
the available actions and objects in an environment, as well as with example
programs that can be executed. We make concrete recommendations about prompt
structure and generation constraints through ablation experiments, demonstrate
state of the art success rates in VirtualHome household tasks, and deploy our
method on a physical robot arm for tabletop tasks. Website at
progprompt.github.io"
Generating Information-Seeking Conversations from Unlabeled Documents,0.864665,"In this paper, we introduce a novel framework, SIMSEEK, (Simulating
information-Seeking conversation from unlabeled documents), and compare its two
variants. In our baseline SIMSEEK-SYM, a questioner generates follow-up
questions upon the predetermined answer by an answerer. On the contrary,
SIMSEEK-ASYM first generates the question and then finds its corresponding
answer under the conversational context. Our experiments show that they can
synthesize effective training resources for CQA and conversational search
tasks. As a result, conversations from SIMSEEK-ASYM not only make more
improvements in our experiments but also are favorably reviewed in a human
evaluation. We finally release a large-scale resource of synthetic
conversations, WIKI-SIMSEEK, containing 2 million CQA pairs built upon
Wikipedia documents. With the dataset, our CQA model achieves state-of-the-art
performance on a recent CQA benchmark, QuAC."
Deepfake Network Architecture Attribution,0.830621,"With the rapid progress of generation technology, it has become necessary to
attribute the origin of fake images. Existing works on fake image attribution
perform multi-class classification on several Generative Adversarial Network
(GAN) models and obtain high accuracies. While encouraging, these works are
restricted to model-level attribution, only capable of handling images
generated by seen models with a specific seed, loss and dataset, which is
limited in real-world scenarios when fake images may be generated by privately
trained models. This motivates us to ask whether it is possible to attribute
fake images to the source models' architectures even if they are finetuned or
retrained under different configurations. In this work, we present the first
study on Deepfake Network Architecture Attribution to attribute fake images on
architecture-level. Based on an observation that GAN architecture is likely to
leave globally consistent fingerprints while traces left by model weights vary
in different regions, we provide a simple yet effective solution named DNA-Det
for this problem. Extensive experiments on multiple cross-test setups and a
large-scale dataset demonstrate the effectiveness of DNA-Det."
DxFormer: A Decoupled Automatic Diagnostic System Based on Decoder-Encoder Transformer with Dense Symptom Representations,0.864665,"Diagnosis-oriented dialogue system queries the patient's health condition and
makes predictions about possible diseases through continuous interaction with
the patient. A few studies use reinforcement learning (RL) to learn the optimal
policy from the joint action space of symptoms and diseases. However, existing
RL (or Non-RL) methods cannot achieve sufficiently good prediction accuracy,
still far from its upper limit. To address the problem, we propose a decoupled
automatic diagnostic framework DxFormer, which divides the diagnosis process
into two steps: symptom inquiry and disease diagnosis, where the transition
from symptom inquiry to disease diagnosis is explicitly determined by the
stopping criteria. In DxFormer, we treat each symptom as a token, and formalize
the symptom inquiry and disease diagnosis to a language generation model and a
sequence classification model respectively. We use the inverted version of
Transformer, i.e., the decoder-encoder structure, to learn the representation
of symptoms by jointly optimizing the reinforce reward and cross entropy loss.
Extensive experiments on three public real-world datasets prove that our
proposed model can effectively learn doctors' clinical experience and achieve
the state-of-the-art results in terms of symptom recall and diagnostic
accuracy."
Diffusion Models for Video Prediction and Infilling,0.864665,"Predicting and anticipating future outcomes or reasoning about missing
information in a sequence are critical skills for agents to be able to make
intelligent decisions. This requires strong, temporally coherent generative
capabilities. Diffusion models have shown remarkable success in several
generative tasks, but have not been extensively explored in the video domain.
We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion
models to videos using 3D convolutions, and introduces a new conditioning
technique during training. By varying the mask we condition on, the model is
able to perform video prediction, infilling, and upsampling. Due to our simple
conditioning scheme, we can utilize the same architecture as used for
unconditional training, which allows us to train the model in a conditional and
unconditional fashion at the same time. We evaluate RaMViD on two benchmark
datasets for video prediction, on which we achieve state-of-the-art results,
and one for video generation. High-resolution videos are provided at
https://sites.google.com/view/video-diffusion-prediction."
In-vehicle alertness monitoring for older adults,0.864665,"Alertness monitoring in the context of driving improves safety and saves
lives. Computer vision based alertness monitoring is an active area of
research. However, the algorithms and datasets that exist for alertness
monitoring are primarily aimed at younger adults (18-50 years old). We present
a system for in-vehicle alertness monitoring for older adults. Through a design
study, we ascertained the variables and parameters that are suitable for older
adults traveling independently in Level 5 vehicles. We implemented a prototype
traveler monitoring system and evaluated the alertness detection algorithm on
ten older adults (70 years and older). We report on the system design and
implementation at a level of detail that is suitable for the beginning
researcher or practitioner. Our study suggests that dataset development is the
foremost challenge for developing alertness monitoring systems targeted at
older adults. This study is the first of its kind for a hitherto under-studied
population and has implications for future work on algorithm development and
system design through participatory methods."
cosFormer: Rethinking Softmax in Attention,0.998054,"Transformer has shown great successes in natural language processing,
computer vision, and audio processing. As one of its core components, the
softmax attention helps to capture long-range dependencies yet prohibits its
scale-up due to the quadratic space and time complexity to the sequence length.
Kernel methods are often adopted to reduce the complexity by approximating the
softmax operator. Nevertheless, due to the approximation errors, their
performances vary in different tasks/corpus and suffer crucial performance
drops when compared with the vanilla softmax attention. In this paper, we
propose a linear transformer called cosFormer that can achieve comparable or
better accuracy to the vanilla transformer in both casual and cross attentions.
cosFormer is based on two key properties of softmax attention: i).
non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme
that can concentrate the distribution of the attention matrix. As its linear
substitute, cosFormer fulfills these properties with a linear operator and a
cosine-based distance re-weighting mechanism. Extensive experiments on language
modeling and text understanding tasks demonstrate the effectiveness of our
method. We further examine our method on long sequences and achieve
state-of-the-art performance on the Long-Range Arena benchmark. The source code
is available at https://github.com/OpenNLPLab/cosFormer."
BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation,0.930517,"We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image."
The Role of ImageNet Classes in Frchet Inception Distance,0.964328,"Fr\'echet Inception Distance (FID) is the primary metric for ranking models
in data-driven generative modeling. While remarkably successful, the metric is
known to sometimes disagree with human judgement. We investigate a root cause
of these discrepancies, and visualize what FID ""looks at"" in generated images.
We show that the feature space that FID is (typically) computed in is so close
to the ImageNet classifications that aligning the histograms of Top-$N$
classifications between sets of generated and real images can reduce FID
substantially -- without actually improving the quality of results. Thus, we
conclude that FID is prone to intentional or accidental distortions. As a
practical example of an accidental distortion, we discuss a case where an
ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while
being worse in terms of human evaluation."
mSLAM: Massively multilingual joint pre-training for speech and text,0.955329,"We present mSLAM, a multilingual Speech and LAnguage Model that learns
cross-lingual cross-modal representations of speech and text by pre-training
jointly on large amounts of unlabeled speech and text in multiple languages.
mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on
character-level text, along with Connectionist Temporal Classification (CTC)
losses on paired speech and transcript data, to learn a single model capable of
learning from and representing both speech and text signals in a shared
representation space. We evaluate mSLAM on several downstream speech
understanding tasks and find that joint pre-training with text improves quality
on speech translation, speech intent classification and speech language-ID
while being competitive on multilingual ASR, when compared against speech-only
pre-training. Our speech translation model demonstrates zero-shot text
translation without seeing any text translation data, providing evidence for
cross-modal alignment of representations. mSLAM also benefits from multi-modal
fine-tuning, further improving the quality of speech translation by directly
leveraging text translation data during the fine-tuning process. Our empirical
analysis highlights several opportunities and challenges arising from
large-scale multimodal pre-training, suggesting directions for future research."
Visual Programming: Compositional visual reasoning without training,0.999999,"We present VISPROG, a neuro-symbolic approach to solving complex and
compositional visual tasks given natural language instructions. VISPROG avoids
the need for any task-specific training. Instead, it uses the in-context
learning ability of large language models to generate python-like modular
programs, which are then executed to get both the solution and a comprehensive
and interpretable rationale. Each line of the generated program may invoke one
of several off-the-shelf computer vision models, image processing routines, or
python functions to produce intermediate outputs that may be consumed by
subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4
diverse tasks - compositional visual question answering, zero-shot reasoning on
image pairs, factual knowledge object tagging, and language-guided image
editing. We believe neuro-symbolic approaches like VISPROG are an exciting
avenue to easily and effectively expand the scope of AI systems to serve the
long tail of complex tasks that people may wish to perform."
Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality,0.974503,"We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground."
MiniViT: Compressing Vision Transformers with Weight Multiplexing,0.967283,"Vision Transformer (ViT) models have recently drawn much attention in
computer vision due to their high model capability. However, ViT models suffer
from huge number of parameters, restricting their applicability on devices with
limited memory. To alleviate this problem, we propose MiniViT, a new
compression framework, which achieves parameter reduction in vision
transformers while retaining the same performance. The central idea of MiniViT
is to multiplex the weights of consecutive transformer blocks. More
specifically, we make the weights shared across layers, while imposing a
transformation on the weights to increase diversity. Weight distillation over
self-attention is also applied to transfer knowledge from large-scale ViT
models to weight-multiplexed compact models. Comprehensive experiments
demonstrate the efficacy of MiniViT, showing that it can reduce the size of the
pre-trained Swin-B transformer by 48\%, while achieving an increase of 1.0\% in
Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,
MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,
without seriously compromising the performance. Finally, we verify the
transferability of MiniViT by reporting its performance on downstream
benchmarks. Code and models are available at here."
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,1.0,"Chain-of-thought prompting has demonstrated remarkable performance on various
natural language reasoning tasks. However, it tends to perform poorly on tasks
which requires solving problems harder than the exemplars shown in the prompts.
To overcome this challenge of easy-to-hard generalization, we propose a novel
prompting strategy, least-to-most prompting. The key idea in this strategy is
to break down a complex problem into a series of simpler subproblems and then
solve them in sequence. Solving each subproblem is facilitated by the answers
to previously solved subproblems. Our experimental results on tasks related to
symbolic manipulation, compositional generalization, and math reasoning reveal
that least-to-most prompting is capable of generalizing to more difficult
problems than those seen in the prompts. A notable finding is that when the
GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve
the compositional generalization benchmark SCAN in any split (including length
split) with an accuracy of at least 99% using just 14 exemplars, compared to
only 16% accuracy with chain-of-thought prompting. This is particularly
noteworthy because neural-symbolic models in the literature that specialize in
solving SCAN are trained on the entire training set containing over 15,000
examples. We have included prompts for all the tasks in the Appendix."
CM3: A Causal Masked Multimodal Model of the Internet,0.988314,"We introduce CM3, a family of causally masked generative models trained over
a large corpus of structured multi-modal documents that can contain both text
and image tokens. Our new causally masked approach generates tokens left to
right while also masking out a small number of long token spans that are
generated at the end of the string, instead of their original positions. The
casual masking object provides a type of hybrid of the more common causal and
masked language models, by enabling full generative modeling while also
providing bidirectional context when generating the masked spans. We train
causally masked language-image models on large-scale web and Wikipedia
articles, where each document contains all of the text, hypertext markup,
hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they
appear in the original HTML source (before masking). The resulting CM3 models
can generate rich structured, multi-modal outputs while conditioning on
arbitrary masked document contexts, and thereby implicitly learn a wide range
of text, image, and cross modal tasks. They can be prompted to recover, in a
zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM.
We set the new state-of-the-art in zero-shot summarization, entity linking, and
entity disambiguation while maintaining competitive performance in the
fine-tuning setting. We can generate images unconditionally, conditioned on
text (like DALL-E) and do captioning all in a zero-shot setting with a single
model."
Third Time's the Charm? Image and Video Editing with StyleGAN3,0.99293,"StyleGAN is arguably one of the most intriguing and well-studied generative
models, demonstrating impressive performance in image generation, inversion,
and manipulation. In this work, we explore the recent StyleGAN3 architecture,
compare it to its predecessor, and investigate its unique advantages, as well
as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained
on unaligned data, one can still use aligned data for training, without
hindering the ability to generate unaligned imagery. Next, our analysis of the
disentanglement of the different latent spaces of StyleGAN3 indicates that the
commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,
underscoring the benefits of using the StyleSpace for fine-grained editing.
Considering image inversion, we observe that existing encoder-based techniques
struggle when trained on unaligned data. We therefore propose an encoding
scheme trained solely on aligned data, yet can still invert unaligned images.
Finally, we introduce a novel video inversion and editing workflow that
leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce
texture sticking and expand the field of view of the edited video."
SimSR: Simple Distance-based State Representation for Deep Reinforcement Learning,0.973652,"This work explores how to learn robust and generalizable state representation
from image-based observations with deep reinforcement learning methods.
Addressing the computational complexity, stringent assumptions and
representation collapse challenges in existing work of bisimulation metric, we
devise Simple State Representation (SimSR) operator. SimSR enables us to design
a stochastic approximation method that can practically learn the mapping
functions (encoders) from observations to latent representation space. In
addition to the theoretical analysis and comparison with the existing work, we
experimented and compared our work with recent state-of-the-art solutions in
visual MuJoCo tasks. The results shows that our model generally achieves better
performance and has better robustness and good generalization."
"""I think this is the most disruptive technology"": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data",0.968952,"Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users."
Demystifying Prompts in Language Models via Perplexity Estimation,0.978255,"Language models can be prompted to perform a wide variety of zero- and
few-shot learning problems. However, performance varies significantly with the
choice of prompt, and we do not yet understand why this happens or how to pick
the best prompts. In this work, we analyze the factors that contribute to this
variance and establish a new empirical hypothesis: the performance of a prompt
is coupled with the extent to which the model is familiar with the language it
contains. Over a wide range of tasks, we show that the lower the perplexity of
the prompt is, the better the prompt is able to perform the task. As a result,
we devise a method for creating prompts: (1) automatically extend a small seed
set of manually written prompts by paraphrasing using GPT3 and backtranslation
and (2) choose the lowest perplexity prompts to get significant gains in
performance."
Multi-Game Decision Transformers,0.967645,"A longstanding goal of the field of AI is a method for learning a highly
capable, generalist agent from diverse experience. In the subfields of vision
and language, this was largely achieved by scaling up transformer-based models
and training them on large, diverse datasets. Motivated by this progress, we
investigate whether the same strategy can be used to produce generalist
reinforcement learning agents. Specifically, we show that a single
transformer-based model - with a single set of weights - trained purely offline
can play a suite of up to 46 Atari games simultaneously at close-to-human
performance. When trained and evaluated appropriately, we find that the same
trends observed in language and vision hold, including scaling of performance
with model size and rapid adaptation to new games via fine-tuning. We compare
several approaches in this multi-game setting, such as online and offline RL
methods and behavioral cloning, and find that our Multi-Game Decision
Transformer models offer the best scalability and performance. We release the
pre-trained models and code to encourage further research in this direction."
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation,0.999976,"Vision-Language Pre-training (VLP) has advanced the performance for many
vision-language tasks. However, most existing pre-trained models only excel in
either understanding-based tasks or generation-based tasks. Furthermore,
performance improvement has been largely achieved by scaling up the dataset
with noisy image-text pairs collected from the web, which is a suboptimal
source of supervision. In this paper, we propose BLIP, a new VLP framework
which transfers flexibly to both vision-language understanding and generation
tasks. BLIP effectively utilizes the noisy web data by bootstrapping the
captions, where a captioner generates synthetic captions and a filter removes
the noisy ones. We achieve state-of-the-art results on a wide range of
vision-language tasks, such as image-text retrieval (+2.7% in average
recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).
BLIP also demonstrates strong generalization ability when directly transferred
to video-language tasks in a zero-shot manner. Code, models, and datasets are
released at https://github.com/salesforce/BLIP."
News Summarization and Evaluation in the Era of GPT-3,0.991461,"The recent success of prompting large language models like GPT-3 has led to a
paradigm shift in NLP research. In this paper, we study its impact on text
summarization, focusing on the classic benchmark domain of news summarization.
First, we investigate how GPT-3 compares against fine-tuned models trained on
large summarization datasets. We show that not only do humans overwhelmingly
prefer GPT-3 summaries, prompted using only a task description, but these also
do not suffer from common dataset-specific issues such as poor factuality.
Next, we study what this means for evaluation, particularly the role of gold
standard test sets. Our experiments show that both reference-based and
reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.
Finally, we evaluate models on a setting beyond generic summarization,
specifically keyword-based summarization, and show how dominant fine-tuning
approaches compare to prompting.
  To support further research, we release: (a) a corpus of 10K generated
summaries from fine-tuned and prompt-based models across 4 standard
summarization benchmarks, (b) 1K human preference judgments comparing different
systems for generic- and keyword-based summarization."
"CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron",0.912505,"Cervical cancer is the seventh most common cancer among all the cancers
worldwide and the fourth most common cancer among women. Cervical cytopathology
image classification is an important method to diagnose cervical cancer. Manual
screening of cytopathology images is time-consuming and error-prone. The
emergence of the automatic computer-aided diagnosis system solves this problem.
This paper proposes a framework called CVM-Cervix based on deep learning to
perform cervical cell classification tasks. It can analyze pap slides quickly
and accurately. CVM-Cervix first proposes a Convolutional Neural Network module
and a Visual Transformer module for local and global feature extraction
respectively, then a Multilayer Perceptron module is designed to fuse the local
and global features for the final classification. Experimental results show the
effectiveness and potential of the proposed CVM-Cervix in the field of cervical
Pap smear image classification. In addition, according to the practical needs
of clinical work, we perform a lightweight post-processing to compress the
model."
Omnivore: A Single Model for Many Visual Modalities,0.999928,"Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together."
Frontiers and Exact Learning of ELI Queries under DL-Lite Ontologies,0.977392,"We study ELI queries (ELIQs) in the presence of ontologies formulated in the
description logic DL-Lite. For the dialect DL-LiteH, we show that ELIQs have a
frontier (set of least general generalizations) that is of polynomial size and
can be computed in polynomial time. In the dialect DL-LiteF, in contrast,
frontiers may be infinite. We identify a natural syntactic restriction that
enables the same positive results as for DL-LiteH. We use out results on
frontiers to show that ELIQs are learnable in polynomial time in the presence
of a DL-LiteH / restricted DL-LiteF ontology in Angluin's framework of exact
learning with only membership queries."
SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection,0.981684,"Convolutional neural networks (CNNs) are good at extracting contexture
features within certain receptive fields, while transformers can model the
global long-range dependency features. By absorbing the advantage of
transformer and the merit of CNN, Swin Transformer shows strong feature
representation ability. Based on it, we propose a cross-modality fusion model
SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin
Transformer to extract the hierarchical features, boosted by attention
mechanism to bridge the gap between two modalities, and guided by edge
information to sharp the contour of salient object. To be specific, two-stream
Swin Transformer encoder first extracts multi-modality features, and then
spatial alignment and channel re-calibration module is presented to optimize
intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided
decoder achieves inter-level cross-modality fusion under the guidance of edge
features. The proposed model outperforms the state-of-the-art models on RGB-D
and RGB-T datasets, showing that it provides more insight into the
cross-modality complementarity task."
Dataset Distillation by Matching Training Trajectories,0.964366,"Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data."
Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,0.902191,"Large language models (LLMs) have shown remarkable reasoning capabilities
given chain-of-thought prompts (examples with intermediate reasoning steps).
Existing benchmarks measure reasoning ability indirectly, by evaluating
accuracy on downstream tasks such as mathematical reasoning. However, it is
unclear how these models obtain the answers and whether they rely on simple
heuristics rather than the generated chain-of-thought. To enable systematic
exploration of the reasoning ability of LLMs, we present a new synthetic
question-answering dataset called PrOntoQA, where each example is generated
from a synthetic world model represented in first-order logic. This allows us
to parse the generated chain-of-thought into symbolic proofs for formal
analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite
capable of making correct individual deduction steps, and so are generally
capable of reasoning, even in fictional contexts. However, they have difficulty
with proof planning: When multiple valid deduction steps are available, they
are not able to systematically explore the different options."
Masked Generative Distillation,0.989691,"Knowledge distillation has been applied to various tasks successfully. The
current distillation algorithm usually improves students' performance by
imitating the output of the teacher. This paper shows that teachers can also
improve students' representation power by guiding students' feature recovery.
From this point of view, we propose Masked Generative Distillation (MGD), which
is simple: we mask random pixels of the student's feature and force it to
generate the teacher's full feature through a simple block. MGD is a truly
general feature-based distillation method, which can be utilized on various
tasks, including image classification, object detection, semantic segmentation
and instance segmentation. We experiment on different models with extensive
datasets and the results show that all the students achieve excellent
improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1
accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,
SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on
ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at
https://github.com/yzd-v/MGD."
Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic,0.991252,"Propaganda is the expression of an opinion or an action by an individual or a
group deliberately designed to influence the opinions or the actions of other
individuals or groups with reference to predetermined ends, which is achieved
by means of well-defined rhetorical and psychological devices. Propaganda
techniques are commonly used in social media to manipulate or to mislead users.
Thus, there has been a lot of recent research on automatic detection of
propaganda techniques in text as well as in memes. However, so far the focus
has been primarily on English. With the aim to bridge this language gap, we ran
a shared task on detecting propaganda techniques in Arabic tweets as part of
the WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to
identify the set of propaganda techniques used in a tweet, which is a
multilabel classification problem, while Subtask~2 asks to detect the
propaganda techniques used in a tweet together with the exact span(s) of text
in which each propaganda technique appears. The task attracted 63 team
registrations, and eventually 14 and 3 teams made submissions for subtask 1 and
2, respectively. Finally, 11 teams submitted system description papers."
SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D Shape Generation,0.970239,"We present a StyleGAN2-based deep learning approach for 3D shape generation,
called SDF-StyleGAN, with the aim of reducing visual and geometric
dissimilarity between generated shapes and a shape collection. We extend
StyleGAN2 to 3D generation and utilize the implicit signed distance function
(SDF) as the 3D shape representation, and introduce two novel global and local
shape discriminators that distinguish real and fake SDF values and gradients to
significantly improve shape geometry and visual quality. We further complement
the evaluation metrics of 3D generative models with the shading-image-based
Fr\'echet inception distance (FID) scores to better assess visual quality and
shape distribution of the generated shapes. Experiments on shape generation
demonstrate the superior performance of SDF-StyleGAN over the state-of-the-art.
We further demonstrate the efficacy of SDF-StyleGAN in various tasks based on
GAN inversion, including shape reconstruction, shape completion from partial
point clouds, single-view image-based shape generation, and shape style
editing. Extensive ablation studies justify the efficacy of our framework
design. Our code and trained models are available at
https://github.com/Zhengxinyang/SDF-StyleGAN."
UniCLIP: Unified Framework for Contrastive Language-Image Pre-training,0.976482,"Pre-training vision-language models with contrastive objectives has shown
promising results that are both scalable to large uncurated datasets and
transferable to many downstream applications. Some following works have
targeted to improve data efficiency by adding self-supervision terms, but
inter-domain (image-text) contrastive loss and intra-domain (image-image)
contrastive loss are defined on individual spaces in those works, so many
feasible combinations of supervision are overlooked. To overcome this issue, we
propose UniCLIP, a Unified framework for Contrastive Language-Image
Pre-training. UniCLIP integrates the contrastive loss of both inter-domain
pairs and intra-domain pairs into a single universal space. The discrepancies
that occur when integrating contrastive loss between different domains are
resolved by the three key components of UniCLIP: (1) augmentation-aware feature
embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure.
UniCLIP outperforms previous vision-language pre-training methods on various
single- and multi-modality downstream tasks. In our experiments, we show that
each component that comprises UniCLIP contributes well to the final
performance."
Exploring Plain Vision Transformer Backbones for Object Detection,0.994474,"We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone
network for object detection. This design enables the original ViT architecture
to be fine-tuned for object detection without needing to redesign a
hierarchical backbone for pre-training. With minimal adaptations for
fine-tuning, our plain-backbone detector can achieve competitive results.
Surprisingly, we observe: (i) it is sufficient to build a simple feature
pyramid from a single-scale feature map (without the common FPN design) and
(ii) it is sufficient to use window attention (without shifting) aided with
very few cross-window propagation blocks. With plain ViT backbones pre-trained
as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the
previous leading methods that were all based on hierarchical backbones,
reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K
pre-training. We hope our study will draw attention to research on
plain-backbone detectors. Code for ViTDet is available in Detectron2."
SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views,0.922119,"We introduce SparseNeuS, a novel neural rendering based method for the task
of surface reconstruction from multi-view images. This task becomes more
difficult when only sparse images are provided as input, a scenario where
existing neural reconstruction approaches usually produce incomplete or
distorted results. Moreover, their inability of generalizing to unseen new
scenes impedes their application in practice. Contrarily, SparseNeuS can
generalize to new scenes and work well with sparse images (as few as 2 or 3).
SparseNeuS adopts signed distance function (SDF) as the surface representation,
and learns generalizable priors from image features by introducing geometry
encoding volumes for generic surface prediction. Moreover, several strategies
are introduced to effectively leverage sparse views for high-quality
reconstruction, including 1) a multi-level geometry reasoning framework to
recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color
blending scheme for more reliable color prediction; 3) a consistency-aware
fine-tuning scheme to control the inconsistent regions caused by occlusion and
noise. Extensive experiments demonstrate that our approach not only outperforms
the state-of-the-art methods, but also exhibits good efficiency,
generalizability, and flexibility."
Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization,0.889543,"Vision transformers (ViTs) are emerging with significantly improved accuracy
in computer vision tasks. However, their complex architecture and enormous
computation/storage demand impose urgent needs for new hardware accelerator
design methodology. This work proposes an FPGA-aware automatic ViT acceleration
framework based on the proposed mixed-scheme quantization. To the best of our
knowledge, this is the first FPGA-based ViT acceleration framework exploring
model quantization. Compared with state-of-the-art ViT quantization work
(algorithmic approach only without hardware acceleration), our quantization
achieves 0.47% to 1.36% higher Top-1 accuracy under the same bit-width.
Compared with the 32-bit floating-point baseline FPGA accelerator, our
accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS
vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base."
Watching the News: Towards VideoQA Models that can Read,0.950213,"Video Question Answering methods focus on commonsense reasoning and visual
cognition of objects or persons and their interactions over time. Current
VideoQA approaches ignore the textual information present in the video.
Instead, we argue that textual information is complementary to the action and
provides essential contextualisation cues to the reasoning process. To this
end, we propose a novel VideoQA task that requires reading and understanding
the text in the video. To explore this direction, we focus on news videos and
require QA systems to comprehend and answer questions about the topics
presented by combining visual and textual cues in the video. We introduce the
``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$
news videos obtained from diverse news channels from around the world. We
demonstrate the limitations of current Scene Text VQA and VideoQA methods and
propose ways to incorporate scene text information into VideoQA methods."
Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior,0.942709,"Learned locomotion policies can rapidly adapt to diverse environments similar
to those experienced during training but lack a mechanism for fast tuning when
they fail in an out-of-distribution test environment. This necessitates a slow
and iterative cycle of reward and environment redesign to achieve good
performance on a new task. As an alternative, we propose learning a single
policy that encodes a structured family of locomotion strategies that solve
training tasks in different ways, resulting in Multiplicity of Behavior (MoB).
Different strategies generalize differently and can be chosen in real-time for
new tasks or environments, bypassing the need for time-consuming retraining. We
release a fast, robust open-source MoB locomotion controller, Walk These Ways,
that can execute diverse gaits with variable footswing, posture, and speed,
unlocking diverse downstream tasks: crouching, hopping, high-speed running,
stair traversal, bracing against shoves, rhythmic dance, and more. Video and
code release: https://gmargo11.github.io/walk-these-ways/"
MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity Recognition,0.999854,"We present MultiCoNER, a large multilingual dataset for Named Entity
Recognition that covers 3 domains (Wiki sentences, questions, and search
queries) across 11 languages, as well as multilingual and code-mixing subsets.
This dataset is designed to represent contemporary challenges in NER, including
low-context scenarios (short and uncased text), syntactically complex entities
like movie titles, and long-tail entity distributions. The 26M token dataset is
compiled from public resources using techniques such as heuristic-based
sentence sampling, template extraction and slotting, and machine translation.
We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a
state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves
moderate performance (macro-F1=54%), highlighting the difficulty of our data.
GEMNET, which uses gazetteers, improvement significantly (average improvement
of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained
language models, and we believe that it can help further research in building
robust NER systems. MultiCoNER is publicly available at
https://registry.opendata.aws/multiconer/ and we hope that this resource will
help advance research in various aspects of NER."
"Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot Challenge on Conversational Task Assistance",0.993697,"Since its inception in 2016, the Alexa Prize program has enabled hundreds of
university students to explore and compete to develop conversational agents
through the SocialBot Grand Challenge. The goal of the challenge is to build
agents capable of conversing coherently and engagingly with humans on popular
topics for 20 minutes, while achieving an average rating of at least 4.0/5.0.
However, as conversational agents attempt to assist users with increasingly
complex tasks, new conversational AI techniques and evaluation platforms are
needed. The Alexa Prize TaskBot challenge, established in 2021, builds on the
success of the SocialBot challenge by introducing the requirements of
interactively assisting humans with real-world Cooking and Do-It-Yourself
tasks, while making use of both voice and visual modalities. This challenge
requires the TaskBots to identify and understand the user's need, identify and
integrate task and domain knowledge into the interaction, and develop new ways
of engaging the user without distracting them from the task at hand, among
other challenges. This paper provides an overview of the TaskBot challenge,
describes the infrastructure support provided to the teams with the CoBot
Toolkit, and summarizes the approaches the participating teams took to overcome
the research challenges. Finally, it analyzes the performance of the competing
TaskBots during the first year of the competition."
Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer,0.926157,"Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html."
Ontology-enhanced Prompt-tuning for Few-shot Learning,0.99755,"Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines."
FlowFormer: A Transformer Architecture for Optical Flow,0.998661,"We introduce optical Flow transFormer, dubbed as FlowFormer, a
transformer-based neural network architecture for learning optical flow.
FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the
cost tokens into a cost memory with alternate-group transformer (AGT) layers in
a novel latent space, and decodes the cost memory via a recurrent transformer
decoder with dynamic positional cost queries. On the Sintel benchmark,
FlowFormer achieves 1.159 and 2.088 average end-point-error (AEPE) on the clean
and final pass, a 16.5% and 15.5% error reduction from the best published
result (1.388 and 2.47). Besides, FlowFormer also achieves strong
generalization performance. Without being trained on Sintel, FlowFormer
achieves 1.01 AEPE on the clean pass of Sintel training set, outperforming the
best published result (1.29) by 21.7%."
Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,0.999184,"We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work."
ComFact: A Benchmark for Linking Contextual Commonsense Knowledge,0.981684,"Understanding rich narratives, such as dialogues and stories, often requires
natural language processing systems to access relevant knowledge from
commonsense knowledge graphs. However, these systems typically retrieve facts
from KGs using simple heuristics that disregard the complex challenges of
identifying situationally-relevant commonsense knowledge (e.g.,
contextualization, implicitness, ambiguity).
  In this work, we propose the new task of commonsense fact linking, where
models are given contexts and trained to identify situationally-relevant
commonsense knowledge from KGs. Our novel benchmark, ComFact, contains ~293k
in-context relevance annotations for commonsense triplets across four
stylistically diverse dialogue and storytelling datasets. Experimental results
confirm that heuristic fact linking approaches are imprecise knowledge
extractors. Learned fact linking models demonstrate across-the-board
performance improvements (~34.6% F1) over these heuristics. Furthermore,
improved knowledge retrieval yielded average downstream improvements of 9.8%
for a dialogue response generation task. However, fact linking models still
significantly underperform humans, suggesting our benchmark is a promising
testbed for research in commonsense augmentation of NLP systems."
Continuous diffusion for categorical data,0.987363,"Diffusion models have quickly become the go-to paradigm for generative
modelling of perceptual signals (such as images and sound) through iterative
refinement. Their success hinges on the fact that the underlying physical
phenomena are continuous. For inherently discrete and categorical data such as
language, various diffusion-inspired alternatives have been proposed. However,
the continuous nature of diffusion models conveys many benefits, and in this
work we endeavour to preserve it. We propose CDCD, a framework for modelling
categorical data with diffusion models that are continuous both in time and
input space. We demonstrate its efficacy on several language modelling tasks."
Robust Speech Recognition via Large-Scale Weak Supervision,0.999121,"We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing."
ETAD: Training Action Detection End to End on a Laptop,0.990596,"Temporal action detection (TAD) with end-to-end training often suffers from
the pain of huge demand for computing resources due to long video duration. In
this work, we propose an efficient temporal action detector (ETAD) that can
train directly from video frames with extremely low GPU memory consumption. Our
main idea is to minimize and balance the heavy computation among features and
gradients in each training iteration. We propose to sequentially forward the
snippet frame through the video encoder, and backward only a small necessary
portion of gradients to update the encoder. To further alleviate the
computational redundancy in training, we propose to dynamically sample only a
small subset of proposals during training. Moreover, various sampling
strategies and ratios are studied for both the encoder and detector. ETAD
achieves state-of-the-art performance on TAD benchmarks with remarkable
efficiency. On ActivityNet-1.3, training ETAD in 18 hours can reach 38.25%
average mAP with only 1.3 GB memory consumption per video under end-to-end
training. Our code will be publicly released."
DeepGen: Diverse Search Ad Generation and Real-Time Customization,0.950213,"We present DeepGen, a system deployed at web scale for automatically creating
sponsored search advertisements (ads) for BingAds customers. We leverage
state-of-the-art natural language generation (NLG) models to generate fluent
ads from advertiser's web pages in an abstractive fashion and solve practical
issues such as factuality and inference speed. In addition, our system creates
a customized ad in real-time in response to the user's search query, therefore
highlighting different aspects of the same product based on what the user is
looking for. To achieve this, our system generates a diverse choice of smaller
pieces of the ad ahead of time and, at query time, selects the most relevant
ones to be stitched into a complete ad. We improve generation diversity by
training a controllable NLG model to generate multiple ads for the same web
page highlighting different selling points. Our system design further improves
diversity horizontally by first running an ensemble of generation models
trained with different objectives and then using a diversity sampling algorithm
to pick a diverse subset of generation results for online selection.
Experimental results show the effectiveness of our proposed system design. Our
system is currently deployed in production, serving ${\sim}4\%$ of global ads
served in Bing."
Exploring Visual Prompts for Adapting Large-Scale Models,0.982586,"We investigate the efficacy of visual prompting to adapt large-scale models
in vision. Following the recent approach from prompt tuning and adversarial
reprogramming, we learn a single image perturbation such that a frozen model
prompted with this perturbation performs a new task. Through comprehensive
experiments, we demonstrate that visual prompting is particularly effective for
CLIP and robust to distribution shift, achieving performance competitive with
standard linear probes. We further analyze properties of the downstream
dataset, prompt design, and output transformation in regard to adaptation
performance. The surprising effectiveness of visual prompting provides a new
perspective on adapting pre-trained models in vision. Code is available at
http://hjbahng.github.io/visual_prompting ."
Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,0.996733,"We present Bit Diffusion: a simple and generic approach for generating
discrete data with continuous state and continuous time diffusion models. The
main idea behind our approach is to first represent the discrete data as binary
bits, and then train a continuous diffusion model to model these bits as real
numbers which we call analog bits. To generate samples, the model first
generates the analog bits, which are then thresholded to obtain the bits that
represent the discrete variables. We further propose two simple techniques,
namely Self-Conditioning and Asymmetric Time Intervals, which lead to a
significant improvement in sample quality. Despite its simplicity, the proposed
approach can achieve strong performance in both discrete image generation and
image captioning tasks. For discrete image generation, we significantly improve
previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)
and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the
best autoregressive model in both sample quality (measured by FID) and
efficiency. For image captioning on MS-COCO dataset, our approach achieves
competitive results compared to autoregressive models."
SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model,0.981684,"Generic image inpainting aims to complete a corrupted image by borrowing
surrounding information, which barely generates novel content. By contrast,
multi-modal inpainting provides more flexible and useful controls on the
inpainted content, \eg, a text prompt can be used to describe an object with
richer attributes, and a mask can be used to constrain the shape of the
inpainted object rather than being only considered as a missing area. We
propose a new diffusion-based model named SmartBrush for completing a missing
region with an object using both text and shape-guidance. While previous work
such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not
support shape guidance and tend to modify background texture surrounding the
generated object. Our model incorporates both text and shape guidance with
precision control. To preserve the background better, we propose a novel
training and sampling strategy by augmenting the diffusion U-net with
object-mask prediction. Lastly, we introduce a multi-task training strategy by
jointly training inpainting with text-to-image generation to leverage more
training data. We conduct extensive experiments showing that our model
outperforms all baselines in terms of visual quality, mask controllability, and
background preservation."
COLD: A Benchmark for Chinese Offensive Language Detection,0.896621,"Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier."
MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer,0.950213,"Self-supervised monocular depth estimation is an attractive solution that
does not require hard-to-source depth labels for training. Convolutional neural
networks (CNNs) have recently achieved great success in this task. However,
their limited receptive field constrains existing network architectures to
reason only locally, dampening the effectiveness of the self-supervised
paradigm. In the light of the recent successes achieved by Vision Transformers
(ViTs), we propose MonoViT, a brand-new framework combining the global
reasoning enabled by ViT models with the flexibility of self-supervised
monocular depth estimation. By combining plain convolutions with Transformer
blocks, our model can reason locally and globally, yielding depth prediction at
a higher level of detail and accuracy, allowing MonoViT to achieve
state-of-the-art performance on the established KITTI dataset. Moreover,
MonoViT proves its superior generalization capacities on other datasets such as
Make3D and DrivingStereo."
REKnow: Enhanced Knowledge for Joint Entity and Relation Extraction,0.950213,"Relation extraction is an important but challenging task that aims to extract
all hidden relational facts from the text. With the development of deep
language models, relation extraction methods have achieved good performance on
various benchmarks. However, we observe two shortcomings of previous methods:
first, there is no unified framework that works well under various relation
extraction settings; second, effectively utilizing external knowledge as
background information is absent. In this work, we propose a knowledge-enhanced
generative model to mitigate these two issues. Our generative model is a
unified framework to sequentially generate relational triplets under various
relation extraction settings and explicitly utilizes relevant knowledge from
Knowledge Graph (KG) to resolve ambiguities. Our model achieves superior
performance on multiple benchmarks and settings, including WebNLG, NYT10, and
TACRED."
Multi-class Token Transformer for Weakly Supervised Semantic Segmentation,0.959163,"This paper proposes a new transformer-based framework to learn class-specific
object localization maps as pseudo labels for weakly supervised semantic
segmentation (WSSS). Inspired by the fact that the attended regions of the
one-class token in the standard vision transformer can be leveraged to form a
class-agnostic localization map, we investigate if the transformer model can
also effectively capture class-specific attention for more discriminative
object localization by learning multiple class tokens within the transformer.
To this end, we propose a Multi-class Token Transformer, termed as MCTformer,
which uses multiple class tokens to learn interactions between the class tokens
and the patch tokens. The proposed MCTformer can successfully produce
class-discriminative object localization maps from class-to-patch attentions
corresponding to different class tokens. We also propose to use a patch-level
pairwise affinity, which is extracted from the patch-to-patch transformer
attention, to further refine the localization maps. Moreover, the proposed
framework is shown to fully complement the Class Activation Mapping (CAM)
method, leading to remarkably superior WSSS results on the PASCAL VOC and MS
COCO datasets. These results underline the importance of the class token for
WSSS."
STEdge: Self-training Edge Detection with Multi-layer Teaching and Regularization,0.950213,"Learning-based edge detection has hereunto been strongly supervised with
pixel-wise annotations which are tedious to obtain manually. We study the
problem of self-training edge detection, leveraging the untapped wealth of
large-scale unlabeled image datasets. We design a self-supervised framework
with multi-layer regularization and self-teaching. In particular, we impose a
consistency regularization which enforces the outputs from each of the multiple
layers to be consistent for the input image and its perturbed counterpart. We
adopt L0-smoothing as the 'perturbation' to encourage edge prediction lying on
salient boundaries following the cluster assumption in self-supervised
learning. Meanwhile, the network is trained with multi-layer supervision by
pseudo labels which are initialized with Canny edges and then iteratively
refined by the network as the training proceeds. The regularization and
self-teaching together attain a good balance of precision and recall, leading
to a significant performance boost over supervised methods, with lightweight
refinement on the target dataset. Furthermore, our method demonstrates strong
cross-dataset generality. For example, it attains 4.8% improvement for ODS and
5.8% for OIS when tested on the unseen BIPED dataset, compared to the
state-of-the-art methods."
PETR: Position Embedding Transformation for Multi-View 3D Object Detection,0.910751,"In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}."
VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments,0.900509,"Visual Place Recognition and Visual Localization are essential components in
navigation and mapping for autonomous vehicles especially in GNSS-denied
navigation scenarios. Recent work has focused on ground or close to ground
applications such as self-driving cars or indoor-scenarios and low-altitude
drone flights. However, applications such as Urban Air Mobility require
operations in large-scale outdoor environments at medium to high altitudes. We
present a new dataset named VPAIR. The dataset was recorded on board a light
aircraft flying at an altitude of more than 300 meters above ground capturing
images with a downwardfacing camera. Each image is paired with a high
resolution reference render including dense depth information and 6-DoF
reference poses. The dataset covers a more than one hundred kilometers long
trajectory over various types of challenging landscapes, e.g. urban, farmland
and forests. Experiments on this dataset illustrate the challenges introduced
by the change in perspective to a bird's eye view such as in-plane rotations."
TabLLM: Few-shot Classification of Tabular Data with Large Language Models,0.980842,"We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting."
NeurMiPs: Neural Mixture of Planar Experts for View Synthesis,0.980607,"We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based
scene representation for modeling geometry and appearance. NeurMiPs leverages a
collection of local planar experts in 3D space as the scene representation.
Each planar expert consists of the parameters of the local rectangular shape
representing geometry and a neural radiance field modeling the color and
opacity. We render novel views by calculating ray-plane intersections and
composite output colors and densities at intersected points to the image.
NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of
the neural radiance field. Experiments demonstrate superior performance and
speed of our proposed method, compared to other 3D representations in novel
view synthesis."
TimeLMs: Diachronic Language Models from Twitter,0.99188,"Despite its importance, the time variable has been largely neglected in the
NLP and language model literature. In this paper, we present TimeLMs, a set of
language models specialized on diachronic Twitter data. We show that a
continual learning strategy contributes to enhancing Twitter-based language
models' capacity to deal with future and out-of-distribution tweets, while
making them competitive with standardized and more monolithic benchmarks. We
also perform a number of qualitative analyses showing how they cope with trends
and peaks in activity involving specific named entities or concept drift."
Concept Bottleneck Model with Additional Unsupervised Concepts,0.901364,"With the increasing demands for accountability, interpretability is becoming
an essential capability for real-world AI applications. However, most methods
utilize post-hoc approaches rather than training the interpretable model. In
this article, we propose a novel interpretable model based on the concept
bottleneck model (CBM). CBM uses concept labels to train an intermediate layer
as the additional visible layer. However, because the number of concept labels
restricts the dimension of this layer, it is difficult to obtain high accuracy
with a small number of labels. To address this issue, we integrate supervised
concepts with unsupervised ones trained with self-explaining neural networks
(SENNs). By seamlessly training these two types of concepts while reducing the
amount of computation, we can obtain both supervised and unsupervised concepts
simultaneously, even for large-sized images. We refer to the proposed model as
the concept bottleneck model with additional unsupervised concepts (CBM-AUC).
We experimentally confirmed that the proposed model outperformed CBM and SENN.
We also visualized the saliency map of each concept and confirmed that it was
consistent with the semantic meanings."
Locating and Editing Factual Associations in GPT,0.980348,"We analyze the storage and recall of factual associations in autoregressive
transformer language models, finding evidence that these associations
correspond to localized, directly-editable computations. We first develop a
causal intervention for identifying neuron activations that are decisive in a
model's factual predictions. This reveals a distinct set of steps in
middle-layer feed-forward modules that mediate factual predictions while
processing subject tokens. To test our hypothesis that these computations
correspond to factual association recall, we modify feed-forward weights to
update specific factual associations using Rank-One Model Editing (ROME). We
find that ROME is effective on a standard zero-shot relation extraction (zsRE)
model-editing task, comparable to existing methods. To perform a more sensitive
evaluation, we also evaluate ROME on a new dataset of counterfactual
assertions, on which it simultaneously maintains both specificity and
generalization, whereas other methods sacrifice one or another. Our results
confirm an important role for mid-layer feed-forward modules in storing factual
associations and suggest that direct manipulation of computational mechanisms
may be a feasible approach for model editing. The code, dataset,
visualizations, and an interactive demo notebook are available at
https://rome.baulab.info/"
UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes,0.930517,"We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision."
BabyBear: Cheap inference triage for expensive language models,0.950213,"Transformer language models provide superior accuracy over previous models
but they are computationally and environmentally expensive. Borrowing the
concept of model cascading from computer vision, we introduce BabyBear, a
framework for cascading models for natural language processing (NLP) tasks to
minimize cost. The core strategy is inference triage, exiting early when the
least expensive model in the cascade achieves a sufficiently high-confidence
prediction. We test BabyBear on several open source data sets related to
document classification and entity recognition. We find that for common NLP
tasks a high proportion of the inference load can be accomplished with cheap,
fast models that have learned by observing a deep learning model. This allows
us to reduce the compute cost of large-scale classification jobs by more than
50% while retaining overall accuracy. For named entity recognition, we save 33%
of the deep learning compute while maintaining an F1 score higher than 95% on
the CoNLL benchmark."
Large Language Models Are Reasoning Teachers,0.974122,"Recent works have shown that chain-of-thought (CoT) prompting can elicit
language models to solve complex reasoning tasks, step-by-step. However,
prompt-based CoT methods are dependent on very large models such as GPT-3 175B
which are prohibitive to deploy at scale. In this paper, we use these large
models as reasoning teachers to enable complex reasoning in smaller models and
reduce model size requirements by several orders of magnitude. We propose
Fine-tune-CoT, a method that generates reasoning samples from very large
teacher models to fine-tune smaller models. We evaluate our method on a wide
range of public models and complex tasks. We find that Fine-tune-CoT enables
substantial reasoning capability in small models, far outperforming
prompt-based baselines and even the teacher model in many tasks. Additionally,
we extend our method by leveraging the teacher model's ability to generate
multiple distinct rationales for each original sample. Enriching the
fine-tuning data with such diverse reasoning results in a substantial
performance boost across datasets, even for very small models. We conduct
ablations and sample studies to understand the emergence of reasoning
capabilities of student models. Our code implementation and data are available
at https://github.com/itsnamgyu/reasoning-teacher."
Flexible Diffusion Modeling of Long Videos,0.933456,"We present a framework for video modeling based on denoising diffusion
probabilistic models that produces long-duration video completions in a variety
of realistic environments. We introduce a generative model that can at
test-time sample any arbitrary subset of video frames conditioned on any other
subset and present an architecture adapted for this purpose. Doing so allows us
to efficiently compare and optimize a variety of schedules for the order in
which frames in a long video are sampled and use selective sparse and
long-range conditioning on previously sampled frames. We demonstrate improved
video modeling over prior work on a number of datasets and sample temporally
coherent videos over 25 minutes in length. We additionally release a new video
modeling dataset and semantically meaningful metrics based on videos generated
in the CARLA autonomous driving simulator."
Black-Box Tuning for Language-Model-as-a-Service,0.950062,"Extremely large pre-trained language models (PTMs) such as GPT-3 are usually
released as a service. It allows users to design task-specific prompts to query
the PTMs through some black-box APIs. In such a scenario, which we call
Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually
unavailable. Can we optimize the task prompts by only accessing the model
inference APIs? This paper proposes the black-box tuning framework to optimize
the continuous prompt prepended to the input text via derivative-free
optimization. Instead of optimizing in the original high-dimensional prompt
space, which is intractable for traditional derivative-free optimization, we
perform optimization in a randomly generated subspace due to the low intrinsic
dimensionality of large PTMs. The experimental results show that the black-box
tuning with RoBERTa on a few labeled samples not only significantly outperforms
manual prompt and GPT-3's in-context learning, but also surpasses the
gradient-based counterparts, i.e., prompt tuning and full model tuning."
MDFEND: Multi-domain Fake News Detection,0.989939,"Fake news spread widely on social media in various domains, which lead to
real-world threats in many aspects like politics, disasters, and finance. Most
existing approaches focus on single-domain fake news detection (SFND), which
leads to unsatisfying performance when these methods are applied to
multi-domain fake news detection. As an emerging field, multi-domain fake news
detection (MFND) is increasingly attracting attention. However, data
distributions, such as word frequency and propagation patterns, vary from
domain to domain, namely domain shift. Facing the challenge of serious domain
shift, existing fake news detection techniques perform poorly for multi-domain
scenarios. Therefore, it is demanding to design a specialized model for MFND.
In this paper, we first design a benchmark of fake news dataset for MFND with
domain label annotated, namely Weibo21, which consists of 4,488 fake news and
4,640 real news from 9 different domains. We further propose an effective
Multi-domain Fake News Detection Model (MDFEND) by utilizing a domain gate to
aggregate multiple representations extracted by a mixture of experts. The
experiments show that MDFEND can significantly improve the performance of
multi-domain fake news detection. Our dataset and code are available at
https://github.com/kennqiang/MDFEND-Weibo21."
MAESTRO: Matched Speech Text Representations through Modality Matching,0.99355,"We present Maestro, a self-supervised training method to unify
representations learnt from speech and text modalities. Self-supervised
learning from speech signals aims to learn the latent structure inherent in the
signal, while self-supervised learning from text attempts to capture lexical
information. Learning aligned representations from unpaired speech and text
sequences is a challenging task. Previous work either implicitly enforced the
representations learnt from these two modalities to be aligned in the latent
space through multitasking and parameter sharing or explicitly through
conversion of modalities via speech synthesis. While the former suffers from
interference between the two modalities, the latter introduces additional
complexity. In this paper, we propose Maestro, a novel algorithm to learn
unified representations from both these modalities simultaneously that can
transfer to diverse downstream tasks such as Automated Speech Recognition (ASR)
and Speech Translation (ST). Maestro learns unified representations through
sequence alignment, duration prediction and matching embeddings in the learned
space through an aligned masked-language model loss. We establish a new
state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative
reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)
and 21 languages to English multilingual ST on CoVoST 2 with an improvement of
2.8 BLEU averaged over 21 languages."
Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection,0.909282,"Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec."
SecureBERT: A Domain-Specific Language Model for Cybersecurity,0.9682,"Natural Language Processing (NLP) has recently gained wide attention in
cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber
automation. Increased connection and automation have revolutionized the world's
economic and cultural infrastructures, while they have introduced risks in
terms of cyber attacks. CTI is information that helps cybersecurity analysts
make intelligent security decisions, that is often delivered in the form of
natural language text, which must be transformed to machine readable format
through an automated procedure before it can be used for automated security
measures.
  This paper proposes SecureBERT, a cybersecurity language model capable of
capturing text connotations in cybersecurity text (e.g., CTI) and therefore
successful in automation for many critical cybersecurity tasks that would
otherwise rely on human expertise and time-consuming manual efforts. SecureBERT
has been trained using a large corpus of cybersecurity text.To make SecureBERT
effective not just in retaining general English understanding, but also when
applied to text with cybersecurity implications, we developed a customized
tokenizer as well as a method to alter pre-trained weights. The SecureBERT is
evaluated using the standard Masked Language Model (MLM) test as well as two
additional standard NLP tasks. Our evaluation studies show that
SecureBERT\footnote{\url{https://github.com/ehsanaghaei/SecureBERT}}
outperforms existing similar models, confirming its capability for solving
crucial NLP tasks in cybersecurity."
"Continual Learning, Fast and Slow",0.90107,"According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}."
Polysemanticity and Capacity in Neural Networks,0.933014,"Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons."
Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning,0.916967,"Wordle is an online word puzzle game that gained viral popularity in January
2022. The goal is to guess a hidden five letter word. After each guess, the
player gains information about whether the letters they guessed are present in
the word, and whether they are in the correct position. Numerous blogs have
suggested guessing strategies and starting word lists that improve the chance
of winning. Optimized algorithms can win 100% of games within five of the six
allowed trials. However, it is infeasible for human players to use these
algorithms due to an inability to perfectly recall all known 5-letter words and
perform complex calculations that optimize information gain. Here, we present
two different methods for choosing starting words along with a framework for
discovering the optimal human strategy based on reinforcement learning. Human
Wordle players can use the rules we discover to optimize their chance of
winning."
Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation,0.918898,"Capturing emotions within a conversation plays an essential role in modern
dialogue systems. However, the weak correlation between emotions and semantics
brings many challenges to emotion recognition in conversation (ERC). Even
semantically similar utterances, the emotion may vary drastically depending on
contexts or speakers. In this paper, we propose a Supervised Prototypical
Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical
Network, the SPCL targets at solving the imbalanced classification problem
through contrastive learning and does not require a large batch size.
Meanwhile, we design a difficulty measure function based on the distance
between classes and introduce curriculum learning to alleviate the impact of
extreme samples. We achieve state-of-the-art results on three widely used
benchmarks. Further, we conduct analytical experiments to demonstrate the
effectiveness of our proposed SPCL and curriculum learning strategy. We release
the code at https://github.com/caskcsg/SPCL."
Solving Quantitative Reasoning Problems with Language Models,0.996592,"Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them."
Large Language Models Struggle to Learn Long-Tail Knowledge,0.939386,"The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail."
"Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers",0.971035,"Autograding short textual answers has become much more feasible due to the
rise of NLP and the increased availability of question-answer pairs brought
about by a shift to online education. Autograding performance is still inferior
to human grading. The statistical and black-box nature of state-of-the-art
machine learning models makes them untrustworthy, raising ethical concerns and
limiting their practical utility. Furthermore, the evaluation of autograding is
typically confined to small, monolingual datasets for a specific question type.
This study uses a large dataset consisting of about 10 million question-answer
pairs from multiple languages covering diverse fields such as math and
language, and strong variation in question and answer syntax. We demonstrate
the effectiveness of fine-tuning transformer models for autograding for such
complex datasets. Our best hyperparameter-tuned model yields an accuracy of
about 86.5\%, comparable to the state-of-the-art models that are less general
and more tuned to a specific type of question, subject, and language. More
importantly, we address trust and ethical concerns. By involving humans in the
autograding process, we show how to improve the accuracy of automatically
graded answers, achieving accuracy equivalent to that of teaching assistants.
We also show how teachers can effectively control the type of errors made by
the system and how they can validate efficiently that the autograder's
performance on individual exams is close to the expected performance."
Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities,0.999482,"Assembly101 is a new procedural activity dataset featuring 4321 videos of
people assembling and disassembling 101 ""take-apart"" toy vehicles. Participants
work without fixed instructions, and the sequences feature rich and natural
variations in action ordering, mistakes, and corrections. Assembly101 is the
first multi-view action dataset, with simultaneous static (8) and egocentric
(4) recordings. Sequences are annotated with more than 100K coarse and 1M
fine-grained action segments, and 18M 3D hand poses. We benchmark on three
action understanding tasks: recognition, anticipation and temporal
segmentation. Additionally, we propose a novel task of detecting mistakes. The
unique recording format and rich set of annotations allow us to investigate
generalization to new toys, cross-view transfer, long-tailed distributions, and
pose vs. appearance. We envision that Assembly101 will serve as a new challenge
to investigate various activity understanding problems."
BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision,0.901585,"We present a novel bird's-eye-view (BEV) detector with perspective
supervision, which converges faster and better suits modern image backbones.
Existing state-of-the-art BEV detectors are often tied to certain depth
pre-trained backbones like VoVNet, hindering the synergy between booming image
backbones and BEV detectors. To address this limitation, we prioritize easing
the optimization of BEV detectors by introducing perspective space supervision.
To this end, we propose a two-stage BEV detector, where proposals from the
perspective head are fed into the bird's-eye-view head for final predictions.
To evaluate the effectiveness of our model, we conduct extensive ablation
studies focusing on the form of supervision and the generality of the proposed
detector. The proposed method is verified with a wide spectrum of traditional
and modern image backbones and achieves new SoTA results on the large-scale
nuScenes dataset. The code shall be released soon."
Evaluating the Text-to-SQL Capabilities of Large Language Models,0.90452,"We perform an empirical evaluation of Text-to-SQL capabilities of the Codex
language model. We find that, without any finetuning, Codex is a strong
baseline on the Spider benchmark; we also analyze the failure modes of Codex in
this setting. Furthermore, we demonstrate on the GeoQuery and Scholar
benchmarks that a small number of in-domain examples provided in the prompt
enables Codex to perform better than state-of-the-art models finetuned on such
few-shot examples."
PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Transformers for Patronizing and Condescending Language Detection,0.897292,"Patronizing and condescending language (PCL) has a large harmful impact and
is difficult to detect, both for human judges and existing NLP systems. At
SemEval-2022 Task 4, we propose a novel Transformer-based model and its
ensembles to accurately understand such language context for PCL detection. To
facilitate comprehension of the subtle and subjective nature of PCL, two
fine-tuning strategies are applied to capture discriminative features from
diverse linguistic behaviour and categorical distribution. The system achieves
remarkable results on the official ranking, including 1st in Subtask 1 and 5th
in Subtask 2. Extensive experiments on the task demonstrate the effectiveness
of our system and its strategies."
"Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report",0.998624,"In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines."
ConvMAE: Masked Convolution Meets Masked Autoencoders,0.950213,"Vision Transformers (ViT) become widely-adopted architectures for various
vision tasks. Masked auto-encoding for feature pretraining and multi-scale
hybrid convolution-transformer architectures can further unleash the potentials
of ViT, leading to state-of-the-art performances on image classification,
detection and semantic segmentation. In this paper, our ConvMAE framework
demonstrates that multi-scale hybrid convolution-transformer can learn more
discriminative representations via the mask auto-encoding scheme. However,
directly using the original masking strategy leads to the heavy computational
cost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the
masked convolution to prevent information leakage in the convolution blocks. A
simple block-wise masking strategy is proposed to ensure computational
efficiency. We also propose to more directly supervise the multi-scale features
of the encoder to boost multi-scale features. Based on our pretrained ConvMAE
models, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared
with MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs
surpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP
respectively. Code and pretrained models are available at
https://github.com/Alpha-VL/ConvMAE."
Vision Transformers for Single Image Dehazing,0.998446,"Image dehazing is a representative low-level vision task that estimates
latent haze-free images from hazy images. In recent years, convolutional neural
network-based methods have dominated image dehazing. However, vision
Transformers, which has recently made a breakthrough in high-level vision
tasks, has not brought new dimensions to image dehazing. We start with the
popular Swin Transformer and find that several of its key designs are
unsuitable for image dehazing. To this end, we propose DehazeFormer, which
consists of various improvements, such as the modified normalization layer,
activation function, and spatial information aggregation scheme. We train
multiple variants of DehazeFormer on various datasets to demonstrate its
effectiveness. Specifically, on the most frequently used SOTS indoor set, our
small model outperforms FFA-Net with only 25% #Param and 5% computational cost.
To the best of our knowledge, our large model is the first method with the PSNR
over 40 dB on the SOTS indoor set, dramatically outperforming the previous
state-of-the-art methods. We also collect a large-scale realistic remote
sensing dehazing dataset for evaluating the method's capability to remove
highly non-homogeneous haze."
AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation,0.967567,"To alleviate the data scarcity problem in End-to-end speech translation (ST),
pre-training on data for speech recognition and machine translation is
considered as an important technique. However, the modality gap between speech
and text prevents the ST model from efficiently inheriting knowledge from the
pre-trained models. In this work, we propose AdaTranS for end-to-end ST. It
adapts the speech features with a new shrinking mechanism to mitigate the
length mismatch between speech and text features by predicting word boundaries.
Experiments on the MUST-C dataset demonstrate that AdaTranS achieves better
performance than the other shrinking-based methods, with higher inference speed
and lower memory usage. Further experiments also show that AdaTranS can be
equipped with additional alignment losses to further improve performance."
LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning,0.981343,"Fine-tuning large pre-trained models on downstream tasks has been adopted in
a variety of domains recently. However, it is costly to update the entire
parameter set of large pre-trained models. Although recently proposed
parameter-efficient transfer learning (PETL) techniques allow updating a small
subset of parameters (e.g. only using 2% of parameters) inside a pre-trained
backbone network for a new task, they only reduce the training memory
requirement by up to 30%. This is because the gradient computation for the
trainable parameters still requires backpropagation through the large
pre-trained backbone model. To address this, we propose Ladder Side-Tuning
(LST), a new PETL technique that can reduce training memory requirements by
more substantial amounts. Unlike existing parameter-efficient methods that
insert additional parameters inside backbone networks, we train a ladder side
network, a small and separate network that takes intermediate activations as
input via shortcut connections (called ladders) from backbone networks and
makes predictions. LST has significantly lower memory requirements than
previous methods, because it does not require backpropagation through the
backbone network, but instead only through the side network and ladder
connections. We evaluate our method with various models (T5 and CLIP-T5) on
both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST
saves 69% of the memory costs to fine-tune the whole network, while other
methods only save 26% of that in similar parameter usages (hence, 2.7x more
memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA
in a low-memory regime. To further show the advantage of this better memory
efficiency, we also apply LST to larger T5 models, attaining better GLUE
performance than full fine-tuning and other PETL methods. The
accuracy-efficiency trade-off also holds on VL tasks."
OneRel:Joint Entity and Relation Extraction with One Module in One Step,0.94397,"Joint entity and relation extraction is an essential task in natural language
processing and knowledge graph construction. Existing approaches usually
decompose the joint extraction task into several basic modules or processing
steps to make it easy to conduct. However, such a paradigm ignores the fact
that the three elements of a triple are interdependent and indivisible.
Therefore, previous joint methods suffer from the problems of cascading errors
and redundant information. To address these issues, in this paper, we propose a
novel joint entity and relation extraction model, named OneRel, which casts
joint extraction as a fine-grained triple classification problem. Specifically,
our model consists of a scoring-based classifier and a relation-specific horns
tagging strategy. The former evaluates whether a token pair and a relation
belong to a factual triple. The latter ensures a simple but effective decoding
process. Extensive experimental results on two widely used datasets demonstrate
that the proposed method performs better than the state-of-the-art baselines,
and delivers consistent performance gain on complex scenarios of various
overlapping patterns and multiple triples."
"An Exploratory Study of Tweets about the SARS-CoV-2 Omicron Variant: Insights from Sentiment Analysis, Language Interpretation, Source Tracking, Type Classification, and Embedded URL Detection",0.945253,"This paper presents the findings of an exploratory study on the continuously
generating Big Data on Twitter related to the sharing of information, news,
views, opinions, ideas, feedback, and experiences about the COVID-19 pandemic,
with a specific focus on the Omicron variant, which is the globally dominant
variant of SARS-CoV-2 at this time. A total of 12028 tweets about the Omicron
variant were studied, and the specific characteristics of tweets that were
analyzed include - sentiment, language, source, type, and embedded URLs. The
findings of this study are manifold. First, from sentiment analysis, it was
observed that 50.5% of tweets had a neutral emotion. The other emotions - bad,
good, terrible, and great were found in 15.6%, 14.0%, 12.5%, and 7.5% of the
tweets, respectively. Second, the findings of language interpretation showed
that 65.9% of the tweets were posted in English. It was followed by Spanish,
French, Italian, and other languages. Third, the findings from source tracking
showed that Twitter for Android was associated with 35.2% of tweets. It was
followed by Twitter Web App, Twitter for iPhone, Twitter for iPad, and other
sources. Fourth, studying the type of tweets revealed that retweets accounted
for 60.8% of the tweets, it was followed by original tweets and replies that
accounted for 19.8% and 19.4% of the tweets, respectively. Fifth, in terms of
embedded URL analysis, the most common domain embedded in the tweets was found
to be twitter.com, which was followed by biorxiv.org, nature.com, and other
domains. Finally, to support similar research in this field, we have developed
a Twitter dataset that comprises more than 500,000 tweets about the SARS-CoV-2
omicron variant since the first detected case of this variant on November 24,
2021."
FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing,0.952565,"We present a benchmark suite of four datasets for evaluating the fairness of
pre-trained language models and the techniques used to fine-tune them for
downstream tasks. Our benchmarks cover four jurisdictions (European Council,
USA, Switzerland, and China), five languages (English, German, French, Italian
and Chinese) and fairness across five attributes (gender, age, region,
language, and legal area). In our experiments, we evaluate pre-trained language
models using several group-robust fine-tuning techniques and show that
performance group disparities are vibrant in many cases, while none of these
techniques guarantee fairness, nor consistently mitigate group disparities.
Furthermore, we provide a quantitative and qualitative analysis of our results,
highlighting open challenges in the development of robustness methods in legal
NLP."
Few-Shot Diffusion Models,0.964063,"Denoising diffusion probabilistic models (DDPM) are powerful hierarchical
latent variable models with remarkable sample generation quality and training
stability. These properties can be attributed to parameter sharing in the
generative hierarchy, as well as a parameter-free diffusion-based inference
procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a
framework for few-shot generation leveraging conditional DDPMs. FSDMs are
trained to adapt the generative process conditioned on a small set of images
from a given class by aggregating image patch information using a set-based
Vision Transformer (ViT). At test time, the model is able to generate samples
from previously unseen classes conditioned on as few as 5 samples from that
class. We empirically show that FSDM can perform few-shot generation and
transfer to new datasets. We benchmark variants of our method on complex vision
datasets for few-shot learning and compare to unconditional and conditional
DDPM baselines. Additionally, we show how conditioning the model on patch-based
input set information improves training convergence."
Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,0.975478,"Social intelligence and Theory of Mind (ToM), i.e., the ability to reason
about the different mental states, intents, and reactions of all people
involved, allow humans to effectively navigate and understand everyday social
interactions. As NLP systems are used in increasingly complex social
situations, their ability to grasp social dynamics becomes crucial. In this
work, we examine the open question of social intelligence and Theory of Mind in
modern NLP systems from an empirical and theory-based perspective. We show that
one of today's largest language models (GPT-3; Brown et al., 2020) lacks this
kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et
al., 2019), which measures models' ability to understand intents and reactions
of participants of social interactions, and ToMi (Le et al., 2019), which
measures whether models can infer mental states and realities of participants
of situations. Our results show that models struggle substantially at these
Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on
SocialIQa and ToMi, respectively. To conclude, we draw on theories from
pragmatics to contextualize this shortcoming of large language models, by
examining the limitations stemming from their data, neural architecture, and
training paradigms. Challenging the prevalent narrative that only scale is
needed, we posit that person-centric NLP approaches might be more effective
towards neural Theory of Mind.
  In our updated version, we also analyze newer instruction tuned and RLFH
models for neural ToM. We find that even ChatGPT and GPT-4 do not display
emergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on
the ToMi questions related to mental states and realities."
Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images,0.910037,"In this paper, we present a generalizable model-free 6-DoF object pose
estimator called Gen6D. Existing generalizable pose estimators either need
high-quality object models or require additional depth maps or object masks in
test time, which significantly limits their application scope. In contrast, our
pose estimator only requires some posed images of the unseen object and is able
to accurately predict the poses of the object in arbitrary environments. Gen6D
consists of an object detector, a viewpoint selector and a pose refiner, all of
which do not require the 3D object model and can generalize to unseen objects.
Experiments show that Gen6D achieves state-of-the-art results on two model-free
datasets: the MOPED dataset and a new GenMOP dataset collected by us. In
addition, on the LINEMOD dataset, Gen6D achieves competitive results compared
with instance-specific pose estimators. Project page:
https://liuyuan-pal.github.io/Gen6D/."
Large Language Models are Few-Shot Clinical Information Extractors,0.904359,"A long-running goal of the clinical NLP community is the extraction of
important variables trapped in clinical notes. However, roadblocks have
included dataset shift from the general domain and a lack of public clinical
corpora and annotations. In this work, we show that large language models, such
as InstructGPT, perform well at zero- and few-shot information extraction from
clinical text despite not being trained specifically for the clinical domain.
Whereas text classification and generation performance have already been
studied extensively in such models, here we additionally demonstrate how to
leverage them to tackle a diverse set of NLP tasks which require more
structured outputs, including span identification, token-level sequence
classification, and relation extraction. Further, due to the dearth of
available data to evaluate these systems, we introduce new datasets for
benchmarking few-shot clinical information extraction based on a manual
re-annotation of the CASI dataset for new tasks. On the clinical extraction
tasks we studied, the GPT-3 systems significantly outperform existing zero- and
few-shot baselines."
HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding,0.995749,"Encoding a driving scene into vector representations has been an essential
task for autonomous driving that can benefit downstream tasks e.g. trajectory
prediction. The driving scene often involves heterogeneous elements such as the
different types of objects (agents, lanes, traffic signs) and the semantic
relations between objects are rich and diverse. Meanwhile, there also exist
relativity across elements, which means that the spatial relation is a relative
concept and need be encoded in a ego-centric manner instead of in a global
coordinate system. Based on these observations, we propose Heterogeneous
Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a
heterogeneous graph with different types of nodes and edges. For heterogeneous
graph construction, we connect different types of nodes according to diverse
semantic relations. For spatial relation encoding, the coordinates of the node
as well as its in-edges are in the local node-centric coordinate system. For
the aggregation module in the graph neural network (GNN), we adopt the
transformer structure in a hierarchical way to fit the heterogeneous nature of
inputs. Experimental results show that HDGT achieves state-of-the-art
performance for the task of trajectory prediction, on INTERACTION Prediction
Challenge and Waymo Open Motion Challenge."
InstructPix2Pix: Learning to Follow Image Editing Instructions,0.999999,"We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions."
Cross-Image Relational Knowledge Distillation for Semantic Segmentation,0.996302,"Current Knowledge Distillation (KD) methods for semantic segmentation often
guide the student to mimic the teacher's structured information generated from
individual data samples. However, they ignore the global semantic relations
among pixels across various images that are valuable for KD. This paper
proposes a novel Cross-Image Relational KD (CIRKD), which focuses on
transferring structured pixel-to-pixel and pixel-to-region relations among the
whole images. The motivation is that a good teacher network could construct a
well-structured feature space in terms of global pixel dependencies. CIRKD
makes the student mimic better structured semantic relations from the teacher,
thus improving the segmentation performance. Experimental results over
Cityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our
proposed approach against state-of-the-art distillation methods. The code is
available at https://github.com/winycg/CIRKD."
PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition,0.903028,"3D Point cloud is becoming a critical data representation in many real-world
applications like autonomous driving, robotics, and medical imaging. Although
the success of deep learning further accelerates the adoption of 3D point
clouds in the physical world, deep learning is notorious for its vulnerability
to adversarial attacks. In this work, we first identify that the
state-of-the-art empirical defense, adversarial training, has a major
limitation in applying to 3D point cloud models due to gradient obfuscation. We
further propose PointDP, a purification strategy that leverages diffusion
models to defend against 3D adversarial attacks. We extensively evaluate
PointDP on six representative 3D point cloud architectures, and leverage 10+
strong and adaptive attacks to demonstrate its lower-bound robustness. Our
evaluation shows that PointDP achieves significantly better robustness than
state-of-the-art purification methods under strong attacks. Results of
certified defenses on randomized smoothing combined with PointDP will be
included in the near future."
"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",0.944582,"We introduce Mintaka, a complex, natural, and multilingual dataset designed
for experimenting with end-to-end question-answering models. Mintaka is
composed of 20,000 question-answer pairs collected in English, annotated with
Wikidata entities, and translated into Arabic, French, German, Hindi, Italian,
Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka
includes 8 types of complex questions, including superlative, intersection, and
multi-hop questions, which were naturally elicited from crowd workers. We run
baselines over Mintaka, the best of which achieves 38% hits@1 in English and
31% hits@1 multilingually, showing that existing models have room for
improvement. We release Mintaka at https://github.com/amazon-research/mintaka."
ActionFormer: Localizing Moments of Actions with Transformers,0.999896,"Self-attention based Transformer models have demonstrated impressive results
for image classification and object detection, and more recently for video
understanding. Inspired by this success, we investigate the application of
Transformer networks for temporal action localization in videos. To this end,
we present ActionFormer -- a simple yet powerful model to identify actions in
time and recognize their categories in a single shot, without using action
proposals or relying on pre-defined anchor windows. ActionFormer combines a
multiscale feature representation with local self-attention, and uses a
light-weighted decoder to classify every moment in time and estimate the
corresponding action boundaries. We show that this orchestrated design results
in major improvements upon prior works. Without bells and whistles,
ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best
prior model by 14.1 absolute percentage points. Further, ActionFormer
demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and
EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available
at http://github.com/happyharrycn/actionformer_release."
Reasoning Like Program Executors,0.957496,"Reasoning over natural language is a long-standing goal for the research
community. However, studies have shown that existing language models are
inadequate in reasoning. To address the issue, we present POET, a novel
reasoning pre-training paradigm. Through pre-training language models with
programs and their execution results, POET empowers language models to harvest
the reasoning knowledge possessed by program executors via a data-driven
approach. POET is conceptually simple and can be instantiated by different
kinds of program executors. In this paper, we showcase two simple instances
POET-Math and POET-Logic, in addition to a complex instance, POET-SQL.
Experimental results on six benchmarks demonstrate that POET can significantly
boost model performance in natural language reasoning, such as numerical
reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on
reasoning-enhancement pre-training, and we hope our analysis would shed light
on the future research of reasoning like program executors."
Are Transformers Effective for Time Series Forecasting?,0.999938,"Recently, there has been a surge of Transformer-based solutions for the
long-term time series forecasting (LTSF) task. Despite the growing performance
over the past few years, we question the validity of this line of research in
this work. Specifically, Transformers is arguably the most successful solution
to extract the semantic correlations among the elements in a long sequence.
However, in time series modeling, we are to extract the temporal relations in
an ordered set of continuous points. While employing positional encoding and
using tokens to embed sub-series in Transformers facilitate preserving some
ordering information, the nature of the \emph{permutation-invariant}
self-attention mechanism inevitably results in temporal information loss. To
validate our claim, we introduce a set of embarrassingly simple one-layer
linear models named LTSF-Linear for comparison. Experimental results on nine
real-life datasets show that LTSF-Linear surprisingly outperforms existing
sophisticated Transformer-based LTSF models in all cases, and often by a large
margin. Moreover, we conduct comprehensive empirical studies to explore the
impacts of various design elements of LTSF models on their temporal relation
extraction capability. We hope this surprising finding opens up new research
directions for the LTSF task. We also advocate revisiting the validity of
Transformer-based solutions for other time series analysis tasks (e.g., anomaly
detection) in the future. Code is available at:
\url{https://github.com/cure-lab/LTSF-Linear}."
A Contrastive Framework for Neural Text Generation,0.921075,"Text generation is of great importance to many natural language processing
applications. However, maximization-based decoding methods (e.g. beam search)
of neural language models often lead to degenerate solutions -- the generated
text is unnatural and contains undesirable repetitions. Existing approaches
introduce stochasticity via sampling or modify training objectives to decrease
probabilities of certain tokens (e.g., unlikelihood training). However, they
often lead to solutions that lack coherence. In this work, we show that an
underlying reason for model degeneration is the anisotropic distribution of
token representations. We present a contrastive solution: (i) SimCTG, a
contrastive training objective to calibrate the model's representation space,
and (ii) a decoding method -- contrastive search -- to encourage diversity
while maintaining coherence in the generated text. Extensive experiments and
analyses on three benchmarks from two languages demonstrate that our proposed
approach significantly outperforms current state-of-the-art text generation
methods as evaluated by both human and automatic metrics."
Large Language Models are Zero-Shot Reasoners,0.999961,"Pretrained large language models (LLMs) are widely used in many sub-fields of
natural language processing (NLP) and generally known as excellent few-shot
learners with task-specific exemplars. Notably, chain of thought (CoT)
prompting, a recent technique for eliciting complex multi-step reasoning
through step-by-step answer examples, achieved the state-of-the-art
performances in arithmetics and symbolic reasoning, difficult system-2 tasks
that do not follow the standard scaling laws for LLMs. While these successes
are often attributed to LLMs' ability for few-shot learning, we show that LLMs
are decent zero-shot reasoners by simply adding ""Let's think step by step""
before each answer. Experimental results demonstrate that our Zero-shot-CoT,
using the same single prompt template, significantly outperforms zero-shot LLM
performances on diverse benchmark reasoning tasks including arithmetics
(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin
Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled
Objects), without any hand-crafted few-shot examples, e.g. increasing the
accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with
large InstructGPT model (text-davinci-002), as well as similar magnitudes of
improvements with another off-the-shelf large model, 540B parameter PaLM. The
versatility of this single prompt across very diverse reasoning tasks hints at
untapped and understudied fundamental zero-shot capabilities of LLMs,
suggesting high-level, multi-task broad cognitive capabilities may be extracted
by simple prompting. We hope our work not only serves as the minimal strongest
zero-shot baseline for the challenging reasoning benchmarks, but also
highlights the importance of carefully exploring and analyzing the enormous
zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or
few-shot exemplars."
TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving,0.99977,"How should we integrate representations from complementary sensors for
autonomous driving? Geometry-based fusion has shown promise for perception
(e.g. object detection, motion forecasting). However, in the context of
end-to-end driving, we find that imitation learning based on existing sensor
fusion methods underperforms in complex driving scenarios with a high density
of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate
image and LiDAR representations using self-attention. Our approach uses
transformer modules at multiple resolutions to fuse perspective view and bird's
eye view feature maps. We experimentally validate its efficacy on a challenging
new benchmark with long routes and dense traffic, as well as the official
leaderboard of the CARLA urban driving simulator. At the time of submission,
TransFuser outperforms all prior work on the CARLA leaderboard in terms of
driving score by a large margin. Compared to geometry-based fusion, TransFuser
reduces the average collisions per kilometer by 48%."
RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL,0.998381,"Relational structures such as schema linking and schema encoding have been
validated as a key component to qualitatively translating natural language into
SQL queries. However, introducing these structural relations comes with prices:
they often result in a specialized model structure, which largely prohibits
using large pretrained models in text-to-SQL. To address this problem, we
propose RASAT: a Transformer seq2seq architecture augmented with relation-aware
self-attention that could leverage a variety of relational structures while
inheriting the pretrained parameters from the T5 model effectively. Our model
can incorporate almost all types of existing relations in the literature, and
in addition, we propose introducing co-reference relations for the multi-turn
scenario. Experimental results on three widely used text-to-SQL datasets,
covering both single-turn and multi-turn scenarios, have shown that RASAT could
achieve state-of-the-art results across all three benchmarks (75.5% EX on
Spider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL)."
STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation,0.94351,"How to learn a better speech representation for end-to-end speech-to-text
translation (ST) with limited labeled data? Existing techniques often attempt
to transfer powerful machine translation (MT) capabilities to ST, but neglect
the representation discrepancy across modalities. In this paper, we propose the
Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.
Specifically, we mix up the representation sequences of different modalities,
and take both unimodal speech sequences and multimodal mixed sequences as input
to the translation model in parallel, and regularize their output predictions
with a self-learning framework. Experiments on MuST-C speech translation
benchmark and further analysis show that our method effectively alleviates the
cross-modal representation discrepancy, and achieves significant improvements
over a strong baseline on eight translation directions."
SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model,0.971035,"Data-driven speech processing models usually perform well with a large amount
of text supervision, but collecting transcribed speech data is costly.
Therefore, we propose SpeechCLIP, a novel framework bridging speech and text
through images to enhance speech models without transcriptions. We leverage
state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images
and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior
state-of-the-art on image-speech retrieval and performs zero-shot speech-text
retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP
can directly retrieve semantically related keywords from speech."
YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,1.0,"YOLOv7 surpasses all known object detectors in both speed and accuracy in the
range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all
known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6
object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based
detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed
and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask
R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as
well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR,
Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors
in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from
scratch without using any other datasets or pre-trained weights. Source code is
released in https://github.com/WongKinYiu/yolov7."
CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization,0.915096,"For a long time, anomaly localization has been widely used in industries.
Previous studies focused on approximating the distribution of normal features
without adaptation to a target dataset. However, since anomaly localization
should precisely discriminate normal and abnormal features, the absence of
adaptation may make the normality of abnormal features overestimated. Thus, we
propose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes
sophisticated anomaly localization using features adapted to the target
dataset. CFA consists of (1) a learnable patch descriptor that learns and
embeds target-oriented features and (2) scalable memory bank independent of the
size of the target dataset. And, CFA adopts transfer learning to increase the
normal feature density so that abnormal features can be clearly distinguished
by applying patch descriptor and memory bank to a pre-trained CNN. The proposed
method outperforms the previous methods quantitatively and qualitatively. For
example, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in
anomaly localization of MVTec AD benchmark. In addition, this paper points out
the negative effects of biased features of pre-trained CNNs and emphasizes the
importance of the adaptation to the target dataset. The code is publicly
available at https://github.com/sungwool/CFA_for_anomaly_localization."
Interactive Language: Talking to Robots in Real Time,0.89058,"We present a framework for building interactive, real-time, natural
language-instructable robots in the real world, and we open source related
assets (dataset, environment, benchmark, and policies). Trained with behavioral
cloning on a dataset of hundreds of thousands of language-annotated
trajectories, a produced policy can proficiently execute an order of magnitude
more commands than previous works: specifically we estimate a 93.5% success
rate on a set of 87,000 unique natural language strings specifying raw
end-to-end visuo-linguo-motor skills in the real world. We find that the same
policy is capable of being guided by a human via real-time language to address
a wide range of precise long-horizon rearrangement goals, e.g. ""make a smiley
face out of blocks"". The dataset we release comprises nearly 600,000
language-labeled trajectories, an order of magnitude larger than prior
available datasets. We hope the demonstrated results and associated assets
enable further advancement of helpful, capable, natural-language-interactable
robots. See videos at https://interactive-language.github.io."
The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,0.955131,"Does prompting a large language model (LLM) like GPT-3 with explanations
improve in-context learning? We study this question on two NLP tasks that
involve reasoning over text, namely question answering and natural language
inference. We test the performance of four LLMs on three textual reasoning
datasets using prompts that include explanations in multiple different styles.
For these tasks, we find that including explanations in the prompts for OPT,
GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to
moderate accuracy improvements over standard few-show learning. However,
text-davinci-002 is able to benefit more substantially.
  We further show that explanations generated by the LLMs may not entail the
models' predictions nor be factually grounded in the input, even on simple
tasks with extractive explanations. However, these flawed explanations can
still be useful as a way to verify LLMs' predictions post-hoc. Through analysis
in our three settings, we show that explanations judged by humans to be
good--logically consistent with the input and the prediction--more likely
cooccur with accurate predictions. Following these observations, we train
calibrators using automatically extracted scores that assess the reliability of
explanations, allowing us to improve performance post-hoc across all of our
datasets."
Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,0.96023,"Can world knowledge learned by large language models (LLMs) be used to act in
interactive environments? In this paper, we investigate the possibility of
grounding high-level tasks, expressed in natural language (e.g. ""make
breakfast""), to a chosen set of actionable steps (e.g. ""open fridge""). While
prior work focused on learning from explicit step-by-step examples of how to
act, we surprisingly find that if pre-trained LMs are large enough and prompted
appropriately, they can effectively decompose high-level tasks into mid-level
plans without any further training. However, the plans produced naively by LLMs
often cannot map precisely to admissible actions. We propose a procedure that
conditions on existing demonstrations and semantically translates the plans to
admissible actions. Our evaluation in the recent VirtualHome environment shows
that the resulting method substantially improves executability over the LLM
baseline. The conducted human evaluation reveals a trade-off between
executability and correctness but shows a promising sign towards extracting
actionable knowledge from language models. Website at
https://huangwl18.github.io/language-planner"
OpenScene: 3D Scene Understanding with Open Vocabularies,0.99284,"Traditional 3D scene understanding approaches rely on labeled 3D datasets to
train a model for a single task with supervision. We propose OpenScene, an
alternative approach where a model predicts dense features for 3D scene points
that are co-embedded with text and image pixels in CLIP feature space. This
zero-shot approach enables task-agnostic training and open-vocabulary queries.
For example, to perform SOTA zero-shot 3D semantic segmentation it first infers
CLIP features for every 3D point and later classifies them based on
similarities to embeddings of arbitrary class labels. More interestingly, it
enables a suite of open-vocabulary scene understanding applications that have
never been done before. For example, it allows a user to enter an arbitrary
text query and then see a heat map indicating which parts of a scene match. Our
approach is effective at identifying objects, materials, affordances,
activities, and room types in complex 3D scenes, all using a single model
trained without any labeled 3D data."
Autoformalization with Large Language Models,0.999774,"Autoformalization is the process of automatically translating from natural
language mathematics to formal specifications and proofs. A successful
autoformalization system could advance the fields of formal verification,
program synthesis, and artificial intelligence. While the long-term goal of
autoformalization seemed elusive for a long time, we show large language models
provide new prospects towards this goal. We make the surprising observation
that LLMs can correctly translate a significant portion ($25.3\%$) of
mathematical competition problems perfectly to formal specifications in
Isabelle/HOL. We demonstrate the usefulness of this process by improving a
previously introduced neural theorem prover via training on these
autoformalized theorems. Our methodology results in a new state-of-the-art
result on the MiniF2F theorem proving benchmark, improving the proof rate from
$29.6\%$ to $35.2\%$."
"""I'm sorry to hear that"": Finding New Biases in Language Models with a Holistic Descriptor Dataset",0.940286,"As language models grow in popularity, it becomes increasingly important to
clearly measure all possible markers of demographic identity in order to avoid
perpetuating existing societal harms. Many datasets for measuring bias
currently exist, but they are restricted in their coverage of demographic axes
and are commonly used with preset bias tests that presuppose which types of
biases models can exhibit. In this work, we present a new, more inclusive bias
measurement dataset, HolisticBias, which includes nearly 600 descriptor terms
across 13 different demographic axes. HolisticBias was assembled in a
participatory process including experts and community members with lived
experience of these terms. These descriptors combine with a set of bias
measurement templates to produce over 450,000 unique sentence prompts, which we
use to explore, identify, and reduce novel forms of bias in several generative
models. We demonstrate that HolisticBias is effective at measuring previously
undetectable biases in token likelihoods from language models, as well as in an
offensiveness classifier. We will invite additions and amendments to the
dataset, which we hope will serve as a basis for more easy-to-use and
standardized methods for evaluating bias in NLP models."
UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression,0.931339,"Geometry problem solving is a well-recognized testbed for evaluating the
high-level multi-modal reasoning capability of deep models. In most existing
works, two main geometry problems: calculation and proving, are usually treated
as two specific tasks, hindering a deep model to unify its reasoning capability
on multiple math tasks. However, in essence, these two tasks have similar
problem representations and overlapped math knowledge which can improve the
understanding and reasoning ability of a deep model on both two tasks.
Therefore, we construct a large-scale Unified Geometry problem benchmark,
UniGeo, which contains 4,998 calculation problems and 9,543 proving problems.
Each proving problem is annotated with a multi-step proof with reasons and
mathematical expressions. The proof can be easily reformulated as a proving
sequence that shares the same formats with the annotated program sequence for
calculation problems. Naturally, we also present a unified multi-task Geometric
Transformer framework, Geoformer, to tackle calculation and proving problems
simultaneously in the form of sequence generation, which finally shows the
reasoning ability can be improved on both two tasks by unifying formulation.
Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that
aims to predict the mathematical expressions in the problem solution, thus
improving the Geoformer model. Experiments on the UniGeo demonstrate that our
proposed Geoformer obtains state-of-the-art performance by outperforming
task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and
proving problems, respectively."
Natural Language to Code Translation with Execution,0.973189,"Generative models of code, pretrained on large corpora of programs, have
shown great success in translating natural language to code (Chen et al., 2021;
Austin et al., 2021; Li et al., 2022, inter alia). While these models do not
explicitly incorporate program semantics (i.e., execution results) during
training, they are able to generate correct solutions for many problems.
However, choosing a single correct program from a generated set for each
problem remains challenging. In this work, we introduce execution result--based
minimum Bayes risk decoding (MBR-EXEC) for program selection and show that it
improves the few-shot performance of pretrained code models on
natural-language-to-code tasks. We select output programs from a generated
candidate set by marginalizing over program implementations that share the same
semantics. Because exact equivalence is intractable, we execute each program on
a small number of test inputs to approximate semantic equivalence. Across
datasets, execution or simulated execution significantly outperforms the
methods that do not involve program semantics. We find that MBR-EXEC
consistently improves over all execution-unaware selection methods, suggesting
it as an effective approach for natural language to code translation. We
open-source our code at github.com/facebookresearch/mbr-exec and data at
dl.fbaipublicfiles.com/mbr-exec/mbr-exec-release.zip"
HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks,0.962097,"Implicit neural representations (INRs) are a rapidly growing research field,
which provides alternative ways to represent multimedia signals. Recent
applications of INRs include image super-resolution, compression of
high-dimensional signals, or 3D rendering. However, these solutions usually
focus on visual data, and adapting them to the audio domain is not trivial.
Moreover, it requires a separately trained model for every data sample. To
address this limitation, we propose HyperSound, a meta-learning method
leveraging hypernetworks to produce INRs for audio signals unseen at training
time. We show that our approach can reconstruct sound waves with quality
comparable to other state-of-the-art models."
Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word Embeddings,0.995483,"Word embeddings have advanced the state of the art in NLP across numerous
tasks. Understanding the contents of dense neural representations is of utmost
interest to the computational semantics community. We propose to focus on
relating these opaque word vectors with human-readable definitions, as found in
dictionaries. This problem naturally divides into two subtasks: converting
definitions into embeddings, and converting embeddings into definitions. This
task was conducted in a multilingual setting, using comparable sets of
embeddings trained homogeneously."
Multiview Stereo with Cascaded Epipolar RAFT,0.93222,"We address multiview stereo (MVS), an important 3D vision task that
reconstructs a 3D model such as a dense point cloud from multiple calibrated
images. We propose CER-MVS (Cascaded Epipolar RAFT Multiview Stereo), a new
approach based on the RAFT (Recurrent All-Pairs Field Transforms) architecture
developed for optical flow. CER-MVS introduces five new changes to RAFT:
epipolar cost volumes, cost volume cascading, multiview fusion of cost volumes,
dynamic supervision, and multiresolution fusion of depth maps. CER-MVS is
significantly different from prior work in multiview stereo. Unlike prior work,
which operates by updating a 3D cost volume, CER-MVS operates by updating a
disparity field. Furthermore, we propose an adaptive thresholding method to
balance the completeness and accuracy of the reconstructed point clouds.
Experiments show that our approach achieves competitive performance on DTU (the
second best among known results) and state-of-the-art performance on the
Tanks-and-Temples benchmark (both the intermediate and advanced set). Code is
available at https://github.com/princeton-vl/CER-MVS"
Realistic One-shot Mesh-based Head Avatars,0.977424,"We present a system for realistic one-shot mesh-based human head avatars
creation, ROME for short. Using a single photograph, our model estimates a
person-specific head mesh and the associated neural texture, which encodes both
local photometric and geometric details. The resulting avatars are rigged and
can be rendered using a neural network, which is trained alongside the mesh and
texture estimators on a dataset of in-the-wild videos. In the experiments, we
observe that our system performs competitively both in terms of head geometry
recovery and the quality of renders, especially for the cross-person
reenactment. See results https://samsunglabs.github.io/rome/"
TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition,0.961667,"Creation of 3D content by stylization is a promising yet challenging problem
in computer vision and graphics research. In this work, we focus on stylizing
photorealistic appearance renderings of a given surface mesh of arbitrary
topology. Motivated by the recent surge of cross-modal supervision of the
Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which
transfers the appearance style of a given 3D shape according to a text prompt
in a photorealistic manner. Technically, we propose to disentangle the
appearance style as the spatially varying bidirectional reflectance
distribution function, the local geometric variation, and the lighting
condition, which are jointly optimized, via supervision of the CLIP loss, by a
spherical Gaussians based differentiable renderer. As such, TANGO enables
photorealistic 3D style transfer by automatically predicting reflectance
effects even for bare, low-quality meshes, without training on a task-specific
dataset. Extensive experiments show that TANGO outperforms existing methods of
text-driven 3D style transfer in terms of photorealistic quality, consistency
of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and
results are available at our project webpage https://cyw-3d.github.io/tango/."
Masked Autoencoders that Listen,0.945524,"This paper studies a simple extension of image-based Masked Autoencoders
(MAE) to self-supervised representation learning from audio spectrograms.
Following the Transformer encoder-decoder design in MAE, our Audio-MAE first
encodes audio spectrogram patches with a high masking ratio, feeding only the
non-masked tokens through encoder layers. The decoder then re-orders and
decodes the encoded context padded with mask tokens, in order to reconstruct
the input spectrogram. We find it beneficial to incorporate local window
attention in the decoder, as audio spectrograms are highly correlated in local
time and frequency bands. We then fine-tune the encoder with a lower masking
ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art
performance on six audio and speech classification tasks, outperforming other
recent models that use external supervised pre-training. The code and models
will be at https://github.com/facebookresearch/AudioMAE."
Back to the Future: On Potential Histories in NLP,0.950213,"Machine learning and NLP require the construction of datasets to train and
fine-tune models. In this context, previous work has demonstrated the
sensitivity of these data sets. For instance, potential societal biases in this
data are likely to be encoded and to be amplified in the models we deploy. In
this work, we draw from developments in the field of history and take a novel
perspective on these problems: considering datasets and models through the lens
of historical fiction surfaces their political nature, and affords
re-configuring how we view the past, such that marginalized discourses are
surfaced. Building on such insights, we argue that contemporary methods for
machine learning are prejudiced towards dominant and hegemonic histories.
Employing the example of neopronouns, we show that by surfacing marginalized
histories within contemporary conditions, we can create models that better
represent the lived realities of traditionally marginalized and excluded
communities."
Image Super-resolution with An Enhanced Group Convolutional Neural Network,0.902458,"CNNs with strong learning abilities are widely chosen to resolve
super-resolution problem. However, CNNs depend on deeper network architectures
to improve performance of image super-resolution, which may increase
computational cost in general. In this paper, we present an enhanced
super-resolution group CNN (ESRGCNN) with a shallow architecture by fully
fusing deep and wide channel features to extract more accurate low-frequency
information in terms of correlations of different channels in single image
super-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is
useful to inherit more long-distance contextual information for resolving
long-term dependency. An adaptive up-sampling operation is gathered into a CNN
to obtain an image super-resolution model with low-resolution images of
different sizes. Extensive experiments report that our ESRGCNN surpasses the
state-of-the-arts in terms of SISR performance, complexity, execution speed,
image quality evaluation and visual effect in SISR. Code is found at
https://github.com/hellloxiaotian/ESRGCNN."
Directed Acyclic Transformer for Non-Autoregressive Machine Translation,0.979019,"Non-autoregressive Transformers (NATs) significantly reduce the decoding
latency by generating all tokens in parallel. However, such independent
predictions prevent NATs from capturing the dependencies between the tokens for
generating multiple possible translations. In this paper, we propose Directed
Acyclic Transfomer (DA-Transformer), which represents the hidden states in a
Directed Acyclic Graph (DAG), where each path of the DAG corresponds to a
specific translation. The whole DAG simultaneously captures multiple
translations and facilitates fast predictions in a non-autoregressive fashion.
Experiments on the raw training data of WMT benchmark show that DA-Transformer
substantially outperforms previous NATs by about 3 BLEU on average, which is
the first NAT model that achieves competitive results with autoregressive
Transformers without relying on knowledge distillation."
ArtFID: Quantitative Evaluation of Neural Style Transfer,0.995823,"The field of neural style transfer has experienced a surge of research
exploring different avenues ranging from optimization-based approaches and
feed-forward models to meta-learning methods. The developed techniques have not
just progressed the field of style transfer, but also led to breakthroughs in
other areas of computer vision, such as all of visual synthesis. However,
whereas quantitative evaluation and benchmarking have become pillars of
computer vision research, the reproducible, quantitative assessment of style
transfer models is still lacking. Even in comparison to other fields of visual
synthesis, where widely used metrics exist, the quantitative evaluation of
style transfer is still lagging behind. To support the automatic comparison of
different style transfer approaches and to study their respective strengths and
weaknesses, the field would greatly benefit from a quantitative measurement of
stylization performance. Therefore, we propose a method to complement the
currently mostly qualitative evaluation schemes. We provide extensive
evaluations and a large-scale user study to show that the proposed metric
strongly coincides with human judgment."
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,1.0,"We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier."
An End-to-End Transformer Model for Crowd Localization,0.891459,"Crowd localization, predicting head positions, is a more practical and
high-level task than simply counting. Existing methods employ pseudo-bounding
boxes or pre-designed localization maps, relying on complex post-processing to
obtain the head positions. In this paper, we propose an elegant, end-to-end
Crowd Localization Transformer named CLTR that solves the task in the
regression-based paradigm. The proposed method views the crowd localization as
a direct set prediction problem, taking extracted features and trainable
embeddings as input of the transformer-decoder. To reduce the ambiguous points
and generate more reasonable matching results, we introduce a KMO-based
Hungarian matcher, which adopts the nearby context as the auxiliary matching
cost. Extensive experiments conducted on five datasets in various data settings
show the effectiveness of our method. In particular, the proposed method
achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and
ShanghaiTech Part A datasets."
ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields,0.936072,"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline
methods with an emphasis on visual fidelity, our paper addresses the online use
case that prioritises real-time adaptability. We present ParticleNeRF, a new
approach that dynamically adapts to changes in the scene geometry by learning
an up-to-date representation online, every 200ms. ParticleNeRF achieves this
using a novel particle-based parametric encoding. We couple features to
particles in space and backpropagate the photometric reconstruction loss into
the particles' position gradients, which are then interpreted as velocity
vectors. Governed by a lightweight physics system to handle collisions, this
lets the features move freely with the changing scene geometry. We demonstrate
ParticleNeRF on various dynamic scenes containing translating, rotating,
articulated, and deformable objects. ParticleNeRF is the first online dynamic
NeRF and achieves fast adaptability with better visual fidelity than
brute-force online InstantNGP and other baseline approaches on dynamic scenes
with online constraints. Videos of our system can be found at our project
website https://sites.google.com/view/particlenerf."
OpenTAL: Towards Open Set Temporal Action Localization,0.997521,"Temporal Action Localization (TAL) has experienced remarkable success under
the supervised learning paradigm. However, existing TAL methods are rooted in
the closed set assumption, which cannot handle the inevitable unknown actions
in open-world scenarios. In this paper, we, for the first time, step toward the
Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on
Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of
uncertainty-aware action classification, actionness prediction, and temporal
location regression. With the proposed importance-balanced EDL method,
classification uncertainty is learned by collecting categorical evidence
majorly from important samples. To distinguish the unknown actions from
background video frames, the actionness is learned by the positive-unlabeled
learning. The classification uncertainty is further calibrated by leveraging
the guidance from the temporal localization quality. The OpenTAL is general to
enable existing TAL models for open set scenarios, and experimental results on
THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method.
The code and pre-trained models are released at
https://www.rit.edu/actionlab/opental."
Black-box Prompt Learning for Pre-trained Language Models,0.950468,"The increasing scale of general-purpose Pre-trained Language Models (PLMs)
necessitates the study of more efficient adaptation across different downstream
tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL)
to resonate with pragmatic interactions between the cloud infrastructure and
edge devices. Particularly, instead of fine-tuning the model in the cloud, we
adapt PLMs by prompt learning, which efficiently optimizes only a few
parameters of the discrete prompts. Moreover, we consider the scenario that we
do not have access to the parameters and gradients of the pre-trained models,
except for its outputs given inputs. This black-box setting secures the cloud
infrastructure from potential attack and misuse to cause a single-point
failure, which is preferable to the white-box counterpart by current
infrastructures. Under this black-box constraint, we apply a variance-reduced
policy gradient algorithm to estimate the gradients of parameters in the
categorical distribution of each discrete prompt. In light of our method, the
user devices can efficiently tune their tasks by querying the PLMs bounded by a
range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the
proposed algorithm achieves significant improvement on eight benchmarks in a
cloud-device collaboration manner. Finally, we conduct in-depth case studies to
comprehensively analyze our method in terms of various data sizes, prompt
lengths, training budgets, optimization objectives, prompt transferability, and
explanations of the learned prompts. Our code will be available at
https://github.com/shizhediao/Black-Box-Prompt-Learning."
BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,0.999918,"We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction."
Classifier-Free Diffusion Guidance,1.0,"Classifier guidance is a recently introduced method to trade off mode
coverage and sample fidelity in conditional diffusion models post training, in
the same spirit as low temperature sampling or truncation in other types of
generative models. Classifier guidance combines the score estimate of a
diffusion model with the gradient of an image classifier and thereby requires
training an image classifier separate from the diffusion model. It also raises
the question of whether guidance can be performed without a classifier. We show
that guidance can be indeed performed by a pure generative model without such a
classifier: in what we call classifier-free guidance, we jointly train a
conditional and an unconditional diffusion model, and we combine the resulting
conditional and unconditional score estimates to attain a trade-off between
sample quality and diversity similar to that obtained using classifier
guidance."
Automatic Chain of Thought Prompting in Large Language Models,0.999909,"Large language models (LLMs) can perform complex reasoning by generating
intermediate reasoning steps. Providing these steps for prompting
demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has
two major paradigms. One leverages a simple prompt like ""Let's think step by
step"" to facilitate step-by-step thinking before answering a question. The
other uses a few manual demonstrations one by one, each composed of a question
and a reasoning chain that leads to an answer. The superior performance of the
second paradigm hinges on the hand-crafting of task-specific demonstrations one
by one. We show that such manual efforts may be eliminated by leveraging LLMs
with the ""Let's think step by step"" prompt to generate reasoning chains for
demonstrations one by one, i.e., let's think not just step by step, but also
one by one. However, these generated chains often come with mistakes. To
mitigate the effect of such mistakes, we find that diversity matters for
automatically constructing demonstrations. We propose an automatic CoT
prompting method: Auto-CoT. It samples questions with diversity and generates
reasoning chains to construct demonstrations. On ten public benchmark reasoning
tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of
the CoT paradigm that requires manual designs of demonstrations. Code is
available at https://github.com/amazon-research/auto-cot"
BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining,0.997384,"Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT."
Block-NeRF: Scalable Large Scene Neural View Synthesis,0.925009,"We present Block-NeRF, a variant of Neural Radiance Fields that can represent
large-scale environments. Specifically, we demonstrate that when scaling NeRF
to render city-scale scenes spanning multiple blocks, it is vital to decompose
the scene into individually trained NeRFs. This decomposition decouples
rendering time from scene size, enables rendering to scale to arbitrarily large
environments, and allows per-block updates of the environment. We adopt several
architectural changes to make NeRF robust to data captured over months under
different environmental conditions. We add appearance embeddings, learned pose
refinement, and controllable exposure to each individual NeRF, and introduce a
procedure for aligning appearance between adjacent NeRFs so that they can be
seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to
create the largest neural scene representation to date, capable of rendering an
entire neighborhood of San Francisco."
Training language models to follow instructions with human feedback,1.0,"Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent."
"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",0.999752,"We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io."
NeuMesh: Learning Disentangled Neural Mesh-based Implicit Field for Geometry and Texture Editing,0.957021,"Very recently neural implicit rendering techniques have been rapidly evolved
and shown great advantages in novel view synthesis and 3D scene reconstruction.
However, existing neural rendering methods for editing purposes offer limited
functionality, e.g., rigid transformation, or not applicable for fine-grained
editing for general objects from daily lives. In this paper, we present a novel
mesh-based representation by encoding the neural implicit field with
disentangled geometry and texture codes on mesh vertices, which facilitates a
set of editing functionalities, including mesh-guided geometry editing,
designated texture editing with texture swapping, filling and painting
operations. To this end, we develop several techniques including learnable sign
indicators to magnify spatial distinguishability of mesh-based representation,
distillation and fine-tuning mechanism to make a steady convergence, and the
spatial-aware optimization strategy to realize precise texture editing.
Extensive experiments and editing examples on both real and synthetic data
demonstrate the superiority of our method on representation quality and editing
ability. Code is available on the project webpage:
https://zju3dv.github.io/neumesh/."
ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing,0.906372,"Sketch-and-extrude is a common and intuitive modeling process in computer
aided design. This paper studies the problem of learning the shape given in the
form of point clouds by inverse sketch-and-extrude. We present ExtrudeNet, an
unsupervised end-to-end network for discovering sketch and extrude from point
clouds. Behind ExtrudeNet are two new technical components: 1) an effective
representation for sketch and extrude, which can model extrusion with freeform
sketches and conventional cylinder and box primitives as well; and 2) a
numerical method for computing the signed distance field which is used in the
network learning. This is the first attempt that uses machine learning to
reverse engineer the sketch-and-extrude modeling process of a shape in an
unsupervised fashion. ExtrudeNet not only outputs a compact, editable and
interpretable representation of the shape that can be seamlessly integrated
into modern CAD software, but also aligns with the standard CAD modeling
process facilitating various editing applications, which distinguishes our work
from existing shape parsing research. Code is released at
https://github.com/kimren227/ExtrudeNet."
Towards Responsible AI for Financial Transactions,0.984019,"The application of AI in finance is increasingly dependent on the principles
of responsible AI. These principles - explainability, fairness, privacy,
accountability, transparency and soundness form the basis for trust in future
AI systems. In this study, we address the first principle by providing an
explanation for a deep neural network that is trained on a mixture of
numerical, categorical and textual inputs for financial transaction
classification. The explanation is achieved through (1) a feature importance
analysis using Shapley additive explanations (SHAP) and (2) a hybrid approach
of text clustering and decision tree classifiers. We then test the robustness
of the model by exposing it to a targeted evasion attack, leveraging the
knowledge we gained about the model through the extracted explanation."
DiffPose: Toward More Reliable 3D Pose Estimation,0.961112,"Monocular 3D human pose estimation is quite challenging due to the inherent
ambiguity and occlusion, which often lead to high uncertainty and
indeterminacy. On the other hand, diffusion models have recently emerged as an
effective tool for generating high-quality images from noise. Inspired by their
capability, we explore a novel pose estimation framework (DiffPose) that
formulates 3D pose estimation as a reverse diffusion process. We incorporate
novel designs into our DiffPose to facilitate the diffusion process for 3D pose
estimation: a pose-specific initialization of pose uncertainty distributions, a
Gaussian Mixture Model-based forward diffusion process, and a
context-conditioned reverse diffusion process. Our proposed DiffPose
significantly outperforms existing methods on the widely used pose estimation
benchmarks Human3.6M and MPI-INF-3DHP. Project page:
https://gongjia0208.github.io/Diffpose/."
ESCM$^2$: Entire Space Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation,0.964419,"Accurate estimation of post-click conversion rate is critical for building
recommender systems, which has long been confronted with sample selection bias
and data sparsity issues. Methods in the Entire Space Multi-task Model (ESMM)
family leverage the sequential pattern of user actions, i.e.
$impression\rightarrow click \rightarrow conversion$ to address data sparsity
issue. However, they still fail to ensure the unbiasedness of CVR estimates. In
this paper, we theoretically demonstrate that ESMM suffers from the following
two problems: (1) Inherent Estimation Bias (IEB), where the estimated CVR of
ESMM is inherently higher than the ground truth; (2) Potential Independence
Priority (PIP) for CTCVR estimation, where there is a risk that the ESMM
overlooks the causality from click to conversion. To this end, we devise a
principled approach named Entire Space Counterfactual Multi-task Modelling
(ESCM$^2$), which employs a counterfactual risk miminizer as a regularizer in
ESMM to address both IEB and PIP issues simultaneously. Extensive experiments
on offline datasets and online environments demonstrate that our proposed
ESCM$^2$ can largely mitigate the inherent IEB and PIP issues and achieve
better performance than baseline models."
Efficient Training of Language Models to Fill in the Middle,0.990984,"We show that autoregressive language models can learn to infill text after we
apply a straightforward transformation to the dataset, which simply moves a
span of text from the middle of a document to its end. While this data
augmentation has garnered much interest in recent years, we provide extensive
evidence that training models with a large fraction of data transformed in this
way does not harm the original left-to-right generative capability, as measured
by perplexity and sampling evaluations across a wide range of scales. Given the
usefulness, simplicity, and efficiency of training models to fill-in-the-middle
(FIM), we suggest that future autoregressive language models be trained with
FIM by default. To this end, we run a series of ablations on key
hyperparameters, such as the data transformation frequency, the structure of
the transformation, and the method of selecting the infill span. We use these
ablations to prescribe strong default settings and best practices to train FIM
models. We have released our best infilling model trained with best practices
in our API, and release our infilling benchmarks to aid future research."
CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI,0.998677,"Human language expression is based on the subjective construal of the
situation instead of the objective truth conditions, which means that speakers'
personalities and emotions after cognitive processing have an important
influence on conversation. However, most existing datasets for conversational
AI ignore human personalities and emotions, or only consider part of them. It's
difficult for dialogue systems to understand speakers' personalities and
emotions although large-scale pre-training language models have been widely
used. In order to consider both personalities and emotions in the process of
conversation generation, we propose CPED, a large-scale Chinese personalized
and emotional dialogue dataset, which consists of multi-source knowledge
related to empathy and personal characteristic. These knowledge covers gender,
Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED
contains more than 12K dialogues of 392 speakers from 40 TV shows. We release
the textual dataset with audio features and video features according to the
copyright claims, privacy issues, terms of service of video platforms. We
provide detailed description of the CPED construction process and introduce
three tasks for conversational AI, including personality recognition, emotion
recognition in conversations as well as personalized and emotional conversation
generation. Finally, we provide baseline systems for these tasks and consider
the function of speakers' personalities and emotions on conversation. Our
motivation is to propose a dataset to be widely adopted by the NLP community as
a new open benchmark for conversational AI research. The full dataset is
available at https://github.com/scutcyr/CPED."
NeuMan: Neural Human Radiance Field from a Single Video,0.968807,"Photorealistic rendering and reposing of humans is important for enabling
augmented reality experiences. We propose a novel framework to reconstruct the
human and the scene that can be rendered with novel human poses and views from
just a single in-the-wild video. Given a video captured by a moving camera, we
train two NeRF models: a human NeRF model and a scene NeRF model. To train
these models, we rely on existing methods to estimate the rough geometry of the
human and the scene. Those rough geometry estimates allow us to create a
warping field from the observation space to the canonical pose-independent
space, where we train the human model in. Our method is able to learn subject
specific details, including cloth wrinkles and accessories, from just a 10
seconds video clip, and to provide high quality renderings of the human under
novel poses, from novel views, together with the background."
FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction,0.912728,"Sequence modeling has demonstrated state-of-the-art performance on natural
language and document understanding tasks. However, it is challenging to
correctly serialize tokens in form-like documents in practice due to their
variety of layout patterns. We propose FormNet, a structure-aware sequence
model to mitigate the suboptimal serialization of forms. First, we design Rich
Attention that leverages the spatial relationship between tokens in a form for
more precise attention score calculation. Second, we construct Super-Tokens for
each word by embedding representations from their neighboring tokens through
graph convolutions. FormNet therefore explicitly recovers local syntactic
information that may have been lost during serialization. In experiments,
FormNet outperforms existing methods with a more compact model size and less
pre-training data, establishing new state-of-the-art performance on CORD, FUNSD
and Payment benchmarks."
Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition,0.995784,"This paper does not attempt to design a state-of-the-art method for visual
recognition but investigates a more efficient way to make use of convolutions
to encode spatial features. By comparing the design principles of the recent
convolutional neural networks ConvNets) and Vision Transformers, we propose to
simplify the self-attention by leveraging a convolutional modulation operation.
We show that such a simple approach can better take advantage of the large
kernels (>=7x7) nested in convolutional layers. We build a family of
hierarchical ConvNets using the proposed convolutional modulation, termed
Conv2Former. Our network is simple and easy to follow. Experiments show that
our Conv2Former outperforms existent popular ConvNets and vision Transformers,
like Swin Transformer and ConvNeXt in all ImageNet classification, COCO object
detection and ADE20k semantic segmentation."
Self-explaining deep models with logic rule reasoning,0.998727,"We present SELOR, a framework for integrating self-explaining capabilities
into a given deep model to achieve both high prediction performance and human
precision. By ""human precision"", we refer to the degree to which humans agree
with the reasons models provide for their predictions. Human precision affects
user trust and allows users to collaborate closely with the model. We
demonstrate that logic rule explanations naturally satisfy human precision with
the expressive power required for good predictive performance. We then
illustrate how to enable a deep model to predict and explain with logic rules.
Our method does not require predefined logic rule sets or human annotations and
can be learned efficiently and easily with widely-used deep learning modules in
a differentiable way. Extensive experiments show that our method gives
explanations closer to human decision logic than other methods while
maintaining the performance of deep learning models."
Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?,0.924961,"Task specification is at the core of programming autonomous robots. A
low-effort modality for task specification is critical for engagement of
non-expert end-users and ultimate adoption of personalized robot agents. A
widely studied approach to task specification is through goals, using either
compact state vectors or goal images from the same robot scene. The former is
hard to interpret for non-experts and necessitates detailed state estimation
and scene understanding. The latter requires the generation of desired goal
image, which often requires a human to complete the task, defeating the purpose
of having autonomous robots. In this work, we explore alternate and more
general forms of goal specification that are expected to be easier for humans
to specify and use such as images obtained from the internet, hand sketches
that provide a visual description of the desired task, or simple language
descriptions. As a preliminary step towards this, we investigate the
capabilities of large scale pre-trained models (foundation models) for
zero-shot goal specification, and find promising results in a collection of
simulated robot manipulation tasks and real-world datasets."
