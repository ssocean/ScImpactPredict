id,title,TNCSI,abstract,OA,authors_title
056f75bf-e9f1-4f92-b125-4d9a10b58be7,Privacy-Preserving Synthetic Educational Data Generation,0.349501,"Institutions collect massive learning traces but they may not disclose it for
privacy issues. Synthetic data generation opens new opportunities for research
in education. In this paper we present a generative model for educational data
that can preserve the privacy of participants, and an evaluation framework for
comparing synthetic data generators. We show how naive pseudonymization can
lead to re-identification threats and suggest techniques to guarantee privacy.
We evaluate our method on existing massive educational open datasets.",https://github.com/Akulen/PrivGen,-1
ef161bf8-b155-42fe-9791-b0e47bdb4021,Learning from Drivers to Tackle the Amazon Last Mile Routing Research Challenge,0.332901,"The goal of the Amazon Last Mile Routing Research Challenge is to integrate
the real-life experience of Amazon drivers into the solution of optimal route
planning and optimization. This paper presents our method that tackles this
challenge by hierarchically combining machine learning and conventional
Traveling Salesperson Problem (TSP) solvers. Our method reaps the benefits from
both worlds. On the one hand, our method encodes driver know-how by learning a
sequential probability model from historical routes at the zone level, where
each zone contains a few parcel stops. It then uses a single step policy
iteration method, known as the Rollout algorithm, to generate plausible zone
sequences sampled from the learned probability model. On the other hand, our
method utilizes proven methods developed in the rich TSP literature to sequence
stops within each zone efficiently. The outcome of such a combination appeared
to be promising. Our method obtained an evaluation score of $0.0374$, which is
comparable to what the top three teams have achieved on the official Challenge
leaderboard. Moreover, our learning-based method is applicable to driving
routes that may exhibit distinct sequential patterns beyond the scope of this
Challenge. The source code of our method is publicly available at
https://github.com/aws-samples/amazon-sagemaker-amazon-routing-challenge-sol",https://github.com/aws-samples/amazon-sagemaker-amazon-routing-challenge-sol,-1
9e68eb1d-21f9-44bf-9b8e-2b2ad20e019c,FairGBM: Gradient Boosting with Fairness Constraints,0.931546,"Tabular data is prevalent in many high-stakes domains, such as financial
services or public policy. Gradient Boosted Decision Trees (GBDT) are popular
in these settings due to their scalability, performance, and low training cost.
While fairness in these domains is a foremost concern, existing in-processing
Fair ML methods are either incompatible with GBDT, or incur in significant
performance losses while taking considerably longer to train. We present
FairGBM, a dual ascent learning framework for training GBDT under fairness
constraints, with little to no impact on predictive performance when compared
to unconstrained GBDT. Since observational fairness metrics are
non-differentiable, we propose smooth convex error rate proxies for common
fairness criteria, enabling gradient-based optimization using a
``proxy-Lagrangian'' formulation. Our implementation shows an order of
magnitude speedup in training time relative to related work, a pivotal aspect
to foster the widespread adoption of FairGBM by real-world practitioners.",https://github.com/feedzai/fairgbm,-1
403f2f4d-434d-4ae1-a854-26b05ce4abb0,Thermodynamics-informed neural networks for physically realistic mixed reality,0.712728,"The imminent impact of immersive technologies in society urges for active
research in real-time and interactive physics simulation for virtual worlds to
be realistic. In this context, realistic means to be compliant to the laws of
physics. In this paper we present a method for computing the dynamic response
of (possibly non-linear and dissipative) deformable objects induced by
real-time user interactions in mixed reality using deep learning. The
graph-based architecture of the method ensures the thermodynamic consistency of
the predictions, whereas the visualization pipeline allows a natural and
realistic user experience. Two examples of virtual solids interacting with
virtual or physical solids in mixed reality scenarios are provided to prove the
performance of the method.",https://github.com/quercushernandez,-1
cf2cb427-b659-450c-9db4-ec31b1108b6e,Predicting Vegetation Stratum Occupancy from Airborne LiDAR Data with Deep Learning,0.440051,"We propose a new deep learning-based method for estimating the occupancy of
vegetation strata from airborne 3D LiDAR point clouds. Our model predicts
rasterized occupancy maps for three vegetation strata corresponding to lower,
medium, and higher cover. Our weakly-supervised training scheme allows our
network to only be supervised with vegetation occupancy values aggregated over
cylindrical plots containing thousands of points. Such ground truth is easier
to produce than pixel-wise or point-wise annotations. Our method outperforms
handcrafted and deep learning baselines in terms of precision by up to 30%,
while simultaneously providing visual and interpretable predictions. We provide
an open-source implementation along with a dataset of 199 agricultural plots to
train and evaluate weakly supervised occupancy regression algorithms.",https://github.com/ekalinicheva/plot_vegetation_coverage,-1
345e3520-81f4-453a-b340-3f5d727ddbaf,Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution,0.108426,"Nowadays, thermal infrared satellite remote sensors enable to extract very
interesting information at large scale, in particular Land Surface Temperature
(LST). However such data are limited in spatial and/or temporal resolutions
which prevents from an analysis at fine scales. For example, MODIS satellite
provides daily acquisitions with 1Km spatial resolutions which is not
sufficient to deal with highly heterogeneous environments as agricultural
parcels. Therefore, image super-resolution is a crucial task to better exploit
MODIS LSTs. This issue is tackled in this paper. We introduce a deep
learning-based algorithm, named Multi-residual U-Net, for super-resolution of
MODIS LST single-images. Our proposed network is a modified version of U-Net
architecture, which aims at super-resolving the input LST image from 1Km to
250m per pixel. The results show that our Multi-residual U-Net outperforms
other state-of-the-art methods.",https://github.com/IMT-Project-LTS-SR/MRUNet-for-MODIS-super-resolution,3504
aa55b467-3f60-4b32-aa54-e853f97f868c,Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery,0.248232,"Discovering out-of-domain (OOD) intent is important for developing new skills
in task-oriented dialogue systems. The key challenges lie in how to transfer
prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD
representations and cluster assignments. Previous methods suffer from in-domain
overfitting problem, and there is a natural gap between representation learning
and clustering objectives. In this paper, we propose a unified K-nearest
neighbor contrastive learning framework to discover OOD intents. Specifically,
for IND pre-training stage, we propose a KCL objective to learn inter-class
discriminative features, while maintaining intra-class diversity, which
alleviates the in-domain overfitting problem. For OOD clustering stage, we
propose a KCC method to form compact clusters by mining true hard negative
samples, which bridges the gap between clustering and representation learning.
Extensive experiments on three benchmark datasets show that our method achieves
substantial improvements over the state-of-the-art methods.",https://github.com/myt517/KCOD,-1
5db72a08-bb4d-4637-97ef-c0ad307a9cbd,Scribble-Supervised LiDAR Semantic Segmentation,0.707702,"Densely annotating LiDAR point clouds remains too expensive and
time-consuming to keep up with the ever growing volume of data. While current
literature focuses on fully-supervised performance, developing efficient
methods that take advantage of realistic weak supervision have yet to be
explored. In this paper, we propose using scribbles to annotate LiDAR point
clouds and release ScribbleKITTI, the first scribble-annotated dataset for
LiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the
performance gap that arises when using such weak annotations. Our pipeline
comprises of three stand-alone contributions that can be combined with any
LiDAR semantic segmentation model to achieve up to 95.7% of the
fully-supervised performance while using only 8% labeled points. Our scribble
annotations and code are available at github.com/ouenal/scribblekitti.",https://github.com/ouenal/scribblekitti,-1
a36eafd6-dede-40d3-a197-f40163a582a7,Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation,0.521025,"Neural machine translation (NMT) has become the de-facto standard in
real-world machine translation applications. However, NMT models can
unpredictably produce severely pathological translations, known as
hallucinations, that seriously undermine user trust. It becomes thus crucial to
implement effective preventive strategies to guarantee their proper
functioning. In this paper, we address the problem of hallucination detection
in NMT by following a simple intuition: as hallucinations are detached from the
source content, they exhibit encoder-decoder attention patterns that are
statistically different from those of good quality translations. We frame this
problem with an optimal transport formulation and propose a fully unsupervised,
plug-in detector that can be used with any attention-based NMT model.
Experimental results show that our detector not only outperforms all previous
model-based detectors, but is also competitive with detectors that employ large
models trained on millions of samples.",https://github.com/deep-spin/,-1
9ac0ceee-ef36-4fba-9ec0-dea4a13c102d,Object Detection in Aerial Images: What Improves the Accuracy?,0.0215421,"Object detection is a challenging and popular computer vision problem. The
problem is even more challenging in aerial images due to significant variation
in scale and viewpoint in a diverse set of object categories. Recently, deep
learning-based object detection approaches have been actively explored for the
problem of object detection in aerial images. In this work, we investigate the
impact of Faster R-CNN for aerial object detection and explore numerous
strategies to improve its performance for aerial images. We conduct extensive
experiments on the challenging iSAID dataset. The resulting adapted Faster
R-CNN obtains a significant mAP gain of 4.96% over its vanilla baseline
counterpart on the iSAID validation set, demonstrating the impact of different
strategies investigated in this work.",None,-1
6e1a1a27-8b3a-45a1-917d-256fd12b90c5,"Revisiting Crowd Counting: State-of-the-art, Trends, and Future Perspectives",0.918591,"Crowd counting is an effective tool for situational awareness in public
places. Automated crowd counting using images and videos is an interesting yet
challenging problem that has gained significant attention in computer vision.
Over the past few years, various deep learning methods have been developed to
achieve state-of-the-art performance. The methods evolved over time vary in
many aspects such as model architecture, input pipeline, learning paradigm,
computational complexity, and accuracy gains etc. In this paper, we present a
systematic and comprehensive review of the most significant contributions in
the area of crowd counting. Although few surveys exist on the topic, our survey
is most up-to date and different in several aspects. First, it provides a more
meaningful categorization of the most significant contributions by model
architectures, learning methods (i.e., loss functions), and evaluation methods
(i.e., evaluation metrics). We chose prominent and distinct works and excluded
similar works. We also sort the well-known crowd counting models by their
performance over benchmark datasets. We believe that this survey can be a good
resource for novice researchers to understand the progressive developments and
contributions over time and the current state-of-the-art.",None,-1
8d95c39b-fa9c-42b3-95d2-bb0d28c23393,Towards High-Fidelity Single-view Holistic Reconstruction of Indoor Scenes,0.783021,"We present a new framework to reconstruct holistic 3D indoor scenes including
both room background and indoor objects from single-view images. Existing
methods can only produce 3D shapes of indoor objects with limited geometry
quality because of the heavy occlusion of indoor scenes. To solve this, we
propose an instance-aligned implicit function (InstPIFu) for detailed object
reconstruction. Combining with instance-aligned attention module, our method is
empowered to decouple mixed local features toward the occluded instances.
Additionally, unlike previous methods that simply represents the room
background as a 3D bounding box, depth map or a set of planes, we recover the
fine geometry of the background via implicit representation. Extensive
experiments on the SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets
demonstrate that our method outperforms existing approaches in both background
and foreground object reconstruction. Our code and model will be made publicly
available.",None,-1
4a1f2909-17fb-4909-acd3-62a8a45e2213,Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation,0.605125,"Unsupervised domain adaptation (UDA) requires source domain samples with
clean ground truth labels during training. Accurately labeling a large number
of source domain samples is time-consuming and laborious. An alternative is to
utilize samples with noisy labels for training. However, training with noisy
labels can greatly reduce the performance of UDA. In this paper, we address the
problem that learning UDA models only with access to noisy labels and propose a
novel method called robust local preserving and global aligning network
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One
is learning a classifier by a robust informative-theoretic-based loss function.
The other is constructing two adjacency weight matrices and two negative weight
matrices by the proposed local preserving module to preserve the local topology
structures of input data. We conduct theoretical analysis on the robustness of
the proposed RLPGA and prove that the robust informative-theoretic-based loss
and the local preserving module are beneficial to reduce the empirical risk of
the target domain. A series of empirical studies show the effectiveness of our
proposed RLPGA.",None,-1
edf3e145-86b7-4a85-9910-7641f1501832,"Artificial Intelligence Software Structured to Simulate Human Working Memory, Mental Imagery, and Mental Continuity",0.0459814,"This article presents an artificial intelligence (AI) architecture intended
to simulate the human working memory system as well as the manner in which it
is updated iteratively. It features several interconnected neural networks
designed to emulate the specialized modules of the cerebral cortex. These are
structured hierarchically and integrated into a global workspace. They are
capable of temporarily maintaining high-level patterns akin to the
psychological items maintained in working memory. This maintenance is made
possible by persistent neural activity in the form of two modalities: sustained
neural firing (resulting in a focus of attention) and synaptic potentiation
(resulting in a short-term store). This persistent activity is updated
iteratively resulting in incremental changes to the content of the working
memory system. As the content stored in working memory gradually evolves,
successive states overlap and are continuous with one another. The present
article will explore how this architecture can lead to gradual shift in the
distribution of coactive representations, ultimately leading to mental
continuity between processing states, and thus to human-like cognition.",None,-1
06542ca6-2ef2-4e50-ac3f-c519420e62b9,Moving Other Way: Exploring Word Mover Distance Extensions,0.168663,"The word mover's distance (WMD) is a popular semantic similarity metric for
two texts. This position paper studies several possible extensions of WMD. We
experiment with the frequency of words in the corpus as a weighting factor and
the geometry of the word vector space. We validate possible extensions of WMD
on six document classification datasets. Some proposed extensions show better
results in terms of the k-nearest neighbor classification error than WMD.",https://github.com/mkusner/wmd,388
2457391f-bfb1-419f-bb54-3495885435bd,What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured,0.197272,"Transformer-based models are now predominant in NLP. They outperform
approaches based on static models in many respects. This success has in turn
prompted research that reveals a number of biases in the language models
generated by transformers. In this paper we utilize this research on biases to
investigate to what extent transformer-based language models allow for
extracting knowledge about object relations (X occurs in Y; X consists of Z;
action A involves using X). To this end, we compare contextualized models with
their static counterparts. We make this comparison dependent on the application
of a number of similarity measures and classifiers. Our results are threefold:
Firstly, we show that the models combined with the different similarity
measures differ greatly in terms of the amount of knowledge they allow for
extracting. Secondly, our results suggest that similarity measures perform much
worse than classifier-based approaches. Thirdly, we show that, surprisingly,
static models perform almost as well as contextualized models -- in some cases
even better.",None,3603
09b76c21-2fc8-4991-b2d5-7d7230e2948d,PreCogIIITH at HinglishEval : Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,0.0665525,"Code-Mixing is a phenomenon of mixing two or more languages in a speech event
and is prevalent in multilingual societies. Given the low-resource nature of
Code-Mixing, machine generation of code-mixed text is a prevalent approach for
data augmentation. However, evaluating the quality of such machine generated
code-mixed text is an open problem. In our submission to HinglishEval, a
shared-task collocated with INLG2022, we attempt to build models factors that
impact the quality of synthetically generated code-mix text by predicting
ratings for code-mix quality.",https://github.com/irshadbhat/csnli,-1
2c363bc1-5a9d-4d41-a9f8-a77a89978b2b,Enhanced Training of Query-Based Object Detection via Selective Query Recollection,0.555931,"This paper investigates a phenomenon where query-based object detectors
mispredict at the last decoding stage while predicting correctly at an
intermediate stage. We review the training process and attribute the overlooked
phenomenon to two limitations: lack of training emphasis and cascading errors
from decoding sequence. We design and present Selective Query Recollection
(SQR), a simple and effective training strategy for query-based object
detectors. It cumulatively collects intermediate queries as decoding stages go
deeper and selectively forwards the queries to the downstream stages aside from
the sequential structure. Such-wise, SQR places training emphasis on later
stages and allows later stages to work with intermediate queries from earlier
stages directly. SQR can be easily plugged into various query-based object
detectors and significantly enhances their performance while leaving the
inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,
and Deformable-DETR across various settings (backbone, number of queries,
schedule) and consistently brings 1.4-2.8 AP improvement.",https://github.com/Fangyi-Chen/SQR,14250
21c2ff42-1a41-4063-8fa9-a966f6e40351,Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer,0.595334,"In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
mainly focused solely on source sentence tokens' attributions. Therefore, we
lack a full understanding of the influences of every input token (source
sentence and target prefix) in the model predictions. In this work, we propose
an interpretability method that tracks input tokens' attributions for both
contexts. Our method, which can be extended to any encoder-decoder
Transformer-based model, allows us to better comprehend the inner workings of
current NMT models. We apply the proposed method to both bilingual and
multilingual Transformers and present insights into their behaviour.",https://github.com/mt-upc/,-1
4b3bc78d-3b67-4783-acfe-ad5fc5e6d0de,Towards an Accountable and Reproducible Federated Learning: A FactSheets Approach,0.153716,"Federated Learning (FL) is a novel paradigm for the shared training of models
based on decentralized and private data. With respect to ethical guidelines, FL
is promising regarding privacy, but needs to excel vis-\`a-vis transparency and
trustworthiness. In particular, FL has to address the accountability of the
parties involved and their adherence to rules, law and principles. We introduce
AF^2 Framework, where we instrument FL with accountability by fusing verifiable
claims with tamper-evident facts, into reproducible arguments. We build on AI
FactSheets for instilling transparency and trustworthiness into the AI
lifecycle and expand it to incorporate dynamic and nested facts, as well as
complex model compositions in FL. Based on our approach, an auditor can
validate, reproduce and certify a FL process. This can be directly applied in
practice to address the challenges of AI engineering and ethics.",None,-1
dd41cdad-0a8a-4d33-b51d-45958e5edc8e,Tractable Boolean and Arithmetic Circuits,0.69657,"Tractable Boolean and arithmetic circuits have been studied extensively in AI
for over two decades now. These circuits were initially proposed as ""compiled
objects,"" meant to facilitate logical and probabilistic reasoning, as they
permit various types of inference to be performed in linear-time and a
feed-forward fashion like neural networks. In more recent years, the role of
tractable circuits has significantly expanded as they became a computational
and semantical backbone for some approaches that aim to integrate knowledge,
reasoning and learning. In this article, we review the foundations of tractable
circuits and some associated milestones, while focusing on their core
properties and techniques that make them particularly useful for the broad aims
of neuro-symbolic AI.",None,-1
91868934-cb28-4bdc-be97-98cae223e341,Spectral Adversarial Training for Robust Graph Neural Network,0.793988,"Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency.",https://github.com/EdisonLeeeee/SAT,-1
1faf2ecc-de48-426a-90cb-d81577c1e0cd,Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost,0.772238,"State-of-the-art NLP systems represent inputs with word embeddings, but these
are brittle when faced with Out-of-Vocabulary (OOV) words. To address this
issue, we follow the principle of mimick-like models to generate vectors for
unseen words, by learning the behavior of pre-trained embeddings using only the
surface form of words. We present a simple contrastive learning framework,
LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional
parameters. Extensive evaluations demonstrate that our lightweight model
achieves similar or even better performances than prior competitors, both on
original datasets and on corrupted variants. Moreover, it can be used in a
plug-and-play fashion with FastText and BERT, where it significantly improves
their robustness.",https://github.com/tigerchen52/LOVE,129114
835c7818-9820-4795-a32e-0f58e3c51a6e,Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues,0.447327,"Movies reflect society and also hold power to transform opinions. Social
biases and stereotypes present in movies can cause extensive damage due to
their reach. These biases are not always found to be the need of storyline but
can creep in as the author's bias. Movie production houses would prefer to
ascertain that the bias present in a script is the story's demand. Today, when
deep learning models can give human-level accuracy in multiple tasks, having an
AI solution to identify the biases present in the script at the writing stage
can help them avoid the inconvenience of stalled release, lawsuits, etc. Since
AI solutions are data intensive and there exists no domain specific data to
address the problem of biases in scripts, we introduce a new dataset of movie
scripts that are annotated for identity bias. The dataset contains dialogue
turns annotated for (i) bias labels for seven categories, viz., gender,
race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains
biases like body shaming, personality bias, etc. (ii) labels for sensitivity,
stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated
with context awareness, (iv) target groups and reason for bias labels and (v)
expert-driven group-validation process for high quality annotations. We also
report various baseline performances for bias identification and category
detection on our dataset.",https://github.com/sahoonihar/HIBD_LREC_2022,-1
091e8782-f6a0-4f57-afc0-ed9d46bb44f6,NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs,0.400171,"Complex query answering (CQA) is an essential task for multi-hop and logical
reasoning on knowledge graphs (KGs). Currently, most approaches are limited to
queries among binary relational facts and pay less attention to n-ary facts
(n>=2) containing more than two entities, which are more prevalent in the real
world. Moreover, previous CQA methods can only make predictions for a few given
types of queries and cannot be flexibly extended to more complex logical
queries, which significantly limits their applications. To overcome these
challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model
for CQA over hyper-relational knowledge graphs (HKGs), which include massive
n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and
fuzzy logic theory to satisfy all n-ary FOL queries, including existential
quantifiers, conjunction, disjunction, and negation. We also propose a parallel
processing algorithm that can train or predict arbitrary n-ary FOL queries in a
single batch, regardless of the kind of each query, with good flexibility and
extensibility. In addition, we generate a new CQA dataset WD50K-NFOL, including
diverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL and
other standard CQA datasets show that NQE is the state-of-the-art CQA method
over HKGs with good generalization capability. Our code and dataset are
publicly available.",https://github.com/LHRLAB/NQE,2442
77d4f737-28db-4dea-a3d6-45f67288b985,"AI Technical Considerations: Data Storage, Cloud usage and AI Pipeline",0.111827,"Artificial intelligence (AI), especially deep learning, requires vast amounts
of data for training, testing, and validation. Collecting these data and the
corresponding annotations requires the implementation of imaging biobanks that
provide access to these data in a standardized way. This requires careful
design and implementation based on the current standards and guidelines and
complying with the current legal restrictions. However, the realization of
proper imaging data collections is not sufficient to train, validate and deploy
AI as resource demands are high and require a careful hybrid implementation of
AI pipelines both on-premise and in the cloud. This chapter aims to help the
reader when technical considerations have to be made about the AI environment
by providing a technical background of different concepts and implementation
aspects involved in data storage, cloud usage, and AI pipelines.",None,-1
ddb92509-ed64-476e-9acc-0989d81f4db5,MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,0.783259,"Visual language data such as plots, charts, and infographics are ubiquitous
in the human world. However, state-of-the-art vision-language models do not
perform well on these data. We propose MatCha (Math reasoning and Chart
derendering pretraining) to enhance visual language models' capabilities in
jointly modeling charts/plots and language data. Specifically, we propose
several pretraining tasks that cover plot deconstruction and numerical
reasoning which are the key capabilities in visual language modeling.
  We perform the MatCha pretraining starting from Pix2Struct, a recently
proposed image-to-text visual language model. On standard benchmarks such as
PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as
much as nearly 20%. We also examine how well MatCha pretraining transfers to
domains such as screenshots, textbook diagrams, and document figures and
observe overall improvement, verifying the usefulness of MatCha pretraining on
broader visual language tasks.",https://github.com/google-research/google-research/tree/master/deplot,-1
74a1795b-ba3e-4be3-89c5-e802b9005ecb,CROON: Automatic Multi-LiDAR Calibration and Refinement Method in Road Scene,0.405083,"Sensor-based environmental perception is a crucial part of the autonomous
driving system. In order to get an excellent perception of the surrounding
environment, an intelligent system would configure multiple LiDARs (3D Light
Detection and Ranging) to cover the distant and near space of the car. The
precision of perception relies on the quality of sensor calibration. This
research aims at developing an accurate, automatic, and robust calibration
strategy for multiple LiDAR systems in the general road scene. We thus propose
CROON (automatiC multi-LiDAR CalibratiOn and Refinement method in rOad sceNe),
a two-stage method including rough and refinement calibration. The first stage
can calibrate the sensor from an arbitrary initial pose, and the second stage
is able to precisely calibrate the sensor iteratively. Specifically, CROON
utilize the nature characteristics of road scene so that it is independent and
easy to apply in large-scale conditions. Experimental results on real-world and
simulated data sets demonstrate the reliability and accuracy of our method. All
the related data sets and codes are open-sourced on the Github website
https://github.com/OpenCalib/LiDAR2LiDAR.",https://github.com/OpenCalib/LiDAR2LiDAR,-1
ac6922c3-6ee1-498c-9e11-b2b1ad20ae02,Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes,0.281679,"Analogy-making gives rise to reasoning, abstraction, flexible categorization
and counterfactual inference -- abilities lacking in even the best AI systems
today. Much research has suggested that analogies are key to non-brittle
systems that can adapt to new domains. Despite their importance, analogies
received little attention in the NLP community, with most research focusing on
simple word analogies. Work that tackled more complex analogies relied heavily
on manually constructed, hard-to-scale input representations. In this work, we
explore a more realistic, challenging setup: our input is a pair of natural
language procedural texts, describing a situation or a process (e.g., how the
heart works/how a pump works). Our goal is to automatically extract entities
and their relations from the text and find a mapping between the different
domains based on relational similarity (e.g., blood is mapped to water). We
develop an interpretable, scalable algorithm and demonstrate that it identifies
the correct mappings 87% of the time for procedural texts and 94% for stories
from cognitive-psychology literature. We show it can extract analogies from a
large dataset of procedural texts, achieving 79% precision (analogy prevalence
in data: 3%). Lastly, we demonstrate that our algorithm is robust to
paraphrasing the input texts.",https://github.com/orensul/analogies_mining,-1
4d4c23bc-51ad-4bf3-b693-54723190de50,Optimization of Directional Landmark Deployment for Visual Observer on SE(3),0.084995,"An optimization method is proposed in this paper for novel deployment of
given number of directional landmarks (location and pose) within a given region
in the 3-D task space. This new deployment technique is built on the geometric
models of both landmarks and the monocular camera. In particular, a new concept
of Multiple Coverage Probability (MCP) is defined to characterize the
probability of at least n landmarks being covered simultaneously by a camera at
a fixed position. The optimization is conducted with respect to the position
and pose of the given number of landmarks to maximize MCP through globally
exploration of the given 3-D space. By adopting the elimination genetic
algorithm, the global optimal solutions can be obtained, which are then applied
to improve the convergent performance of the visual observer on SE(3) as a
demonstration example. Both simulation and experimental results are presented
to validate the effectiveness of the proposed landmark deployment optimization
method.",None,-1
26de8a97-12c7-477a-96a3-b721a444b99e,Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema Linking Graph,0.763441,"The generalizability to new databases is of vital importance to Text-to-SQL
systems which aim to parse human utterances into SQL statements. Existing works
achieve this goal by leveraging the exact matching method to identify the
lexical matching between the question words and the schema items. However,
these methods fail in other challenging scenarios, such as the synonym
substitution in which the surface form differs between the corresponding
question words and schema items. In this paper, we propose a framework named
ISESL-SQL to iteratively build a semantic enhanced schema-linking graph between
question tokens and database schemas. First, we extract a schema linking graph
from PLMs through a probing procedure in an unsupervised manner. Then the
schema linking graph is further optimized during the training process through a
deep graph learning method. Meanwhile, we also design an auxiliary task called
graph regularization to improve the schema information mentioned in the
schema-linking graph. Extensive experiments on three benchmarks demonstrate
that ISESL-SQL could consistently outperform the baselines and further
investigations show its generalizability and robustness.",https://github.com/THU-BPM/ISESL-SQL,7471
267eb576-f730-4dcb-8ecf-ca8bd9f5881d,Learning Facial Liveness Representation for Domain Generalized Face Anti-spoofing,0.360746,"Face anti-spoofing (FAS) aims at distinguishing face spoof attacks from the
authentic ones, which is typically approached by learning proper models for
performing the associated classification task. In practice, one would expect
such models to be generalized to FAS in different image domains. Moreover, it
is not practical to assume that the type of spoof attacks would be known in
advance. In this paper, we propose a deep learning model for addressing the
aforementioned domain-generalized face anti-spoofing task. In particular, our
proposed network is able to disentangle facial liveness representation from the
irrelevant ones (i.e., facial content and image domain features). The resulting
liveness representation exhibits sufficient domain invariant properties, and
thus it can be applied for performing domain-generalized FAS. In our
experiments, we conduct experiments on five benchmark datasets with various
settings, and we verify that our model performs favorably against
state-of-the-art approaches in identifying novel types of spoof attacks in
unseen image domains.",None,9487
33cffb09-a51a-4892-94a3-3e2a2e45f169,Multi-task Active Learning for Pre-trained Transformer-based Models,0.338477,"Multi-task learning, in which several tasks are jointly learned by a single
model, allows NLP models to share information from multiple annotations and may
facilitate better predictions when the tasks are inter-related. This technique,
however, requires annotating the same text with multiple annotation schemes
which may be costly and laborious. Active learning (AL) has been demonstrated
to optimize annotation processes by iteratively selecting unlabeled examples
whose annotation is most valuable for the NLP model. Yet, multi-task active
learning (MT-AL) has not been applied to state-of-the-art pre-trained
Transformer-based NLP models. This paper aims to close this gap. We explore
various multi-task selection criteria in three realistic multi-task scenarios,
reflecting different relations between the participating tasks, and demonstrate
the effectiveness of multi-task compared to single-task selection. Our results
suggest that MT-AL can be effectively used in order to minimize annotation
efforts for multi-task NLP models.",https://github.com/rotmanguy/MTAL,7661
ad7f95ad-86fe-4eaa-a488-b6ba088febca,PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence,0.432169,"Work on social media rumour verification utilises signals from posts, their
propagation and users involved. Other lines of work target identifying and
fact-checking claims based on information from Wikipedia, or trustworthy news
articles without considering social media context. However works combining the
information from social media with external evidence from the wider web are
lacking. To facilitate research in this direction, we release a novel dataset,
PHEMEPlus, an extension of the PHEME benchmark, which contains social media
conversations as well as relevant external evidence for each rumour. We
demonstrate the effectiveness of incorporating such evidence in improving
rumour verification models. Additionally, as part of the evidence collection,
we evaluate various ways of query formulation to identify the most effective
method.",https://github.com/JohnNLP/PhemePlus,-1
48655f97-01e6-4d68-a2b7-9c64ca0aad2f,Reducing Exploitability with Population Based Training,0.468461,"Self-play reinforcement learning has achieved state-of-the-art, and often
superhuman, performance in a variety of zero-sum games. Yet prior work has
found that policies that are highly capable against regular opponents can fail
catastrophically against adversarial policies: an opponent trained explicitly
against the victim. Prior defenses using adversarial training were able to make
the victim robust to a specific adversary, but the victim remained vulnerable
to new ones. We conjecture this limitation was due to insufficient diversity of
adversaries seen during training. We analyze a defense using population based
training to pit the victim against a diverse set of opponents. We evaluate this
defense's robustness against new adversaries in two low-dimensional
environments. This defense increases robustness against adversaries, as
measured by the number of attacker training timesteps to exploit the victim.
Furthermore, we show that robustness is correlated with the size of the
opponent population.",https://github.com/HumanCompatibleAI/reducing-exploitability,-1
b00bba84-6a85-4451-9b8d-3f75d178d682,A Curriculum Learning Approach for Multi-domain Text Classification Using Keyword weight Ranking,0.14201,"Text classification is a very classic NLP task, but it has two prominent
shortcomings: On the one hand, text classification is deeply domain-dependent.
That is, a classifier trained on the corpus of one domain may not perform so
well in another domain. On the other hand, text classification models require a
lot of annotated data for training. However, for some domains, there may not
exist enough annotated data. Therefore, it is valuable to investigate how to
efficiently utilize text data from different domains to improve the performance
of models in various domains. Some multi-domain text classification models are
trained by adversarial training to extract shared features among all domains
and the specific features of each domain. We noted that the distinctness of the
domain-specific features is different, so in this paper, we propose to use a
curriculum learning strategy based on keyword weight ranking to improve the
performance of multi-domain text classification models. The experimental
results on the Amazon review and FDU-MTL datasets show that our curriculum
learning strategy effectively improves the performance of multi-domain text
classification models based on adversarial learning and outperforms
state-of-the-art methods.",None,-1
62dda78a-0855-4c18-ac4b-532c562fa6a4,Better Few-Shot Relation Extraction with Label Prompt Dropout,0.667347,"Few-shot relation extraction aims to learn to identify the relation between
two entities based on very limited training examples. Recent efforts found that
textual labels (i.e., relation names and relation descriptions) could be
extremely useful for learning class representations, which will benefit the
few-shot learning task. However, what is the best way to leverage such label
information in the learning process is an important research question. Existing
works largely assume such textual labels are always present during both
learning and prediction. In this work, we argue that such approaches may not
always lead to optimal results. Instead, we present a novel approach called
label prompt dropout, which randomly removes label descriptions in the learning
process. Our experiments show that our approach is able to lead to improved
class representations, yielding significantly better results on the few-shot
relation extraction task.",https://github.com/jzhang38/LPD,-1
280c4df9-d3ca-428c-9158-13011dfeaa10,BIOS: An Algorithmically Generated Biomedical Knowledge Graph,0.412466,"Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for
biomedical and healthcare big data and artificial intelligence (AI),
facilitating natural language processing, model development, and data exchange.
For decades, these knowledge graphs have been developed via expert curation;
however, this method can no longer keep up with today's AI development, and a
transition to algorithmically generated BioMedKGs is necessary. In this work,
we introduce the Biomedical Informatics Ontology System (BIOS), the first
large-scale publicly available BioMedKG generated completely by machine
learning algorithms. BIOS currently contains 4.1 million concepts, 7.4 million
terms in two languages, and 7.3 million relation triplets. We present the
methodology for developing BIOS, including the curation of raw biomedical
terms, computational identification of synonymous terms and aggregation of
these terms to create concept nodes, semantic type classification of the
concepts, relation identification, and biomedical machine translation. We
provide statistics on the current BIOS content and perform preliminary
assessments of term quality, synonym grouping, and relation extraction. The
results suggest that machine learning-based BioMedKG development is a viable
alternative to traditional expert curation.",None,-1
891d023b-d167-4b8f-b8d4-1232c2c05c8b,Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?,0.606776,"Several popular Transformer based language models have been found to be
successful for text-driven brain encoding. However, existing literature
leverages only pretrained text Transformer models and has not explored the
efficacy of task-specific learned Transformer representations. In this work, we
explore transfer learning from representations learned for ten popular natural
language processing tasks (two syntactic and eight semantic) for predicting
brain responses from two diverse datasets: Pereira (subjects reading sentences
from paragraphs) and Narratives (subjects listening to the spoken stories).
Encoding models based on task features are used to predict activity in
different regions across the whole brain. Features from coreference resolution,
NER, and shallow syntax parsing explain greater variance for the reading
activity. On the other hand, for the listening activity, tasks such as
paraphrase generation, summarization, and natural language inference show
better encoding performance. Experiments across all 10 task representations
provide the following cognitive insights: (i) language left hemisphere has
higher predictive brain activity versus language right hemisphere, (ii)
posterior medial cortex, temporo-parieto-occipital junction, dorsal frontal
lobe have higher correlation versus early auditory and auditory association
cortex, (iii) syntactic and semantic tasks display a good predictive
performance across brain regions for reading and listening stimuli resp.",None,-1
8a376ddb-bd1c-4d96-a781-8cc43b0c3441,Designing Network Design Strategies Through Gradient Path Analysis,0.997823,"Designing a high-efficiency and high-quality expressive network architecture
has always been the most important research topic in the field of deep
learning. Most of today's network design strategies focus on how to integrate
features extracted from different layers, and how to design computing units to
effectively extract these features, thereby enhancing the expressiveness of the
network. This paper proposes a new network design strategy, i.e., to design the
network architecture based on gradient path analysis. On the whole, most of
today's mainstream network design strategies are based on feed forward path,
that is, the network architecture is designed based on the data path. In this
paper, we hope to enhance the expressive ability of the trained model by
improving the network learning ability. Due to the mechanism driving the
network parameter learning is the backward propagation algorithm, we design
network design strategies based on back propagation path. We propose the
gradient path design strategies for the layer-level, the stage-level, and the
network-level, and the design strategies are proved to be superior and feasible
from theoretical analysis and experiments.",https://github.com/ultralytics/yolov5/,-1
7c65cb6b-6c11-4773-9031-16a13e1d2492,Hero-Gang Neural Model For Named Entity Recognition,0.667511,"Named entity recognition (NER) is a fundamental and important task in NLP,
aiming at identifying named entities (NEs) from free text. Recently, since the
multi-head attention mechanism applied in the Transformer model can effectively
capture longer contextual information, Transformer-based models have become the
mainstream methods and have achieved significant performance in this task.
Unfortunately, although these models can capture effective global context
information, they are still limited in the local feature and position
information extraction, which is critical in NER. In this paper, to address
this limitation, we propose a novel Hero-Gang Neural structure (HGN), including
the Hero and Gang module, to leverage both global and local information to
promote NER. Specifically, the Hero module is composed of a Transformer-based
encoder to maintain the advantage of the self-attention mechanism, and the Gang
module utilizes a multi-window recurrent module to extract local features and
position information under the guidance of the Hero module. Afterward, the
proposed multi-window attention effectively combines global information and
multiple local features for predicting entity labels. Experimental results on
several benchmark datasets demonstrate the effectiveness of our proposed model.",https://github.com/jinpeng01/HGN,-1
5af61ddc-1d66-478f-b796-a73cc1f31ef2,Learning Representations for Hyper-Relational Knowledge Graphs,0.782163,"Knowledge graphs (KGs) have gained prominence for their ability to learn
representations for uni-relational facts. Recently, research has focused on
modeling hyper-relational facts, which move beyond the restriction of
uni-relational facts and allow us to represent more complex and real-world
information. However, existing approaches for learning representations on
hyper-relational KGs majorly focus on enhancing the communication from
qualifiers to base triples while overlooking the flow of information from base
triple to qualifiers. This can lead to suboptimal qualifier representations,
especially when a large amount of qualifiers are presented. It motivates us to
design a framework that utilizes multiple aggregators to learn representations
for hyper-relational facts: one from the perspective of the base triple and the
other one from the perspective of the qualifiers. Experiments demonstrate the
effectiveness of our framework for hyper-relational knowledge graph completion
across multiple datasets. Furthermore, we conduct an ablation study that
validates the importance of the various components in our framework. The code
to reproduce our results can be found at
\url{https://github.com/HarryShomer/QUAD}.",https://github.com/HarryShomer/QUAD,-1
713274a2-f282-4f12-b22d-f1a3e7f9428d,EfficientNeRF: Efficient Neural Radiance Fields,0.894623,"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.",https://github.com/dvlab-research/EfcientNeRF,-1
09d6c9cd-366c-4953-bea4-6d2263e30ae8,BayesFormer: Transformer with Uncertainty Estimation,0.426247,"Transformer has become ubiquitous due to its dominant performance in various
NLP and image processing tasks. However, it lacks understanding of how to
generate mathematically grounded uncertainty estimates for transformer
architectures. Models equipped with such uncertainty estimates can typically
improve predictive performance, make networks robust, avoid over-fitting and
used as acquisition function in active learning. In this paper, we introduce
BayesFormer, a Transformer model with dropouts designed by Bayesian theory. We
proposed a new theoretical framework to extend the approximate variational
inference-based dropout to Transformer-based architectures. Through extensive
experiments, we validate the proposed architecture in four paradigms and show
improvements across the board: language modeling and classification,
long-sequence understanding, machine translation and acquisition function for
active learning.",None,-1
04b992a2-2f68-4235-8366-7694d991edd6,Pushing the Performance Limit of Scene Text Recognizer without Human Annotation,0.272282,"Scene text recognition (STR) attracts much attention over the years because
of its wide application. Most methods train STR model in a fully supervised
manner which requires large amounts of labeled data. Although synthetic data
contributes a lot to STR, it suffers from the real-tosynthetic domain gap that
restricts model performance. In this work, we aim to boost STR models by
leveraging both synthetic data and the numerous real unlabeled images,
exempting human annotation cost thoroughly. A robust consistency regularization
based semi-supervised framework is proposed for STR, which can effectively
solve the instability issue due to domain inconsistency between synthetic and
real images. A character-level consistency regularization is designed to
mitigate the misalignment between characters in sequence recognition. Extensive
experiments on standard text recognition benchmarks demonstrate the
effectiveness of the proposed method. It can steadily improve existing STR
models, and boost an STR model to achieve new state-of-the-art results. To our
best knowledge, this is the first consistency regularization based framework
that applies successfully to STR.",None,-1
15d55e96-66a1-4d0a-974f-2d18764165ef,CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task,0.997742,"We present the joint contribution of IST and Unbabel to the WMT 2022 Shared
Task on Quality Estimation (QE). Our team participated on all three subtasks:
(i) Sentence and Word-level Quality Prediction; (ii) Explainable QE; and (iii)
Critical Error Detection. For all tasks we build on top of the COMET framework,
connecting it with the predictor-estimator architecture of OpenKiwi, and
equipping it with a word-level sequence tagger and an explanation extractor.
Our results suggest that incorporating references during pretraining improves
performance across several language pairs on downstream tasks, and that jointly
training with sentence and word-level objectives yields a further boost.
Furthermore, combining attention and gradient information proved to be the top
strategy for extracting good explanations of sentence-level QE models. Overall,
our submissions achieved the best results for all three tasks for almost all
language pairs by a considerable margin.",https://github.com/Unbabel/COMET,-1
a5169f59-fae1-41cb-85ef-b2a60134bdee,Image Classification with Small Datasets: Overview and Benchmark,0.649493,"Image classification with small datasets has been an active research area in
the recent past. However, as research in this scope is still in its infancy,
two key ingredients are missing for ensuring reliable and truthful progress: a
systematic and extensive overview of the state of the art, and a common
benchmark to allow for objective comparisons between published methods. This
article addresses both issues. First, we systematically organize and connect
past studies to consolidate a community that is currently fragmented and
scattered. Second, we propose a common benchmark that allows for an objective
comparison of approaches. It consists of five datasets spanning various domains
(e.g., natural images, medical imagery, satellite data) and data types (RGB,
grayscale, multispectral). We use this benchmark to re-evaluate the standard
cross-entropy baseline and ten existing methods published between 2017 and 2021
at renowned venues. Surprisingly, we find that thorough hyper-parameter tuning
on held-out validation data results in a highly competitive baseline and
highlights a stunted growth of performance over the years. Indeed, only a
single specialized method dating back to 2019 clearly wins our benchmark and
outperforms the baseline classifier.",https://github.com/lorenzobrigato/gem,-1
7e2f13de-c604-4a81-a8a3-e957612d9e7f,GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models,0.871951,"Prevalent semantic segmentation solutions are, in essence, a dense
discriminative classifier of p(class|pixel feature). Though straightforward,
this de facto paradigm neglects the underlying data distribution p(pixel
feature|class), and struggles to identify out-of-distribution data. Going
beyond this, we propose GMMSeg, a new family of segmentation models that rely
on a dense generative classifier for the joint distribution p(pixel
feature,class). For each class, GMMSeg builds Gaussian Mixture Models (GMMs)
via Expectation-Maximization (EM), so as to capture class-conditional
densities. Meanwhile, the deep dense representation is end-to-end trained in a
discriminative manner, i.e., maximizing p(class|pixel feature). This endows
GMMSeg with the strengths of both generative and discriminative models. With a
variety of segmentation architectures and backbones, GMMSeg outperforms the
discriminative counterparts on three closed-set datasets. More impressively,
without any modification, GMMSeg even performs well on open-world datasets. We
believe this work brings fundamental insights into the related fields.",https://github.com/leonnnop/GMMSeg,-1
cd951cec-3e0e-480b-9a0b-bdb212bbe05e,Comparing Performance of Different Linguistically-Backed Word Embeddings for Cyberbullying Detection,0.00517364,"In most cases, word embeddings are learned only from raw tokens or in some
cases, lemmas. This includes pre-trained language models like BERT. To
investigate on the potential of capturing deeper relations between lexical
items and structures and to filter out redundant information, we propose to
preserve the morphological, syntactic and other types of linguistic information
by combining them with the raw tokens or lemmas. This means, for example,
including parts-of-speech or dependency information within the used lexical
features. The word embeddings can then be trained on the combinations instead
of just raw tokens. It is also possible to later apply this method to the
pre-training of huge language models and possibly enhance their performance.
This would aid in tackling problems which are more sophisticated from the point
of view of linguistic representation, such as detection of cyberbullying.",None,-1
41c07ff2-d73a-4d73-8fbc-730480811b98,Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching,0.704124,"Ontology Matching (OM) plays an important role in many domains such as
bioinformatics and the Semantic Web, and its research is becoming increasingly
popular, especially with the application of machine learning (ML) techniques.
Although the Ontology Alignment Evaluation Initiative (OAEI) represents an
impressive effort for the systematic evaluation of OM systems, it still suffers
from several limitations including limited evaluation of subsumption mappings,
suboptimal reference mappings, and limited support for the evaluation of
ML-based systems. To tackle these limitations, we introduce five new biomedical
OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes
both equivalence and subsumption matching; the quality of reference mappings is
ensured by human curation, ontology pruning, etc.; and a comprehensive
evaluation framework is proposed to measure OM performance from various
perspectives for both ML-based and non-ML-based OM systems. We report
evaluation results for OM systems of different types to demonstrate the usage
of these resources, all of which are publicly available as part of the new
BioML track at OAEI 2022.",https://github.com/KRR-Oxford/DeepOnto,-1
5b1bd9a9-be38-4e8a-ac80-9238b2876521,ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select,0.64656,"We study the problem of extracting N-ary relation tuples from scientific
articles. This task is challenging because the target knowledge tuples can
reside in multiple parts and modalities of the document. Our proposed method
ReSel decomposes this task into a two-stage procedure that first retrieves the
most relevant paragraph/table and then selects the target entity from the
retrieved component. For the high-level retrieval stage, ReSel designs a simple
and effective feature set, which captures multi-level lexical and semantic
similarities between the query and components. For the low-level selection
stage, ReSel designs a cross-modal entity correlation graph along with a
multi-view architecture, which models both semantic and document-structural
relations between entities. Our experiments on three scientific information
extraction datasets show that ReSel outperforms state-of-the-art baselines
significantly.",https://github.com/night-chen/ReSel,-1
5a29b876-7f3d-4456-b8b4-8b5e32efb5f6,Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping,0.656316,"In machine learning, an agent needs to estimate uncertainty to efficiently
explore and adapt and to make effective decisions. A common approach to
uncertainty estimation maintains an ensemble of models. In recent years,
several approaches have been proposed for training ensembles, and conflicting
views prevail with regards to the importance of various ingredients of these
approaches. In this paper, we aim to address the benefits of two ingredients --
prior functions and bootstrapping -- which have come into question. We show
that prior functions can significantly improve an ensemble agent's joint
predictions across inputs and that bootstrapping affords additional benefits if
the signal-to-noise ratio varies across inputs. Our claims are justified by
both theoretical and experimental results.",https://github.com/deepmind/enn,-1
7d21ba25-d9f5-4ab5-b105-da5809a3f785,Correlation-Aware Deep Tracking,0.97059,"Robustness and discrimination power are two fundamental requirements in
visual object tracking. In most tracking paradigms, we find that the features
extracted by the popular Siamese-like networks cannot fully discriminatively
model the tracked targets and distractor objects, hindering them from
simultaneously meeting these two requirements. While most methods focus on
designing robust correlation operations, we propose a novel target-dependent
feature network inspired by the self-/cross-attention scheme. In contrast to
the Siamese-like feature extraction, our network deeply embeds cross-image
feature correlation in multiple layers of the feature network. By extensively
matching the features of the two images through multiple layers, it is able to
suppress non-target features, resulting in instance-varying feature extraction.
The output features of the search image can be directly used for predicting
target locations without extra correlation step. Moreover, our model can be
flexibly pre-trained on abundant unpaired images, leading to notably faster
convergence than the existing methods. Extensive experiments show our method
achieves the state-of-the-art results while running at real-time. Our feature
networks also can be applied to existing tracking pipelines seamlessly to raise
the tracking performance. Code will be available.",https://github.com/visionml/pytracking,-1
14bef619-4e8a-4e37-ab81-56154af52411,Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition,0.634918,"Due to the absence of explicit connectives, implicit discourse relation
recognition (IDRR) remains a challenging task in discourse analysis. The
critical step for IDRR is to learn high-quality discourse relation
representations between two arguments. Recent methods tend to integrate the
whole hierarchical information of senses into discourse relation
representations for multi-level sense recognition. Nevertheless, they
insufficiently incorporate the static hierarchical structure containing all
senses (defined as global hierarchy), and ignore the hierarchical sense label
sequence corresponding to each instance (defined as local hierarchy). For the
purpose of sufficiently exploiting global and local hierarchies of senses to
learn better discourse relation representations, we propose a novel GlObal and
Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of
hierarchies with the aid of multi-task learning and contrastive learning.
Experimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our
method remarkably outperforms current state-of-the-art models at all
hierarchical levels. Our code is publicly available at
https://github.com/YJiangcm/GOLF_for_IDRR",https://github.com/YJiangcm/GOLF_for_IDRR,-1
5502c399-df1a-4bd7-b880-4405d7785a77,Edge-enhanced Feature Distillation Network for Efficient Super-Resolution,0.606906,"With the recently massive development in convolution neural networks,
numerous lightweight CNN-based image super-resolution methods have been
proposed for practical deployments on edge devices. However, most existing
methods focus on one specific aspect: network or loss design, which leads to
the difficulty of minimizing the model size. To address the issue, we conclude
block devising, architecture searching, and loss design to obtain a more
efficient SR structure. In this paper, we proposed an edge-enhanced feature
distillation network, named EFDN, to preserve the high-frequency information
under constrained resources. In detail, we build an edge-enhanced convolution
block based on the existing reparameterization methods. Meanwhile, we propose
edge-enhanced gradient loss to calibrate the reparameterized path training.
Experimental results show that our edge-enhanced strategies preserve the edge
and significantly improve the final restoration quality. Code is available at
https://github.com/icandle/EFDN.",https://github.com/icandle/EFDN,-1
da26ef92-f520-476e-b7a9-3a3f1e05519b,Representation Learning for Resource-Constrained Keyphrase Generation,0.178092,"State-of-the-art keyphrase generation methods generally depend on large
annotated datasets, limiting their performance in domains with limited
annotated data. To overcome this challenge, we design a data-oriented approach
that first identifies salient information using retrieval-based corpus-level
statistics, and then learns a task-specific intermediate representation based
on a pre-trained language model using large-scale unlabeled documents. We
introduce salient span recovery and salient span prediction as denoising
training objectives that condense the intra-article and inter-article knowledge
essential for keyphrase generation. Through experiments on multiple keyphrase
generation benchmarks, we show the effectiveness of the proposed approach for
facilitating low-resource keyphrase generation and zero-shot domain adaptation.
Our method especially benefits the generation of absent keyphrases, approaching
the performance of models trained with large training sets.",https://github.com/xiaowu0162/low-resource-kpgen,-1
cca70b9b-4812-4c99-81ba-41febe45c4a6,FreGAN: Exploiting Frequency Components for Training GANs under Limited Data,0.862824,"Training GANs under limited data often leads to discriminator overfitting and
memorization issues, causing divergent training. Existing approaches mitigate
the overfitting by employing data augmentations, model regularization, or
attention mechanisms. However, they ignore the frequency bias of GANs and take
poor consideration towards frequency information, especially high-frequency
signals that contain rich details. To fully utilize the frequency information
of limited data, this paper proposes FreGAN, which raises the model's frequency
awareness and draws more attention to producing high-frequency signals,
facilitating high-quality generation. In addition to exploiting both real and
generated images' frequency information, we also involve the frequency signals
of real images as a self-supervised constraint, which alleviates the GAN
disequilibrium and encourages the generator to synthesize adequate rather than
arbitrary frequency signals. Extensive results demonstrate the superiority and
effectiveness of our FreGAN in ameliorating generation quality in the low-data
regime (especially when training data is less than 100). Besides, FreGAN can be
seamlessly applied to existing regularization and attention mechanism models to
further boost the performance.",https://github.com/kobeshegu/FreGAN_NeurIPS2022,-1
b15dd227-71c9-42fb-a613-b6e88b7d1731,Cold Posteriors through PAC-Bayes,0.426247,"We investigate the cold posterior effect through the lens of PAC-Bayes
generalization bounds. We argue that in the non-asymptotic setting, when the
number of training samples is (relatively) small, discussions of the cold
posterior effect should take into account that approximate Bayesian inference
does not readily provide guarantees of performance on out-of-sample data.
Instead, out-of-sample error is better described through a generalization
bound. In this context, we explore the connections between the ELBO objective
from variational inference and the PAC-Bayes objectives. We note that, while
the ELBO and PAC-Bayes objectives are similar, the latter objectives naturally
contain a temperature parameter $\lambda$ which is not restricted to be
$\lambda=1$. For both regression and classification tasks, in the case of
isotropic Laplace approximations to the posterior, we show how this
PAC-Bayesian interpretation of the temperature parameter captures the cold
posterior effect.",None,-1
d44be552-8f92-4164-b69d-399bc80d9b67,Pre-training to Match for Unified Low-shot Relation Extraction,0.588604,"Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard.",https://github.com/fc-liu/MCMN,-1
73428662-21b4-47a9-8231-3bb9efd6c27b,Hyper-parameter tuning of physics-informed neural networks: Application to Helmholtz problems,0.564891,"We consider physics-informed neural networks (PINNs) [Raissi et al.,
J.~Comput. Phys. 278 (2019) 686-707] for forward physical problems. In order to
find optimal PINNs configuration, we introduce a hyper-parameter optimization
(HPO) procedure via Gaussian processes-based Bayesian optimization. We apply
the HPO to Helmholtz equation for bounded domains and conduct a thorough study,
focusing on: (i) performance, (ii) the collocation points density $r$ and (iii)
the frequency $\kappa$, confirming the applicability and necessity of the
method. Numerical experiments are performed in two and three dimensions,
including comparison to finite element methods.",https://github.com/lululxvi/deepxde/,-1
93c5c6d7-32d8-4672-9308-842c3214e1c9,From Understanding the Population Dynamics of the NSGA-II to the First Proven Lower Bounds,0.981402,"Due to the more complicated population dynamics of the NSGA-II, none of the
existing runtime guarantees for this algorithm is accompanied by a non-trivial
lower bound. Via a first mathematical understanding of the population dynamics
of the NSGA-II, that is, by estimating the expected number of individuals
having a certain objective value, we prove that the NSGA-II with suitable
population size needs $\Omega(Nn\log n)$ function evaluations to find the
Pareto front of the OneMinMax problem and $\Omega(Nn^k)$ evaluations on the
OneJumpZeroJump problem with jump size $k$. These bounds are asymptotically
tight (that is, they match previously shown upper bounds) and show that the
NSGA-II here does not even in terms of the parallel runtime (number of
iterations) profit from larger population sizes. For the OneJumpZeroJump
problem and when the same sorting is used for the computation of the crowding
distance contributions of the two objectives, we even obtain a runtime estimate
that is tight including the leading constant.",None,-1
c58397b0-f373-4556-9969-e7cb83bd7cc9,Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation,0.922771,"Non-autoregressive translation (NAT) models are typically trained with the
cross-entropy loss, which forces the model outputs to be aligned verbatim with
the target sentence and will highly penalize small shifts in word positions.
Latent alignment models relax the explicit alignment by marginalizing out all
monotonic latent alignments with the CTC loss. However, they cannot handle
non-monotonic alignments, which is non-negligible as there is typically global
word reordering in machine translation. In this work, we explore non-monotonic
latent alignments for NAT. We extend the alignment space to non-monotonic
alignments to allow for the global word reordering and further consider all
alignments that overlap with the target sentence. We non-monotonically match
the alignments to the target sentence and train the latent alignment model to
maximize the F1 score of non-monotonic matching. Extensive experiments on major
WMT benchmarks show that our method substantially improves the translation
performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14
En-De with only one-iteration decoding, closing the gap between
non-autoregressive and autoregressive models.",None,-1
f194b725-2a3c-47b7-a96e-12f8f4b17967,SYNERgy between SYNaptic consolidation and Experience Replay for general continual learning,0.341433,"Continual learning (CL) in the brain is facilitated by a complex set of
mechanisms. This includes the interplay of multiple memory systems for
consolidating information as posited by the complementary learning systems
(CLS) theory and synaptic consolidation for protecting the acquired knowledge
from erasure. Thus, we propose a general CL method that creates a synergy
between SYNaptic consolidation and dual memory Experience Replay (SYNERgy). Our
method maintains a semantic memory that accumulates and consolidates
information across the tasks and interacts with the episodic memory for
effective replay. It further employs synaptic consolidation by tracking the
importance of parameters during the training trajectory and anchoring them to
the consolidated parameters in the semantic memory. To the best of our
knowledge, our study is the first to employ dual memory experience replay in
conjunction with synaptic consolidation that is suitable for general CL whereby
the network does not utilize task boundaries or task labels during training or
inference. Our evaluation on various challenging CL scenarios and
characteristics analyses demonstrate the efficacy of incorporating both
synaptic consolidation and CLS theory in enabling effective CL in DNNs.",https://github.com/NeurAI-Lab/SYNERgy,-1
0faae8ae-6191-4d00-9054-13abb3fe4a54,Learning-Based Dimensionality Reduction for Computing Compact and Effective Local Feature Descriptors,0.307016,"A distinctive representation of image patches in form of features is a key
component of many computer vision and robotics tasks, such as image matching,
image retrieval, and visual localization. State-of-the-art descriptors, from
hand-crafted descriptors such as SIFT to learned ones such as HardNet, are
usually high dimensional; 128 dimensions or even more. The higher the
dimensionality, the larger the memory consumption and computational time for
approaches using such descriptors. In this paper, we investigate multi-layer
perceptrons (MLPs) to extract low-dimensional but high-quality descriptors. We
thoroughly analyze our method in unsupervised, self-supervised, and supervised
settings, and evaluate the dimensionality reduction results on four
representative descriptors. We consider different applications, including
visual localization, patch verification, image matching and retrieval. The
experiments show that our lightweight MLPs achieve better dimensionality
reduction than PCA. The lower-dimensional descriptors generated by our approach
outperform the original higher-dimensional descriptors in downstream tasks,
especially for the hand-crafted ones. The code will be available at
https://github.com/PRBonn/descriptor-dr.",https://github.com/PRBonn/descriptor-dr,-1
753871d4-6984-46ae-a7e4-e50b9fd2ee73,Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning,0.707335,"Recent advances in deep learning significantly boost the performance of
salient object detection (SOD) at the expense of labeling larger-scale
per-pixel annotations. To relieve the burden of labor-intensive labeling, deep
unsupervised SOD methods have been proposed to exploit noisy labels generated
by handcrafted saliency methods. However, it is still difficult to learn
accurate saliency details from rough noisy labels. In this paper, we propose to
learn saliency from synthetic but clean labels, which naturally has higher
pixel-labeling quality without the effort of manual annotations. Specifically,
we first construct a novel synthetic SOD dataset by a simple copy-paste
strategy. Considering the large appearance differences between the synthetic
and real-world scenarios, directly training with synthetic data will lead to
performance degradation on real-world scenarios. To mitigate this problem, we
propose a novel unsupervised domain adaptive SOD method to adapt between these
two domains by uncertainty-aware self-training. Experimental results show that
our proposed method outperforms the existing state-of-the-art deep unsupervised
SOD methods on several benchmark datasets, and is even comparable to
fully-supervised ones.",None,-1
c82fbaa1-9799-4ace-924d-0797a56e25f1,The State of Sparse Training in Deep Reinforcement Learning,0.597717,"The use of sparse neural networks has seen rapid growth in recent years,
particularly in computer vision. Their appeal stems largely from the reduced
number of parameters required to train and store, as well as in an increase in
learning efficiency. Somewhat surprisingly, there have been very few efforts
exploring their use in Deep Reinforcement Learning (DRL). In this work we
perform a systematic investigation into applying a number of existing sparse
training techniques on a variety of DRL agents and environments. Our results
corroborate the findings from sparse training in the computer vision domain -
sparse networks perform better than dense networks for the same parameter count
- in the DRL domain. We provide detailed analyses on how the various components
in DRL are affected by the use of sparse networks and conclude by suggesting
promising avenues for improving the effectiveness of sparse training methods,
as well as for advancing their use in DRL.",None,-1
2f2b5789-c9dd-45d1-a94b-aa93ad706531,Identifying epidemic related Tweets using noisy learning,0.0785126,"Supervised learning algorithms are heavily reliant on annotated datasets to
train machine learning models. However, the curation of the annotated datasets
is laborious and time consuming due to the manual effort involved and has
become a huge bottleneck in supervised learning. In this work, we apply the
theory of noisy learning to generate weak supervision signals instead of manual
annotation. We curate a noisy labeled dataset using a labeling heuristic to
identify epidemic related tweets. We evaluated the performance using a large
epidemic corpus and our results demonstrate that models trained with noisy data
in a class imbalanced and multi-classification weak supervision setting
achieved performance greater than 90%.",None,514
5f95ba8d-38c8-4eb9-a81f-027bdd3086fa,"Improving Depression estimation from facial videos with face alignment, training optimization and scheduling",0.324765,"Deep learning models have shown promising results in recognizing depressive
states using video-based facial expressions. While successful models typically
leverage using 3D-CNNs or video distillation techniques, the different use of
pretraining, data augmentation, preprocessing, and optimization techniques
across experiments makes it difficult to make fair architectural comparisons.
We propose instead to enhance two simple models based on ResNet-50 that use
only static spatial information by using two specific face alignment methods
and improved data augmentation, optimization, and scheduling techniques. Our
extensive experiments on benchmark datasets obtain similar results to
sophisticated spatio-temporal models for single streams, while the score-level
fusion of two different streams outperforms state-of-the-art methods. Our
findings suggest that specific modifications in the preprocessing and training
process result in noticeable differences in the performance of the models and
could hide the actual originally attributed to the use of different neural
network architectures.",None,-1
98db860c-9885-4c3e-b0ca-2efd2df384fe,In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,0.402515,"Single-task models have proven pivotal in solving specific tasks; however,
they have limitations in real-world applications where multi-tasking is
necessary and domain shifts are exhibited. Recently, instructional prompts have
shown significant improvement towards multi-task generalization; however, the
effect of instructional prompts and Multi-Task Learning (MTL) has not been
systematically studied in the biomedical domain. Motivated by this, this paper
explores the impact of instructional prompts for biomedical MTL. We introduce
the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)
various categories. Using this meta-dataset, we propose a unified model termed
In-BoXBART, that can jointly learn all tasks of the BoX without any
task-specific modules. To the best of our knowledge, this is the first attempt
to propose a unified model in the biomedical domain and use instructions to
achieve generalization across several biomedical tasks. Experimental results
indicate that the proposed model: 1) outperforms the single-task baseline by
~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)
shows ~23% improvement compared to the single-task baseline in few-shot
learning (i.e., 32 instances per task) on an average. Our analysis indicates
that there is significant room for improvement across tasks in the BoX,
implying the scope for future research direction.",https://github.com/Mihir3009/In-BoXBART,-1
240acf2e-100a-46cf-b5b2-3032cd243525,ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework,0.86921,"In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost.",https://github.com/GANWANSHUI/ES6D.git,12636
476954fd-bfda-45ad-9cc4-153319fe18cb,Exploring Optimal Granularity for Extractive Summarization of Unstructured Health Records: Analysis of the Largest Multi-Institutional Archive of Health Records in Japan,0.374159,"Automated summarization of clinical texts can reduce the burden of medical
professionals. ""Discharge summaries"" are one promising application of the
summarization, because they can be generated from daily inpatient records. Our
preliminary experiment suggests that 20-31% of the descriptions in discharge
summaries overlap with the content of the inpatient records. However, it
remains unclear how the summaries should be generated from the unstructured
source. To decompose the physician's summarization process, this study aimed to
identify the optimal granularity in summarization. We first defined three types
of summarization units with different granularities to compare the performance
of the discharge summary generation: whole sentences, clinical segments, and
clauses. We defined clinical segments in this study, aiming to express the
smallest medically meaningful concepts. To obtain the clinical segments, it was
necessary to automatically split the texts in the first stage of the pipeline.
Accordingly, we compared rule-based methods and a machine learning method, and
the latter outperformed the formers with an F1 score of 0.846 in the splitting
task. Next, we experimentally measured the accuracy of extractive summarization
using the three types of units, based on the ROUGE-1 metric, on a
multi-institutional national archive of health records in Japan. The measured
accuracies of extractive summarization using whole sentences, clinical
segments, and clauses were 31.91, 36.15, and 25.18, respectively. We found that
the clinical segments yielded higher accuracy than sentences and clauses. This
result indicates that summarization of inpatient records demands finer
granularity than sentence-oriented processing. Although we used only Japanese
health records, it can be interpreted as follows: physicians extract ""concepts
of medical significance"" from patient records and recombine them ...",None,-1
0be7f24d-a14f-4c95-81e1-e22e02c1f951,Sequential Nature of Recommender Systems Disrupts the Evaluation Process,0.0587957,"Datasets are often generated in a sequential manner, where the previous
samples and intermediate decisions or interventions affect subsequent samples.
This is especially prominent in cases where there are significant human-AI
interactions, such as in recommender systems. To characterize the importance of
this relationship across samples, we propose to use adversarial attacks on
popular evaluation processes. We present sequence-aware boosting attacks and
provide a lower bound on the amount of extra information that can be exploited
from a confidential test set solely based on the order of the observed data. We
use real and synthetic data to test our methods and show that the evaluation
process on the MovieLense-100k dataset can be affected by $\sim1\%$ which is
important when considering the close competition. Codes are publicly available.",https://github.com/alishiraliGit/augmented-boosting-attack,-1
b08122b7-f411-478c-8a9b-316a5920bf91,Boosting 3D Adversarial Attacks with Attacking On Frequency,0.648193,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks. Recently, 3D adversarial attacks, especially adversarial attacks on
point clouds, have elicited mounting interest. However, adversarial point
clouds obtained by previous methods show weak transferability and are easy to
defend. To address these problems, in this paper we propose a novel point cloud
attack (dubbed AOF) that pays more attention on the low-frequency component of
point clouds. We combine the losses from point cloud and its low-frequency
component to craft adversarial samples. Extensive experiments validate that AOF
can improve the transferability significantly compared to state-of-the-art
(SOTA) attacks, and is more robust to SOTA 3D defense methods. Otherwise,
compared to clean point clouds, adversarial point clouds obtained by AOF
contain more deformation than outlier.",None,-1
e28fb8bc-5926-468e-9ce9-09d3e0c0fb9a,Converge to the Truth: Factual Error Correction via Iterative Constrained Editing,0.529544,"Given a possibly false claim sentence, how can we automatically correct it
with minimal editing? Existing methods either require a large number of pairs
of false and corrected claims for supervised training or do not handle well
errors spanning over multiple tokens within an utterance. In this paper, we
propose VENCE, a novel method for factual error correction (FEC) with minimal
edits. VENCE formulates the FEC problem as iterative sampling editing actions
with respect to a target density function. We carefully design the target
function with predicted truthfulness scores from an offline trained fact
verification model. VENCE samples the most probable editing positions based on
back-calculated gradients of the truthfulness score concerning input tokens and
the editing actions using a distantly-supervised language model (T5).
Experiments on a public dataset show that VENCE improves the well-adopted SARI
metric by 5.3 (or a relative improvement of 11.8%) over the previous best
distantly-supervised methods.",https://github.com/jiangjiechen/VENCE,-1
1c1db1b6-76c0-4066-a852-ee50d2d49192,Adam Mickiewicz University at WMT 2022: NER-Assisted and Quality-Aware Neural Machine Translation,0.49849,"This paper presents Adam Mickiewicz University's (AMU) submissions to the
constrained track of the WMT 2022 General MT Task. We participated in the
Ukrainian $\leftrightarrow$ Czech translation directions. The systems are a
weighted ensemble of four models based on the Transformer (big) architecture.
The models use source factors to utilize the information about named entities
present in the input. Each of the models in the ensemble was trained using only
the data provided by the shared task organizers. A noisy back-translation
technique was used to augment the training corpora. One of the models in the
ensemble is a document-level model, trained on parallel and synthetic longer
sequences. During the sentence-level decoding process, the ensemble generated
the n-best list. The n-best list was merged with the n-best list generated by a
single document-level model which translated multiple sentences at a time.
Finally, existing quality estimation models and minimum Bayes risk decoding
were used to rerank the n-best list so that the best hypothesis was chosen
according to the COMET evaluation metric. According to the automatic evaluation
results, our systems rank first in both translation directions.",None,-1
20ab1573-960d-4e49-9b3e-edfdfd3e7b53,Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Theoretic Quantities,0.443142,"Recently proposed methods in data subset selection, that is active learning
and active sampling, use Fisher information, Hessians, similarity matrices
based on gradients, and gradient lengths to estimate how informative data is
for a model's training. Are these different approaches connected, and if so,
how? We revisit the fundamentals of Bayesian optimal experiment design and show
that these recently proposed methods can be understood as approximations to
information-theoretic quantities: among them, the mutual information between
predictions and model parameters, known as expected information gain or BALD in
machine learning, and the mutual information between predictions of acquisition
candidates and test samples, known as expected predictive information gain. We
develop a comprehensive set of approximations using Fisher information and
observed information and derive a unified framework that connects seemingly
disparate literature. Although Bayesian methods are often seen as separate from
non-Bayesian ones, the sometimes fuzzy notion of ""informativeness"" expressed in
various non-Bayesian objectives leads to the same couple of information
quantities, which were, in principle, already known by Lindley (1956) and
MacKay (1992).",None,-1
094a051d-2860-412d-999f-4fa5de010f43,Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes,0.214331,"In many sequential tasks, a model needs to remember relevant events from the
distant past to make correct predictions. Unfortunately, a straightforward
application of gradient based training requires intermediate computations to be
stored for every element of a sequence. This requires to store prohibitively
large intermediate data if a sequence consists of thousands or even millions
elements, and as a result, makes learning of very long-term dependencies
infeasible. However, the majority of sequence elements can usually be predicted
by taking into account only temporally local information. On the other hand,
predictions affected by long-term dependencies are sparse and characterized by
high uncertainty given only local information. We propose MemUP, a new training
method that allows to learn long-term dependencies without backpropagating
gradients through the whole sequence at a time. This method can potentially be
applied to any recurrent architecture. LSTM network trained with MemUP performs
better or comparable to baselines while requiring to store less intermediate
data.",https://github.com/griver/memup,1912
8cfd991c-3893-489d-bf9d-1b2e90e5ac19,V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer,0.997489,"In this paper, we investigate the application of Vehicle-to-Everything (V2X)
communication to improve the perception performance of autonomous vehicles. We
present a robust cooperative perception framework with V2X communication using
a novel vision Transformer. Specifically, we build a holistic attention model,
namely V2X-ViT, to effectively fuse information across on-road agents (i.e.,
vehicles and infrastructure). V2X-ViT consists of alternating layers of
heterogeneous multi-agent self-attention and multi-scale window self-attention,
which captures inter-agent interaction and per-agent spatial relationships.
These key modules are designed in a unified Transformer architecture to handle
common V2X challenges, including asynchronous information sharing, pose errors,
and heterogeneity of V2X components. To validate our approach, we create a
large-scale V2X perception dataset using CARLA and OpenCDA. Extensive
experimental results demonstrate that V2X-ViT sets new state-of-the-art
performance for 3D object detection and achieves robust performance even under
harsh, noisy environments. The code is available at
https://github.com/DerrickXuNu/v2x-vit.",https://github.com/DerrickXuNu/v2x-vit,-1
44e843e9-fab1-41e4-a31f-8c846c73bb3e,Learning State-Aware Visual Representations from Audible Interactions,0.387151,"We propose a self-supervised algorithm to learn representations from
egocentric video data. Recently, significant efforts have been made to capture
humans interacting with their own environments as they go about their daily
activities. In result, several large egocentric datasets of interaction-rich
multi-modal data have emerged. However, learning representations from videos
can be challenging. First, given the uncurated nature of long-form continuous
videos, learning effective representations require focusing on moments in time
when interactions take place. Second, visual representations of daily
activities should be sensitive to changes in the state of the environment.
However, current successful multi-modal learning frameworks encourage
representation invariance over time. To address these challenges, we leverage
audio signals to identify moments of likely interactions which are conducive to
better learning. We also propose a novel self-supervised objective that learns
from audible state changes caused by interactions. We validate these
contributions extensively on two large-scale egocentric datasets,
EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on
several downstream tasks, including action recognition, long-term action
anticipation, and object state change classification.",https://github.com/HimangiM/RepLAI,-1
abfe7816-bac2-4de7-9ba0-961ccdf5c0ac,Community Question Answering Entity Linking via Leveraging Auxiliary Data,0.659144,"Community Question Answering (CQA) platforms contain plenty of CQA texts
(i.e., questions and answers corresponding to the question) where named
entities appear ubiquitously. In this paper, we define a new task of CQA entity
linking (CQAEL) as linking the textual entity mentions detected from CQA texts
with their corresponding entities in a knowledge base. This task can facilitate
many downstream applications including expert finding and knowledge base
enrichment. Traditional entity linking methods mainly focus on linking entities
in news documents, and are suboptimal over this new task of CQAEL since they
cannot effectively leverage various informative auxiliary data involved in the
CQA platform to aid entity linking, such as parallel answers and two types of
meta-data (i.e., topic tags and users). To remedy this crucial issue, we
propose a novel transformer-based framework to effectively harness the
knowledge delivered by different kinds of auxiliary data to promote the linking
performance. We validate the superiority of our framework through extensive
experiments over a newly released CQAEL data set against state-of-the-art
entity linking methods.",https://github.com/yhLeeee/CQA EntityLinking,-1
2444e61b-c869-42c0-ab22-5276dc105df2,Anomaly Detection via Reverse Distillation from One-Class Embedding,0.999872,"Knowledge distillation (KD) achieves promising results on the challenging
problem of unsupervised anomaly detection (AD).The representation discrepancy
of anomalies in the teacher-student (T-S) model provides essential evidence for
AD. However, using similar or identical architectures to build the teacher and
student models in previous studies hinders the diversity of anomalous
representations. To tackle this problem, we propose a novel T-S model
consisting of a teacher encoder and a student decoder and introduce a simple
yet effective ""reverse distillation"" paradigm accordingly. Instead of receiving
raw images directly, the student network takes teacher model's one-class
embedding as input and targets to restore the teacher's multiscale
representations. Inherently, knowledge distillation in this study starts from
abstract, high-level presentations to low-level features. In addition, we
introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S
model. The obtained compact embedding effectively preserves essential
information on normal patterns, but abandons anomaly perturbations. Extensive
experimentation on AD and one-class novelty detection benchmarks shows that our
method surpasses SOTA performance, demonstrating our proposed approach's
effectiveness and generalizability.",None,431
ed0ae364-418e-4422-9fe1-aafc6840b06e,Analysing the Greek Parliament Records with Emotion Classification,0.261627,"In this project, we tackle emotion classification for the Greek language,
presenting and releasing a new dataset in Greek. We fine-tune and assess
Transformer-based masked language models that were pre-trained on monolingual
and multilingual resources, and we present the results per emotion and by
aggregating at the sentiment and subjectivity level. The potential of the
presented resources is investigated by detecting and studying the emotion of
`disgust' in the Greek Parliament records. We: (a) locate the months with the
highest values from 1989 to present, (b) rank the Greek political parties based
on the presence of this emotion in their speeches, and (c) study the emotional
context shift of words used to stigmatise people.",None,-1
1ef86b61-c91f-4663-ab21-f5671d4f902f,Gym-saturation: an OpenAI Gym environment for saturation provers,0.438439,"`gym-saturation` is an OpenAI Gym environment for reinforcement learning (RL)
agents capable of proving theorems. Currently, only theorems written in a
formal language of the Thousands of Problems for Theorem Provers (TPTP) library
in clausal normal form (CNF) are supported. `gym-saturation` implements the
'given clause' algorithm (similar to the one used in Vampire and E Prover).
Being written in Python, `gym-saturation` was inspired by PyRes. In contrast to
the monolithic architecture of a typical Automated Theorem Prover (ATP),
`gym-saturation` gives different agents opportunities to select clauses
themselves and train from their experience. Combined with a particular agent,
`gym-saturation` can work as an ATP. Even with a non trained agent based on
heuristics, `gym-saturation` can find refutations for 688 (of 8257) CNF
problems from TPTP v7.5.0.",None,20
4ca56bd8-897e-4f65-894e-a6750052ead1,CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text,0.77687,"Climate change is threatening human health in unprecedented orders and many
ways. These threats are expected to grow unless effective and evidence-based
policies are developed and acted upon to minimize or eliminate them. Attaining
such a task requires the highest degree of the flow of knowledge from science
into policy. The multidisciplinary, location-specific, and vastness of
published science makes it challenging to keep track of novel work in this
area, as well as making the traditional knowledge synthesis methods inefficient
in infusing science into policy. To this end, we consider developing multiple
domain-specific language models (LMs) with different variations from Climate-
and Health-related information, which can serve as a foundational step toward
capturing available knowledge to enable solving different tasks, such as
detecting similarities between climate- and health-related concepts,
fact-checking, relation extraction, evidence of health effects to policy text
generation, and more. To our knowledge, this is the first work that proposes
developing multiple domain-specific language models for the considered domains.
We will make the developed models, resources, and codebase available for the
researchers.",None,-1
acfc9543-b0b9-4dbc-be85-cfc7908deefc,Exploring Global Diversity and Local Context for Video Summarization,0.090327,"Video summarization aims to automatically generate a diverse and concise
summary which is useful in large-scale video processing. Most of the methods
tend to adopt self-attention mechanism across video frames, which fails to
model the diversity of video frames. To alleviate this problem, we revisit the
pairwise similarity measurement in self-attention mechanism and find that the
existing inner-product affinity leads to discriminative features rather than
diversified features. In light of this phenomenon, we propose global diverse
attention which uses the squared Euclidean distance instead to compute the
affinities. Moreover, we model the local contextual information by novel local
contextual attention to remove the redundancy in the video. By combining these
two attention mechanisms, a video SUMmarization model with Diversified
Contextual Attention scheme is developed, namely SUM-DCA. Extensive experiments
are conducted on benchmark data sets to verify the effectiveness and the
superiority of SUM-DCA in terms of F-score and rank-based evaluation without
any bells and whistles.",None,-1
feb6af05-4c7b-4402-9d49-bd600664de25,Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation,0.758711,"Neural networks tend to gradually forget the previously learned knowledge
when learning multiple tasks sequentially from dynamic data distributions. This
problem is called \textit{catastrophic forgetting}, which is a fundamental
challenge in the continual learning of neural networks. In this work, we
observe that catastrophic forgetting not only occurs in continual learning but
also affects the traditional static training. Neural networks, especially
neural machine translation models, suffer from catastrophic forgetting even if
they learn from a static training set. To be specific, the final model pays
imbalanced attention to training samples, where recently exposed samples
attract more attention than earlier samples. The underlying cause is that
training samples do not get balanced training in each model update, so we name
this problem \textit{imbalanced training}. To alleviate this problem, we
propose Complementary Online Knowledge Distillation (COKD), which uses
dynamically updated teacher models trained on specific data orders to
iteratively provide complementary knowledge to the student model. Experimental
results on multiple machine translation tasks show that our method successfully
alleviates the problem of imbalanced training and achieves substantial
improvements over strong baseline systems.",https://github.com/ictnlp/COKD,-1
f4c89dd5-2755-44b0-994f-5c3cbaea43c7,"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",0.788986,"Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for ""general continual learning"". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings.",https://github.com/NeurAI-Lab/CLS-ER,-1
8acbe7fb-c8db-42f6-949f-6da1287d2cab,Multi-Modal Experience Inspired AI Creation,0.0796279,"AI creation, such as poem or lyrics generation, has attracted increasing
attention from both industry and academic communities, with many promising
models proposed in the past few years. Existing methods usually estimate the
outputs based on single and independent visual or textual information. However,
in reality, humans usually make creations according to their experiences, which
may involve different modalities and be sequentially correlated. To model such
human capabilities, in this paper, we define and solve a novel AI creation
problem based on human experiences. More specifically, we study how to generate
texts based on sequential multi-modal information. Compared with the previous
works, this task is much more difficult because the designed model has to well
understand and adapt the semantics among different modalities and effectively
convert them into the output in a sequential manner. To alleviate these
difficulties, we firstly design a multi-channel sequence-to-sequence
architecture equipped with a multi-modal attention network. For more effective
optimization, we then propose a curriculum negative sampling strategy tailored
for the sequential inputs. To benchmark this problem and demonstrate the
effectiveness of our model, we manually labeled a new multi-modal experience
dataset. With this dataset, we conduct extensive experiments by comparing our
model with a series of representative baselines, where we can demonstrate
significant improvements in our model based on both automatic and
human-centered metrics. The code and data are available at:
\url{https://github.com/Aman-4-Real/MMTG}.",https://github.com/Aman-4-Real/MMTG,-1
67cc52bc-81b9-49df-ae51-2d5394e227a9,RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning,0.0901042,"Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in English natural
language. While the progress is promising, it is currently unclear if these
models indeed perform logical reasoning by understanding the underlying logical
semantics in the language. To this end, we propose RobustLR, a suite of
evaluation datasets that evaluate the robustness of these models to minimal
logical edits in rulebases and some standard logical equivalence conditions. In
our experiments with RoBERTa and T5, we find that the models trained in prior
works do not perform consistently on the different perturbations in RobustLR,
thus showing that the models are not robust to the proposed logical
perturbations. Further, we find that the models find it especially hard to
learn logical negation and disjunction operators. Overall, using our evaluation
sets, we demonstrate some shortcomings of the deductive reasoning-based
language models, which can eventually help towards designing better models for
logical reasoning over natural language. All the datasets and code base have
been made publicly available.",https://github.com/INK-USC/RobustLR,-1
fdd786fe-b5be-41a7-8155-f90e3d01aa1f,Designing Decision Support Systems for Emergency Response: Challenges and Opportunities,0.205838,"Designing effective emergency response management (ERM) systems to respond to
incidents such as road accidents is a major problem faced by communities. In
addition to responding to frequent incidents each day (about 240 million
emergency medical services calls and over 5 million road accidents in the US
each year), these systems also support response during natural hazards.
Recently, there has been a consistent interest in building decision support and
optimization tools that can help emergency responders provide more efficient
and effective response. This includes a number of principled subsystems that
implement early incident detection, incident likelihood forecasting and
strategic resource allocation and dispatch policies. In this paper, we
highlight the key challenges and provide an overview of the approach developed
by our team in collaboration with our community partners.",None,-1
b3bb9e92-9917-4a14-b0c4-2b1e5efd5ece,An Energy-aware and Fault-tolerant Deep Reinforcement Learning based approach for Multi-agent Patrolling Problems,0.139771,"Autonomous vehicles are suited for continuous area patrolling problems.
However, finding an optimal patrolling strategy can be challenging for many
reasons. Firstly, patrolling environments are often complex and can include
unknown environmental factors, such as wind or landscape. Secondly, autonomous
vehicles can have failures or hardware constraints, such as limited battery
life. Importantly, patrolling large areas often requires multiple agents that
need to collectively coordinate their actions. In this work, we consider these
limitations and propose an approach based on model-free, deep multi-agent
reinforcement learning. In this approach, the agents are trained to patrol an
environment with various unknown dynamics and factors. They can automatically
recharge themselves to support continuous collective patrolling. A distributed
homogeneous multi-agent architecture is proposed, where all patrolling agents
execute identical policies locally based on their local observations and shared
location information. This architecture provides a patrolling system that can
tolerate agent failures and allow supplementary agents to be added to replace
failed agents or to increase the overall patrol performance. The solution is
validated through simulation experiments from multiple perspectives, including
the overall patrol performance, the efficiency of battery recharging
strategies, the overall fault tolerance, and the ability to cooperate with
supplementary agents.",None,-1
da003ede-a671-41b0-a64d-e647f2c8157c,NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models,0.783808,"Neural image caption generation (NICG) models have received massive attention
from the research community due to their excellent performance in visual
understanding. Existing work focuses on improving NICG model accuracy while
efficiency is less explored. However, many real-world applications require
real-time feedback, which highly relies on the efficiency of NICG models.
Recent research observed that the efficiency of NICG models could vary for
different inputs. This observation brings in a new attack surface of NICG
models, i.e., An adversary might be able to slightly change inputs to cause the
NICG models to consume more computational resources. To further understand such
efficiency-oriented threats, we propose a new attack approach, NICGSlowDown, to
evaluate the efficiency robustness of NICG models. Our experimental results
show that NICGSlowDown can generate images with human-unnoticeable
perturbations that will increase the NICG model latency up to 483.86%. We hope
this research could raise the community's concern about the efficiency
robustness of NICG models.",https://github.com/NICGSlowDown,-1
0d2436e5-ccec-4c64-b98f-f93ce589dbe5,A multi-domain virtual network embedding algorithm with delay prediction,0.0328,"Virtual network embedding (VNE) is an crucial part of network virtualization
(NV), which aims to map the virtual networks (VNs) to a shared substrate
network (SN). With the emergence of various delay-sensitive applications, how
to improve the delay performance of the system has become a hot topic in
academic circles. Based on extensive research, we proposed a multi-domain
virtual network embedding algorithm based on delay prediction (DP-VNE).
Firstly, the candidate physical nodes are selected by estimating the delay of
virtual requests, then particle swarm optimization (PSO) algorithm is used to
optimize the mapping process, so as to reduce the delay of the system. The
simulation results show that compared with the other three advanced algorithms,
the proposed algorithm can significantly reduce the system delay while keeping
other indicators unaffected.",None,9769
24749b87-12b5-4693-ade3-4380b3cb55f6,Diffeomorphic Counterfactuals with Generative Models,0.510276,"Counterfactuals can explain classification decisions of neural networks in a
human interpretable way. We propose a simple but effective method to generate
such counterfactuals. More specifically, we perform a suitable diffeomorphic
coordinate transformation and then perform gradient ascent in these coordinates
to find counterfactuals which are classified with great confidence as a
specified target class. We propose two methods to leverage generative models to
construct such suitable coordinate systems that are either exactly or
approximately diffeomorphic. We analyze the generation process theoretically
using Riemannian differential geometry and validate the quality of the
generated counterfactuals using various qualitative and quantitative measures.",https://github.com/annahdo/counterfactuals,-1
1eff7b31-2e1a-4c7d-9e93-7d6d1f71ccb6,OCR Improves Machine Translation for Low-Resource Languages,0.789373,"We aim to investigate the performance of current OCR systems on low resource
languages and low resource scripts. We introduce and make publicly available a
novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with
noise, for 60 low-resource languages in low resource scripts. We evaluate
state-of-the-art OCR systems on our benchmark and analyse most common errors.
We show that OCR monolingual data is a valuable resource that can increase
performance of Machine Translation models, when used in backtranslation. We
then perform an ablation study to investigate how OCR errors impact Machine
Translation performance and determine what is the minimum level of OCR quality
needed for the monolingual data to be useful for Machine Translation.",https://github.com/facebookresearch/flores,-1
cd82e0ff-23fb-4936-9618-0d3abb2907aa,Visual-tactile Fusion for Transparent Object Grasping in Complex Backgrounds,0.438341,"The accurate detection and grasping of transparent objects are challenging
but of significance to robots. Here, a visual-tactile fusion framework for
transparent object grasping under complex backgrounds and variant light
conditions is proposed, including the grasping position detection, tactile
calibration, and visual-tactile fusion based classification. First, a
multi-scene synthetic grasping dataset generation method with a Gaussian
distribution based data annotation is proposed. Besides, a novel grasping
network named TGCNN is proposed for grasping position detection, showing good
results in both synthetic and real scenes. In tactile calibration, inspired by
human grasping, a fully convolutional network based tactile feature extraction
method and a central location based adaptive grasping strategy are designed,
improving the success rate by 36.7% compared to direct grasping. Furthermore, a
visual-tactile fusion method is proposed for transparent objects
classification, which improves the classification accuracy by 34%. The proposed
framework synergizes the advantages of vision and touch, and greatly improves
the grasping efficiency of transparent objects.",None,-1
9493c86c-b9ce-44f8-b693-913da87cec32,Class-Aware Universum Inspired Re-Balance Learning for Long-Tailed Recognition,0.051928,"Data augmentation for minority classes is an effective strategy for
long-tailed recognition, thus developing a large number of methods. Although
these methods all ensure the balance in sample quantity, the quality of the
augmented samples is not always satisfactory for recognition, being prone to
such problems as over-fitting, lack of diversity, semantic drift, etc. For
these issues, we propose the Class-aware Universum Inspired Re-balance
Learning(CaUIRL) for long-tailed recognition, which endows the Universum with
class-aware ability to re-balance individual minority classes from both sample
quantity and quality. In particular, we theoretically prove that the
classifiers learned by CaUIRL are consistent with those learned under the
balanced condition from a Bayesian perspective. In addition, we further develop
a higher-order mixup approach, which can automatically generate class-aware
Universum(CaU) data without resorting to any external data. Unlike the
traditional Universum, such generated Universum additionally takes the domain
similarity, class separability, and sample diversity into account. Extensive
experiments on benchmark datasets demonstrate the surprising advantages of our
method, especially the top1 accuracy in minority classes is improved by 1.9% 6%
compared to the state-of-the-art method.",None,-1
26ddfd99-4fda-4037-9aa2-715c9ddf5549,Knowledge Transfer from Answer Ranking to Answer Generation,0.243627,"Recent studies show that Question Answering (QA) based on Answer Sentence
Selection (AS2) can be improved by generating an improved answer from the top-k
ranked answer sentences (termed GenQA). This allows for synthesizing the
information from multiple candidates into a concise, natural-sounding answer.
However, creating large-scale supervised training data for GenQA models is very
challenging. In this paper, we propose to train a GenQA model by transferring
knowledge from a trained AS2 model, to overcome the aforementioned issue.
First, we use an AS2 model to produce a ranking over answer candidates for a
set of questions. Then, we use the top ranked candidate as the generation
target, and the next k top ranked candidates as context for training a GenQA
model. We also propose to use the AS2 model prediction scores for loss
weighting and score-conditioned input/output shaping, to aid the knowledge
transfer. Our evaluation on three public and one large industrial datasets
demonstrates the superiority of our approach over the AS2 baseline, and GenQA
trained using supervised data.",https://github.com/amazon-research/wqa-genqa-knowledge-transfer,14771
9a8fb4b3-4606-4c93-9f93-4efd0e9b5da4,BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment,0.731738,"This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge.",https://github.com/Algolzw/BSRT,-1
8639273d-7012-4b91-a742-4e046a7b46eb,Federated Graph-based Networks with Shared Embedding,0.0647393,"Nowadays, user privacy is becoming an issue that cannot be bypassed for
system developers, especially for that of web applications where data can be
easily transferred through internet. Thankfully, federated learning proposes an
innovative method to train models with distributed devices while data are kept
in local storage. However, unlike general neural networks, although graph-based
networks have achieved great success in classification tasks and advanced
recommendation system, its high performance relies on the rich context provided
by a graph structure, which is vulnerable when data attributes are incomplete.
Therefore, the latter becomes a realistic problem when implementing federated
learning for graph-based networks. Knowing that data embedding is a
representation in a different space, we propose our Federated Graph-based
Networks with Shared Embedding (Feras), which uses shared embedding data to
train the network and avoids the direct sharing of original data. A solid
theoretical proof of the convergence of Feras is given in this work.
Experiments on different datasets (PPI, Flickr, Reddit) are conducted to show
the efficiency of Feras for centralized learning. Finally, Feras enables the
training of current graph-based models in the federated learning framework for
privacy concern.",https://github.com/GraphSAINT/GraphSAINT,-1
6368371a-20ca-46ce-807b-b677291cbfb9,Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning,0.193824,"While the empirical success of self-supervised learning (SSL) heavily relies
on the usage of deep nonlinear models, existing theoretical works on SSL
understanding still focus on linear ones. In this paper, we study the role of
nonlinearity in the training dynamics of contrastive learning (CL) on one and
two-layer nonlinear networks with homogeneous activation $h(x) = h'(x)x$. We
have two major theoretical discoveries. First, the presence of nonlinearity can
lead to many local optima even in 1-layer setting, each corresponding to
certain patterns from the data distribution, while with linear activation, only
one major pattern can be learned. This suggests that models with lots of
parameters can be regarded as a \emph{brute-force} way to find these local
optima induced by nonlinearity. Second, in the 2-layer case, linear activation
is proven not capable of learning specialized weights into diverse patterns,
demonstrating the importance of nonlinearity. In addition, for 2-layer setting,
we also discover \emph{global modulation}: those local patterns discriminative
from the perspective of global-level patterns are prioritized to learn, further
characterizing the learning process. Simulation verifies our theoretical
findings.",None,-1
06a9e8da-8822-407f-890e-35d192946e86,Same Author or Just Same Topic? Towards Content-Independent Style Representations,0.323622,"Linguistic style is an integral component of language. Recent advances in the
development of style representations have increasingly used training objectives
from authorship verification (AV): Do two texts have the same author? The
assumption underlying the AV training task (same author approximates same
writing style) enables self-supervised and, thus, extensive training. However,
a good performance on the AV task does not ensure good ""general-purpose"" style
representations. For example, as the same author might typically write about
certain topics, representations trained on AV might also encode content
information instead of style alone. We introduce a variation of the AV training
task that controls for content using conversation or domain labels. We evaluate
whether known style dimensions are represented and preferred over content
information through an original variation to the recently proposed STEL
framework. We find that representations trained by controlling for conversation
are better than representations trained with domain or no content control at
representing style independent from content.",https://github.com/nlpsoc/Style-Embeddings,-1
a97a37c1-74fb-493f-a5ca-3d6af1caaaca,PETR: Position Embedding Transformation for Multi-View 3D Object Detection,1.0,"In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}.",https://github.com/megvii-research/PETR,-1
17b3d591-9b1c-4b9b-905c-2faba47f485b,Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis,0.326677,"Existing studies on multimodal sentiment analysis heavily rely on textual
modality and unavoidably induce the spurious correlations between textual words
and sentiment labels. This greatly hinders the model generalization ability. To
address this problem, we define the task of out-of-distribution (OOD)
multimodal sentiment analysis. This task aims to estimate and mitigate the bad
effect of textual modality for strong OOD generalization. To this end, we
embrace causal inference, which inspects the causal relationships via a causal
graph. From the graph, we find that the spurious correlations are attributed to
the direct effect of textual modality on the model prediction while the
indirect one is more reliable by considering multimodal semantics. Inspired by
this, we devise a model-agnostic counterfactual framework for multimodal
sentiment analysis, which captures the direct effect of textual modality via an
extra text model and estimates the indirect one by a multimodal model. During
the inference, we first estimate the direct effect by the counterfactual
inference, and then subtract it from the total effect of all modalities to
obtain the indirect effect for reliable prediction. Extensive experiments show
the superior effectiveness and generalization ability of our proposed
framework.",https://github.com/Teng-Sun/CLUE_model,-1
1ab33d5c-ce9c-4767-a48d-81511c21a4b4,Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation,0.67135,"After the great success of Vision Transformer variants (ViTs) in computer
vision, it has also demonstrated great potential in domain adaptive semantic
segmentation. Unfortunately, straightforwardly applying local ViTs in domain
adaptive semantic segmentation does not bring in expected improvement. We find
that the pitfall of local ViTs is due to the severe high-frequency components
generated during both the pseudo-label construction and features alignment for
target domains. These high-frequency components make the training of local ViTs
very unsmooth and hurt their transferability. In this paper, we introduce a
low-pass filtering mechanism, momentum network, to smooth the learning dynamics
of target domain features and pseudo labels. Furthermore, we propose a dynamic
of discrepancy measurement to align the distributions in the source and target
domains via dynamic weights to evaluate the importance of the samples. After
tackling the above issues, extensive experiments on sim2real benchmarks show
that the proposed method outperforms the state-of-the-art methods. Our codes
are available at https://github.com/alpc91/TransDA",https://github.com/alpc91/TransDA,24953
be6280df-b5da-4c4c-b143-3e7c9ba69f32,P-Transformer: Towards Better Document-to-Document Neural Machine Translation,0.354798,"Directly training a document-to-document (Doc2Doc) neural machine translation
(NMT) via Transformer from scratch, especially on small datasets usually fails
to converge. Our dedicated probing tasks show that 1) both the absolute
position and relative position information gets gradually weakened or even
vanished once it reaches the upper encoder layers, and 2) the vanishing of
absolute position information in encoder output causes the training failure of
Doc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer
(P-Transformer) to enhance both the absolute and relative position information
in both self-attention and cross-attention. Specifically, we integrate absolute
positional information, i.e., position embeddings, into the query-key pairs
both in self-attention and cross-attention through a simple yet effective
addition operation. Moreover, we also integrate relative position encoding in
self-attention. The proposed P-Transformer utilizes sinusoidal position
encoding and does not require any task-specified position embedding, segment
embedding, or attention mechanism. Through the above methods, we build a
Doc2Doc NMT model with P-Transformer, which ingests the source document and
completely generates the target document in a sequence-to-sequence (seq2seq)
way. In addition, P-Transformer can be applied to seq2seq-based
document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)
translation. Extensive experimental results of Doc2Doc NMT show that
P-Transformer significantly outperforms strong baselines on widely-used 9
document-level datasets in 7 language pairs, covering small-, middle-, and
large-scales, and achieves a new state-of-the-art. Experimentation on discourse
phenomena shows that our Doc2Doc NMT models improve the translation quality in
both BLEU and discourse coherence. We make our code available on Github.",None,-1
b414ab38-87f8-4d06-bc75-47edf42c7509,MMKGR: Multi-hop Multi-modal Knowledge Graph Reasoning,0.551736,"Multi-modal knowledge graphs (MKGs) include not only the relation triplets,
but also related multi-modal auxiliary data (i.e., texts and images), which
enhance the diversity of knowledge. However, the natural incompleteness has
significantly hindered the applications of MKGs. To tackle the problem,
existing studies employ the embedding-based reasoning models to infer the
missing knowledge after fusing the multi-modal features. However, the reasoning
performance of these methods is limited due to the following problems: (1)
ineffective fusion of multi-modal auxiliary features; (2) lack of complex
reasoning ability as well as inability to conduct the multi-hop reasoning which
is able to infer more missing knowledge. To overcome these problems, we propose
a novel model entitled MMKGR (Multi-hop Multi-modal Knowledge Graph Reasoning).
Specifically, the model contains the following two components: (1) a unified
gate-attention network which is designed to generate effective multi-modal
complementary features through sufficient attention interaction and noise
reduction; (2) a complementary feature-aware reinforcement learning method
which is proposed to predict missing elements by performing the multi-hop
reasoning process, based on the features obtained in component (1). The
experimental results demonstrate that MMKGR outperforms the state-of-the-art
approaches in the MKG reasoning task.",None,-1
891cd8b2-a9a2-4950-ae8f-400ac2ab8ba3,AutoSNN: Towards Energy-Efficient Spiking Neural Networks,0.938501,"Spiking neural networks (SNNs) that mimic information transmission in the
brain can energy-efficiently process spatio-temporal information through
discrete and sparse spikes, thereby receiving considerable attention. To
improve accuracy and energy efficiency of SNNs, most previous studies have
focused solely on training methods, and the effect of architecture has rarely
been studied. We investigate the design choices used in the previous studies in
terms of the accuracy and number of spikes and figure out that they are not
best-suited for SNNs. To further improve the accuracy and reduce the spikes
generated by SNNs, we propose a spike-aware neural architecture search
framework called AutoSNN. We define a search space consisting of architectures
without undesirable design choices. To enable the spike-aware architecture
search, we introduce a fitness that considers both the accuracy and number of
spikes. AutoSNN successfully searches for SNN architectures that outperform
hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate
the effectiveness of AutoSNN on various datasets including neuromorphic
datasets.",https://github.com/nabk89/AutoSNN,-1
1bf4f8d0-95b1-4e0b-a4d5-ac46217148fe,CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability,0.807851,"Convolutional neural networks (CNNs) have dominated the field of computer
vision for nearly a decade due to their strong ability to learn local features.
However, due to their limited receptive field, CNNs fail to model the global
context. On the other hand, transformer, an attention-based architecture can
model the global context easily. Despite this, there are limited studies that
investigate the effectiveness of transformers in crowd counting. In addition,
the majority of the existing crowd counting methods are based on the regression
of density maps which requires point-level annotation of each person present in
the scene. This annotation task is laborious and also error-prone. This has led
to increased focus on weakly-supervised crowd counting methods which require
only the count-level annotations. In this paper, we propose a weakly-supervised
method for crowd counting using a pyramid vision transformer. We have conducted
extensive evaluations to validate the effectiveness of the proposed method. Our
method is comparable to the state-of-the-art on the benchmark crowd datasets.
More importantly, it shows remarkable generalizability.",None,2627
5809328a-bd5c-4720-9cf3-a546ca7055eb,Circular Pythagorean fuzzy sets and applications to multi-criteria decision making,0.644202,"In this paper, we introduce the concept of circular Pythagorean fuzzy set
(value) (C-PFS(V)) as a new generalization of both circular intuitionistic
fuzzy sets (C-IFSs) proposed by Atannassov and Pythagorean fuzzy sets (PFSs)
proposed by Yager. A circular Pythagorean fuzzy set is represented by a circle
that represents the membership degree and the non-membership degree and whose
center consists of non-negative real numbers $\mu$ and $\nu$ with the condition
$\mu^2+\nu^2\leq 1$. A C-PFS models the fuzziness of the uncertain information
more properly thanks to its structure that allows modelling the information
with points of a circle of a certain center and a radius. Therefore, a C-PFS
lets decision makers to evaluate objects in a larger and more flexible region
and thus more sensitive decisions can be made. After defining the concept of
C-PFS we define some fundamental set operations between C-PFSs and propose some
algebraic operations between C-PFVs via general $t$-norms and $t$-conorms. By
utilizing these algebraic operations, we introduce some weighted aggregation
operators to transform input values represented by C-PFVs to a single output
value. Then to determine the degree of similarity between C-PFVs we define a
cosine similarity measure based on radius. Furthermore, we develop a method to
transform a collection of Pythagorean fuzzy values to a PFS. Finally, a method
is given to solve multi-criteria decision making problems in circular
Pythagorean fuzzy environment and the proposed method is practiced to a problem
about selecting the best photovoltaic cell from the literature. We also study
the comparison analysis and time complexity of the proposed method.",None,-1
840c9c1d-2adc-4b60-a6cd-64b4929ba172,Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation,0.363848,"We present a semi-supervised learning approach to the temporal action
segmentation task. The goal of the task is to temporally detect and segment
actions in long, untrimmed procedural videos, where only a small set of videos
are densely labelled, and a large collection of videos are unlabelled. To this
end, we propose two novel loss functions for the unlabelled data: an action
affinity loss and an action continuity loss. The action affinity loss guides
the unlabelled samples learning by imposing the action priors induced from the
labelled set. Action continuity loss enforces the temporal continuity of
actions, which also provides frame-wise classification supervision. In
addition, we propose an Adaptive Boundary Smoothing (ABS) approach to build
coarser action boundaries for more robust and reliable learning. The proposed
loss functions and ABS were evaluated on three benchmarks. Results show that
they significantly improved action segmentation performance with a low amount
(5% and 10%) of labelled data and achieved comparable results to full
supervision with 50% labelled data. Furthermore, ABS succeeded in boosting
performance when integrated into fully-supervised learning.",None,-1
6884b2b9-05c2-4f61-a981-ff2062a89536,PartAL: Efficient Partial Active Learning in Multi-Task Visual Settings,0.112365,"Multi-task learning is central to many real-world applications.
Unfortunately, obtaining labelled data for all tasks is time-consuming,
challenging, and expensive. Active Learning (AL) can be used to reduce this
burden. Existing techniques typically involve picking images to be annotated
and providing annotations for all tasks.
  In this paper, we show that it is more effective to select not only the
images to be annotated but also a subset of tasks for which to provide
annotations at each AL iteration. Furthermore, the annotations that are
provided can be used to guess pseudo-labels for the tasks that remain
unannotated. We demonstrate the effectiveness of our approach on several
popular multi-task datasets.",None,-1
02adf522-7f92-474c-9a82-ba46a2acf5b3,Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data,0.995833,"Retrieval-based methods have been shown to be effective in NLP tasks via
introducing external knowledge. However, the indexing and retrieving of
large-scale corpora bring considerable computational cost. Surprisingly, we
found that REtrieving from the traINing datA (REINA) only can lead to
significant gains on multiple NLG and NLU tasks. We retrieve the labeled
training instances most similar to the input text and then concatenate them
with the input to feed into the model to generate the output. Experimental
results show that this simple method can achieve significantly better
performance on a variety of NLU and NLG tasks, including summarization, machine
translation, language modeling, and question answering tasks. For instance, our
proposed method achieved state-of-the-art results on XSum, BigPatent, and
CommonsenseQA. Our code is released, https://github.com/microsoft/REINA .",https://github.com/microsoft/REINA,-1
18570b69-d97a-408c-91c7-c3bb4732bec9,QCRI's COVID-19 Disinformation Detector: A System to Fight the COVID-19 Infodemic in Social Media,0.258738,"Fighting the ongoing COVID-19 infodemic has been declared as one of the most
important focus areas by the World Health Organization since the onset of the
COVID-19 pandemic. While the information that is consumed and disseminated
consists of promoting fake cures, rumors, and conspiracy theories to spreading
xenophobia and panic, at the same time there is information (e.g., containing
advice, promoting cure) that can help different stakeholders such as
policy-makers. Social media platforms enable the infodemic and there has been
an effort to curate the content on such platforms, analyze and debunk them.
While a majority of the research efforts consider one or two aspects (e.g.,
detecting factuality) of such information, in this study we focus on a
multifaceted approach, including an
API,\url{https://app.swaggerhub.com/apis/yifan2019/Tanbih/0.8.0/} and a demo
system,\url{https://covid19.tanbih.org}, which we made freely and publicly
available. We believe that this will facilitate researchers and different
stakeholders. A screencast of the API services and demo is
available.\url{https://youtu.be/zhbcSvxEKMk}",https://github.com/GateNLP/CANTM,-1
1edb5b28-336f-4b30-afd4-d8be3f72bb7e,Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas,0.458816,"Vision and language navigation (VLN) is a challenging visually-grounded
language understanding task. Given a natural language navigation instruction, a
visual agent interacts with a graph-based environment equipped with panorama
images and tries to follow the described route. Most prior work has been
conducted in indoor scenarios where best results were obtained for navigation
on routes that are similar to the training routes, with sharp drops in
performance when testing on unseen environments. We focus on VLN in outdoor
scenarios and find that in contrast to indoor VLN, most of the gain in outdoor
VLN on unseen data is due to features like junction type embedding or heading
delta that are specific to the respective environment graph, while image
information plays a very minor role in generalizing VLN to unseen outdoor
areas. These findings show a bias to specifics of graph representations of
urban environments, demanding that VLN tasks grow in scale and diversity of
geographical environments.",https://github.com/raphael-sch/,-1
dfd8496d-3783-4d0b-88a7-bd8e04264581,Towards Developing Safety Assurance Cases for Learning-Enabled Medical Cyber-Physical Systems,0.139441,"Machine Learning (ML) technologies have been increasingly adopted in Medical
Cyber-Physical Systems (MCPS) to enable smart healthcare. Assuring the safety
and effectiveness of learning-enabled MCPS is challenging, as such systems must
account for diverse patient profiles and physiological dynamics and handle
operational uncertainties. In this paper, we develop a safety assurance case
for ML controllers in learning-enabled MCPS, with an emphasis on establishing
confidence in the ML-based predictions. We present the safety assurance case in
detail for Artificial Pancreas Systems (APS) as a representative application of
learning-enabled MCPS, and provide a detailed analysis by implementing a deep
neural network for the prediction in APS. We check the sufficiency of the ML
data and analyze the correctness of the ML-based prediction using formal
verification. Finally, we outline open research problems based on our
experience in this paper.",https://github.com/jxx123/simglucose,-1
7fb6e46e-5c39-44ff-943f-5d6bb6a1cc37,FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation,0.303219,"Recent model-based reference-free metrics for open-domain dialogue evaluation
exhibit promising correlations with human judgment. However, they either
perform turn-level evaluation or look at a single dialogue quality dimension.
One would expect a good evaluation metric to assess multiple quality dimensions
at the dialogue level. To this end, we are motivated to propose a
multi-dimensional dialogue-level metric, which consists of three sub-metrics
with each targeting a specific dimension. The sub-metrics are trained with
novel self-supervised objectives and exhibit strong correlations with human
judgment for their respective dimensions. Moreover, we explore two approaches
to combine the sub-metrics: metric ensemble and multitask learning. Both
approaches yield a holistic metric that significantly outperforms individual
sub-metrics. Compared to the existing state-of-the-art metric, the combined
metrics achieve around 16% relative improvement on average across three
high-quality dialogue-level evaluation benchmarks.",https://github.com/e0397123/FineD-Eval,-1
82206b4a-a332-4b86-8520-4bff11a6a242,Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image,0.746049,"Recently, RGBD-based category-level 6D object pose estimation has achieved
promising improvement in performance, however, the requirement of depth
information prohibits broader applications. In order to relieve this problem,
this paper proposes a novel approach named Object Level Depth reconstruction
Network (OLD-Net) taking only RGB images as input for category-level 6D object
pose estimation. We propose to directly predict object-level depth from a
monocular RGB image by deforming the category-level shape prior into
object-level depth and the canonical NOCS representation. Two novel modules
named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth
Reconstruction (SDDR) module are introduced to learn high fidelity object-level
depth and delicate shape representations. At last, the 6D object pose is solved
by aligning the predicted canonical representation with the back-projected
object-level depth. Extensive experiments on the challenging CAMERA25 and
REAL275 datasets indicate that our model, though simple, achieves
state-of-the-art performance.",None,-1
782c55ee-ea82-49ec-a9f1-2b9d445014ed,Custom Structure Preservation in Face Aging,0.932335,"In this work, we propose a novel architecture for face age editing that can
produce structural modifications while maintaining relevant details present in
the original image. We disentangle the style and content of the input image and
propose a new decoder network that adopts a style-based strategy to combine the
style and content representations of the input image while conditioning the
output on the target age. We go beyond existing aging methods allowing users to
adjust the degree of structure preservation in the input image during
inference. To this purpose, we introduce a masking mechanism, the CUstom
Structure Preservation module, that distinguishes relevant regions in the input
image from those that should be discarded. CUSP requires no additional
supervision. Finally, our quantitative and qualitative analysis which include a
user study, show that our method outperforms prior art and demonstrates the
effectiveness of our strategy regarding image editing and adjustable structure
preservation. Code and pretrained models are available at
https://github.com/guillermogotre/CUSP.",https://github.com/guillermogotre/CUSP,-1
484007fa-443a-44b0-a23c-c1f56ad35172,Leveraging Log Instructions in Log-based Anomaly Detection,0.0798259,"Artificial Intelligence for IT Operations (AIOps) describes the process of
maintaining and operating large IT systems using diverse AI-enabled methods and
tools for, e.g., anomaly detection and root cause analysis, to support the
remediation, optimization, and automatic initiation of self-stabilizing IT
activities. The core step of any AIOps workflow is anomaly detection, typically
performed on high-volume heterogeneous data such as log messages (logs),
metrics (e.g., CPU utilization), and distributed traces. In this paper, we
propose a method for reliable and practical anomaly detection from system logs.
It overcomes the common disadvantage of related works, i.e., the need for a
large amount of manually labeled training data, by building an anomaly
detection model with log instructions from the source code of 1000+ GitHub
projects. The instructions from diverse systems contain rich and heterogenous
information about many different normal and abnormal IT events and serve as a
foundation for anomaly detection. The proposed method, named ADLILog, combines
the log instructions and the data from the system of interest (target system)
to learn a deep neural network model through a two-phase learning procedure.
The experimental results show that ADLILog outperforms the related approaches
by up to 60% on the F1 score while satisfying core non-functional requirements
for industrial deployments such as unsupervised design, efficient model
updates, and small model sizes.",https://github.com/ADLILog/ADLILog,-1
0f46b066-52a9-45ae-8b3a-e2add85ff397,A Dataset for Medical Instructional Video Classification and Question Answering,0.51062,"This paper introduces a new challenge and datasets to foster research toward
designing systems that can understand medical videos and provide visual answers
to natural language questions. We believe medical videos may provide the best
possible answers to many first aids, medical emergency, and medical education
questions. Toward this, we created the MedVidCL and MedVidQA datasets and
introduce the tasks of Medical Video Classification (MVC) and Medical Visual
Answer Localization (MVAL), two tasks that focus on cross-modal (medical
language and medical video) understanding. The proposed tasks and datasets have
the potential to support the development of sophisticated downstream
applications that can benefit the public and medical practitioners. Our
datasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated
questions and answers timestamps from 899 videos for the MVAL task. These
datasets have been verified and corrected by medical informatics experts. We
have also benchmarked each task with the created MedVidCL and MedVidQA datasets
and proposed the multimodal learning methods that set competitive baselines for
future research.",https://github.com/deepaknlp/MedVidQACL,-1
6486e288-1fcd-4f25-aa39-8d0a223426b8,"Movement Analytics: Current Status, Application to Manufacturing, and Future Prospects from an AI Perspective",0.21082,"Data-driven decision making is becoming an integral part of manufacturing
companies. Data is collected and commonly used to improve efficiency and
produce high quality items for the customers. IoT-based and other forms of
object tracking are an emerging tool for collecting movement data of
objects/entities (e.g. human workers, moving vehicles, trolleys etc.) over
space and time. Movement data can provide valuable insights like process
bottlenecks, resource utilization, effective working time etc. that can be used
for decision making and improving efficiency.
  Turning movement data into valuable information for industrial management and
decision making requires analysis methods. We refer to this process as movement
analytics. The purpose of this document is to review the current state of work
for movement analytics both in manufacturing and more broadly.
  We survey relevant work from both a theoretical perspective and an
application perspective. From the theoretical perspective, we put an emphasis
on useful methods from two research areas: machine learning, and logic-based
knowledge representation. We also review their combinations in view of movement
analytics, and we discuss promising areas for future development and
application. Furthermore, we touch on constraint optimization.
  From an application perspective, we review applications of these methods to
movement analytics in a general sense and across various industries. We also
describe currently available commercial off-the-shelf products for tracking in
manufacturing, and we overview main concepts of digital twins and their
applications.",None,-1
11a21cd3-39f1-41e9-b8e0-bcdb3214a534,An Approach for Improving Automatic Mouth Emotion Recognition,0.409753,"The study proposes and tests a technique for automated emotion recognition
through mouth detection via Convolutional Neural Networks (CNN), meant to be
applied for supporting people with health disorders with communication skills
issues (e.g. muscle wasting, stroke, autism, or, more simply, pain) in order to
recognize emotions and generate real-time feedback, or data feeding supporting
systems. The software system starts the computation identifying if a face is
present on the acquired image, then it looks for the mouth location and
extracts the corresponding features. Both tasks are carried out using Haar
Feature-based Classifiers, which guarantee fast execution and promising
performance. If our previous works focused on visual micro-expressions for
personalized training on a single user, this strategy aims to train the system
also on generalized faces data sets.",https://github.com/weiliu89/caffe/tree/ssd/models/bvlc_alexnet,-1
f0db1084-6000-4b2e-8d35-602b103da021,Large Language Models Struggle to Learn Long-Tail Knowledge,0.999999,"The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.",https://github.com/kingoflolz/mesh-transformer-jax,-1
66f76036-5f69-4cad-bf84-124de69b2982,Finding and Listing Front-door Adjustment Sets,0.748572,"Identifying the effects of new interventions from data is a significant
challenge found across a wide range of the empirical sciences. A well-known
strategy for identifying such effects is Pearl's front-door (FD) criterion
(Pearl, 1995). The definition of the FD criterion is declarative, only allowing
one to decide whether a specific set satisfies the criterion. In this paper, we
present algorithms for finding and enumerating possible sets satisfying the FD
criterion in a given causal diagram. These results are useful in facilitating
the practical applications of the FD criterion for causal effects estimation
and helping scientists to select estimands with desired properties, e.g., based
on cost, feasibility of measurement, or statistical power.",https://github.com/CausalAILab/FrontdoorAdjustmentSets,-1
4833999f-aefb-407c-a255-9187cc53c814,A Deep Dive into Deep Cluster,0.0340961,"Deep Learning has demonstrated a significant improvement against traditional
machine learning approaches in different domains such as image and speech
recognition. Their success on benchmark datasets is transferred to the
real-world through pretrained models by practitioners. Pretraining visual
models using supervised learning requires a significant amount of expensive
data annotation. To tackle this limitation, DeepCluster - a simple and scalable
unsupervised pretraining of visual representations - has been proposed.
However, the underlying work of the model is not yet well understood. In this
paper, we analyze DeepCluster internals and exhaustively evaluate the impact of
various hyperparameters over a wide range of values on three different
datasets. Accordingly, we propose an explanation of why the algorithm works in
practice. We also show that DeepCluster convergence and performance highly
depend on the interplay between the quality of the randomly initialized filters
of the convolutional layer and the selected number of clusters. Furthermore, we
demonstrate that continuous clustering is not critical for DeepCluster
convergence. Therefore, early stopping of the clustering phase will reduce the
training time and allow the algorithm to scale to large datasets. Finally, we
derive plausible hyperparameter selection criteria in a semi-supervised
setting.",None,-1
052c624d-91dd-4c86-a496-4f3f288991e0,SS-MFAR : Semi-supervised Multi-task Facial Affect Recognition,0.18977,"Automatic affect recognition has applications in many areas such as
education, gaming, software development, automotives, medical care, etc. but it
is non trivial task to achieve appreciable performance on in-the-wild data
sets. In-the-wild data sets though represent real-world scenarios better than
synthetic data sets, the former ones suffer from the problem of incomplete
labels. Inspired by semi-supervised learning, in this paper, we introduce our
submission to the Multi-Task-Learning Challenge at the 4th Affective Behavior
Analysis in-the-wild (ABAW) 2022 Competition. The three tasks that are
considered in this challenge are valence-arousal(VA) estimation, classification
of expressions into 6 basic (anger, disgust, fear, happiness, sadness,
surprise), neutral, and the 'other' category and 12 action units(AU) numbered
AU-{1,2,4,6,7,10,12,15,23,24,25,26}. Our method Semi-supervised Multi-task
Facial Affect Recognition titled SS-MFAR uses a deep residual network with task
specific classifiers for each of the tasks along with adaptive thresholds for
each expression class and semi-supervised learning for the incomplete labels.
Source code is available at https://github.com/1980x/ABAW2022DMACS.",https://github.com/1980x/ABAW2022DMACS,-1
15924d16-f0a0-43fc-a33a-5c68127238ba,Boosting 3D Object Detection via Object-Focused Image Fusion,0.642357,"3D object detection has achieved remarkable progress by taking point clouds
as the only input. However, point clouds often suffer from incomplete geometric
structures and the lack of semantic information, which makes detectors hard to
accurately classify detected objects. In this work, we focus on how to
effectively utilize object-level information from images to boost the
performance of point-based 3D detector. We present DeMF, a simple yet effective
method to fuse image information into point features. Given a set of point
features and image feature maps, DeMF adaptively aggregates image features by
taking the projected 2D location of the 3D point as reference. We evaluate our
method on the challenging SUN RGB-D dataset, improving state-of-the-art results
by a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at
https://github.com/haoy945/DeMF.",https://github.com/haoy945/DeMF,-1
5c5de268-071e-4c7b-9168-cd0158ffd7d4,Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework,0.532069,"Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and thereby
achieves better SiMT performance.",https://github.com/pytorch/fairseq/tree/master/examples/simultaneous_translation,-1
704c7e5d-e37d-4eeb-a1f5-c5adbe253e83,Self-consistent Reasoning For Solving Math Word Problems,0.451123,"Math word problems (MWPs) is a task that automatically derives solution
expression from a giving math problems in text. The previous studies suffer
from spurious correlations between input text and output expression. To
mitigate this issue, we propose a self-consistent reasoning framework called
SCR, which attempts to adopt a pruning strategy to correct the output
distribution shift so as to implicitly fix those spurious correlative samples.
Specifically, we firstly obtain a sub-network by pruning a roberta2tree model,
for the sake to use the gap on output distribution between the original
roberta2tree model and the pruned sub-network to expose spurious correlative
samples. Then, we calibrate the output distribution shift by applying symmetric
Kullback-Leibler divergence to alleviate spurious correlations. In addition,
SCR generates equivalent expressions, thereby, capturing the original text's
logic rather than relying on hints from original text. Extensive experiments on
two large-scale benchmarks demonstrate that our model substantially outperforms
the strong baseline methods.",None,-1
689f9cca-effa-49a2-9a9e-b0f7a70b4dbe,Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models,0.754409,"Recently the prompt-tuning paradigm has attracted significant attention. By
only tuning continuous prompts with a frozen pre-trained language model (PLM),
prompt-tuning takes a step towards deploying a shared frozen PLM to serve
numerous downstream tasks. Although prompt-tuning shows good performance on
certain natural language understanding (NLU) tasks, its effectiveness on
natural language generation (NLG) tasks is still under-explored. In this paper,
we argue that one of the factors hindering the development of prompt-tuning on
NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different
from the pretraining corpus). For example, our preliminary exploration reveals
a large performance gap between prompt-tuning and fine-tuning when unfamiliar
inputs occur frequently in NLG tasks. This motivates us to propose
input-tuning, which fine-tunes both the continuous prompts and the input
representations, leading to a more effective way to adapt unfamiliar inputs to
frozen PLMs. Our proposed input-tuning is conceptually simple and empirically
powerful. Experimental results on seven NLG tasks demonstrate that input-tuning
is significantly and consistently better than prompt-tuning. Furthermore, on
three of these tasks, input-tuning can achieve a comparable or even better
performance than fine-tuning.",None,-1
1d7f5bdd-0944-438b-ae93-af5fd2bad0b3,Deep Neural Networks as Complex Networks,0.348481,"Deep Neural Networks are, from a physical perspective, graphs whose `links`
and `vertices` iteratively process data and solve tasks sub-optimally. We use
Complex Network Theory (CNT) to represents Deep Neural Networks (DNNs) as
directed weighted graphs: within this framework, we introduce metrics to study
DNNs as dynamical systems, with a granularity that spans from weights to
layers, including neurons. CNT discriminates networks that differ in the number
of parameters and neurons, the type of hidden layers and activations, and the
objective task. We further show that our metrics discriminate low vs. high
performing networks. CNT is a comprehensive method to reason about DNNs and a
complementary approach to explain a model's behavior that is physically
grounded to networks theory and goes beyond the well-studied input-output
relation.",None,-1
1b4332d3-2407-4461-9ce1-801e23388bf9,CSL: A Large-scale Chinese Scientific Literature Dataset,0.923326,"Scientific literature serves as a high-quality corpus, supporting a lot of
Natural Language Processing (NLP) research. However, existing datasets are
centered around the English language, which restricts the development of
Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese
Scientific Literature dataset, which contains the titles, abstracts, keywords
and academic fields of 396k papers. To our knowledge, CSL is the first
scientific document dataset in Chinese. The CSL can serve as a Chinese corpus.
Also, this semi-structured data is a natural annotation that can constitute
many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the
performance of models across scientific domain tasks, i.e., summarization,
keyword generation and text classification. We analyze the behavior of existing
text-to-text models on the evaluation tasks and reveal the challenges for
Chinese scientific NLP tasks, which provides a valuable reference for future
research. Data and code are available at https://github.com/ydli-ai/CSL",https://github.com/ydli-ai/CSL,57782
e0dd306f-c5aa-4372-aa80-02a386f47745,Summary Markov Models for Event Sequences,0.157259,"Datasets involving sequences of different types of events without meaningful
time stamps are prevalent in many applications, for instance when extracted
from textual corpora. We propose a family of models for such event sequences --
summary Markov models -- where the probability of observing an event type
depends only on a summary of historical occurrences of its influencing set of
event types. This Markov model family is motivated by Granger causal models for
time series, with the important distinction that only one event can occur in a
position in an event sequence. We show that a unique minimal influencing set
exists for any set of event types of interest and choice of summary function,
formulate two novel models from the general family that represent specific
sequence dynamics, and propose a greedy search algorithm for learning them from
event sequence data. We conduct an experimental investigation comparing the
proposed models with relevant baselines, and illustrate their knowledge
acquisition and discovery capabilities through case studies involving sequences
from text.",None,-1
0d75ae0b-cc63-49be-a1c7-23ba36930748,How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,0.784596,"Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection.",None,648
8703decd-0080-465a-8bd4-c98a0e1c22c4,Depth Field Networks for Generalizable Multi-view Scene Representation,0.441336,"Modern 3D computer vision leverages learning to boost geometric reasoning,
mapping image data to classical structures such as cost volumes or epipolar
constraints to improve matching. These architectures are specialized according
to the particular problem, and thus require significant task-specific tuning,
often leading to poor domain generalization performance. Recently, generalist
Transformer architectures have achieved impressive results in tasks such as
optical flow and depth estimation by encoding geometric priors as inputs rather
than as enforced constraints. In this paper, we extend this idea and propose to
learn an implicit, multi-view consistent scene representation, introducing a
series of 3D data augmentation techniques as a geometric inductive prior to
increase view diversity. We also show that introducing view synthesis as an
auxiliary task further improves depth estimation. Our Depth Field Networks
(DeFiNe) achieve state-of-the-art results in stereo and video depth estimation
without explicit geometric constraints, and improve on zero-shot domain
generalization by a wide margin.",None,-1
c482ad23-bc50-4ab6-8769-deb64803ed96,DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,0.804437,"Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks.",https://www.github.com/amazon-research/dq-bart/,-1
2b747305-5b8c-4dbc-93a7-68ee030e0680,Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics,0.507148,"Measuring Sentence Textual Similarity (STS) is a classic task that can be
applied to many downstream NLP applications such as text generation and
retrieval. In this paper, we focus on unsupervised STS that works on various
domains but only requires minimal data and computational resources.
Theoretically, we propose a light-weighted Expectation-Correction (EC)
formulation for STS computation. EC formulation unifies unsupervised STS
approaches including the cosine similarity of Additively Composed (AC) sentence
embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose
the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the
compositional phrase semantics by composing multiple recursive EC formulations.
ROTS finishes in linear time and is faster than its predecessors. ROTS is
empirically more effective and scalable than previous approaches. Extensive
experiments on 29 STS tasks under various settings show the clear advantage of
ROTS over existing approaches. Detailed ablation studies demonstrate the
effectiveness of our approaches.",https://github.com/zihao-wang/rots,-1
487d6955-37ea-4866-aa74-1b47a37b26a6,PLM-ICD: Automatic ICD Coding with Pretrained Language Models,0.523438,"Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD",https://github.com/MiuLab/PLM-ICD,-1
668dbf34-0816-4f0b-b4d4-aa2eb3faecca,Unobserved Local Structures Make Compositional Generalization Hard,0.506526,"While recent work has convincingly showed that sequence-to-sequence models
struggle to generalize to new compositions (termed compositional
generalization), little is known on what makes compositional generalization
hard on a particular test instance. In this work, we investigate what are the
factors that make generalization to certain test instances challenging. We
first substantiate that indeed some examples are more difficult than others by
showing that different models consistently fail or succeed on the same test
instances. Then, we propose a criterion for the difficulty of an example: a
test instance is hard if it contains a local structure that was not observed at
training time. We formulate a simple decision rule based on this criterion and
empirically show it predicts instance-level generalization well across 5
different semantic parsing datasets, substantially better than alternative
decision rules. Last, we show local structures can be leveraged for creating
difficult adversarial compositional splits and also to improve compositional
generalization under limited training budgets by strategically selecting
examples for the training set.",https://github.com/benbogin/unobserved-local-structures,-1
7e5f1eef-2d31-42c4-99e7-2a9244f74b85,KinyaBERT: a Morphology-aware Kinyarwanda Language Model,0.884318,"Pre-trained language models such as BERT have been successful at tackling
many natural language processing tasks. However, the unsupervised sub-word
tokenization methods commonly used in these models (e.g., byte-pair encoding -
BPE) are sub-optimal at handling morphologically rich languages. Even given a
morphological analyzer, naive sequencing of morphemes into a standard BERT
architecture is inefficient at capturing morphological compositionality and
expressing word-relative syntactic regularities. We address these challenges by
proposing a simple yet effective two-tier BERT architecture that leverages a
morphological analyzer and explicitly represents morphological
compositionality. Despite the success of BERT, most of its evaluations have
been conducted on high-resource languages, obscuring its applicability on
low-resource languages. We evaluate our proposed method on the low-resource
morphologically rich Kinyarwanda language, naming the proposed model
architecture KinyaBERT. A robust set of experimental results reveal that
KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity
recognition task and by 4.3% in average score of a machine-translated GLUE
benchmark. KinyaBERT fine-tuning has better convergence and achieves more
robust results on multiple tasks even in the presence of translation noise.",https://github.com/anzeyimana/kinyabert-acl2022,-1
f8d12ffa-a34d-4bae-81a1-c340930ea103,Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance,0.937823,"Human-translated text displays distinct features from naturally written text
in the same language. This phenomena, known as translationese, has been argued
to confound the machine translation (MT) evaluation. Yet, we find that existing
work on translationese neglects some important factors and the conclusions are
mostly correlational but not causal. In this work, we collect CausalMT, a
dataset where the MT training data are also labeled with the human translation
directions. We inspect two critical factors, the train-test direction match
(whether the human translation directions in the training and test sets are
aligned), and data-model direction match (whether the model learns in the same
direction as the human translation direction in the dataset). We show that
these two factors have a large causal effect on the MT performance, in addition
to the test-model direction mismatch highlighted by existing work on the impact
of translationese. In light of our findings, we provide a set of suggestions
for MT training and evaluation. Our code and data are at
https://github.com/EdisonNi-hku/CausalMT",https://github.com/EdisonNi-hku/CausalMT,-1
57083e1b-a566-461c-aafc-499936b6a76a,An Application of a Runtime Epistemic Probabilistic Event Calculus to Decision-making in e-Health Systems,0.424044,"We present and discuss a runtime architecture that integrates sensorial data
and classifiers with a logic-based decision-making system in the context of an
e-Health system for the rehabilitation of children with neuromotor disorders.
In this application, children perform a rehabilitation task in the form of
games. The main aim of the system is to derive a set of parameters the child's
current level of cognitive and behavioral performance (e.g., engagement,
attention, task accuracy) from the available sensors and classifiers (e.g., eye
trackers, motion sensors, emotion recognition techniques) and take decisions
accordingly. These decisions are typically aimed at improving the child's
performance by triggering appropriate re-engagement stimuli when their
attention is low, by changing the game or making it more difficult when the
child is losing interest in the task as it is too easy. Alongside
state-of-the-art techniques for emotion recognition and head pose estimation,
we use a runtime variant of a probabilistic and epistemic logic programming
dialect of the Event Calculus, known as the Epistemic Probabilistic Event
Calculus. In particular, the probabilistic component of this symbolic framework
allows for a natural interface with the machine learning techniques. We
overview the architecture and its components, and show some of its
characteristics through a discussion of a running example and experiments.
Under consideration for publication in Theory and Practice of Logic Programming
(TPLP).",https://github.com/dasaro/pec-anglican,-1
7ffe9b46-76d9-4fc3-894d-0ee6bf02eeb2,HLDC: Hindi Legal Documents Corpus,0.758396,"Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: https://github.com/Exploration-Lab/HLDC",https://github.com/Exploration-Lab/HLDC,-1
9af7e8d3-1947-4c75-ae08-f40785ae381b,Coalescing Global and Local Information for Procedural Text Understanding,0.367772,"Procedural text understanding is a challenging language reasoning task that
requires models to track entity states across the development of a narrative. A
complete procedural understanding solution should combine three core aspects:
local and global views of the inputs, and global view of outputs. Prior methods
considered a subset of these aspects, resulting in either low precision or low
recall. In this paper, we propose Coalescing Global and Local Information
(CGLI), a new model that builds entity- and timestep-aware input
representations (local input) considering the whole context (global input), and
we jointly model the entity states with a structured prediction objective
(global output). Thus, CGLI simultaneously optimizes for both precision and
recall. We extend CGLI with additional output layers and integrate it into a
story reasoning framework. Extensive experiments on a popular procedural text
understanding dataset show that our model achieves state-of-the-art results;
experiments on a story reasoning benchmark show the positive impact of our
model on downstream reasoning.",https://github.com/Mayer123/CGLI,-1
7d49b2a7-3004-48d2-b260-82a741ce87e9,SAL-CNN: Estimate the Remaining Useful Life of Bearings Using Time-frequency Information,0.0853306,"In modern industrial production, the prediction ability of the remaining
useful life (RUL) of bearings directly affects the safety and stability of the
system. Traditional methods require rigorous physical modeling and perform
poorly for complex systems. In this paper, an end-to-end RUL prediction method
is proposed, which uses short-time Fourier transform (STFT) as preprocessing.
Considering the time correlation of signal sequences, a long and short-term
memory network is designed in CNN, incorporating the convolutional block
attention module, and understanding the decision-making process of the network
from the interpretability level. Experiments were carried out on the 2012PHM
dataset and compared with other methods, and the results proved the
effectiveness of the method.",None,-1
730b26a3-b026-48d6-9e0f-eb3a3a5454f1,Object Type Clustering using Markov Directly-Follow Multigraph in Object-Centric Process Mining,0.247241,"Object-centric process mining is a new paradigm with more realistic
assumptions about underlying data by considering several case notions, e.g., an
order handling process can be analyzed based on order, item, package, and route
case notions. Including many case notions can result in a very complex model.
To cope with such complexity, this paper introduces a new approach to cluster
similar case notions based on Markov Directly-Follow Multigraph, which is an
extended version of the well-known Directly-Follow Graph supported by many
industrial and academic process mining tools. This graph is used to calculate a
similarity matrix for discovering clusters of similar case notions based on a
threshold. A threshold tuning algorithm is also defined to identify sets of
different clusters that can be discovered based on different levels of
similarity. Thus, the cluster discovery will not rely on merely analysts'
assumptions. The approach is implemented and released as a part of a python
library, called processmining, and it is evaluated through a Purchase to Pay
(P2P) object-centric event log file. Some discovered clusters are evaluated by
discovering Directly Follow-Multigraph by flattening the log based on the
clusters. The similarity between identified clusters is also evaluated by
calculating the similarity between the behavior of the process models
discovered for each case notion using inductive miner based on footprints
conformance checking.",https://github.com/jalaliamin/ResearchCode/tree/main/ot-clustering-markov-dfm-ocpm,-1
1fca88a7-aea8-4ea8-a41a-1ea39a49eab5,A context-aware knowledge transferring strategy for CTC-based ASR,0.747647,"Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method.",None,810
37499101-bd78-451c-9b3b-ca60d45f9d30,Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions,0.752118,"Role-oriented dialogue summarization is to generate summaries for different
roles in the dialogue, e.g., merchants and consumers. Existing methods handle
this task by summarizing each role's content separately and thus are prone to
ignore the information from other roles. However, we believe that other roles'
content could benefit the quality of summaries, such as the omitted information
mentioned by other roles. Therefore, we propose a novel role interaction
enhanced method for role-oriented dialogue summarization. It adopts cross
attention and decoder self-attention interactions to interactively acquire
other roles' critical information. The cross attention interaction aims to
select other roles' critical dialogue utterances, while the decoder
self-attention interaction aims to obtain key information from other roles'
summaries. Experimental results have shown that our proposed method
significantly outperforms strong baselines on two public role-oriented dialogue
summarization datasets. Extensive analyses have demonstrated that other roles'
content could help generate summaries with more complete semantics and correct
topic structures.",https://github.com/xiaolinAndy/RODS,15943
8569c238-795e-4667-9111-7aaa6fdd555c,CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification,0.724332,"Existing computer vision research in artwork struggles with artwork's
fine-grained attributes recognition and lack of curated annotated datasets due
to their costly creation. To the best of our knowledge, we are one of the first
methods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural
network on a variety of artwork images and text descriptions pairs. CLIP is
able to learn directly from free-form art descriptions, or, if available,
curated fine-grained labels. Model's zero-shot capability allows predicting
accurate natural language description for a given image, without directly
optimizing for the task. Our approach aims to solve 2 challenges: instance
retrieval and fine-grained artwork attribute recognition. We use the iMet
Dataset, which we consider the largest annotated artwork dataset. In this
benchmark we achieved competitive results using only self-supervision.",https://github.com/KeremTurgutlu/clip_art,-1
8e97948d-8bc7-4ba9-9609-e753f98cb348,Protecting President Zelenskyy against Deep Fakes,0.188399,"The 2022 Russian invasion of Ukraine is being fought on two fronts: a brutal
ground war and a duplicitous disinformation campaign designed to conceal and
justify Russia's actions. This campaign includes at least one example of a
deep-fake video purportedly showing Ukrainian President Zelenskyy admitting
defeat and surrendering. In anticipation of future attacks of this form, we
describe a facial and gestural behavioral model that captures distinctive
characteristics of Zelenskyy's speaking style. Trained on over eight hours of
authentic video from four different settings, we show that this behavioral
model can distinguish Zelenskyy from deep-fake imposters.This model can play an
important role -- particularly during the fog of war -- in distinguishing the
real from the fake.",None,-1
374daa14-6780-4860-9181-a9a4b0d5fe11,Optimal Fine-Grained N:M sparsity for Activations and Neural Gradients,0.479383,"In deep learning, fine-grained N:M sparsity reduces the data footprint and
bandwidth of a General Matrix multiply (GEMM) by x2, and doubles throughput by
skipping computation of zero values. So far, it was only used to prune weights.
We examine how this method can be used also for activations and their gradients
(i.e., ""neural gradients""). To this end, we first establish a tensor-level
optimality criteria. Previous works aimed to minimize the mean-square-error
(MSE) of each pruned block. We show that while minimization of the MSE works
fine for pruning the activations, it catastrophically fails for the neural
gradients. Instead, we show that optimal pruning of the neural gradients
requires an unbiased minimum-variance pruning mask. We design such specialized
masks, and find that in most cases, 1:2 sparsity is sufficient for training,
and 2:4 sparsity is usually enough when this is not the case. Further, we
suggest combining several such methods together in order to potentially speed
up training even more. A reference implementation is supplied in
https://github.com/brianchmiel/Act-and-Grad-structured-sparsity.",https://github.com/brianchmiel/Act-and-Grad-structured-sparsity,-1
544f247a-7d6a-471f-bc35-783aa2309acc,"Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses",0.269929,"Extracting information from radiofrequency (RF) signals using artificial
neural networks at low energy cost is a critical need for a wide range of
applications from radars to health. These RF inputs are composed of multiples
frequencies. Here we show that magnetic tunnel junctions can process analogue
RF inputs with multiple frequencies in parallel and perform synaptic
operations. Using a backpropagation-free method called extreme learning, we
classify noisy images encoded by RF signals, using experimental data from
magnetic tunnel junctions functioning as both synapses and neurons. We achieve
the same accuracy as an equivalent software neural network. These results are a
key step for embedded radiofrequency artificial intelligence.",None,-1
6c5acaa3-1c44-43f7-b169-e1fcc2f39827,Switch Trajectory Transformer with Distributional Value Approximation for Multi-Task Reinforcement Learning,0.230538,"We propose SwitchTT, a multi-task extension to Trajectory Transformer but
enhanced with two striking features: (i) exploiting a sparsely activated model
to reduce computation cost in multi-task offline model learning and (ii)
adopting a distributional trajectory value estimator that improves policy
performance, especially in sparse reward settings. These two enhancements make
SwitchTT suitable for solving multi-task offline reinforcement learning
problems, where model capacity is critical for absorbing the vast quantities of
knowledge available in the multi-task dataset. More specifically, SwitchTT
exploits switch transformer model architecture for multi-task policy learning,
allowing us to improve model capacity without proportional computation cost.
Also, SwitchTT approximates the distribution rather than the expectation of
trajectory value, mitigating the effects of the Monte-Carlo Value estimator
suffering from poor sample complexity, especially in the sparse-reward setting.
We evaluate our method using the suite of ten sparse-reward tasks from the
gym-mini-grid environment.We show an improvement of 10% over Trajectory
Transformer across 10-task learning and obtain up to 90% increase in offline
model training speed. Our results also demonstrate the advantage of the switch
transformer model for absorbing expert knowledge and the importance of value
distribution in evaluating the trajectory.",None,-1
7d426427-2cb8-4534-b890-04ba8e7dcfe6,Dialect-robust Evaluation of Generated Text,0.353535,"Evaluation metrics that are not robust to dialect variation make it
impossible to tell how well systems perform for many groups of users, and can
even penalize systems for producing text in lower-resource dialects. However,
currently, there exists no way to quantify how metrics respond to change in the
dialect of a generated utterance. We thus formalize dialect robustness and
dialect awareness as goals for NLG evaluation metrics. We introduce a suite of
methods and corresponding statistical tests one can use to assess metrics in
light of the two goals. Applying the suite to current state-of-the-art metrics,
we demonstrate that they are not dialect-robust and that semantic perturbations
frequently lead to smaller decreases in a metric than the introduction of
dialect features. As a first step to overcome this limitation, we propose a
training schema, NANO, which introduces regional and language information to
the pretraining process of a metric. We demonstrate that NANO provides a
size-efficient way for models to improve the dialect robustness while
simultaneously improving their performance on the standard metric benchmark.",https://github.com/google-research/bleurt,-1
69fd0853-4bc6-4c4d-a04a-8dcc9a86194b,Training a Vision Transformer from scratch in less than 24 hours with 1 GPU,0.0978354,"Transformers have become central to recent advances in computer vision.
However, training a vision Transformer (ViT) model from scratch can be resource
intensive and time consuming. In this paper, we aim to explore approaches to
reduce the training costs of ViT models. We introduce some algorithmic
improvements to enable training a ViT model from scratch with limited hardware
(1 GPU) and time (24 hours) resources. First, we propose an efficient approach
to add locality to the ViT architecture. Second, we develop a new image size
curriculum learning strategy, which allows to reduce the number of patches
extracted from each image at the beginning of the training. Finally, we propose
a new variant of the popular ImageNet1k benchmark by adding hardware and time
constraints. We evaluate our contributions on this benchmark, and show they can
significantly improve performances given the proposed training budget. We will
share the code in https://github.com/BorealisAI/efficient-vit-training.",https://github.com/BorealisAI/efcient-vit-training,-1
733512a1-5372-4bc4-890b-8008db74f4db,FAN-Trans: Online Knowledge Distillation for Facial Action Unit Detection,0.705288,"Due to its importance in facial behaviour analysis, facial action unit (AU)
detection has attracted increasing attention from the research community.
Leveraging the online knowledge distillation framework, we propose the
``FANTrans"" method for AU detection. Our model consists of a hybrid network of
convolution and transformer blocks to learn per-AU features and to model AU
co-occurrences. The model uses a pre-trained face alignment network as the
feature extractor. After further transformation by a small learnable add-on
convolutional subnet, the per-AU features are fed into transformer blocks to
enhance their representation. As multiple AUs often appear together, we propose
a learnable attention drop mechanism in the transformer block to learn the
correlation between the features for different AUs. We also design a classifier
that predicts AU presence by considering all AUs' features, to explicitly
capture label dependencies. Finally, we make the attempt of adapting online
knowledge distillation in the training stage for this task, further improving
the model's performance. Experiments on the BP4D and DISFA datasets
demonstrating the effectiveness of proposed method.",https://github.com/rwightman/pytorch-image-models,-1
222ac523-6497-46fa-9661-8f1674dafe80,An Online Approach to Solve the Dynamic Vehicle Routing Problem with Stochastic Trip Requests for Paratransit Services,0.601083,"Many transit agencies operating paratransit and microtransit services have to
respond to trip requests that arrive in real-time, which entails solving hard
combinatorial and sequential decision-making problems under uncertainty. To
avoid decisions that lead to significant inefficiency in the long term,
vehicles should be allocated to requests by optimizing a non-myopic utility
function or by batching requests together and optimizing a myopic utility
function. While the former approach is typically offline, the latter can be
performed online. We point out two major issues with such approaches when
applied to paratransit services in practice. First, it is difficult to batch
paratransit requests together as they are temporally sparse. Second, the
environment in which transit agencies operate changes dynamically (e.g.,
traffic conditions), causing estimates that are learned offline to become
stale. To address these challenges, we propose a fully online approach to solve
the dynamic vehicle routing problem (DVRP) with time windows and stochastic
trip requests that is robust to changing environmental dynamics by
construction. We focus on scenarios where requests are relatively sparse - our
problem is motivated by applications to paratransit services. We formulate DVRP
as a Markov decision process and use Monte Carlo tree search to evaluate
actions for any given state. Accounting for stochastic requests while
optimizing a non-myopic utility function is computationally challenging;
indeed, the action space for such a problem is intractably large in practice.
To tackle the large action space, we leverage the structure of the problem to
design heuristics that can sample promising actions for the tree search. Our
experiments using real-world data from our partner agency show that the
proposed approach outperforms existing state-of-the-art approaches both in
terms of performance and robustness.",https://github.com/smarttransit-ai/iccps-2022-paratransit-public,-1
3807b5eb-5d43-4950-ae73-7468edab6e71,TODE-Trans: Transparent Object Depth Estimation with Transformer,0.547453,"Transparent objects are widely used in industrial automation and daily life.
However, robust visual recognition and perception of transparent objects have
always been a major challenge. Currently, most commercial-grade depth cameras
are still not good at sensing the surfaces of transparent objects due to the
refraction and reflection of light. In this work, we present a
transformer-based transparent object depth estimation approach from a single
RGB-D input. We observe that the global characteristics of the transformer make
it easier to extract contextual information to perform depth estimation of
transparent areas. In addition, to better enhance the fine-grained features, a
feature fusion module (FFM) is designed to assist coherent prediction. Our
empirical evidence demonstrates that our model delivers significant
improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on
REL compared to previous state-of-the-art convolutional-based counterparts in
ClearGrasp dataset. Extensive results show that our transformer-based model
enables better aggregation of the object's RGB and inaccurate depth information
to obtain a better depth representation. Our code and the pre-trained model
will be available at https://github.com/yuchendoudou/TODE.",None,6668
d404aa7d-4d0f-4a57-9b77-4f9ee83bf8d9,A Large Scale Search Dataset for Unbiased Learning to Rank,0.910732,"The unbiased learning to rank (ULTR) problem has been greatly advanced by
recent deep learning techniques and well-designed debias algorithms. However,
promising results on the existing benchmark datasets may not be extended to the
practical scenario due to the following disadvantages observed from those
popular benchmark datasets: (1) outdated semantic feature extraction where
state-of-the-art large scale pre-trained language models like BERT cannot be
exploited due to the missing of the original text;(2) incomplete display
features for in-depth study of ULTR, e.g., missing the displayed abstract of
documents for analyzing the click necessary bias; (3) lacking real-world user
feedback, leading to the prevalence of synthetic datasets in the empirical
study. To overcome the above disadvantages, we introduce the Baidu-ULTR
dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008
expert annotated queries, which is orders of magnitude larger than the existing
ones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained
language model for easy usage; (2) sufficient display information such as
position, displayed height, and displayed abstract, enabling the comprehensive
study of different biases with advanced techniques such as causal discovery and
meta-learning; and (3) rich user feedback on search result pages (SERPs) like
dwelling time, allowing for user engagement optimization and promoting the
exploration of multi-task learning in ULTR. In this paper, we present the
design principle of Baidu-ULTR and the performance of benchmark ULTR algorithms
on this new data resource, favoring the exploration of ranking for long-tail
queries and pre-training tasks for ranking. The Baidu-ULTR dataset and
corresponding baseline implementation are available at
https://github.com/ChuXiaokai/baidu_ultr_dataset.",https://github.com/ChuXiaokai/baidu_ultr_dataset,-1
4698279d-eb26-4d7e-af7b-fd07b5e665fa,A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network,0.649139,"With the advent of the screen-reading era, the confidential documents
displayed on the screen can be easily captured by a camera without leaving any
traces. Thus, this paper proposes a novel screen-shooting resilient
watermarking scheme for document image using deep neural network. By applying
this scheme, when the watermarked image is displayed on the screen and captured
by a camera, the watermark can be still extracted from the captured
photographs. Specifically, our scheme is an end-to-end neural network with an
encoder to embed watermark and a decoder to extract watermark. During the
training process, a distortion layer between encoder and decoder is added to
simulate the distortions introduced by screen-shooting process in real scenes,
such as camera distortion, shooting distortion, light source distortion.
Besides, an embedding strength adjustment strategy is designed to improve the
visual quality of the watermarked image with little loss of extraction
accuracy. The experimental results show that the scheme has higher robustness
and visual quality than other three recent state-of-the-arts. Specially, even
if the shooting distances and angles are in extreme, our scheme can also obtain
high extraction accuracy.",https://github.com/gslxr/Screen-Shooting-Resilient-Document-Image-Watermarking,-1
d886b9f5-61e1-4468-b840-ee9d9f35cc78,Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion,0.420609,"Automatic discourse processing is bottlenecked by data: current discourse
formalisms pose highly demanding annotation tasks involving large taxonomies of
discourse relations, making them inaccessible to lay annotators. This work
instead adopts the linguistic framework of Questions Under Discussion (QUD) for
discourse analysis and seeks to derive QUD structures automatically. QUD views
each sentence as an answer to a question triggered in prior context; thus, we
characterize relationships between sentences as free-form questions, in
contrast to exhaustive fine-grained taxonomies. We develop the
first-of-its-kind QUD parser that derives a dependency structure of questions
over full documents, trained using a large, crowdsourced question-answering
dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD
dependency parsing is possible for language models trained with this
crowdsourced, generalizable annotation scheme. We illustrate how our QUD
structure is distinct from RST trees, and demonstrate the utility of QUD
analysis in the context of document simplification. Our findings show that QUD
parsing is an appealing alternative for automatic discourse processing.",https://github.com/wjko/discourse-qa,-1
631e3b42-08dd-41ea-9a11-c2d13933a7fc,The Best Decisions Are Not the Best Advice: Making Adherence-Aware Recommendations,0.804561,"Many high-stake decisions follow an expert-in-loop structure in that a human
operator receives recommendations from an algorithm but is the ultimate
decision maker. Hence, the algorithm's recommendation may differ from the
actual decision implemented in practice. However, most algorithmic
recommendations are obtained by solving an optimization problem that assumes
recommendations will be perfectly implemented. We propose an adherence-aware
optimization framework to capture the dichotomy between the recommended and the
implemented policy and analyze the impact of partial adherence on the optimal
recommendation. We show that overlooking the partial adherence phenomenon, as
is currently being done by most recommendation engines, can lead to arbitrarily
severe performance deterioration, compared with both the current human baseline
performance and what is expected by the recommendation algorithm. Our framework
also provides useful tools to analyze the structure and to compute optimal
recommendation policies that are naturally immune against such human
deviations, and are guaranteed to improve upon the baseline policy.",None,-1
60139d80-a3ef-4879-8205-03fde1c20401,CZU-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and 10 wearable inertial sensors,0.400692,"Human action recognition has been widely used in many fields of life, and
many human action datasets have been published at the same time. However, most
of the multi-modal databases have some shortcomings in the layout and number of
sensors, which cannot fully represent the action features. Regarding the
problems, this paper proposes a freely available dataset, named CZU-MHAD
(Changzhou University: a comprehensive multi-modal human action dataset). It
consists of 22 actions and three modals temporal synchronized data. These
modals include depth videos and skeleton positions from a kinect v2 camera, and
inertial signals from 10 wearable sensors. Compared with single modal sensors,
multi-modal sensors can collect different modal data, so the use of multi-modal
sensors can describe actions more accurately. Moreover, CZU-MHAD obtains the
3-axis acceleration and 3-axis angular velocity of 10 main motion joints by
binding inertial sensors to them, and these data were captured at the same
time. Experimental results are provided to show that this dataset can be used
to study structural relationships between different parts of the human body
when performing actions and fusion approaches that involve multi-modal sensor
data.",https://github.com/yujmo/czu mhad/,-1
9dfa3dff-882f-4a41-af22-79fb0cda62f7,Biological Robots: Perspectives on an Emerging Interdisciplinary Field,0.70404,"Advances in science and engineering often reveal the limitations of classical
approaches initially used to understand, predict, and control phenomena. With
progress, conceptual categories must often be re-evaluated to better track
recently discovered invariants across disciplines. It is essential to refine
frameworks and resolve conflicting boundaries between disciplines such that
they better facilitate, not restrict, experimental approaches and capabilities.
In this essay, we discuss issues at the intersection of developmental biology,
computer science, and robotics. In the context of biological robots, we explore
changes across concepts and previously distinct fields that are driven by
recent advances in materials, information, and life sciences. Herein, each
author provides their own perspective on the subject, framed by their own
disciplinary training. We argue that as with computation, certain aspects of
developmental biology and robotics are not tied to specific materials; rather,
the consilience of these fields can help to shed light on issues of multi-scale
control, self-assembly, and relationships between form and function. We hope
new fields can emerge as boundaries arising from technological limitations are
overcome, furthering practical applications from regenerative medicine to
useful synthetic living machines.",None,-1
3a437b23-0b99-444f-9e5a-b72b3fc9e4fa,BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate Speech in Different Social Contexts,0.763829,"Social media platforms and online streaming services have spawned a new breed
of Hate Speech (HS). Due to the massive amount of user-generated content on
these sites, modern machine learning techniques are found to be feasible and
cost-effective to tackle this problem. However, linguistically diverse datasets
covering different social contexts in which offensive language is typically
used are required to train generalizable models. In this paper, we identify the
shortcomings of existing Bangla HS datasets and introduce a large manually
labeled dataset BD-SHS that includes HS in different social contexts. The
labeling criteria were prepared following a hierarchical annotation process,
which is the first of its kind in Bangla HS to the best of our knowledge. The
dataset includes more than 50,200 offensive comments crawled from online social
networking sites and is at least 60% larger than any existing Bangla HS
datasets. We present the benchmark result of our dataset by training different
NLP models resulting in the best one achieving an F1-score of 91.0%. In our
experiments, we found that a word embedding trained exclusively using 1.47
million comments from social media and streaming sites consistently resulted in
better modeling of HS detection in comparison to other pre-trained embeddings.
Our dataset and all accompanying codes is publicly available at
github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media",https://github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media,-1
03fa7109-2e04-4ffc-9d1e-a12b7f829f43,Prosodic Alignment for off-screen automatic dubbing,0.833214,"The goal of automatic dubbing is to perform speech-to-speech translation
while achieving audiovisual coherence. This entails isochrony, i.e.,
translating the original speech by also matching its prosodic structure into
phrases and pauses, especially when the speaker's mouth is visible. In previous
work, we introduced a prosodic alignment model to address isochrone or
on-screen dubbing. In this work, we extend the prosodic alignment model to also
address off-screen dubbing that requires less stringent synchronization
constraints. We conduct experiments on four dubbing directions - English to
French, Italian, German and Spanish - on a publicly available collection of TED
Talks and on publicly available YouTube videos. Empirical results show that
compared to our previous work the extended prosodic alignment model provides
significantly better subjective viewing experience on videos in which on-screen
and off-screen automatic dubbing is applied for sentences with speakers mouth
visible and not visible, respectively.",None,-1
b86818f4-6847-4045-9ba5-2891599b4162,MonoGround: Detecting Monocular 3D Objects from the Ground,0.696527,"Monocular 3D object detection has attracted great attention for its
advantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping
essence from the monocular imaging process, monocular 3D object detection
suffers from inaccurate depth estimation and thus has poor 3D detection
results. To alleviate this problem, we propose to introduce the ground plane as
a prior in the monocular 3d object detection. The ground plane prior serves as
an additional geometric condition to the ill-posed mapping and an extra source
in depth estimation. In this way, we can get a more accurate depth estimation
from the ground. Meanwhile, to take full advantage of the ground plane prior,
we propose a depth-align training strategy and a precise two-stage depth
inference method tailored for the ground plane prior. It is worth noting that
the introduced ground plane prior requires no extra data sources like LiDAR,
stereo images, and depth information. Extensive experiments on the KITTI
benchmark show that our method could achieve state-of-the-art results compared
with other methods while maintaining a very fast speed. Our code and models are
available at https://github.com/cfzd/MonoGround.",https://github.com/cfzd/MonoGround,-1
5b320b39-fc32-4405-b98b-4fde3fb2f381,Learning Personalized Human-Aware Robot Navigation Using Virtual Reality Demonstrations from a User Study,0.646624,"For the most comfortable, human-aware robot navigation, subjective user
preferences need to be taken into account. This paper presents a novel
reinforcement learning framework to train a personalized navigation controller
along with an intuitive virtual reality demonstration interface. The conducted
user study provides evidence that our personalized approach significantly
outperforms classical approaches with more comfortable human-robot experiences.
We achieve these results using only a few demonstration trajectories from
non-expert users, who predominantly appreciate the intuitive demonstration
setup. As we show in the experiments, the learned controller generalizes well
to states not covered in the demonstration data, while still reflecting user
preferences during navigation. Finally, we transfer the navigation controller
without loss in performance to a real robot.",None,-1
8ca7daee-a019-4554-945f-695e016d028c,Spiking Approximations of the MaxPooling Operation in Deep SNNs,0.454638,"Spiking Neural Networks (SNNs) are an emerging domain of biologically
inspired neural networks that have shown promise for low-power AI. A number of
methods exist for building deep SNNs, with Artificial Neural Network
(ANN)-to-SNN conversion being highly successful. MaxPooling layers in
Convolutional Neural Networks (CNNs) are an integral component to downsample
the intermediate feature maps and introduce translational invariance, but the
absence of their hardware-friendly spiking equivalents limits such CNNs'
conversion to deep SNNs. In this paper, we present two hardware-friendly
methods to implement Max-Pooling in deep SNNs, thus facilitating easy
conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute
SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware
(with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our
approach.",None,-1
5365c4c5-f043-4eb9-b836-22aacd82e114,Computationally efficient joint coordination of multiple electric vehicle charging points using reinforcement learning,0.12151,"A major challenge in todays power grid is to manage the increasing load from
electric vehicle (EV) charging. Demand response (DR) solutions aim to exploit
flexibility therein, i.e., the ability to shift EV charging in time and thus
avoid excessive peaks or achieve better balancing. Whereas the majority of
existing research works either focus on control strategies for a single EV
charger, or use a multi-step approach (e.g., a first high level aggregate
control decision step, followed by individual EV control decisions), we rather
propose a single-step solution that jointly coordinates multiple charging
points at once. In this paper, we further refine an initial proposal using
reinforcement learning (RL), specifically addressing computational challenges
that would limit its deployment in practice. More precisely, we design a new
Markov decision process (MDP) formulation of the EV charging coordination
process, exhibiting only linear space and time complexity (as opposed to the
earlier quadratic space complexity). We thus improve upon earlier
state-of-the-art, demonstrating 30% reduction of training time in our case
study using real-world EV charging session data. Yet, we do not sacrifice the
resulting performance in meeting the DR objectives: our new RL solutions still
improve the performance of charging demand coordination by 40-50% compared to a
business-as-usual policy (that charges EV fully upon arrival) and 20-30%
compared to a heuristic policy (that uniformly spreads individual EV charging
over time).",None,-1
e50ada6d-f65a-4011-bc6f-97ac0aff2a73,TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack,0.285864,"Existing textual adversarial attacks usually utilize the gradient or
prediction confidence to generate adversarial examples, making it hard to be
deployed in real-world applications. To this end, we consider a rarely
investigated but more rigorous setting, namely hard-label attack, in which the
attacker can only access the prediction label. In particular, we find we can
learn the importance of different words via the change on prediction label
caused by word substitutions on the adversarial examples. Based on this
observation, we propose a novel adversarial attack, termed Text Hard-label
attacker (TextHacker). TextHacker randomly perturbs lots of words to craft an
adversarial example. Then, TextHacker adopts a hybrid local search algorithm
with the estimation of word importance from the attack history to minimize the
adversarial perturbation. Extensive evaluations for text classification and
textual entailment show that TextHacker significantly outperforms existing
hard-label attacks regarding the attack performance as well as adversary
quality.",https://github.com/JHL-HUST/TextHacker,-1
891b38a2-569d-437b-b13b-fef9c75a12d4,Photo-realistic 360 Head Avatars in the Wild,0.112387,"Delivering immersive, 3D experiences for human communication requires a
method to obtain 360 degree photo-realistic avatars of humans. To make these
experiences accessible to all, only commodity hardware, like mobile phone
cameras, should be necessary to capture the data needed for avatar creation.
For avatars to be rendered realistically from any viewpoint, we require
training images and camera poses from all angles. However, we cannot rely on
there being trackable features in the foreground or background of all images
for use in estimating poses, especially from the side or back of the head. To
overcome this, we propose a novel landmark detector trained on synthetic data
to estimate camera poses from 360 degree mobile phone videos of a human head
for use in a multi-stage optimization process which creates a photo-realistic
avatar. We perform validation experiments with synthetic data and showcase our
method on 360 degree avatars trained from mobile phone videos.",None,-1
133a22d2-b7d2-42dd-bece-fcb6635a6a5c,Traffic Sign Classification Using Deep and Quantum Neural Networks,0.272593,"Quantum Neural Networks (QNNs) are an emerging technology that can be used in
many applications including computer vision. In this paper, we presented a
traffic sign classification system implemented using a hybrid quantum-classical
convolutional neural network. Experiments on the German Traffic Sign
Recognition Benchmark dataset indicate that currently QNN do not outperform
classical DCNN (Deep Convolutuional Neural Networks), yet still provide an
accuracy of over 90% and are a definitely promising solution for advanced
computer vision.",None,743
9c164c6e-9e05-4ba2-beb9-e3a90dbdb7a9,Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation,0.109199,"Averaging predictions of a deep ensemble of networks is apopular and
effective method to improve predictive performance andcalibration in various
benchmarks and Kaggle competitions. However, theruntime and training cost of
deep ensembles grow linearly with the size ofthe ensemble, making them
unsuitable for many applications. Averagingensemble weights instead of
predictions circumvents this disadvantageduring inference and is typically
applied to intermediate checkpoints ofa model to reduce training cost. Albeit
effective, only few works haveimproved the understanding and the performance of
weight averaging.Here, we revisit this approach and show that a simple weight
fusion (WF)strategy can lead to a significantly improved predictive performance
andcalibration. We describe what prerequisites the weights must meet interms of
weight space, functional space and loss. Furthermore, we presenta new test
method (called oracle test) to measure the functional spacebetween weights. We
demonstrate the versatility of our WF strategy acrossstate of the art
segmentation CNNs and Transformers as well as real worlddatasets such as
BDD100K and Cityscapes. We compare WF with similarapproaches and show our
superiority for in- and out-of-distribution datain terms of predictive
performance and calibration.",None,-1
0cbaf748-c5b6-4106-86c1-7f76665b4241,Detecting Driver Drowsiness as an Anomaly Using LSTM Autoencoders,0.54876,"In this paper, an LSTM autoencoder-based architecture is utilized for
drowsiness detection with ResNet-34 as feature extractor. The problem is
considered as anomaly detection for a single subject; therefore, only the
normal driving representations are learned and it is expected that drowsiness
representations, yielding higher reconstruction losses, are to be distinguished
according to the knowledge of the network. In our study, the confidence levels
of normal and anomaly clips are investigated through the methodology of label
assignment such that training performance of LSTM autoencoder and
interpretation of anomalies encountered during testing are analyzed under
varying confidence rates. Our method is experimented on NTHU-DDD and
benchmarked with a state-of-the-art anomaly detection method for driver
drowsiness. Results show that the proposed model achieves detection rate of
0.8740 area under curve (AUC) and is able to provide significant improvements
on certain scenarios.",None,2076
639117bd-5a73-4acd-99b3-8d12b2cee2f4,Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective,0.302117,"Recent years have witnessed remarkable success achieved by graph neural
networks (GNNs) in many real-world applications such as recommendation and drug
discovery. Despite the success, oversmoothing has been identified as one of the
key issues which limit the performance of deep GNNs. It indicates that the
learned node representations are highly indistinguishable due to the stacked
aggregators. In this paper, we propose a new perspective to look at the
performance degradation of deep GNNs, i.e., feature overcorrelation. Through
empirical and theoretical study on this matter, we demonstrate the existence of
feature overcorrelation in deeper GNNs and reveal potential reasons leading to
this issue. To reduce the feature correlation, we propose a general framework
DeCorr which can encourage GNNs to encode less redundant information. Extensive
experiments have demonstrated that DeCorr can help enable deeper GNNs and is
complementary to existing techniques tackling the oversmoothing issue.",https://github.com/ChandlerBang/DeCorr,-1
196cdc86-f062-48cc-83d4-6097924d2382,HYU at SemEval-2022 Task 2: Effective Idiomaticity Detection with Consideration at Different Levels of Contextualization,0.0143118,"We propose a unified framework that enables us to consider various aspects of
contextualization at different levels to better identify the idiomaticity of
multi-word expressions. Through extensive experiments, we demonstrate that our
approach based on the inter- and inner-sentence context of a target MWE is
effective in improving the performance of related models. We also share our
experience in detail on the task of SemEval-2022 Tasks 2 such that future work
on the same task can be benefited from this.",None,614
db336156-4c42-44fe-a197-b8ee601096fa,Sockeye 3: Fast Neural Machine Translation with PyTorch,0.454404,"Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine
Translation (NMT). Now based on PyTorch, Sockeye 3 provides faster model
implementations and more advanced features with a further streamlined codebase.
This enables broader experimentation with faster iteration, efficient training
of stronger and faster models, and the flexibility to move new ideas quickly
from research to production. When running comparable models, Sockeye 3 is up to
126% faster than other PyTorch implementations on GPUs and up to 292% faster on
CPUs. Sockeye 3 is open source software released under the Apache 2.0 license.",https://github.com/awslabs/sockeye,-1
50ff8b97-25bb-4113-b8f5-19043c78d939,OTExtSum: Extractive Text Summarisation with Optimal Transport,0.231678,"Extractive text summarisation aims to select salient sentences from a
document to form a short yet informative summary. While learning-based methods
have achieved promising results, they have several limitations, such as
dependence on expensive training and lack of interpretability. Therefore, in
this paper, we propose a novel non-learning-based method by for the first time
formulating text summarisation as an Optimal Transport (OT) problem, namely
Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction
is conceptualised as obtaining an optimal summary that minimises the
transportation cost to a given document regarding their semantic distributions.
Such a cost is defined by the Wasserstein distance and used to measure the
summary's semantic coverage of the original document. Comprehensive experiments
on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and
CNN/DM demonstrate that our proposed method outperforms the state-of-the-art
non-learning-based methods and several recent learning-based methods in terms
of the ROUGE metric.",https://github.com/peggypytang/OTExtSum/,-1
27a4536a-f0c4-4e06-b8fd-e59a8ebc598d,The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization,0.670134,"We present a novel benchmark and associated evaluation metrics for assessing
the performance of text anonymization methods. Text anonymization, defined as
the task of editing a text document to prevent the disclosure of personal
information, currently suffers from a shortage of privacy-oriented annotated
text resources, making it difficult to properly evaluate the level of privacy
protection offered by various anonymization methods. This paper presents TAB
(Text Anonymization Benchmark), a new, open-source annotated corpus developed
to address this shortage. The corpus comprises 1,268 English-language court
cases from the European Court of Human Rights (ECHR) enriched with
comprehensive annotations about the personal information appearing in each
document, including their semantic category, identifier type, confidential
attributes, and co-reference relations. Compared to previous work, the TAB
corpus is designed to go beyond traditional de-identification (which is limited
to the detection of predefined semantic categories), and explicitly marks which
text spans ought to be masked in order to conceal the identity of the person to
be protected. Along with presenting the corpus and its annotation layers, we
also propose a set of evaluation metrics that are specifically tailored towards
measuring the performance of text anonymization, both in terms of privacy
protection and utility preservation. We illustrate the use of the benchmark and
the proposed metrics by assessing the empirical performance of several baseline
text anonymization models. The full corpus along with its privacy-oriented
annotation guidelines, evaluation scripts and baseline models are available on:
https://github.com/NorskRegnesentral/text-anonymisation-benchmark",https://github.com/NorskRegnesentral/text-anonymization-benchmark,-1
f8430f13-504e-4c01-a472-c963bc4cefd7,A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes,0.0689087,"We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.",https://github.com/avinashpaliwal/Super-SloMo,-1
399eb08a-da16-4e61-9803-90372b72a351,Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification,0.278623,"The main question we address in this paper is how to scale up visual
recognition of unseen classes, also known as zero-shot learning, to tens of
thousands of categories as in the ImageNet-21K benchmark. At this scale,
especially with many fine-grained categories included in ImageNet-21K, it is
critical to learn quality visual semantic representations that are
discriminative enough to recognize unseen classes and distinguish them from
seen ones. We propose a \emph{H}ierarchical \emph{G}raphical knowledge
\emph{R}epresentation framework for the confidence-based classification method,
dubbed as HGR-Net. Our experimental results demonstrate that HGR-Net can grasp
class inheritance relations by utilizing hierarchical conceptual knowledge. Our
method significantly outperformed all existing techniques, boosting the
performance by 7\% compared to the runner-up approach on the ImageNet-21K
benchmark. We show that HGR-Net is learning-efficient in few-shot scenarios. We
also analyzed our method on smaller datasets like ImageNet-21K-P, 2-hops and
3-hops, demonstrating its generalization ability. Our benchmark and code are
available at https://kaiyi.me/p/hgrnet.html.",https://kaiyi.me/p/hgrnet.html,-1
079fd354-4f75-422c-af06-c29419da7ce7,nerf2nerf: Pairwise Registration of Neural Radiance Fields,0.60803,"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io",None,9133
5f1d231f-f7ae-457b-b7fb-24bd691e6ef7,BlobGAN: Spatially Disentangled Scene Representations,0.800611,"We propose an unsupervised, mid-level representation for a generative model
of scenes. The representation is mid-level in that it is neither per-pixel nor
per-image; rather, scenes are modeled as a collection of spatial, depth-ordered
""blobs"" of features. Blobs are differentiably placed onto a feature grid that
is decoded into an image by a generative adversarial network. Due to the
spatial uniformity of blobs and the locality inherent to convolution, our
network learns to associate different blobs with different entities in a scene
and to arrange these blobs to capture scene layout. We demonstrate this
emergent behavior by showing that, despite training without any supervision,
our method enables applications such as easy manipulation of objects within a
scene (e.g., moving, removing, and restyling furniture), creation of feasible
scenes given constraints (e.g., plausible rooms with drawers at a particular
location), and parsing of real-world images into constituent parts. On a
challenging multi-category dataset of indoor scenes, BlobGAN outperforms
StyleGAN2 in image quality as measured by FID. See our project page for video
results and interactive demo: https://www.dave.ml/blobgan",None,-1
03c00ae7-6ab7-4b44-b88c-a1e66500980a,Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers,0.103145,"Text-to-SQL parsers typically struggle with databases unseen during the train
time. Adapting parsers to new databases is a challenging problem due to the
lack of natural language queries in the new schemas. We present ReFill, a
framework for synthesizing high-quality and textually diverse parallel datasets
for adapting a Text-to-SQL parser to a target schema. ReFill learns to
retrieve-and-edit text queries from the existing schemas and transfers them to
the target schema. We show that retrieving diverse existing text, masking their
schema-specific tokens, and refilling with tokens relevant to the target
schema, leads to significantly more diverse text queries than achievable by
standard SQL-to-Text generation methods. Through experiments spanning multiple
databases, we demonstrate that fine-tuning parsers on datasets synthesized
using ReFill consistently outperforms the prior data-augmentation methods.",https://github.com/awasthiabhijeet/refill,-1
4d53e077-f887-4400-8add-6c576c963278,ALTO: A Large-Scale Dataset for UAV Visual Place Recognition and Localization,0.812782,"We present the ALTO dataset, a vision-focused dataset for the development and
benchmarking of Visual Place Recognition and Localization methods for Unmanned
Aerial Vehicles. The dataset is composed of two long (approximately 150km and
260km) trajectories flown by a helicopter over Ohio and Pennsylvania, and it
includes high precision GPS-INS ground truth location data, high precision
accelerometer readings, laser altimeter readings, and RGB downward facing
camera imagery. In addition, we provide reference imagery over the flight
paths, which makes this dataset suitable for VPR benchmarking and other tasks
common in Localization, such as image registration and visual odometry. To the
author's knowledge, this is the largest real-world aerial-vehicle dataset of
this kind. Our dataset is available at https://github.com/MetaSLAM/ALTO.",https://github.com/MetaSLAM/ALTO,-1
65897303-c417-48f0-9025-8bb592d25edb,Deep Feature Rotation for Multimodal Image Style Transfer,0.124339,"Recently, style transfer is a research area that attracts a lot of attention,
which transfers the style of an image onto a content target. Extensive research
on style transfer has aimed at speeding up processing or generating
high-quality stylized images. Most approaches only produce an output from a
content and style image pair, while a few others use complex architectures and
can only produce a certain number of outputs. In this paper, we propose a
simple method for representing style features in many ways called Deep Feature
Rotation (DFR), while not only producing diverse outputs but also still
achieving effective stylization compared to more complex methods. Our approach
is representative of the many ways of augmentation for intermediate feature
embedding without consuming too much computational expense. We also analyze our
method by visualizing output in different rotation weights. Our code is
available at https://github.com/sonnguyen129/deep-feature-rotation.",None,-1
603203f6-ccc1-4945-bdfb-6d21ecc3d272,Continual VQA for Disaster Response Systems,0.0580357,"Visual Question Answering (VQA) is a multi-modal task that involves answering
questions from an input image, semantically understanding the contents of the
image and answering it in natural language. Using VQA for disaster management
is an important line of research due to the scope of problems that are answered
by the VQA system. However, the main challenge is the delay caused by the
generation of labels in the assessment of the affected areas. To tackle this,
we deployed pre-trained CLIP model, which is trained on visual-image pairs.
however, we empirically see that the model has poor zero-shot performance.
Thus, we instead use pre-trained embeddings of text and image from this model
for our supervised training and surpass previous state-of-the-art results on
the FloodNet dataset. We expand this to a continual setting, which is a more
real-life scenario. We tackle the problem of catastrophic forgetting using
various experience replay methods. Our training runs are available at:
https://wandb.ai/compyle/continual_vqa_final. Our code is available at
https://github.com/AdityaKane2001/continual_vqa.",None,-1
502f1f30-ed03-4005-849b-dc03d8d9caca,Provable Benefits of Representational Transfer in Reinforcement Learning,0.441008,"We study the problem of representational transfer in RL, where an agent first
pretrains in a number of source tasks to discover a shared representation,
which is subsequently used to learn a good policy in a \emph{target task}. We
propose a new notion of task relatedness between source and target tasks, and
develop a novel approach for representational transfer under this assumption.
Concretely, we show that given generative access to source tasks, we can
discover a representation, using which subsequent linear RL techniques quickly
converge to a near-optimal policy in the target task.
  The sample complexity is close to knowing the ground truth features in the
target task, and comparable to prior representation learning results in the
source tasks. We complement our positive results with lower bounds without
generative access, and validate our findings with empirical evaluation on rich
observation MDPs that require deep exploration. In our experiments, we observe
a speed up in learning in the target by pre-training, and also validate the
need for generative access in source tasks.",None,-1
3f3640bd-dd94-48da-8d0b-9a4dd1b1dadb,NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages,0.935154,"Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages.",https://github.com/andria009/IndonesianSentimentLexicon,32819
0e0aab0f-1f25-4ed0-b413-1be201b4f18d,Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation,0.662052,"Target-guided response generation enables dialogue systems to smoothly
transition a conversation from a dialogue context toward a target sentence.
Such control is useful for designing dialogue systems that direct a
conversation toward specific goals, such as creating non-obtrusive
recommendations or introducing new topics in the conversation. In this paper,
we introduce a new technique for target-guided response generation, which first
finds a bridging path of commonsense knowledge concepts between the source and
the target, and then uses the identified bridging path to generate transition
responses. Additionally, we propose techniques to re-purpose existing dialogue
datasets for target-guided generation. Experiments reveal that the proposed
techniques outperform various baselines on this task. Finally, we observe that
the existing automated metrics for this task correlate poorly with human
judgement ratings. We propose a novel evaluation metric that we demonstrate is
more reliable for target-guided response evaluation. Our work generally enables
dialogue system designers to exercise more control over the conversations that
their systems produce.",https://github.com/prakhargupt/target-guided-dialogue-coda,-1
1c17e97c-a65b-48a5-bd92-b08741415b51,Ditto: Building Digital Twins of Articulated Objects from Interaction,0.930759,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",https://ut-austin-rpl.github.io/Ditto/,18716
78aa65eb-b5a1-4ead-9726-32c7971a02ff,"Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",0.369857,"The energy requirements of current natural language processing models
continue to grow at a rapid, unsustainable pace. Recent works highlighting this
problem conclude there is an urgent need for methods that reduce the energy
needs of NLP and machine learning more broadly. In this article, we investigate
techniques that can be used to reduce the energy consumption of common NLP
applications. In particular, we focus on techniques to measure energy usage and
different hardware and datacenter-oriented settings that can be tuned to reduce
energy consumption for training and inference for language models. We
characterize the impact of these settings on metrics such as computational
performance and energy consumption through experiments conducted on a high
performance computing system as well as popular cloud computing platforms.
These techniques can lead to significant reduction in energy consumption when
training language models or their use for inference. For example,
power-capping, which limits the maximum power a GPU can consume, can enable a
15\% decrease in energy usage with marginal increase in overall computation
time when training a transformer-based language model.",https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py,-1
4f0e9897-b6ce-4306-96be-0b3c2cbe59f9,RELIC: Retrieving Evidence for Literary Claims,0.768384,"Humanities scholars commonly provide evidence for claims that they make about
a work of literature (e.g., a novel) in the form of quotations from the work.
We collect a large-scale dataset (RELiC) of 78K literary quotations and
surrounding critical analysis and use it to formulate the novel task of
literary evidence retrieval, in which models are given an excerpt of literary
analysis surrounding a masked quotation and asked to retrieve the quoted
passage from the set of all passages in the work. Solving this retrieval task
requires a deep understanding of complex literary and linguistic phenomena,
which proves challenging to methods that overwhelmingly rely on lexical and
semantic similarity matching. We implement a RoBERTa-based dense passage
retriever for this task that outperforms existing pretrained information
retrieval baselines; however, experiments and analysis by human domain experts
indicate that there is substantial room for improvement over our dense
retriever.",None,-1
5ef2d88c-0a15-49bc-b5fd-4c586714e51a,On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning,0.312833,"Occlusion caused by vegetation is an essential problem for remote sensing
applications in areas, such as search and rescue, wildfire detection, wildlife
observation, surveillance, border control, and others. Airborne Optical
Sectioning (AOS) is an optical, wavelength-independent synthetic aperture
imaging technique that supports computational occlusion removal in real-time.
It can be applied with manned or unmanned aircrafts, such as drones. In this
article, we demonstrate a relationship between forest density and field of view
(FOV) of applied imaging systems. This finding was made with the help of a
simulated procedural forest model which offers the consideration of more
realistic occlusion properties than our previous statistical model. While AOS
has been explored with automatic and autonomous research prototypes in the
past, we present a free AOS integration for DJI systems. It enables bluelight
organizations and others to use and explore AOS with compatible, manually
operated, off-the-shelf drones. The (digitally cropped) default FOV for this
implementation was chosen based on our new finding.",https://github.com/tensorware/aos-simulation,-1
48975078-7932-43e5-9454-7bf6466b0230,Federated Learning with Position-Aware Neurons,0.657481,"Federated Learning (FL) fuses collaborative models from local nodes without
centralizing users' data. The permutation invariance property of neural
networks and the non-i.i.d. data across clients make the locally updated
parameters imprecisely aligned, disabling the coordinate-based parameter
averaging. Traditional neurons do not explicitly consider position information.
Hence, we propose Position-Aware Neurons (PANs) as an alternative, fusing
position-related values (i.e., position encodings) into neuron outputs. PANs
couple themselves to their positions and minimize the possibility of
dislocation, even updating on heterogeneous data. We turn on/off PANs to
disable/enable the permutation invariance property of neural networks. PANs are
tightly coupled with positions when applied to FL, making parameters across
clients pre-aligned and facilitating coordinate-based parameter averaging. PANs
are algorithm-agnostic and could universally improve existing FL algorithms.
Furthermore, ""FL with PANs"" is simple to implement and computationally
friendly.",https://github.com/IBM/FedMA,-1
4c2a73f9-486a-47f6-8342-2ea0c048a54f,Extreme Masking for Learning Instance and Distributed Visual Representations,0.524127,"The paper presents a scalable approach for learning spatially distributed
visual representations over individual tokens and a holistic instance
representation simultaneously. We use self-attention blocks to represent
spatially distributed tokens, followed by cross-attention blocks to aggregate
the holistic image instance. The core of the approach is the use of extremely
large token masking (75\%-90\%) as the data augmentation for supervision. Our
model, named ExtreMA, follows the plain BYOL approach where the instance
representation from the unmasked subset is trained to predict that from the
intact input. Instead of encouraging invariance across inputs, the model is
required to capture informative variations in an image. The paper makes three
contributions: 1) It presents random masking as a strong and computationally
efficient data augmentation for siamese representation learning. 2) With
multiple sampling per instance, extreme masking greatly speeds up learning and
improves performance with more data. 3) ExtreMA obtains stronger linear probing
performance than masked modeling methods, and better transfer performance than
prior contrastive models.",None,-1
18bb3ab3-d9e6-4f55-bf72-6e930ad427da,Interpretable Molecular Graph Generation via Monotonic Constraints,0.626729,"Designing molecules with specific properties is a long-lasting research
problem and is central to advancing crucial domains such as drug discovery and
material science. Recent advances in deep graph generative models treat
molecule design as graph generation problems which provide new opportunities
toward the breakthrough of this long-lasting problem. Existing models, however,
have many shortcomings, including poor interpretability and controllability
toward desired molecular properties. This paper focuses on new methodologies
for molecule generation with interpretable and controllable deep generative
models, by proposing new monotonically-regularized graph variational
autoencoders. The proposed models learn to represent the molecules with latent
variables and then learn the correspondence between them and molecule
properties parameterized by polynomial functions. To further improve the
intepretability and controllability of molecule generation towards desired
properties, we derive new objectives which further enforce monotonicity of the
relation between some latent variables and target molecule properties such as
toxicity and clogP. Extensive experimental evaluation demonstrates the
superiority of the proposed framework on accuracy, novelty, disentanglement,
and control towards desired molecular properties. The code is open-source at
https://anonymous.4open.science/r/MDVAE-FD2C.",None,-1
df860de0-870c-4f43-9b00-a33699d25380,Relighting4D: Neural Relightable Human from Videos,0.559845,"Human relighting is a highly desirable yet challenging task. Existing works
either require expensive one-light-at-a-time (OLAT) captured data using light
stage or cannot freely change the viewpoints of the rendered body. In this
work, we propose a principled framework, Relighting4D, that enables
free-viewpoints relighting from only human videos under unknown illuminations.
Our key insight is that the space-time varying geometry and reflectance of the
human body can be decomposed as a set of neural fields of normal, occlusion,
diffuse, and specular maps. These neural fields are further integrated into
reflectance-aware physically based rendering, where each vertex in the neural
field absorbs and reflects the light from the environment. The whole framework
can be learned from videos in a self-supervised manner, with physically
informed priors designed for regularization. Extensive experiments on both real
and synthetic datasets demonstrate that our framework is capable of relighting
dynamic human actors with free-viewpoints.",None,-1
382554ca-51f2-4325-ba7e-850c55488aac,Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations,0.102238,"Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into multi-view compressed representations before
feeding them into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.",https://github.com/DAMO-NLP-SG/MVCR,11730
c12c0820-690b-41e2-ac7c-4b7a257f2793,GeoFill: Reference-Based Image Inpainting with Better Geometric Understanding,0.119934,"Reference-guided image inpainting restores image pixels by leveraging the
content from another single reference image. The primary challenge is how to
precisely place the pixels from the reference image into the hole region.
Therefore, understanding the 3D geometry that relates pixels between two views
is a crucial step towards building a better model. Given the complexity of
handling various types of reference images, we focus on the scenario where the
images are captured by freely moving the same camera around. Compared to the
previous work, we propose a principled approach that does not make heuristic
assumptions about the planarity of the scene. We leverage a monocular depth
estimate and predict relative pose between cameras, then align the reference
image to the target by a differentiable 3D reprojection and a joint
optimization of relative pose and depth map scale and offset. Our approach
achieves state-of-the-art performance on both RealEstate10K and
MannequinChallenge dataset with large baselines, complex geometry and extreme
camera motions. We experimentally verify our approach is also better at
handling large holes.",None,-1
a2db1e46-ac2c-4bfb-9ac2-925a9da89b3e,Towards Privacy-Preserving Person Re-identification via Person Identify Shift,0.505474,"Recently privacy concerns of person re-identification (ReID) raise more and
more attention and preserving the privacy of the pedestrian images used by ReID
methods become essential. De-identification (DeID) methods alleviate privacy
issues by removing the identity-related of the ReID data. However, most of the
existing DeID methods tend to remove all personal identity-related information
and compromise the usability of de-identified data on the ReID task. In this
paper, we aim to develop a technique that can achieve a good trade-off between
privacy protection and data usability for person ReID. To achieve this, we
propose a novel de-identification method designed explicitly for person ReID,
named Person Identify Shift (PIS). PIS removes the absolute identity in a
pedestrian image while preserving the identity relationship between image
pairs. By exploiting the interpolation property of variational auto-encoder,
PIS shifts each pedestrian image from the current identity to another with a
new identity, resulting in images still preserving the relative identities.
Experimental results show that our method has a better trade-off between
privacy-preserving and model performance than existing de-identification
methods and can defend against human and model attacks for data privacy.",None,-1
a77e94cf-ddc1-4f64-aab0-2c46fa485c62,CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,0.40863,"Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed
inputs during training -- helps reduce model reliance on spurious correlations
and improves generalization to out-of-distribution (OOD) data. Prior work on
generating counterfactuals only considered restricted classes of perturbations,
limiting their effectiveness. We present COunterfactual Generation via
Retrieval and Editing (CORE), a retrieval-augmented generation framework for
creating diverse counterfactual perturbations for CDA. For each training
example, CORE first performs a dense retrieval over a task-related unlabeled
text corpus using a learned bi-encoder and extracts relevant counterfactual
excerpts. CORE then incorporates these into prompts to a large language model
with few-shot learning capabilities, for counterfactual editing. Conditioning
language model edits on naturally occurring data results in diverse
perturbations. Experiments on natural language inference and sentiment analysis
benchmarks show that CORE counterfactuals are more effective at improving
generalization to OOD data compared to other DA approaches. We also show that
the CORE retrieval framework can be used to encourage diversity in manually
authored perturbations",https://github.com/tanay2001/CORE,-1
954af74c-2d8a-42d8-8345-5f506187fc75,Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs,0.529341,"Question answering over temporal knowledge graphs (KGs) efficiently uses
facts contained in a temporal KG, which records entity relations and when they
occur in time, to answer natural language questions (e.g., ""Who was the
president of the US before Obama?""). These questions often involve three
time-related challenges that previous work fail to adequately address: 1)
questions often do not specify exact timestamps of interest (e.g., ""Obama""
instead of 2000); 2) subtle lexical differences in time relations (e.g.,
""before"" vs ""after""); 3) off-the-shelf temporal KG embeddings that previous
work builds on ignore the temporal order of timestamps, which is crucial for
answering temporal-order related questions. In this paper, we propose a
time-sensitive question answering (TSQA) framework to tackle these problems.
TSQA features a timestamp estimation module to infer the unwritten timestamp
from the question. We also employ a time-sensitive KG encoder to inject
ordering information into the temporal KG embeddings that TSQA is based on.
With the help of techniques to reduce the search space for potential answers,
TSQA significantly outperforms the previous state of the art on a new benchmark
for question answering over temporal KGs, especially achieving a 32% (absolute)
error reduction on complex questions that require multiple steps of reasoning
over facts in the temporal KG.",https://github.com/apoorvumang/CronKGQA,-1
b98b5da4-03db-4a40-8992-93590526bf3b,BigColor: Colorization using a Generative Color Prior for Natural Images,0.820194,"For realistic and vivid colorization, generative priors have recently been
exploited. However, such generative priors often fail for in-the-wild complex
images due to their limited representation space. In this paper, we propose
BigColor, a novel colorization approach that provides vivid colorization for
diverse in-the-wild images with complex structures. While previous generative
priors are trained to synthesize both image structures and colors, we learn a
generative color prior to focus on color synthesis given the spatial structure
of an image. In this way, we reduce the burden of synthesizing image structures
from the generative prior and expand its representation space to cover diverse
images. To this end, we propose a BigGAN-inspired encoder-generator network
that uses a spatial feature map instead of a spatially-flattened BigGAN latent
code, resulting in an enlarged representation space. Our method enables robust
colorization for diverse inputs in a single forward pass, supports arbitrary
input resolutions, and provides multi-modal colorization results. We
demonstrate that BigColor significantly outperforms existing methods especially
on in-the-wild images with complex structures.",https://github.com/jantic/DeOldify,-1
256b1027-601f-496b-86e5-e39895a4a29e,Exploration with Multi-Sample Target Values for Distributional Reinforcement Learning,0.0344912,"Distributional reinforcement learning (RL) aims to learn a value-network that
predicts the full distribution of the returns for a given state, often modeled
via a quantile-based critic. This approach has been successfully integrated
into common RL methods for continuous control, giving rise to algorithms such
as Distributional Soft Actor-Critic (DSAC). In this paper, we introduce
multi-sample target values (MTV) for distributional RL, as a principled
replacement for single-sample target value estimation, as commonly employed in
current practice. The improved distributional estimates further lend themselves
to UCB-based exploration. These two ideas are combined to yield our
distributional RL algorithm, E2DC (Extra Exploration with Distributional
Critics). We evaluate our approach on a range of continuous control tasks and
demonstrate state-of-the-art model-free performance on difficult tasks such as
Humanoid control. We provide further insight into the method via visualization
and analysis of the learned distributions and their evolution during training.",https://github.com/vitchyr/rlkit,-1
6876bbce-5274-4fad-94fc-18378b1ecf3c,Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee,0.619791,"The objective of this study is to address the problem of
background/foreground separation with missing pixels by combining the video
acquisition, video recovery, background/foreground separation into a single
framework. To achieve this, a smooth robust tensor completion (SRTC) model is
proposed to recover the data and decompose it into the static background and
smooth foreground, respectively. Specifically, the static background is modeled
by the low-rank tucker decomposition and the smooth foreground (moving objects)
is modeled by the spatiotemporal continuity, which is enforced by the total
variation regularization. An efficient algorithm based on tensor proximal
alternating minimization (tenPAM) is implemented to solve the proposed model
with global convergence guarantee under very mild conditions. Extensive
experiments on real data demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches for background/foreground
separation with missing pixels.",https://github.com/ZihengLi6321/MCOS,-1
23526894-5c3f-4637-ace1-8e6938ee8f32,Reinforced Lin-Kernighan-Helsgaun Algorithms for the Traveling Salesman Problems,0.25658,"TSP is a classical NP-hard combinatorial optimization problem with many
practical variants. LKH is one of the state-of-the-art local search algorithms
for the TSP. LKH-3 is a powerful extension of LKH that can solve many TSP
variants. Both LKH and LKH-3 associate a candidate set to each city to improve
the efficiency, and have two different methods, $\alpha$-measure and POPMUSIC,
to decide the candidate sets. In this work, we first propose a Variable
Strategy Reinforced LKH (VSR-LKH) algorithm, which incorporates three
reinforcement learning methods (Q-learning, Sarsa, Monte Carlo) with LKH, for
the TSP. We further propose a new algorithm called VSR-LKH-3 that combines the
variable strategy reinforcement learning method with LKH-3 for typical TSP
variants, including the TSP with time windows (TSPTW) and Colored TSP (CTSP).
The proposed algorithms replace the inflexible traversal operations in LKH and
LKH-3 and let the algorithms learn to make a choice at each search step by
reinforcement learning. Both LKH and LKH-3, with either $\alpha$-measure or
POPMUSIC, can be significantly improved by our methods. Extensive experiments
on 236 widely-used TSP benchmarks with up to 85,900 cities demonstrate the
excellent performance of VSR-LKH. VSR-LKH-3 also significantly outperforms the
state-of-the-art heuristics for TSPTW and CTSP.",https://github.com/JHL-HUST/VSR-LKH-V2,-1
ae518cfb-872c-41dd-a76f-bce49fc42534,A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking,0.0905944,"Dialogue state tracking (DST) module is an important component for
task-oriented dialog systems to understand users' goals and needs. Collecting
dialogue state labels including slots and values can be costly, especially with
the wide application of dialogue systems in more and more new-rising domains.
In this paper, we focus on how to utilize the language understanding and
generation ability of pre-trained language models for DST. We design a dual
prompt learning framework for few-shot DST. Specifically, we consider the
learning of slot generation and value generation as dual tasks, and two prompts
are designed based on such a dual structure to incorporate task-related
knowledge of these two tasks respectively. In this way, the DST task can be
formulated as a language modeling task efficiently under few-shot settings.
Experimental results on two task-oriented dialogue datasets show that the
proposed method not only outperforms existing state-of-the-art few-shot
methods, but also can generate unseen slots. It indicates that DST-related
knowledge can be probed from PLM and utilized to address low-resource DST
efficiently with the help of prompt learning.",None,-1
4cab9935-bc3e-4ddc-99c1-b006c46c0e35,Adapting BigScience Multilingual Model to Unseen Languages,0.2418,"We benchmark different strategies of adding new languages (German and Korean)
into the BigScience's pretrained multilingual language model with 1.3 billion
parameters that currently supports 13 languages. We investigate the factors
that affect the language adaptability of the model and the trade-offs between
computational costs and expected performance.",https://huggingface.co/bigscience/tr5b-1B3-multilingual-alpha-checkpoints/tree/main,-1
0d2520db-67e3-44c7-b570-5e5ce90aa648,Towards Self-Supervised Category-Level Object Pose and Size Estimation,0.532729,"In this work, we tackle the challenging problem of category-level object pose
and size estimation from a single depth image. Although previous
fully-supervised works have demonstrated promising performance, collecting
ground-truth pose labels is generally time-consuming and labor-intensive.
Instead, we propose a label-free method that learns to enforce the geometric
consistency between category template mesh and observed object point cloud
under a self-supervision manner. Specifically, our method consists of three key
components: differentiable shape deformation, registration, and rendering. In
particular, shape deformation and registration are applied to the template mesh
to eliminate the differences in shape, pose and scale. A differentiable
renderer is then deployed to enforce geometric consistency between point clouds
lifted from the rendered depth and the observed scene for self-supervision. We
evaluate our approach on real-world datasets and find that our approach
outperforms the simple traditional baseline by large margins while being
competitive with some fully-supervised approaches.",https://github.com/mentian/object-deformnet,-1
98b59aa0-dc5f-450a-b69c-e9cc43f1304e,Automated Identification of Eviction Status from Electronic Health Record Notes,0.928906,"Objective: Evictions are important social and behavioral determinants of
health. Evictions are associated with a cascade of negative events that can
lead to unemployment, housing insecurity/homelessness, long-term poverty, and
mental health problems. In this study, we developed a natural language
processing system to automatically detect eviction status from electronic
health record (EHR) notes.
  Materials and Methods: We first defined eviction status (eviction presence
and eviction period) and then annotated eviction status in 5000 EHR notes from
the Veterans Health Administration (VHA). We developed a novel model, KIRESH,
that has shown to substantially outperform other state-of-the-art models such
as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.
Moreover, we designed a novel prompt to further improve the model performance
by using the intrinsic connection between the two sub-tasks of eviction
presence and period prediction. Finally, we used the Temperature Scaling-based
Calibration on our KIRESH-Prompt method to avoid over-confidence issues arising
from the imbalance dataset.
  Results: KIRESH-Prompt substantially outperformed strong baseline models
including fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153
Macro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,
0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also
conducted additional experiments on a benchmark social determinants of health
(SBDH) dataset to demonstrate the generalizability of our methods.
  Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction
status classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an
eviction surveillance system to help address the US Veterans' housing
insecurity.",https://github.com/hibaahsan/MIMIC-SBDH,-1
a1bc6326-9db3-45d0-a711-920905843f28,NewsStories: Illustrating articles with visual summaries,0.522404,"Recent self-supervised approaches have used large-scale image-text datasets
to learn powerful representations that transfer to many tasks without
finetuning. These methods often assume that there is one-to-one correspondence
between its images and their (short) captions. However, many tasks require
reasoning about multiple images and long text narratives, such as describing
news articles with visual summaries. Thus, we explore a novel setting where the
goal is to learn a self-supervised visual-language representation that is
robust to varying text length and the number of images. In addition, unlike
prior work which assumed captions have a literal relation to the image, we
assume images only contain loose illustrative correspondence with the text. To
explore this problem, we introduce a large-scale multimodal dataset containing
over 31M articles, 22M images and 1M videos. We show that state-of-the-art
image-text alignment methods are not robust to longer narratives with multiple
images. Finally, we introduce an intuitive baseline that outperforms these
methods on zero-shot image-set retrieval by 10% on the GoodNews dataset.",None,-1
1c1f8232-13fb-4146-96cb-e17a1c4e4100,Ontologically Faithful Generation of Non-Player Character Dialogues,0.441543,"We introduce a language generation task grounded in a popular video game
environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)
requires models to produce trees of dialogue between video game characters that
accurately reflect quest and entity specifications stated in natural language.
KNUDGE is constructed from side quest dialogues drawn directly from game data
of Obsidian Entertainment's The Outer Worlds, leading to real-world
complexities in generation: (1) dialogues are branching trees as opposed to
linear chains of utterances; (2) utterances must remain faithful to the game
lore -- character personas, backstories, and entity relationships; and (3) a
dialogue must accurately reveal new quest details to the human player. We
report results for a set of neural generation models using supervised and
in-context learning techniques; we find competent performance but room for
future work addressing the challenges of creating realistic, game-quality
dialogues.",https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py,-1
7d882d11-f093-4558-843c-ad6154619724,DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,0.856209,"Language-guided Embodied AI benchmarks requiring an agent to navigate an
environment and manipulate objects typically allow one-way communication: the
human user gives a natural language command to the agent, and the agent can
only follow the command passively. We present DialFRED, a dialogue-enabled
embodied instruction following benchmark based on the ALFRED benchmark.
DialFRED allows an agent to actively ask questions to the human user; the
additional information in the user's response is used by the agent to better
complete its task. We release a human-annotated dataset with 53K task-relevant
questions and answers and an oracle to answer questions. To solve DialFRED, we
propose a questioner-performer framework wherein the questioner is pre-trained
with the human-annotated data and fine-tuned with reinforcement learning. We
make DialFRED publicly available and encourage researchers to propose and
evaluate their solutions to building dialog-enabled embodied agents.",https://github.com/xfgao/DialFRED,-1
2f2d0c80-58a4-429b-9de3-ea895c5812ed,OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering,0.819046,"The information in tables can be an important complement to text, making
table-based question answering (QA) systems of great value. The intrinsic
complexity of handling tables often adds an extra burden to both model design
and data annotation. In this paper, we aim to develop a simple table-based QA
model with minimal annotation effort. Motivated by the fact that table-based QA
requires both alignment between questions and tables and the ability to perform
complicated reasoning over multiple table elements, we propose an omnivorous
pretraining approach that consumes both natural and synthetic data to endow
models with these respective abilities. Specifically, given freely available
tables, we leverage retrieval to pair them with relevant natural sentences for
mask-based pretraining, and synthesize NL questions by converting SQL sampled
from tables for pretraining with a QA loss. We perform extensive experiments in
both few-shot and full settings, and the results clearly demonstrate the
superiority of our model OmniTab, with the best multitasking approach achieving
an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively,
also establishing a new state-of-the-art on WikiTableQuestions. Detailed
ablations and analyses reveal different characteristics of natural and
synthetic data, shedding light on future directions in omnivorous pretraining.
Code, pretraining data, and pretrained models are available at
https://github.com/jzbjyb/OmniTab.",https://github.com/jzbjyb/OmniTab,-1
d5282d22-eed0-4c3c-9ddc-b6d4748b0c02,CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding,0.18199,"Task-oriented personal assistants enable people to interact with a host of
devices and services using natural language. One of the challenges of making
neural dialogue systems available to more users is the lack of training data
for all but a few languages. Zero-shot methods try to solve this issue by
acquiring task knowledge in a high-resource language such as English with the
aim of transferring it to the low-resource language(s). To this end, we
introduce CrossAligner, the principal method of a variety of effective
approaches for zero-shot cross-lingual transfer based on learning alignment
from unlabelled parallel data. We present a quantitative analysis of individual
methods as well as their weighted combinations, several of which exceed
state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test
sets and three benchmark multilingual datasets. A detailed qualitative error
analysis of the best methods shows that our fine-tuned language models can
zero-shot transfer the task knowledge better than anticipated.",https://github.com/huawei-noah/noah-research,-1
552bf9be-eb44-420f-ab60-94dcaa49cb8d,The future is different: Large pre-trained language models fail in prediction tasks,0.0538775,"Large pre-trained language models (LPLM) have shown spectacular success when
fine-tuned on downstream supervised tasks. Yet, it is known that their
performance can drastically drop when there is a distribution shift between the
data used during training and that used at inference time. In this paper we
focus on data distributions that naturally change over time and introduce four
new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and
POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display
average performance drops of about 88% (in the best case!) when predicting the
popularity of future posts from sub-reddits whose topic distribution changes
with time. We then introduce a simple methodology that leverages neural
variational dynamic topic models and attention mechanisms to infer temporal
language model representations for regression tasks. Our models display
performance drops of only about 40% in the worst cases (2% in the best ones)
when predicting the popularity of future posts, while using only about 7% of
the total number of parameters of LPLM and providing interpretable
representations that offer insight into real-world events, like the GameStop
short squeeze of 2021",None,-1
580034de-5650-48dc-a2e2-f6880a2e723b,Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction,0.758732,"Recently, prompt-tuning has attracted growing interests in event argument
extraction (EAE). However, the existing prompt-tuning methods have not achieved
satisfactory performance due to the lack of consideration of entity
information. In this paper, we propose a bi-directional iterative prompt-tuning
method for EAE, where the EAE task is treated as a cloze-style task to take
full advantage of entity information and pre-trained language models (PLMs).
Furthermore, our method explores event argument interactions by introducing the
argument roles of contextual entities into prompt construction. Since template
and verbalizer are two crucial components in a cloze-style prompt, we propose
to utilize the role label semantic knowledge to construct a semantic verbalizer
and design three kinds of templates for the EAE task. Experiments on the ACE
2005 English dataset with standard and low-resource settings show that the
proposed method significantly outperforms the peer state-of-the-art methods.
Our code is available at https://github.com/HustMinsLab/BIP.",https://github.com/HustMinsLab/BIP,-1
1d193ead-8453-47e6-8aeb-bafec4e58d7c,Zero-Shot Retrieval with Search Agents and Hybrid Environments,0.667085,"Learning to search is the task of building artificial agents that learn to
autonomously use a search box to find information. So far, it has been shown
that current language models can learn symbolic query reformulation policies,
in combination with traditional term-based retrieval, but fall short of
outperforming neural retrievers. We extend the previous learning to search
setup to a hybrid environment, which accepts discrete query refinement
operations, after a first-pass retrieval step via a dual encoder. Experiments
on the BEIR task show that search agents, trained via behavioral cloning,
outperform the underlying search system based on a combined dual encoder
retriever and cross encoder reranker. Furthermore, we find that simple
heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance
by several nDCG points. The search agent based on HRE (HARE) matches
state-of-the-art performance, balanced in both zero-shot and in-domain
evaluations, via interpretable actions, and at twice the speed.",None,2397
af87a928-bf44-45c3-8c01-ce2cebe6f6a4,Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences,0.635942,"Generating complex behaviors that satisfy the preferences of non-expert users
is a crucial requirement for AI agents. Interactive reward learning from
trajectory comparisons (a.k.a. RLHF) is one way to allow non-expert users to
convey complex objectives by expressing preferences over short clips of agent
behaviors. Even though this parametric method can encode complex tacit
knowledge present in the underlying tasks, it implicitly assumes that the human
is unable to provide richer feedback than binary preference labels, leading to
intolerably high feedback complexity and poor user experience. While providing
a detailed symbolic closed-form specification of the objectives might be
tempting, it is not always feasible even for an expert user. However, in most
cases, humans are aware of how the agent should change its behavior along
meaningful axes to fulfill their underlying purpose, even if they are not able
to fully specify task objectives symbolically. Using this as motivation, we
introduce the notion of Relative Behavioral Attributes, which allows the users
to tweak the agent behavior through symbolic concepts (e.g., increasing the
softness or speed of agents' movement). We propose two practical methods that
can learn to model any kind of behavioral attributes from ordered behavior
clips. We demonstrate the effectiveness of our methods on four tasks with nine
different behavioral attributes, showing that once the attributes are learned,
end users can produce desirable agent behaviors relatively effortlessly, by
providing feedback just around ten times. This is over an order of magnitude
less than that required by the popular learning-from-human-preferences
baselines. The supplementary video and source code are available at:
https://guansuns.github.io/pages/rba.",None,-1
35d672ba-e53f-48f5-864a-59acbd8932f5,Check-worthy Claim Detection across Topics for Automated Fact-checking,0.322943,"An important component of an automated fact-checking system is the claim
check-worthiness detection system, which ranks sentences by prioritising them
based on their need to be checked. Despite a body of research tackling the
task, previous research has overlooked the challenging nature of identifying
check-worthy claims across different topics. In this paper, we assess and
quantify the challenge of detecting check-worthy claims for new, unseen topics.
After highlighting the problem, we propose the AraCWA model to mitigate the
performance deterioration when detecting check-worthy claims across topics. The
AraCWA model enables boosting the performance for new topics by incorporating
two components for few-shot learning and data augmentation. Using a publicly
available dataset of Arabic tweets consisting of 14 different topics, we
demonstrate that our proposed data augmentation strategy achieves substantial
improvements across topics overall, where the extent of the improvement varies
across topics. Further, we analyse the semantic similarities between topics,
suggesting that the similarity metric could be used as a proxy to determine the
difficulty level of an unseen topic prior to undertaking the task of labelling
the underlying sentences.",None,7490
3fcff545-8435-403c-80ff-e7ab761c35e5,Depth-aware Neural Style Transfer using Instance Normalization,0.454876,"Neural Style Transfer (NST) is concerned with the artistic stylization of
visual media. It can be described as the process of transferring the style of
an artistic image onto an ordinary photograph. Recently, a number of studies
have considered the enhancement of the depth-preserving capabilities of the NST
algorithms to address the undesired effects that occur when the input content
images include numerous objects at various depths. Our approach uses a deep
residual convolutional network with instance normalization layers that utilizes
an advanced depth prediction network to integrate depth preservation as an
additional loss function to content and style. We demonstrate results that are
effective in retaining the depth and global structure of content images. Three
different evaluation processes show that our system is capable of preserving
the structure of the stylized results while exhibiting style-capture
capabilities and aesthetic qualities comparable or superior to state-of-the-art
methods. Project page:
https://ioannoue.github.io/depth-aware-nst-using-in.html.",https://ioannoue.github.io/depth-aware-nst-using-in.html,97
a38a5781-3be8-47c9-b2bb-f97ca900011f,Semantically Proportional Patchmix for Few-Shot Learning,0.0229563,"Few-shot learning aims to classify unseen classes with only a limited number
of labeled data. Recent works have demonstrated that training models with a
simple transfer learning strategy can achieve competitive results in few-shot
classification. Although excelling at distinguishing training data, these
models are not well generalized to unseen data, probably due to insufficient
feature representations on evaluation. To tackle this issue, we propose
Semantically Proportional Patchmix (SePPMix), in which patches are cut and
pasted among training images and the ground truth labels are mixed
proportionally to the semantic information of the patches. In this way, we can
improve the generalization ability of the model by regional dropout effect
without introducing severe label noise. To learn more robust representations of
data, we further take rotate transformation on the mixed images and predict
rotations as a rule-based regularizer. Extensive experiments on prevalent
few-shot benchmarks have shown the effectiveness of our proposed method.",None,-1
1ee5ffde-dd8c-4fb3-843b-77ee06ab924d,Supervised Machine Learning for Effective Missile Launch Based on Beyond Visual Range Air Combat Simulations,0.938358,"This work compares supervised machine learning methods using reliable data
from constructive simulations to estimate the most effective moment for
launching missiles during air combat. We employed resampling techniques to
improve the predictive model, analyzing accuracy, precision, recall, and
f1-score. Indeed, we could identify the remarkable performance of the models
based on decision trees and the significant sensitivity of other algorithms to
resampling techniques. The models with the best f1-score brought values of
0.379 and 0.465 without and with the resampling technique, respectively, which
is an increase of 22.69%. Thus, if desirable, resampling techniques can improve
the model's recall and f1-score with a slight decline in accuracy and
precision. Therefore, through data obtained through constructive simulations,
it is possible to develop decision support tools based on machine learning
models, which may improve the flight quality in BVR air combat, increasing the
effectiveness of offensive missions to hit a particular target.",https://github.com/jpadantas/effective-missile-launch,-1
9379c13d-a345-4ce5-9450-7d0016c62406,CenterFormer: Center-based Transformer for 3D Object Detection,0.850695,"Query-based transformer has shown great potential in constructing long-range
attention in many image-domain tasks, but has rarely been considered in
LiDAR-based 3D object detection due to the overwhelming size of the point cloud
data. In this paper, we propose CenterFormer, a center-based transformer
network for 3D object detection. CenterFormer first uses a center heatmap to
select center candidates on top of a standard voxel-based point cloud encoder.
It then uses the feature of the center candidate as the query embedding in the
transformer. To further aggregate features from multiple frames, we design an
approach to fuse features through cross-attention. Lastly, regression heads are
added to predict the bounding box on the output center feature representation.
Our design reduces the convergence difficulty and computational complexity of
the transformer structure. The results show significant improvements over the
strong baseline of anchor-free object detection networks. CenterFormer achieves
state-of-the-art performance for a single model on the Waymo Open Dataset, with
73.7% mAPH on the validation set and 75.6% mAPH on the test set, significantly
outperforming all previously published CNN and transformer-based methods. Our
code is publicly available at https://github.com/TuSimple/centerformer",https://github.com/TuSimple/centerformer,-1
52bbf16c-086f-4702-9625-5dec01403f58,Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking,0.998752,"Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors. Recent researches start from the pretrained knowledge of language
models and take multimodal information into CSC models to improve the
performance. However, they overlook the rich knowledge in the dictionary, the
reference book where one can learn how one character should be pronounced,
written, and used. In this paper, we propose the LEAD framework, which renders
the CSC model to learn heterogeneous knowledge from the dictionary in terms of
phonetics, vision, and meaning. LEAD first constructs positive and negative
samples according to the knowledge of character phonetics, glyphs, and
definitions in the dictionary. Then a unified contrastive learning-based
training scheme is employed to refine the representations of the CSC models.
Extensive experiments and detailed analyses on the SIGHAN benchmark datasets
demonstrate the effectiveness of our proposed methods.",https://github.com/geekjuruo/LEAD,-1
b2ad9197-bb19-4fe3-879d-f8afd35d8848,"YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception",0.969665,"Over the last decade, multi-tasking learning approaches have achieved
promising results in solving panoptic driving perception problems, providing
both high-precision and high-efficiency performance. It has become a popular
paradigm when designing networks for real-time practical autonomous driving
system, where computation resources are limited. This paper proposed an
effective and efficient multi-task learning network to simultaneously perform
the task of traffic object detection, drivable road area segmentation and lane
detection. Our model achieved the new state-of-the-art (SOTA) performance in
terms of accuracy and speed on the challenging BDD100K dataset. Especially, the
inference time is reduced by half compared to the previous SOTA model. Code
will be released in the near future.",None,-1
525148e1-3418-457a-a0a4-94f69319dfe1,On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for Multimodal Sentiment Analysis,0.337562,"This paper investigates the effectiveness and implementation of
modality-specific large-scale pre-trained encoders for multimodal sentiment
analysis~(MSA). Although the effectiveness of pre-trained encoders in various
fields has been reported, conventional MSA methods employ them for only
linguistic modality, and their application has not been investigated. This
paper compares the features yielded by large-scale pre-trained encoders with
conventional heuristic features. One each of the largest pre-trained encoders
publicly available for each modality are used; CLIP-ViT, WavLM, and BERT for
visual, acoustic, and linguistic modalities, respectively. Experiments on two
datasets reveal that methods with domain-specific pre-trained encoders attain
better performance than those with conventional features in both unimodal and
multimodal scenarios. We also find it better to use the outputs of the
intermediate layers of the encoders than those of the output layer. The codes
are available at https://github.com/ando-hub/MSA_Pretrain.",https://github.com/ando-hub/MSA_Pretrain,-1
7f515285-0389-454a-8315-97d66c2dd888,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,1.0,"We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.",None,-1
7f8cddeb-07a1-4d70-964e-06e547f0bd0d,Assessing Digital Language Support on a Global Scale,0.551968,"The users of endangered languages struggle to thrive in a digitally-mediated
world. We have developed an automated method for assessing how well every
language recognized by ISO 639 is faring in terms of digital language support.
The assessment is based on scraping the names of supported languages from the
websites of 143 digital tools selected to represent a full range of ways that
digital technology can support languages. The method uses Mokken scale analysis
to produce an explainable model for quantifying digital language support and
monitoring it on a global scale.",https://github.com/sil-ai/dls-results,-1
df13d9ff-ef78-4c70-a9cc-f7491b97268c,Contextually Enhanced ES-dRNN with Dynamic Attention for Short-Term Load Forecasting,0.414567,"In this paper, we propose a new short-term load forecasting (STLF) model
based on contextually enhanced hybrid and hierarchical architecture combining
exponential smoothing (ES) and a recurrent neural network (RNN). The model is
composed of two simultaneously trained tracks: the context track and the main
track. The context track introduces additional information to the main track.
It is extracted from representative series and dynamically modulated to adjust
to the individual series forecasted by the main track. The RNN architecture
consists of multiple recurrent layers stacked with hierarchical dilations and
equipped with recently proposed attentive dilated recurrent cells. These cells
enable the model to capture short-term, long-term and seasonal dependencies
across time series as well as to weight dynamically the input information. The
model produces both point forecasts and predictive intervals. The experimental
part of the work performed on 35 forecasting problems shows that the proposed
model outperforms in terms of accuracy its predecessor as well as standard
statistical models and state-of-the-art machine learning models.",https://github.com/slaweks17/ES-adRNN-with-Context,-1
5e23df06-91d0-42a6-ad22-fd5c71646f15,Explainability in reinforcement learning: perspective and position,0.467401,"Artificial intelligence (AI) has been embedded into many aspects of people's
daily lives and it has become normal for people to have AI make decisions for
them. Reinforcement learning (RL) models increase the space of solvable
problems with respect to other machine learning paradigms. Some of the most
interesting applications are in situations with non-differentiable expected
reward function, operating in unknown or underdefined environment, as well as
for algorithmic discovery that surpasses performance of any teacher, whereby
agent learns from experimental experience through simple feedback. The range of
applications and their social impact is vast, just to name a few: genomics,
game-playing (chess, Go, etc.), general optimization, financial investment,
governmental policies, self-driving cars, recommendation systems, etc. It is
therefore essential to improve the trust and transparency of RL-based systems
through explanations. Most articles dealing with explainability in artificial
intelligence provide methods that concern supervised learning and there are
very few articles dealing with this in the area of RL. The reasons for this are
the credit assignment problem, delayed rewards, and the inability to assume
that data is independently and identically distributed (i.i.d.). This position
paper attempts to give a systematic overview of existing methods in the
explainable RL area and propose a novel unified taxonomy, building and
expanding on the existing ones. The position section describes pragmatic
aspects of how explainability can be observed. The gap between the parties
receiving and generating the explanation is especially emphasized. To reduce
the gap and achieve honesty and truthfulness of explanations, we set up three
pillars: proactivity, risk attitudes, and epistemological constraints. To this
end, we illustrate our proposal on simple variants of the shortest path
problem.",None,-1
4d25e921-ccbc-4f83-af8e-75e8f1960a95,Learning Discriminative Representations and Decision Boundaries for Open Intent Detection,0.328646,"Open intent detection is a significant problem in natural language
understanding, which aims to identify the unseen open intent while ensuring
known intent identification performance. However, current methods face two
major challenges. Firstly, they struggle to learn friendly representations to
detect the open intent with prior knowledge of only known intents. Secondly,
there is a lack of an effective approach to obtaining specific and compact
decision boundaries for known intents. To address these issues, this paper
presents an original framework called DA-ADB, which successively learns
distance-aware intent representations and adaptive decision boundaries for open
intent detection. Specifically, we first leverage distance information to
enhance the distinguishing capability of the intent representations. Then, we
design a novel loss function to obtain appropriate decision boundaries by
balancing both empirical and open space risks. Extensive experiments
demonstrate the effectiveness of the proposed distance-aware and boundary
learning strategies. Compared to state-of-the-art methods, our framework
achieves substantial improvements on three benchmark datasets. Furthermore, it
yields robust performance with varying proportions of labeled data and known
categories.",https://github.com/thuiar/TEXTOIR,-1
3c489e5a-78cb-4017-a005-598ff2a5199f,CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection,0.568435,"Video anomaly detection (VAD) -- commonly formulated as a multiple-instance
learning problem in a weakly-supervised manner due to its labor-intensive
nature -- is a challenging problem in video surveillance where the frames of
anomaly need to be localized in an untrimmed video. In this paper, we first
propose to utilize the ViT-encoded visual features from CLIP, in contrast with
the conventional C3D or I3D features in the domain, to efficiently extract
discriminative representations in the novel technique. We then model temporal
dependencies and nominate the snippets of interest by leveraging our proposed
Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of
TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA
outperforms the existing state-of-the-art (SOTA) methods by a large margin on
three commonly-used benchmark datasets in the VAD problem (UCF-Crime,
ShanghaiTech Campus, and XD-Violence). Our source code is available at
https://github.com/joos2010kj/CLIP-TSA.",https://github.com/joos2010kj/CLIP-TSA,-1
d26cbcb3-685a-42d9-bc2e-0e9a6f7cbe80,Analyzing Gender Representation in Multilingual Models,0.125003,"Multilingual language models were shown to allow for nontrivial transfer
across scripts and languages. In this work, we study the structure of the
internal representations that enable this transfer. We focus on the
representation of gender distinctions as a practical case study, and examine
the extent to which the gender concept is encoded in shared subspaces across
different languages. Our analysis shows that gender representations consist of
several prominent components that are shared across languages, alongside
language-specific components. The existence of language-independent and
language-specific components provides an explanation for an intriguing
empirical observation we make: while gender classification transfers well
across languages, interventions for gender removal, trained on a single
language, do not transfer easily to others.",https://github.com/gonenhila/multilingual_gender,-1
4d403386-9589-4939-9a59-3b55a33e6bd7,drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM,0.0550841,"This paper describes our system for SemEval-2022 Task 2 Multilingual
Idiomaticity Detection and Sentence Embedding sub-task B. We modify a standard
BERT sentence transformer by adding embeddings for each idioms, which are
created using BERTRAM and a small number of contexts. We show that this
technique increases the quality of idiom representations and leads to better
performance on the task. We also perform analysis on our final results and show
that the quality of the produced idiom embeddings is highly sensitive to the
quality of the input contexts.",https://github.com/drsphelps/semeval-task-2,-1
2bc5af35-666f-4bf4-ad39-074f49ed568b,Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record,0.247186,"Objective: Social determinants of health (SDOH) impact health outcomes and
are documented in the electronic health record (EHR) through structured data
and unstructured clinical notes. However, clinical notes often contain more
comprehensive SDOH information, detailing aspects such as status, severity, and
temporality. This work has two primary objectives: i) develop a natural
language processing (NLP) information extraction model to capture detailed SDOH
information and ii) evaluate the information gain achieved by applying the SDOH
extractor to clinical narratives and combining the extracted representations
with existing structured data.
  Materials and Methods: We developed a novel SDOH extractor using a deep
learning entity and relation extraction architecture to characterize SDOH
across various dimensions. In an EHR case study, we applied the SDOH extractor
to a large clinical data set with 225,089 patients and 430,406 notes with
social history sections and compared the extracted SDOH information with
existing structured data.
  Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the
EHR case study, we found extracted SDOH information complements existing
structured data with 32% of homeless patients, 19% of current tobacco users,
and 10% of drug users only having these health risk factors documented in the
clinical narrative.
  Conclusions: Utilizing EHR data to identify SDOH health risk factors and
social needs may improve patient care and outcomes. Semantic representations of
text-encoded SDOH information can augment existing structured data, and this
more comprehensive SDOH representation can assist health systems in identifying
and addressing these social needs.",https://github.com/uw-bionlp/mspert,8863
73883c09-8f0f-4e7e-afef-c73f4f5c61f7,Brain Principles Programming,0.106277,"In the monograph, STRONG ARTIFICIAL INTELLIGENCE. On the Approaches to
Superintelligence, published by Sberbank, provides a cross-disciplinary review
of general artificial intelligence. As an anthropomorphic direction of
research, it considers Brain Principles Programming, BPP) the formalization of
universal mechanisms (principles) of the brain's work with information, which
are implemented at all levels of the organization of nervous tissue. This
monograph provides a formalization of these principles in terms of the category
theory. However, this formalization is not enough to develop algorithms for
working with information. In this paper, for the description and modeling of
Brain Principles Programming, it is proposed to apply mathematical models and
algorithms developed by us earlier that model cognitive functions, which are
based on well-known physiological, psychological and other natural science
theories. The paper uses mathematical models and algorithms of the following
theories: P.K.Anokhin's Theory of Functional Brain Systems, Eleonor Rosh's
prototypical categorization theory, Bob Rehter's theory of causal models and
natural classification. As a result, the formalization of the BPP is obtained
and computer examples are given that demonstrate the algorithm's operation.",None,-1
6783a9ef-20e3-4965-90c5-412f94c68baa,On-device Synaptic Memory Consolidation using Fowler-Nordheim Quantum-tunneling,0.246096,"Synaptic memory consolidation has been heralded as one of the key mechanisms
for supporting continual learning in neuromorphic Artificial Intelligence (AI)
systems. Here we report that a Fowler-Nordheim (FN) quantum-tunneling device
can implement synaptic memory consolidation similar to what can be achieved by
algorithmic consolidation models like the cascade and the elastic weight
consolidation (EWC) models. The proposed FN-synapse not only stores the
synaptic weight but also stores the synapse's historical usage statistic on the
device itself. We also show that the operation of the FN-synapse is
near-optimal in terms of the synaptic lifetime and we demonstrate that a
network comprising FN-synapses outperforms a comparable EWC network for a small
benchmark continual learning task. With an energy footprint of femtojoules per
synaptic update, we believe that the proposed FN-synapse provides an
ultra-energy-efficient approach for implementing both synaptic memory
consolidation and persistent learning.",None,-1
e9cfa1c6-ffae-423a-a0a6-247c6afebaa3,Interpretable Proof Generation via Iterative Backward Reasoning,0.128776,"We present IBR, an Iterative Backward Reasoning model to solve the proof
generation tasks on rule-based Question Answering (QA), where models are
required to reason over a series of textual rules and facts to find out the
related proof path and derive the final answer. We handle the limitations of
existed works in two folds: 1) enhance the interpretability of reasoning
procedures with detailed tracking, by predicting nodes and edges in the proof
path iteratively backward from the question; 2) promote the efficiency and
accuracy via reasoning on the elaborate representations of nodes and history
paths, without any intermediate texts that may introduce external noise during
proof generation. There are three main modules in IBR, QA and proof strategy
prediction to obtain the answer and offer guidance for the following procedure;
parent node prediction to determine a node in the existing proof that a new
child node will link to; child node prediction to find out which new node will
be added to the proof. Experiments on both synthetic and paraphrased datasets
demonstrate that IBR has better in-domain performance as well as cross-domain
transferability than several strong baselines. Our code and models are
available at https://github.com/find-knowledge/IBR .",https://github.com/find-knowledge/IBR,-1
6bcb97d9-eeb6-4ec0-9927-a8dada3f46e8,S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers,0.656669,"The task of converting a natural language question into an executable SQL
query, known as text-to-SQL, is an important branch of semantic parsing. The
state-of-the-art graph-based encoder has been successfully used in this task
but does not model the question syntax well. In this paper, we propose
S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL
parsers, which effectively leverages the syntactic dependency information of
questions in text-to-SQL to improve the performance. We also employ the
decoupling constraint to induce diverse relational edge embedding, which
further improves the network's performance. Experiments on the Spider and
robustness setting Spider-Syn demonstrate that the proposed approach
outperforms all existing methods when pre-training models are used, resulting
in a performance ranks first on the Spider leaderboard.",None,-1
2001efb3-ae9e-49fb-bdc3-c81635d521a1,PVO: Panoptic Visual Odometry,0.889759,"We present PVO, a novel panoptic visual odometry framework to achieve more
comprehensive modeling of the scene motion, geometry, and panoptic segmentation
information. Our PVO models visual odometry (VO) and video panoptic
segmentation (VPS) in a unified view, which makes the two tasks mutually
beneficial. Specifically, we introduce a panoptic update module into the VO
Module with the guidance of image panoptic segmentation. This Panoptic-Enhanced
VO Module can alleviate the impact of dynamic objects in the camera pose
estimation with a panoptic-aware dynamic mask. On the other hand, the
VO-Enhanced VPS Module also improves the segmentation accuracy by fusing the
panoptic segmentation result of the current frame on the fly to the adjacent
frames, using geometric information such as camera pose, depth, and optical
flow obtained from the VO Module. These two modules contribute to each other
through recurrent iterative optimization. Extensive experiments demonstrate
that PVO outperforms state-of-the-art methods in both visual odometry and video
panoptic segmentation tasks.",https://zju3dv.github.io/pvo/,-1
5e3e1e26-be28-4f28-af4f-9e34c058ae00,Depth Estimation with Simplified Transformer,0.681898,"Transformer and its variants have shown state-of-the-art results in many
vision tasks recently, ranging from image classification to dense prediction.
Despite of their success, limited work has been reported on improving the model
efficiency for deployment in latency-critical applications, such as autonomous
driving and robotic navigation. In this paper, we aim at improving upon the
existing transformers in vision, and propose a method for self-supervised
monocular Depth Estimation with Simplified Transformer (DEST), which is
efficient and particularly suitable for deployment on GPU-based platforms.
Through strategic design choices, our model leads to significant reduction in
model size, complexity, as well as inference latency, while achieving superior
accuracy as compared to state-of-the-art. We also show that our design
generalize well to other dense prediction task without bells and whistles.",None,-1
64869df1-2e5f-4b96-bdb2-fa38dcf5dc10,Considerations for meaningful sign language machine translation based on glosses,0.678136,"Automatic sign language processing is gaining popularity in Natural Language
Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in
particular, sign language translation based on glosses is a prominent approach.
In this paper, we review recent works on neural gloss translation. We find that
limitations of glosses in general and limitations of specific datasets are not
discussed in a transparent manner and that there is no common standard for
evaluation.
  To address these issues, we put forward concrete recommendations for future
research on gloss translation. Our suggestions advocate awareness of the
inherent limitations of gloss-based approaches, realistic datasets, stronger
baselines and convincing evaluation.",None,-1
3c03453e-83d9-4d08-b62a-3da33c379a2b,CNNs and Transformers Perceive Hybrid Images Similar to Humans,0.088686,"Hybrid images is a technique to generate images with two interpretations that
change as a function of viewing distance. It has been utilized to study
multiscale processing of images by the human visual system. Using 63,000 hybrid
images across 10 fruit categories, here we show that predictions of deep
learning vision models qualitatively matches with the human perception of these
images. Our results provide yet another evidence in support of the hypothesis
that Convolutional Neural Networks (CNNs) and Transformers are good at modeling
the feedforward sweep of information in the ventral stream of visual cortex.
Code and data is available at https://github.com/aliborji/hybrid_images.git.",https://github.com/aliborji/hybrid_images.git,-1
9ebf677f-a53f-4308-8f88-28a72d30d405,Studying Bias in GANs through the Lens of Race,0.978034,"In this work, we study how the performance and evaluation of generative image
models are impacted by the racial composition of their training datasets. By
examining and controlling the racial distributions in various training
datasets, we are able to observe the impacts of different training
distributions on generated image quality and the racial distributions of the
generated images. Our results show that the racial compositions of generated
images successfully preserve that of the training data. However, we observe
that truncation, a technique used to generate higher quality images during
inference, exacerbates racial imbalances in the data. Lastly, when examining
the relationship between image quality and race, we find that the highest
perceived visual quality images of a given race come from a distribution where
that race is well-represented, and that annotators consistently prefer
generated images of white people over those of Black people.",None,-1
d21e4321-9eae-4e06-b727-3342c9d43911,Lymphoma segmentation from 3D PET-CT images using a deep evidential network,0.304672,"An automatic evidential segmentation method based on Dempster-Shafer theory
and deep learning is proposed to segment lymphomas from three-dimensional
Positron Emission Tomography (PET) and Computed Tomography (CT) images. The
architecture is composed of a deep feature-extraction module and an evidential
layer. The feature extraction module uses an encoder-decoder framework to
extract semantic feature vectors from 3D inputs. The evidential layer then uses
prototypes in the feature space to compute a belief function at each voxel
quantifying the uncertainty about the presence or absence of a lymphoma at this
location. Two evidential layers are compared, based on different ways of using
distances to prototypes for computing mass functions. The whole model is
trained end-to-end by minimizing the Dice loss function. The proposed
combination of deep feature extraction and evidential segmentation is shown to
outperform the baseline UNet model as well as three other state-of-the-art
models on a dataset of 173 patients.",https://github.com/iWeisskohl,-1
160d8797-dce0-4e04-bbf0-480616ebb474,SERE: Exploring Feature Self-relation for Self-supervised Transformer,0.189226,"Learning representations with self-supervision for convolutional networks
(CNN) has been validated to be effective for vision tasks. As an alternative to
CNN, vision transformers (ViT) have strong representation ability with spatial
self-attention and channel-level feedforward networks. Recent works reveal that
self-supervised learning helps unleash the great potential of ViT. Still, most
works follow self-supervised strategies designed for CNN, e.g., instance-level
discrimination of samples, but they ignore the properties of ViT. We observe
that relational modeling on spatial and channel dimensions distinguishes ViT
from other networks. To enforce this property, we explore the feature
SElf-RElation (SERE) for training self-supervised ViT. Specifically, instead of
conducting self-supervised learning solely on feature embeddings from multiple
views, we utilize the feature self-relations, i.e., spatial/channel
self-relations, for self-supervised learning. Self-relation based learning
further enhances the relation modeling ability of ViT, resulting in stronger
representations that stably improve performance on multiple downstream tasks.
Our source code is publicly available at: https://github.com/MCG-NKU/SERE.",https://github.com/MCG-NKU/SERE,-1
08831fb6-4171-40aa-b754-0130931a4cc0,WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation,0.513385,"Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN",https://github.com/kobeshegu/ECCV2022_WaveGAN,2755
bedb5828-1b56-48a2-9dfb-deb0d7dbcae3,Truth Set Algebra: A New Way to Prove Undefinability,0.872864,"The article proposes a new technique for proving the undefinability of
logical connectives through each other and illustrates the technique with
several examples. Some of the obtained results are new proofs of the existing
theorems, others are original to this work.",None,-1
42177f69-5ec8-4c25-81c9-8365df475146,Understanding Zero-Shot Adversarial Robustness for Large-Scale Models,0.731581,"Pretrained large-scale vision-language models like CLIP have exhibited strong
generalization over unseen tasks. Yet imperceptible adversarial perturbations
can significantly reduce CLIP's performance on new tasks. In this work, we
identify and explore the problem of \emph{adapting large-scale models for
zero-shot adversarial robustness}. We first identify two key factors during
model adaption -- training losses and adaptation methods -- that affect the
model's zero-shot adversarial robustness. We then propose a text-guided
contrastive adversarial training loss, which aligns the text embeddings and the
adversarial visual features with contrastive learning on a small set of
training data. We apply this training loss to two adaption methods, model
finetuning and visual prompt tuning. We find that visual prompt tuning is more
effective in the absence of texts, while finetuning wins in the existence of
text guidance. Overall, our approach significantly improves the zero-shot
adversarial robustness over CLIP, seeing an average improvement of over 31
points over ImageNet and 15 zero-shot datasets. We hope this work can shed
light on understanding the zero-shot adversarial robustness of large-scale
models.",https://github.com/cvlab-columbia/ZSRobust4FoundationModel,-1
20592e5f-796c-47f7-a035-317780214428,Unified Line and Paragraph Detection by Graph Convolutional Networks,0.240863,"We formulate the task of detecting lines and paragraphs in a document into a
unified two-level clustering problem. Given a set of text detection boxes that
roughly correspond to words, a text line is a cluster of boxes and a paragraph
is a cluster of lines. These clusters form a two-level tree that represents a
major part of the layout of a document. We use a graph convolutional network to
predict the relations between text detection boxes and then build both levels
of clusters from these predictions. Experimentally, we demonstrate that the
unified approach can be highly efficient while still achieving state-of-the-art
quality for detecting paragraphs in public benchmarks and real-world images.",None,1989
0f2d7158-41c0-416e-8350-99ba28039377,Uncertainty Quantification for Competency Assessment of Autonomous Agents,0.205691,"For safe and reliable deployment in the real world, autonomous agents must
elicit appropriate levels of trust from human users. One method to build trust
is to have agents assess and communicate their own competencies for performing
given tasks. Competency depends on the uncertainties affecting the agent,
making accurate uncertainty quantification vital for competency assessment. In
this work, we show how ensembles of deep generative models can be used to
quantify the agent's aleatoric and epistemic uncertainties when forecasting
task outcomes as part of competency assessment.",None,-1
99b6144f-6495-4dc7-9181-93d93d269895,Explanations from Large Language Models Make Small Reasoners Better,0.607495,"Integrating free-text explanations to in-context learning of large language
models (LLM) is shown to elicit strong reasoning capabilities along with
reasonable explanations. In this paper, we consider the problem of leveraging
the explanations generated by LLM to improve the training of small reasoners,
which are more favorable in real-production deployment due to their low cost.
We systematically explore three explanation generation approaches from LLM and
utilize a multi-task learning framework to facilitate small models to acquire
strong reasoning power together with explanation generation capabilities.
Experiments on multiple reasoning tasks show that our method can consistently
and significantly outperform finetuning baselines across different settings,
and even perform better than finetuning/prompting a 60x larger GPT-3 (175B)
model by up to 9.5% in accuracy. As a side benefit, human evaluation further
shows that our method can generate high-quality explanations to justify its
predictions, moving towards the goal of explainable AI.",https://github.com/eladsegal/strategyqa,-1
39bf0807-55f6-4fe4-b30f-1fa3604f6a50,"Theories of ""Gender"" in NLP Bias Research",0.859482,"The rise of concern around Natural Language Processing (NLP) technologies
containing and perpetuating social biases has led to a rich and rapidly growing
area of research. Gender bias is one of the central biases being analyzed, but
to date there is no comprehensive analysis of how ""gender"" is theorized in the
field. We survey nearly 200 articles concerning gender bias in NLP to discover
how the field conceptualizes gender both explicitly (e.g. through definitions
of terms) and implicitly (e.g. through how gender is operationalized in
practice). In order to get a better idea of emerging trajectories of thought,
we split these articles into two sections by time.
  We find that the majority of the articles do not make their theorization of
gender explicit, even if they clearly define ""bias."" Almost none use a model of
gender that is intersectional or inclusive of nonbinary genders; and many
conflate sex characteristics, social gender, and linguistic gender in ways that
disregard the existence and experience of trans, nonbinary, and intersex
people. There is an increase between the two time-sections in statements
acknowledging that gender is a complicated reality, however, very few articles
manage to put this acknowledgment into practice. In addition to analyzing these
findings, we provide specific recommendations to facilitate interdisciplinary
work, and to incorporate theory and methodology from Gender Studies. Our hope
is that this will produce more inclusive gender bias research in NLP.",None,-1
9b342689-0a9c-434d-864f-826a28a317d8,Training language models to follow instructions with human feedback,1.0,"Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.",https://github.com/openai/following-instructions-human-feedback,-1
6c645d57-fa46-44c9-a5f2-13d4af67f7bb,Network Pruning via Feature Shift Minimization,0.204435,"Channel pruning is widely used to reduce the complexity of deep network
models. Recent pruning methods usually identify which parts of the network to
discard by proposing a channel importance criterion. However, recent studies
have shown that these criteria do not work well in all conditions. In this
paper, we propose a novel Feature Shift Minimization (FSM) method to compress
CNN models, which evaluates the feature shift by converging the information of
both features and filters. Specifically, we first investigate the compression
efficiency with some prevalent methods in different layer-depths and then
propose the feature shift concept. Then, we introduce an approximation method
to estimate the magnitude of the feature shift, since it is difficult to
compute it directly. Besides, we present a distribution-optimization algorithm
to compensate for the accuracy loss and improve the network compression
efficiency. The proposed method yields state-of-the-art performance on various
benchmark networks and datasets, verified by extensive experiments. Our codes
are available at: https://github.com/lscgx/FSM.",https://github.com/lscgx/FSM,-1
f19db7db-0d06-45dc-a428-aefee2bb6362,AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages,0.512352,"In recent years, multilingual pre-trained language models have gained
prominence due to their remarkable performance on numerous downstream Natural
Language Processing tasks (NLP). However, pre-training these large multilingual
language models requires a lot of training data, which is not available for
African Languages. Active learning is a semi-supervised learning algorithm, in
which a model consistently and dynamically learns to identify the most
beneficial samples to train itself on, in order to achieve better optimization
and performance on downstream tasks. Furthermore, active learning effectively
and practically addresses real-world data scarcity. Despite all its benefits,
active learning, in the context of NLP and especially multilingual language
models pretraining, has received little consideration. In this paper, we
present AfroLM, a multilingual language model pretrained from scratch on 23
African languages (the largest effort to date) using our novel self-active
learning framework. Pretrained on a dataset significantly (14x) smaller than
existing baselines, AfroLM outperforms many multilingual pretrained language
models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text
classification, and sentiment analysis). Additional out-of-domain sentiment
analysis experiments show that \textbf{AfroLM} is able to generalize well
across various domains. We release the code source, and our datasets used in
our framework at https://github.com/bonaventuredossou/MLM_AL.",https://github.com/bonaventuredossou/MLM_AL,-1
6c4ecd8b-b950-42bf-87aa-2e9bbb016a88,"A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding",0.21396,"Collecting sufficient labeled data for spoken language understanding (SLU) is
expensive and time-consuming. Recent studies achieved promising results by
using pre-trained models in low-resource scenarios. Inspired by this, we aim to
ask: which (if any) pre-training strategies can improve performance across SLU
benchmarks? To answer this question, we employ four types of pre-trained models
and their combinations for SLU. We leverage self-supervised speech and language
models (LM) pre-trained on large quantities of unpaired data to extract strong
speech and text representations. We also explore using supervised models
pre-trained on larger external automatic speech recognition (ASR) or SLU
corpora. We conduct extensive experiments on the SLU Evaluation (SLUE)
benchmark and observe self-supervised pre-trained models to be more powerful,
with pre-trained LM and speech models being most beneficial for the Sentiment
Analysis and Named Entity Recognition task, respectively.",None,-1
d592b983-4c25-4cef-a6db-9c9a1f55752f,CONSISTENT: Open-Ended Question Generation From News Articles,0.398173,"Recent work on question generation has largely focused on factoid questions
such as who, what, where, when about basic facts. Generating open-ended why,
how, what, etc. questions that require long-form answers have proven more
difficult. To facilitate the generation of open-ended questions, we propose
CONSISTENT, a new end-to-end system for generating open-ended questions that
are answerable from and faithful to the input text. Using news articles as a
trustworthy foundation for experimentation, we demonstrate our model's strength
over several baselines using both automatic and human=based evaluations. We
contribute an evaluation dataset of expert-generated open-ended questions.We
discuss potential downstream applications for news media organizations.",https://github.com/tuhinjubcse/OpenD,-1
fec7aba1-9b4a-48e8-9f2b-518113db5751,Class-Specific Semantic Reconstruction for Open Set Recognition,0.76311,"Open set recognition enables deep neural networks (DNNs) to identify samples
of unknown classes, while maintaining high classification accuracy on samples
of known classes. Existing methods basing on auto-encoder (AE) and prototype
learning show great potential in handling this challenging task. In this study,
we propose a novel method, called Class-Specific Semantic Reconstruction
(CSSR), that integrates the power of AE and prototype learning. Specifically,
CSSR replaces prototype points with manifolds represented by class-specific
AEs. Unlike conventional prototype-based methods, CSSR models each known class
on an individual AE manifold, and measures class belongingness through AE's
reconstruction error. Class-specific AEs are plugged into the top of the DNN
backbone and reconstruct the semantic representations learned by the DNN
instead of the raw image. Through end-to-end learning, the DNN and the AEs
boost each other to learn both discriminative and representative information.
The results of experiments conducted on multiple datasets show that the
proposed method achieves outstanding performance in both close and open set
recognition and is sufficiently simple and flexible to incorporate into
existing frameworks.",None,-1
5425464b-302b-461d-ab9c-6416cc68a7f2,Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling,0.956349,"Existing research generally treats Chinese character as a minimum unit for
representation. However, such Chinese character representation will suffer two
bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich
internal features (e.g., radicals and strokes); and 2) Parameter bottleneck,
each individual character has to be represented by a unique vector. In this
paper, we introduce a novel representation method for Chinese characters to
break the bottlenecks, namely StrokeNet, which represents a Chinese character
by a Latinized stroke sequence (e.g., ""ao1 (concave)"" to ""ajaie"" and ""tu1
(convex)"" to ""aeaqe""). Specifically, StrokeNet maps each stroke to a specific
Latin character, thus allowing similar Chinese characters to have similar Latin
representations. With the introduction of StrokeNet to neural machine
translation (NMT), many powerful but not applicable techniques to non-Latin
languages (e.g., shared subword vocabulary learning and ciphertext-based data
augmentation) can now be perfectly implemented. Experiments on the widely-used
NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT
tasks show that StrokeNet can provide a significant performance boost over the
strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17
Chinese-English task which is better than any previously reported results
without using monolingual data. Code and scripts are freely available at
https://github.com/zjwang21/StrokeNet.",https://github.com/zjwang21/StrokeNet,-1
5c3b46b5-9356-4f0d-9a70-14bf3f86a0fe,InterTrack: Interaction Transformer for 3D Multi-Object Tracking,0.388691,"3D multi-object tracking (MOT) is a key problem for autonomous vehicles,
required to perform well-informed motion planning in dynamic environments.
Particularly for densely occupied scenes, associating existing tracks to new
detections remains challenging as existing systems tend to omit critical
contextual information. Our proposed solution, InterTrack, introduces the
Interaction Transformer for 3D MOT to generate discriminative object
representations for data association. We extract state and shape features for
each track and detection, and efficiently aggregate global information via
attention. We then perform a learned regression on each track/detection feature
pair to estimate affinities, and use a robust two-stage data association and
track management approach to produce the final tracks. We validate our approach
on the nuScenes 3D MOT benchmark, where we observe significant improvements,
particularly on classes with small physical sizes and clustered objects. As of
submission, InterTrack ranks 1st in overall AMOTA among methods using
CenterPoint detections.",https://github.com/open-mmlab/OpenPCDet,-1
722233d1-d82c-416b-a51f-291d7f0db7e5,TextMatcher: Cross-Attentional Neural Network to Compare Image and Text,0.0173474,"We study a novel multimodal-learning problem, which we call text matching:
given an image containing a single-line text and a candidate text
transcription, the goal is to assess whether the text represented in the image
corresponds to the candidate text. We devise the first machine-learning model
specifically designed for this problem. The proposed model, termed TextMatcher,
compares the two inputs by applying a cross-attention mechanism over the
embedding representations of image and text, and it is trained in an end-to-end
fashion. We extensively evaluate the empirical performance of TextMatcher on
the popular IAM dataset. Results attest that, compared to a baseline and
existing models designed for related problems, TextMatcher achieves higher
performance on a variety of configurations, while at the same time running
faster at inference time. We also showcase TextMatcher in a real-world
application scenario concerning the automatic processing of bank cheques.",https://github.com/ayumiymk/aster.pytorch,-1
b664de8a-86d8-4a97-9d7e-8e97b6bf58d4,Invariant Descriptors for Intrinsic Reflectance Optimization,0.23521,"Intrinsic image decomposition aims to factorize an image into albedo
(reflectance) and shading (illumination) sub-components. Being ill-posed and
under-constrained, it is a very challenging computer vision problem. There are
infinite pairs of reflectance and shading images that can reconstruct the same
input. To address the problem, Intrinsic Images in the Wild provides an
optimization framework based on a dense conditional random field (CRF)
formulation that considers long-range material relations. We improve upon their
model by introducing illumination invariant image descriptors: color ratios.
The color ratios and the reflectance intrinsic are both invariant to
illumination and thus are highly correlated. Through detailed experiments, we
provide ways to inject the color ratios into the dense CRF optimization. Our
approach is physics-based, learning-free and leads to more accurate and robust
reflectance decompositions.",https://github.com/seanbell/intrinsic,-1
23b8528a-83df-4f4a-aaa0-4f1d01ba6be9,Point-Level Region Contrast for Object Detection Pre-Training,0.695713,"In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available.",None,115578
652fa54c-01dc-4731-8102-90049d967fa7,MAViL: Masked Audio-Video Learners,0.904404,"We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks.",https://github.com/facebookresearch/MAViL,-1
c5e4046c-5948-4a8a-b5a1-fc05f86d1fa1,On the Explainability of Natural Language Processing Deep Models,0.737148,"While there has been a recent explosion of work on ExplainableAI ExAI on deep
models that operate on imagery and tabular data, textual datasets present new
challenges to the ExAI community. Such challenges can be attributed to the lack
of input structure in textual data, the use of word embeddings that add to the
opacity of the models and the difficulty of the visualization of the inner
workings of deep models when they are trained on textual data.
  Lately, methods have been developed to address the aforementioned challenges
and present satisfactory explanations on Natural Language Processing (NLP)
models. However, such methods are yet to be studied in a comprehensive
framework where common challenges are properly stated and rigorous evaluation
practices and metrics are proposed. Motivated to democratize ExAI methods in
the NLP field, we present in this work a survey that studies model-agnostic as
well as model-specific explainability methods on NLP models. Such methods can
either develop inherently interpretable NLP models or operate on pre-trained
models in a post-hoc manner. We make this distinction and we further decompose
the methods into three categories according to what they explain: (1) word
embeddings (input-level), (2) inner workings of NLP models (processing-level)
and (3) models' decisions (output-level). We also detail the different
evaluation approaches interpretability methods in the NLP field. Finally, we
present a case-study on the well-known neural machine translation in an
appendix and we propose promising future research directions for ExAI in the
NLP field.",None,-1
08cb1604-2de3-499f-8ed8-71b1e78b03d5,Unfooling Perturbation-Based Post Hoc Explainers,0.778764,"Monumental advancements in artificial intelligence (AI) have lured the
interest of doctors, lenders, judges, and other professionals. While these
high-stakes decision-makers are optimistic about the technology, those familiar
with AI systems are wary about the lack of transparency of its decision-making
processes. Perturbation-based post hoc explainers offer a model agnostic means
of interpreting these systems while only requiring query-level access. However,
recent work demonstrates that these explainers can be fooled adversarially.
This discovery has adverse implications for auditors, regulators, and other
sentinels. With this in mind, several natural questions arise - how can we
audit these black box systems? And how can we ascertain that the auditee is
complying with the audit in good faith? In this work, we rigorously formalize
this problem and devise a defense against adversarial attacks on
perturbation-based explainers. We propose algorithms for the detection
(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our
novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our
approach successfully detects whether a black box system adversarially conceals
its decision-making process and mitigates the adversarial attack on real-world
data for the prevalent explainers, LIME and SHAP.",https://github.com/craymichael/unfooling,-1
8ed01dc3-4b8c-466e-952c-fc6c080b17ea,MSANet: Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation,0.386243,"Few-shot segmentation aims to segment unseen-class objects given only a
handful of densely labeled samples. Prototype learning, where the support
feature yields a singleor several prototypes by averaging global and local
object information, has been widely used in FSS. However, utilizing only
prototype vectors may be insufficient to represent the features for all
training data. To extract abundant features and make more precise predictions,
we propose a Multi-Similarity and Attention Network (MSANet) including two
novel modules, a multi-similarity module and an attention module. The
multi-similarity module exploits multiple feature-maps of support images and
query images to estimate accurate semantic relationships. The attention module
instructs the network to concentrate on class-relevant information. The network
is tested on standard FSS datasets, PASCAL-5i 1-shot, PASCAL-5i 5-shot,
COCO-20i 1-shot, and COCO-20i 5-shot. The MSANet with the backbone of
ResNet-101 achieves the state-of-the-art performance for all 4-benchmark
datasets with mean intersection over union (mIoU) of 69.13%, 73.99%, 51.09%,
56.80%, respectively. Code is available at
https://github.com/AIVResearch/MSANet",https://github.com/AIVResearch/MSANet,-1
41f9d4a2-9ae7-481c-b1bf-33a10a66e34f,Human-Centered Concept Explanations for Neural Networks,0.508133,"Understanding complex machine learning models such as deep neural networks
with explanations is crucial in various applications. Many explanations stem
from the model perspective, and may not necessarily effectively communicate why
the model is making its predictions at the right level of abstraction. For
example, providing importance weights to individual pixels in an image can only
express which parts of that particular image are important to the model, but
humans may prefer an explanation which explains the prediction by concept-based
thinking. In this work, we review the emerging area of concept based
explanations. We start by introducing concept explanations including the class
of Concept Activation Vectors (CAV) which characterize concepts using vectors
in appropriate spaces of neural activations, and discuss different properties
of useful concepts, and approaches to measure the usefulness of concept
vectors. We then discuss approaches to automatically extract concepts, and
approaches to address some of their caveats. Finally, we discuss some case
studies that showcase the utility of such concept-based explanations in
synthetic settings and real world applications.",None,-1
f9b8ad81-68f4-4845-bb8b-9b6cedb964d3,SRL-SOA: Self-Representation Learning with Sparse 1D-Operational Autoencoder for Hyperspectral Image Band Selection,0.551468,"The band selection in the hyperspectral image (HSI) data processing is an
important task considering its effect on the computational complexity and
accuracy. In this work, we propose a novel framework for the band selection
problem: Self-Representation Learning (SRL) with Sparse 1D-Operational
Autoencoder (SOA). The proposed SLR-SOA approach introduces a novel autoencoder
model, SOA, that is designed to learn a representation domain where the data
are sparsely represented. Moreover, the network composes of 1D-operational
layers with the non-linear neuron model. Hence, the learning capability of
neurons (filters) is greatly improved with shallow architectures. Using compact
architectures is especially crucial in autoencoders as they tend to overfit
easily because of their identity mapping objective. Overall, we show that the
proposed SRL-SOA band selection approach outperforms the competing methods over
two HSI data including Indian Pines and Salinas-A considering the achieved land
cover classification accuracies. The software implementation of the SRL-SOA
approach is shared publicly at https://github.com/meteahishali/SRL-SOA.",https://github.com/meteahishali/SRL-SOA,-1
19e31adf-30be-4f59-8a83-471acf2b76d2,Features Fusion Framework for Multimodal Irregular Time-series Events,0.292714,"Some data from multiple sources can be modeled as multimodal time-series
events which have different sampling frequencies, data compositions, temporal
relations and characteristics. Different types of events have complex nonlinear
relationships, and the time of each event is irregular. Neither the classical
Recurrent Neural Network (RNN) model nor the current state-of-the-art
Transformer model can deal with these features well. In this paper, a features
fusion framework for multimodal irregular time-series events is proposed based
on the Long Short-Term Memory networks (LSTM). Firstly, the complex features
are extracted according to the irregular patterns of different events.
Secondly, the nonlinear correlation and complex temporal dependencies
relationship between complex features are captured and fused into a tensor.
Finally, a feature gate are used to control the access frequency of different
tensors. Extensive experiments on MIMIC-III dataset demonstrate that the
proposed framework significantly outperforms to the existing methods in terms
of AUC (the area under Receiver Operating Characteristic curve) and AP (Average
Precision).",None,22
9f7f6770-13dd-4c44-bb88-fdf7ad25cd1c,Machine Learning Methods in Solving the Boolean Satisfiability Problem,0.849792,"This paper reviews the recent literature on solving the Boolean
satisfiability problem (SAT), an archetypal NP-complete problem, with the help
of machine learning techniques. Despite the great success of modern SAT solvers
to solve large industrial instances, the design of handcrafted heuristics is
time-consuming and empirical. Under the circumstances, the flexible and
expressive machine learning methods provide a proper alternative to solve this
long-standing problem. We examine the evolving ML-SAT solvers from naive
classifiers with handcrafted features to the emerging end-to-end SAT solvers
such as NeuroSAT, as well as recent progress on combinations of existing CDCL
and local search solvers with machine learning methods. Overall, solving SAT
with machine learning is a promising yet challenging research topic. We
conclude the limitations of current works and suggest possible future
directions.",None,-1
6b0e9330-8faf-4191-a7d6-e248657bc590,Learning Feynman Diagrams using Graph Neural Networks,0.0457752,"In the wake of the growing popularity of machine learning in particle
physics, this work finds a new application of geometric deep learning on
Feynman diagrams to make accurate and fast matrix element predictions with the
potential to be used in analysis of quantum field theory. This research uses
the graph attention layer which makes matrix element predictions to 1
significant figure accuracy above 90% of the time. Peak performance was
achieved in making predictions to 3 significant figure accuracy over 10% of the
time with less than 200 epochs of training, serving as a proof of concept on
which future works can build upon for better performance. Finally, a procedure
is suggested, to use the network to make advancements in quantum field theory
by constructing Feynman diagrams with effective particles that represent
non-perturbative calculations.",https://github.com/Clearbloo/Feynman_GNN.git,-1
c2bdc759-1dec-449f-82ed-b078447ce7a2,Learning to Act with Affordance-Aware Multimodal Neural SLAM,0.260727,"Recent years have witnessed an emerging paradigm shift toward embodied
artificial intelligence, in which an agent must learn to solve challenging
tasks by interacting with its environment. There are several challenges in
solving embodied multimodal tasks, including long-horizon planning,
vision-and-language grounding, and efficient exploration. We focus on a
critical bottleneck, namely the performance of planning and navigation. To
tackle this challenge, we propose a Neural SLAM approach that, for the first
time, utilizes several modalities for exploration, predicts an affordance-aware
semantic map, and plans over it at the same time. This significantly improves
exploration efficiency, leads to robust long-horizon planning, and enables
effective vision-and-language grounding. With the proposed Affordance-aware
Multimodal Neural SLAM (AMSLAM) approach, we obtain more than 40% improvement
over prior published work on the ALFRED benchmark and set a new
state-of-the-art generalization performance at a success rate of 23.48% on the
test unseen scenes.",https://github.com/amazon-research/multimodal-neuralslam,-1
8e6a0d5a-34bf-4e6e-8140-3f186b99890b,Rectifying homographies for stereo vision: analytical solution for minimal distortion,0.0863072,"Stereo rectification is the determination of two image transformations (or
homographies) that map corresponding points on the two images, projections of
the same point in the 3D space, onto the same horizontal line in the
transformed images. Rectification is used to simplify the subsequent stereo
correspondence problem and speeding up the matching process. Rectifying
transformations, in general, introduce perspective distortion on the obtained
images, which shall be minimised to improve the accuracy of the following
algorithm dealing with the stereo correspondence problem. The search for the
optimal transformations is usually carried out relying on numerical
optimisation. This work proposes a closed-form solution for the rectifying
homographies that minimise perspective distortion. The experimental comparison
confirms its capability to solve the convergence issues of the previous
formulation. Its Python implementation is provided.",https://github.com/decadenza/DirectStereoRectification,-1
67948564-809c-4841-8683-cb5d72a02af1,Learning Deep Sensorimotor Policies for Vision-based Autonomous Drone Racing,0.717279,"Autonomous drones can operate in remote and unstructured environments,
enabling various real-world applications. However, the lack of effective
vision-based algorithms has been a stumbling block to achieving this goal.
Existing systems often require hand-engineered components for state estimation,
planning, and control. Such a sequential design involves laborious tuning,
human heuristics, and compounding delays and errors. This paper tackles the
vision-based autonomous-drone-racing problem by learning deep sensorimotor
policies. We use contrastive learning to extract robust feature representations
from the input images and leverage a two-stage learning-by-cheating framework
for training a neural network policy. The resulting policy directly infers
control commands with feature representations learned from raw images, forgoing
the need for globally-consistent state estimation, trajectory planning, and
handcrafted control design. Our experimental results indicate that our
vision-based policy can achieve the same level of racing performance as the
state-based policy while being robust against different visual disturbances and
distractors. We believe this work serves as a stepping-stone toward developing
intelligent vision-based autonomous systems that control the drone purely from
image inputs, like human pilots.",None,-1
5d35185e-725a-4a85-9a62-425ef8de6395,Graph Reasoning Transformer for Image Parsing,0.614242,"Capturing the long-range dependencies has empirically proven to be effective
on a wide range of computer vision tasks. The progressive advances on this
topic have been made through the employment of the transformer framework with
the help of the multi-head attention mechanism. However, the attention-based
image patch interaction potentially suffers from problems of redundant
interactions of intra-class patches and unoriented interactions of inter-class
patches. In this paper, we propose a novel Graph Reasoning Transformer (GReaT)
for image parsing to enable image patches to interact following a relation
reasoning pattern. Specifically, the linearly embedded image patches are first
projected into the graph space, where each node represents the implicit visual
center for a cluster of image patches and each edge reflects the relation
weight between two adjacent nodes. After that, global relation reasoning is
performed on this graph accordingly. Finally, all nodes including the relation
information are mapped back into the original space for subsequent processes.
Compared to the conventional transformer, GReaT has higher interaction
efficiency and a more purposeful interaction pattern. Experiments are carried
out on the challenging Cityscapes and ADE20K datasets. Results show that GReaT
achieves consistent performance gains with slight computational overheads on
the state-of-the-art transformer baselines.",https://github.com/open-mmlab/mmsegmentation,-1
bc3d0b1e-4dca-4c01-8797-d5834d7a338a,Relation Regularized Scene Graph Generation,0.267057,"Scene graph generation (SGG) is built on top of detected objects to predict
object pairwise visual relations for describing the image content abstraction.
Existing works have revealed that if the links between objects are given as
prior knowledge, the performance of SGG is significantly improved. Inspired by
this observation, in this article, we propose a relation regularized network
(R2-Net), which can predict whether there is a relationship between two objects
and encode this relation into object feature refinement and better SGG.
Specifically, we first construct an affinity matrix among detected objects to
represent the probability of a relationship between two objects. Graph
convolution networks (GCNs) over this relation affinity matrix are then used as
object encoders, producing relation-regularized representations of objects.
With these relation-regularized features, our R2-Net can effectively refine
object labels and generate scene graphs. Extensive experiments are conducted on
the visual genome dataset for three SGG tasks (i.e., predicate classification,
scene graph classification, and scene graph detection), demonstrating the
effectiveness of our proposed method. Ablation studies also verify the key
roles of our proposed components in performance improvement.",None,-1
5ec8a3de-2d04-4e46-9f8a-5dae9dbb81ba,Localizing Visual Sounds the Easy Way,0.991228,"Unsupervised audio-visual source localization aims at localizing visible
sound sources in a video without relying on ground-truth localization for
training. Previous works often seek high audio-visual similarities for likely
positive (sounding) regions and low similarities for likely negative regions.
However, accurately distinguishing between sounding and non-sounding regions is
challenging without manual annotations. In this work, we propose a simple yet
effective approach for Easy Visual Sound Localization, namely EZ-VSL, without
relying on the construction of positive and/or negative regions during
training. Instead, we align audio and visual spaces by seeking audio-visual
representations that are aligned in, at least, one location of the associated
image, while not matching other images, at any location. We also introduce a
novel object guided localization scheme at inference time for improved
precision. Our simple and effective framework achieves state-of-the-art
performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In
particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to
83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is
available at https://github.com/stoneMo/EZ-VSL.",https://github.com/stoneMo/EZ-VSL,-1
56d28333-10eb-40e0-852d-0ddead3e05a1,"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink",0.999982,"Machine Learning (ML) workloads have rapidly grown in importance, but raised
concerns about their carbon footprint. Four best practices can reduce ML
training energy by up to 100x and CO2 emissions up to 1000x. By following best
practices, overall ML energy use (across research, development, and production)
held steady at <15% of Google's total energy use for the past three years. If
the whole ML field were to adopt best practices, total carbon emissions from
training would reduce. Hence, we recommend that ML papers include emissions
explicitly to foster competition on more than just model quality. Estimates of
emissions in papers that omitted them have been off 100x-100,000x, so
publishing emissions has the added benefit of ensuring accurate accounting.
Given the importance of climate change, we must get the numbers right to make
certain that we work on its biggest challenges.",None,-1
9aab8ed9-7452-4596-a799-ee46573731ff,Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models,0.682942,"End-to-end spoken language understanding (SLU) systems are gaining popularity
over cascaded approaches due to their simplicity and ability to avoid error
propagation. However, these systems model sequence labeling as a sequence
prediction task causing a divergence from its well-established token-level
tagging formulation. We build compositional end-to-end SLU systems that
explicitly separate the added complexity of recognizing spoken mentions in SLU
from the NLU task of sequence labeling. By relying on intermediate decoders
trained for ASR, our end-to-end systems transform the input modality from
speech to token-level representations that can be used in the traditional
sequence labeling framework. This composition of ASR and NLU formulations in
our end-to-end SLU system offers direct compatibility with pre-trained ASR and
NLU systems, allows performance monitoring of individual components and enables
the use of globally normalized losses like CRF, making them attractive in
practical scenarios. Our models outperform both cascaded and direct end-to-end
models on a labeling task of named entity recognition across SLU benchmarks.",https://github.com/espnet/espnet,-1
b59db4e9-fae6-4eef-bc11-77aa2e19077a,Bootstrapped Transformer for Offline Reinforcement Learning,0.697955,"Offline reinforcement learning (RL) aims at learning policies from previously
collected static trajectory data without interacting with the real environment.
Recent works provide a novel perspective by viewing offline RL as a generic
sequence generation problem, adopting sequence models such as Transformer
architecture to model distributions over trajectories, and repurposing beam
search as a planning algorithm. However, the training datasets utilized in
general offline RL tasks are quite limited and often suffer from insufficient
distribution coverage, which could be harmful to training sequence generation
models yet has not drawn enough attention in the previous works. In this paper,
we propose a novel algorithm named Bootstrapped Transformer, which incorporates
the idea of bootstrapping and leverages the learned model to self-generate more
offline data to further boost the sequence model training. We conduct extensive
experiments on two offline RL benchmarks and demonstrate that our model can
largely remedy the existing offline RL training limitations and beat other
strong baseline methods. We also analyze the generated pseudo data and the
revealed characteristics may shed some light on offline RL training. The codes
are available at https://seqml.github.io/bootorl.",https://github.com/rail-berkeley/d4rl/blob/master/README.md,-1
b750f8c0-8fea-49a0-9b9e-deed41bcb5f2,Compression of Generative Pre-trained Language Models via Quantization,0.980507,"The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.",None,-1
d20a1f56-d851-4377-9fc3-86fcb9ef7210,Re-Examining Calibration: The Case of Question Answering,0.701852,"For users to trust model predictions, they need to understand model outputs,
particularly their confidence - calibration aims to adjust (calibrate) models'
confidence to match expected accuracy. We argue that the traditional
calibration evaluation does not promote effective calibrations: for example, it
can encourage always assigning a mediocre confidence score to all predictions,
which does not help users distinguish correct predictions from wrong ones.
Building on those observations, we propose a new calibration metric, MacroCE,
that better captures whether the model assigns low confidence to wrong
predictions and high confidence to correct predictions. Focusing on the
practical application of open-domain question answering, we examine
conventional calibration methods applied on the widely-used retriever-reader
pipeline, all of which do not bring significant gains under our new MacroCE
metric. Toward better calibration, we propose a new calibration method
(ConsCal) that uses not just final model predictions but whether multiple model
checkpoints make consistent predictions. Altogether, we provide an alternative
view of calibration along with a new metric, re-evaluation of existing
calibration methods on our metric, and proposal of a more effective calibration
method.",https://github.com/NoviScl/calibrateQA,-1
7a0b3490-f3c9-4f36-a91d-e6cee5a9b418,Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation,1.0,"A diffusion model learns to predict a vector field of gradients. We propose
to apply chain rule on the learned gradients, and back-propagate the score of a
diffusion model through the Jacobian of a differentiable renderer, which we
instantiate to be a voxel radiance field. This setup aggregates 2D scores at
multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D
model for 3D data generation. We identify a technical challenge of distribution
mismatch that arises in this application, and propose a novel estimation
mechanism to resolve it. We run our algorithm on several off-the-shelf
diffusion image generative models, including the recently released Stable
Diffusion trained on the large-scale LAION dataset.",https://github.com/ashawkey/stable-dreamfusion,-1
378e4ced-2043-4f39-9b54-2fb5711318e6,Look to the Right: Mitigating Relative Position Bias in Extractive Question Answering,0.19095,"Extractive question answering (QA) models tend to exploit spurious
correlations to make predictions when a training set has unintended biases.
This tendency results in models not being generalizable to examples where the
correlations do not hold. Determining the spurious correlations QA models can
exploit is crucial in building generalizable QA models in real-world
applications; moreover, a method needs to be developed that prevents these
models from learning the spurious correlations even when a training set is
biased. In this study, we discovered that the relative position of an answer,
which is defined as the relative distance from an answer span to the closest
question-context overlap word, can be exploited by QA models as superficial
cues for making predictions. Specifically, we find that when the relative
positions in a training set are biased, the performance on examples with
relative positions unseen during training is significantly degraded. To
mitigate the performance degradation for unseen relative positions, we propose
an ensemble-based debiasing method that does not require prior knowledge about
the distribution of relative positions. We demonstrate that the proposed method
mitigates the models' reliance on relative positions using the biased and full
SQuAD dataset. We hope that this study can help enhance the generalization
ability of QA models in real-world applications.",https://github.com/KazutoshiShinoda/RelativePositionBias,-1
4eb1e160-d8e8-41d5-85e9-277175a90fda,Sequential Manipulation Planning on Scene Graph,0.839497,"We devise a 3D scene graph representation, contact graph+ (cg+), for
efficient sequential task planning. Augmented with predicate-like attributes,
this contact graph-based representation abstracts scene layouts with succinct
geometric information and valid robot-scene interactions. Goal configurations,
naturally specified on contact graphs, can be produced by a genetic algorithm
with a stochastic optimization method. A task plan is then initialized by
computing the Graph Editing Distance (GED) between the initial contact graphs
and the goal configurations, which generates graph edit operations
corresponding to possible robot actions. We finalize the task plan by imposing
constraints to regulate the temporal feasibility of graph edit operations,
ensuring valid task and motion correspondences. In a series of simulations and
experiments, robots successfully complete complex sequential object
rearrangement tasks that are difficult to specify using conventional planning
language like Planning Domain Definition Language (PDDL), demonstrating the
high feasibility and potential of robot sequential task planning on contact
graph.",https://sites.google.com/view/planning-on-graph,-1
3a08c969-9289-49fa-8a33-7d3df6f1e17d,AI Art in Architecture,0.38374,"Recent diffusion-based AI art platforms are able to create impressive images
from simple text descriptions. This makes them powerful tools for concept
design in any discipline that requires creativity in visual design tasks. This
is also true for early stages of architectural design with multiple stages of
ideation, sketching and modelling. In this paper, we investigate how applicable
diffusion-based models already are to these tasks. We research the
applicability of the platforms Midjourney, DALL-E 2 and StableDiffusion to a
series of common use cases in architectural design to determine which are
already solvable or might soon be. We also analyze how they are already being
used by analyzing a data set of 40 million Midjourney queries with NLP methods
to extract common usage patterns. With this insights we derived a workflow to
interior and exterior design that combines the strengths of the individual
platforms.",None,3533
45c2e06f-4937-4013-9020-f68f34839539,Resolving Semantic Confusions for Improved Zero-Shot Detection,0.0933065,"Zero-shot detection (ZSD) is a challenging task where we aim to recognize and
localize objects simultaneously, even when our model has not been trained with
visual samples of a few target (""unseen"") classes. Recently, methods employing
generative models like GANs have shown some of the best results, where
unseen-class samples are generated based on their semantics by a GAN trained on
seen-class data, enabling vanilla object detectors to recognize unseen objects.
However, the problem of semantic confusion still remains, where the model is
sometimes unable to distinguish between semantically-similar classes. In this
work, we propose to train a generative model incorporating a triplet loss that
acknowledges the degree of dissimilarity between classes and reflects them in
the generated samples. Moreover, a cyclic-consistency loss is also enforced to
ensure that generated visual samples of a class highly correspond to their own
semantics. Extensive experiments on two benchmark ZSD datasets - MSCOCO and
PASCAL-VOC - demonstrate significant gains over the current ZSD methods,
reducing semantic confusion and improving detection for the unseen classes.",https://github.com/sandipan211/ZSD-SC-Resolver,-1
fb35a31a-cd02-4f84-a4a1-fca95fb0efcf,Incorporating Context into Subword Vocabularies,0.389576,"Most current popular subword tokenizers are trained based on word frequency
statistics over a corpus, without considering information about co-occurrence
or context. Nevertheless, the resulting vocabularies are used in language
models' highly contextualized settings. We present SaGe, a tokenizer that
tailors subwords for their downstream use by baking in the contextualized
signal at the vocabulary creation phase. We show that SaGe does a better job
than current widespread tokenizers in keeping token contexts cohesive, while
not incurring a large price in terms of encoding efficiency or domain
robustness. SaGe improves performance on English GLUE classification tasks as
well as on NER, and on Inference and NER in Turkish, demonstrating its
robustness to language properties such as morphological exponence and
agglutination.",https://github.com/MeLeLbgu/SaGe,-1
4f779920-588b-4c09-a7f4-c9561747ef0b,Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning,0.767272,"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset.",https://github.com/XDUxyLi/SCEN-master,-1
4bbe4736-9fde-4716-9b98-6d113afe54c3,Aggregate effects of advertising decisions: a complex systems look at search engine advertising via an experimental study,0.449051,"Purpose: We model group advertising decisions, which are the collective
decisions of every single advertiser within the set of advertisers who are
competing in the same auction or vertical industry, and examine resulting
market outcomes, via a proposed simulation framework named EXP-SEA
(Experimental Platform for Search Engine Advertising) supporting experimental
studies of collective behaviors in the context of search engine advertising.
Design: We implement the EXP-SEA to validate the proposed simulation framework,
also conduct three experimental studies on the aggregate impact of electronic
word-of-mouth, the competition level, and strategic bidding behaviors. EXP-SEA
supports heterogeneous participants, various auction mechanisms, and also
ranking and pricing algorithms. Findings: Findings from our three experiments
show that (a) both the market profit and advertising indexes such as number of
impressions and number of clicks are larger when the eWOM effect presents,
meaning social media certainly has some effect on search engine advertising
outcomes, (b) the competition level has a monotonic increasing effect on the
market performance, thus search engines have an incentive to encourage both the
eWOM among search users and competition among advertisers, and (c) given the
market-level effect of the percentage of advertisers employing a dynamic greedy
bidding strategy, there is a cut-off point for strategic bidding behaviors.
Originality: This is one of the first research works to explore collective
group decisions and resulting phenomena in the complex context of search engine
advertising via developing and validating a simulation framework that supports
assessments of various advertising strategies and estimations of the impact of
mechanisms on the search market.",None,-1
8a529b1d-d012-434c-9c8e-d794cd7d92b4,D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning,0.0952773,"While combining imitation learning (IL) and reinforcement learning (RL) is a
promising way to address poor sample efficiency in autonomous behavior
acquisition, methods that do so typically assume that the requisite behavior
demonstrations are provided by an expert that behaves optimally with respect to
a task reward. If, however, suboptimal demonstrations are provided, a
fundamental challenge appears in that the demonstration-matching objective of
IL conflicts with the return-maximization objective of RL. This paper
introduces D-Shape, a new method for combining IL and RL that uses ideas from
reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape
allows learning from suboptimal demonstrations while retaining the ability to
find the optimal policy with respect to the task reward. We experimentally
validate D-Shape in sparse-reward gridworld domains, showing that it both
improves over RL in terms of sample efficiency and converges consistently to
the optimal policy in the presence of suboptimal demonstrations.",None,-1
8d9899b1-68ee-437a-a951-a0c923e1efb3,PanoDepth: A Two-Stage Approach for Monocular Omnidirectional Depth Estimation,0.480173,"Omnidirectional 3D information is essential for a wide range of applications
such as Virtual Reality, Autonomous Driving, Robotics, etc. In this paper, we
propose a novel, model-agnostic, two-stage pipeline for omnidirectional
monocular depth estimation. Our proposed framework PanoDepth takes one 360
image as input, produces one or more synthesized views in the first stage, and
feeds the original image and the synthesized images into the subsequent stereo
matching stage. In the second stage, we propose a differentiable Spherical
Warping Layer to handle omnidirectional stereo geometry efficiently and
effectively. By utilizing the explicit stereo-based geometric constraints in
the stereo matching stage, PanoDepth can generate dense high-quality depth. We
conducted extensive experiments and ablation studies to evaluate PanoDepth with
both the full pipeline as well as the individual modules in each stage. Our
results show that PanoDepth outperforms the state-of-the-art approaches by a
large margin for 360 monocular depth estimation.",None,-1
f76fd05d-f5f8-401e-be4e-45736413a462,A Framework for Multi-stage Bonus Allocation in meal delivery Platform,0.714977,"Online meal delivery is undergoing explosive growth, as this service is
becoming increasingly popular. A meal delivery platform aims to provide
excellent and stable services for customers and restaurants. However, in
reality, several hundred thousand orders are canceled per day in the Meituan
meal delivery platform since they are not accepted by the crowd soucing
drivers. The cancellation of the orders is incredibly detrimental to the
customer's repurchase rate and the reputation of the Meituan meal delivery
platform. To solve this problem, a certain amount of specific funds is provided
by Meituan's business managers to encourage the crowdsourcing drivers to accept
more orders. To make better use of the funds, in this work, we propose a
framework to deal with the multi-stage bonus allocation problem for a meal
delivery platform. The objective of this framework is to maximize the number of
accepted orders within a limited bonus budget. This framework consists of a
semi-black-box acceptance probability model, a Lagrangian dual-based dynamic
programming algorithm, and an online allocation algorithm. The semi-black-box
acceptance probability model is employed to forecast the relationship between
the bonus allocated to order and its acceptance probability, the Lagrangian
dual-based dynamic programming algorithm aims to calculate the empirical
Lagrangian multiplier for each allocation stage offline based on the historical
data set, and the online allocation algorithm uses the results attained in the
offline part to calculate a proper delivery bonus for each order. To verify the
effectiveness and efficiency of our framework, both offline experiments on a
real-world data set and online A/B tests on the Meituan meal delivery platform
are conducted. Our results show that using the proposed framework, the total
order cancellations can be decreased by more than 25\% in reality.",None,-1
a7c73e1a-5fb8-4e47-9972-c2a34b9c78fa,From Examples to Rules: Neural Guided Rule Synthesis for Information Extraction,0.0379499,"While deep learning approaches to information extraction have had many
successes, they can be difficult to augment or maintain as needs shift.
Rule-based methods, on the other hand, can be more easily modified. However,
crafting rules requires expertise in linguistics and the domain of interest,
making it infeasible for most users. Here we attempt to combine the advantages
of these two directions while mitigating their drawbacks. We adapt recent
advances from the adjacent field of program synthesis to information
extraction, synthesizing rules from provided examples. We use a
transformer-based architecture to guide an enumerative search, and show that
this reduces the number of steps that need to be explored before a rule is
found. Further, we show that without training the synthesis algorithm on the
specific domain, our synthesized rules achieve state-of-the-art performance on
the 1-shot scenario of a task that focuses on few-shot learning for relation
classification, and competitive performance in the 5-shot scenario.",https://github.com/clulab/releases/tree/master/lrec2022-odinsynth,-1
