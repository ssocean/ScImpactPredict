id,title,TNCSI,abstract,OA,authors_title
db60f15d-a654-4627-8c23-49e5f5fce5ad,Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport,0.230818,"Optimal Transport (OT) problem investigates a transport map that bridges two
distributions while minimizing a given cost function. In this regard, OT
between tractable prior distribution and data has been utilized for generative
modeling tasks. However, OT-based methods are susceptible to outliers and face
optimization challenges during training. In this paper, we propose a novel
generative model based on the semi-dual formulation of Unbalanced Optimal
Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution
matching. This approach provides better robustness against outliers, stability
during training, and faster convergence. We validate these properties
empirically through experiments. Moreover, we study the theoretical upper-bound
of divergence between distributions in UOT. Our model outperforms existing
OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 6.36
on CelebA-HQ-256. The code is available at
\url{https://github.com/Jae-Moo/UOTM}.",None,-1
503a5a0f-0fed-44b7-b2cb-f1a15d2dbda0,Safe Reinforcement Learning via Probabilistic Logic Shields,0.690326,"Safe Reinforcement learning (Safe RL) aims at learning optimal policies while
staying safe. A popular solution to Safe RL is shielding, which uses a logical
safety specification to prevent an RL agent from taking unsafe actions.
However, traditional shielding techniques are difficult to integrate with
continuous, end-to-end deep RL methods. To this end, we introduce Probabilistic
Logic Policy Gradient (PLPG). PLPG is a model-based Safe RL technique that uses
probabilistic logic programming to model logical safety constraints as
differentiable functions. Therefore, PLPG can be seamlessly applied to any
policy gradient algorithm while still providing the same convergence
guarantees. In our experiments, we show that PLPG learns safer and more
rewarding policies compared to other state-of-the-art shielding techniques.",None,-1
2085b21d-f02e-4e28-a895-3c2f39c59b9b,NeAT: Neural Artistic Tracing for Beautiful Style Transfer,0.398484,"Style transfer is the task of reproducing the semantic contents of a source
image in the artistic style of a second target image. In this paper, we present
NeAT, a new state-of-the art feed-forward style transfer method. We
re-formulate feed-forward style transfer as image editing, rather than image
generation, resulting in a model which improves over the state-of-the-art in
both preserving the source content and matching the target style. An important
component of our model's success is identifying and fixing ""style halos"", a
commonly occurring artefact across many style transfer techniques. In addition
to training and testing on standard datasets, we introduce the BBST-4M dataset,
a new, large scale, high resolution dataset of 4M images. As a component of
curating this data, we present a novel model able to classify if an image is
stylistic. We use BBST-4M to improve and measure the generalization of NeAT
across a huge variety of styles. Not only does NeAT offer state-of-the-art
quality and generalization, it is designed and trained for fast inference at
high resolution.",None,-1
cd895266-672d-405f-8488-0eb2a2889cc6,"Presence of informal language, such as emoticons, hashtags, and slang, impact the performance of sentiment analysis models on social media text?",0.150183,"This study aimed to investigate the influence of the presence of informal
language, such as emoticons and slang, on the performance of sentiment analysis
models applied to social media text. A convolutional neural network (CNN) model
was developed and trained on three datasets: a sarcasm dataset, a sentiment
dataset, and an emoticon dataset. The model architecture was held constant for
all experiments and the model was trained on 80% of the data and tested on 20%.
The results revealed that the model achieved an accuracy of 96.47% on the
sarcasm dataset, with the lowest accuracy for class 1. On the sentiment
dataset, the model achieved an accuracy of 95.28%. The amalgamation of sarcasm
and sentiment datasets improved the accuracy of the model to 95.1%, and the
addition of emoticon dataset has a slight positive impact on the accuracy of
the model to 95.37%. The study suggests that the presence of informal language
has a restricted impact on the performance of sentiment analysis models applied
to social media text. However, the inclusion of emoticon data to the model can
enhance the accuracy slightly.",None,-1
ff62ba9c-abbd-4147-a6ed-60935f356c1f,MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions,0.684426,"Audio-driven portrait animation aims to synthesize portrait videos that are
conditioned by given audio. Animating high-fidelity and multimodal video
portraits has a variety of applications. Previous methods have attempted to
capture different motion modes and generate high-fidelity portrait videos by
training different models or sampling signals from given videos. However,
lacking correlation learning between lip-sync and other movements (e.g., head
pose/eye blinking) usually leads to unnatural results. In this paper, we
propose a unified system for multi-person, diverse, and high-fidelity talking
portrait generation. Our method contains three stages, i.e., 1) Mapping-Once
network with Dual Attentions (MODA) generates talking representation from given
audio. In MODA, we design a dual-attention module to encode accurate mouth
movements and diverse modalities. 2) Facial composer network generates dense
and detailed face landmarks, and 3) temporal-guided renderer syntheses stable
videos. Extensive evaluations demonstrate that the proposed system produces
more natural and realistic video portraits compared to previous methods.",None,-1
b453c70b-4baa-4328-8924-ce624b75f985,Time out of Mind: Generating Rate of Speech conditioned on emotion and speaker,0.119054,"Voice synthesis has seen significant improvements in the past decade
resulting in highly intelligible voices. Further investigations have resulted
in models that can produce variable speech, including conditional emotional
expression. The problem lies, however, in a focus on phrase-level modifications
and prosodic vocal features. Using the CREMA-D dataset we have trained a GAN
conditioned on emotion to generate worth lengths for a given input text. These
word lengths are relative to neutral speech and can be provided, through speech
synthesis markup language (SSML) to a text-to-speech (TTS) system to generate
more expressive speech. Additionally, a generative model is also trained using
implicit maximum likelihood estimation (IMLE) and a comparative analysis with
GANs is included. We were able to achieve better performances on objective
measures for neutral speech, and better time alignment for happy speech when
compared to an out-of-box model. However, further investigation of subjective
evaluation is required.",None,-1
876d7bbf-fa6a-44b0-868e-9e422a8887fb,HGDNet: A Height-Hierarchy Guided Dual-Decoder Network for Single View Building Extraction and Height Estimation,0.684299,"Unifying the correlative single-view satellite image building extraction and
height estimation tasks indicates a promising way to share representations and
acquire generalist model for large-scale urban 3D reconstruction. However, the
common spatial misalignment between building footprints and
stereo-reconstructed nDSM height labels incurs degraded performance on both
tasks. To address this issue, we propose a Height-hierarchy Guided Dual-decoder
Network (HGDNet) to estimate building height. Under the guidance of synthesized
discrete height-hierarchy nDSM, auxiliary height-hierarchical building
extraction branch enhance the height estimation branch with implicit
constraints, yielding an accuracy improvement of more than 6% on the DFC 2023
track2 dataset. Additional two-stage cascade architecture is adopted to achieve
more accurate building extraction. Experiments on the DFC 2023 Track 2 dataset
shows the superiority of the proposed method in building height estimation
({\delta}1:0.8012), instance extraction (AP50:0.7730), and the final average
score 0.7871 ranks in the first place in test phase.",None,-1
2f3ccafd-db8a-4724-bef9-ad1c649e403d,"Meta-Diversity Search in Complex Systems, A Recipe for Artificial Open-Endedness ?",0.471738,"Can we build an artificial system that would be able to generate endless
surprises if ran ""forever"" in Minecraft? While there is not a single path
toward solving that grand challenge, this article presents what we believe to
be some working ingredients for the endless generation of novel increasingly
complex artifacts in Minecraft. Our framework for an open-ended system includes
two components: a complex system used to recursively grow and complexify
artifacts over time, and a discovery algorithm that leverages the concept of
meta-diversity search. Since complex systems have shown to enable the emergence
of considerable complexity from set of simple rules, we believe them to be
great candidates to generate all sort of artifacts in Minecraft. Yet, the space
of possible artifacts that can be generated by these systems is often unknown,
challenging to characterize and explore. Therefore automating the long-term
discovery of novel and increasingly complex artifacts in these systems is an
exciting research field. To approach these challenges, we formulate the problem
of meta-diversity search where an artificial ""discovery assistant""
incrementally learns a diverse set of representations to characterize behaviors
and searches to discover diverse patterns within each of them. A successful
discovery assistant should continuously seek for novel sources of diversities
while being able to quickly specialize the search toward a new unknown type of
diversity. To implement those ideas in the Minecraft environment, we simulate
an artificial ""chemistry"" system based on Lenia continuous cellular automaton
for generating artifacts, as well as an artificial ""discovery assistant""
(called Holmes) for the artifact-discovery process. Holmes incrementally learns
a hierarchy of modular representations to characterize divergent sources of
diversity and uses a goal-based intrinsically-motivated exploration as the
diversity search strategy.",None,-1
97a23dce-67bc-4eb8-a4f5-e4ac63b8b27d,Diffusion Models for Imperceptible and Transferable Adversarial Attack,0.831842,"Many existing adversarial attacks generate $L_p$-norm perturbations on image
RGB space. Despite some achievements in transferability and attack success
rate, the crafted adversarial examples are easily perceived by human eyes.
Towards visual imperceptibility, some recent works explore unrestricted attacks
without $L_p$-norm constraints, yet lacking transferability of attacking
black-box models. In this work, we propose a novel imperceptible and
transferable attack by leveraging both the generative and discriminative power
of diffusion models. Specifically, instead of direct manipulation in pixel
space, we craft perturbations in the latent space of diffusion models. Combined
with well-designed content-preserving structures, we can generate
human-insensitive perturbations embedded with semantic clues. For better
transferability, we further ""deceive"" the diffusion model which can be viewed
as an implicit recognition surrogate, by distracting its attention away from
the target regions. To our knowledge, our proposed method, DiffAttack, is the
first that introduces diffusion models into the adversarial attack field.
Extensive experiments on various model structures, datasets, and defense
methods have demonstrated the superiority of our attack over the existing
attack methods.",None,-1
09fb1d19-8400-4056-b228-fdbcbd0b59ad,Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?,0.807754,"Large Language Models (LLMs) are advancing at a rapid pace, with significant
improvements at natural language processing and coding tasks. Yet, their
ability to work with formal languages representing data, specifically within
the realm of knowledge graph engineering, remains under-investigated. To
evaluate the proficiency of various LLMs, we created a set of five tasks that
probe their ability to parse, understand, analyze, and create knowledge graphs
serialized in Turtle syntax. These tasks, each embodying distinct degrees of
complexity and being able to scale with the size of the problem, have been
integrated into our automated evaluation system, the LLM-KG-Bench. The
evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,
Claude 1.3, and Claude 2.0, as well as two freely accessible offline models,
GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth
understanding of the strengths and shortcomings of LLMs in relation to their
application within RDF knowledge graph engineering workflows utilizing Turtle
representation. While our findings show that the latest commercial models
outperform their forerunners in terms of proficiency with the Turtle language,
they also reveal an apparent weakness. These models fall short when it comes to
adhering strictly to the output formatting constraints, a crucial requirement
in this context.",None,-1
2b8cd9f0-18dd-4786-9f28-8d6066b18e3f,Multi-Modal Evaluation Approach for Medical Image Segmentation,0.0768659,"Manual segmentation of medical images (e.g., segmenting tumors in CT scans)
is a high-effort task that can be accelerated with machine learning techniques.
However, selecting the right segmentation approach depends on the evaluation
function, particularly in medical image segmentation where we must deal with
dependency between voxels. For instance, in contrast to classical systems where
the predictions are either correct or incorrect, predictions in medical image
segmentation may be partially correct and incorrect simultaneously. In this
paper, we explore this expressiveness to extract the useful properties of these
systems and formally define a novel multi-modal evaluation (MME) approach to
measure the effectiveness of different segmentation methods. This approach
improves the segmentation evaluation by introducing new relevant and
interpretable characteristics, including detection property, boundary
alignment, uniformity, total volume, and relative volume. Our proposed approach
is open-source and publicly available for use. We have conducted several
reproducible experiments, including the segmentation of pancreas, liver tumors,
and multi-organs datasets, to show the applicability of the proposed approach.",None,-1
3d735731-4fbe-49cb-b370-4fc2c5bf8a44,KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment,0.991822,"Recent legislation of the ""right to be forgotten"" has led to the interest in
machine unlearning, where the learned models are endowed with the function to
forget information about specific training instances as if they have never
existed in the training set. Previous work mainly focuses on computer vision
scenarios and largely ignores the essentials of unlearning in NLP field, where
text data contains more explicit and sensitive personal information than
images. In this paper, we propose a general unlearning framework called KGA to
induce forgetfulness. Different from previous work that tries to recover
gradients or forces models to perform close to one specific distribution, KGA
maintains distribution differences (i.e., knowledge gap). This relaxes the
distribution assumption. Furthermore, we first apply the unlearning method to
various NLP tasks (i.e., classification, translation, response generation) and
propose several unlearning evaluation metrics with pertinence. Experiments on
large-scale datasets show that KGA yields comprehensive improvements over
baselines, where extensive analyses further validate the effectiveness of KGA
and provide insight into unlearning for NLP tasks.",None,-1
4e6177e5-c74b-499f-9079-5f162d52db32,Introduction to Presentation Attacks in Signature Biometrics and Recent Advances,0.0710573,"Applications based on biometric authentication have received a lot of
interest in the last years due to the breathtaking results obtained using
personal traits such as face or fingerprint. However, it is important not to
forget that these biometric systems have to withstand different types of
possible attacks. This chapter carries out an analysis of different
Presentation Attack (PA) scenarios for on-line handwritten signature
verification. The main contributions of this chapter are: i) an updated
overview of representative methods for Presentation Attack Detection (PAD) in
signature biometrics; ii) a description of the different levels of PAs existing
in on-line signature verification regarding the amount of information available
to the impostor, as well as the training, effort, and ability to perform the
forgeries; and iii) an evaluation of the system performance in signature
biometrics under different scenarios considering recent publicly available
signature databases, DeepSignDB and SVC2021_EvalDB. This work is in line with
recent efforts in the Common Criteria standardization community towards
security evaluation of biometric systems.",None,-1
45cb1afc-831b-4f33-a75e-5929299afdcc,Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints,0.0612848,"Neural QCFG is a grammar-based sequence-tosequence (seq2seq) model with
strong inductive biases on hierarchical structures. It excels in
interpretability and generalization but suffers from expensive inference. In
this paper, we study two low-rank variants of Neural QCFG for faster inference
with different trade-offs between efficiency and expressiveness. Furthermore,
utilizing the symbolic interface provided by the grammar, we introduce two soft
constraints over tree hierarchy and source coverage. We experiment with various
datasets and find that our models outperform vanilla Neural QCFG in most
settings.",None,-1
3a3a2345-575d-4857-9cad-20f4d3d93b10,RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation,0.21484,"Glass-like objects are widespread in daily life but remain intractable to be
segmented for most existing methods. The transparent property makes it
difficult to be distinguished from background, while the tiny separation
boundary further impedes the acquisition of their exact contour. In this paper,
by revealing the key co-evolution demand of semantic and boundary learning, we
propose a Selective Mutual Evolution (SME) module to enable the reciprocal
feature learning between them. Then to exploit the global shape context, we
propose a Structurally Attentive Refinement (SAR) module to conduct a
fine-grained feature refinement for those ambiguous points around the boundary.
Finally, to further utilize the multi-scale representation, we integrate the
above two modules into a cascaded structure and then introduce a Reciprocal
Feature Evolution Network (RFENet) for effective glass-like object
segmentation. Extensive experiments demonstrate that our RFENet achieves
state-of-the-art performance on three popular public datasets.",None,-1
0ff7e8e7-7023-48a4-9f10-3e60d89231ff,Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,0.926908,"Most unsupervised image anomaly localization methods suffer from
overgeneralization because of the high generalization abilities of
convolutional neural networks, leading to unreliable predictions. To mitigate
the overgeneralization, this study proposes to collaboratively optimize normal
and abnormal feature distributions with the assistance of synthetic anomalies,
namely collaborative discrepancy optimization (CDO). CDO introduces a margin
optimization module and an overlap optimization module to optimize the two key
factors determining the localization performance, i.e., the margin and the
overlap between the discrepancy distributions (DDs) of normal and abnormal
samples. With CDO, a large margin and a small overlap between normal and
abnormal DDs are obtained, and the prediction reliability is boosted.
Experiments on MVTec2D and MVTec3D show that CDO effectively mitigates the
overgeneralization and achieves great anomaly localization performance with
real-time computation efficiency. A real-world automotive plastic parts
inspection application further demonstrates the capability of the proposed CDO.
Code is available on https://github.com/caoyunkang/CDO.",None,-1
4b7d1b2b-f5b1-4760-a28b-1c7490d31fc7,Achieving Long-term Fairness in Submodular Maximization through Randomization,0.290776,"Submodular function optimization has numerous applications in machine
learning and data analysis, including data summarization which aims to identify
a concise and diverse set of data points from a large dataset. It is important
to implement fairness-aware algorithms when dealing with data items that may
contain sensitive attributes like race or gender, to prevent biases that could
lead to unequal representation of different groups. With this in mind, we
investigate the problem of maximizing a monotone submodular function while
meeting group fairness constraints. Unlike previous studies in this area, we
allow for randomized solutions, with the objective being to calculate a
distribution over feasible sets such that the expected number of items selected
from each group is subject to constraints in the form of upper and lower
thresholds, ensuring that the representation of each group remains balanced in
the long term. Here a set is considered feasible if its size does not exceed a
constant value of $b$. Our research includes the development of a series of
approximation algorithms for this problem.",None,-1
bc03dcd7-d75e-49cc-ab2a-ca6228a6c5db,Energy Efficiency of Training Neural Network Architectures: An Empirical Study,0.524001,"The evaluation of Deep Learning models has traditionally focused on criteria
such as accuracy, F1 score, and related measures. The increasing availability
of high computational power environments allows the creation of deeper and more
complex models. However, the computations needed to train such models entail a
large carbon footprint. In this work, we study the relations between DL model
architectures and their environmental impact in terms of energy consumed and
CO$_2$ emissions produced during training by means of an empirical study using
Deep Convolutional Neural Networks. Concretely, we study: (i) the impact of the
architecture and the location where the computations are hosted on the energy
consumption and emissions produced; (ii) the trade-off between accuracy and
energy efficiency; and (iii) the difference on the method of measurement of the
energy consumed using software-based and hardware-based tools.",None,-1
06be7c24-10aa-4172-9653-715e3ec0c180,Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network and Self-Attention Mechanism,0.0998625,"Instrument playing technique (IPT) is a key element of musical presentation.
However, most of the existing works for IPT detection only concern monophonic
music signals, yet little has been done to detect IPTs in polyphonic
instrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we
formulate it as a frame-level multi-label classification problem and apply it
to Guzheng, a Chinese plucked string instrument. We create a new dataset,
Guzheng\_Tech99, containing Guzheng recordings and onset, offset, pitch, IPT
annotations of each note. Because different IPTs vary a lot in their lengths,
we propose a new method to solve this problem using multi-scale network and
self-attention. The multi-scale network extracts features from different
scales, and the self-attention mechanism applied to the feature maps at the
coarsest scale further enhances the long-range feature extraction. Our approach
outperforms existing works by a large margin, indicating its effectiveness in
IPT detection.",None,-1
ae28d68a-1e44-479f-8860-76decf8620fe,Multi-View Azimuth Stereo via Tangent Space Consistency,0.640484,"We present a method for 3D reconstruction only using calibrated multi-view
surface azimuth maps. Our method, multi-view azimuth stereo, is effective for
textureless or specular surfaces, which are difficult for conventional
multi-view stereo methods. We introduce the concept of tangent space
consistency: Multi-view azimuth observations of a surface point should be
lifted to the same tangent space. Leveraging this consistency, we recover the
shape by optimizing a neural implicit surface representation. Our method
harnesses the robust azimuth estimation capabilities of photometric stereo
methods or polarization imaging while bypassing potentially complex zenith
angle estimation. Experiments using azimuth maps from various sources validate
the accurate shape recovery with our method, even without zenith angles.",None,-1
a0c43641-5ca8-4f58-887f-858e0a13a0c2,Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors,0.66934,"It is vital to infer signed distance functions (SDFs) from 3D point clouds.
The latest methods rely on generalizing the priors learned from large scale
supervision. However, the learned priors do not generalize well to various
geometric variations that are unseen during training, especially for extremely
sparse point clouds. To resolve this issue, we present a neural network to
directly infer SDFs from single sparse point clouds without using signed
distance supervision, learned priors or even normals. Our insight here is to
learn surface parameterization and SDFs inference in an end-to-end manner. To
make up the sparsity, we leverage parameterized surfaces as a coarse surface
sampler to provide many coarse surface estimations in training iterations,
according to which we mine supervision and our thin plate splines (TPS) based
network infers SDFs as smooth functions in a statistical way. Our method
significantly improves the generalization ability and accuracy in unseen point
clouds. Our experimental results show our advantages over the state-of-the-art
methods in surface reconstruction for sparse point clouds under synthetic
datasets and real scans.The code is available at
\url{https://github.com/chenchao15/NeuralTPS}.",None,-1
4d8d4de7-ca23-423f-9bcc-cfa729d5977a,Instance-based Max-margin for Practical Few-shot Recognition,0.0410332,"In order to mimic the human few-shot learning (FSL) ability better and to
make FSL closer to real-world applications, this paper proposes a practical FSL
(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to
human prior knowledge) and recognizes many novel classes simultaneously.
Compared to traditional FSL, pFSL is simpler in its formulation, easier to
evaluate, more challenging and more practical. To cope with the rarity of
training examples, this paper proposes IbM2, an instance-based max-margin
method not only for the new pFSL setting, but also works well in traditional
FSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random
noise applied to the instances into a mechanism to achieve maximum margin in
the many-way pFSL (or traditional FSL) recognition task. Experiments with
various self-supervised pretraining methods and diverse many- or few-way FSL
tasks show that IbM2 almost always leads to improvements compared to its
respective baseline methods, and in most cases the improvements are
significant. With both the new pFSL setting and novel IbM2 method, this paper
shows that practical few-shot learning is both viable and promising.",None,-1
310a657b-04d9-4a82-a4ee-79c034f0aa31,OmniTracker: Unifying Object Tracking by Tracking-with-Detection,0.585013,"Object tracking (OT) aims to estimate the positions of target objects in a
video sequence. Depending on whether the initial states of target objects are
specified by provided annotations in the first frame or the categories, OT
could be classified as instance tracking (e.g., SOT and VOS) and category
tracking (e.g., MOT, MOTS, and VIS) tasks. Combing the advantages of the best
practices developed in both communities, we propose a novel
tracking-with-detection paradigm, where tracking supplements appearance priors
for detection and detection provides tracking with candidate bounding boxes for
association. Equipped with such a design, a unified tracking model,
OmniTracker, is further presented to resolve all the tracking tasks with a
fully shared network architecture, model weights, and inference pipeline.
Extensive experiments on 7 tracking datasets, including LaSOT, TrackingNet,
DAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that OmniTracker achieves
on-par or even better results than both task-specific and unified tracking
models.",None,-1
3e72d740-38b4-42f9-aefd-7f7ac3029ba1,WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents,0.764767,"In this paper, we introduce WeLayout, a novel system for segmenting the
layout of corporate documents, which stands for WeChat Layout Analysis System.
Our approach utilizes a sophisticated ensemble of DINO and YOLO models,
specifically developed for the ICDAR 2023 Competition on Robust Layout
Segmentation. Our method significantly surpasses the baseline, securing a top
position on the leaderboard with a mAP of 70.0. To achieve this performance, we
concentrated on enhancing various aspects of the task, such as dataset
augmentation, model architecture, bounding box refinement, and model ensemble
techniques. Additionally, we trained the data separately for each document
category to ensure a higher mean submission score. We also developed an
algorithm for cell matching to further improve our performance. To identify the
optimal weights and IoU thresholds for our model ensemble, we employed a
Bayesian optimization algorithm called the Tree-Structured Parzen Estimator.
Our approach effectively demonstrates the benefits of combining query-based and
anchor-free models for achieving robust layout segmentation in corporate
documents.",None,-1
3306f17b-daf9-404b-a176-6d1ad67458ca,InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling,0.898925,"Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.",None,-1
3498587d-6307-49fd-9a41-e650df473934,The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features,0.451123,"AI systems have been known to amplify biases in real-world data. Explanations
may help human-AI teams address these biases for fairer decision-making.
Typically, explanations focus on salient input features. If a model is biased
against some protected group, explanations may include features that
demonstrate this bias, but when biases are realized through proxy features, the
relationship between this proxy feature and the protected one may be less clear
to a human. In this work, we study the effect of the presence of protected and
proxy features on participants' perception of model fairness and their ability
to improve demographic parity over an AI alone. Further, we examine how
different treatments -- explanations, model bias disclosure and proxy
correlation disclosure -- affect fairness perception and parity. We find that
explanations help people detect direct but not indirect biases. Additionally,
regardless of bias type, explanations tend to increase agreement with model
biases. Disclosures can help mitigate this effect for indirect biases,
improving both unfairness recognition and decision-making fairness. We hope
that our findings can help guide further research into advancing explanations
in support of fair human-AI decision-making.",None,-1
8fd072bd-29e7-4df6-bc0a-f72fca49f66b,Benchmarking the Generation of Fact Checking Explanations,0.538315,"Fighting misinformation is a challenging, yet crucial, task. Despite the
growing number of experts being involved in manual fact-checking, this activity
is time-consuming and cannot keep up with the ever-increasing amount of Fake
News produced daily. Hence, automating this process is necessary to help curb
misinformation. Thus far, researchers have mainly focused on claim veracity
classification. In this paper, instead, we address the generation of
justifications (textual explanation of why a claim is classified as either true
or false) and benchmark it with novel datasets and advanced baselines. In
particular, we focus on summarization approaches over unstructured knowledge
(i.e. news articles) and we experiment with several extractive and abstractive
strategies. We employed two datasets with different styles and structures, in
order to assess the generalizability of our findings. Results show that in
justification production summarization benefits from the claim information,
and, in particular, that a claim-driven extractive step improves abstractive
summarization performances. Finally, we show that although cross-dataset
experiments suffer from performance degradation, a unique model trained on a
combination of the two datasets is able to retain style information in an
efficient manner.",None,-1
518e56c2-a5c4-4013-80a8-2f31bce667ec,Spatio-Temporal AU Relational Graph Representation Learning For Facial Action Units Detection,0.933374,"This paper presents our Facial Action Units (AUs) detection submission to the
fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our approach
consists of three main modules: (i) a pre-trained facial representation encoder
which produce a strong facial representation from each input face image in the
input sequence; (ii) an AU-specific feature generator that specifically learns
a set of AU features from each facial representation; and (iii) a
spatio-temporal graph learning module that constructs a spatio-temporal graph
representation. This graph representation describes AUs contained in all frames
and predicts the occurrence of each AU based on both the modeled spatial
information within the corresponding face and the learned temporal dynamics
among frames. The experimental results show that our approach outperformed the
baseline and the spatio-temporal graph representation learning allows our model
to generate the best results among all ablated systems. Our model ranks at the
4th place in the AU recognition track at the 5th ABAW Competition. Our code is
publicly available at https://github.com/wzh125/ABAW-5.",None,-1
49751f1c-95a8-40dc-b647-ee3c430a2a6e,Generating Data for Symbolic Language with Large Language Models,0.224437,"While large language models (LLMs) bring not only performance but also
complexity, recent work has started to turn LLMs into data generators rather
than task inferencers, where another affordable task model is trained for
efficient deployment and inference. However, such an approach has primarily
been applied to natural language tasks and has not yet been explored for
symbolic language tasks with complex structured outputs (e.g., semantic parsing
and code generation). In this paper, we propose SymGen which utilizes LLMs for
generating various annotation-expensive symbolic language data. SymGen consists
of an informative prompt to steer generation and an agreement-based verifier to
improve data correctness. We conduct extensive experiments on six symbolic
language tasks across various settings. Compared with the LLMs, we demonstrate
the 1\%-sized task model can achieve comparable or better performance, largely
cutting inference and deployment costs. We also show that generated data with
only a few human demonstrations can be as effective as over 10 times the amount
of human-annotated data when training the task model, saving a considerable
amount of annotation effort. SymGen sheds new light on data generation for
complex tasks, and we release the code at
\href{https://github.com/HKUNLP/SymGen}{https://github.com/HKUNLP/SymGen}.",None,-1
6050d5a2-cbd9-46cd-b58e-1b20303e77d3,AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling,0.699167,"Business optimisation refers to the process of finding and implementing
efficient and cost-effective means of operation to bring a competitive
advantage for businesses. Synthesizing problem formulations is an integral part
of business optimisation, which relies on human expertise to construct problem
formulations using optimisation languages. Interestingly, with advancements in
Large Language Models (LLMs), the human expertise needed in problem formulation
can be minimized. However, developing an LLM for problem formulation is
challenging, due to training data, token limitations, and lack of appropriate
performance metrics. For the requirement of training data, recent attention has
been directed towards fine-tuning pre-trained LLMs for downstream tasks rather
than training an LLM from scratch for a specific task. In this paper, we adopt
an LLM fine-tuning approach and propose an AI-Copilot for business optimisation
problem formulation. For token limitations, we introduce modularization and
prompt engineering techniques to synthesize complex problem formulations as
modules that fit into the token limits of LLMs. Additionally, we design
performance evaluation metrics that are better suited for assessing the
accuracy and quality of problem formulations. The experiment results
demonstrate that with this approach we can synthesize complex and large problem
formulations for a typical business optimisation problem in production
scheduling.",None,-1
80c17c1e-7cbf-41a9-bfb0-9de4f9e148d6,NeuralKG-ind: A Python Library for Inductive Knowledge Graph Representation Learning,0.224397,"Since the dynamic characteristics of knowledge graphs, many inductive
knowledge graph representation learning (KGRL) works have been proposed in
recent years, focusing on enabling prediction over new entities. NeuralKG-ind
is the first library of inductive KGRL as an important update of NeuralKG
library. It includes standardized processes, rich existing methods, decoupled
modules, and comprehensive evaluation metrics. With NeuralKG-ind, it is easy
for researchers and engineers to reproduce, redevelop, and compare inductive
KGRL methods. The library, experimental methodologies, and model
re-implementing results of NeuralKG-ind are all publicly released at
https://github.com/zjukg/NeuralKG/tree/ind .",None,-1
a0e0bf4d-3830-45e7-8985-2295a3342999,Robustness Verification for Knowledge-Based Logic of Risky Driving Scenes,0.294035,"Many decision-making scenarios in modern life benefit from the decision
support of artificial intelligence algorithms, which focus on a data-driven
philosophy and automated programs or systems. However, crucial decision issues
related to security, fairness, and privacy should consider more human knowledge
and principles to supervise such AI algorithms to reach more proper solutions
and to benefit society more effectively. In this work, we extract
knowledge-based logic that defines risky driving formats learned from public
transportation accident datasets, which haven't been analyzed in detail to the
best of our knowledge. More importantly, this knowledge is critical for
recognizing traffic hazards and could supervise and improve AI models in
safety-critical systems. Then we use automated verification methods to verify
the robustness of such logic. More specifically, we gather 72 accident datasets
from Data.gov and organize them by state. Further, we train Decision Tree and
XGBoost models on each state's dataset, deriving accident judgment logic.
Finally, we deploy robustness verification on these tree-based models under
multiple parameter combinations.",None,-1
5de9e5b9-cc3c-43f0-a156-82c775f6c8c0,VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining,0.0744134,"In this paper, we describe VivesDebate-Speech, a corpus of spoken
argumentation created to leverage audio features for argument mining tasks. The
creation of this corpus represents an important contribution to the
intersection of speech processing and argument mining communities, and one of
the most complete publicly available resources in this topic. Moreover, we have
performed a set of first-of-their-kind experiments which show an improvement
when integrating audio features into the argument mining pipeline. The provided
results can be used as a baseline for future research.",None,-1
27f1923f-4583-453d-9364-eca23ea6715d,Context-faithful Prompting for Large Language Models,0.210835,"Large language models (LLMs) encode parametric knowledge about world facts
and have shown remarkable performance in knowledge-driven NLP tasks. However,
their reliance on parametric knowledge may cause them to overlook contextual
cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,
knowledge acquisition tasks). In this paper, we seek to assess and enhance
LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction
with abstention. We demonstrate that LLMs' faithfulness can be significantly
improved using carefully designed prompting strategies. In particular, we
identify opinion-based prompts and counterfactual demonstrations as the most
effective methods. Opinion-based prompts reframe the context as a narrator's
statement and inquire about the narrator's opinions, while counterfactual
demonstrations use instances containing false facts to improve faithfulness in
knowledge conflict situations. Neither technique requires additional training.
We conduct experiments on three datasets of two standard NLP tasks, machine
reading comprehension and relation extraction, and the results demonstrate
significant improvement in faithfulness to contexts. Code and data are released
at https://github.com/wzhouad/context-faithful-llm.",None,-1
0832cf03-fe1d-4468-abd5-2d790e080a4c,InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery,0.994606,"The rapid evolution of artificial intelligence in drug discovery encounters
challenges with generalization and extensive training, yet Large Language
Models (LLMs) offer promise in reshaping interactions with complex molecular
data. Our novel contribution, InstructMol, a multi-modal LLM, effectively
aligns molecular structures with natural language via an instruction-tuning
approach, utilizing a two-stage training strategy that adeptly combines limited
domain-specific data with molecular and textual information. InstructMol
showcases substantial performance improvements in drug discovery-related
molecular tasks, surpassing leading LLMs and significantly reducing the gap
with specialized models, thereby establishing a robust foundation for a
versatile and dependable drug discovery assistant.",None,-1
45ae6077-1ebd-4dad-8c03-2ef34eb10a3c,GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,0.999874,"Multi-query attention (MQA), which only uses a single key-value head,
drastically speeds up decoder inference. However, MQA can lead to quality
degradation, and moreover it may not be desirable to train a separate model
just for faster inference. We (1) propose a recipe for uptraining existing
multi-head language model checkpoints into models with MQA using 5% of original
pre-training compute, and (2) introduce grouped-query attention (GQA), a
generalization of multi-query attention which uses an intermediate (more than
one, less than number of query heads) number of key-value heads. We show that
uptrained GQA achieves quality close to multi-head attention with comparable
speed to MQA.",None,-1
d7d393cd-d49b-4531-88b3-8d0adec79f06,Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network,0.532092,"Recently, the open-vocabulary semantic segmentation problem has attracted
increasing attention and the best performing methods are based on two-stream
networks: one stream for proposal mask generation and the other for segment
classification using a pretrained visual-language model. However, existing
two-stream methods require passing a great number of (up to a hundred) image
crops into the visual-language model, which is highly inefficient. To address
the problem, we propose a network that only needs a single pass through the
visual-language model for each input image. Specifically, we first propose a
novel network adaptation approach, termed patch severance, to restrict the
harmful interference between the patch embeddings in the pre-trained visual
encoder. We then propose classification anchor learning to encourage the
network to spatially focus on more discriminative features for classification.
Extensive experiments demonstrate that the proposed method achieves outstanding
performance, surpassing state-of-the-art methods while being 4 to 7 times
faster at inference. Code: https://github.com/CongHan0808/DeOP.git",None,-1
de74be41-ea61-4451-8a58-4b60cd12a6cb,Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation,0.0382688,"Although procedural generation is popular among game developers, academic
research on the topic has primarily focused on new applications, with some
research into empirical analysis. In this paper we relate theoretical work in
information theory to the generation of content for games. We prove that there
is a relationship between the Kolomogorov complexity of the most complex
artifact a generator can produce, and the size of that generator's possibility
space. In doing so, we identify the limiting relationship between the knowledge
encoded in a generator, the density of its output space, and the intricacy of
the artifacts it produces. We relate our result to the experience of expert
procedural generator designers, and illustrate it with some examples.",None,-1
ccfe715f-28c0-4354-9aa1-0dc1d8721a9d,Exploring the Compositional Generalization in Context Dependent Text-to-SQL Parsing,0.0955333,"In the context-dependent Text-to-SQL task, the generated SQL statements are
refined iteratively based on the user input utterance from each interaction.
The input text from each interaction can be viewed as component modifications
to the previous SQL statements, which could be further extracted as the
modification patterns. Since these modification patterns could also be combined
with other SQL statements, the models are supposed to have the compositional
generalization to these novel combinations. This work is the first exploration
of compositional generalization in context-dependent Text-to-SQL scenarios. To
facilitate related studies, we constructed two challenging benchmarks named
\textsc{CoSQL-CG} and \textsc{SParC-CG} by recombining the modification
patterns and existing SQL statements. The following experiments show that all
current models struggle on our proposed benchmarks. Furthermore, we found that
better aligning the previous SQL statements with the input utterance could give
models better compositional generalization ability. Based on these
observations, we propose a method named \texttt{p-align} to improve the
compositional generalization of Text-to-SQL models. Further experiments
validate the effectiveness of our method. Source code and data are available.",None,-1
75874c14-a8d1-477d-bfa2-efa4a26fe71a,ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation,0.779296,"Joint speech-language training is challenging due to the large demand for
training data and GPU consumption, as well as the modality gap between speech
and language. We present ComSL, a speech-language model built atop a composite
architecture of public pretrained speech-only and language-only models and
optimized data-efficiently for spoken language tasks. Particularly, we propose
to incorporate cross-modality learning into transfer learning and conduct them
simultaneously for downstream tasks in a multi-task learning manner. Our
approach has demonstrated effectiveness in end-to-end speech-to-text
translation tasks, achieving a new state-of-the-art average BLEU score of 31.5
on the multilingual speech to English text translation task for 21 languages,
as measured on the public CoVoST2 evaluation set.",None,-1
6e4437f6-b08a-4f92-a326-d9f3ee46bddc,A novel efficient Multi-view traffic-related object detection framework,0.684739,"With the rapid development of intelligent transportation system applications,
a tremendous amount of multi-view video data has emerged to enhance vehicle
perception. However, performing video analytics efficiently by exploiting the
spatial-temporal redundancy from video data remains challenging. Accordingly,
we propose a novel traffic-related framework named CEVAS to achieve efficient
object detection using multi-view video data. Briefly, a fine-grained input
filtering policy is introduced to produce a reasonable region of interest from
the captured images. Also, we design a sharing object manager to manage the
information of objects with spatial redundancy and share their results with
other vehicles. We further derive a content-aware model selection policy to
select detection methods adaptively. Experimental results show that our
framework significantly reduces response latency while achieving the same
detection accuracy as the state-of-the-art methods.",None,-1
e65540a5-6ee9-4323-b689-d0fe32fd2b97,UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations,0.104981,"Language technologies that accurately model the dynamics of events must
perform commonsense reasoning. Existing work evaluating commonsense reasoning
focuses on making inferences about common, everyday situations. To instead
investigate the ability to model unusual, unexpected, and unlikely situations,
we explore the task of uncommonsense abductive reasoning. Given a piece of
context with an unexpected outcome, this task requires reasoning abductively to
generate an explanation that makes the unexpected outcome more likely in the
context. To this end, we curate and release a new English language corpus
called UNcommonsense. We characterize the performance differences between human
explainers and the best-performing large language models, finding that
model-enhanced human-written explanations achieve the highest quality by
trading off between specificity and diversity. Finally, we experiment with
several imitation learning algorithms to train open and accessible language
models on this task. When compared with the vanilla supervised fine-tuning
approach, these methods consistently reduce lose rates on both common and
uncommonsense abductive reasoning judged by human evaluators.",None,-1
7f44238e-236f-4a4c-9794-cf20a8254435,Multi3DRefer: Grounding Text Description to Multiple 3D Objects,0.728358,"We introduce the task of localizing a flexible number of objects in
real-world 3D scenes using natural language descriptions. Existing 3D visual
grounding tasks focus on localizing a unique object given a text description.
However, such a strict setting is unnatural as localizing potentially multiple
objects is a common need in real-world scenarios and robotic tasks (e.g.,
visual navigation and object rearrangement). To address this setting we propose
Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains
61926 descriptions of 11609 objects, where zero, single or multiple target
objects are referenced by each description. We also introduce a new evaluation
metric and benchmark methods from prior work to enable further investigation of
multi-modal 3D scene understanding. Furthermore, we develop a better baseline
leveraging 2D features from CLIP by rendering object proposals online with
contrastive learning, which outperforms the state of the art on the ScanRefer
benchmark.",None,-1
f535710c-5332-49ac-b25f-906302e935bc,Can Language Models Laugh at YouTube Short-form Videos?,0.288684,"As short-form funny videos on social networks are gaining popularity, it
becomes demanding for AI models to understand them for better communication
with humans. Unfortunately, previous video humor datasets target specific
domains, such as speeches or sitcoms, and mostly focus on verbal cues. We
curate a user-generated dataset of 10K multimodal funny videos from YouTube,
called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both
verbal and visual elements contributing to humor. After filtering, we annotate
each video with timestamps and text explanations for funny moments. Our
ExFunTube is unique over existing datasets in that our videos cover a wide
range of domains with various types of humor that necessitate a multimodal
understanding of the content. Also, we develop a zero-shot video-to-text
prompting to maximize video humor understanding of large language models
(LLMs). With three different evaluation methods using automatic scores,
rationale quality experiments, and human evaluations, we show that our
prompting significantly improves LLMs' ability for humor explanation.",None,-1
2c8360c1-8065-4905-8146-3fc589ff8d97,Video Generation Beyond a Single Clip,0.0584624,"We tackle the long video generation problem, i.e.~generating videos beyond
the output length of video generation models. Due to the computation resource
constraints, video generation models can only generate video clips that are
relatively short compared with the length of real videos. Existing works apply
a sliding window approach to generate long videos at inference time, which is
often limited to generating recurrent events or homogeneous content. To
generate long videos covering diverse content and multiple events, we propose
to use additional guidance to control the video generation process. We further
present a two-stage approach to the problem, which allows us to utilize
existing video generation models to generate high-quality videos within a small
time window while modeling the video holistically based on the input guidance.
The proposed approach is complementary to existing efforts on video generation,
which focus on generating realistic video within a fixed time window. Extensive
experiments on challenging real-world videos validate the benefit of the
proposed method, which improves over state-of-the-art by up to 9.5% in
objective metrics and is preferred by users more than 80% of time.",None,-1
69f860f7-5e69-4c6a-925b-35a79bcbc085,Beyond the Prior Forgery Knowledge: Mining Critical Clues for General Face Forgery Detection,0.757616,"Face forgery detection is essential in combating malicious digital face
attacks. Previous methods mainly rely on prior expert knowledge to capture
specific forgery clues, such as noise patterns, blending boundaries, and
frequency artifacts. However, these methods tend to get trapped in local
optima, resulting in limited robustness and generalization capability. To
address these issues, we propose a novel Critical Forgery Mining (CFM)
framework, which can be flexibly assembled with various backbones to boost
their generalization and robustness performance. Specifically, we first build a
fine-grained triplet and suppress specific forgery traces through prior
knowledge-agnostic data augmentation. Subsequently, we propose a fine-grained
relation learning prototype to mine critical information in forgeries through
instance and local similarity-aware losses. Moreover, we design a novel
progressive learning controller to guide the model to focus on principal
feature components, enabling it to learn critical forgery features in a
coarse-to-fine manner. The proposed method achieves state-of-the-art forgery
detection performance under various challenging evaluation settings.",None,-1
3c57d930-f327-4326-adcc-9bbd593e45d4,Reference-guided Controllable Inpainting of Neural Radiance Fields,0.730219,"The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led
to a desire for NeRF editing tools. Here, we focus on inpainting regions in a
view-consistent and controllable manner. In addition to the typical NeRF inputs
and masks delineating the unwanted region in each view, we require only a
single inpainted view of the scene, i.e., a reference view. We use monocular
depth estimators to back-project the inpainted view to the correct 3D
positions. Then, via a novel rendering technique, a bilateral solver can
construct view-dependent effects in non-reference views, making the inpainted
region appear consistent from any view. For non-reference disoccluded regions,
which cannot be supervised by the single reference view, we devise a method
based on image inpainters to guide both the geometry and appearance. Our
approach shows superior performance to NeRF inpainting baselines, with the
additional advantage that a user can control the generated scene via a single
inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d",None,-1
83ed36ad-b0b3-4af1-b551-dcc749242cbc,Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots,0.0677967,"Diffusion models have made impressive progress in text-to-image synthesis.
However, training such large-scale models (e.g. Stable Diffusion), from scratch
requires high computational costs and massive high-quality text-image pairs,
which becomes unaffordable in other languages. To handle this challenge, we
propose IAP, a simple but effective method to transfer English Stable Diffusion
into Chinese. IAP optimizes only a separate Chinese text encoder with all other
parameters fixed to align Chinese semantics space to the English one in CLIP.
To achieve this, we innovatively treat images as pivots and minimize the
distance of attentive features produced from cross-attention between images and
each language respectively. In this way, IAP establishes connections of
Chinese, English and visual semantics in CLIP's embedding space efficiently,
advancing the quality of the generated image with direct Chinese prompts.
Experimental results show that our method outperforms several strong Chinese
diffusion models with only 5%~10% training data.",None,-1
43dc8c4a-e87d-4620-a711-1c1ee3ba87ab,Analysis of Recent Trends in Face Recognition Systems,0.406701,"With the tremendous advancements in face recognition technology, face
modality has been widely recognized as a significant biometric identifier in
establishing a person's identity rather than any other biometric trait like
fingerprints that require contact sensors. However, due to inter-class
similarities and intra-class variations, face recognition systems generate
false match and false non-match errors respectively. Recent research focuses on
improving the robustness of extracted features and the pre-processing
algorithms to enhance recognition accuracy. Since face recognition has been
extensively used for several applications ranging from law enforcement to
surveillance systems, the accuracy and performance of face recognition must be
the finest. In this paper various face recognition systems are discussed and
analysed like RPRV, LWKPCA, SVM Model, LTrP based SPM and a deep learning
framework for recognising images from CCTV. All these face recognition methods,
their implementations and performance evaluations are compared to derive the
best outcome for future developmental works.",None,-1
6d455856-c27b-481f-a2a3-5af0088bb337,Steerable Equivariant Representation Learning,0.104248,"Pre-trained deep image representations are useful for post-training tasks
such as classification through transfer learning, image retrieval, and object
detection. Data augmentations are a crucial aspect of pre-training robust
representations in both supervised and self-supervised settings. Data
augmentations explicitly or implicitly promote invariance in the embedding
space to the input image transformations. This invariance reduces
generalization to those downstream tasks which rely on sensitivity to these
particular data augmentations. In this paper, we propose a method of learning
representations that are instead equivariant to data augmentations. We achieve
this equivariance through the use of steerable representations. Our
representations can be manipulated directly in embedding space via learned
linear maps. We demonstrate that our resulting steerable and equivariant
representations lead to better performance on transfer learning and robustness:
e.g. we improve linear probe top-1 accuracy by between 1% to 3% for transfer;
and ImageNet-C accuracy by upto 3.4%. We further show that the steerability of
our representations provides significant speedup (nearly 50x) for test-time
augmentations; by applying a large number of augmentations for
out-of-distribution detection, we significantly improve OOD AUC on the
ImageNet-C dataset over an invariant representation.",None,-1
20c9f264-747f-4444-93db-4c1b5856b01e,Organizational Governance of Emerging Technologies: AI Adoption in Healthcare,0.890001,"Private and public sector structures and norms refine how emerging technology
is used in practice. In healthcare, despite a proliferation of AI adoption, the
organizational governance surrounding its use and integration is often poorly
understood. What the Health AI Partnership (HAIP) aims to do in this research
is to better define the requirements for adequate organizational governance of
AI systems in healthcare settings and support health system leaders to make
more informed decisions around AI adoption. To work towards this understanding,
we first identify how the standards for the AI adoption in healthcare may be
designed to be used easily and efficiently. Then, we map out the precise
decision points involved in the practical institutional adoption of AI
technology within specific health systems. Practically, we achieve this through
a multi-organizational collaboration with leaders from major health systems
across the United States and key informants from related fields. Working with
the consultancy IDEO [dot] org, we were able to conduct usability-testing
sessions with healthcare and AI ethics professionals. Usability analysis
revealed a prototype structured around mock key decision points that align with
how organizational leaders approach technology adoption. Concurrently, we
conducted semi-structured interviews with 89 professionals in healthcare and
other relevant fields. Using a modified grounded theory approach, we were able
to identify 8 key decision points and comprehensive procedures throughout the
AI adoption lifecycle. This is one of the most detailed qualitative analyses to
date of the current governance structures and processes involved in AI adoption
by health systems in the United States. We hope these findings can inform
future efforts to build capabilities to promote the safe, effective, and
responsible adoption of emerging technologies in healthcare.",None,-1
e5795310-d0f2-4553-9ddf-3ef67954f5d1,Drafting Event Schemas using Language Models,0.819449,"Past work has studied event prediction and event language modeling, sometimes
mediated through structured representations of knowledge in the form of event
schemas. Such schemas can lead to explainable predictions and forecasting of
unseen events given incomplete information. In this work, we look at the
process of creating such schemas to describe complex events. We use large
language models (LLMs) to draft schemas directly in natural language, which can
be further refined by human curators as necessary. Our focus is on whether we
can achieve sufficient diversity and recall of key events and whether we can
produce the schemas in a sufficiently descriptive style. We show that large
language models are able to achieve moderate recall against schemas taken from
two different datasets, with even better results when multiple prompts and
multiple samples are combined. Moreover, we show that textual entailment
methods can be used for both matching schemas to instances of events as well as
evaluating overlap between gold and predicted schemas. Our method paves the way
for easier distillation of event knowledge from large language model into
schemas.",None,-1
ba77c968-bce9-4424-b914-7d6d43459c70,Multimodality of AI for Education: Towards Artificial General Intelligence,0.888273,"This paper presents a comprehensive examination of how multimodal artificial
intelligence (AI) approaches are paving the way towards the realization of
Artificial General Intelligence (AGI) in educational contexts. It scrutinizes
the evolution and integration of AI in educational systems, emphasizing the
crucial role of multimodality, which encompasses auditory, visual, kinesthetic,
and linguistic modes of learning. This research delves deeply into the key
facets of AGI, including cognitive frameworks, advanced knowledge
representation, adaptive learning mechanisms, strategic planning, sophisticated
language processing, and the integration of diverse multimodal data sources. It
critically assesses AGI's transformative potential in reshaping educational
paradigms, focusing on enhancing teaching and learning effectiveness, filling
gaps in existing methodologies, and addressing ethical considerations and
responsible usage of AGI in educational settings. The paper also discusses the
implications of multimodal AI's role in education, offering insights into
future directions and challenges in AGI development. This exploration aims to
provide a nuanced understanding of the intersection between AI, multimodality,
and education, setting a foundation for future research and development in AGI.",None,-1
0ae7bfc4-09f5-42b6-89b0-0644f9d644b5,Towards Answering Climate Questionnaires from Unstructured Climate Reports,0.339356,"The topic of Climate Change (CC) has received limited attention in NLP
despite its urgency. Activists and policymakers need NLP tools to effectively
process the vast and rapidly growing unstructured textual climate reports into
structured form. To tackle this challenge we introduce two new large-scale
climate questionnaire datasets and use their existing structure to train
self-supervised models. We conduct experiments to show that these models can
learn to generalize to climate disclosures of different organizations types
than seen during training. We then use these models to help align texts from
unstructured climate documents to the semi-structured questionnaires in a human
pilot study. Finally, to support further NLP research in the climate domain we
introduce a benchmark of existing climate text classification datasets to
better evaluate and compare existing models.",None,-1
4ece9c20-d878-4b1d-9ea2-89a7c83414cd,Artificial Intelligence for Drug Discovery: Are We There Yet?,0.908088,"Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages.",None,-1
dbe6c0c7-a677-4cf3-a879-23d1f6e0bc3f,Tailoring Requirements Engineering for Responsible AI,0.203406,"Requirements Engineering (RE) is the discipline for identifying, analyzing,
as well as ensuring the implementation and delivery of user, technical, and
societal requirements. Recently reported issues concerning the acceptance of
Artificial Intelligence (AI) solutions after deployment, e.g. in the medical,
automotive, or scientific domains, stress the importance of RE for designing
and delivering Responsible AI systems. In this paper, we argue that RE should
not only be carefully conducted but also tailored for Responsible AI. We
outline related challenges for research and practice.",None,-1
66eef24c-e86f-480e-8c0f-48abdac83bea,Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning,0.545031,"In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly
autonomously in solving a task, but can request help from an external expert
when needed. However, knowing when to request such assistance is critical: too
few requests can lead to the robot making mistakes, but too many requests can
overload the expert. In this paper, we present a Reinforcement Learning based
approach to this problem, where a semi-autonomous agent asks for external
assistance when it has low confidence in the eventual success of the task. The
confidence level is computed by estimating the variance of the return from the
current state. We show that this estimate can be iteratively improved during
training using a Bellman-like recursion. On discrete navigation problems with
both fully- and partially-observable state information, we show that our method
makes effective use of a limited budget of expert calls at run-time, despite
having no access to the expert at training time.",None,-1
3ccf6df5-de07-4c9e-b9f2-8dc3ea376d36,Dynamic Interactive Relation Capturing via Scene Graph Learning for Robotic Surgical Report Generation,0.382921,"For robot-assisted surgery, an accurate surgical report reflects clinical
operations during surgery and helps document entry tasks, post-operative
analysis and follow-up treatment. It is a challenging task due to many complex
and diverse interactions between instruments and tissues in the surgical scene.
Although existing surgical report generation methods based on deep learning
have achieved large success, they often ignore the interactive relation between
tissues and instrumental tools, thereby degrading the report generation
performance. This paper presents a neural network to boost surgical report
generation by explicitly exploring the interactive relation between tissues and
surgical instruments. We validate the effectiveness of our method on a
widely-used robotic surgery benchmark dataset, and experimental results show
that our network can significantly outperform existing state-of-the-art
surgical report generation methods (e.g., 7.48% and 5.43% higher for BLEU-1 and
ROUGE).",None,-1
00530425-502e-43ef-9333-da36d4b7db17,A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations,0.609728,"Emotion recognition in conversations (ERC), the task of recognizing the
emotion of each utterance in a conversation, is crucial for building empathetic
machines. Existing studies focus mainly on capturing context- and
speaker-sensitive dependencies on the textual modality but ignore the
significance of multimodal information. Different from emotion recognition in
textual conversations, capturing intra- and inter-modal interactions between
utterances, learning weights between different modalities, and enhancing modal
representations play important roles in multimodal ERC. In this paper, we
propose a transformer-based model with self-distillation (SDT) for the task.
The transformer-based model captures intra- and inter-modal interactions by
utilizing intra- and inter-modal transformers, and learns weights between
modalities dynamically by designing a hierarchical gated fusion strategy.
Furthermore, to learn more expressive modal representations, we treat soft
labels of the proposed model as extra training supervision. Specifically, we
introduce self-distillation to transfer knowledge of hard and soft labels from
the proposed model to each modality. Experiments on IEMOCAP and MELD datasets
demonstrate that SDT outperforms previous state-of-the-art baselines.",None,-1
1c2d1942-c114-4b92-a937-ffde93afe780,C-Pack: Packaged Resources To Advance General Chinese Embedding,0.999983,"We introduce C-Pack, a package of resources that significantly advance the
field of general Chinese embeddings. C-Pack includes three critical resources.
1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6
tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated
from labeled and unlabeled Chinese corpora for training embedding models. 3)
C-TEM is a family of embedding models covering multiple sizes. Our models
outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the
time of the release. We also integrate and optimize the entire suite of
training methods for C-TEM. Along with our resources on general Chinese
embedding, we release our data and models for English text embeddings. The
English models achieve state-of-the-art performance on MTEB benchmark;
meanwhile, our released English data is 2 times larger than the Chinese data.
All these resources are made publicly available at
https://github.com/FlagOpen/FlagEmbedding.",None,-1
1b158085-e057-4c27-bc94-97bc1b33a800,Orthogonal Annotation Benefits Barely-supervised Medical Image Segmentation,0.540742,"Recent trends in semi-supervised learning have significantly boosted the
performance of 3D semi-supervised medical image segmentation. Compared with 2D
images, 3D medical volumes involve information from different directions, e.g.,
transverse, sagittal, and coronal planes, so as to naturally provide
complementary views. These complementary views and the intrinsic similarity
among adjacent 3D slices inspire us to develop a novel annotation way and its
corresponding semi-supervised model for effective segmentation. Specifically,
we firstly propose the orthogonal annotation by only labeling two orthogonal
slices in a labeled volume, which significantly relieves the burden of
annotation. Then, we perform registration to obtain the initial pseudo labels
for sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,
we propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that
exploits dense pseudo labels in early stage and sparse labels in later stage
and meanwhile forces consistent output of two networks. Experimental results on
three benchmark datasets validated our effectiveness in performance and
efficiency in annotation. For example, with only 10 annotated slices, our
method reaches a Dice up to 86.93% on KiTS19 dataset.",None,-1
88a90f43-2c14-4bc7-a5fa-87f483837d8c,Adaptive Gating in Mixture-of-Experts based Language Models,0.00963634,"Large language models, such as OpenAI's ChatGPT, have demonstrated
exceptional language understanding capabilities in various NLP tasks. Sparsely
activated mixture-of-experts (MoE) has emerged as a promising solution for
scaling models while maintaining a constant number of computational operations.
Existing MoE model adopts a fixed gating network where each token is computed
by the same number of experts. However, this approach contradicts our intuition
that the tokens in each sequence vary in terms of their linguistic complexity
and, consequently, require different computational costs. Little is discussed
in prior research on the trade-off between computation per token and model
performance. This paper introduces adaptive gating in MoE, a flexible training
strategy that allows tokens to be processed by a variable number of experts
based on expert probability distribution. The proposed framework preserves
sparsity while improving training efficiency. Additionally, curriculum learning
is leveraged to further reduce training time. Extensive experiments on diverse
NLP tasks show that adaptive gating reduces at most 22.5% training time while
maintaining inference quality. Moreover, we conduct a comprehensive analysis of
the routing decisions and present our insights when adaptive gating is used.",None,-1
013e25ac-6062-4e70-a439-d8ccdef4e229,Segment Anything,1.0,"We introduce the Segment Anything (SA) project: a new task, model, and
dataset for image segmentation. Using our efficient model in a data collection
loop, we built the largest segmentation dataset to date (by far), with over 1
billion masks on 11M licensed and privacy respecting images. The model is
designed and trained to be promptable, so it can transfer zero-shot to new
image distributions and tasks. We evaluate its capabilities on numerous tasks
and find that its zero-shot performance is impressive -- often competitive with
or even superior to prior fully supervised results. We are releasing the
Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and
11M images at https://segment-anything.com to foster research into foundation
models for computer vision.",None,-1
add603cf-6c6e-43ce-8ff1-35a4d1c1a589,Dynamic fairness-aware recommendation through multi-agent social choice,0.276668,"Algorithmic fairness in the context of personalized recommendation presents
significantly different challenges to those commonly encountered in
classification tasks. Researchers studying classification have generally
considered fairness to be a matter of achieving equality of outcomes between a
protected and unprotected group, and built algorithmic interventions on this
basis. We argue that fairness in real-world application settings in general,
and especially in the context of personalized recommendation, is much more
complex and multi-faceted, requiring a more general approach. We propose a
model to formalize multistakeholder fairness in recommender systems as a two
stage social choice problem. In particular, we express recommendation fairness
as a novel combination of an allocation and an aggregation problem, which
integrate both fairness concerns and personalized recommendation provisions,
and derive new recommendation techniques based on this formulation. Simulations
demonstrate the ability of the framework to integrate multiple fairness
concerns in a dynamic way.",None,-1
ab8af330-a810-473a-a0cb-380c7542519e,Diffusion-based Document Layout Generation,0.637846,"We develop a diffusion-based approach for various document layout sequence
generation. Layout sequences specify the contents of a document design in an
explicit format. Our novel diffusion-based approach works in the sequence
domain rather than the image domain in order to permit more complex and
realistic layouts. We also introduce a new metric, Document Earth Mover's
Distance (Doc-EMD). By considering similarity between heterogeneous categories
document designs, we handle the shortcomings of prior document metrics that
only evaluate the same category of layouts. Our empirical analysis shows that
our diffusion-based approach is comparable to or outperforming other previous
methods for layout generation across various document datasets. Moreover, our
metric is capable of differentiating documents better than previous metrics for
specific cases.",None,-1
23202304-37ab-41fe-b2d9-fda552579672,HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution,0.872119,"The rise of large language models (LLMs) had a transformative impact on
search, ushering in a new era of search engines that are capable of generating
search results in natural language text, imbued with citations for supporting
sources. Building generative information-seeking models demands openly
accessible datasets, which currently remain lacking. In this paper, we
introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative
Retrieval for Information-seeking Dataset) for building end-to-end generative
information-seeking models that are capable of retrieving candidate quotes and
generating attributed explanations. Unlike recent efforts that focus on human
evaluation of black-box proprietary search engines, we built our dataset atop
the English subset of MIRACL, a publicly available information retrieval
dataset. HAGRID is constructed based on human and LLM collaboration. We first
automatically collect attributed explanations that follow an in-context
citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to
evaluate the LLM explanations based on two criteria: informativeness and
attributability. HAGRID serves as a catalyst for the development of
information-seeking models with better attribution capabilities.",None,-1
2af95135-4570-4e87-a4da-63195519f53a,CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion,0.825314,"This paper proposes a novel diffusion-based model, CompoDiff, for solving
zero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper
also introduces a new synthetic dataset, named SynthTriplets18M, with 18.8
million reference images, conditions, and corresponding target image triplets
to train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the
previous CIR approaches, such as poor generalizability due to the small dataset
scale and the limited types of conditions. CompoDiff not only achieves a new
state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,
and GeneCIS, but also enables a more versatile and controllable CIR by
accepting various conditions, such as negative text, and image mask conditions.
CompoDiff also shows the controllability of the condition strength between text
and image queries and the trade-off between inference speed and performance,
which are unavailable with existing CIR methods. The code and dataset are
available at https://github.com/navervision/CompoDiff",None,-1
c876267a-6c45-48ab-a794-7d23dd8c0fda,An Empirical Comparison of LM-based Question and Answer Generation Methods,0.51331,"Question and answer generation (QAG) consists of generating a set of
question-answer pairs given a context (e.g. a paragraph). This task has a
variety of applications, such as data augmentation for question answering (QA)
models, information retrieval and education. In this paper, we establish
baselines with three different QAG methodologies that leverage
sequence-to-sequence language model (LM) fine-tuning. Experiments show that an
end-to-end QAG model, which is computationally light at both training and
inference times, is generally robust and outperforms other more convoluted
approaches. However, there are differences depending on the underlying
generative LM. Finally, our analysis shows that QA models fine-tuned solely on
generated question-answer pairs can be competitive when compared to supervised
QA models trained on human-labeled data.",None,-1
d54426f6-3d84-474e-a9a6-1fb332619bd1,A Multimodal Analysis of Influencer Content on Twitter,0.624476,"Influencer marketing involves a wide range of strategies in which brands
collaborate with popular content creators (i.e., influencers) to leverage their
reach, trust, and impact on their audience to promote and endorse products or
services. Because followers of influencers are more likely to buy a product
after receiving an authentic product endorsement rather than an explicit direct
product promotion, the line between personal opinions and commercial content
promotion is frequently blurred. This makes automatic detection of regulatory
compliance breaches related to influencer advertising (e.g., misleading
advertising or hidden sponsorships) particularly difficult. In this work, we
(1) introduce a new Twitter (now X) dataset consisting of 15,998 influencer
posts mapped into commercial and non-commercial categories for assisting in the
automatic detection of commercial influencer content; (2) experiment with an
extensive set of predictive models that combine text and visual information
showing that our proposed cross-attention approach outperforms state-of-the-art
multimodal models; and (3) conduct a thorough analysis of strengths and
limitations of our models. We show that multimodal modeling is useful for
identifying commercial posts, reducing the amount of false positives, and
capturing relevant context that aids in the discovery of undisclosed commercial
posts.",None,-1
ce6b1f0a-73bf-4587-b3b3-df61f4a3c214,PointWavelet: Learning in Spectral Domain for 3D Point Cloud Analysis,0.154608,"With recent success of deep learning in 2D visual recognition, deep
learning-based 3D point cloud analysis has received increasing attention from
the community, especially due to the rapid development of autonomous driving
technologies. However, most existing methods directly learn point features in
the spatial domain, leaving the local structures in the spectral domain poorly
investigated. In this paper, we introduce a new method, PointWavelet, to
explore local graphs in the spectral domain via a learnable graph wavelet
transform. Specifically, we first introduce the graph wavelet transform to form
multi-scale spectral graph convolution to learn effective local structural
representations. To avoid the time-consuming spectral decomposition, we then
devise a learnable graph wavelet transform, which significantly accelerates the
overall training process. Extensive experiments on four popular point cloud
datasets, ModelNet40, ScanObjectNN, ShapeNet-Part, and S3DIS, demonstrate the
effectiveness of the proposed method on point cloud classification and
segmentation.",None,-1
f29018c2-05a5-4940-83ce-f5bba204f254,DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers,0.444631,"We propose a novel talking head synthesis pipeline called ""DiT-Head"", which
is based on diffusion transformers and uses audio as a condition to drive the
denoising process of a diffusion model. Our method is scalable and can
generalise to multiple identities while producing high-quality results. We
train and evaluate our proposed approach and compare it against existing
methods of talking head synthesis. We show that our model can compete with
these methods in terms of visual quality and lip-sync accuracy. Our results
highlight the potential of our proposed approach to be used for a wide range of
applications, including virtual assistants, entertainment, and education. For a
video demonstration of the results and our user study, please refer to our
supplementary material.",None,-1
87bc61b7-82ca-4b6e-9740-fe8135d67fab,Deep Anomaly Detection under Labeling Budget Constraints,0.498994,"Selecting informative data points for expert feedback can significantly
improve the performance of anomaly detection (AD) in various contexts, such as
medical diagnostics or fraud detection. In this paper, we determine a set of
theoretical conditions under which anomaly scores generalize from labeled
queries to unlabeled data. Motivated by these results, we propose a data
labeling strategy with optimal data coverage under labeling budget constraints.
In addition, we propose a new learning framework for semi-supervised AD.
Extensive experiments on image, tabular, and video data sets show that our
approach results in state-of-the-art semi-supervised AD performance under
labeling budget constraints.",None,-1
2659b12e-3e97-4d21-8e3e-e7d8251ae3fe,Model-based learning for location-to-channel mapping,0.394077,"Modern communication systems rely on accurate channel estimation to achieve
efficient and reliable transmission of information. As the communication
channel response is highly related to the user's location, one can use a neural
network to map the user's spatial coordinates to the channel coefficients.
However, these latter are rapidly varying as a function of the location, on the
order of the wavelength. Classical neural architectures being biased towards
learning low frequency functions (spectral bias), such mapping is therefore
notably difficult to learn. In order to overcome this limitation, this paper
presents a frugal, model-based network that separates the low frequency from
the high frequency components of the target mapping function. This yields an
hypernetwork architecture where the neural network only learns low frequency
sparse coefficients in a dictionary of high frequency components. Simulation
results show that the proposed neural network outperforms standard approaches
on realistic synthetic data.",None,-1
186913b0-c9e7-46ca-962b-3564f287b8dc,Towards Designing a ChatGPT Conversational Companion for Elderly People,0.783826,"Loneliness and social isolation are serious and widespread problems among
older people, affecting their physical and mental health, quality of life, and
longevity. In this paper, we propose a ChatGPT-based conversational companion
system for elderly people. The system is designed to provide companionship and
help reduce feelings of loneliness and social isolation. The system was
evaluated with a preliminary study. The results showed that the system was able
to generate responses that were relevant to the created elderly personas.
However, it is essential to acknowledge the limitations of ChatGPT, such as
potential biases and misinformation, and to consider the ethical implications
of using AI-based companionship for the elderly, including privacy concerns.",None,-1
334b44ef-419f-40bb-b4ab-d2c971e6447a,NOVIS: A Case for End-to-End Near-Online Video Instance Segmentation,0.243209,"Until recently, the Video Instance Segmentation (VIS) community operated
under the common belief that offline methods are generally superior to a frame
by frame online processing. However, the recent success of online methods
questions this belief, in particular, for challenging and long video sequences.
We understand this work as a rebuttal of those recent observations and an
appeal to the community to focus on dedicated near-online VIS approaches. To
support our argument, we present a detailed analysis on different processing
paradigms and the new end-to-end trainable NOVIS (Near-Online Video Instance
Segmentation) method. Our transformer-based model directly predicts
spatio-temporal mask volumes for clips of frames and performs instance tracking
between clips via overlap embeddings. NOVIS represents the first near-online
VIS approach which avoids any handcrafted tracking heuristics. We outperform
all existing VIS methods by large margins and provide new state-of-the-art
results on both YouTube-VIS (2019/2021) and the OVIS benchmarks.",None,-1
e300e96f-fc87-4a05-9549-dcaca40ca812,Self-Sufficient Framework for Continuous Sign Language Recognition,0.908278,"The goal of this work is to develop self-sufficient framework for Continuous
Sign Language Recognition (CSLR) that addresses key issues of sign language
recognition. These include the need for complex multi-scale features such as
hands, face, and mouth for understanding, and absence of frame-level
annotations. To this end, we propose (1) Divide and Focus Convolution (DFConv)
which extracts both manual and non-manual features without the need for
additional networks or annotations, and (2) Dense Pseudo-Label Refinement
(DPLR) which propagates non-spiky frame-level pseudo-labels by combining the
ground truth gloss sequence labels with the predicted sequence. We demonstrate
that our model achieves state-of-the-art performance among RGB-based methods on
large-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing
comparable results with better efficiency when compared to other approaches
that use multi-modality or extra annotations.",None,-1
474255ad-daf9-4ab9-84d8-72a26ec31b8e,A Huber Loss Minimization Approach to Byzantine Robust Federated Learning,0.284568,"Federated learning systems are susceptible to adversarial attacks. To combat
this, we introduce a novel aggregator based on Huber loss minimization, and
provide a comprehensive theoretical analysis. Under independent and identically
distributed (i.i.d) assumption, our approach has several advantages compared to
existing methods. Firstly, it has optimal dependence on $\epsilon$, which
stands for the ratio of attacked clients. Secondly, our approach does not need
precise knowledge of $\epsilon$. Thirdly, it allows different clients to have
unequal data sizes. We then broaden our analysis to include non-i.i.d data,
such that clients have slightly different distributions.",None,-1
4b22f579-d633-40e8-86fc-ccf2a9c07329,GPT is becoming a Turing machine: Here are some ways to program it,0.3861,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",None,-1
2e775e60-ba3d-4e95-864a-c9b6d7107473,M2ConceptBase: A Fine-grained Aligned Multi-modal Conceptual Knowledge Base,0.35,"Large multi-modal models (LMMs) have demonstrated promising intelligence
owing to the rapid development of pre-training techniques. However, their
fine-grained cross-modal alignment ability is constrained by the coarse
alignment in image-text pairs. This limitation hinders awareness of
fine-grained concepts, resulting in sub-optimal performance. In this paper, we
propose a multi-modal conceptual knowledge base, named M2ConceptBase, which
aims to provide fine-grained alignment between images and concepts.
Specifically, M2ConceptBase models concepts as nodes, associating each with
relevant images and detailed text, thereby enhancing LMMs' cross-modal
alignment with rich conceptual knowledge. To collect concept-image and
concept-description alignments, we propose a context-aware multi-modal symbol
grounding approach that considers context information in existing large-scale
image-text pairs with respect to each concept. A cutting-edge large language
model supplements descriptions for concepts not grounded via our symbol
grounding approach. Finally, our M2ConceptBase contains more than 951K images
and 152K concepts, each associating with an average of 6.27 images and a single
detailed description. We conduct experiments on the OK-VQA task, demonstrating
that our M2ConceptBase facilitates the model in achieving state-of-the-art
performance. Moreover, we construct a comprehensive benchmark to evaluate the
concept understanding of LMMs and show that M2ConceptBase could effectively
improve LMMs' concept understanding and cross-modal alignment abilities.",None,-1
520b94de-ab6f-4630-8aca-3bbdcbb5eca0,AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation,0.799138,"To apply optical flow in practice, it is often necessary to resize the input
to smaller dimensions in order to reduce computational costs. However,
downsizing inputs makes the estimation more challenging because objects and
motion ranges become smaller. Even though recent approaches have demonstrated
high-quality flow estimation, they tend to fail to accurately model small
objects and precise boundaries when the input resolution is lowered,
restricting their applicability to high-resolution inputs. In this paper, we
introduce AnyFlow, a robust network that estimates accurate flow from images of
various resolutions. By representing optical flow as a continuous
coordinate-based representation, AnyFlow generates outputs at arbitrary scales
from low-resolution inputs, demonstrating superior performance over prior works
in capturing tiny objects with detail preservation on a wide range of scenes.
We establish a new state-of-the-art performance of cross-dataset generalization
on the KITTI dataset, while achieving comparable accuracy on the online
benchmarks to other SOTA methods.",None,-1
c39f1264-97fd-4eb1-a301-e39b35f75a87,Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation,0.588244,"Zero-shot instance segmentation aims to detect and precisely segment objects
of unseen categories without any training samples. Since the model is trained
on seen categories, there is a strong bias that the model tends to classify all
the objects into seen categories. Besides, there is a natural confusion between
background and novel objects that have never shown up in training. These two
challenges make novel objects hard to be raised in the final instance
segmentation results. It is desired to rescue novel objects from background and
dominated seen categories. To this end, we propose D$^2$Zero with
Semantic-Promoted Debiasing and Background Disambiguation to enhance the
performance of Zero-shot instance segmentation. Semantic-promoted debiasing
utilizes inter-class semantic relationships to involve unseen categories in
visual feature training and learns an input-conditional classifier to conduct
dynamical classification based on the input image. Background disambiguation
produces image-adaptive background representation to avoid mistaking novel
objects for background. Extensive experiments show that we significantly
outperform previous state-of-the-art methods by a large margin, e.g., 16.86%
improvement on COCO. Project page: https://henghuiding.github.io/D2Zero/",None,-1
507fc34a-ebbe-489b-a8ba-41b3a888e998,Algorithmic Transparency and Manipulation,0.773198,"A series of recent papers raises worries about the manipulative potential of
algorithmic transparency. But while the concern is apt and relevant, it is
based on a fraught understanding of manipulation. Therefore, this paper draws
attention to the indifference view of manipulation, which explains better than
the vulnerability view why algorithmic transparency has manipulative potential.
The paper also raises pertinent research questions for future studies of
manipulation in the context of algorithmic transparency.",None,-1
ea20b59e-64f5-4009-b6cf-5c73e86096fa,Optimization-Inspired Cross-Attention Transformer for Compressive Sensing,0.852979,"By integrating certain optimization solvers with deep neural networks, deep
unfolding network (DUN) with good interpretability and high performance has
attracted growing attention in compressive sensing (CS). However, existing DUNs
often improve the visual quality at the price of a large number of parameters
and have the problem of feature information loss during iteration. In this
paper, we propose an Optimization-inspired Cross-attention Transformer (OCT)
module as an iterative process, leading to a lightweight OCT-based Unfolding
Framework (OCTUF) for image CS. Specifically, we design a novel Dual Cross
Attention (Dual-CA) sub-module, which consists of an Inertia-Supplied Cross
Attention (ISCA) block and a Projection-Guided Cross Attention (PGCA) block.
ISCA block introduces multi-channel inertia forces and increases the memory
effect by a cross attention mechanism between adjacent iterations. And, PGCA
block achieves an enhanced information interaction, which introduces the
inertia force into the gradient descent step through a cross attention block.
Extensive CS experiments manifest that our OCTUF achieves superior performance
compared to state-of-the-art methods while training lower complexity. Codes are
available at https://github.com/songjiechong/OCTUF.",None,-1
2122d3d4-e953-4025-bbc5-74b0221d7946,Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification,0.331872,"The success of deep learning models on multi-hop fact verification has
prompted researchers to understand the behavior behind their veracity. One
possible way is erasure search: obtaining the rationale by entirely removing a
subset of input without compromising the veracity prediction. Although
extensively explored, existing approaches fall within the scope of the
single-granular (tokens or sentences) explanation, which inevitably leads to
explanation redundancy and inconsistency. To address such issues, this paper
explores the viability of multi-granular rationale extraction with consistency
and faithfulness for explainable multi-hop fact verification. In particular,
given a pretrained veracity prediction model, both the token-level explainer
and sentence-level explainer are trained simultaneously to obtain
multi-granular rationales via differentiable masking. Meanwhile, three
diagnostic properties (fidelity, consistency, salience) are introduced and
applied to the training process, to ensure that the extracted rationales
satisfy faithfulness and consistency. Experimental results on three multi-hop
fact verification datasets show that the proposed approach outperforms some
state-of-the-art baselines.",None,-1
c7910dcf-8dba-49f0-95de-dbf5d9a8bf55,What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations,0.755915,"We propose and address a new generalisation problem: can a model trained for
action recognition successfully classify actions when they are performed within
a previously unseen scenario and in a previously unseen location? To answer
this question, we introduce the Action Recognition Generalisation Over
scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from
the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We
demonstrate recognition models struggle to generalise over 10 proposed test
splits, each of an unseen scenario in an unseen location. We thus propose CIR,
a method to represent each video as a Cross-Instance Reconstruction of videos
from other domains. Reconstructions are paired with text narrations to guide
the learning of a domain generalisable representation. We provide extensive
analysis and ablations on ARGO1M that show CIR outperforms prior domain
generalisation works on all test splits. Code and data:
https://chiaraplizz.github.io/what-can-a-cook/.",None,-1
367704e3-48b3-43a5-8872-f151b49a341e,Integrating UMLS Knowledge into Large Language Models for Medical Question Answering,0.42805,"Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.",None,-1
154a5be7-4dc6-40de-8956-868baa5a2f75,ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency,0.69659,"We present ShapeClipper, a novel method that reconstructs 3D object shapes
from real-world single-view RGB images. Instead of relying on laborious 3D,
multi-view or camera pose annotation, ShapeClipper learns shape reconstruction
from a set of single-view segmented images. The key idea is to facilitate shape
learning via CLIP-based shape consistency, where we encourage objects with
similar CLIP encodings to share similar shapes. We also leverage off-the-shelf
normals as an additional geometric constraint so the model can learn better
bottom-up reasoning of detailed surface geometry. These two novel consistency
constraints, when used to regularize our model, improve its ability to learn
both global shape structure and local geometric details. We evaluate our method
over three challenging real-world datasets, Pix3D, Pascal3D+, and OpenImages,
where we achieve superior performance over state-of-the-art methods.",None,-1
237a20f9-c9dc-401c-b659-10937e87ef5a,When 3D Bounding-Box Meets SAM: Point Cloud Instance Segmentation with Weak-and-Noisy Supervision,0.628217,"Learning from bounding-boxes annotations has shown great potential in
weakly-supervised 3D point cloud instance segmentation. However, we observed
that existing methods would suffer severe performance degradation with
perturbed bounding box annotations. To tackle this issue, we propose a
complementary image prompt-induced weakly-supervised point cloud instance
segmentation (CIP-WPIS) method. CIP-WPIS leverages pretrained knowledge
embedded in the 2D foundation model SAM and 3D geometric prior to achieve
accurate point-wise instance labels from the bounding box annotations.
Specifically, CP-WPIS first selects image views in which 3D candidate points of
an instance are fully visible. Then, we generate complementary background and
foreground prompts from projections to obtain SAM 2D instance mask predictions.
According to these, we assign the confidence values to points indicating the
likelihood of points belonging to the instance. Furthermore, we utilize 3D
geometric homogeneity provided by superpoints to decide the final instance
label assignments. In this fashion, we achieve high-quality 3D point-wise
instance labels. Extensive experiments on both Scannet-v2 and S3DIS benchmarks
demonstrate that our method is robust against noisy 3D bounding-box annotations
and achieves state-of-the-art performance.",None,-1
631c0be2-95eb-43d5-90b7-1816d9b49c0c,Which Tokens to Use? Investigating Token Reduction in Vision Transformers,0.542913,"Since the introduction of the Vision Transformer (ViT), researchers have
sought to make ViTs more efficient by removing redundant information in the
processed tokens. While different methods have been explored to achieve this
goal, we still lack understanding of the resulting reduction patterns and how
those patterns differ across token reduction methods and datasets. To close
this gap, we set out to understand the reduction patterns of 10 different token
reduction methods using four image classification datasets. By systematically
comparing these methods on the different classification tasks, we find that the
Top-K pruning method is a surprisingly strong baseline. Through in-depth
analysis of the different methods, we determine that: the reduction patterns
are generally not consistent when varying the capacity of the backbone model,
the reduction patterns of pruning-based methods significantly differ from fixed
radial patterns, and the reduction patterns of pruning-based methods are
correlated across classification datasets. Finally we report that the
similarity of reduction patterns is a moderate-to-strong proxy for model
performance. Project page at https://vap.aau.dk/tokens.",None,-1
ddfbf5cd-f727-44ae-9077-6ddb20aa0220,Coarse-to-Fine Multi-Scene Pose Regression with Transformers,0.341869,"Absolute camera pose regressors estimate the position and orientation of a
camera given the captured image alone. Typically, a convolutional backbone with
a multi-layer perceptron (MLP) head is trained using images and pose labels to
embed a single reference scene at a time. Recently, this scheme was extended to
learn multiple scenes by replacing the MLP head with a set of fully connected
layers. In this work, we propose to learn multi-scene absolute camera pose
regression with Transformers, where encoders are used to aggregate activation
maps with self-attention and decoders transform latent features and scenes
encoding into pose predictions. This allows our model to focus on general
features that are informative for localization, while embedding multiple scenes
in parallel. We extend our previous MS-Transformer approach
\cite{shavit2021learning} by introducing a mixed classification-regression
architecture that improves the localization accuracy. Our method is evaluated
on commonly benchmark indoor and outdoor datasets and has been shown to exceed
both multi-scene and state-of-the-art single-scene absolute pose regressors.",None,-1
f240740a-bad7-4ec1-a6c4-415a047d312b,PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter,0.592891,"The Retrieval Question Answering (ReQA) task employs the retrieval-augmented
framework, composed of a retriever and generator. The generator formulates the
answer based on the documents retrieved by the retriever. Incorporating Large
Language Models (LLMs) as generators is beneficial due to their advanced QA
capabilities, but they are typically too large to be fine-tuned with budget
constraints while some of them are only accessible via APIs. To tackle this
issue and further improve ReQA performance, we propose a trainable Pluggable
Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box.
Positioned between the retriever and generator in a Pluggable manner, PRCA
refines the retrieved information by operating in a token-autoregressive
strategy via maximizing rewards of the reinforcement learning phase. Our
experiments validate PRCA's effectiveness in enhancing ReQA performance on
three datasets by up to 20% improvement to fit black-box LLMs into existing
frameworks, demonstrating its considerable potential in the LLMs era.",None,-1
50a2a7d6-2565-4fae-a420-e78b7e0bc001,Understanding and Predicting Human Label Variation in Natural Language Inference through Explanation,0.113992,"Human label variation (Plank 2022), or annotation disagreement, exists in
many natural language processing (NLP) tasks. To be robust and trusted, NLP
models need to identify such variation and be able to explain it. To this end,
we created the first ecologically valid explanation dataset with diverse
reasoning, LiveNLI. LiveNLI contains annotators' highlights and free-text
explanations for the label(s) of their choice for 122 English Natural Language
Inference items, each with at least 10 annotations. We used its explanations
for chain-of-thought prompting, and found there is still room for improvement
in GPT-3's ability to predict label distribution with in-context learning.",None,-1
1906b87f-d3b0-45f0-95ea-24f5dd642451,Transformer-based World Models Are Happy With 100k Interactions,0.685862,"Deep neural networks have been successful in many reinforcement learning
settings. However, compared to human learners they are overly data hungry. To
build a sample-efficient world model, we apply a transformer to real-world
episodes in an autoregressive manner: not only the compact latent states and
the taken actions but also the experienced or predicted rewards are fed into
the transformer, so that it can attend flexibly to all three modalities at
different time steps. The transformer allows our world model to access previous
states directly, instead of viewing them through a compressed recurrent state.
By utilizing the Transformer-XL architecture, it is able to learn long-term
dependencies while staying computationally efficient. Our transformer-based
world model (TWM) generates meaningful, new experience, which is used to train
a policy that outperforms previous model-free and model-based reinforcement
learning algorithms on the Atari 100k benchmark.",None,-1
fd4c19a4-2443-4c31-99be-8ca506ee0d8c,Improving Code-Switching and Named Entity Recognition in ASR with Speech Editing based Data Augmentation,0.444514,"Recently, end-to-end (E2E) automatic speech recognition (ASR) models have
made great strides and exhibit excellent performance in general speech
recognition. However, there remain several challenging scenarios that E2E
models are not competent in, such as code-switching and named entity
recognition (NER). Data augmentation is a common and effective practice for
these two scenarios. However, the current data augmentation methods mainly rely
on audio splicing and text-to-speech (TTS) models, which might result in
discontinuous, unrealistic, and less diversified speech. To mitigate these
potential issues, we propose a novel data augmentation method by applying the
text-based speech editing model. The augmented speech from speech editing
systems is more coherent and diversified, also more akin to real speech. The
experimental results on code-switching and NER tasks show that our proposed
method can significantly outperform the audio splicing and neural TTS based
data augmentation systems.",None,-1
7a5ea646-b7e8-471e-8579-4f49f69b261b,Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,0.735778,"Hate speech is a severe issue that affects many online platforms. So far,
several studies have been performed to develop robust hate speech detection
systems. Large language models like ChatGPT have recently shown a great promise
in performing several tasks, including hate speech detection. However, it is
crucial to comprehend the limitations of these models to build robust hate
speech detection systems. To bridge this gap, our study aims to evaluate the
strengths and weaknesses of the ChatGPT model in detecting hate speech at a
granular level across 11 languages. Our evaluation employs a series of
functionality tests that reveals various intricate failures of the model which
the aggregate metrics like macro F1 or accuracy are not able to unfold. In
addition, we investigate the influence of complex emotions, such as the use of
emojis in hate speech, on the performance of the ChatGPT model. Our analysis
highlights the shortcomings of the generative models in detecting certain types
of hate speech and highlighting the need for further research and improvements
in the workings of these models.",None,-1
bf39bf12-668c-4c6a-90ee-6a20dcdc7f40,Prefix Propagation: Parameter-Efficient Tuning for Long Sequences,0.326133,"Parameter-efficient tuning aims to mitigate the large memory requirements of
adapting pretrained language models for downstream tasks. For example, one
popular method, prefix-tuning, prepends trainable tokens to sequences while
freezing the rest of the model's parameters. Although such models attain
comparable performance with fine-tuning when applied to sequences with short to
moderate lengths, we show their inferior performance when modelling long
sequences. To bridge this gap, we propose prefix-propagation, a simple but
effective approach that conditions prefixes on previous hidden states. We
empirically demonstrate that prefix-propagation outperforms prefix-tuning
across long-document tasks, while using 50% fewer parameters. To further
investigate the proposed architecture, we also show its advantage in
calibration, and perform additional study on its relationship with kernel
attention. To the best of our knowledge, this work is the first to focus on
parameter-efficient learning for long-sequence language tasks.",None,-1
62e63047-1773-43a8-967f-206eec444352,Can Generative Large Language Models Perform ASR Error Correction?,0.925919,"ASR error correction is an interesting option for post processing speech
recognition system outputs. These error correction models are usually trained
in a supervised fashion using the decoding results of a target ASR system. This
approach can be computationally intensive and the model is tuned to a specific
ASR system. Recently generative large language models (LLMs) have been applied
to a wide range of natural language processing tasks, as they can operate in a
zero-shot or few shot fashion. In this paper we investigate using ChatGPT, a
generative LLM, for ASR error correction. Based on the ASR N-best output, we
propose both unconstrained and constrained, where a member of the N-best list
is selected, approaches. Additionally, zero and 1-shot settings are evaluated.
Experiments show that this generative LLM approach can yield performance gains
for two different state-of-the-art ASR architectures, transducer and
attention-encoder-decoder based, and multiple test sets.",None,-1
31aae24d-049f-4ed7-9953-dd1515775b5b,SpotEM: Efficient Video Search for Episodic Memory,0.796142,"The goal in episodic memory (EM) is to search a long egocentric video to
answer a natural language query (e.g., ""where did I leave my purse?""). Existing
EM methods exhaustively extract expensive fixed-length clip features to look
everywhere in the video for the answer, which is infeasible for long
wearable-camera videos that span hours or even days. We propose SpotEM, an
approach to achieve efficiency for a given EM method while maintaining good
accuracy. SpotEM consists of three key ideas: 1) a novel clip selector that
learns to identify promising video regions to search conditioned on the
language query; 2) a set of low-cost semantic indexing features that capture
the context of rooms, objects, and interactions that suggest where to look; and
3) distillation losses that address the optimization issues arising from
end-to-end joint training of the clip selector and EM model. Our experiments on
200+ hours of video from the Ego4D EM Natural Language Queries benchmark and
three different EM models demonstrate the effectiveness of our approach:
computing only 10% - 25% of the clip features, we preserve 84% - 97% of the
original EM model's accuracy. Project page:
https://vision.cs.utexas.edu/projects/spotem",None,-1
712c38f5-c3c0-4f3f-8b99-43df134e13dd,Zero-Shot Transfer of Haptics-Based Object Insertion Policies,0.540574,"Humans naturally exploit haptic feedback during contact-rich tasks like
loading a dishwasher or stocking a bookshelf. Current robotic systems focus on
avoiding unexpected contact, often relying on strategically placed environment
sensors. Recently, contact-exploiting manipulation policies have been trained
in simulation and deployed on real robots. However, they require some form of
real-world adaptation to bridge the sim-to-real gap, which might not be
feasible in all scenarios. In this paper we train a contact-exploiting
manipulation policy in simulation for the contact-rich household task of
loading plates into a slotted holder, which transfers without any fine-tuning
to the real robot. We investigate various factors necessary for this zero-shot
transfer, like time delay modeling, memory representation, and domain
randomization. Our policy transfers with minimal sim-to-real gap and
significantly outperforms heuristic and learnt baselines. It also generalizes
to plates of different sizes and weights. Demonstration videos and code are
available at https://sites.google.com/view/compliant-object-insertion.",None,-1
fc800a40-2a65-4a4b-8468-1caa53a2dcf6,Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling,0.713495,"The emergence of on-demand ride pooling services allows each vehicle to serve
multiple passengers at a time, thus increasing drivers' income and enabling
passengers to travel at lower prices than taxi/car on-demand services (only one
passenger can be assigned to a car at a time like UberX and Lyft). Although
on-demand ride pooling services can bring so many benefits, ride pooling
services need a well-defined matching strategy to maximize the benefits for all
parties (passengers, drivers, aggregation companies and environment), in which
the regional dispatching of vehicles has a significant impact on the matching
and revenue. Existing algorithms often only consider revenue maximization,
which makes it difficult for requests with unusual distribution to get a ride.
How to increase revenue while ensuring a reasonable assignment of requests
brings a challenge to ride pooling service companies (aggregation companies).
In this paper, we propose a framework for vehicle dispatching for ride pooling
tasks, which splits the city into discrete dispatching regions and uses the
reinforcement learning (RL) algorithm to dispatch vehicles in these regions. We
also consider the mutual information (MI) between vehicle and order
distribution as the intrinsic reward of the RL algorithm to improve the
correlation between their distributions, thus ensuring the possibility of
getting a ride for unusually distributed requests. In experimental results on a
real-world taxi dataset, we demonstrate that our framework can significantly
increase revenue up to an average of 3\% over the existing best on-demand ride
pooling method.",None,-1
97064003-7925-44f1-a06b-ade79b5453d4,Language Models as a Service: Overview of a New Paradigm and its Challenges,0.00852568,"Some of the most powerful language models currently are proprietary systems,
accessible only via (typically restrictive) web or software programming
interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. In
contrast with scenarios where full model access is available, as in the case of
open-source models, such closed-off language models present specific challenges
for evaluating, benchmarking, and testing them. This paper has two goals: on
the one hand, we delineate how the aforementioned challenges act as impediments
to the accessibility, replicability, reliability, and trustworthiness of LMaaS.
We systematically examine the issues that arise from a lack of information
about language models for each of these four aspects. We conduct a detailed
analysis of existing solutions and put forth a number of considered
recommendations, and highlight the directions for future advancements. On the
other hand, it serves as a comprehensive resource for existing knowledge on
current, major LMaaS, offering a synthesized overview of the licences and
capabilities their interfaces offer.",None,-1
df204016-fb34-4425-a4c7-6d7f9ca2723e,Improving neural network representations using human similarity judgments,0.66225,"Deep neural networks have reached human-level performance on many computer
vision tasks. However, the objectives used to train these networks enforce only
that similar images are embedded at similar locations in the representation
space, and do not directly constrain the global structure of the resulting
space. Here, we explore the impact of supervising this global structure by
linearly aligning it with human similarity judgments. We find that a naive
approach leads to large changes in local representational structure that harm
downstream performance. Thus, we propose a novel method that aligns the global
structure of representations while preserving their local structure. This
global-local transform considerably improves accuracy across a variety of
few-shot learning and anomaly detection tasks. Our results indicate that human
visual representations are globally organized in a way that facilitates
learning from few examples, and incorporating this global structure into neural
network representations improves performance on downstream tasks.",None,-1
0b8a569d-f855-4558-a116-e8ffcc763c26,Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings,0.369776,"Languages are dynamic entities, where the meanings associated with words
constantly change with time. Detecting the semantic variation of words is an
important task for various NLP applications that must make time-sensitive
predictions. Existing work on semantic variation prediction have predominantly
focused on comparing some form of an averaged contextualised representation of
a target word computed from a given corpus. However, some of the previously
associated meanings of a target word can become obsolete over time (e.g.
meaning of gay as happy), while novel usages of existing words are observed
(e.g. meaning of cell as a mobile phone). We argue that mean representations
alone cannot accurately capture such semantic variations and propose a method
that uses the entire cohort of the contextualised embeddings of the target
word, which we refer to as the sibling distribution. Experimental results on
SemEval-2020 Task 1 benchmark dataset for semantic variation prediction show
that our method outperforms prior work that consider only the mean embeddings,
and is comparable to the current state-of-the-art. Moreover, a qualitative
analysis shows that our method detects important semantic changes in words that
are not captured by the existing methods. Source code is available at
https://github.com/a1da4/svp-gauss .",None,-1
3f4ddbf6-1650-4654-bea3-78337325e1a1,SeqXGPT: Sentence-Level AI-Generated Text Detection,0.871646,"Widely applied large language models (LLMs) can generate human-like content,
raising concerns about the abuse of LLMs. Therefore, it is important to build
strong AI-generated text (AIGT) detectors. Current works only consider
document-level AIGT detection, therefore, in this paper, we first introduce a
sentence-level detection challenge by synthesizing a dataset that contains
documents that are polished with LLMs, that is, the documents contain sentences
written by humans and sentences modified by LLMs. Then we propose
\textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes
log probability lists from white-box LLMs as features for sentence-level AIGT
detection. These features are composed like \textit{waves} in speech processing
and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution
and self-attention networks. We test it in both sentence and document-level
detection challenges. Experimental results show that previous methods struggle
in solving sentence-level AIGT detection, while our method not only
significantly surpasses baseline methods in both sentence and document-level
detection challenges but also exhibits strong generalization capabilities.",None,-1
167e6a62-45d1-4446-8209-03dc4db9c211,Dynamic Path-Controllable Deep Unfolding Network for Compressive Sensing,0.968284,"Deep unfolding network (DUN) that unfolds the optimization algorithm into a
deep neural network has achieved great success in compressive sensing (CS) due
to its good interpretability and high performance. Each stage in DUN
corresponds to one iteration in optimization. At the test time, all the
sampling images generally need to be processed by all stages, which comes at a
price of computation burden and is also unnecessary for the images whose
contents are easier to restore. In this paper, we focus on CS reconstruction
and propose a novel Dynamic Path-Controllable Deep Unfolding Network (DPC-DUN).
DPC-DUN with our designed path-controllable selector can dynamically select a
rapid and appropriate route for each image and is slimmable by regulating
different performance-complexity tradeoffs. Extensive experiments show that our
DPC-DUN is highly flexible and can provide excellent performance and dynamic
adjustment to get a suitable tradeoff, thus addressing the main requirements to
become appealing in practice. Codes are available at
https://github.com/songjiechong/DPC-DUN.",None,-1
54a3ace3-54fe-4f51-931a-4bd3ec6f8086,MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages in China,0.998497,"Current large language models demonstrate deficiencies in understanding
low-resource languages, particularly the minority languages in China. This
limitation stems from the scarcity of available pre-training data. To address
this accessibility challenge, we present MC$^2$, a Multilingual Corpus of
Minority Languages in China, which is the largest open-source corpus of its
kind so far. MC$^2$ includes four underrepresented languages: Tibetan, Uyghur,
Kazakh, and Mongolian. Notably, we focus on the less common writing systems of
Kazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian
script, respectively, which have been long neglected in previous corpus
construction efforts. Recognizing the prevalence of language contamination
within existing corpora, we adopt a quality-centric solution for collecting
MC$^2$, prioritizing accuracy while enhancing diversity. Furthermore, we
underscore the importance of attending to the multiplicity of writing systems,
which is closely related to the cultural awareness of the resulting models. The
MC$^2$ corpus and related models are made public to the community.",None,-1
a5564152-b1d4-412d-91f0-5910b2084e44,Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4,0.999682,"Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.",None,-1
4bc19c99-fb0d-465d-b383-2de204113d31,From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,0.994528,"Understanding emotions during conversation is a fundamental aspect of human
communication, driving NLP research for Emotion Recognition in Conversation
(ERC). While considerable research has focused on discerning emotions of
individual speakers in monolingual dialogues, understanding the emotional
dynamics in code-mixed conversations has received relatively less attention.
This motivates our undertaking of ERC for code-mixed conversations in this
study. Recognizing that emotional intelligence encompasses a comprehension of
worldly knowledge, we propose an innovative approach that integrates
commonsense information with dialogue context to facilitate a deeper
understanding of emotions. To achieve this, we devise an efficient pipeline
that extracts relevant commonsense from existing knowledge graphs based on the
code-mixed input. Subsequently, we develop an advanced fusion technique that
seamlessly combines the acquired commonsense information with the dialogue
representation obtained from a dedicated dialogue understanding module. Our
comprehensive experimentation showcases the substantial performance improvement
obtained through the systematic incorporation of commonsense in ERC. Both
quantitative assessments and qualitative analyses further corroborate the
validity of our hypothesis, reaffirming the pivotal role of commonsense
integration in enhancing ERC.",None,-1
4765d3ab-9a93-416a-9ba1-23b8ab946ed5,Multi-step Jailbreaking Privacy Attacks on ChatGPT,1.0,"With the rapid progress of large language models (LLMs), many downstream NLP
tasks can be well solved given appropriate prompts. Though model developers and
researchers work hard on dialog safety to avoid generating harmful content from
LLMs, it is still challenging to steer AI-generated content (AIGC) for the
human good. As powerful LLMs are devouring existing text data from various
domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether
the private information is included in the training data and what privacy
threats can these LLMs and their downstream applications bring. In this paper,
we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by
ChatGPT and show that application-integrated LLMs may cause new privacy
threats. To this end, we conduct extensive experiments to support our claims
and discuss LLMs' privacy implications.",None,-1
66ba3158-a9d7-44f6-94ed-5fa7ea4c50a6,How to Choose Pretrained Handwriting Recognition Models for Single Writer Fine-Tuning,0.379607,"Recent advancements in Deep Learning-based Handwritten Text Recognition (HTR)
have led to models with remarkable performance on both modern and historical
manuscripts in large benchmark datasets. Nonetheless, those models struggle to
obtain the same performance when applied to manuscripts with peculiar
characteristics, such as language, paper support, ink, and author handwriting.
This issue is very relevant for valuable but small collections of documents
preserved in historical archives, for which obtaining sufficient annotated
training data is costly or, in some cases, unfeasible. To overcome this
challenge, a possible solution is to pretrain HTR models on large datasets and
then fine-tune them on small single-author collections. In this paper, we take
into account large, real benchmark datasets and synthetic ones obtained with a
styled Handwritten Text Generation model. Through extensive experimental
analysis, also considering the amount of fine-tuning lines, we give a
quantitative indication of the most relevant characteristics of such data for
obtaining an HTR model able to effectively transcribe manuscripts in small
collections with as little as five real fine-tuning lines.",None,-1
b40df4fc-d6a7-4e3b-9e37-607eeb9236cc,FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding,0.726271,"Although Domain Adaptation in Semantic Scene Segmentation has shown
impressive improvement in recent years, the fairness concerns in the domain
adaptation have yet to be well defined and addressed. In addition, fairness is
one of the most critical aspects when deploying the segmentation models into
human-related real-world applications, e.g., autonomous driving, as any unfair
predictions could influence human safety. In this paper, we propose a novel
Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In
particular, from the proposed formulated fairness objective, a new adaptation
framework will be introduced based on the fair treatment of class
distributions. Moreover, to generally model the context of structural
dependency, a new conditional structural constraint is introduced to impose the
consistency of predicted segmentation. Thanks to the proposed Conditional
Structure Network, the self-attention mechanism has sufficiently modeled the
structural information of segmentation. Through the ablation studies, the
proposed method has shown the performance improvement of the segmentation
models and promoted fairness in the model predictions. The experimental results
on the two standard benchmarks, i.e., SYNTHIA $\to$ Cityscapes and GTA5 $\to$
Cityscapes, have shown that our method achieved State-of-the-Art (SOTA)
performance.",None,-1
20a0c98c-1543-4fb1-a875-43dbe1ab0cf7,T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities,0.0643313,"Diffusion Probabilistic Field (DPF) models the distribution of continuous
functions defined over metric spaces. While DPF shows great potential for
unifying data generation of various modalities including images, videos, and 3D
geometry, it does not scale to a higher data resolution. This can be attributed
to the ``scaling property'', where it is difficult for the model to capture
local structures through uniform sampling. To this end, we propose a new model
comprising of a view-wise sampling algorithm to focus on local structure
learning, and incorporating additional guidance, e.g., text description, to
complement the global geometry. The model can be scaled to generate
high-resolution data while unifying multiple modalities. Experimental results
on data generation in various modalities demonstrate the effectiveness of our
model, as well as its potential as a foundation framework for scalable
modality-unified visual content generation.",None,-1
1a352dad-3f5e-48ed-af85-902e260ce4b6,MagicEdit: High-Fidelity and Temporally Coherent Video Editing,0.810554,"In this report, we present MagicEdit, a surprisingly simple yet effective
solution to the text-guided video editing task. We found that high-fidelity and
temporally coherent video-to-video translation can be achieved by explicitly
disentangling the learning of content, structure and motion signals during
training. This is in contradict to most existing methods which attempt to
jointly model both the appearance and temporal representation within a single
framework, which we argue, would lead to degradation in per-frame quality.
Despite its simplicity, we show that MagicEdit supports various downstream
video editing tasks, including video stylization, local editing, video-MagicMix
and video outpainting.",None,-1
41d14c9a-deda-4b85-a2f0-5419cd033063,BiLMa: Bidirectional Local-Matching for Text-based Person Re-identification,0.826398,"Text-based person re-identification (TBPReID) aims to retrieve person images
represented by a given textual query. In this task, how to effectively align
images and texts globally and locally is a crucial challenge. Recent works have
obtained high performances by solving Masked Language Modeling (MLM) to align
image/text parts. However, they only performed uni-directional (i.e., from
image to text) local-matching, leaving room for improvement by introducing
opposite-directional (i.e., from text to image) local-matching. In this work,
we introduce Bidirectional Local-Matching (BiLMa) framework that jointly
optimize MLM and Masked Image Modeling (MIM) in TBPReID model training. With
this framework, our model is trained so as the labels of randomly masked both
image and text tokens are predicted by unmasked tokens. In addition, to narrow
the semantic gap between image and text in MIM, we propose Semantic MIM
(SemMIM), in which the labels of masked image tokens are automatically given by
a state-of-the-art human parser. Experimental results demonstrate that our
BiLMa framework with SemMIM achieves state-of-the-art Rank@1 and mAP scores on
three benchmarks.",None,-1
2613e5fa-e5d5-43a2-b21f-f2f0f028e773,TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective,0.649053,"Vision Transformers (ViTs) have demonstrated powerful representation ability
in various visual tasks thanks to their intrinsic data-hungry nature. However,
we unexpectedly find that ViTs perform vulnerably when applied to face
recognition (FR) scenarios with extremely large datasets. We investigate the
reasons for this phenomenon and discover that the existing data augmentation
approach and hard sample mining strategy are incompatible with ViTs-based FR
backbone due to the lack of tailored consideration on preserving face
structural information and leveraging each local token information. To remedy
these problems, this paper proposes a superior FR model called TransFace, which
employs a patch-level data augmentation strategy named DPAP and a hard sample
mining strategy named EHSM. Specially, DPAP randomly perturbs the amplitude
information of dominant patches to expand sample diversity, which effectively
alleviates the overfitting problem in ViTs. EHSM utilizes the information
entropy in the local tokens to dynamically adjust the importance weight of easy
and hard samples during training, leading to a more stable prediction.
Experiments on several benchmarks demonstrate the superiority of our TransFace.
Code and models are available at https://github.com/DanJun6737/TransFace.",None,-1
d3f87d0c-6249-44bd-bbc7-1c5e681b8034,Designing Behavior Trees from Goal-Oriented LTLf Formulas,0.737184,"Temporal logic can be used to formally specify autonomous agent goals, but
synthesizing planners that guarantee goal satisfaction can be computationally
prohibitive. This paper shows how to turn goals specified using a subset of
finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that
guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for
achievement goals can be derived using achievement-oriented task mission
grammars, leading to missions made up of tasks combined using LTL operators.
Constructing BTs from LTL formulas leads to a relaxed behavior synthesis
problem in which a wide range of planners can implement the action nodes in the
BT. Importantly, any successful trace induced by the planners satisfies the
corresponding LTL formula. The usefulness of the approach is demonstrated in
two ways: a) exploring the alignment between two planners and LTL goals, and b)
solving a sequential key-door problem for a Fetch robot.",None,-1
227657e1-f4ba-4ab5-936d-e6cdb6e7f60c,"""You might think about slightly revising the title"": identifying hedges in peer-tutoring interactions",0.991125,"Hedges play an important role in the management of conversational
interaction. In peer tutoring, they are notably used by tutors in dyads (pairs
of interlocutors) experiencing low rapport to tone down the impact of
instructions and negative feedback. Pursuing the objective of building a
tutoring agent that manages rapport with students in order to improve learning,
we used a multimodal peer-tutoring dataset to construct a computational
framework for identifying hedges. We compared approaches relying on pre-trained
resources with others that integrate insights from the social science
literature. Our best performance involved a hybrid approach that outperforms
the existing baseline while being easier to interpret. We employ a model
explainability tool to explore the features that characterize hedges in
peer-tutoring conversations, and we identify some novel features, and the
benefits of such a hybrid model approach.",None,-1
3a8b808a-63a1-451b-a47a-c6aea5d8934d,Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences,0.151729,"Previous studies show that intermediate supervision signals benefit various
Natural Language Processing tasks. However, it is not clear whether there exist
intermediate signals that benefit Neural Machine Translation (NMT). Borrowing
techniques from Statistical Machine Translation, we propose intermediate
signals which are intermediate sequences from the ""source-like"" structure to
the ""target-like"" structure. Such intermediate sequences introduce an inductive
bias that reflects a domain-agnostic principle of translation, which reduces
spurious correlations that are harmful to out-of-domain generalisation.
Furthermore, we introduce a full-permutation multi-task learning to alleviate
the spurious causal relations from intermediate sequences to the target, which
results from exposure bias. The Minimum Bayes Risk decoding algorithm is used
to pick the best candidate translation from all permutations to further improve
the performance. Experiments show that the introduced intermediate signals can
effectively improve the domain robustness of NMT and reduces the amount of
hallucinations on out-of-domain translation. Further analysis shows that our
methods are especially promising in low-resource scenarios.",None,-1
1b72ce7c-0c51-49ea-91c9-4675b36b7c52,Sanity checks and improvements for patch visualisation in prototype-based image classification,0.0304891,"In this work, we perform an in-depth analysis of the visualisation methods
implemented in two popular self-explaining models for visual classification
based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets
(CUB-200-2011 and Stanford Cars), we first show that such methods do not
correctly identify the regions of interest inside of the images, and therefore
do not reflect the model behaviour. Secondly, using a deletion metric, we
demonstrate quantitatively that saliency methods such as Smoothgrads or PRP
provide more faithful image patches. We also propose a new relevance metric
based on the segmentation of the object provided in some datasets (e.g.
CUB-200-2011) and show that the imprecise patch visualisations generated by
ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated
by the use of more faithful methods. Finally, we discuss the implications of
our findings for other prototype-based models sharing the same visualisation
method.",None,-1
3d74e05b-382c-4a6c-8469-94bb86b4ec5c,TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models,0.65292,"Data augmentation has been established as an efficacious approach to
supplement useful information for low-resource datasets. Traditional
augmentation techniques such as noise injection and image transformations have
been widely used. In addition, generative data augmentation (GDA) has been
shown to produce more diverse and flexible data. While generative adversarial
networks (GANs) have been frequently used for GDA, they lack diversity and
controllability compared to text-to-image diffusion models. In this paper, we
propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the
capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image
(T2I) generative models for data augmentation. By conditioning the T2I model on
detailed descriptions produced by T2T models, we are able to generate
photo-realistic labeled images in a flexible and controllable manner.
Experiments on in-domain classification, cross-domain classification, and image
captioning tasks show consistent improvements over other data augmentation
baselines. Analytical studies in varied settings, including few-shot,
long-tail, and adversarial, further reinforce the effectiveness of TTIDA in
enhancing performance and increasing robustness.",None,-1
8980c8df-9365-40cd-94ca-f355202bca9d,"CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction, and its Application to Two-player Mahjong",0.274597,"Counterfactual Regret Minimization(CFR) has shown its success in Texas
Hold'em poker. We apply this algorithm to another popular incomplete
information game, Mahjong. Compared to the poker game, Mahjong is much more
complex with many variants. We study two-player Mahjong by conducting game
theoretical analysis and making a hierarchical abstraction to CFR based on
winning policies. This framework can be generalized to other imperfect
information games.",None,-1
55a9bc3f-817d-40de-affa-08b06089839e,Sensemaking About Contraceptive Methods Across Online Platforms,0.0754004,"Selecting a birth control method is a complex healthcare decision. While
birth control methods provide important benefits, they can also cause
unpredictable side effects and be stigmatized, leading many people to seek
additional information online, where they can find reviews, advice, hypotheses,
and experiences of other birth control users. However, the relationships
between their healthcare concerns, sensemaking activities, and online settings
are not well understood. We gather texts about birth control shared on Twitter,
Reddit, and WebMD -- platforms with different affordances, moderation, and
audiences -- to study where and how birth control is discussed online. Using a
combination of topic modeling and hand annotation, we identify and characterize
the dominant sensemaking practices across these platforms, and we create
lexicons to draw comparisons across birth control methods and side effects. We
use these to measure variations from survey reports of side effect experiences
and method usage. Our findings characterize how online platforms are used to
make sense of difficult healthcare choices and highlight unmet needs of birth
control users.",None,-1
1d5890fe-0279-43e0-b735-b468eac6ef30,Parameter Efficient Local Implicit Image Function Network for Face Segmentation,0.169099,"Face parsing is defined as the per-pixel labeling of images containing human
faces. The labels are defined to identify key facial regions like eyes, lips,
nose, hair, etc. In this work, we make use of the structural consistency of the
human face to propose a lightweight face-parsing method using a Local Implicit
Function network, FP-LIIF. We propose a simple architecture having a
convolutional encoder and a pixel MLP decoder that uses 1/26th number of
parameters compared to the state-of-the-art models and yet matches or
outperforms state-of-the-art models on multiple datasets, like CelebAMask-HQ
and LaPa. We do not use any pretraining, and compared to other works, our
network can also generate segmentation at different resolutions without any
changes in the input resolution. This work enables the use of facial
segmentation on low-compute or low-bandwidth devices because of its higher FPS
and smaller model size.",None,-1
ea8f8173-9878-4ec2-a83c-eeb874627d06,A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,0.667166,"Recent instruction fine-tuned models can solve multiple NLP tasks when
prompted to do so, with machine translation (MT) being a prominent use case.
However, current research often focuses on standard performance benchmarks,
leaving compelling fairness and ethical considerations behind. In MT, this
might lead to misgendered translations, resulting, among other harms, in the
perpetuation of stereotypes and prejudices. In this work, we address this gap
by investigating whether and to what extent such models exhibit gender bias in
machine translation and how we can mitigate it. Concretely, we compute
established gender bias metrics on the WinoMT corpus from English to German and
Spanish. We discover that IFT models default to male-inflected translations,
even disregarding female occupational stereotypes. Next, using interpretability
methods, we unveil that models systematically overlook the pronoun indicating
the gender of a target occupation in misgendered translations. Finally, based
on this finding, we propose an easy-to-implement and effective bias mitigation
solution based on few-shot learning that leads to significantly fairer
translations.",None,-1
5d18f9ee-b185-4774-8f34-40037d2d77d2,Exploiting Neighborhood Structural Features for Change Detection,0.226176,"In this letter, a novel method for change detection is proposed using
neighborhood structure correlation. Because structure features are insensitive
to the intensity differences between bi-temporal images, we perform the
correlation analysis on structure features rather than intensity information.
First, we extract the structure feature maps by using multi-orientated gradient
information. Then, the structure feature maps are used to obtain the
Neighborhood Structural Correlation Image (NSCI), which can represent the
context structure information. In addition, we introduce a measure named
matching error which can be used to improve neighborhood information.
Subsequently, a change detection model based on the random forest is
constructed. The NSCI feature and matching error are used as the model inputs
for training and prediction. Finally, the decision tree voting is used to
produce the change detection result. To evaluate the performance of the
proposed method, it was compared with three state-of-the-art change detection
methods. The experimental results on two datasets demonstrated the
effectiveness and robustness of the proposed method.",None,-1
afe46827-c928-4015-a666-2eff23330e4b,GarmentTracking: Category-Level Garment Pose Tracking,0.741504,"Garments are important to humans. A visual system that can estimate and track
the complete garment pose can be useful for many downstream tasks and
real-world applications. In this work, we present a complete package to address
the category-level garment pose tracking task: (1) A recording system
VR-Garment, with which users can manipulate virtual garment models in
simulation through a VR interface. (2) A large-scale dataset VR-Folding, with
complex garment pose configurations in manipulation like flattening and
folding. (3) An end-to-end online tracking framework GarmentTracking, which
predicts complete garment pose both in canonical space and task space given a
point cloud sequence. Extensive experiments demonstrate that the proposed
GarmentTracking achieves great performance even when the garment has large
non-rigid deformation. It outperforms the baseline approach on both speed and
accuracy. We hope our proposed solution can serve as a platform for future
research. Codes and datasets are available in
https://garment-tracking.robotflow.ai.",None,-1
22f7570e-ad3e-48ef-907d-fcb0e20db5c7,Physics-Inspired Interpretability Of Machine Learning Models,0.109413,"The ability to explain decisions made by machine learning models remains one
of the most significant hurdles towards widespread adoption of AI in highly
sensitive areas such as medicine, cybersecurity or autonomous driving. Great
interest exists in understanding which features of the input data prompt model
decision making. In this contribution, we propose a novel approach to identify
relevant features of the input data, inspired by methods from the energy
landscapes field, developed in the physical sciences. By identifying conserved
weights within groups of minima of the loss landscapes, we can identify the
drivers of model decision making. Analogues to this idea exist in the molecular
sciences, where coordinate invariants or order parameters are employed to
identify critical features of a molecule. However, no such approach exists for
machine learning loss landscapes. We will demonstrate the applicability of
energy landscape methods to machine learning models and give examples, both
synthetic and from the real world, for how these methods can help to make
models more interpretable.",None,-1
15ceca32-a874-476d-86aa-30100968a25d,Visual Watermark Removal Based on Deep Learning,0.244895,"In recent years as the internet age continues to grow, sharing images on
social media has become a common occurrence. In certain cases, watermarks are
used as protection for the ownership of the image, however, in more cases, one
may wish to remove these watermark images to get the original image without
obscuring. In this work, we proposed a deep learning method based technique for
visual watermark removal. Inspired by the strong image translation performance
of the U-structure, an end-to-end deep neural network model named AdvancedUnet
is proposed to extract and remove the visual watermark simultaneously. On the
other hand, we embed some effective RSU module instead of the common residual
block used in UNet, which increases the depth of the whole architecture without
significantly increasing the computational cost. The deep-supervised hybrid
loss guides the network to learn the transformation between the input image and
the ground truth in a multi-scale and three-level hierarchy. Comparison
experiments demonstrate the effectiveness of our method.",None,-1
8809e893-6b9f-4044-96a2-ddd64189b999,Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis,0.0996835,"Generative Engineering Design approaches driven by Deep Generative Models
(DGM) have been proposed to facilitate industrial engineering processes. In
such processes, designs often come in the form of images, such as blueprints,
engineering drawings, and CAD models depending on the level of detail. DGMs
have been successfully employed for synthesis of natural images, e.g.,
displaying animals, human faces and landscapes. However, industrial design
images are fundamentally different from natural scenes in that they contain
rich structural patterns and long-range dependencies, which are challenging for
convolution-based DGMs to generate. Moreover, DGM-driven generation process is
typically triggered based on random noisy inputs, which outputs unpredictable
samples and thus cannot perform an efficient industrial design exploration. We
tackle these challenges by proposing a novel model Self-Attention Adversarial
Latent Autoencoder (SA-ALAE), which allows generating feasible design images of
complex engineering parts. With SA-ALAE, users can not only explore novel
variants of an existing design, but also control the generation process by
operating in latent space. The potential of SA-ALAE is shown by generating
engineering blueprints in a real automotive design task.",None,-1
83748743-fcaa-4a58-b73c-feed0083c73a,Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora,0.695636,"Grammatical error correction (GEC) is the task of correcting typos, spelling,
punctuation and grammatical issues in text. Approaching the problem as a
sequence-to-sequence task, we compare the use of a common subword unit
vocabulary and byte-level encoding. Initial synthetic training data is created
using an error-generating pipeline, and used for finetuning two subword-level
models and one byte-level model. Models are then finetuned further on
hand-corrected error corpora, including texts written by children, university
students, dyslexic and second-language writers, and evaluated over different
error types and origins. We show that a byte-level model enables higher
correction quality than a subword approach, not only for simple spelling
errors, but also for more complex semantic, stylistic and grammatical issues.
In particular, initial training on synthetic corpora followed by finetuning on
a relatively small parallel corpus of real-world errors helps the byte-level
model correct a wide range of commonly occurring errors. Our experiments are
run for the Icelandic language but should hold for other similar languages,
particularly morphologically rich ones.",None,-1
c12cc9e8-f223-4d0d-8fcd-991b9b8394cf,MVDream: Multi-view Diffusion for 3D Generation,1.0,"We introduce MVDream, a diffusion model that is able to generate consistent
multi-view images from a given text prompt. Learning from both 2D and 3D data,
a multi-view diffusion model can achieve the generalizability of 2D diffusion
models and the consistency of 3D renderings. We demonstrate that such a
multi-view diffusion model is implicitly a generalizable 3D prior agnostic to
3D representations. It can be applied to 3D generation via Score Distillation
Sampling, significantly enhancing the consistency and stability of existing
2D-lifting methods. It can also learn new concepts from a few 2D examples, akin
to DreamBooth, but for 3D generation.",None,-1
7bbce0af-4295-49e3-b1cf-61c1019c2fd8,Single Frame Semantic Segmentation Using Multi-Modal Spherical Images,0.147475,"In recent years, the research community has shown a lot of interest to
panoramic images that offer a 360-degree directional perspective. Multiple data
modalities can be fed, and complimentary characteristics can be utilized for
more robust and rich scene interpretation based on semantic segmentation, to
fully realize the potential. Existing research, however, mostly concentrated on
pinhole RGB-X semantic segmentation. In this study, we propose a
transformer-based cross-modal fusion architecture to bridge the gap between
multi-modal fusion and omnidirectional scene perception. We employ
distortion-aware modules to address extreme object deformations and panorama
distortions that result from equirectangular representation. Additionally, we
conduct cross-modal interactions for feature rectification and information
exchange before merging the features in order to communicate long-range
contexts for bi-modal and tri-modal feature streams. In thorough tests using
combinations of four different modality types in three indoor panoramic-view
datasets, our technique achieved state-of-the-art mIoU performance: 60.60% on
Stanford2D3DS (RGB-HHA), 71.97% Structured3D (RGB-D-N), and 35.92% Matterport3D
(RGB-D). We plan to release all codes and trained models soon.",None,-1
2d31df11-7eb7-4b97-b62c-4c9c21b445af,MTS-Mixers: Multivariate Time Series Forecasting via Factorized Temporal and Channel Mixing,0.8702,"Multivariate time series forecasting has been widely used in various
practical scenarios. Recently, Transformer-based models have shown significant
potential in forecasting tasks due to the capture of long-range dependencies.
However, recent studies in the vision and NLP fields show that the role of
attention modules is not clear, which can be replaced by other token
aggregation operations. This paper investigates the contributions and
deficiencies of attention mechanisms on the performance of time series
forecasting. Specifically, we find that (1) attention is not necessary for
capturing temporal dependencies, (2) the entanglement and redundancy in the
capture of temporal and channel interaction affect the forecasting performance,
and (3) it is important to model the mapping between the input and the
prediction sequence. To this end, we propose MTS-Mixers, which use two
factorized modules to capture temporal and channel dependencies. Experimental
results on several real-world datasets show that MTS-Mixers outperform existing
Transformer-based models with higher efficiency.",None,-1
7ca5c37f-47d7-409b-9bf3-208b1f6eb7c9,Addressing Cold Start Problem for End-to-end Automatic Speech Scoring,0.266786,"Integrating automatic speech scoring/assessment systems has become a critical
aspect of second-language speaking education. With self-supervised learning
advancements, end-to-end speech scoring approaches have exhibited promising
results. However, this study highlights the significant decrease in the
performance of speech scoring systems in new question contexts, thereby
identifying this as a cold start problem in terms of items. With the finding of
cold-start phenomena, this paper seeks to alleviate the problem by following
methods: 1) prompt embeddings, 2) question context embeddings using BERT or
CLIP models, and 3) choice of the pretrained acoustic model. Experiments are
conducted on TOEIC speaking test datasets collected from
English-as-a-second-language (ESL) learners rated by professional TOEIC
speaking evaluators. The results demonstrate that the proposed framework not
only exhibits robustness in a cold-start environment but also outperforms the
baselines for known content.",None,-1
9314641c-3813-4e46-b569-dbf444b4185d,Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism,0.323201,"Transformer-based Large Language Models (LLMs) are the state-of-the-art for
natural language tasks. Recent work has attempted to decode, by reverse
engineering the role of linear layers, the internal mechanisms by which LLMs
arrive at their final predictions for text completion tasks. Yet little is
known about the specific role of attention heads in producing the final token
prediction. We propose Attention Lens, a tool that enables researchers to
translate the outputs of attention heads into vocabulary tokens via learned
attention-head-specific transformations called lenses. Preliminary findings
from our trained lenses indicate that attention heads play highly specialized
roles in language models. The code for Attention Lens is available at
github.com/msakarvadia/AttentionLens.",None,-1
3194dddf-acda-43a2-b88c-0457eb99ad1d,Fine-Tashkeel: Finetuning Byte-Level Models for Accurate Arabic Text Diacritization,0.524996,"Most of previous work on learning diacritization of the Arabic language
relied on training models from scratch. In this paper, we investigate how to
leverage pre-trained language models to learn diacritization. We finetune
token-free pre-trained multilingual models (ByT5) to learn to predict and
insert missing diacritics in Arabic text, a complex task that requires
understanding the sentence semantics and the morphological structure of the
tokens. We show that we can achieve state-of-the-art on the diacritization task
with minimal amount of training and no feature engineering, reducing WER by
40%. We release our finetuned models for the greater benefit of the researchers
in the community.",None,-1
87023de7-6dfa-499a-8b89-ea4f0506f4d0,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,0.701527,"In this paper, we propose CGI-Stereo, a novel neural network architecture
that can concurrently achieve real-time performance, competitive accuracy, and
strong generalization ability. The core of our CGI-Stereo is a Context and
Geometry Fusion (CGF) block which adaptively fuses context and geometry
information for more effective cost aggregation and meanwhile provides feedback
to feature learning to guide more effective contextual feature extraction. The
proposed CGF can be easily embedded into many existing stereo matching
networks, such as PSMNet, GwcNet and ACVNet. The resulting networks show a
significant improvement in accuracy. Specially, the model which incorporates
our CGF with ACVNet ranks $1^{st}$ on the KITTI 2012 and 2015 leaderboards
among all the published methods. We further propose an informative and concise
cost volume, named Attention Feature Volume (AFV), which exploits a correlation
volume as attention weights to filter a feature volume. Based on CGF and AFV,
the proposed CGI-Stereo outperforms all other published real-time methods on
KITTI benchmarks and shows better generalization ability than other real-time
methods. Code is available at https://github.com/gangweiX/CGI-Stereo.",None,-1
1171fcaa-1fe7-420d-bf6f-60b7cb4cdabc,Full Resolution Repetition Counting,0.123625,"Given an untrimmed video, repetitive actions counting aims to estimate the
number of repetitions of class-agnostic actions. To handle the various length
of videos and repetitive actions, also optimization challenges in end-to-end
video model training, down-sampling is commonly utilized in recent
state-of-the-art methods, leading to ignorance of several repetitive samples.
In this paper, we attempt to understand repetitive actions from a full temporal
resolution view, by combining offline feature extraction and temporal
convolution networks. The former step enables us to train repetition counting
network without down-sampling while preserving all repetition regardless of the
video length and action frequency, and the later network models all frames in a
flexible and dynamically expanding temporal receptive field to retrieve all
repetitions with a global aspect. We experimentally demonstrate that our method
achieves better or comparable performance in three public datasets, i.e.,
TransRAC, UCFRep and QUVA. We expect this work will encourage our community to
think about the importance of full temporal resolution.",None,-1
648b2dd7-dfff-41b4-ba1e-f875d07176cf,Optimized Table Tokenization for Table Structure Recognition,0.37221,"Extracting tables from documents is a crucial task in any document conversion
pipeline. Recently, transformer-based models have demonstrated that
table-structure can be recognized with impressive accuracy using
Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table,
such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent
the structure of the table. Since the token representation of the table
structure has a significant impact on the accuracy and run-time performance of
any Im2Seq model, we investigate in this paper how table-structure
representation can be optimised. We propose a new, optimised table-structure
language (OTSL) with a minimized vocabulary and specific rules. The benefits of
OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and
shortens the sequence length to half of HTML on average. Consequently, model
accuracy improves significantly, inference time is halved compared to
HTML-based models, and the predicted table structures are always syntactically
correct. This in turn eliminates most post-processing needs.",None,-1
2e98998c-0f86-49b3-8a55-f6d6405f2010,Why Can Large Language Models Generate Correct Chain-of-Thoughts?,0.381856,"This paper delves into the capabilities of large language models (LLMs),
specifically focusing on advancing the theoretical comprehension of
chain-of-thought prompting. We investigate how LLMs can be effectively induced
to generate a coherent chain of thoughts. To achieve this, we introduce a
two-level hierarchical graphical model tailored for natural language
generation. Within this framework, we establish a compelling geometrical
convergence rate that gauges the likelihood of an LLM-generated chain of
thoughts compared to those originating from the true language. Our findings
provide a theoretical justification for the ability of LLMs to produce the
correct sequence of thoughts (potentially) explaining performance gains in
tasks demanding reasoning skills.",None,-1
f64a82aa-1068-45ff-957c-c7dd7cf26fca,A Deep Behavior Path Matching Network for Click-Through Rate Prediction,0.557947,"User behaviors on an e-commerce app not only contain different kinds of
feedback on items but also sometimes imply the cognitive clue of the user's
decision-making. For understanding the psychological procedure behind user
decisions, we present the behavior path and propose to match the user's current
behavior path with historical behavior paths to predict user behaviors on the
app. Further, we design a deep neural network for behavior path matching and
solve three difficulties in modeling behavior paths: sparsity, noise
interference, and accurate matching of behavior paths. In particular, we
leverage contrastive learning to augment user behavior paths, provide behavior
path self-activation to alleviate the effect of noise, and adopt a two-level
matching mechanism to identify the most appropriate candidate. Our model shows
excellent performance on two real-world datasets, outperforming the
state-of-the-art CTR model. Moreover, our model has been deployed on the
Meituan food delivery platform and has accumulated 1.6% improvement in CTR and
1.8% improvement in advertising revenue.",None,-1
d61e85a4-c856-4a28-91c2-cbc3670a1ae3,AffectMachine-Classical: A novel system for generating affective classical music,0.035964,"This work introduces a new music generation system, called
AffectMachine-Classical, that is capable of generating affective Classic music
in real-time. AffectMachine was designed to be incorporated into biofeedback
systems (such as brain-computer-interfaces) to help users become aware of, and
ultimately mediate, their own dynamic affective states. That is, this system
was developed for music-based MedTech to support real-time emotion
self-regulation in users. We provide an overview of the rule-based,
probabilistic system architecture, describing the main aspects of the system
and how they are novel. We then present the results of a listener study that
was conducted to validate the ability of the system to reliably convey target
emotions to listeners. The findings indicate that AffectMachine-Classical is
very effective in communicating various levels of Arousal ($R^2 = .96$) to
listeners, and is also quite convincing in terms of Valence (R^2 = .90). Future
work will embed AffectMachine-Classical into biofeedback systems, to leverage
the efficacy of the affective music for emotional well-being in listeners.",None,-1
6b5998e2-d7c8-4655-b4e6-6634f087390a,SceneCalib: Automatic Targetless Calibration of Cameras and Lidars in Autonomous Driving,0.439047,"Accurate camera-to-lidar calibration is a requirement for sensor data fusion
in many 3D perception tasks. In this paper, we present SceneCalib, a novel
method for simultaneous self-calibration of extrinsic and intrinsic parameters
in a system containing multiple cameras and a lidar sensor. Existing methods
typically require specially designed calibration targets and human operators,
or they only attempt to solve for a subset of calibration parameters. We
resolve these issues with a fully automatic method that requires no explicit
correspondences between camera images and lidar point clouds, allowing for
robustness to many outdoor environments. Furthermore, the full system is
jointly calibrated with explicit cross-camera constraints to ensure that
camera-to-camera and camera-to-lidar extrinsic parameters are consistent.",None,-1
00c4db51-d9e5-4a0e-98e6-7c5f4dd8f2e9,UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM,0.868616,"We present an uncertainty learning framework for dense neural simultaneous
localization and mapping (SLAM). Estimating pixel-wise uncertainties for the
depth input of dense SLAM methods allows re-weighing the tracking and mapping
losses towards image regions that contain more suitable information that is
more reliable for SLAM. To this end, we propose an online framework for sensor
uncertainty estimation that can be trained in a self-supervised manner from
only 2D input data. We further discuss the advantages of the uncertainty
learning for the case of multi-sensor input. Extensive analysis,
experimentation, and ablations show that our proposed modeling paradigm
improves both mapping and tracking accuracy and often performs better than
alternatives that require ground truth depth or 3D. Our experiments show that
we achieve a 38\% and 27\% lower absolute trajectory tracking error (ATE) on
the 7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset
using two types of depth sensors, we report an 11\% F1-score improvement on
RGBD SLAM compared to the recent state-of-the-art neural implicit approaches.
Source code: https://github.com/kev-in-ta/UncLe-SLAM.",None,-1
4ee8eda3-462e-479a-9292-962b0cd76d3b,Findings of the TSAR-2022 Shared Task on Multilingual Lexical Simplification,0.991412,"We report findings of the TSAR-2022 shared task on multilingual lexical
simplification, organized as part of the Workshop on Text Simplification,
Accessibility, and Readability TSAR-2022 held in conjunction with EMNLP 2022.
The task called the Natural Language Processing research community to
contribute with methods to advance the state of the art in multilingual lexical
simplification for English, Portuguese, and Spanish. A total of 14 teams
submitted the results of their lexical simplification systems for the provided
test data. Results of the shared task indicate new benchmarks in Lexical
Simplification with English lexical simplification quantitative results
noticeably higher than those obtained for Spanish and (Brazilian) Portuguese.",None,-1
3b262386-998f-4c11-9afb-05dc445b0382,Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI,0.121113,"Large Language Models (LLMs) are capable of reasoning over diverse input data
modalities through pre-trained encoders. However, the growing diversity of
input data modalities prevents incorporating all modalities into LLMs,
especially when LLMs are deployed on resource-constrained edge devices for
embodied AI applications. Instead, a better option is to adaptively involve
only the useful modalities at runtime, depending on the current environmental
contexts and task requirements. For such modality adaptation, existing work
adopts fixed connections between encoders and the LLM's input layer, leading to
high training cost at runtime and ineffective cross-modal interaction. In this
paper, we address these limitations by presenting mPnP-LLM, a new technique
that allows fully elastic, automated and prompt runtime modality adaptation, by
connecting unimodal encoders to a flexible set of last LLM blocks and making
such latent connections fully trainable at runtime. Experiments over the
nuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction
and 30% GPU memory usage reduction, while retaining on-par accuracy with the
existing schemes. Under the same compute budget, mPnP-LLM improves the task
accuracy by up to 4% compared to the best existing scheme.",None,-1
484b4a10-e957-4a48-bff1-17e0e182120f,Analysis over vision-based models for pedestrian action anticipation,0.527633,"Anticipating human actions in front of autonomous vehicles is a challenging
task. Several papers have recently proposed model architectures to address this
problem by combining multiple input features to predict pedestrian crossing
actions. This paper focuses specifically on using images of the pedestrian's
context as an input feature. We present several spatio-temporal model
architectures that utilize standard CNN and Transformer modules to serve as a
backbone for pedestrian anticipation. However, the objective of this paper is
not to surpass state-of-the-art benchmarks but rather to analyze the positive
and negative predictions of these models. Therefore, we provide insights on the
explainability of vision-based Transformer models in the context of pedestrian
action prediction. We will highlight cases where the model can achieve correct
quantitative results but falls short in providing human-like explanations
qualitatively, emphasizing the importance of investing in explainability for
pedestrian action anticipation problems.",None,-1
79a669b4-dafb-42b1-84f0-6651793a2c04,Red Teaming Language Model Detectors with Language Models,0.700509,"The prevalence and strong capability of large language models (LLMs) present
significant safety and ethical risks if exploited by malicious users. To
prevent the potentially deceptive usage of LLMs, recent works have proposed
algorithms to detect LLM-generated text and protect LLMs. In this paper, we
investigate the robustness and reliability of these LLM detectors under
adversarial attacks. We study two types of attack strategies: 1) replacing
certain words in an LLM's output with their synonyms given the context; 2)
automatically searching for an instructional prompt to alter the writing style
of the generation. In both strategies, we leverage an auxiliary LLM to generate
the word replacements or the instructional prompt. Different from previous
works, we consider a challenging setting where the auxiliary LLM can also be
protected by a detector. Experiments reveal that our attacks effectively
compromise the performance of all detectors in the study with plausible
generations, underscoring the urgent need to improve the robustness of
LLM-generated text detection systems.",None,-1
2e8b928d-6f49-4831-b22f-b964756d5439,Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,0.764274,"Medical Visual Question Answering (VQA) is an important challenge, as it
would lead to faster and more accurate diagnoses and treatment decisions. Most
existing methods approach it as a multi-class classification problem, which
restricts the outcome to a predefined closed-set of curated answers. We focus
on open-ended VQA and motivated by the recent advances in language models
consider it as a generative task. Leveraging pre-trained language models, we
introduce a novel method particularly suited for small, domain-specific,
medical datasets. To properly communicate the medical images to the language
model, we develop a network that maps the extracted visual features to a set of
learnable tokens. Then, alongside the question, these learnable tokens directly
prompt the language model. We explore recent parameter-efficient fine-tuning
strategies for language models, which allow for resource- and data-efficient
fine-tuning. We evaluate our approach on the prime medical VQA benchmarks,
namely, Slake, OVQA and PathVQA. The results demonstrate that our approach
outperforms existing methods across various training settings while also being
computationally efficient.",None,-1
fb66689b-12fd-4c6f-bf0d-af77be872825,Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning,0.368393,"The Contrastive Language-Image Pre-training (CLIP) has recently shown
remarkable generalization on ""zero-shot"" training and has applied to many
downstream tasks. We explore the adaptation of CLIP to achieve a more efficient
and generalized action recognition method. We propose that the key lies in
explicitly modeling the motion cues flowing in video frames. To that end, we
design a two-stream motion modeling block to capture motion and spatial
information at the same time. And then, the obtained motion cues are utilized
to drive a dynamic prompts learner to generate motion-aware prompts, which
contain much semantic information concerning human actions. In addition, we
propose a multimodal communication block to achieve a collaborative learning
and further improve the performance. We conduct extensive experiments on
HMDB-51, UCF-101, and Kinetics-400 datasets. Our method outperforms most
existing state-of-the-art methods by a significant margin on ""few-shot"" and
""zero-shot"" training. We also achieve competitive performance on ""closed-set""
training with extremely few trainable parameters and additional computational
costs.",None,-1
e81d19c7-82dc-4b3e-9905-0f9c832885da,Autoregressive Modeling with Lookahead Attention,0.175191,"To predict the next token, autoregressive models ordinarily examine the past.
Could they also benefit from also examining hypothetical futures? We consider a
novel Transformer-based autoregressive architecture that estimates the
next-token distribution by extrapolating multiple continuations of the past,
according to some proposal distribution, and attending to these extended
strings. This architecture draws insights from classical AI systems such as
board game players: when making a local decision, a policy may benefit from
exploring possible future trajectories and analyzing them. On multiple tasks
including morphological inflection and Boolean satisfiability, our lookahead
model is able to outperform the ordinary Transformer model of comparable size.
However, on some tasks, it appears to be benefiting from the extra computation
without actually using the lookahead information. We discuss possible variant
architectures as well as future speedups.",None,-1
115f1c8a-8f6f-4dbd-8890-16b29811589b,"KGConv, a Conversational Corpus grounded in Wikidata",0.110941,"We present KGConv, a large, conversational corpus of 71k conversations where
each question-answer pair is grounded in a Wikidata fact. Conversations contain
on average 8.6 questions and for each Wikidata fact, we provide multiple
variants (12 on average) of the corresponding question using templates, human
annotations, hand-crafted rules and a question rewriting neural model. We
provide baselines for the task of Knowledge-Based, Conversational Question
Generation. KGConv can further be used for other generation and analysis tasks
such as single-turn question generation from Wikidata triples, question
rewriting, question answering from conversation or from knowledge graphs and
quiz generation.",None,-1
e30921b6-a296-4115-952f-a02804e7059b,OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for Real-time 3D Hand Gesture Classification and Segmentation,0.561454,"Continuous mid-air hand gesture recognition based on captured hand pose
streams is fundamental for human-computer interaction, particularly in AR / VR.
However, many of the methods proposed to recognize heterogeneous hand gestures
are tested only on the classification task, and the real-time low-latency
gesture segmentation in a continuous stream is not well addressed in the
literature. For this task, we propose the On-Off deep Multi-View Multi-Task
paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related
to hand pose and movement to generate rich gesture descriptions, along with
using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the
classical MVMT paradigm, where all of the multiple tasks have to be active at
each time, by allowing specific tasks to switch on/off depending on whether
they can apply to the input. We show that OO-dMVMT defines the new SotA on
continuous/online 3D skeleton-based gesture recognition in terms of gesture
classification accuracy, segmentation accuracy, false positives, and decision
latency while maintaining real-time operation.",None,-1
37cb3538-c301-417d-a43b-1bfbbbeff671,Data-Free Distillation of Language Model by Text-to-Text Transfer,0.252638,"Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the
model when original training data is unavailable. Previous works for DFKD in
NLP mainly focus on distilling encoder-only structures like BERT on
classification tasks, which overlook the notable progress of generative
language modeling. In this work, we propose a novel DFKD framework, namely
DFKD-T$^{3}$, where the pretrained generative language model can also serve as
a controllable data generator for model compression. This novel framework
DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to
transform the general domain corpus to compression-friendly task data,
targeting to improve both the \textit{specificity} and \textit{diversity}.
Extensive experiments show that our method can boost the distillation
performance in various downstream tasks such as sentiment analysis, linguistic
acceptability, and information extraction. Furthermore, we show that the
generated texts can be directly used for distilling other language models and
outperform the SOTA methods, making our method more appealing in a general DFKD
setting. Our code is available at
https://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\_T3.",None,-1
dd0cdc31-082a-42ad-b149-663c27b158a0,LLM Cognitive Judgements Differ From Human,0.0780877,"Large Language Models (LLMs) have lately been on the spotlight of
researchers, businesses, and consumers alike. While the linguistic capabilities
of such models have been studied extensively, there is growing interest in
investigating them as cognitive subjects. In the present work I examine GPT-3
and ChatGPT capabilities on an limited-data inductive reasoning task from the
cognitive science literature. The results suggest that these models' cognitive
judgements are not human-like.",None,-1
079b688b-29ae-4569-ac63-ae308cbe2bae,Comparing Measures of Linguistic Diversity Across Social Media Language Data and Census Data at Subnational Geographic Areas,0.0683018,"This paper describes a preliminary study on the comparative linguistic
ecology of online spaces (i.e., social media language data) and real-world
spaces in Aotearoa New Zealand (i.e., subnational administrative areas). We
compare measures of linguistic diversity between these different spaces and
discuss how social media users align with real-world populations. The results
from the current study suggests that there is potential to use online social
media language data to observe spatial and temporal changes in linguistic
diversity at subnational geographic areas; however, further work is required to
understand how well social media represents real-world behaviour.",None,-1
189a2e30-d59c-4e8a-b6a0-265bf0afff86,Playing the Werewolf game with artificial intelligence for language understanding,0.932801,"The Werewolf game is a social deduction game based on free natural language
communication, in which players try to deceive others in order to survive. An
important feature of this game is that a large portion of the conversations are
false information, and the behavior of artificial intelligence (AI) in such a
situation has not been widely investigated. The purpose of this study is to
develop an AI agent that can play Werewolf through natural language
conversations. First, we collected game logs from 15 human players. Next, we
fine-tuned a Transformer-based pretrained language model to construct a value
network that can predict a posterior probability of winning a game at any given
phase of the game and given a candidate for the next action. We then developed
an AI agent that can interact with humans and choose the best voting target on
the basis of its probability from the value network. Lastly, we evaluated the
performance of the agent by having it actually play the game with human
players. We found that our AI agent, Deep Wolf, could play Werewolf as
competitively as average human players in a villager or a betrayer role,
whereas Deep Wolf was inferior to human players in a werewolf or a seer role.
These results suggest that current language models have the capability to
suspect what others are saying, tell a lie, or detect lies in conversations.",None,-1
4756b0af-6a8b-4ac3-bb04-15402fbc4504,Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks,0.83648,"Artificial Intelligence-Generated Content (AIGC) refers to the use of AI to
automate the information creation process while fulfilling the personalized
requirements of users. However, due to the instability of AIGC models, e.g.,
the stochastic nature of diffusion models, the quality and accuracy of the
generated content can vary significantly. In wireless edge networks, the
transmission of incorrectly generated content may unnecessarily consume network
resources. Thus, a dynamic AIGC service provider (ASP) selection scheme is
required to enable users to connect to the most suited ASP, improving the
users' satisfaction and quality of generated content. In this article, we first
review the AIGC techniques and their applications in wireless networks. We then
present the AIGC-as-a-service (AaaS) concept and discuss the challenges in
deploying AaaS at the edge networks. Yet, it is essential to have performance
metrics to evaluate the accuracy of AIGC services. Thus, we introduce several
image-based perceived quality evaluation metrics. Then, we propose a general
and effective model to illustrate the relationship between computational
resources and user-perceived quality evaluation metrics. To achieve efficient
AaaS and maximize the quality of generated content in wireless edge networks,
we propose a deep reinforcement learning-enabled algorithm for optimal ASP
selection. Simulation results show that the proposed algorithm can provide a
higher quality of generated content to users and achieve fewer crashed tasks by
comparing with four benchmarks, i.e., overloading-avoidance, random,
round-robin policies, and the upper-bound schemes.",None,-1
8baf4453-dab3-48fc-b8c2-9156c359b93a,Pruning Pre-trained Language Models with Principled Importance and Self-regularization,0.0447912,"Iterative pruning is one of the most effective compression methods for
pre-trained language models. We discovered that finding the optimal pruning
decision is an equality-constrained 0-1 Integer Linear Programming problem. The
solution to this optimization problem leads to a principled importance
criterion which we use to rank parameters during iterative model pruning. To
mitigate the poor generalization at high sparsity levels, we propose a
self-regularization scheme where model prediction is regularized by the latest
checkpoint with increasing sparsity throughout pruning. Our experiments on
natural language understanding, question-answering, named entity recognition,
and data-to-text generation with various Transformer-based PLMs show the
effectiveness of the approach at various sparsity levels.",None,-1
e4c8262d-d808-4f8d-9f4b-36b2dd6d7397,Explicit and Implicit Knowledge Distillation via Unlabeled Data,0.562731,"Data-free knowledge distillation is a challenging model lightweight task for
scenarios in which the original dataset is not available. Previous methods
require a lot of extra computational costs to update one or more generators and
their naive imitate-learning lead to lower distillation efficiency. Based on
these observations, we first propose an efficient unlabeled sample selection
method to replace high computational generators and focus on improving the
training efficiency of the selected samples. Then, a class-dropping mechanism
is designed to suppress the label noise caused by the data domain shifts.
Finally, we propose a distillation method that incorporates explicit features
and implicit structured relations to improve the effect of distillation.
Experimental results show that our method can quickly converge and obtain
higher accuracy than other state-of-the-art methods.",None,-1
29adefb2-9bae-4271-a96d-8bb056be869c,BotShape: A Novel Social Bots Detection Approach via Behavioral Patterns,0.706988,"An essential topic in online social network security is how to accurately
detect bot accounts and relieve their harmful impacts (e.g., misinformation,
rumor, and spam) on genuine users. Based on a real-world data set, we construct
behavioral sequences from raw event logs. After extracting critical
characteristics from behavioral time series, we observe differences between
bots and genuine users and similar patterns among bot accounts. We present a
novel social bot detection system BotShape, to automatically catch behavioral
sequences and characteristics as features for classifiers to detect bots. We
evaluate the detection performance of our system in ground-truth instances,
showing an average accuracy of 98.52% and an average f1-score of 96.65% on
various types of classifiers. After comparing it with other research, we
conclude that BotShape is a novel approach to profiling an account, which could
improve performance for most methods by providing significant behavioral
features.",None,-1
12a0a136-83bb-4d98-9bf2-552c4a80fd45,CD-CTFM: A Lightweight CNN-Transformer Network for Remote Sensing Cloud Detection Fusing Multiscale Features,0.456315,"Clouds in remote sensing images inevitably affect information extraction,
which hinder the following analysis of satellite images. Hence, cloud detection
is a necessary preprocessing procedure. However, the existing methods have
numerous calculations and parameters. In this letter, a lightweight
CNN-Transformer network, CD-CTFM, is proposed to solve the problem. CD-CTFM is
based on encoder-decoder architecture and incorporates the attention mechanism.
In the decoder part, we utilize a lightweight network combing CNN and
Transformer as backbone, which is conducive to extract local and global
features simultaneously. Moreover, a lightweight feature pyramid module is
designed to fuse multiscale features with contextual information. In the
decoder part, we integrate a lightweight channel-spatial attention module into
each skip connection between encoder and decoder, extracting low-level features
while suppressing irrelevant information without introducing many parameters.
Finally, the proposed model is evaluated on two cloud datasets, 38-Cloud and
MODIS. The results demonstrate that CD-CTFM achieves comparable accuracy as the
state-of-art methods. At the same time, CD-CTFM outperforms state-of-art
methods in terms of efficiency.",None,-1
e47e5210-2c26-4d34-ba79-4e0d292e6725,Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance,0.542139,"We propose the use of conversational GPT models for easy and quick few-shot
text classification in the financial domain using the Banking77 dataset. Our
approach involves in-context learning with GPT-3.5 and GPT-4, which minimizes
the technical expertise required and eliminates the need for expensive GPU
computing while yielding quick and accurate results. Additionally, we fine-tune
other pre-trained, masked language models with SetFit, a recent contrastive
learning technique, to achieve state-of-the-art results both in full-data and
few-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can
outperform fine-tuned, non-generative models even with fewer examples. However,
subscription fees associated with these solutions may be considered costly for
small organizations. Lastly, we find that generative models perform better on
the given task when shown representative samples selected by a human expert
rather than when shown random ones. We conclude that a) our proposed methods
offer a practical solution for few-shot tasks in datasets with limited label
availability, and b) our state-of-the-art results can inspire future work in
the area.",None,-1
98d3d72e-4ff5-4d91-b445-abe015ffc706,"SHARP Challenge 2023: Solving CAD History and pArameters Recovery from Point clouds and 3D scans. Overview, Datasets, Metrics, and Baselines",0.394077,"Recent breakthroughs in geometric Deep Learning (DL) and the availability of
large Computer-Aided Design (CAD) datasets have advanced the research on
learning CAD modeling processes and relating them to real objects. In this
context, 3D reverse engineering of CAD models from 3D scans is considered to be
one of the most sought-after goals for the CAD industry. However, recent
efforts assume multiple simplifications limiting the applications in real-world
settings. The SHARP Challenge 2023 aims at pushing the research a step closer
to the real-world scenario of CAD reverse engineering through dedicated
datasets and tracks. In this paper, we define the proposed SHARP 2023 tracks,
describe the provided datasets, and propose a set of baseline methods along
with suitable evaluation metrics to assess the performance of the track
solutions. All proposed datasets along with useful routines and the evaluation
metrics are publicly available.",None,-1
e7080e40-8b24-43b9-8a41-cd11d68b5d99,PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations,0.632673,"PokerKit is an open-source Python library designed to overcome the
restrictions of existing poker game simulation and hand evaluation tools, which
typically support only a handful of poker variants and lack flexibility in game
state control. In contrast, PokerKit significantly expands this scope by
supporting an extensive array of poker variants and it provides a flexible
architecture for users to define their custom games. This paper details the
design and implementation of PokerKit, including its intuitive programmatic
API, multi-variant game support, and a unified hand evaluation suite across
different hand types. The flexibility of PokerKit allows for applications in
diverse areas, such as poker AI development, tool creation, and online poker
casino implementation. PokerKit's reliability has been established through
static type checking, extensive doctests, and unit tests, achieving 99% code
coverage. The introduction of PokerKit represents a significant contribution to
the field of computer poker, fostering future research and advanced AI
development for a wide variety of poker games. The source code is available at
https://github.com/uoftcprg/pokerkit",None,-1
990b5096-87f4-4b37-ae0b-30ea7aa28375,Token Imbalance Adaptation for Radiology Report Generation,0.185047,"Imbalanced token distributions naturally exist in text documents, leading
neural language models to overfit on frequent tokens. The token imbalance may
dampen the robustness of radiology report generators, as complex medical terms
appear less frequently but reflect more medical information. In this study, we
demonstrate how current state-of-the-art models fail to generate infrequent
tokens on two standard benchmark datasets (IU X-RAY and MIMIC-CXR) of radiology
report generation. % However, no prior study has proposed methods to adapt
infrequent tokens for text generators feeding with medical images. To solve the
challenge, we propose the \textbf{T}oken \textbf{Im}balance Adapt\textbf{er}
(\textit{TIMER}), aiming to improve generation robustness on infrequent tokens.
The model automatically leverages token imbalance by an unlikelihood loss and
dynamically optimizes generation processes to augment infrequent tokens. We
compare our approach with multiple state-of-the-art methods on the two
benchmarks. Experiments demonstrate the effectiveness of our approach in
enhancing model robustness overall and infrequent tokens. Our ablation analysis
shows that our reinforcement learning method has a major effect in adapting
token imbalance for radiology report generation.",None,-1
c5c4b7b7-4057-4bd3-89c2-672a3b0a0e6f,Mobile User Interface Element Detection Via Adaptively Prompt Tuning,0.908859,"Recent object detection approaches rely on pretrained vision-language models
for image-text alignment. However, they fail to detect the Mobile User
Interface (MUI) element since it contains additional OCR information, which
describes its content and function but is often ignored. In this paper, we
develop a new MUI element detection dataset named MUI-zh and propose an
Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR
information. APT is a lightweight and effective module to jointly optimize
category prompts across different modalities. For every element, APT uniformly
encodes its visual features and OCR descriptions to dynamically adjust the
representation of frozen category prompts. We evaluate the effectiveness of our
plug-and-play APT upon several existing CLIP-based detectors for both standard
and open-vocabulary MUI element detection. Extensive experiments show that our
method achieves considerable improvements on two datasets. The datasets is
available at \url{github.com/antmachineintelligence/MUI-zh}.",None,-1
b7a1dc81-0265-4b0d-b5cd-05f7cfd2dc74,An Overview on Language Models: Recent Developments and Outlook,0.452605,"Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner, while pre-trained
language models (PLMs) cover broader concepts and can be used in both causal
sequential modeling and fine-tuning for downstream applications. PLMs have
their own training paradigms (usually self-supervised) and serve as foundation
models in modern NLP systems. This overview paper provides an introduction to
both CLMs and PLMs from five aspects, i.e., linguistic units, architectures,
training methods, evaluation methods, and applications. Furthermore, we discuss
the relationship between CLMs and PLMs and shed light on the future directions
of language modeling in the pre-trained era.",None,-1
7f3c92f2-9cd3-4e36-afa5-323a2dcf25e9,Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions,0.040278,"While there is much recent interest in studying why Transformer-based large
language models make predictions the way they do, the complex computations
performed within each layer have made their behavior somewhat opaque. To
mitigate this opacity, this work presents a linear decomposition of final
hidden states from autoregressive language models based on each initial input
token, which is exact for virtually all contemporary Transformer architectures.
This decomposition allows the definition of probability distributions that
ablate the contribution of specific input tokens, which can be used to analyze
their influence on model probabilities over a sequence of upcoming words with
only one forward pass from the model. Using the change in next-word probability
as a measure of importance, this work first examines which context words make
the biggest contribution to language model predictions. Regression experiments
suggest that Transformer-based language models rely primarily on collocational
associations, followed by linguistic factors such as syntactic dependencies and
coreference relationships in making next-word predictions. Additionally,
analyses using these measures to predict syntactic dependencies and coreferent
mention spans show that collocational association and repetitions of the same
token largely explain the language models' predictions on these tasks.",None,-1
9f049fe7-17ab-49c3-b3a4-491be1b04b9f,Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop,0.525956,"This paper presents our project proposal for extracting biomedical
information from German clinical narratives with limited amounts of
annotations. We first describe the applied strategies in transfer learning and
active learning for solving our problem. After that, we discuss the design of
the user interface for both supplying model inspection and obtaining user
annotations in the interactive environment.",None,-1
5fa1473f-91df-4c43-b15a-7bc6010c9bc9,In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making,0.933393,"The current literature on AI-advised decision making -- involving explainable
AI systems advising human decision makers -- presents a series of inconclusive
and confounding results. To synthesize these findings, we propose a simple
theory that elucidates the frequent failure of AI explanations to engender
appropriate reliance and complementary decision making performance. We argue
explanations are only useful to the extent that they allow a human decision
maker to verify the correctness of an AI's prediction, in contrast to other
desiderata, e.g., interpretability or spelling out the AI's reasoning process.
Prior studies find in many decision making contexts AI explanations do not
facilitate such verification. Moreover, most tasks fundamentally do not allow
easy verification, regardless of explanation method, limiting the potential
benefit of any type of explanation. We also compare the objective of
complementary performance with that of appropriate reliance, decomposing the
latter into the notions of outcome-graded and strategy-graded reliance.",None,-1
bcfd8f11-9382-4f70-8eed-6a053896f1b7,Area of interest adaption using feature importance,0.263474,"In this paper, we present two approaches and algorithms that adapt areas of
interest (AOI) or regions of interest (ROI), respectively, to the eye tracking
data quality and classification task. The first approach uses feature
importance in a greedy way and grows or shrinks AOIs in all directions. The
second approach is an extension of the first approach, which divides the AOIs
into areas and calculates a direction of growth, i.e. a gradient. Both
approaches improve the classification results considerably in the case of
generalized AOIs, but can also be used for qualitative analysis. In qualitative
analysis, the algorithms presented allow the AOIs to be adapted to the data,
which means that errors and inaccuracies in eye tracking data can be better
compensated for. A good application example is abstract art, where manual AOIs
annotation is hardly possible, and data-driven approaches are mainly used for
initial AOIs.
  Link:
https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FAOIGradient&mode=list",None,-1
25dcc8ee-78b1-4c67-b430-fc220e929e64,Predictable MDP Abstraction for Unsupervised Model-Based RL,0.390385,"A key component of model-based reinforcement learning (RL) is a dynamics
model that predicts the outcomes of actions. Errors in this predictive model
can degrade the performance of model-based controllers, and complex Markov
decision processes (MDPs) can present exceptionally difficult prediction
problems. To mitigate this issue, we propose predictable MDP abstraction (PMA):
instead of training a predictive model on the original MDP, we train a model on
a transformed MDP with a learned action space that only permits predictable,
easy-to-model actions, while covering the original state-action space as much
as possible. As a result, model learning becomes easier and more accurate,
which allows robust, stable model-based planning or model-based RL. This
transformation is learned in an unsupervised manner, before any task is
specified by the user. Downstream tasks can then be solved with model-based
control in a zero-shot fashion, without additional environment interactions. We
theoretically analyze PMA and empirically demonstrate that PMA leads to
significant improvements over prior unsupervised model-based RL approaches in a
range of benchmark environments. Our code and videos are available at
https://seohong.me/projects/pma/",None,-1
81f8e602-d81e-4e53-b496-12acfd51a59d,PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View,0.496942,"3D plane recovery from a single image can usually be divided into several
subtasks of plane detection, segmentation, parameter estimation and possibly
depth estimation. Previous works tend to solve this task by either extending
the RCNN-based segmentation network or the dense pixel embedding-based
clustering framework. However, none of them tried to integrate above related
subtasks into a unified framework but treat them separately and sequentially,
which we suspect is potentially a main source of performance limitation for
existing approaches. Motivated by this finding and the success of query-based
learning in enriching reasoning among semantic entities, in this paper, we
propose PlaneRecTR, a Transformer-based architecture, which for the first time
unifies all subtasks related to single-view plane recovery with a single
compact model. Extensive quantitative and qualitative experiments demonstrate
that our proposed unified learning achieves mutual benefits across subtasks,
obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane
datasets. Codes are available at https://github.com/SJingjia/PlaneRecTR.",None,-1
abe6d20c-e676-4c8b-aec1-15802cc3c608,You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis,0.317934,"Expressing attitude or stance toward entities and concepts is an integral
part of human behavior and personality. Recently, evaluative language data has
become more accessible with social media's rapid growth, enabling large-scale
opinion analysis. However, surprisingly little research examines the
relationship between personality and evaluative language. To bridge this gap,
we introduce the notion of evaluative topics, obtained by applying topic models
to pre-filtered evaluative text from social media. We then link evaluative
topics to individual text authors to build their evaluative profiles. We apply
evaluative profiling to Reddit comments labeled with personality scores and
conduct an exploratory study on the relationship between evaluative topics and
Big Five personality facets, aiming for a more interpretable, facet-level
analysis. Finally, we validate our approach by observing correlations
consistent with prior research in personality psychology.",None,-1
68d84852-133a-4079-a179-08a96df9e5b9,Revisiting Deformable Convolution for Depth Completion,0.108424,"Depth completion, which aims to generate high-quality dense depth maps from
sparse depth maps, has attracted increasing attention in recent years. Previous
work usually employs RGB images as guidance, and introduces iterative spatial
propagation to refine estimated coarse depth maps. However, most of the
propagation refinement methods require several iterations and suffer from a
fixed receptive field, which may contain irrelevant and useless information
with very sparse input. In this paper, we address these two challenges
simultaneously by revisiting the idea of deformable convolution. We propose an
effective architecture that leverages deformable kernel convolution as a
single-pass refinement module, and empirically demonstrate its superiority. To
better understand the function of deformable convolution and exploit it for
depth completion, we further systematically investigate a variety of
representative strategies. Our study reveals that, different from prior work,
deformable convolution needs to be applied on an estimated depth map with a
relatively high density for better performance. We evaluate our model on the
large-scale KITTI dataset and achieve state-of-the-art level performance in
both accuracy and inference speed. Our code is available at
https://github.com/AlexSunNik/ReDC.",None,-1
fc82aeff-bcd3-460d-9ba9-dee65dea0086,Hidden Citations Obscure True Impact in Science,0.439,"References, the mechanism scientists rely on to signal previous knowledge,
lately have turned into widely used and misused measures of scientific impact.
Yet, when a discovery becomes common knowledge, citations suffer from
obliteration by incorporation. This leads to the concept of hidden citation,
representing a clear textual credit to a discovery without a reference to the
publication embodying it. Here, we rely on unsupervised interpretable machine
learning applied to the full text of each paper to systematically identify
hidden citations. We find that for influential discoveries hidden citations
outnumber citation counts, emerging regardless of publishing venue and
discipline. We show that the prevalence of hidden citations is not driven by
citation counts, but rather by the degree of the discourse on the topic within
the text of the manuscripts, indicating that the more discussed is a discovery,
the less visible it is to standard bibliometric analysis. Hidden citations
indicate that bibliometric measures offer a limited perspective on quantifying
the true impact of a discovery, raising the need to extract knowledge from the
full text of the scientific corpus.",None,-1
272e5e2e-f014-41a1-b751-1d063c911b8e,Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond,0.855567,"Recently, multi-modality scene perception tasks, e.g., image fusion and scene
understanding, have attracted widespread attention for intelligent vision
systems. However, early efforts always consider boosting a single task
unilaterally and neglecting others, seldom investigating their underlying
connections for joint promotion. To overcome these limitations, we establish
the hierarchical dual tasks-driven deep model to bridge these tasks.
Concretely, we firstly construct an image fusion module to fuse complementary
characteristics and cascade dual task-related modules, including a
discriminator for visual effects and a semantic network for feature
measurement. We provide a bi-level perspective to formulate image fusion and
follow-up downstream tasks. To incorporate distinct task-related responses for
image fusion, we consider image fusion as a primary goal and dual modules as
learnable constraints. Furthermore, we develop an efficient first-order
approximation to compute corresponding gradients and present dynamic weighted
aggregation to balance the gradients for fusion learning. Extensive experiments
demonstrate the superiority of our method, which not only produces visually
pleasant fused results but also realizes significant promotion for detection
and segmentation than the state-of-the-art approaches.",None,-1
33271215-15e7-4581-b855-0ae4455fbefb,Tracking Everything Everywhere All at Once,0.999968,"We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/",None,-1
dd17ad75-41cd-4858-a2d4-10a6e7c89073,Learning Good Features to Transfer Across Tasks and Domains,0.14356,"Availability of labelled data is the major obstacle to the deployment of deep
learning algorithms for computer vision tasks in new domains. The fact that
many frameworks adopted to solve different tasks share the same architecture
suggests that there should be a way of reusing the knowledge learned in a
specific setting to solve novel tasks with limited or no additional
supervision. In this work, we first show that such knowledge can be shared
across tasks by learning a mapping between task-specific deep features in a
given domain. Then, we show that this mapping function, implemented by a neural
network, is able to generalize to novel unseen domains. Besides, we propose a
set of strategies to constrain the learned feature spaces, to ease learning and
increase the generalization capability of the mapping network, thereby
considerably improving the final performance of our framework. Our proposal
obtains compelling results in challenging synthetic-to-real adaptation
scenarios by transferring knowledge between monocular depth estimation and
semantic segmentation tasks.",None,-1
fb241db8-3cc2-43c1-bb7e-d5767779ac05,GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer,0.731505,"Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.",None,-1
190adac1-f81e-409c-9b91-1e0b840640b5,Learning Rational Subgoals from Demonstrations and Instructions,0.222193,"We present a framework for learning useful subgoals that support efficient
long-term planning to achieve novel goals. At the core of our framework is a
collection of rational subgoals (RSGs), which are essentially binary
classifiers over the environmental states. RSGs can be learned from
weakly-annotated data, in the form of unsegmented demonstration trajectories,
paired with abstract task descriptions, which are composed of terms initially
unknown to the agent (e.g., collect-wood then craft-boat then go-across-river).
Our framework also discovers dependencies between RSGs, e.g., the task
collect-wood is a helpful subgoal for the task craft-boat. Given a goal
description, the learned subgoals and the derived dependencies facilitate
off-the-shelf planning algorithms, such as A* and RRT, by setting helpful
subgoals as waypoints to the planner, which significantly improves
performance-time efficiency.",None,-1
e2d19006-1050-447f-837a-1c2be2d62c4f,Triplet Knowledge Distillation,0.291629,"In Knowledge Distillation, the teacher is generally much larger than the
student, making the solution of the teacher likely to be difficult for the
student to learn. To ease the mimicking difficulty, we introduce a triplet
knowledge distillation mechanism named TriKD. Besides teacher and student,
TriKD employs a third role called anchor model. Before distillation begins, the
pre-trained anchor model delimits a subspace within the full solution space of
the target problem. Solutions within the subspace are expected to be easy
targets that the student could mimic well. Distillation then begins in an
online manner, and the teacher is only allowed to express solutions within the
aforementioned subspace. Surprisingly, benefiting from accurate but
easy-to-mimic hints, the student can finally perform well. After the student is
well trained, it can be used as the new anchor for new students, forming a
curriculum learning strategy. Our experiments on image classification and face
recognition with various models clearly demonstrate the effectiveness of our
method. Furthermore, the proposed TriKD is also effective in dealing with the
overfitting issue. Moreover, our theoretical analysis supports the rationality
of our triplet distillation.",None,-1
93a36a13-8c16-42b9-b30c-16c3c42c24b0,FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation,0.833426,"Recent attention in instance segmentation has focused on query-based models.
Despite being non-maximum suppression (NMS)-free and end-to-end, the
superiority of these models on high-accuracy real-time benchmarks has not been
well demonstrated. In this paper, we show the strong potential of query-based
models on efficient instance segmentation algorithm designs. We present
FastInst, a simple, effective query-based framework for real-time instance
segmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) while
yielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bells
and whistles. Specifically, FastInst follows the meta-architecture of recently
introduced Mask2Former. Its key designs include instance activation-guided
queries, dual-path update strategy, and ground truth mask-guided learning,
which enable us to use lighter pixel decoders, fewer Transformer decoder
layers, while achieving better performance. The experiments show that FastInst
outperforms most state-of-the-art real-time counterparts, including strong
fully convolutional baselines, in both speed and accuracy. Code can be found at
https://github.com/junjiehe96/FastInst .",None,-1
715ca8ac-2502-466e-8ba8-fd70b54168c2,Wavelet-based Unsupervised Label-to-Image Translation,0.312598,"Semantic Image Synthesis (SIS) is a subclass of image-to-image translation
where a semantic layout is used to generate a photorealistic image.
State-of-the-art conditional Generative Adversarial Networks (GANs) need a huge
amount of paired data to accomplish this task while generic unpaired
image-to-image translation frameworks underperform in comparison, because they
color-code semantic layouts and learn correspondences in appearance instead of
semantic content. Starting from the assumption that a high quality generated
image should be segmented back to its semantic layout, we propose a new
Unsupervised paradigm for SIS (USIS) that makes use of a self-supervised
segmentation loss and whole image wavelet based discrimination. Furthermore, in
order to match the high-frequency distribution of real images, a novel
generator architecture in the wavelet domain is proposed. We test our
methodology on 3 challenging datasets and demonstrate its ability to bridge the
performance gap between paired and unpaired models.",None,-1
029d9729-d388-45e7-87bf-eda2a1b82252,EKILA: Synthetic Media Provenance and Attribution for Generative Art,0.196448,"We present EKILA; a decentralized framework that enables creatives to receive
recognition and reward for their contributions to generative AI (GenAI). EKILA
proposes a robust visual attribution technique and combines this with an
emerging content provenance standard (C2PA) to address the problem of synthetic
image provenance -- determining the generative model and training data
responsible for an AI-generated image. Furthermore, EKILA extends the
non-fungible token (NFT) ecosystem to introduce a tokenized representation for
rights, enabling a triangular relationship between the asset's Ownership,
Rights, and Attribution (ORA). Leveraging the ORA relationship enables creators
to express agency over training consent and, through our attribution model, to
receive apportioned credit, including royalty payments for the use of their
assets in GenAI.",None,-1
24e22350-4467-4aac-b099-81ca4a22d8ea,NFTVis: Visual Analysis of NFT Performance,0.0590804,"A non-fungible token (NFT) is a data unit stored on the blockchain. Nowadays,
more and more investors and collectors (NFT traders), who participate in
transactions of NFTs, have an urgent need to assess the performance of NFTs.
However, there are two challenges for NFT traders when analyzing the
performance of NFT. First, the current rarity models have flaws and are
sometimes not convincing. In addition, NFT performance is dependent on multiple
factors, such as images (high-dimensional data), history transactions
(network), and market evolution (time series). It is difficult to take
comprehensive consideration and analyze NFT performance efficiently. To address
these challenges, we propose NFTVis, a visual analysis system that facilitates
assessing individual NFT performance. A new NFT rarity model is proposed to
quantify NFTs with images. Four well-coordinated views are designed to
represent the various factors affecting the performance of the NFT. Finally, we
evaluate the usefulness and effectiveness of our system using two case studies
and user studies.",None,-1
91d10da5-9f5f-4ea0-9cb0-33fb99be5a63,$\varepsilon$ K <MASK>: Integrating Yorb cultural greetings into machine translation,0.338169,"This paper investigates the performance of massively multilingual neural
machine translation (NMT) systems in translating Yor\`ub\'a greetings
($\varepsilon$ k\'u [MASK]), which are a big part of Yor\`ub\'a language and
culture, into English. To evaluate these models, we present IkiniYor\`ub\'a, a
Yor\`ub\'a-English translation dataset containing some Yor\`ub\'a greetings,
and sample use cases. We analysed the performance of different multilingual NMT
systems including Google and NLLB and show that these models struggle to
accurately translate Yor\`ub\'a greetings into English. In addition, we trained
a Yor\`ub\'a-English model by finetuning an existing NMT model on the training
split of IkiniYor\`ub\'a and this achieved better performance when compared to
the pre-trained multilingual NMT models, although they were trained on a large
volume of data.",None,-1
328aeed9-1ae8-4734-870e-47dc5ff38580,Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,0.667992,"Synthesizing interaction-involved human motions has been challenging due to
the high complexity of 3D environments and the diversity of possible human
behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to
synthesize natural and plausible long-term human movements in complex indoor
environments. The key motivation of LAMA is to build a unified framework to
encompass a series of everyday motions including locomotion, scene interaction,
and object manipulation. Unlike existing methods that require motion data
""paired"" with scanned 3D scenes for supervision, we formulate the problem as a
test-time optimization by using human motion capture data only for synthesis.
LAMA leverages a reinforcement learning framework coupled with a motion
matching algorithm for optimization, and further exploits a motion editing
framework via manifold learning to cover possible variations in interaction and
manipulation. Throughout extensive experiments, we demonstrate that LAMA
outperforms previous approaches in synthesizing realistic motions in various
challenging scenarios. Project page: https://jiyewise.github.io/projects/LAMA/ .",None,-1
75bdd410-73a0-44a3-9713-b8d20aea4772,Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion,0.993121,"We introduce a method for generating realistic pedestrian trajectories and
full-body animations that can be controlled to meet user-defined goals. We draw
on recent advances in guided diffusion modeling to achieve test-time
controllability of trajectories, which is normally only associated with
rule-based systems. Our guided diffusion model allows users to constrain
trajectories through target waypoints, speed, and specified social groups while
accounting for the surrounding environment context. This trajectory diffusion
model is integrated with a novel physics-based humanoid controller to form a
closed-loop, full-body pedestrian animation system capable of placing large
crowds in a simulated environment with varying terrains. We further propose
utilizing the value function learned during RL training of the animation
controller to guide diffusion to produce trajectories better suited for
particular scenarios such as collision avoidance and traversing uneven terrain.
Video results are available on the project page at
https://nv-tlabs.github.io/trace-pace .",None,-1
966fc684-2b74-479c-b92e-aba47c2577c5,TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation,0.251848,"Test-time adaptive (TTA) semantic segmentation adapts a source pre-trained
image semantic segmentation model to unlabeled batches of target domain test
images, different from real-world, where samples arrive one-by-one in an online
fashion. To tackle online settings, we propose TransAdapt, a framework that
uses transformer and input transformations to improve segmentation performance.
Specifically, we pre-train a transformer-based module on a segmentation network
that transforms unsupervised segmentation output to a more reliable supervised
output, without requiring test-time online training. To also facilitate
test-time adaptation, we propose an unsupervised loss based on the transformed
input that enforces the model to be invariant and equivariant to photometric
and geometric perturbations, respectively. Overall, our framework produces
higher quality segmentation masks with up to 17.6% and 2.8% mIOU improvement
over no-adaptation and competitive baselines, respectively.",None,-1
7cda2ccf-8ca4-4353-8c47-68cc2c85ab82,Action Capsules: Human Skeleton Action Recognition,0.292164,"Due to the compact and rich high-level representations offered,
skeleton-based human action recognition has recently become a highly active
research topic. Previous studies have demonstrated that investigating joint
relationships in spatial and temporal dimensions provides effective information
critical to action recognition. However, effectively encoding global
dependencies of joints during spatio-temporal feature extraction is still
challenging. In this paper, we introduce Action Capsule which identifies
action-related key joints by considering the latent correlation of joints in a
skeleton sequence. We show that, during inference, our end-to-end network pays
attention to a set of joints specific to each action, whose encoded
spatio-temporal features are aggregated to recognize the action. Additionally,
the use of multiple stages of action capsules enhances the ability of the
network to classify similar actions. Consequently, our network outperforms the
state-of-the-art approaches on the N-UCLA dataset and obtains competitive
results on the NTURGBD dataset. This is while our approach has significantly
lower computational requirements based on GFLOPs measurements.",None,-1
5295b821-05b4-4c44-b172-2af26fd32999,AU-aware graph convolutional network for Macro- and Micro-expression spotting,0.766672,"Automatic Micro-Expression (ME) spotting in long videos is a crucial step in
ME analysis but also a challenging task due to the short duration and low
intensity of MEs. When solving this problem, previous works generally lack in
considering the structures of human faces and the correspondence between
expressions and relevant facial muscles. To address this issue for better
performance of ME spotting, this paper seeks to extract finer spatial features
by modeling the relationships between facial Regions of Interest (ROIs).
Specifically, we propose a graph convolutional-based network, called
Action-Unit-aWare Graph Convolutional Network (AUW-GCN). Furthermore, to inject
prior information and to cope with the problem of small datasets, AU-related
statistics are encoded into the network. Comprehensive experiments show that
our results outperform baseline methods consistently and achieve new SOTA
performance in two benchmark datasets,CAS(ME)^2 and SAMM-LV. Our code is
available at https://github.com/xjtupanda/AUW-GCN.",None,-1
8bbf6060-f4cf-4f55-bf55-c454e6f6da21,Can Prompt Learning Benefit Radiology Report Generation?,0.11551,"Radiology report generation aims to automatically provide clinically
meaningful descriptions of radiology images such as MRI and X-ray. Although
great success has been achieved in natural scene image captioning tasks,
radiology report generation remains challenging and requires prior medical
knowledge. In this paper, we propose PromptRRG, a method that utilizes prompt
learning to activate a pretrained model and incorporate prior knowledge. Since
prompt learning for radiology report generation has not been explored before,
we begin with investigating prompt designs and categorise them based on varying
levels of knowledge: common, domain-specific and disease-enriched prompts.
Additionally, we propose an automatic prompt learning mechanism to alleviate
the burden of manual prompt engineering. This is the first work to
systematically examine the effectiveness of prompt learning for radiology
report generation. Experimental results on the largest radiology report
generation benchmark, MIMIC-CXR, demonstrate that our proposed method achieves
state-of-the-art performance. Code will be available upon the acceptance.",None,-1
bf292d98-55c9-4dd9-9dd8-5698b63daed3,CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction,0.656728,"Recent advances in neural reconstruction using posed image sequences have
made remarkable progress. However, due to the lack of depth information,
existing volumetric-based techniques simply duplicate 2D image features of the
object surface along the entire camera ray. We contend this duplication
introduces noise in empty and occluded spaces, posing challenges for producing
high-quality 3D geometry. Drawing inspiration from traditional multi-view
stereo methods, we propose an end-to-end 3D neural reconstruction framework
CVRecon, designed to exploit the rich geometric embedding in the cost volumes
to facilitate 3D geometric feature learning. Furthermore, we present
Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature
representation that encodes view-dependent information with improved integrity
and robustness. Through comprehensive experiments, we demonstrate that our
approach significantly improves the reconstruction quality in various metrics
and recovers clear fine details of the 3D geometries. Our extensive ablation
studies provide insights into the development of effective 3D geometric feature
learning schemes. Project page: https://cvrecon.ziyue.cool/",None,-1
597c84ec-21b9-4a65-8f8c-a6db029d9ad6,Inferring Capabilities from Task Performance with Bayesian Triangulation,0.905794,"As machine learning models become more general, we need to characterise them
in richer, more meaningful ways. We describe a method to infer the cognitive
profile of a system from diverse experimental data. To do so, we introduce
measurement layouts that model how task-instance features interact with system
capabilities to affect performance. These features must be triangulated in
complex ways to be able to infer capabilities from non-populational data -- a
challenge for traditional psychometric and inferential tools. Using the
Bayesian probabilistic programming library PyMC, we infer different cognitive
profiles for agents in two scenarios: 68 actual contestants in the AnimalAI
Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery.
We showcase the potential for capability-oriented evaluation.",None,-1
3c4da41b-9b78-4b3d-bd9e-f09ee09669c3,Interactive Video Corpus Moment Retrieval using Reinforcement Learning,0.0912843,"Known-item video search is effective with human-in-the-loop to interactively
investigate the search result and refine the initial query. Nevertheless, when
the first few pages of results are swamped with visually similar items, or the
search target is hidden deep in the ranked list, finding the know-item target
usually requires a long duration of browsing and result inspection. This paper
tackles the problem by reinforcement learning, aiming to reach a search target
within a few rounds of interaction by long-term learning from user feedbacks.
Specifically, the system interactively plans for navigation path based on
feedback and recommends a potential target that maximizes the long-term reward
for user comment. We conduct experiments for the challenging task of video
corpus moment retrieval (VCMR) to localize moments from a large video corpus.
The experimental results on TVR and DiDeMo datasets verify that our proposed
work is effective in retrieving the moments that are hidden deep inside the
ranked lists of CONQUER and HERO, which are the state-of-the-art auto-search
engines for VCMR.",None,-1
98fa507b-83de-4c93-a6b1-3a2a2b11a27b,Evaluation of African American Language Bias in Natural Language Generation,0.323077,"We evaluate how well LLMs understand African American Language (AAL) in
comparison to their performance on White Mainstream English (WME), the
encouraged ""standard"" form of English taught in American classrooms. We measure
LLM performance using automatic metrics and human judgments for two tasks: a
counterpart generation task, where a model generates AAL (or WME) given WME (or
AAL), and a masked span prediction (MSP) task, where models predict a phrase
that was removed from their input. Our contributions include: (1) evaluation of
six pre-trained, large language models on the two language generation tasks;
(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop
lyrics, focus groups, and linguistic interviews) with human-annotated
counterparts in WME; and (3) documentation of model performance gaps that
suggest bias and identification of trends in lack of understanding of AAL
features.",None,-1
5fdc45db-0cb3-4b59-9859-5925800e31a5,Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations,0.0685499,"This document presents adequate formal terminology for the mathematical
specification of a subset of Agent Based Models (ABMs) in the field of
Demography. The simulation of the targeted ABMs follows a fixedstep
single-clocked pattern. The proposed terminology further improves the model
understanding and can act as a stand-alone protocol for the specification and
optionally the documentation of a significant set of (demographic) ABMs.
Nevertheless, it is imaginable the this terminology can serve as an inspiring
basis for further improvement to the largely-informal widely-used model
documentation and communication O.D.D. protocol [Grimm and et al., 2020,
Amouroux et al., 2010] to reduce many sources of ambiguity which hinder model
replications by other modelers. A published demographic model documentation,
largely simplified version of the Lone Parent Model [Gostoli and Silverman,
2020] is separately published in [Elsheikh, 2023c] as illustration for the
formal terminology presented here. The model was implemented in the Julia
language [Elsheikh, 2023b] based on the Agents.jl julia package [Datseris et
al., 2022].",None,-1
e62eb71b-e987-4bb0-a0e1-6e6c4881c79c,Prompt2Model: Generating Deployable Models from Natural Language Instructions,0.379631,"Large language models (LLMs) enable system builders today to create competent
NLP systems through prompting, where they only need to describe the task in
natural language and provide a few examples. However, in other ways, LLMs are a
step backward from traditional special-purpose NLP models; they require
extensive computational resources for deployment and can be gated behind APIs.
In this paper, we propose Prompt2Model, a general-purpose method that takes a
natural language task description like the prompts provided to LLMs, and uses
it to train a special-purpose model that is conducive to deployment. This is
done through a multi-step process of retrieval of existing datasets and
pretrained models, dataset generation using LLMs, and supervised fine-tuning on
these retrieved and generated datasets. Over three tasks, we demonstrate that
given the same few-shot prompt as input, Prompt2Model trains models that
outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%
while being up to 700 times smaller. We also show that this data can be used to
obtain reliable performance estimates of model performance, enabling model
developers to assess model reliability before deployment. Prompt2Model is
available open-source at https://github.com/neulab/prompt2model.",None,-1
778789cc-3f6b-4e6a-aad6-2698dae7590b,Toward Open-ended Embodied Tasks Solving,0.640492,"Empowering embodied agents, such as robots, with Artificial Intelligence (AI)
has become increasingly important in recent years. A major challenge is task
open-endedness. In practice, robots often need to perform tasks with novel
goals that are multifaceted, dynamic, lack a definitive ""end-state"", and were
not encountered during training. To tackle this problem, this paper introduces
\textit{Diffusion for Open-ended Goals} (DOG), a novel framework designed to
enable embodied AI to plan and act flexibly and dynamically for open-ended task
goals. DOG synergizes the generative prowess of diffusion models with
state-of-the-art, training-free guidance techniques to adaptively perform
online planning and control. Our evaluations demonstrate that DOG can handle
various kinds of novel task goals not seen during training, in both maze
navigation and robot control problems. Our work sheds light on enhancing
embodied AI's adaptability and competency in tackling open-ended goals.",None,-1
2150016b-ce0a-43d6-bea7-7b81a4562bcc,Theory of Mind for Multi-Agent Collaboration via Large Language Models,0.855026,"While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.",None,-1
d89b7240-85a7-4e28-ba92-b79187cd8905,Estimation of the qualification and behavior of a contributor and aggregation of his answers in a crowdsourcing context,0.747399,"Crowdsourcing is the outsourcing of tasks to a crowd of contributors on a
dedicated platform. The crowd on these platforms is very diversified and
includes various profiles of contributors which generates data of uneven
quality. However, majority voting, which is the aggregating method commonly
used in platforms, gives equal weight to each contribution. To overcome this
problem, we propose a method, MONITOR, which estimates the contributor's
profile and aggregates the collected data by taking into account their possible
imperfections thanks to the theory of belief functions. To do so, MONITOR
starts by estimating the profile of the contributor through his qualification
for the task and his behavior.Crowdsourcing campaigns have been carried out to
collect the necessary data to test MONITOR on real data in order to compare it
to existing approaches. The results of the experiments show that thanks to the
use of the MONITOR method, we obtain a better rate of correct answer after
aggregation of the contributions compared to the majority voting. Our
contributions in this article are for the first time the proposal of a model
that takes into account both the qualification of the contributor and his
behavior in the estimation of his profile. For the second one, the weakening
and the aggregation of the answers according to the estimated profiles.",None,-1
2967aa29-43d2-4c45-8cde-bbe068af0da9,Automatic Truss Design with Reinforcement Learning,0.318084,"Truss layout design, namely finding a lightweight truss layout satisfying all
the physical constraints, is a fundamental problem in the building industry.
Generating the optimal layout is a challenging combinatorial optimization
problem, which can be extremely expensive to solve by exhaustive search.
Directly applying end-to-end reinforcement learning (RL) methods to truss
layout design is infeasible either, since only a tiny portion of the entire
layout space is valid under the physical constraints, leading to particularly
sparse rewards for RL training. In this paper, we develop AutoTruss, a
two-stage framework to efficiently generate both lightweight and valid truss
layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse
collection of valid layouts. Then RL is applied to iteratively refine the valid
solutions. We conduct experiments and ablation studies in popular truss layout
design test cases in both 2D and 3D settings. AutoTruss outperforms the
best-reported layouts by 25.1% in the most challenging 3D test cases, resulting
in the first effective deep-RL-based approach in the truss layout design
literature.",None,-1
25aa282d-2848-4100-9a36-bc21ec3700ea,Scalable Knowledge Graph Construction and Inference on Human Genome Variants,0.439751,"Real-world knowledge can be represented as a graph consisting of entities and
relationships between the entities. The need for efficient and scalable
solutions arises when dealing with vast genomic data, like RNA-sequencing.
Knowledge graphs offer a powerful approach for various tasks in such
large-scale genomic data, such as analysis and inference. In this work,
variant-level information extracted from the RNA-sequences of vaccine-na\""ive
COVID-19 patients have been represented as a unified, large knowledge graph.
Variant call format (VCF) files containing the variant-level information were
annotated to include further information for each variant. The data records in
the annotated files were then converted to Resource Description Framework (RDF)
triples. Each VCF file obtained had an associated CADD scores file that
contained the raw and Phred-scaled scores for each variant. An ontology was
defined for the VCF and CADD scores files. Using this ontology and the
extracted information, a large, scalable knowledge graph was created. Available
graph storage was then leveraged to query and create datasets for further
downstream tasks. We also present a case study using the knowledge graph and
perform a classification task using graph machine learning. We also draw
comparisons between different Graph Neural Networks (GNNs) for the case study.",None,-1
89b48828-89f7-4e1f-b110-bbea24d51e68,KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases,0.959316,"Large language models (LLMs) have demonstrated impressive impact in the field
of natural language processing, but they still struggle with several issues
regarding, such as completeness, timeliness, faithfulness and adaptability.
While recent efforts have focuses on connecting LLMs with external knowledge
sources, the integration of knowledge bases (KBs) remains understudied and
faces several challenges. In this paper, we introduce KnowledGPT, a
comprehensive framework to bridge LLMs with various knowledge bases,
facilitating both the retrieval and storage of knowledge. The retrieval process
employs the program of thought prompting, which generates search language for
KBs in code format with pre-defined functions for KB operations. Besides
retrieval, KnowledGPT offers the capability to store knowledge in a
personalized KB, catering to individual user demands. With extensive
experiments, we show that by integrating LLMs with KBs, KnowledGPT properly
answers a broader range of questions requiring world knowledge compared with
vanilla LLMs, utilizing both knowledge existing in widely-known KBs and
extracted into personalized KBs.",None,-1
06981561-be9e-44ff-a135-7d5454d77187,Beyond Prompts: Exploring the Design Space of Mixed-Initiative Co-Creativity Systems,0.917915,"Generative Artificial Intelligence systems have been developed for image,
code, story, and game generation with the goal of facilitating human
creativity. Recent work on neural generative systems has emphasized one
particular means of interacting with AI systems: the user provides a
specification, usually in the form of prompts, and the AI system generates the
content. However, there are other configurations of human and AI coordination,
such as co-creativity (CC) in which both human and AI systems can contribute to
content creation, and mixed-initiative (MI) in which both human and AI systems
can initiate content changes. In this paper, we define a hypothetical human-AI
configuration design space consisting of different means for humans and AI
systems to communicate creative intent to each other. We conduct a human
participant study with 185 participants to understand how users want to
interact with differently configured MI-CC systems. We find out that MI-CC
systems with more extensive coverage of the design space are rated higher or on
par on a variety of creative and goal-completion metrics, demonstrating that
wider coverage of the design space can improve user experience and achievement
when using the system; Preference varies greatly between expertise groups,
suggesting the development of adaptive, personalized MI-CC systems;
Participants identified new design space dimensions including scrutability --
the ability to poke and prod at models -- and explainability.",None,-1
1063ebe1-e451-400e-928b-a283369cb247,Cross-head Supervision for Crowd Counting with Noisy Annotations,0.564541,"Noisy annotations such as missing annotations and location shifts often exist
in crowd counting datasets due to multi-scale head sizes, high occlusion, etc.
These noisy annotations severely affect the model training, especially for
density map-based methods. To alleviate the negative impact of noisy
annotations, we propose a novel crowd counting model with one convolution head
and one transformer head, in which these two heads can supervise each other in
noisy areas, called Cross-Head Supervision. The resultant model, CHS-Net, can
synergize different types of inductive biases for better counting. In addition,
we develop a progressive cross-head supervision learning strategy to stabilize
the training process and provide more reliable supervision. Extensive
experimental results on ShanghaiTech and QNRF datasets demonstrate superior
performance over state-of-the-art methods. Code is available at
https://github.com/RaccoonDML/CHSNet.",None,-1
0ad1e116-2dab-4cdf-b0e7-9ee46544ff30,Deep Neural Networks for Encrypted Inference with TFHE,0.465609,"Fully homomorphic encryption (FHE) is an encryption method that allows to
perform computation on encrypted data, without decryption. FHE preserves the
privacy of the users of online services that handle sensitive data, such as
health data, biometrics, credit scores and other personal information. A common
way to provide a valuable service on such data is through machine learning and,
at this time, Neural Networks are the dominant machine learning model for
unstructured data. In this work we show how to construct Deep Neural Networks
(DNN) that are compatible with the constraints of TFHE, an FHE scheme that
allows arbitrary depth computation circuits. We discuss the constraints and
show the architecture of DNNs for two computer vision tasks. We benchmark the
architectures using the Concrete stack, an open-source implementation of TFHE.",None,-1
b9c33112-e4a0-4416-9b24-b1692be54815,Continuous Layout Editing of Single Images with Diffusion Models,0.250926,"Recent advancements in large-scale text-to-image diffusion models have
enabled many applications in image editing. However, none of these methods have
been able to edit the layout of single existing images. To address this gap, we
propose the first framework for layout editing of a single image while
preserving its visual properties, thus allowing for continuous editing on a
single image. Our approach is achieved through two key modules. First, to
preserve the characteristics of multiple objects within an image, we
disentangle the concepts of different objects and embed them into separate
textual tokens using a novel method called masked textual inversion. Next, we
propose a training-free optimization method to perform layout control for a
pre-trained diffusion model, which allows us to regenerate images with learned
concepts and align them with user-specified layouts. As the first framework to
edit the layout of existing images, we demonstrate that our method is effective
and outperforms other baselines that were modified to support this task. Our
code will be freely available for public use upon acceptance.",None,-1
824fda89-2ce5-45ee-8589-afd445353c92,Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT,0.722989,"The abundance of information on social media has increased the necessity of
accurate real-time rumour detection. Manual techniques of identifying and
verifying fake news generated by AI tools are impracticable and time-consuming
given the enormous volume of information generated every day. This has sparked
an increase in interest in creating automated systems to find fake news on the
Internet. The studies in this research demonstrate that the BERT and RobertA
models with fine-tuning had the best success in detecting AI generated news.
With a score of 98%, tweaked RobertA in particular showed excellent precision.
In conclusion, this study has shown that neural networks can be used to
identify bogus news AI generation news created by ChatGPT. The RobertA and BERT
models' excellent performance indicates that these models can play a critical
role in the fight against misinformation.",None,-1
0c3803a5-a66d-455b-ae51-3125c278666a,AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes,0.698791,"We propose attribute-aware multimodal entity linking, where the input is a
mention described with a text and image, and the goal is to predict the
corresponding target entity from a multimodal knowledge base (KB) where each
entity is also described with a text description, a visual image and a set of
attributes and values. To support this research, we construct AMELI, a
large-scale dataset consisting of 18,472 reviews and 35,598 products. To
establish baseline performance on AMELI, we experiment with the current
state-of-the-art multimodal entity linking approaches and our enhanced
attribute-aware model and demonstrate the importance of incorporating the
attribute information into the entity linking process. To be best of our
knowledge, we are the first to build benchmark dataset and solutions for the
attribute-aware multimodal entity linking task. Datasets and codes will be made
publicly available.",None,-1
fe38b8f1-e24b-4731-9185-c4fc72736657,CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare,0.652744,"In the era of modern healthcare, swiftly generating medical question
summaries is crucial for informed and timely patient care. Despite the
increasing complexity and volume of medical data, existing studies have focused
solely on text-based summarization, neglecting the integration of visual
information. Recognizing the untapped potential of combining textual queries
with visual representations of medical conditions, we introduce the Multimodal
Medical Question Summarization (MMQS) Dataset. This dataset, a major
contribution to our work, pairs medical queries with visual aids, facilitating
a richer and more nuanced understanding of patient needs. We also propose a
framework, utilizing the power of Contrastive Language Image Pretraining(CLIP)
and Large Language Models(LLMs), consisting of four modules that identify
medical disorders, generate relevant context, filter medical concepts, and
craft visually aware summaries. Our comprehensive framework harnesses the power
of CLIP, a multimodal foundation model, and various general-purpose LLMs,
comprising four main modules: the medical disorder identification module, the
relevant context generation module, the context filtration module for
distilling relevant medical concepts and knowledge, and finally, a
general-purpose LLM to generate visually aware medical question summaries.
Leveraging our MMQS dataset, we showcase how visual cues from images enhance
the generation of medically nuanced summaries. This multimodal approach not
only enhances the decision-making process in healthcare but also fosters a more
nuanced understanding of patient queries, laying the groundwork for future
research in personalized and responsive medical care",None,-1
7a03f6cb-9907-4804-8143-392e0a1745e9,"Learning, Fast and Slow: A Goal-Directed Memory-Based Approach for Dynamic Environments",0.0291728,"Model-based next state prediction and state value prediction are slow to
converge. To address these challenges, we do the following: i) Instead of a
neural network, we do model-based planning using a parallel memory retrieval
system (which we term the slow mechanism); ii) Instead of learning state
values, we guide the agent's actions using goal-directed exploration, by using
a neural network to choose the next action given the current state and the goal
state (which we term the fast mechanism). The goal-directed exploration is
trained online using hippocampal replay of visited states and future imagined
states every single time step, leading to fast and efficient training.
Empirical studies show that our proposed method has a 92% solve rate across 100
episodes in a dynamically changing grid world, significantly outperforming
state-of-the-art actor critic mechanisms such as PPO (54%), TRPO (50%) and A2C
(24%). Ablation studies demonstrate that both mechanisms are crucial. We posit
that the future of Reinforcement Learning (RL) will be to model goals and
sub-goals for various tasks, and plan it out in a goal-directed memory-based
approach.",None,-1
15eafe64-a7ef-4435-aa1e-6efb6788a5a9,"Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection",0.748567,"This paper aims for high-performance offline LiDAR-based 3D object detection.
We first observe that experienced human annotators annotate objects from a
track-centric perspective. They first label the objects with clear shapes in a
track, and then leverage the temporal coherence to infer the annotations of
obscure objects. Drawing inspiration from this, we propose a high-performance
offline detector in a track-centric perspective instead of the conventional
object-centric perspective. Our method features a bidirectional tracking module
and a track-centric learning module. Such a design allows our detector to infer
and refine a complete track once the object is detected at a certain moment. We
refer to this characteristic as ""onCe detecTed, neveR Lost"" and name the
proposed system CTRL. Extensive experiments demonstrate the remarkable
performance of our method, surpassing the human-level annotating accuracy and
the previous state-of-the-art methods in the highly competitive Waymo Open
Dataset without model ensemble. The code will be made publicly available at
https://github.com/tusen-ai/SST.",None,-1
5ab1cf87-b8fb-4a17-a346-b4858b022b5b,Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation,0.591151,"Multilingualism is widespread around the world and code-switching (CSW) is a
common practice among different language pairs/tuples across locations and
regions. However, there is still not much progress in building successful CSW
systems, despite the recent advances in Massive Multilingual Language Models
(MMLMs). We investigate the reasons behind this setback through a critical
study about the existing CSW data sets (68) across language pairs in terms of
the collection and preparation (e.g. transcription and annotation) stages. This
in-depth analysis reveals that \textbf{a)} most CSW data involves English
ignoring other language pairs/tuples \textbf{b)} there are flaws in terms of
representativeness in data collection and preparation stages due to ignoring
the location based, socio-demographic and register variation in CSW. In
addition, lack of clarity on the data selection and filtering stages shadow the
representativeness of CSW data sets. We conclude by providing a short
check-list to improve the representativeness for forthcoming studies involving
CSW data collection and preparation.",None,-1
3185c662-662a-4625-9f52-f9781cab3fe6,ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer,0.643781,"Large-scale language models, like ChatGPT, have garnered significant media
attention and stunned the public with their remarkable capacity for generating
coherent text from short natural language prompts. In this paper, we aim to
conduct a systematic inspection of ChatGPT's performance in two controllable
generation tasks, with respect to ChatGPT's ability to adapt its output to
different target audiences (expert vs. layman) and writing styles (formal vs.
informal). Additionally, we evaluate the faithfulness of the generated text,
and compare the model's performance with human-authored texts. Our findings
indicate that the stylistic variations produced by humans are considerably
larger than those demonstrated by ChatGPT, and the generated texts diverge from
human samples in several characteristics, such as the distribution of word
types. Moreover, we observe that ChatGPT sometimes incorporates factual errors
or hallucinations when adapting the text to suit a specific style.",None,-1
dbdcd7b3-a487-47c0-acea-fb0e0b99d998,LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning,0.872162,"Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)
competition is detailed in this paper. Unlike conventional video captioning
tasks, GEBC demands that the captioning model possess an understanding of
immediate changes in status around the designated video boundary, making it a
difficult task. This paper proposes an effective model LLMVA-GEBC (Large
Language Model with Video Adapter for Generic Event Boundary Captioning): (1)
We utilize a pretrained LLM for generating human-like captions with high
quality. (2) To adapt the model to the GEBC task, we take the video Q-former as
an adapter and train it with the frozen visual feature extractors and LLM. Our
proposed method achieved a 76.14 score on the test set and won the first place
in the challenge. Our code is available at
https://github.com/zjr2000/LLMVA-GEBC .",None,-1
55c1a611-0e71-4b8d-ab0e-818f67b82586,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,0.97303,"Neural radiance fields enable state-of-the-art photorealistic view synthesis.
However, existing radiance field representations are either too
compute-intensive for real-time rendering or require too much memory to scale
to large scenes. We present a Memory-Efficient Radiance Field (MERF)
representation that achieves real-time rendering of large-scale scenes in a
browser. MERF reduces the memory consumption of prior sparse volumetric
radiance fields using a combination of a sparse feature grid and
high-resolution 2D feature planes. To support large-scale unbounded scenes, we
introduce a novel contraction function that maps scene coordinates into a
bounded volume while still allowing for efficient ray-box intersection. We
design a lossless procedure for baking the parameterization used during
training into a model that achieves real-time rendering while still preserving
the photorealistic view synthesis quality of a volumetric radiance field.",None,-1
7165044c-9738-4b53-b0f4-874651d871b8,Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators,0.692428,"The recent success of Large Language Models (LLMs) signifies an impressive
stride towards artificial general intelligence. They have shown a promising
prospect in automatically completing tasks upon user instructions, functioning
as brain-like coordinators. The associated risks will be revealed as we
delegate an increasing number of tasks to machines for automated completion. A
big question emerges: how can we make machines behave responsibly when helping
humans automate tasks as personal copilots? In this paper, we explore this
question in depth from the perspectives of feasibility, completeness and
security. In specific, we present Responsible Task Automation (ResponsibleTA)
as a fundamental framework to facilitate responsible collaboration between
LLM-based coordinators and executors for task automation with three empowered
capabilities: 1) predicting the feasibility of the commands for executors; 2)
verifying the completeness of executors; 3) enhancing the security (e.g., the
protection of users' privacy). We further propose and compare two paradigms for
implementing the first two capabilities. One is to leverage the generic
knowledge of LLMs themselves via prompt engineering while the other is to adopt
domain-specific learnable models. Moreover, we introduce a local memory
mechanism for achieving the third capability. We evaluate our proposed
ResponsibleTA on UI task automation and hope it could bring more attentions to
ensuring LLMs more responsible in diverse scenarios.",None,-1
f86627ce-a46c-4721-b666-2e95ba23227e,Distributionally Robust Optimization and Invariant Representation Learning for Addressing Subgroup Underrepresentation: Mechanisms and Limitations,0.137489,"Spurious correlation caused by subgroup underrepresentation has received
increasing attention as a source of bias that can be perpetuated by deep neural
networks (DNNs). Distributionally robust optimization has shown success in
addressing this bias, although the underlying working mechanism mostly relies
on upweighting under-performing samples as surrogates for those
underrepresented in data. At the same time, while invariant representation
learning has been a powerful choice for removing nuisance-sensitive features,
it has been little considered in settings where spurious correlations are
caused by significant underrepresentation of subgroups. In this paper, we take
the first step to better understand and improve the mechanisms for debiasing
spurious correlation due to subgroup underrepresentation in medical image
classification. Through a comprehensive evaluation study, we first show that 1)
generalized reweighting of under-performing samples can be problematic when
bias is not the only cause for poor performance, while 2) naive invariant
representation learning suffers from spurious correlations itself. We then
present a novel approach that leverages robust optimization to facilitate the
learning of invariant representations at the presence of spurious correlations.
Finetuned classifiers utilizing such representation demonstrated improved
abilities to reduce subgroup performance disparity, while maintaining high
average and worst-group performance.",None,-1
0025ba29-9290-4422-a8cc-81f244d95fb5,DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization,0.356585,"Stable Diffusion (SD) customization approaches enable users to personalize SD
model outputs, greatly enhancing the flexibility and diversity of AI art.
However, they also allow individuals to plagiarize specific styles or subjects
from copyrighted images, which raises significant concerns about potential
copyright infringement. To address this issue, we propose an invisible
data-free universal adversarial watermark (DUAW), aiming to protect a myriad of
copyrighted images from different customization approaches across various
versions of SD models. First, DUAW is designed to disrupt the variational
autoencoder during SD customization. Second, DUAW operates in a data-free
context, where it is trained on synthetic images produced by a Large Language
Model (LLM) and a pretrained SD model. This approach circumvents the necessity
of directly handling copyrighted images, thereby preserving their
confidentiality. Once crafted, DUAW can be imperceptibly integrated into
massive copyrighted images, serving as a protective measure by inducing
significant distortions in the images generated by customized SD models.
Experimental results demonstrate that DUAW can effectively distort the outputs
of fine-tuned SD models, rendering them discernible to both human observers and
a simple classifier.",None,-1
8e7dd217-ec49-4242-bc91-104efbd3ceb3,Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples,0.719559,"The objective of this work is to explore the learning of visually grounded
speech models (VGS) from multilingual perspective. Bilingual VGS models are
generally trained with an equal number of spoken captions from both languages.
However, in reality, there can be an imbalance among the languages for the
available spoken captions. Our key contribution in this work is to leverage the
power of a high-resource language in a bilingual visually grounded speech model
to improve the performance of a low-resource language. We introduce two methods
to distill the knowledge of high-resource language into low-resource languages:
(1) incorporating a strong pre-trained high-resource language encoder and (2)
using semantically similar spoken captions. Our experiments show that combining
these two approaches effectively enables the low-resource language to surpass
the performances of monolingual and bilingual counterparts for cross-modal
retrieval tasks.",None,-1
2d9c58ba-b3f9-4466-8c67-db7db421f9d6,Supervised Deep Learning for Content-Aware Image Retargeting with Fourier Convolutions,0.162898,"Image retargeting aims to alter the size of the image with attention to the
contents. One of the main obstacles to training deep learning models for image
retargeting is the need for a vast labeled dataset. Labeled datasets are
unavailable for training deep learning models in the image retargeting tasks.
As a result, we present a new supervised approach for training deep learning
models. We use the original images as ground truth and create inputs for the
model by resizing and cropping the original images. A second challenge is
generating different image sizes in inference time. However, regular
convolutional neural networks cannot generate images of different sizes than
the input image. To address this issue, we introduced a new method for
supervised learning. In our approach, a mask is generated to show the desired
size and location of the object. Then the mask and the input image are fed to
the network. Comparing image retargeting methods and our proposed method
demonstrates the model's ability to produce high-quality retargeted images.
Afterward, we compute the image quality assessment score for each output image
based on different techniques and illustrate the effectiveness of our approach.",None,-1
55d6ee9c-a49c-499c-a286-76296bdd8f3c,Unsupervised Chunking with Hierarchical RNN,0.1506,"In Natural Language Processing (NLP), predicting linguistic structures, such
as parsing and chunking, has mostly relied on manual annotations of syntactic
structures. This paper introduces an unsupervised approach to chunking, a
syntactic task that involves grouping words in a non-hierarchical manner. We
present a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to
model word-to-chunk and chunk-to-sentence compositions. Our approach involves a
two-stage training process: pretraining with an unsupervised parser and
finetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset
reveal a notable improvement over existing unsupervised methods, enhancing
phrase F1 score by up to 6 percentage points. Further, finetuning with
downstream tasks results in an additional performance improvement.
Interestingly, we observe that the emergence of the chunking structure is
transient during the neural model's downstream-task training. This study
contributes to the advancement of unsupervised syntactic structure discovery
and opens avenues for further research in linguistic theory.",None,-1
a15934b8-01be-4666-87c7-def11300fac9,Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations,0.650402,"We introduce Affective Visual Dialog, an emotion explanation and reasoning
task as a testbed for research on understanding the formation of emotions in
visually grounded conversations. The task involves three skills: (1)
Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3)
Affective emotion explanation generation based on the dialog. Our key
contribution is the collection of a large-scale dataset, dubbed AffectVisDial,
consisting of 50K 10-turn visually grounded dialogs as well as concluding
emotion attributions and dialog-informed textual emotion explanations,
resulting in a total of 27,180 working hours. We explain our design decisions
in collecting the dataset and introduce the questioner and answerer tasks that
are associated with the participants in the conversation. We train and
demonstrate solid Affective Visual Dialog baselines adapted from
state-of-the-art models. Remarkably, the responses generated by our models show
promising emotional reasoning abilities in response to visually grounded
conversations. Our project page is available at
https://affective-visual-dialog.github.io.",None,-1
2bfc4078-18fa-438e-9385-e217b61c4205,Rethinking the Localization in Weakly Supervised Object Localization,0.466265,"Weakly supervised object localization (WSOL) is one of the most popular and
challenging tasks in computer vision. This task is to localize the objects in
the images given only the image-level supervision. Recently, dividing WSOL into
two parts (class-agnostic object localization and object classification) has
become the state-of-the-art pipeline for this task. However, existing solutions
under this pipeline usually suffer from the following drawbacks: 1) they are
not flexible since they can only localize one object for each image due to the
adopted single-class regression (SCR) for localization; 2) the generated pseudo
bounding boxes may be noisy, but the negative impact of such noise is not well
addressed. To remedy these drawbacks, we first propose to replace SCR with a
binary-class detector (BCD) for localizing multiple objects, where the detector
is trained by discriminating the foreground and background. Then we design a
weighted entropy (WE) loss using the unlabeled data to reduce the negative
impact of noisy bounding boxes. Extensive experiments on the popular
CUB-200-2011 and ImageNet-1K datasets demonstrate the effectiveness of our
method.",None,-1
5e11c0e9-1eb4-434b-803e-b3ce3f00f95c,EvCenterNet: Uncertainty Estimation for Object Detection using Evidential Learning,0.072154,"Uncertainty estimation is crucial in safety-critical settings such as
automated driving as it provides valuable information for several downstream
tasks including high-level decision making and path planning. In this work, we
propose EvCenterNet, a novel uncertainty-aware 2D object detection framework
using evidential learning to directly estimate both classification and
regression uncertainties. To employ evidential learning for object detection,
we devise a combination of evidential and focal loss functions for the sparse
heatmap inputs. We introduce class-balanced weighting for regression and
heatmap prediction to tackle the class imbalance encountered by evidential
learning. Moreover, we propose a learning scheme to actively utilize the
predicted heatmap uncertainties to improve the detection performance by
focusing on the most uncertain points. We train our model on the KITTI dataset
and evaluate it on challenging out-of-distribution datasets including BDD100K
and nuImages. Our experiments demonstrate that our approach improves the
precision and minimizes the execution time loss in relation to the base model.",None,-1
56d34426-a1d7-46bd-be75-406bc20fda80,Tab-CoT: Zero-shot Tabular Chain of Thought,0.336235,"The chain-of-though (CoT) prompting methods were successful in various
natural language processing (NLP) tasks thanks to their ability to unveil the
underlying complex reasoning processes. Such reasoning processes typically
exhibit implicitly structured steps. Recent efforts also started investigating
methods to encourage more explicitly structured reasoning procedures to be
captured. In this work, we propose Tab-CoT, a novel tabular-format CoT
prompting method, which allows the complex reasoning process to be explicitly
modelled in a highly structured manner. Despite its simplicity, we show that
our approach is capable of performing reasoning across multiple dimensions
(i.e., both rows and columns). We demonstrate our approach's strong zero-shot
and few-shot capabilities through extensive experiments on a range of reasoning
tasks.",None,-1
7ded54c5-7c86-41ff-91ff-ece85b186ce1,Rethinking Voice-Face Correlation: A Geometry View,0.835534,"Previous works on voice-face matching and voice-guided face synthesis
demonstrate strong correlations between voice and face, but mainly rely on
coarse semantic cues such as gender, age, and emotion. In this paper, we aim to
investigate the capability of reconstructing the 3D facial shape from voice
from a geometry perspective without any semantic information. We propose a
voice-anthropometric measurement (AM)-face paradigm, which identifies
predictable facial AMs from the voice and uses them to guide 3D face
reconstruction. By leveraging AMs as a proxy to link the voice and face
geometry, we can eliminate the influence of unpredictable AMs and make the face
geometry tractable. Our approach is evaluated on our proposed dataset with
ground-truth 3D face scans and corresponding voice recordings, and we find
significant correlations between voice and specific parts of the face geometry,
such as the nasal cavity and cranium. Our work offers a new perspective on
voice-face correlation and can serve as a good empirical study for
anthropometry science.",None,-1
03e89b22-2df3-4c1c-8928-2d4f39ca053a,See and Think: Embodied Agent in Virtual Environment,0.447497,"Large language models (LLMs) have achieved impressive progress on several
open-world tasks. Recently, using LLMs to build embodied agents has been a
hotspot. In this paper, we propose STEVE, a comprehensive and visionary
embodied agent in the Minecraft virtual environment. STEVE consists of three
key components: vision perception, language instruction, and code action.
Vision perception involves the interpretation of visual information in the
environment, which is then integrated into the LLMs component with agent state
and task instruction. Language instruction is responsible for iterative
reasoning and decomposing complex tasks into manageable guidelines. Code action
generates executable skill actions based on retrieval in skill database,
enabling the agent to interact effectively within the Minecraft environment. We
also collect STEVE-21K dataset, which includes 600$+$ vision-environment pairs,
20K knowledge question-answering pairs, and 200$+$ skill-code pairs. We conduct
continuous block search, knowledge question and answering, and tech tree
mastery to evaluate the performance. Extensive experiments show that STEVE
achieves at most $1.5 \times$ faster unlocking key tech trees and $2.5 \times$
quicker in block search tasks compared to previous state-of-the-art methods.",None,-1
80b2b6e5-30ab-41b8-9663-a2999f8db9fa,Chain-of-Skills: A Configurable Model for Open-domain Question Answering,0.341764,"The retrieval model is an indispensable component for real-world
knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As
separate retrieval skills are annotated for different datasets, recent work
focuses on customized methods, limiting the model transferability and
scalability. In this work, we propose a modular retriever where individual
modules correspond to key skills that can be reused across datasets. Our
approach supports flexible skill configurations based on the target domain to
boost performance. To mitigate task interference, we design a novel
modularization parameterization inspired by sparse Transformer. We demonstrate
that our model can benefit from self-supervised pretraining on Wikipedia and
fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our
approach outperforms recent self-supervised retrievers in zero-shot evaluations
and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA
and OTT-QA.",None,-1
3fb18932-2670-4f41-90f3-8069f6d44557,Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations,0.825474,"Large language models (LLMs) can generate fluent natural language texts when
given relevant documents as background context. This ability has attracted
considerable interest in developing industry applications of LLMs. However,
LLMs are prone to generate hallucinations that are not supported by the
provided sources. In this paper, we propose a hierarchical framework to detect
and mitigate such ungrounded hallucination. Our framework uses Chain of Natural
Language Inference (CoNLI) for hallucination detection and hallucination
reduction via post-editing. Our approach achieves state-of-the-art performance
on hallucination detection and enhances text quality through rewrite, using
LLMs without any fine-tuning or domain-specific prompt engineering. We show
that this simple plug-and-play framework can serve as an effective choice for
hallucination detection and reduction, achieving competitive performance across
various contexts.",None,-1
c77ef60c-9db3-4198-9d23-619494b39f46,Strategize Before Teaching: A Conversational Tutoring System with Pedagogy Self-Distillation,0.728035,"Conversational tutoring systems (CTSs) aim to help students master
educational material with natural language interaction in the form of a dialog.
CTSs have become a key pillar in educational data mining research. A key
challenge in CTSs is to engage the student in the conversation while exposing
them to a diverse set of teaching strategies, akin to a human teacher, thereby,
helping them learn in the process. Different from previous work that generates
responses given the strategies as input, we propose to jointly predict teaching
strategies and generate tutor responses accordingly, which fits a more
realistic application scenario. We benchmark several competitive models on
three dialog tutoring datasets and propose a unified framework that combines
teaching response generation and pedagogical strategy prediction, where a
self-distillation mechanism is adopted to guide the teaching strategy learning
and facilitate tutor response generation. Our experiments and analyses shed
light on how teaching strategies affect dialog tutoring.",None,-1
46075124-8e16-433c-a688-ee50b899af0f,Closing the Curious Case of Neural Text Degeneration,0.204046,"Despite their ubiquity in language generation, it remains unknown why
truncation sampling heuristics like nucleus sampling are so effective. We
provide a theoretical explanation for the effectiveness of the truncation
sampling by proving that truncation methods that discard tokens below some
probability threshold (the most common type of truncation) can guarantee that
all sampled tokens have nonzero true probability. However, thresholds are a
coarse heuristic, and necessarily discard some tokens with nonzero true
probability as well. In pursuit of a more precise sampling strategy, we show
that we can leverage a known source of model errors, the softmax bottleneck, to
prove that certain tokens have nonzero true probability, without relying on a
threshold. Based on our findings, we develop an experimental truncation
strategy and the present pilot studies demonstrating the promise of this type
of algorithm. Our evaluations show that our method outperforms its
threshold-based counterparts under automatic and human evaluation metrics for
low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical
findings and pilot experiments provide both insight into why truncation
sampling works, and make progress toward more expressive sampling algorithms
that better surface the generative capabilities of large language models.",None,-1
1ce9c45c-1536-49d2-a3fc-64f927c7393e,Demystifying Misconceptions in Social Bots Research,0.922643,"Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.",None,-1
4259f9e9-93e4-4ba0-9666-8473003a3f72,WYWEB: A NLP Evaluation Benchmark For Classical Chinese,0.649107,"To fully evaluate the overall performance of different NLP models in a given
domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and
CLUE. The fi eld of natural language understanding has traditionally focused on
benchmarks for various tasks in languages such as Chinese, English, and
multilingua, however, there has been a lack of attention given to the area of
classical Chinese, also known as ""wen yan wen"", which has a rich history
spanning thousands of years and holds signifi cant cultural and academic value.
For the prosperity of the NLP community, in this paper, we introduce the WYWEB
evaluation benchmark, which consists of nine NLP tasks in classical Chinese,
implementing sentence classifi cation, sequence labeling, reading
comprehension, and machine translation. We evaluate the existing pre-trained
language models, which are all struggling with this benchmark. We also
introduce a number of supplementary datasets and additional tools to help
facilitate further progress on classical Chinese NLU. The github repository is
https://github.com/baudzhou/WYWEB.",None,-1
087c6a6d-eb45-4001-8fef-90cf76ad96b8,Diffusion Model for Generative Image Denoising,0.444449,"In supervised learning for image denoising, usually the paired clean images
and noisy images are collected or synthesised to train a denoising model. L2
norm loss or other distance functions are used as the objective function for
training. It often leads to an over-smooth result with less image details. In
this paper, we regard the denoising task as a problem of estimating the
posterior distribution of clean images conditioned on noisy images. We apply
the idea of diffusion model to realize generative image denoising. According to
the noise model in denoising tasks, we redefine the diffusion process such that
it is different from the original one. Hence, the sampling of the posterior
distribution is a reverse process of dozens of steps from the noisy image. We
consider three types of noise model, Gaussian, Gamma and Poisson noise. With
the guarantee of theory, we derive a unified strategy for model training. Our
method is verified through experiments on three types of noise models and
achieves excellent performance.",None,-1
94ef1726-638a-4d24-b8b7-e8cd2539c0f0,Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models,0.488097,"Text-to-image generative models have demonstrated remarkable capabilities in
generating high-quality images based on textual prompts. However, crafting
prompts that accurately capture the user's creative intent remains challenging.
It often involves laborious trial-and-error procedures to ensure that the model
interprets the prompts in alignment with the user's intention. To address the
challenges, we present Promptify, an interactive system that supports prompt
exploration and refinement for text-to-image generative models. Promptify
utilizes a suggestion engine powered by large language models to help users
quickly explore and craft diverse prompts. Our interface allows users to
organize the generated images flexibly, and based on their preferences,
Promptify suggests potential changes to the original prompt. This feedback loop
enables users to iteratively refine their prompts and enhance desired features
while avoiding unwanted ones. Our user study shows that Promptify effectively
facilitates the text-to-image workflow and outperforms an existing baseline
tool widely used for text-to-image generation.",None,-1
626168a9-1afd-4793-9e3c-bf589286c537,Glancing Future for Simultaneous Machine Translation,0.541831,"Simultaneous machine translation (SiMT) outputs translation while reading the
source sentence. Unlike conventional sequence-to-sequence (seq2seq) training,
existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training,
where the model predicts target tokens based on partial source tokens. However,
the prefix2prefix training diminishes the ability of the model to capture
global information and introduces forced predictions due to the absence of
essential source information. Consequently, it is crucial to bridge the gap
between the prefix2prefix training and seq2seq training to enhance the
translation capability of the SiMT model. In this paper, we propose a novel
method that glances future in curriculum learning to achieve the transition
from the seq2seq training to prefix2prefix training. Specifically, we gradually
reduce the available source information from the whole sentence to the prefix
corresponding to that latency. Our method is applicable to a wide range of SiMT
methods and experiments demonstrate that our method outperforms strong
baselines.",None,-1
a47ea070-f163-4df8-b6e9-0cb602a9c256,ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics,0.844938,"We introduce ProofNet, a benchmark for autoformalization and formal proving
of undergraduate-level mathematics. The ProofNet benchmarks consists of 371
examples, each consisting of a formal theorem statement in Lean 3, a natural
language theorem statement, and a natural language proof. The problems are
primarily drawn from popular undergraduate pure mathematics textbooks and cover
topics such as real and complex analysis, linear algebra, abstract algebra, and
topology. We intend for ProofNet to be a challenging benchmark that will drive
progress in autoformalization and automatic theorem proving. We report baseline
results on statement autoformalization via in-context learning. Moreover, we
introduce two novel statement autoformalization methods: prompt retrieval and
distilled backtranslation.",None,-1
a9bb4ddf-af4f-48d3-bf2d-ef1363d31b63,Boosting Theory-of-Mind Performance in Large Language Models via Prompting,0.306653,"Large language models (LLMs) excel in many tasks in 2023, but they still face
challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require
understanding agents' beliefs, goals, and mental states, are essential for
common-sense reasoning involving humans, making it crucial to enhance LLM
performance in this area. This study measures the ToM performance of GPT-4 and
three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates
the effectiveness of in-context learning in improving their ToM comprehension.
We evaluated prompts featuring two-shot chain of thought reasoning and
step-by-step thinking instructions. We found that LLMs trained with
Reinforcement Learning from Human Feedback (RLHF) (all models excluding
Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed
best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell
short of the 87% human accuracy on the test set. However, when supplied with
prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM
accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate
prompting enhances LLM ToM reasoning, and they underscore the context-dependent
nature of LLM cognitive capacities.",None,-1
06f762bf-06ff-4a16-8df8-4ceced1271e7,Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities,0.649518,"One of the challenges of studying common neurological disorders is disease
heterogeneity including differences in causes, neuroimaging characteristics,
comorbidities, or genetic variation. Normative modelling has become a popular
method for studying such cohorts where the 'normal' behaviour of a
physiological system is modelled and can be used at subject level to detect
deviations relating to disease pathology. For many heterogeneous diseases, we
expect to observe abnormalities across a range of neuroimaging and biological
variables. However, thus far, normative models have largely been developed for
studying a single imaging modality. We aim to develop a multi-modal normative
modelling framework where abnormality is aggregated across variables of
multiple modalities and is better able to detect deviations than uni-modal
baselines. We propose two multi-modal VAE normative models to detect subject
level deviations across T1 and DTI data. Our proposed models were better able
to detect diseased individuals, capture disease severity, and correlate with
patient cognition than baseline approaches. We also propose a multivariate
latent deviation metric, measuring deviations from the joint latent space,
which outperformed feature-based metrics.",None,-1
972f433e-c71c-433c-841f-cf95fbbdcec9,Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis,0.924298,"Generative latent diffusion models have been established as state-of-the-art
in data generation. One promising application is generation of realistic
synthetic medical imaging data for open data sharing without compromising
patient privacy. Despite the promise, the capacity of such models to memorize
sensitive patient training data and synthesize samples showing high resemblance
to training data samples is relatively unexplored. Here, we assess the
memorization capacity of 3D latent diffusion models on photon-counting coronary
computed tomography angiography and knee magnetic resonance imaging datasets.
To detect potential memorization of training samples, we utilize
self-supervised models based on contrastive learning. Our results suggest that
such latent diffusion models indeed memorize training data, and there is a dire
need for devising strategies to mitigate memorization.",None,-1
52e457f3-b34a-4b23-b843-60364a23ad08,Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models,0.380881,"Large language models (LLMs) can perform a wide range of tasks by following
natural language instructions, without the necessity of task-specific
fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by
the quality of these instructions, and manually writing effective instructions
for each task is a laborious and subjective process. In this paper, we
introduce Auto-Instruct, a novel method to automatically improve the quality of
instructions provided to LLMs. Our method leverages the inherent generative
ability of LLMs to produce diverse candidate instructions for a given task, and
then ranks them using a scoring model trained on a variety of 575 existing NLP
tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both
human-written instructions and existing baselines of LLM-generated
instructions. Furthermore, our method exhibits notable generalizability even
with other LLMs that are not incorporated into its training process.",None,-1
214b959f-7f72-4899-945a-12cc8fa977d9,Knowledge-Guided Short-Context Action Anticipation in Human-Centric Videos,0.137915,"This work focuses on anticipating long-term human actions, particularly using
short video segments, which can speed up editing workflows through improved
suggestions while fostering creativity by suggesting narratives. To this end,
we imbue a transformer network with a symbolic knowledge graph for action
anticipation in video segments by boosting certain aspects of the transformer's
attention mechanism at run-time. Demonstrated on two benchmark datasets,
Breakfast and 50Salads, our approach outperforms current state-of-the-art
methods for long-term action anticipation using short video context by up to
9%.",None,-1
1e064217-4aaf-4ae0-8f29-93f96715b0aa,Controlling Steering with Energy-Based Models,0.0599695,"So-called implicit behavioral cloning with energy-based models has shown
promising results in robotic manipulation tasks. We tested if the method's
advantages carry on to controlling the steering of a real self-driving car with
an end-to-end driving model. We performed an extensive comparison of the
implicit behavioral cloning approach with explicit baseline approaches, all
sharing the same neural network backbone architecture. Baseline explicit models
were trained with regression (MAE) loss, classification loss (softmax and
cross-entropy on a discretization), or as mixture density networks (MDN). While
models using the energy-based formulation performed comparably to baseline
approaches in terms of safety driver interventions, they had a higher whiteness
measure, indicating higher jerk. To alleviate this, we show two methods that
can be used to improve the smoothness of steering. We confirmed that
energy-based models handle multimodalities slightly better than simple
regression, but this did not translate to significantly better driving ability.
We argue that the steering-only road-following task has too few multimodalities
to benefit from energy-based models. This shows that applying implicit
behavioral cloning to real-world tasks can be challenging, and further
investigation is needed to bring out the theoretical advantages of energy-based
models.",None,-1
560c4e6d-317d-4274-8781-bf98f4ccb23f,Semantic relatedness in DBpedia: A comparative and experimental assessment,0.407235,"Evaluating semantic relatedness of Web resources is still an open challenge.
This paper focuses on knowledge-based methods, which represent an alternative
to corpus-based approaches, and rely in general on the availability of
knowledge graphs. In particular, we have selected 10 methods from the existing
literature, that have been organized according to it adjacent resources, triple
patterns, and triple weights-based methods. They have been implemented and
evaluated by using DBpedia as reference RDF knowledge graph. Since DBpedia is
continuously evolving, the experimental results provided by these methods in
the literature are not comparable. For this reason, in this work, such methods
have been experimented by running them all at once on the same DBpedia release
and against 14 well-known golden datasets. On the basis of the correlation
values with human judgment obtained according to the experimental results,
weighting the RDF triples in combination with evaluating all the directed paths
linking the compared resources is the best strategy in order to compute
semantic relatedness in DBpedia.",None,-1
44676467-3cba-46c0-b868-17f1995044e3,"Democratising AI: Multiple Meanings, Goals, and Methods",0.996472,"Numerous parties are calling for the democratisation of AI, but the phrase is
used to refer to a variety of goals, the pursuit of which sometimes conflict.
This paper identifies four kinds of AI democratisation that are commonly
discussed: (1) the democratisation of AI use, (2) the democratisation of AI
development, (3) the democratisation of AI profits, and (4) the democratisation
of AI governance. Numerous goals and methods of achieving each form of
democratisation are discussed. The main takeaway from this paper is that AI
democratisation is a multifarious and sometimes conflicting concept that should
not be conflated with improving AI accessibility. If we want to move beyond
ambiguous commitments to democratising AI, to productive discussions of
concrete policies and trade-offs, then we need to recognise the principal role
of the democratisation of AI governance in navigating tradeoffs and risks
across decisions around use, development, and profits.",None,-1
b017153a-3e96-42d7-a382-2508901274ca,A Psycholinguistic Analysis of BERT's Representations of Compounds,0.497774,"This work studies the semantic representations learned by BERT for compounds,
that is, expressions such as sunlight or bodyguard. We build on recent studies
that explore semantic information in Transformers at the word level and test
whether BERT aligns with human semantic intuitions when dealing with
expressions (e.g., sunlight) whose overall meaning depends -- to a various
extent -- on the semantics of the constituent words (sun, light). We leverage a
dataset that includes human judgments on two psycholinguistic measures of
compound semantic analysis: lexeme meaning dominance (LMD; quantifying the
weight of each constituent toward the compound meaning) and semantic
transparency (ST; evaluating the extent to which the compound meaning is
recoverable from the constituents' semantics). We show that BERT-based measures
moderately align with human intuitions, especially when using contextualized
representations, and that LMD is overall more predictable than ST. Contrary to
the results reported for 'standard' words, higher, more contextualized layers
are the best at representing compound meaning. These findings shed new light on
the abilities of BERT in dealing with fine-grained semantic phenomena.
Moreover, they can provide insights into how speakers represent compounds.",None,-1
6adf2f79-9dd7-40eb-9bf2-498598a66d84,KPIs-Based Clustering and Visualization of HPC jobs: a Feature Reduction Approach,0.538556,"High-Performance Computing (HPC) systems need to be constantly monitored to
ensure their stability. The monitoring systems collect a tremendous amount of
data about different parameters or Key Performance Indicators (KPIs), such as
resource usage, IO waiting time, etc. A proper analysis of this data, usually
stored as time series, can provide insight in choosing the right management
strategies as well as the early detection of issues. In this paper, we
introduce a methodology to cluster HPC jobs according to their KPI indicators.
Our approach reduces the inherent high dimensionality of the collected data by
applying two techniques to the time series: literature-based and variance-based
feature extraction. We also define a procedure to visualize the obtained
clusters by combining the two previous approaches and the Principal Component
Analysis (PCA). Finally, we have validated our contributions on a real data set
to conclude that those KPIs related to CPU usage provide the best cohesion and
separation for clustering analysis and the good results of our visualization
methodology.",None,-1
dc2b10e3-a4c4-4a10-99bd-f4280626bd41,Learning Attention as Disentangler for Compositional Zero-shot Learning,0.309626,"Compositional zero-shot learning (CZSL) aims at learning visual concepts
(i.e., attributes and objects) from seen compositions and combining concept
knowledge into unseen compositions. The key to CZSL is learning the
disentanglement of the attribute-object composition. To this end, we propose to
exploit cross-attentions as compositional disentanglers to learn disentangled
concept embeddings. For example, if we want to recognize an unseen composition
""yellow flower"", we can learn the attribute concept ""yellow"" and object concept
""flower"" from different yellow objects and different flowers respectively. To
further constrain the disentanglers to learn the concept of interest, we employ
a regularization at the attention level. Specifically, we adapt the earth
mover's distance (EMD) as a feature similarity metric in the cross-attention
module. Moreover, benefiting from concept disentanglement, we improve the
inference process and tune the prediction score by combining multiple concept
probabilities. Comprehensive experiments on three CZSL benchmark datasets
demonstrate that our method significantly outperforms previous works in both
closed- and open-world settings, establishing a new state-of-the-art.",None,-1
8004099c-dded-4157-ae51-7a6c6f35037b,ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection,0.55537,"Fanfiction, a popular form of creative writing set within established
fictional universes, has gained a substantial online following. However,
ensuring the well-being and safety of participants has become a critical
concern in this community. The detection of triggering content, material that
may cause emotional distress or trauma to readers, poses a significant
challenge. In this paper, we describe our approach for the Trigger Detection
shared task at PAN CLEF 2023, where we want to detect multiple triggering
content in a given Fanfiction document. For this, we build a hierarchical model
that uses recurrence over Transformer-based language models. In our approach,
we first split long documents into smaller sized segments and use them to
fine-tune a Transformer model. Then, we extract feature embeddings from the
fine-tuned Transformer model, which are used as input in the training of
multiple LSTM models for trigger detection in a multi-label setting. Our model
achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the
validation set, which are higher than the baseline results shared at PAN CLEF
2023.",None,-1
1f57e986-d86f-4f6e-a249-a5297a8c45ec,PharmacyGPT: The AI Pharmacist,0.51322,"In this study, we introduce PharmacyGPT, a novel framework to assess the
capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in
emulating the role of clinical pharmacists. Our methodology encompasses the
utilization of LLMs to generate comprehensible patient clusters, formulate
medication plans, and forecast patient outcomes. We conduct our investigation
using real data acquired from the intensive care unit (ICU) at the University
of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable
insights into the potential applications and limitations of LLMs in the field
of clinical pharmacy, with implications for both patient care and the
development of future AI-driven healthcare solutions. By evaluating the
performance of PharmacyGPT, we aim to contribute to the ongoing discourse
surrounding the integration of artificial intelligence in healthcare settings,
ultimately promoting the responsible and efficacious use of such technologies.",None,-1
401ba8ae-21e6-4d41-819f-f05e29740a1f,Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages,0.123823,"The impressive achievements of transformers force NLP researchers to delve
into how these models represent the underlying structure of natural language.
In this paper, we propose a novel standpoint to investigate the above issue:
using typological similarities among languages to observe how their respective
monolingual models encode structural information. We aim to layer-wise compare
transformers for typologically similar languages to observe whether these
similarities emerge for particular layers. For this investigation, we propose
to use Centered Kernel Alignment to measure similarity among weight matrices.
We found that syntactic typological similarity is consistent with the
similarity between the weights in the middle layers, which are the pretrained
BERT layers to which syntax encoding is generally attributed. Moreover, we
observe that a domain adaptation on semantically equivalent texts enhances this
similarity among weight matrices.",None,-1
eaaa3fc9-3717-4ccb-bdfa-7e407e2017cb,SFD2: Semantic-guided Feature Detection and Description,0.90005,"Visual localization is a fundamental task for various applications including
autonomous driving and robotics. Prior methods focus on extracting large
amounts of often redundant locally reliable features, resulting in limited
efficiency and accuracy, especially in large-scale environments under
challenging conditions. Instead, we propose to extract globally reliable
features by implicitly embedding high-level semantics into both the detection
and description processes. Specifically, our semantic-aware detector is able to
detect keypoints from reliable regions (e.g. building, traffic lane) and
suppress unreliable areas (e.g. sky, car) implicitly instead of relying on
explicit semantic labels. This boosts the accuracy of keypoint matching by
reducing the number of features sensitive to appearance changes and avoiding
the need of additional segmentation networks at test time. Moreover, our
descriptors are augmented with semantics and have stronger discriminative
ability, providing more inliers at test time. Particularly, experiments on
long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons
datasets demonstrate that our model outperforms previous local features and
gives competitive accuracy to advanced matchers but is about 2 and 3 times
faster when using 2k and 4k keypoints, respectively.",None,-1
00669bc8-0892-4bb1-9793-778adb893c43,Sparse4D v2: Recurrent Temporal Fusion with Sparse Model,0.99147,"Sparse algorithms offer great flexibility for multi-view temporal perception
tasks. In this paper, we present an enhanced version of Sparse4D, in which we
improve the temporal fusion module by implementing a recursive form of
multi-frame feature sampling. By effectively decoupling image features and
structured anchor features, Sparse4D enables a highly efficient transformation
of temporal features, thereby facilitating temporal fusion solely through the
frame-by-frame transmission of sparse features. The recurrent temporal fusion
approach provides two main benefits. Firstly, it reduces the computational
complexity of temporal fusion from $O(T)$ to $O(1)$, resulting in significant
improvements in inference speed and memory usage. Secondly, it enables the
fusion of long-term information, leading to more pronounced performance
improvements due to temporal fusion. Our proposed approach, Sparse4Dv2, further
enhances the performance of the sparse perception algorithm and achieves
state-of-the-art results on the nuScenes 3D detection benchmark. Code will be
available at \url{https://github.com/linxuewu/Sparse4D}.",None,-1
2fbde41c-5b4d-4080-bed7-df5873e27150,XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,0.814564,"In recent years, pre-trained language models have undergone rapid development
with the emergence of large-scale models. However, there is a lack of
open-sourced chat models specifically designed for the Chinese language,
especially in the field of Chinese finance, at the scale of hundreds of
billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese
chat model to date, built upon the BLOOM-176B architecture. Additionally, we
propose a novel training method called hybrid-tuning to mitigate catastrophic
forgetting. By combining general-domain with domain-specific knowledge and
integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable
of providing accurate and contextually appropriate responses in the Chinese
financial domain.",None,-1
494c9a1f-5c31-4701-9b64-e0074fde69e0,$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models,0.529348,"Fine-tuning a language model on a new domain is standard practice for domain
adaptation. However, it can be infeasible when it comes to modern large-scale
language models such as GPT-3, which can only be accessed through APIs, making
it difficult to access the internal parameters of the model. In this paper, we
propose $k$NN-Adapter, a method to effectively adapt these black-box large
language models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the
retrieval-augmented language model, and adaptively learns to interpolate the
output of the language model with retrieval results from a datastore consisting
of the target domain data. Our experiments on four different domains
demonstrate that $k$NN-Adapter significantly improves perplexity, and works
particularly well in settings with limited access to LLMs. Additionally, we
show that $k$NN-Adapter is more effective than fine-tuning when the amount of
training data is limited. We also release a dataset to encourage further study.",None,-1
f305c19b-e669-49e5-8c61-bd7eb33b8d72,Kernel Density Bayesian Inverse Reinforcement Learning,0.46325,"Inverse reinforcement learning~(IRL) is a powerful framework to infer an
agent's reward function by observing its behavior, but IRL algorithms that
learn point estimates of the reward function can be misleading because there
may be several functions that describe an agent's behavior equally well. A
Bayesian approach to IRL models a distribution over candidate reward functions,
alleviating the shortcomings of learning a point estimate. However, several
Bayesian IRL algorithms use a $Q$-value function in place of the likelihood
function. The resulting posterior is computationally intensive to calculate,
has few theoretical guarantees, and the $Q$-value function is often a poor
approximation for the likelihood. We introduce kernel density Bayesian IRL
(KD-BIRL), which uses conditional kernel density estimation to directly
approximate the likelihood, providing an efficient framework that, with a
modified reward function parameterization, is applicable to environments with
complex and infinite state spaces. We demonstrate KD-BIRL's benefits through a
series of experiments in Gridworld environments and a simulated sepsis
treatment task.",None,-1
73d99270-8b87-4f88-8a2c-fbe5578212e8,Nonrigid Object Contact Estimation With Regional Unwrapping Transformer,0.393469,"Acquiring contact patterns between hands and nonrigid objects is a common
concern in the vision and robotics community. However, existing learning-based
methods focus more on contact with rigid ones from monocular images. When
adopting them for nonrigid contact, a major problem is that the existing
contact representation is restricted by the geometry of the object.
Consequently, contact neighborhoods are stored in an unordered manner and
contact features are difficult to align with image cues. At the core of our
approach lies a novel hand-object contact representation called RUPs (Region
Unwrapping Profiles), which unwrap the roughly estimated hand-object surfaces
as multiple high-resolution 2D regional profiles. The region grouping strategy
is consistent with the hand kinematic bone division because they are the
primitive initiators for a composite contact pattern. Based on this
representation, our Regional Unwrapping Transformer (RUFormer) learns the
correlation priors across regions from monocular inputs and predicts
corresponding contact and deformed transformations. Our experiments demonstrate
that the proposed framework can robustly estimate the deformed degrees and
deformed transformations, which makes it suitable for both nonrigid and rigid
contact.",None,-1
c521f0e6-c5d6-4d78-8ceb-8eb4d1f82953,Channelformer: Attention based Neural Solution for Wireless Channel Estimation and Effective Online Training,0.635372,"In this paper, we propose an encoder-decoder neural architecture (called
Channelformer) to achieve improved channel estimation for orthogonal
frequency-division multiplexing (OFDM) waveforms in downlink scenarios. The
self-attention mechanism is employed to achieve input precoding for the input
features before processing them in the decoder. In particular, we implement
multi-head attention in the encoder and a residual convolutional neural
architecture as the decoder, respectively. We also employ a customized
weight-level pruning to slim the trained neural network with a fine-tuning
process, which reduces the computational complexity significantly to realize a
low complexity and low latency solution. This enables reductions of up to 70\%
in the parameters, while maintaining an almost identical performance compared
with the complete Channelformer. We also propose an effective online training
method based on the fifth generation (5G) new radio (NR) configuration for the
modern communication systems, which only needs the available information at the
receiver for online training. Using industrial standard channel models, the
simulations of attention-based solutions show superior estimation performance
compared with other candidate neural network methods for channel estimation.",None,-1
8f871cc8-f3fb-40f3-a477-9bd75da858cd,XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words,0.416717,"Due to the absence of explicit word boundaries in the speech stream, the task
of segmenting spoken sentences into word units without text supervision is
particularly challenging. In this work, we leverage the most recent
self-supervised speech models that have proved to quickly adapt to new tasks
through fine-tuning, even in low resource conditions. Taking inspiration from
semi-supervised learning, we fine-tune an XLS-R model to predict word
boundaries themselves produced by top-tier speech segmentation systems: DPDP,
VG-HuBERT, GradSeg and DP-Parse. Once XLS-R is fine-tuned, it is used to infer
new word boundary labels that are used in turn for another fine-tuning step.
Our method consistently improves the performance of each system and sets a new
state-of-the-art that is, on average 130% higher than the previous one as
measured by the F1 score on correctly discovered word tokens on five corpora
featuring different languages. Finally, our system can segment speech from
languages unseen during fine-tuning in a zero-shot fashion.",None,-1
e189df86-186b-4285-a48c-9e74b6e83be3,Audio-Visual Contrastive Learning with Temporal Self-Supervision,0.471514,"We propose a self-supervised learning approach for videos that learns
representations of both the RGB frames and the accompanying audio without human
supervision. In contrast to images that capture the static scene appearance,
videos also contain sound and temporal scene dynamics. To leverage the temporal
and aural dimension inherent to videos, our method extends temporal
self-supervision to the audio-visual setting and integrates it with multi-modal
contrastive objectives. As temporal self-supervision, we pose playback speed
and direction recognition in both modalities and propose intra- and inter-modal
temporal ordering tasks. Furthermore, we design a novel contrastive objective
in which the usual pairs are supplemented with additional sample-dependent
positives and negatives sampled from the evolving feature space. In our model,
we apply such losses among video clips and between videos and their temporally
corresponding audio clips. We verify our model design in extensive ablation
experiments and evaluate the video and audio representations in transfer
experiments to action recognition and retrieval on UCF101 and HMBD51, audio
classification on ESC50, and robust video fingerprinting on VGG-Sound, with
state-of-the-art results.",None,-1
b0f3a6e7-2c40-48d3-81da-df7a6130ad5a,A Generalist Dynamics Model for Control,0.599784,"We investigate the use of transformer sequence models as dynamics models
(TDMs) for control. We find that TDMs exhibit strong generalization
capabilities to unseen environments, both in a few-shot setting, where a
generalist TDM is fine-tuned with small amounts of data from the target
environment, and in a zero-shot setting, where a generalist TDM is applied to
an unseen environment without any further training. Here, we demonstrate that
generalizing system dynamics can work much better than generalizing optimal
behavior directly as a policy. Additional results show that TDMs also perform
well in a single-environment learning setting when compared to a number of
baseline models. These properties make TDMs a promising ingredient for a
foundation model of control.",None,-1
6d9477d5-a5bc-4f1a-a925-cbcf75803ff9,DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph,0.367551,"In this work, we present a web application named DBLPLink, which performs
entity linking over the DBLP scholarly knowledge graph. DBLPLink uses
text-to-text pre-trained language models, such as T5, to produce entity label
spans from an input text question. Entity candidates are fetched from a
database based on the labels, and an entity re-ranker sorts them based on
entity embeddings, such as TransE, DistMult and ComplEx. The results are
displayed so that users may compare and contrast the results between T5-small,
T5-base and the different KG embeddings used. The demo can be accessed at
https://ltdemos.informatik.uni-hamburg.de/dblplink/.",None,-1
92da8ce6-9a5c-41b2-ab49-695383ea5ed1,Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain,0.826762,"Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
Parameter-Efficient Fine-Tuning (PEFT) techniques for fine-tuning language
models significantly reduce computational requirements by selectively
fine-tuning small subsets of parameters. In this study, we propose a two-step
PEFT framework and evaluate it in the clinical domain. Our approach combines a
specialised PEFT adapter layer designed for clinical domain adaptation with
another adapter specialised for downstream tasks. We evaluate the framework on
multiple clinical outcome prediction datasets, comparing it to clinically
trained language models. Our framework achieves a better AUROC score averaged
across all clinical downstream tasks compared to clinical language models. In
particular, we observe large improvements of 4-5% AUROC in large-scale
multilabel classification tasks, such as diagnoses and procedures
classification. To our knowledge, this study is the first to provide an
extensive empirical analysis of the interplay between PEFT techniques and
domain adaptation in an important real-world domain of clinical applications.",None,-1
22b79ce0-8e7c-400a-96af-0deeb429fc47,Artificial General Intelligence for Medical Imaging,0.727174,"In this review, we explore the potential applications of Artificial General
Intelligence (AGI) models in healthcare, focusing on foundational Large
Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We
emphasize the importance of integrating clinical expertise, domain knowledge,
and multimodal capabilities into AGI models. In addition, we lay out key
roadmaps that guide the development and deployment of healthcare AGI models.
Throughout the review, we provide critical perspectives on the potential
challenges and pitfalls associated with deploying large-scale AGI models in the
medical field. This comprehensive review aims to offer insights into the future
implications of AGI in medical imaging, healthcare and beyond.",None,-1
745714d6-08a3-497f-a668-7099699c1c44,"Unsupervised Learning on a DIET: Datum IndEx as Target Free of Self-Supervision, Reconstruction, Projector Head",0.0601642,"Costly, noisy, and over-specialized, labels are to be set aside in favor of
unsupervised learning if we hope to learn cheap, reliable, and transferable
models. To that end, spectral embedding, self-supervised learning, or
generative modeling have offered competitive solutions. Those methods however
come with numerous challenges \textit{e.g.} estimating geodesic distances,
specifying projector architectures and anti-collapse losses, or specifying
decoder architectures and reconstruction losses. In contrast, we introduce a
simple explainable alternative -- coined \textbf{DIET} -- to learn
representations from unlabeled data, free of those challenges. \textbf{DIET} is
blatantly simple: take one's favorite classification setup and use the
\textbf{D}atum \textbf{I}nd\textbf{E}x as its \textbf{T}arget class,
\textit{i.e. each sample is its own class}, no further changes needed.
\textbf{DIET} works without a decoder/projector network, is not based on
positive pairs nor reconstruction, introduces no hyper-parameters, and works
out-of-the-box across datasets and architectures. Despite \textbf{DIET}'s
simplicity, the learned representations are of high-quality and often on-par
with the state-of-the-art \textit{e.g.} using a linear classifier on top of
DIET's learned representation reaches $71.4\%$ on CIFAR100 with a Resnet101,
$52.5\%$ on TinyImagenet with a Resnext50.",None,-1
6e41f41e-d741-430e-8346-30f1edc06798,HoloDiffusion: Training a 3D Diffusion Model using 2D Images,0.821449,"Diffusion models have emerged as the best approach for generative modeling of
2D images. Part of their success is due to the possibility of training them on
millions if not billions of images with a stable learning objective. However,
extending these models to 3D remains difficult for two reasons. First, finding
a large quantity of 3D training data is much more complex than for 2D images.
Second, while it is conceptually trivial to extend the models to operate on 3D
rather than 2D grids, the associated cubic growth in memory and compute
complexity makes this infeasible. We address the first challenge by introducing
a new diffusion setup that can be trained, end-to-end, with only posed 2D
images for supervision; and the second challenge by proposing an image
formation model that decouples model memory from spatial memory. We evaluate
our method on real-world data, using the CO3D dataset which has not been used
to train 3D generative models before. We show that our diffusion models are
scalable, train robustly, and are competitive in terms of sample quality and
fidelity to existing approaches for 3D generative modeling.",None,-1
5772ea3f-1484-445c-b791-973835991dbd,Safe POMDP Online Planning via Shielding,0.440463,"Partially observable Markov decision processes (POMDPs) have been widely used
in many robotic applications for sequential decision-making under uncertainty.
POMDP online planning algorithms such as Partially Observable Monte-Carlo
Planning (POMCP) can solve very large POMDPs with the goal of maximizing the
expected return. But the resulting policies cannot provide safety guarantees
which are imperative for real-world safety-critical tasks (e.g., autonomous
driving). In this work, we consider safety requirements represented as
almost-sure reach-avoid specifications (i.e., the probability to reach a set of
goal states is one and the probability to reach a set of unsafe states is
zero). We compute shields that restrict unsafe actions which would violate the
almost-sure reach-avoid specifications. We then integrate these shields into
the POMCP algorithm for safe POMDP online planning. We propose four distinct
shielding methods, differing in how the shields are computed and integrated,
including factored variants designed to improve scalability. Experimental
results on a set of benchmark domains demonstrate that the proposed shielding
methods successfully guarantee safety (unlike the baseline POMCP without
shielding) on large POMDPs, with negligible impact on the runtime for online
planning.",None,-1
4a6e1c8e-b407-4ef1-91f2-c3eb9064205a,Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method,0.308445,"Large Language Models (LLMs) have shown great potential in Natural Language
Processing (NLP) tasks. However, recent literature reveals that LLMs generate
nonfactual responses intermittently, which impedes the LLMs' reliability for
further utilization. In this paper, we propose a novel self-detection method to
detect which questions that a LLM does not know that are prone to generate
nonfactual results. Specifically, we first diversify the textual expressions
for a given question and collect the corresponding answers. Then we examine the
divergencies between the generated answers to identify the questions that the
model may generate falsehoods. All of the above steps can be accomplished by
prompting the LLMs themselves without referring to any other external
resources. We conduct comprehensive experiments and demonstrate the
effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,
and GPT-4.",None,-1
37700bb1-6916-43ac-8fad-ef602adf08c4,SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities,0.88527,"The rapid advance in artificial intelligence technology has facilitated the
prosperity of digital humanities research. Against such backdrop, research
methods need to be transformed in the intelligent processing of ancient texts,
which is a crucial component of digital humanities research, so as to adapt to
new development trends in the wave of AIGC. In this study, we propose a GPT
model called SikuGPT based on the corpus of Siku Quanshu. The model's
performance in tasks such as intralingual translation and text classification
exceeds that of other GPT-type models aimed at processing ancient texts.
SikuGPT's ability to process traditional Chinese ancient texts can help promote
the organization of ancient information and knowledge services, as well as the
international dissemination of Chinese ancient culture.",None,-1
53a39c2c-0750-442f-87d3-bfd772258540,GeoAdapt: Self-Supervised Test-Time Adaptation in LiDAR Place Recognition Using Geometric Priors,0.451123,"LiDAR place recognition approaches based on deep learning suffer from
significant performance degradation when there is a shift between the
distribution of training and test datasets, often requiring re-training the
networks to achieve peak performance. However, obtaining accurate ground truth
data for new training data can be prohibitively expensive, especially in
complex or GPS-deprived environments. To address this issue we propose
GeoAdapt, which introduces a novel auxiliary classification head to generate
pseudo-labels for re-training on unseen environments in a self-supervised
manner. GeoAdapt uses geometric consistency as a prior to improve the
robustness of our generated pseudo-labels against domain shift, improving the
performance and reliability of our Test-Time Adaptation approach. Comprehensive
experiments show that GeoAdapt significantly boosts place recognition
performance across moderate to severe domain shifts, and is competitive with
fully supervised test-time adaptation approaches. Our code is available at
https://github.com/csiro-robotics/GeoAdapt.",None,-1
1788a401-9851-47db-aaed-26c8cc8ea901,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,0.81512,"Detecting factual errors in textual information, whether generated by large
language models (LLM) or curated by humans, is crucial for making informed
decisions. LLMs' inability to attribute their claims to external knowledge and
their tendency to hallucinate makes it difficult to rely on their responses.
Humans, too, are prone to factual errors in their writing. Since manual
detection and correction of factual errors is labor-intensive, developing an
automatic approach can greatly reduce human effort. We present FLEEK, a
prototype tool that automatically extracts factual claims from text, gathers
evidence from external knowledge sources, evaluates the factuality of each
claim, and suggests revisions for identified errors using the collected
evidence. Initial empirical evaluation on fact error detection (77-85\% F1)
shows the potential of FLEEK. A video demo of FLEEK can be found at
https://youtu.be/NapJFUlkPdQ.",None,-1
3bf467a8-41a8-45bf-a48e-e78095fa25d0,Using Artificial Populations to Study Psychological Phenomena in Neural Models,0.0579342,"The recent proliferation of research into transformer based natural language
processing has led to a number of studies which attempt to detect the presence
of human-like cognitive behavior in the models. We contend that, as is true of
human psychology, the investigation of cognitive behavior in language models
must be conducted in an appropriate population of an appropriate size for the
results to be meaningful. We leverage work in uncertainty estimation in a novel
approach to efficiently construct experimental populations. The resultant tool,
PopulationLM, has been made open source. We provide theoretical grounding in
the uncertainty estimation literature and motivation from current cognitive
work regarding language models. We discuss the methodological lessons from
other scientific communities and attempt to demonstrate their application to
two artificial population studies. Through population based experimentation we
find that language models exhibit behavior consistent with typicality effects
among categories highly represented in training. However, we find that language
models don't tend to exhibit structural priming effects. Generally, our results
show that single models tend to over estimate the presence of cognitive
behaviors in neural models.",None,-1
2e91d036-d76d-414c-900e-9b58193edde2,Agent-based Learning of Materials Datasets from Scientific Literature,0.863402,"Advancements in machine learning and artificial intelligence are transforming
materials discovery. Yet, the availability of structured experimental data
remains a bottleneck. The vast corpus of scientific literature presents a
valuable and rich resource of such data. However, manual dataset creation from
these resources is challenging due to issues in maintaining quality and
consistency, scalability limitations, and the risk of human error and bias.
Therefore, in this work, we develop a chemist AI agent, powered by large
language models (LLMs), to overcome these challenges by autonomously creating
structured datasets from natural language text, ranging from sentences and
paragraphs to extensive scientific research articles. Our chemist AI agent,
Eunomia, can plan and execute actions by leveraging the existing knowledge from
decades of scientific research articles, scientists, the Internet and other
tools altogether. We benchmark the performance of our approach in three
different information extraction tasks with various levels of complexity,
including solid-state impurity doping, metal-organic framework (MOF) chemical
formula, and property relations. Our results demonstrate that our zero-shot
agent, with the appropriate tools, is capable of attaining performance that is
either superior or comparable to the state-of-the-art fine-tuned materials
information extraction methods. This approach simplifies compilation of machine
learning-ready datasets for various materials discovery applications, and
significantly ease the accessibility of advanced natural language processing
tools for novice users in natural language. The methodology in this work is
developed as an open-source software on https://github.com/AI4ChemS/Eunomia.",None,-1
ff240bc4-dc98-4e7a-a506-82a789843790,Slovo: Russian Sign Language Dataset,0.55192,"One of the main challenges of the sign language recognition task is the
difficulty of collecting a suitable dataset due to the gap between
hard-of-hearing and hearing societies. In addition, the sign language in each
country differs significantly, which obliges the creation of new data for each
of them. This paper presents the Russian Sign Language (RSL) video dataset
Slovo, produced using crowdsourcing platforms. The dataset contains 20,000
FullHD recordings, divided into 1,000 classes of isolated RSL gestures received
by 194 signers. We also provide the entire dataset creation pipeline, from data
collection to video annotation, with the following demo application. Several
neural networks are trained and evaluated on the Slovo to demonstrate its
teaching ability. Proposed data and pre-trained models are publicly available.",None,-1
26689db1-53da-4594-ae6a-8dda951cbd8a,Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification,0.983116,"Due to the modality gap between visible and infrared images with high visual
ambiguity, learning \textbf{diverse} modality-shared semantic concepts for
visible-infrared person re-identification (VI-ReID) remains a challenging
problem. Body shape is one of the significant modality-shared cues for VI-ReID.
To dig more diverse modality-shared cues, we expect that erasing
body-shape-related semantic concepts in the learned features can force the ReID
model to extract more and other modality-shared features for identification. To
this end, we propose shape-erased feature learning paradigm that decorrelates
modality-shared features in two orthogonal subspaces. Jointly learning
shape-related feature in one subspace and shape-erased features in the
orthogonal complement achieves a conditional mutual information maximization
between shape-erased feature and identity discarding body shape information,
thus enhancing the diversity of the learned representation explicitly.
Extensive experiments on SYSU-MM01, RegDB, and HITSZ-VCM datasets demonstrate
the effectiveness of our method.",None,-1
36d01dd3-646e-4abf-8884-c13cdbacf708,HIORE: Leveraging High-order Interactions for Unified Entity Relation Extraction,0.343354,"Entity relation extraction consists of two sub-tasks: entity recognition and
relation extraction. Existing methods either tackle these two tasks separately
or unify them with word-by-word interactions. In this paper, we propose HIORE,
a new method for unified entity relation extraction. The key insight is to
leverage the high-order interactions, i.e., the complex association among word
pairs, which contains richer information than the first-order word-by-word
interactions. For this purpose, we first devise a W-shape DNN (WNet) to capture
coarse-level high-order connections. Then, we build a heuristic high-order
graph and further calibrate the representations with a graph neural network
(GNN). Experiments on three benchmarks (ACE04, ACE05, SciERC) show that HIORE
achieves the state-of-the-art performance on relation extraction and an
improvement of 1.1~1.8 F1 points over the prior best unified model.",None,-1
9f178ba2-fc86-418c-9270-6d3dd6ea7e82,Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation,0.913446,"Large Language Models (LLMs) can generate biased and toxic responses. Yet
most prior work on LLM gender bias evaluation requires predefined
gender-related phrases or gender stereotypes, which are challenging to be
comprehensively collected and are limited to explicit bias evaluation. In
addition, we believe that instances devoid of gender-related language or
explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in
this work, we propose a conditional text generation mechanism without the need
for predefined gender phrases and stereotypes. This approach employs three
types of inputs generated through three distinct strategies to probe LLMs,
aiming to show evidence of explicit and implicit gender biases in LLMs. We also
utilize explicit and implicit evaluation metrics to evaluate gender bias in
LLMs under different strategies. Our experiments demonstrate that an increased
model size does not consistently lead to enhanced fairness and all tested LLMs
exhibit explicit and/or implicit gender bias, even when explicit gender
stereotypes are absent in the inputs.",None,-1
caf6575c-c5a7-45d2-aea7-52d9b0a48dbe,Asynchronous Events-based Panoptic Segmentation using Graph Mixer Neural Network,0.070841,"In the context of robotic grasping, object segmentation encounters several
difficulties when faced with dynamic conditions such as real-time operation,
occlusion, low lighting, motion blur, and object size variability. In response
to these challenges, we propose the Graph Mixer Neural Network that includes a
novel collaborative contextual mixing layer, applied to 3D event graphs formed
on asynchronous events. The proposed layer is designed to spread spatiotemporal
correlation within an event graph at four nearest neighbor levels parallelly.
We evaluate the effectiveness of our proposed method on the Event-based
Segmentation (ESD) Dataset, which includes five unique image degradation
challenges, including occlusion, blur, brightness, trajectory, scale variance,
and segmentation of known and unknown objects. The results show that our
proposed approach outperforms state-of-the-art methods in terms of mean
intersection over the union and pixel accuracy. Code available at:
https://github.com/sanket0707/GNN-Mixer.git",None,-1
d4e779ac-ddea-4657-a2b5-bf7319f880ad,A Geometric Perspective on Diffusion Models,0.174317,"Recent years have witnessed significant progress in developing effective
training and fast sampling techniques for diffusion models. A remarkable
advancement is the use of stochastic differential equations (SDEs) and their
marginal-preserving ordinary differential equations (ODEs) to describe data
perturbation and generative modeling in a unified framework. In this paper, we
carefully inspect the ODE-based sampling of a popular variance-exploding SDE
and reveal several intriguing structures of its sampling dynamics. We discover
that the data distribution and the noise distribution are smoothly connected
with a quasi-linear sampling trajectory and another implicit denoising
trajectory that even converges faster. Meanwhile, the denoising trajectory
governs the curvature of the corresponding sampling trajectory and its various
finite differences yield all second-order samplers used in practice.
Furthermore, we establish a theoretical relationship between the optimal
ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with
which we can characterize the asymptotic behavior of diffusion models and
identify the empirical score deviation.",None,-1
5f7660ba-c5d0-4362-8cdb-5b77e2397e2a,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,0.776024,"A robot operating in a household environment will see a wide range of unique
and unfamiliar objects. While a system could train on many of these, it is
infeasible to predict all the objects a robot will see. In this paper, we
present a method to generalize object manipulation skills acquired from a
limited number of demonstrations, to novel objects from unseen shape
categories. Our approach, Local Neural Descriptor Fields (L-NDF), utilizes
neural descriptors defined on the local geometry of the object to effectively
transfer manipulation demonstrations to novel objects at test time. In doing
so, we leverage the local geometry shared between objects to produce a more
general manipulation framework. We illustrate the efficacy of our approach in
manipulating novel objects in novel poses -- both in simulation and in the real
world.",None,-1
5ad841be-044e-4edd-90ad-b7fbb27bdf11,"Milestones in Autonomous Driving and Intelligent Vehicles Part I: Control, Computing System Design, Communication, HD Map, Testing, and Human Behaviors",0.867843,"Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing
at a rapid pace due to the convenience, safety, and economic benefits. Although
a number of surveys have reviewed research achievements in this field, they are
still limited in specific tasks and lack systematic summaries and research
directions in the future. Our work is divided into 3 independent articles and
the first part is a Survey of Surveys (SoS) for total technologies of AD and
IVs that involves the history, summarizes the milestones, and provides the
perspectives, ethics, and future research directions. This is the second part
(Part I for this technical survey) to review the development of control,
computing system design, communication, High Definition map (HD map), testing,
and human behaviors in IVs. In addition, the third part (Part II for this
technical survey) is to review the perception and planning sections. The
objective of this paper is to involve all the sections of AD, summarize the
latest technical milestones, and guide abecedarians to quickly understand the
development of AD and IVs. Combining the SoS and Part II, we anticipate that
this work will bring novel and diverse insights to researchers and
abecedarians, and serve as a bridge between past and future.",None,-1
d033b69a-e7d3-41df-9fce-dc64f82f65a4,Imitating Human Behaviour with Diffusion Models,1.0,"Diffusion models have emerged as powerful generative models in the
text-to-image domain. This paper studies their application as
observation-to-action models for imitating human behaviour in sequential
environments. Human behaviour is stochastic and multimodal, with structured
correlations between action dimensions. Meanwhile, standard modelling choices
in behaviour cloning are limited in their expressiveness and may introduce bias
into the cloned policy. We begin by pointing out the limitations of these
choices. We then propose that diffusion models are an excellent fit for
imitating human behaviour, since they learn an expressive distribution over the
joint action space. We introduce several innovations to make diffusion models
suitable for sequential environments; designing suitable architectures,
investigating the role of guidance, and developing reliable sampling
strategies. Experimentally, diffusion models closely match human demonstrations
in a simulated robotic control task and a modern 3D gaming environment.",None,-1
bff30f11-9b84-46ca-99fe-c1c5c14abf2a,Large Language Model Enhanced Multi-Agent Systems for 6G Communications,0.897881,"The rapid development of the Large Language Model (LLM) presents huge
opportunities for 6G communications, e.g., network optimization and management
by allowing users to input task requirements to LLMs by nature language.
However, directly applying native LLMs in 6G encounters various challenges,
such as a lack of private communication data and knowledge, limited logical
reasoning, evaluation, and refinement abilities. Integrating LLMs with the
capabilities of retrieval, planning, memory, evaluation and reflection in
agents can greatly enhance the potential of LLMs for 6G communications. To this
end, we propose a multi-agent system with customized communication knowledge
and tools for solving communication related tasks using natural language,
comprising three components: (1) Multi-agent Data Retrieval (MDR), which
employs the condensate and inference agents to refine and summarize
communication knowledge from the knowledge base, expanding the knowledge
boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning
(MCP), which utilizes multiple planning agents to generate feasible solutions
for the communication related task from different perspectives based on the
retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which
utilizes the evaluation agent to assess the solutions, and applies the
reflexion agent and refinement agent to provide improvement suggestions for
current solutions. Finally, we validate the effectiveness of the proposed
multi-agent system by designing a semantic communication system, as a case
study of 6G communications.",None,-1
eb9b8a1a-e551-4036-bcf4-d935f636010a,Spatial Knowledge-Infused Hierarchical Learning: An Application in Flood Mapping on Earth Imagery,0.522586,"Deep learning for Earth imagery plays an increasingly important role in
geoscience applications such as agriculture, ecology, and natural disaster
management. Still, progress is often hindered by the limited training labels.
Given Earth imagery with limited training labels, a base deep neural network
model, and a spatial knowledge base with label constraints, our problem is to
infer the full labels while training the neural network. The problem is
challenging due to the sparse and noisy input labels, spatial uncertainty
within the label inference process, and high computational costs associated
with a large number of sample locations. Existing works on neuro-symbolic
models focus on integrating symbolic logic into neural networks (e.g., loss
function, model architecture, and training label augmentation), but these
methods do not fully address the challenges of spatial data (e.g., spatial
uncertainty, the trade-off between spatial granularity and computational
costs). To bridge this gap, we propose a novel Spatial Knowledge-Infused
Hierarchical Learning (SKI-HL) framework that iteratively infers sample labels
within a multi-resolution hierarchy. Our framework consists of a module to
selectively infer labels in different resolutions based on spatial uncertainty
and a module to train neural network parameters with uncertainty-aware
multi-instance learning. Extensive experiments on real-world flood mapping
datasets show that the proposed model outperforms several baseline methods. The
code is available at \url{https://github.com/ZelinXu2000/SKI-HL}.",None,-1
58ffba51-221e-46de-a1b0-a6e95b76ba09,Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models,0.174284,"Large pre-trained language models (PLMs) have demonstrated strong performance
on natural language understanding (NLU) tasks through fine-tuning. However,
fine-tuned models still suffer from overconfident predictions, especially in
out-of-domain settings. In this paper, we tackle the problem of calibrating
fine-tuned language models. We demonstrate that the PLMs are well-calibrated on
the masked language modeling task with robust predictive confidence under
domain shift, yet the fine-tuned models fail to retain such property due to
catastrophic forgetting, which impacts the calibration on the downstream
classification task. In light of these observations, we evaluate the
calibration of several methods that preserve pre-trained features and show that
preserving pre-trained features can improve the calibration of fine-tuned
language models. Among these methods, our proposed method that encourages the
fine-tuned model to learn generative representations with auxiliary language
modeling objective achieves competitive accuracy and the lowest expected
calibration error compared to several strong baselines under both in-domain and
out-of-domain settings on three downstream NLU tasks.",None,-1
72ed5c0d-7f2f-4ffb-8fcf-39bb88e6ba34,IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,0.493701,"Although counterfactual reasoning is a fundamental aspect of intelligence,
the lack of large-scale counterfactual open-domain question-answering (QA)
benchmarks makes it difficult to evaluate and improve models on this ability.
To address this void, we introduce the first such dataset, named IfQA, where
each question is based on a counterfactual presupposition via an ""if"" clause.
For example, if Los Angeles was on the east coast of the U.S., what would be
the time difference between Los Angeles and Paris? Such questions require
models to go beyond retrieving direct factual knowledge from the Web: they must
identify the right information to retrieve and reason about an imagined
situation that may even go against the facts built into their parameters. The
IfQA dataset contains over 3,800 questions that were annotated annotated by
crowdworkers on relevant Wikipedia passages. Empirical analysis reveals that
the IfQA dataset is highly challenging for existing open-domain QA methods,
including supervised retrieve-then-read pipeline methods (EM score 36.2), as
well as recent few-shot approaches such as chain-of-thought prompting with
GPT-3 (EM score 27.4). The unique challenges posed by the IfQA benchmark will
push open-domain QA research on both retrieval and counterfactual reasoning
fronts.",None,-1
39239bb3-6269-49c3-90d1-23cede34f3b6,IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction,0.845477,"Reliable multi-agent trajectory prediction is crucial for the safe planning
and control of autonomous systems. Compared with single-agent cases, the major
challenge in simultaneously processing multiple agents lies in modeling complex
social interactions caused by various driving intentions and road conditions.
Previous methods typically leverage graph-based message propagation or
attention mechanism to encapsulate such interactions in the format of marginal
probabilistic distributions. However, it is inherently sub-optimal. In this
paper, we propose IPCC-TP, a novel relevance-aware module based on Incremental
Pearson Correlation Coefficient to improve multi-agent interaction modeling.
IPCC-TP learns pairwise joint Gaussian Distributions through the
tightly-coupled estimation of the means and covariances according to
interactive incremental movements. Our module can be conveniently embedded into
existing multi-agent prediction methods to extend original motion distribution
decoders. Extensive experiments on nuScenes and Argoverse 2 datasets
demonstrate that IPCC-TP improves the performance of baselines by a large
margin.",None,-1
90fa03b1-0d2a-4864-b3b6-4d5ccb90ddda,AFPN: Asymptotic Feature Pyramid Network for Object Detection,0.829378,"Multi-scale features are of great importance in encoding objects with scale
variance in object detection tasks. A common strategy for multi-scale feature
extraction is adopting the classic top-down and bottom-up feature pyramid
networks. However, these approaches suffer from the loss or degradation of
feature information, impairing the fusion effect of non-adjacent levels. This
paper proposes an asymptotic feature pyramid network (AFPN) to support direct
interaction at non-adjacent levels. AFPN is initiated by fusing two adjacent
low-level features and asymptotically incorporates higher-level features into
the fusion process. In this way, the larger semantic gap between non-adjacent
levels can be avoided. Given the potential for multi-object information
conflicts to arise during feature fusion at each spatial location, adaptive
spatial fusion operation is further utilized to mitigate these inconsistencies.
We incorporate the proposed AFPN into both two-stage and one-stage object
detection frameworks and evaluate with the MS-COCO 2017 validation and test
datasets. Experimental evaluation shows that our method achieves more
competitive results than other state-of-the-art feature pyramid networks. The
code is available at
\href{https://github.com/gyyang23/AFPN}{https://github.com/gyyang23/AFPN}.",None,-1
06b59f20-056a-4950-ab0d-6b31b86d3914,Songs Across Borders: Singable and Controllable Neural Lyric Translation,0.484255,"The development of general-domain neural machine translation (NMT) methods
has advanced significantly in recent years, but the lack of naturalness and
musical constraints in the outputs makes them unable to produce singable lyric
translations. This paper bridges the singability quality gap by formalizing
lyric translation into a constrained translation problem, converting
theoretical guidance and practical techniques from translatology literature to
prompt-driven NMT approaches, exploring better adaptation methods, and
instantiating them to an English-Chinese lyric translation system. Our model
achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and
word boundary recall. In our subjective evaluation, our model shows a 75%
relative enhancement on overall quality, compared against naive fine-tuning
(Code available at https://github.com/Sonata165/ControllableLyricTranslation).",None,-1
e7b2980e-25e1-45bf-8b92-08d45ac5158a,GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab,0.406418,"The integration of robots in chemical experiments has enhanced experimental
efficiency, but lacking the human intelligence to comprehend literature, they
seldom provide assistance in experimental design. Therefore, achieving
full-process autonomy from experiment design to validation in self-driven
laboratories (SDL) remains a challenge. The introduction of Generative
Pre-trained Transformers (GPT), particularly GPT-4, into robotic
experimentation offers a solution. We introduce GPT-Lab, a paradigm that
employs GPT models to give robots human-like intelligence. With our robotic
experimentation platform, GPT-Lab mines literature for materials and methods
and validates findings through high-throughput synthesis. As a demonstration,
GPT-Lab analyzed 500 articles, identified 18 potential reagents, and
successfully produced an accurate humidity colorimetric sensor with a root mean
square error (RMSE) of 2.68%. This showcases the rapid materials discovery and
validation potential of our system.",None,-1
12ef3f0f-d03b-4c58-82be-43d5d557d501,Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature,0.234385,"We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large
language model to engage in meaningful interactions with Astronomy papers using
in-context prompting. To optimize for efficiency, we employ a distillation
technique that effectively reduces the size of the original input paper by
50\%, while maintaining the paragraph structure and overall semantic integrity.
We then explore the model's responses using a multi-document context (ten
distilled documents). Our findings indicate that GPT-4 excels in the
multi-document domain, providing detailed answers contextualized within the
framework of related research findings. Our results showcase the potential of
large language models for the astronomical community, offering a promising
avenue for further exploration, particularly the possibility of utilizing the
models for hypothesis generation.",None,-1
43d2c366-347b-4992-aae6-1ce46d8eedbb,C3: Zero-shot Text-to-SQL with ChatGPT,0.935833,"This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,
which achieves 82.3\% in terms of execution accuracy on the holdout test set of
Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the
Spider Challenge. C3 consists of three key components: Clear Prompting (CP),
Calibration with Hints (CH), and Consistent Output (CO), which are
corresponding to the model input, model bias and model output respectively. It
provides a systematic treatment for zero-shot Text-to-SQL. Extensive
experiments have been conducted to verify the effectiveness and efficiency of
our proposed method.",None,-1
976fdf72-6e70-4e0d-9912-9ae980934c75,IMF: Interactive Multimodal Fusion Model for Link Prediction,0.8842,"Link prediction aims to identify potential missing triples in knowledge
graphs. To get better results, some recent studies have introduced multimodal
information to link prediction. However, these methods utilize multimodal
information separately and neglect the complicated interaction between
different modalities. In this paper, we aim at better modeling the
inter-modality information and thus introduce a novel Interactive Multimodal
Fusion (IMF) model to integrate knowledge from different modalities. To this
end, we propose a two-stage multimodal fusion framework to preserve
modality-specific knowledge as well as take advantage of the complementarity
between different modalities. Instead of directly projecting different
modalities into a unified space, our multimodal fusion module limits the
representations of different modalities independent while leverages bilinear
pooling for fusion and incorporates contrastive learning as additional
constraints. Furthermore, the decision fusion module delivers the learned
weighted average over the predictions of all modalities to better incorporate
the complementarity of different modalities. Our approach has been demonstrated
to be effective through empirical evaluations on several real-world datasets.
The implementation code is available online at
https://github.com/HestiaSky/IMF-Pytorch.",None,-1
